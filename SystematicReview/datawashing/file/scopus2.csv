作者,作者 ID,标题,年份,来源出版物名称,卷,期,论文编号,起始页码,结束页码,页码计数,施引文献,DOI,链接,归属机构,带归属机构的作者,摘要,作者关键字,索引关键字,文献类型,出版阶段,开放获取,来源出版物,EID
"Kim J.-H., Jeong J.-W.","57205720618;57205722835;","Multi-view multi-modal head-gaze estimation for advanced indoor user interaction",2022,"Computers, Materials and Continua","70","3",,"5107","5132",,,"10.32604/cmc.2022.021107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117047706&doi=10.32604%2fcmc.2022.021107&partnerID=40&md5=304c891eee8c6e2f5b4d7f28298b5081","Department of Computer Engineering, Kumoh National Institute of Technology, Gumi, 39177, South Korea; Department of Data Science, Seoul National University of Science and Technology, Seoul, 01811, South Korea","Kim, J.-H., Department of Computer Engineering, Kumoh National Institute of Technology, Gumi, 39177, South Korea; Jeong, J.-W., Department of Data Science, Seoul National University of Science and Technology, Seoul, 01811, South Korea","Gaze estimation is one of the most promising technologies for supporting indoor monitoring and interaction systems. However, previous gaze estimation techniques generally work only in a controlled laboratory environment because they require a number of high-resolution eye images. This makes them unsuitable for welfare and healthcare facilities with the following challenging characteristics: 1) users’ continuous movements, 2) various lighting conditions, and 3) a limited amount of available data. To address these issues, we introduce a multi-view multi-modal head-gaze estimation system that translates the user’s head orientation into the gaze direction. The proposed system captures the user using multiple cameras with depth and infrared modalities to train more robust gaze estimators under the aforementioned conditions. To this end, we implemented a deep learning pipeline that can handle different types and combinations of data. The proposed system was evaluated using the data collected from 10 volunteer participants to analyze how the use of single/multiple cameras and modalities affect the performance of head-gaze estimators. Through various experiments, we found that 1) an infrared-modality provides more useful features than a depth-modality, 2) multi-view multi-modal approaches provide better accuracy than single-view single-modal approaches, and 3) the proposed estimators achieve a high inference efficiency that can be used in real-time applications. © 2022 Tech Science Press. All rights reserved.","Deep learning; Head-gaze estimation; Human-computer interaction; Indoor monitoring","Cameras; Deep learning; Modal analysis; Deep learning; Gaze estimation; Head-gaze estimation; Indoor monitoring; Interaction systems; Monitoring system; Multi-modal; Multi-views; Multiple cameras; User interaction; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85117047706
"Cao Y., Ding Y., Proctor R.W., Duffy V.G., Liu Y., Zhang X.","55470381500;55788266100;7101687932;7007049892;57226614859;56550074800;","Detecting users’ usage intentions for websites employing deep learning on eye-tracking data",2021,"Information Technology and Management","22","4",,"281","292",,,"10.1007/s10799-021-00336-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113983156&doi=10.1007%2fs10799-021-00336-6&partnerID=40&md5=530c1c006092f6985e4e07e1860bff2d","School of Economics and Management, Anhui Polytechnic University, NO. 8 Beijing Middle Road, Jiujiang district, Wuhu, 241000, China; School of Industrial Engineering, Purdue University, West Lafayette, United States","Cao, Y., School of Economics and Management, Anhui Polytechnic University, NO. 8 Beijing Middle Road, Jiujiang district, Wuhu, 241000, China; Ding, Y., School of Economics and Management, Anhui Polytechnic University, NO. 8 Beijing Middle Road, Jiujiang district, Wuhu, 241000, China; Proctor, R.W., School of Industrial Engineering, Purdue University, West Lafayette, United States; Duffy, V.G., School of Industrial Engineering, Purdue University, West Lafayette, United States; Liu, Y., School of Economics and Management, Anhui Polytechnic University, NO. 8 Beijing Middle Road, Jiujiang district, Wuhu, 241000, China; Zhang, X., School of Economics and Management, Anhui Polytechnic University, NO. 8 Beijing Middle Road, Jiujiang district, Wuhu, 241000, China","We proposed a method employing deep learning (DL) on eye-tracking data and applied this method to detect intentions to use apparel websites that differed in factors of depth, breadth, and location of navigation. Results showed that users’ intentions could be predicted by combining a deep neural network algorithm and metrics recorded from an eye-tracker. Using all of the eye-tracking metric features attained the best accuracy when predicting usage/not-usage intention to websites. In addition, the results suggest that for apparel websites with the same depth, designers can increase usage intention by using a larger number of navigation items and placing the navigation at the top and left of the homepage. The results show that building intelligent usage intention-detection systems is possible for the range of websites we examined and is also computationally practical. Hence, the study motivates future investigations that focus on design of such systems. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Behavioral intention; Deep learning; Eye-tracking; Website",,Article,"Final","",Scopus,2-s2.0-85113983156
"Meier I.B., Buegler M., Harms R., Seixas A., Çöltekin A., Tarnanas I.","57221867410;57221865756;57193828961;56567942200;8598265600;6508357922;","Using a Digital Neuro Signature to measure longitudinal individual-level change in Alzheimer’s disease: the Altoida large cohort study",2021,"npj Digital Medicine","4","1","101","","",,,"10.1038/s41746-021-00470-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108845870&doi=10.1038%2fs41746-021-00470-z&partnerID=40&md5=45b67af521a1cc35e4fc33d952c15d1c","Chione GmbH, Binz, Switzerland; Altoida Inc., Houston, TX, United States; NYU Grossman School of Medicine, Department of Population Health, Department of Psychiatry, New York, NY, United States; University of Applied Sciences and Arts Northwestern Switzerland, Windisch, Switzerland; Global Brain Health Institute, San Francisco, CA, United States; Trinity College Dublin, College Green, Dublin, Ireland; Hellenic Initiative Against Alzheimer’s Disease, BiHeLab Ionian University, Kerkira, Greece","Meier, I.B., Chione GmbH, Binz, Switzerland; Buegler, M., Altoida Inc., Houston, TX, United States; Harms, R., Altoida Inc., Houston, TX, United States; Seixas, A., NYU Grossman School of Medicine, Department of Population Health, Department of Psychiatry, New York, NY, United States; Çöltekin, A., University of Applied Sciences and Arts Northwestern Switzerland, Windisch, Switzerland; Tarnanas, I., Altoida Inc., Houston, TX, United States, Global Brain Health Institute, San Francisco, CA, United States, Trinity College Dublin, College Green, Dublin, Ireland, Hellenic Initiative Against Alzheimer’s Disease, BiHeLab Ionian University, Kerkira, Greece","Conventional neuropsychological assessments for Alzheimer’s disease are burdensome and inaccurate at detecting mild cognitive impairment and predicting Alzheimer’s disease risk. Altoida’s Digital Neuro Signature (DNS), a longitudinal cognitive test consisting of two active digital biomarker metrics, alleviates these limitations. By comparison to conventional neuropsychological assessments, DNS results in faster evaluations (10 min vs 45–120 min), and generates higher test-retest in intraindividual assessment, as well as higher accuracy at detecting abnormal cognition. This study comparatively evaluates the performance of Altoida’s DNS and conventional neuropsychological assessments in intraindividual assessments of cognition and function by means of two semi-naturalistic observational experiments with 525 participants in laboratory and clinical settings. The results show that DNS is consistently more sensitive than conventional neuropsychological assessments at capturing longitudinal individual-level change, both with respect to intraindividual variability and dispersion (intraindividual variability across multiple tests), across three participant groups: healthy controls, mild cognitive impairment, and Alzheimer’s disease. Dispersion differences between DNS and conventional neuropsychological assessments were more pronounced with more advanced disease stages, and DNS-intraindividual variability was able to predict conversion from mild cognitive impairment to Alzheimer’s disease. These findings are instrumental for patient monitoring and management, remote clinical trial assessment, and timely interventions, and will hopefully contribute to a better understanding of Alzheimer’s disease. © 2021, The Author(s).",,"amyloid beta protein; apolipoprotein E; biological marker; tau protein; adult; aged; Altoida Digital Neuro Signature; Alzheimer disease; Article; attention; augmented reality; cerebrospinal fluid analysis; Clinical Dementia Rating; clinical outcome; cognition; cohort analysis; comparative effectiveness; controlled study; disease exacerbation; dispersion; eye tracking; female; healthy aging; human; intermethod comparison; machine learning; major clinical study; male; memory; mild cognitive impairment; motor performance; neuroimaging; neuropsychological test; nuclear magnetic resonance imaging; observational study; patient care; patient monitoring; processing speed; reaction time; velocity; very elderly; Wechsler memory scale",Article,"Final","",Scopus,2-s2.0-85108845870
"Zhuang J., Dong Y., Bai H.","57201854167;13104876700;37050495500;","Ensemble learning with siamese networks for visual tracking",2021,"Neurocomputing","464",,,"497","506",,,"10.1016/j.neucom.2021.08.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114630403&doi=10.1016%2fj.neucom.2021.08.025&partnerID=40&md5=d6d83f25dab538dd9f41e1acdb3e2888","Beijing University of Posts and Telecommunications, China; Beijing Faceall Technology Co.,Ltd, Beijing, China","Zhuang, J., Beijing University of Posts and Telecommunications, China; Dong, Y., Beijing University of Posts and Telecommunications, China; Bai, H., Beijing Faceall Technology Co.,Ltd, Beijing, China","Ensemble learning (EL) is an effective and commonly used technique to improve visual tasks’ accuracy, such as classification and detection. However, EL is rarely used in visual tracking. To fill this knowledge gap, we first have completed some research to investigate why knowledge distillation was ineffective in visual tracking tasks. Comparing the difference between the classification and visual tracking, conclusions are given: (i) Numerous simple negative examples are redundant, while only a few hard negative samples are valid for visual tracking knowledge distillation. (ii) The hint knowledge flows differently between classification and visual tracking. To solve the above problems, we design two new loss functions and integrate them into the proposed Ensemble Learning (EL) framework that can be employed in Siamese architectures such as SiamFC, SiamRPN, SiamFC+, and SiamRPN+. The EL treats two Siamese networks as students and enables them to learn collaboratively. A better solution is yielded by the EL framework than training students individually. Experiments on OTB-2013, OTB-2015, VOT2015, VOT2016, VOT2017, VOT2018, LaSOT and TrackingNet have verified the effectiveness of our proposed technique on boosting the performance for the four Siamese algorithms. The EL-SiamRPN+ achieves leading performance in the challenges. © 2021 Elsevier B.V.","Deep learning; Knowledge distillation; Siamese network; Visual tracking","Distillation; Vision; Deep learning; Ensemble learning; Knowledge distillation; Knowledge gaps; Learning frameworks; Performance; Siamese network; Simple++; Visual tasks; Visual Tracking; Deep learning; algorithm; article; case report; clinical article; deep learning; distillation; eye tracking; human; human experiment; loss of function mutation",Article,"Final","",Scopus,2-s2.0-85114630403
"Ploumpis S., Ververas E., Sullivan E.O., Moschoglou S., Wang H., Pears N., Smith W.A.P., Gecer B., Zafeiriou S.","56648048200;57200120997;57219441614;57193191974;57210362342;6602738213;8855793100;57192671195;8883680000;","Towards a Complete 3D Morphable Model of the Human Head",2021,"IEEE Transactions on Pattern Analysis and Machine Intelligence","43","11",,"4142","4160",,6,"10.1109/TPAMI.2020.2991150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116514929&doi=10.1109%2fTPAMI.2020.2991150&partnerID=40&md5=6b299cf4805aedfc023c6d00abfadee2","Department of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Department of Computer Science, University of York, York, YO10 5DD, United Kingdom","Ploumpis, S., Department of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Ververas, E., Department of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Sullivan, E.O., Department of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Moschoglou, S., Department of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Wang, H., Department of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Pears, N., Department of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom, Department of Computer Science, University of York, York, YO10 5DD, United Kingdom; Smith, W.A.P., Department of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom, Department of Computer Science, University of York, York, YO10 5DD, United Kingdom; Gecer, B., Department of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Zafeiriou, S., Department of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom","Three-dimensional morphable models (3DMMs) are powerful statistical tools for representing the 3D shapes and textures of an object class. Here we present the most complete 3DMM of the human head to date that includes face, cranium, ears, eyes, teeth and tongue. To achieve this, we propose two methods for combining existing 3DMMs of different overlapping head parts: (i). use a regressor to complete missing parts of one model using the other, and (ii). use the Gaussian Process framework to blend covariance matrices from multiple models. Thus, we build a new combined face-and-head shape model that blends the variability and facial detail of an existing face model (the LSFM) with the full head modelling capability of an existing head model (the LYHM). Then we construct and fuse a highly-detailed ear model to extend the variation of the ear shape. Eye and eye region models are incorporated into the head model, along with basic models of the teeth, tongue and inner mouth cavity. The new model achieves state-of-the-art performance. We use our model to reconstruct full head representations from single, unconstrained images allowing us to parameterize craniofacial shape and texture, along with the ear shape, eye gaze and eye color. © 1979-2012 IEEE.","3D reconstruction; 3DMM; craniofacial 3DMM; morphable model combination","3D modeling; Image reconstruction; Statistical mechanics; Textures; Three dimensional computer graphics; 3D reconstruction; 3DMM; Craniofacial; Craniofacial 3DMM; Head model; Human head; Model combination; Morphable model; Morphable model combination; Shape and textures; Covariance matrix",Article,"Final","",Scopus,2-s2.0-85116514929
"Cho D.-Y., Kang M.-K.","57200616887;55453176100;","Human gaze-aware attentive object detection for ambient intelligence",2021,"Engineering Applications of Artificial Intelligence","106",,"104471","","",,,"10.1016/j.engappai.2021.104471","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116125642&doi=10.1016%2fj.engappai.2021.104471&partnerID=40&md5=3ea3c0aa35286fa3f69c99076ca9e8ed","Korea Institute of Science and Technology (KIST), Seongbuk-gu, Seoul, 02792, South Korea","Cho, D.-Y., Korea Institute of Science and Technology (KIST), Seongbuk-gu, Seoul, 02792, South Korea; Kang, M.-K., Korea Institute of Science and Technology (KIST), Seongbuk-gu, Seoul, 02792, South Korea","Understanding human behavior and the surrounding environment is essential for realizing ambient intelligence (AmI), for which eye gaze and object information are reliable cues. In this study, the authors propose a novel human gaze-aware attentive object detection framework as an elemental technology for AmI. The proposed framework detects users’ attentive objects and shows more precise and robust performance against object-scale variations. A novel Adaptive-3D-Region-of-Interest (Ada-3D-RoI) scheme is designed as a front-end module, and scalable detection network structures are proposed to maximize cost-efficiency. The experiments show that the detection rate is improved up to 97.6% on small objects (14.1% on average), and it is selectively tunable with a tradeoff between accuracy and computational complexity. In addition, the qualitative results demonstrate that the proposed framework detects a user's single object-of-interest only, even when the target object is occluded or extremely small. Complementary matters for follow-up study are presented as suggestions to extend the results of the proposed framework to further practical AmI applications. This study will help develop advanced AmI applications that demand a higher-level understanding of scene context and human behavior such as human–robot symbiosis, remote-/autonomous control, and augmented/mixed reality. © 2021 The Author(s)","Affective ambient intelligence; Augmented and mixed reality; Human–computer interaction; Object recognition","Ambient intelligence; Artificial intelligence; Behavioral research; Image segmentation; Mixed reality; Object detection; Object recognition; Affective ambient intelligence; Eye-gaze; Human behaviors; Mixed reality; Object information; Objects recognition; Region-of-interest; Regions of interest; Robust performance; Surrounding environment; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85116125642
"Ansari M.F., Kasprowski P., Obetkal M.","57221608762;8940684200;57279844500;","Gaze tracking using an unmodified web camera and convolutional neural network",2021,"Applied Sciences (Switzerland)","11","19","9068","","",,,"10.3390/app11199068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116006986&doi=10.3390%2fapp11199068&partnerID=40&md5=18fcbeeec0e7eebe010141c9d3cc9c29","Department of Applied Informatics, Silesian University of Technology, Gliwice, 44-100, Poland","Ansari, M.F., Department of Applied Informatics, Silesian University of Technology, Gliwice, 44-100, Poland; Kasprowski, P., Department of Applied Informatics, Silesian University of Technology, Gliwice, 44-100, Poland; Obetkal, M., Department of Applied Informatics, Silesian University of Technology, Gliwice, 44-100, Poland","Gaze estimation plays a significant role in understating human behavior and in human– computer interaction. Currently, there are many methods accessible for gaze estimation. However, most approaches need additional hardware for data acquisition which adds an extra cost to gaze tracking. The classic gaze tracking approaches usually require systematic prior knowledge or expertise for practical operations. Moreover, they are fundamentally based on the characteristics of the eye region, utilizing infrared light and iris glint to track the gaze point. It requires high-quality images with particular environmental conditions and another light source. Recent studies on appearance-based gaze estimation have demonstrated the capability of neural networks, especially convolutional neural networks (CNN), to decode gaze information present in eye images and achieved significantly simplified gaze estimation. In this paper, a gaze estimation method that utilizes a CNN for gaze estimation that can be applied to various platforms without additional hardware is presented. An easy and fast data collection method is used for collecting face and eyes images from an unmodified desktop camera. The proposed method registered good results; it proves that it is possible to predict the gaze with reasonable accuracy without any additional tools. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network; Gaze tracking; Human computer interaction",,Article,"Final","",Scopus,2-s2.0-85116006986
"Hsiao S.-W., Peng P.-H., Tsao Y.-C.","9841389000;57226784204;57195065642;","A method for the analysis of the interaction between users and objects in 3D navigational space",2021,"Advanced Engineering Informatics","50",,"101364","","",,,"10.1016/j.aei.2021.101364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112602928&doi=10.1016%2fj.aei.2021.101364&partnerID=40&md5=48f19e940dab022f9954a9d5d64cdc90","Department of Industrial Design, National Cheng Kung University, Tainan, 70101, Taiwan","Hsiao, S.-W., Department of Industrial Design, National Cheng Kung University, Tainan, 70101, Taiwan; Peng, P.-H., Department of Industrial Design, National Cheng Kung University, Tainan, 70101, Taiwan; Tsao, Y.-C., Department of Industrial Design, National Cheng Kung University, Tainan, 70101, Taiwan","Along with the improvement of eye-tracking technology, more and more distinct field of researches have introduced movements of the eye in relation to the head to understand user behavior. Most of current researches focus on the perception process of single 2-dimensional images by fixed eye-tracking devices or the head-mount devices. A method of applying eye-tracking on the analysis of the interaction between users and objects in 3D navigational space is proposed in this article. It aims to understand the visual stimulation of 3D objects and the user's spatial navigational reactions while receiving the stimulation, and proposes the concept of 3D object attention heat map. It also proposes to construct a computational visual attention model for different geometric featured 3D objects by applying the method of feature curves. The VR results of this study also provide future assistance in the incoming immersive world. This study sets to promote eye-tracking from the mainstream of 2D field to 3D spaces and points to a deeper understanding between human and artificial product or natural objects. It would also serve an important role in the field of human-computer interaction, product usability, aids devices for cognition degenerative individuals, and even the field of visual recognition of daily human behavior. © 2021 Elsevier Ltd","3D Interactive Cognition; Eye-Tracking; Human-Computer Interaction (HCI); Visual attention","Behavioral research; Eye movements; Eye tracking; Human computer interaction; Navigation; Eye tracking devices; Eye tracking technologies; Human behaviors; Natural objects; Product usability; Visual attention model; Visual recognition; Visual stimulation; Object tracking",Article,"Final","",Scopus,2-s2.0-85112602928
"Wu Q., Dey N., Shi F., Crespo R.G., Sherratt R.S.","57212567709;55356190900;23398821000;57221209077;7004090619;","Emotion classification on eye-tracking and electroencephalograph fused signals employing deep gradient neural networks",2021,"Applied Soft Computing","110",,"107752","","",,,"10.1016/j.asoc.2021.107752","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111540398&doi=10.1016%2fj.asoc.2021.107752&partnerID=40&md5=f559f12fc47768259c841ac31e4325dc","Institute of Universal Design, Zhejiang Sci-Tech University, Hangzhou, China; Department of Computer Science and Engineering, JIS University, Kolkata, 700109, India; Rutgers Cancer Institute of New Jersey, New Brunswick, NJ  08903, United States; Department of Computer Science and Technology, Universidad Internacional de La Rioja, Logroño, Spain; Department of Biomedical Engineering, the University of ReadingUK  RG6 6AY, United Kingdom","Wu, Q., Institute of Universal Design, Zhejiang Sci-Tech University, Hangzhou, China; Dey, N., Department of Computer Science and Engineering, JIS University, Kolkata, 700109, India; Shi, F., Rutgers Cancer Institute of New Jersey, New Brunswick, NJ  08903, United States; Crespo, R.G., Department of Computer Science and Technology, Universidad Internacional de La Rioja, Logroño, Spain; Sherratt, R.S., Department of Biomedical Engineering, the University of ReadingUK  RG6 6AY, United Kingdom","Emotion produces complex neural processes and physiological changes under appropriate event stimulation. Physiological signals have the advantage of better reflecting a person's actual emotional state than facial expressions or voice signals. An electroencephalogram (EEG) is a signal obtained by collecting, amplifying, and recording the human brain's weak bioelectric signals on the scalp. The eye-tracking (E.T.) signal records the potential difference between the retina and the cornea and the potential generated by the eye movement muscle. Furthermore, the different modalities of physiological signals will contain various information representations of human emotions. Finding this different modal information is of great help to get higher recognition accuracy. The E.T. and EEG signals are synchronized and fused in this research, and an effective deep learning (DL) method was used to combine different modalities. This article proposes a technique based on a fusion model of the Gaussian mixed model (GMM) with the Butterworth and Chebyshev signal filter. Features extraction on EEG and E.T. are subsequently calculated. Secondly, the self-similarity (SSIM), energy (E), complexity (C), high order crossing (HOC), and power spectral density (PSD) for EGG, and electrooculography power density estimation ((EOG-PDE), center gravity frequency (CGF), frequency variance (F.V.), root mean square frequency (RMSF) for E.T. are selected hereafter; the max–min method is applied for vector normalization. Finally, a deep gradient neural network (DGNN) for EEG and E.T. multimodal signal classification is proposed. The proposed neural network predicted the emotions under the eight emotions event stimuli experiment with 88.10% accuracy. For the evaluation indices of accuracy (Ac), precision (Pr), recall (Re), F-measurement (Fm), precision–recall (P.R.) curve, true-positive rate (TPR) of receiver operating characteristic curve (ROC), the area under the curve (AUC), true-accept rate (TAR), and interaction on union (IoU), the proposed method also performs with high efficiency compared with several typical neural networks including the artificial neural network (ANN), SqueezeNet, GoogleNet, ResNet-50, DarkNet-53, ResNet-18, Inception-ResNet, Inception-v3, and ResNet-101. © 2021 Elsevier B.V.","Electroencephalogram; Emotion stimuli; Eye-tracking; Fused deep neural network; Gaussian mixed model; Signal process","Bioelectric phenomena; Butterworth filters; Chebyshev filters; Complex networks; Deep learning; Deep neural networks; Electroencephalography; Eye movements; Eye tracking; Frequency estimation; Neural networks; Spectral density; Electro-encephalogram (EEG); Emotion classification; Highorder crossing (HOC); Information representation; Physiological signals; Power spectral densities (PSD); Receiver operating characteristic curves; Signal classification; Biomedical signal processing",Article,"Final","",Scopus,2-s2.0-85111540398
"Oishi S., Koide K., Yokozuka M., Banno A.","54793314500;57191584619;36027020900;55952407700;","4D Attention: Comprehensive Framework for Spatio-Temporal Gaze Mapping",2021,"IEEE Robotics and Automation Letters","6","4","9484834","7240","7247",,,"10.1109/LRA.2021.3097274","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110844998&doi=10.1109%2fLRA.2021.3097274&partnerID=40&md5=8bb13047997dab2b2dbe26f166297bf9","Mobile Robotics Research Team (MR2T), National Institute of Advanced Industrial Science and Technology (AIST), Ibaraki, 3058568, Japan","Oishi, S., Mobile Robotics Research Team (MR2T), National Institute of Advanced Industrial Science and Technology (AIST), Ibaraki, 3058568, Japan; Koide, K., Mobile Robotics Research Team (MR2T), National Institute of Advanced Industrial Science and Technology (AIST), Ibaraki, 3058568, Japan; Yokozuka, M., Mobile Robotics Research Team (MR2T), National Institute of Advanced Industrial Science and Technology (AIST), Ibaraki, 3058568, Japan; Banno, A., Mobile Robotics Research Team (MR2T), National Institute of Advanced Industrial Science and Technology (AIST), Ibaraki, 3058568, Japan","This study presents a framework for capturing human attention in the spatio-temporal domain using eye-tracking glasses. Attention mapping is a key technology for human perceptual activity analysis or Human-Robot Interaction (HRI) to support human visual cognition; however, measuring human attention in dynamic environments is challenging owing to the difficulty in localizing the subject and dealing with moving objects. To address this, we present a comprehensive framework, 4D Attention, for unified gaze mapping onto static and dynamic objects. Specifically, we estimate the glasses pose by leveraging a loose coupling of direct visual localization and Inertial Measurement Unit (IMU) values. Further, by installing reconstruction components into our framework, dynamic objects not captured in the 3D environment map are instantiated based on the input images. Finally, a scene rendering component synthesizes a first-person view with identification (ID) textures and performs direct 2D-3D gaze association. Quantitative evaluations showed the effectiveness of our framework. Additionally, we demonstrated the applications of 4D Attention through experiments in real situations.1 © 2016 IEEE.","intention recognition; Localization; multi-modal perception for HRI; visual tracking","Agricultural robots; Eye tracking; Glass; Mapping; Textures; 3-D environments; Activity analysis; Dynamic environments; Human robot Interaction (HRI); Inertial measurement unit; Quantitative evaluation; Spatio-temporal domains; Visual localization; Human robot interaction",Article,"Final","",Scopus,2-s2.0-85110844998
"Moon S., Zhang C., Park S., Zhang H., Kim W.-S., Ko J.H.","57277193900;57193807497;57277843000;57216240708;55492044000;56921245400;","A Sub-Milliwatt and Sub-Millisecond 3-D Gaze Estimator for Ultra Low-Power AR Applications",2021,"UbiComp/ISWC 2021 - Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers",,,,"481","485",,,"10.1145/3460418.3479360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115950725&doi=10.1145%2f3460418.3479360&partnerID=40&md5=ac898f026459de09d3608817f5b2c9bd","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Sait China Lab, Samsung Research, China-Beijing (SRC-B), Beijing, China; Samsung Advanced Institute of Technology (SAIT), Suwon, South Korea; College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea","Moon, S., Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Zhang, C., Sait China Lab, Samsung Research, China-Beijing (SRC-B), Beijing, China; Park, S., Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Zhang, H., Sait China Lab, Samsung Research, China-Beijing (SRC-B), Beijing, China; Kim, W.-S., Samsung Advanced Institute of Technology (SAIT), Suwon, South Korea; Ko, J.H., College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea","The critical factors of real-Time gaze tracking are high accuracy and user-friendliness such as low latency and no run-Time calibration. Existing gaze estimator hardware designs are based on 2D regression algorithms as 2D methods have a simple computation process. However, they require multiple run-Time calibration steps, and are vulnerable to head motions. On the other hand, the 3D model-based method can maintain better accuracy than the 2D method without run-Time calibration steps, and is robust to head motions. In this paper, we aim to design the first 3D model-based gaze estimator hardware that consumes less than 1mW power and 1ms latency per frame. The simulation results based on the hardware synthesis show that the proposed design requires 172μW and 0.5ms per frame, while maintaining less than 0.9° error. © 2021 ACM.","Gaze Tracker;Gaze Vector Estimation;Smart Glasses;ASIC","3D modeling; Application specific integrated circuits; Eye tracking; Integrated circuit design; 3D models; 3d-modeling; Gaze tracker; Gaze tracker;; Gaze vector estimation;; Head motion; Runtimes; Smart glass; Smart glass;; Vector estimation; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-85115950725
"Geisler D., Duchowski A.T., Kasneci E.","57189847283;6701824388;56059892600;","Predicting visual perceivability of scene objects through spatio-temporal modeling of retinal receptive fields",2021,"Neurocomputing","453",,,"667","680",,1,"10.1016/j.neucom.2020.07.119","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094123388&doi=10.1016%2fj.neucom.2020.07.119&partnerID=40&md5=6720cb6b0483eb47a9388c3f8f414737","University of Tuebingen, Germany; Clemson UniversitySC, United States","Geisler, D., University of Tuebingen, Germany; Duchowski, A.T., Clemson UniversitySC, United States; Kasneci, E., University of Tuebingen, Germany","Retinal processing of a visual scene is an essential step of human visual perception. Although foveal vision is linked to the visual attention, perception is by not means limited to this region. Rather, the retinal field of view ranges from 60° nasal to 107° temporal, and from 70° superior to 80° inferior. Whether a scene object is visually perceived depends on both its visual appearance as well as its retinal location. We present a framework to evaluate the visual stimulus of a scene object with regard to different types of retinal receptive fields. Driven by gaze location provided by an eye tracker, the estimated retinal response considers the visual appearance of the object, its eccentricity in the users field of view, and the capabilities at the local retinal region. A desktop experiment shows that, in additional to foveal processing, the estimated retinal response leads to a significant increase in classification accuracy in terms of whether an object is reported as perceived by the user. © 2020 Elsevier B.V.","Eye-tracking; Scene evaluation; Visual perception; Visual stimulus","Behavioral research; Eye tracking; Object tracking; Ophthalmology; Visualization; Classification accuracy; Human visual perception; Receptive fields; Retinal processing; Spatio-temporal models; Visual appearance; Visual Attention; Visual stimulus; Aldehydes",Article,"Final","",Scopus,2-s2.0-85094123388
"Han Y., Han B., Gao X.","57203657364;57187487000;7403873424;","Human scanpath estimation based on semantic segmentation guided by common eye fixation behaviors",2021,"Neurocomputing","453",,,"705","717",,,"10.1016/j.neucom.2020.07.121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092457537&doi=10.1016%2fj.neucom.2020.07.121&partnerID=40&md5=caa5401ef6563b3c0a1086ce943b3b71","School of Electronic Engineering, Xidian University, Xi'an, 710071, China","Han, Y., School of Electronic Engineering, Xidian University, Xi'an, 710071, China; Han, B., School of Electronic Engineering, Xidian University, Xi'an, 710071, China; Gao, X., School of Electronic Engineering, Xidian University, Xi'an, 710071, China","To explore the dynamic process of complex human eye movement behavior, we proposed a new model to simulate human scanpath when subjects observed natural images freely. Previous methods almost focused on finding effective and advanced technology, such as machine learning or deep learning, for estimating human scanpath. In contrast, our proposed method devoted to find a new way that could use the intrinsic property of eye-tracking data between different races to guide the design of a deep network. Inspired by that, the model of human scanpath estimation was established, which based on a semantic segmentation module guided by common eye fixation behaviors between people with different cultures. The semantic segmentation module could deal with locating fixations positions and the fixations ranking problem in parallel and generate human scanpath combined with the output of common attention portions (CAP) generator. The common attention portions (CAP) generator was designed to optimize the performance of semantic segmentation module and extract the common eye fixation behaviors between people with different cultures. We evaluated the performance of our model on three public eye-tracking datasets by comparing the result generated from our model with the ground truth of scanpath produced by a new method in this work. The proposed model also achieved the encouraging performance compared with some classic and fashionable models. © 2020 Elsevier B.V.","Common eye fixation behavior; Human scanpath estimation; Semantic segmentation","Deep learning; Eye movements; Eye tracking; Image segmentation; Semantics; Advanced technology; Dynamic process; Eye fixations; Intrinsic property; Movement behavior; Natural images; Ranking problems; Semantic segmentation; Behavioral research; adult; article; attention; deep learning; eye fixation; eye tracking; female; human; human experiment; male; race",Article,"Final","",Scopus,2-s2.0-85092457537
"Bickerdt J., Sonnenberg J., Gollnick C., Kasneci E.","57225930730;57225978707;57283147100;56059892600;","Geopositioned 3D areas of interest for gaze analysis",2021,"Proceedings - 13th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2021",,,,"1","11",,,"10.1145/3409118.3475138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116245136&doi=10.1145%2f3409118.3475138&partnerID=40&md5=44dcf4b98db4caad41a4628853266254","Volkswagen AG, Wolfsburg, Germany; Eberhard Karls Universität, Tübingen, Germany","Bickerdt, J., Volkswagen AG, Wolfsburg, Germany; Sonnenberg, J., Volkswagen AG, Wolfsburg, Germany; Gollnick, C., Volkswagen AG, Wolfsburg, Germany; Kasneci, E., Eberhard Karls Universität, Tübingen, Germany","To understand driver's gaze behavior, the gaze is usually matched to surrounding objects or static areas of interest (AOI) at fixed positions around the car. Full surround object tracking allows for an understanding of the traffic situation. However, because it requires an extensive sensor set and a lot of processing power, it's not yet broadly available in production cars. The use of static AOIs only requires the addition of eye tracking sensors. They are at fixed positions around the car and can't adapt to the environment, therefore their usefulness is limited. We propose geopositioned 3D AOIs. With adaptability and the use of a small sensor set, they combine the strengths of both methods. To test 3D AOIs' capabilities for gaze analysis, a driving simulator study with 74 participants was conducted. We show that 3D AOIs are suitable for driver's gaze analysis and a promising tool for driver intention prediction. © 2021 Association for Computing Machinery.","Areas of interest; Automotive; Driving simulation; Eye tracking","Area of interest; Automotives; Driving simulation; Eye-tracking; Gaze analysis; Gaze behaviours; Object Tracking; Processing power; Sensor sets; Traffic situations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116245136
"Wu J., Jiang J., Qi M., Li X.","57204812127;34770048100;7102098287;57006841900;","Towards accurate estimation for visual object tracking with multi-hierarchy feature aggregation",2021,"Neurocomputing","451",,,"252","264",,,"10.1016/j.neucom.2021.04.075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105524478&doi=10.1016%2fj.neucom.2021.04.075&partnerID=40&md5=6f2c5e5e15b153b40525433cb24f10f1","Hefei University of Technology, Hefei, China; Key Laboratory of Knowledge Engineering with Big Data (Hefei University of Technology), Ministry of Education, Hefei, China","Wu, J., Hefei University of Technology, Hefei, China; Jiang, J., Hefei University of Technology, Hefei, China, Key Laboratory of Knowledge Engineering with Big Data (Hefei University of Technology), Ministry of Education, Hefei, China; Qi, M., Hefei University of Technology, Hefei, China, Key Laboratory of Knowledge Engineering with Big Data (Hefei University of Technology), Ministry of Education, Hefei, China; Li, X., Hefei University of Technology, Hefei, China, Key Laboratory of Knowledge Engineering with Big Data (Hefei University of Technology), Ministry of Education, Hefei, China","Many methods achieve the visual object tracking task with deep learning technologies. As the deep features of different levels contain various semantic information and functions, this paper presents a multi-hierarchy feature aggregation approach to tackle the specific issues in the tracking task, which consists of two aspects. On one hand, this paper integrates the features captured by the offline and online classifiers at the score level, which constructs complementary roles of these classifiers to enhance the stability of classification. Besides, the proposed offline classifier is continuously optimized with different levels of features to reinforce classification constraints. On the other hand, we design a butterfly attention module to promote the capacity of multi-hierarchy feature aggregation in the regression network, which aims to fuse and strengthen the multi-scale features by attending to their spatial information. It can capture more spatial contexts by utilizing the self-attention mechanism during the fusion procedure, and preserve the hierarchy of the features during the strengthening process. Extensive experiments on four public datasets, i.e., VOT2018, OTB100, NFS and LaSOT datasets, demonstrate the effectiveness of the proposed methods. © 2021 Elsevier B.V.","Butterfly attention module; Classification score fusion; Multi-hierarchy feature aggregation; Visual object tracking","Deep learning; Semantics; Tracking (position); Accurate estimation; Butterfly attention module; Classification score fusion; Feature aggregation; Learning technology; Multi-hierarchy; Multi-hierarchy feature aggregation; Offline; Semantics Information; Visual object tracking; Classification (of information); article; attention; butterfly; classifier; eye tracking; nonhuman",Article,"Final","",Scopus,2-s2.0-85105524478
"Alinaghi N., Kattenbeck M., Golab A., Giannopoulos I.","57184346400;55765280000;57226593596;57117903600;","Will you take this turn? Gaze-based turning activity recognition during navigation",2021,"Leibniz International Proceedings in Informatics, LIPIcs","208",,"5","VII","",,1,"10.4230/LIPIcs.GIScience.2021.II.5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115751979&doi=10.4230%2fLIPIcs.GIScience.2021.II.5&partnerID=40&md5=fa46c428dc495a475ea3fcb7ebf7cb08","Geoinformation, TU Wien, Austria; Institute of Advanced Research in Artificial Intelligence (IARAI), Vienna, Austria","Alinaghi, N., Geoinformation, TU Wien, Austria; Kattenbeck, M., Geoinformation, TU Wien, Austria; Golab, A., Geoinformation, TU Wien, Austria; Giannopoulos, I., Geoinformation, TU Wien, Austria, Institute of Advanced Research in Artificial Intelligence (IARAI), Vienna, Austria","Decision making is an integral part of wayfinding and people progressively use navigation systems to facilitate this task. The primary decision, which is also the main source of navigation error, is about the turning activity, i.e., to decide either to turn left or right or continue straight forward. The fundamental step to deal with this error, before applying any preventive approaches, e.g., providing more information, or any compensatory solutions, e.g., pre-calculating alternative routes, could be to predict and recognize the potential turning activity. This paper aims to address this step by predicting the turning decision of pedestrian wayfinders, before the actual action takes place, using primarily gaze-based features. Applying Machine Learning methods, the results of the presented experiment demonstrate an overall accuracy of 91% within three seconds before arriving at a decision point. Beyond the application perspective, our findings also shed light on the cognitive processes of decision making as reflected by the wayfinder's gaze behaviour: incorporating environmental and user-related factors to the model, results in a noticeable change with respect to the importance of visual search features in turn activity recognition. © Negar Alinaghi, Markus Kattenbeck, Antonia Golab, and Ioannis Giannopoulos; licensed under Creative Commons License CC-BY 4.0","Activity recognition; Eye tracking; Machine learning; Wayfinding","Behavioral research; Decision making; Machine learning; Navigation systems; Pattern recognition; Activity recognition; Alternative routes; Decisions makings; Eye-tracking; Integral part; Machine learning methods; Navigation error; Overall accuracies; Way finding; Wayfinders; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85115751979
"Baazeem I., Al-Khalifa H., Al-Salman A.","57189262163;16241119700;55901316400;","Cognitively driven arabic text readability assessment using eye-tracking",2021,"Applied Sciences (Switzerland)","11","18","8607","","",,,"10.3390/app11188607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115219770&doi=10.3390%2fapp11188607&partnerID=40&md5=162675ca34814e304ba278f8d5cf135f","College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; The National Center for Data Analytics and Artificial Intelligence, King Abdulaziz City for Science and Technology, Riyadh, 11442, Saudi Arabia","Baazeem, I., College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia, The National Center for Data Analytics and Artificial Intelligence, King Abdulaziz City for Science and Technology, Riyadh, 11442, Saudi Arabia; Al-Khalifa, H., College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia; Al-Salman, A., College of Computer and Information Sciences, King Saud University, Riyadh, 11543, Saudi Arabia","Using physiological data helps to identify the cognitive processing in the human brain. One method of obtaining these behavioral signals is by using eye-tracking technology. Previous cognitive psychology literature shows that readable and difficult-to-read texts are associated with certain eye movement patterns, which has recently encouraged researchers to use these patterns for readability assessment tasks. However, although it seems promising, this research direction has not been explored adequately, particularly for Arabic. The Arabic language is defined by its own rules and has its own characteristics and challenges. There is still a clear gap in determining the potential of using eye-tracking measures to improve Arabic text. Motivated by this, we present a pilot study to explore the extent to which eye-tracking measures enhance Arabic text readability. We collected the eye movements of 41 participants while reading Arabic texts to provide real-time processing of the text; these data were further analyzed and used to build several readability prediction models using different regression algorithms. The findings show an improvement in the readability prediction task, which requires further investigation. To the best of our knowledge, this work is the first study to explore the relationship between Arabic readability and eye movement patterns. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Arabic language; Eye movements; Eye-tracking; Human processing; Machine learning; Natural language processing; Readability assessment; Text difficulty",,Article,"Final","",Scopus,2-s2.0-85115219770
"Islam M.R., Sakamoto S., Yamada Y., Vargo A.W., Iwata M., Iwamura M., Kise K.","55765000561;57221687645;57210570496;57191334988;7402168530;57217857890;16178222100;","Self-supervised Learning for Reading Activity Classification",2021,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","5","3","3478088","","",,,"10.1145/3478088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115144188&doi=10.1145%2f3478088&partnerID=40&md5=33596745d3b54f0a3817fff58bec9946","Osaka Prefecture University, Sakai, Japan; Bsmrstu, Gopalganj, Bangladesh","Islam, M.R., Osaka Prefecture University, Sakai, Japan, Bsmrstu, Gopalganj, Bangladesh; Sakamoto, S., Osaka Prefecture University, Sakai, Japan; Yamada, Y., Osaka Prefecture University, Sakai, Japan; Vargo, A.W., Osaka Prefecture University, Sakai, Japan; Iwata, M., Osaka Prefecture University, Sakai, Japan; Iwamura, M., Osaka Prefecture University, Sakai, Japan; Kise, K., Osaka Prefecture University, Sakai, Japan","Reading analysis can relay information about user's confidence and habits and can be used to construct useful feedback. A lack of labeled data inhibits the effective application of fully-supervised Deep Learning (DL) for automatic reading analysis. We propose a Self-supervised Learning (SSL) method for reading analysis. Previously, SSL has been effective in physical human activity recognition (HAR) tasks, but it has not been applied to cognitive HAR tasks like reading. We first evaluate the proposed method on a four-class classification task on reading detection using electrooculography datasets, followed by an evaluation of a two-class classification task of confidence estimation on multiple-choice questions using eye-tracking datasets. Fully-supervised DL and support vector machines (SVMs) are used as comparisons for the proposed SSL method. The results show that the proposed SSL method is superior to the fully-supervised DL and SVM for both tasks, especially when training data is scarce. This result indicates the proposed method is the superior choice for reading analysis tasks. These results are important for informing the design of automatic reading analysis platforms. © 2021 ACM.","confidence estimation; fully-supervised deep learning; reading analysis; reading detection; Self-supervised learning","Classification (of information); Deep learning; Eye tracking; Automatic reading; Classification tasks; Confidence estimation; Human activity recognition; Multiple choice questions; Reading activities; Relay information; Support vector machine (SVMs); Support vector machines",Article,"Final","",Scopus,2-s2.0-85115144188
"Araya R., Sossa-Rivera J.","56729031800;57260384000;","Automatic Detection of Gaze and Body Orientation in Elementary School Classrooms",2021,"Frontiers in Robotics and AI","8",,"729832","","",,,"10.3389/frobt.2021.729832","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115018287&doi=10.3389%2ffrobt.2021.729832&partnerID=40&md5=4caf6bb3b9b063bcd98a8cfbf238953d","Institute of Education, Universidad de Chile, Santiago, Chile","Araya, R., Institute of Education, Universidad de Chile, Santiago, Chile; Sossa-Rivera, J., Institute of Education, Universidad de Chile, Santiago, Chile","Detecting the direction of the gaze and orientation of the body of both teacher and students is essential to estimate who is paying attention to whom. It also provides vital clues for understanding their unconscious, non-verbal behavior. These are called “honest signals” since they are unconscious subtle patterns in our interaction with other people that help reveal the focus of our attention. Inside the classroom, they provide important clues about teaching practices and students' responses to different conscious and unconscious teaching strategies. Scanning this non-verbal behavior in the classroom can provide important feedback to the teacher in order for them to improve their teaching practices. This type of analysis usually requires sophisticated eye-tracking equipment, motion sensors, or multiple cameras. However, for this to be a useful tool in the teacher's daily practice, an alternative must be found using only a smartphone. A smartphone is the only instrument that a teacher always has at their disposal and is nowadays considered truly ubiquitous. Our study looks at data from a group of first-grade classrooms. We show how video recordings on a teacher's smartphone can be used in order to estimate the direction of the teacher and students’ gaze, as well as their body orientation. Using the output from the OpenPose software, we run Machine Learning (ML) algorithms to train an estimator to recognize the direction of the students’ gaze and body orientation. We found that the level of accuracy achieved is comparable to that of human observers watching frames from the videos. The mean square errors (RMSE) of the predicted pitch and yaw angles for head and body directions are on average 11% lower than the RMSE between human annotators. However, our solution is much faster, avoids the tedium of doing it manually, and makes it possible to design solutions that give the teacher feedback as soon as they finish the class. © Copyright © 2021 Araya and Sossa-Rivera.","body orientation detection; gaze detection; non-verbal behavior; student attention; teaching practices",,Article,"Final","",Scopus,2-s2.0-85115018287
"Samsami M.M., Zaheryani S.M.S., Yazdi M.","56132373500;57205245202;57220410020;","Astute, fine and fast method of iris segmentation in unlimited circumstances",2021,"Neural Computing and Applications","33","17",,"10961","10973",,,"10.1007/s00521-020-05646-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098727755&doi=10.1007%2fs00521-020-05646-4&partnerID=40&md5=aa54d8e811ac8950ad669c7cc35a8767","Department of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Comprehensive Ophthalmologist, Urmia University of Medical Sciences, Urmia, Iran; Department of Communications and Electronic Engineering, Shiraz University, Shiraz, Iran","Samsami, M.M., Department of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Zaheryani, S.M.S., Comprehensive Ophthalmologist, Urmia University of Medical Sciences, Urmia, Iran; Yazdi, M., Department of Communications and Electronic Engineering, Shiraz University, Shiraz, Iran","Currently, Iris detection is considered as a significant module for robust biometric systems and high-speed applications such as eye tracking. Most iris segmentation models are based on machine learning algorithms or geometric methods. In this paper, we use an elliptical Hough transform to firstly detect the shape of the palpebral fissure. Then, a correlation-based circular Hough transform (we named it CCHT) is proposed to extract iris from the surrounding structures. One of the advantages of the proposed method is its ability to determine the closed-eye images, in order to remove these images in the process of eye tracking procedure. Moreover, the algorithm is simple and fast which make it suitable for on-line eye tracking. Experimental results on UBIRIS, which contains some defocused and eyelid-occluded images as non-ideal and noisy frames, indicate that the proposed method is efficient and much faster, in comparison with the previous approaches and encouraging improved accuracy on iris detection. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.","Biometry; Eye tracking; Hough transform; Iris segmentation","Biometrics; Hough transforms; Image enhancement; Learning algorithms; Machine learning; Biometric systems; Circular Hough transforms; Eye tracking procedures; Fast methods; Geometric method; High-speed applications; Iris detection; Iris segmentation; Eye tracking",Article,"Final","",Scopus,2-s2.0-85098727755
"Pejić M., Savić G., Segedinac M.","57221260592;57196759158;36700468100;","Determining Gaze Behavior Patterns in On-Screen Testing",2021,"Journal of Educational Computing Research","59","5",,"896","925",,,"10.1177/0735633120978617","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097309698&doi=10.1177%2f0735633120978617&partnerID=40&md5=49deeac734dd5c87237536afc2ba9bc6","Department of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia","Pejić, M., Department of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Savić, G., Department of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Segedinac, M., Department of Computing and Control Engineering, Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia","This study proposes a software system for determining gaze patterns in on-screen testing. The system applies machine learning techniques to eye-movement data obtained from an eye-tracking device to categorize students according to their gaze behavior pattern while solving an on-screen test. These patterns are determined by converting eye movement coordinates into a sequence of regions of interest. The proposed software system extracts features from the sequence and performs clustering that groups students by their gaze pattern. To determine gaze patterns, the system contains components for communicating with an eye-tracking device, collecting and preprocessing students’ gaze data, and visualizing data using different presentation methods. This study presents a methodology to determine gaze patterns and the implementation details of the proposed software. The research was evaluated by determining the gaze patterns of 51 undergraduate students who took a general knowledge test containing 20 questions. This study aims to provide a software infrastructure that can use students’ gaze patterns as an additional indicator of their reading behaviors and their processing attention or difficulty, among other factors. © The Author(s) 2020.","data mining; eye tracking; gaze behavior patterns; machine learning; on-screen testing",,Article,"Final","",Scopus,2-s2.0-85097309698
"Chiquet S., Martarelli C.S., Mast F.W.","57189715349;55217977300;7006023670;","Eye movements to absent objects during mental imagery and visual memory in immersive virtual reality",2021,"Virtual Reality","25","3",,"655","667",,,"10.1007/s10055-020-00478-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093937723&doi=10.1007%2fs10055-020-00478-y&partnerID=40&md5=ded352fa32c19128ca7ac1517832872d","Department of Psychology, University of Bern, Bern, 3012, Switzerland; Swiss Distance University Institute, Brig, 3900, Switzerland","Chiquet, S., Department of Psychology, University of Bern, Bern, 3012, Switzerland; Martarelli, C.S., Swiss Distance University Institute, Brig, 3900, Switzerland; Mast, F.W., Department of Psychology, University of Bern, Bern, 3012, Switzerland","The role of eye movements in mental imagery and visual memory is typically investigated by presenting stimuli or scenes on a two-dimensional (2D) computer screen. When questioned about objects that had previously been presented on-screen, people gaze back to the location of the stimuli, even though those regions are blank during retrieval. It remains unclear whether this behavior is limited to a highly controlled experimental setting using 2D screens or whether it also occurs in a more naturalistic setting. The present study aims to overcome this shortcoming. Three-dimensional (3D) objects were presented along a circular path in an immersive virtual room. During retrieval, participants were given two tasks: to visualize the objects, which they had encoded before, and to evaluate a statement about visual details of the object. We observed longer fixation duration in the area, on which the object was previously displayed, when compared to other possible target locations. However, in 89% of the time, participants fixated none of the predefined areas. On the one hand, this shows that looking at nothing may be overestimated in 2D screen-based paradigm, on the other hand, the looking at nothing effect was still present in the 3D immersive virtual reality setting, and thus it extends external validity of previous findings. Eye movements during retrieval reinstate spatial information of previously inspected stimuli. © 2020, The Author(s).","Eye movements; Eye tracking; Mental imagery; Virtual reality; Visual memory","Eye movements; Computer screens; External validities; Fixation duration; Immersive virtual reality; Spatial informations; Target location; Three-dimensional (3D) objects; Two Dimensional (2 D); Virtual reality",Article,"Final","",Scopus,2-s2.0-85093937723
"Wu W., Sun W., Wu Q.M.J., Zhang C., Yang Y., Yu H., Lu B.-L.","57196185897;57161531200;55613291700;57212766848;56379191400;23092642000;57226221430;","Faster Single Model Vigilance Detection Based on Deep Learning",2021,"IEEE Transactions on Cognitive and Developmental Systems","13","3","8946740","621","630",,,"10.1109/TCDS.2019.2963073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077391176&doi=10.1109%2fTCDS.2019.2963073&partnerID=40&md5=01d7704fe4990168eaa7aa9aacc4e347","College of Electrical and Information Engineering, State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body, Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, Hunan University, Changsha, China; Department of Electrical and Computer Engineering, University of Windsor, Windsor, Canada; College of Electrical and Information Engineering, Hunan University of Technology, Zhuzhou, China; Computer Science Department, Lakehead University, Thunder Bay, Canada; Department of Computer Science and Engineering, Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China","Wu, W., College of Electrical and Information Engineering, State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body, Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, Hunan University, Changsha, China; Sun, W., College of Electrical and Information Engineering, State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body, Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, Hunan University, Changsha, China; Wu, Q.M.J., Department of Electrical and Computer Engineering, University of Windsor, Windsor, Canada; Zhang, C., College of Electrical and Information Engineering, Hunan University of Technology, Zhuzhou, China; Yang, Y., Computer Science Department, Lakehead University, Thunder Bay, Canada; Yu, H., College of Electrical and Information Engineering, State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body, Hunan Key Laboratory of Intelligent Robot Technology in Electronic Manufacturing, Hunan University, Changsha, China; Lu, B.-L., Department of Computer Science and Engineering, Key Lab. of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China","Various reports have shown that the rate of road traffic accidents has increased due to reduced driver vigilance. Therefore, an accurate estimation of the driver's alertness status plays an important part. To estimate vigilance, we adopt a novel strategy that is a deep autoencoder with subnetwork nodes (DAESN). The proposed network model is designed not only for sparse representation but also for dimension reduction. Some hidden layers are not calculated by randomly acquired, but by replacement technologies. Unlike the traditional electrooculogram (EOG) signals, the forehead EOG (EOGF) signals are collected through forehead electrodes that do not have to surround the eyes, which has a convenient and effective practical application. The root-mean-square error (RMSE) and correlation coefficient (COR) while separately using three EOGF features improved to 0.11/0.79, 0.10/0.83, and 0.11/0.80, respectively. Implemented in an experimental environment, percentage of eye closure over time is calculated in real time through SMI eye-tracking-glasses, up to 120 frames/s. In addition, the time to extract features from the raw signal and display the prediction is only 34 ms, that is the level of the driver's fatigue can be detected quickly. The experimental study shows that the proposed model for vigilance analysis has better robustness and learning capability. © 2016 IEEE.","Deep learning (DL); dimension reduction; single model; vigilance detection","Dimensionality reduction; Eye tracking; Learning systems; Mean square error; Correlation coefficient; Dimension reduction; Experimental environment; Learning capabilities; Road traffic accidents; Root mean square errors; Single models; Sparse representation; Deep learning",Article,"Final","",Scopus,2-s2.0-85077391176
"Zheng L.J., Mountstephens J., Teo J.","57216398510;36915612500;57201882145;","A Comparative Investigation of Eye Fixation-based 4-Class Emotion Recognition in Virtual Reality Using Machine Learning",2021,"Proceedings - 2021 11th IEEE International Conference on Control System, Computing and Engineering, ICCSCE 2021",,,,"19","22",,,"10.1109/ICCSCE52189.2021.9530980","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116290347&doi=10.1109%2fICCSCE52189.2021.9530980&partnerID=40&md5=8a54b30e19d6764db43a291424c5d546","Universiti Malaysia Sabah, Evolutionary Computing Laboratory Faculty of Computing and Informatics, Sabah, Kota Kinabalu, Malaysia","Zheng, L.J., Universiti Malaysia Sabah, Evolutionary Computing Laboratory Faculty of Computing and Informatics, Sabah, Kota Kinabalu, Malaysia; Mountstephens, J., Universiti Malaysia Sabah, Evolutionary Computing Laboratory Faculty of Computing and Informatics, Sabah, Kota Kinabalu, Malaysia; Teo, J., Universiti Malaysia Sabah, Evolutionary Computing Laboratory Faculty of Computing and Informatics, Sabah, Kota Kinabalu, Malaysia","Research on emotion recognition that relies purely on eye-tracking data is very limited although the usability of eye-tracking technology has great potential for emotional recognition. This paper proposes a novel approach for 4-class emotion classification using eye-tracking data solely in virtual reality (VR) with machine learning algorithms. We classify emotions into four specific classes using VR stimulus. Eye fixation data was used as the emotional-relevant feature in this investigation. A presentation of 3600 videos, which contains four different sessions, was played in VR to evoke the user's emotions. The eye-tracking data was collected and recorded using an add-on eye-tracker in the VR headset. Three classifiers were used in the experiment, which are k-nearest neighbor (KNN), random forest (RF), and support vector machine (SVM). The findings showed that RF has the best performance among the classifiers, and achieved the highest accuracy of 80.55%. © 2021 IEEE.","emotion recognition; eye-tracking; fixation; machine learning; virtual reality","Decision trees; E-learning; Learning algorithms; Nearest neighbor search; Speech recognition; Support vector machines; Virtual reality; Emotion classification; Emotion recognition; Emotional recognition; Eye fixations; Eye tracking technologies; Eye-tracking; Fixation; Machine-learning; Random forests; Tracking data; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116290347
"Beriwal M., Agrawal S.","57289219100;57258637900;","Techniques for suicidal ideation prediction: A qualitative systematic review",2021,"2021 International Conference on INnovations in Intelligent SysTems and Applications, INISTA 2021 - Proceedings",,,,"","",,,"10.1109/INISTA52262.2021.9548444","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116638160&doi=10.1109%2fINISTA52262.2021.9548444&partnerID=40&md5=ad655f7960ed9283b18768c523002c43","NMIMS University, Dept. of Computer Engineering MPSTME, Mumbai, India","Beriwal, M., NMIMS University, Dept. of Computer Engineering MPSTME, Mumbai, India; Agrawal, S., NMIMS University, Dept. of Computer Engineering MPSTME, Mumbai, India","Suicide is an increasingly present issue in our society whose eradication could be greatly aided by decision support technologies that can objectively identify early markers of suicidal ideation. We present our paper which reviews various existing techniques for suicidal ideation prediction. These techniques are broadly divided into two major categories - 1) Text-based indicators (which employ NLP) and 2) Behavioural indicators (like eye gaze and smile which are classified using ML algorithms). The existing techniques aim at classifying individuals as suicidal or non-suicidal using classifiers such as Random Forest, Logistic Regression, Naïve Bayes, K-Nearest Neighbour (KNN), Support Vector Machine (SVM), Multi-Layer Perceptron and XGBoost. Each technique has its advantages and disadvantages. We aim to bring all these diverse techniques to one place and try to show how each one of them is individually contributing to prediction. We also shed light on how combining these techniques and eliminating the disadvantages could lead to better suicide prediction. © 2021 IEEE.","Classification; Depression; Machine Learning; Neural Networks; NLP; Prediction; Reddit; Social Media; Suicidal Ideation; Twitter; Visual markers","Decision support systems; Decision trees; Logistic regression; Natural language processing systems; Nearest neighbor search; Social networking (online); Support vector machines; Decision supports; Depression; Machine-learning; Neural-networks; Reddit; Social media; Suicidal ideation; Support technology; Systematic Review; Visual markers; Forecasting",Conference Paper,"Final","",Scopus,2-s2.0-85116638160
"Tu Z., Zhou A., Gan C., Jiang B., Hussain A., Luo B.","23767320700;57210793477;57225846058;56890202300;19734290900;57203411639;","A novel domain activation mapping-guided network (DA-GNT) for visual tracking",2021,"Neurocomputing","449",,,"443","454",,,"10.1016/j.neucom.2021.03.056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105690837&doi=10.1016%2fj.neucom.2021.03.056&partnerID=40&md5=d4f584198683d5f7c77c748f1a3132f7","Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University230601, China; School of Computing, Merchiston Campus, Edinburgh Napier University, Edinburgh, Scotland  EH10 5DT, United Kingdom","Tu, Z., Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University230601, China; Zhou, A., Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University230601, China; Gan, C., Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University230601, China; Jiang, B., Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University230601, China; Hussain, A., School of Computing, Merchiston Campus, Edinburgh Napier University, Edinburgh, Scotland  EH10 5DT, United Kingdom; Luo, B., Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University230601, China","Conventional convolution neural network (CNN)-based visual trackers are easily influenced by too much background information in candidate samples. Further, extreme imbalance of foreground and background samples has a negative impact on training the classifier, whereas features learned from limited data are insufficient to train the classifier. To address these problems, we propose a novel deep neural network for visual tracking, termed the domain activation mapping guided network (DA-GNT). First, we introduce the class activation mapping with weakly supervised localization in multi-domain to identify the most discriminative regions in the bounding box and suppress the background in the positive sample. Next, to further increase the discriminability of deep feature representation, we utilize an ensemble network to achieve a kind of multi-view feature representation and a channel attention mechanism for adaptive feature selection. Finally, we propose a simple but effective data augmentation method to further increase the positive samples for our network training. Extensive experiments on two widely used benchmark datasets demonstrate the effectiveness of the proposed tracking method against many state-of-the-art trackers. The novel DA-GNT is thus posited as a potential benchmark resource for the computer vision and machine learning research community. © 2021 Elsevier B.V.","Attention mechanism; Data augmentation; Deep neural networks; Visual tracking; Weakly supervised localization","Classification (of information); Computer vision; Deep neural networks; Flow visualization; Mapping; Personnel training; Supervised learning; Activation mapping; Attention mechanisms; Convolution neural network; Data augmentation; Feature representation; Network-based; Neural-networks; Novel domain; Visual Tracking; Weakly supervised localization; Chemical activation; article; attention; computer vision; deep neural network; eye tracking; feature selection",Article,"Final","",Scopus,2-s2.0-85105690837
"Mania K., McNamara A., Polychronakis A.","6602471750;35253845600;57235077300;","Gaze-aware displays and interaction",2021,"ACM SIGGRAPH 2021 Courses, SIGGRAPH 2021",,,"9","","",,,"10.1145/3450508.3464606","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113709595&doi=10.1145%2f3450508.3464606&partnerID=40&md5=4438d834350efc261600384fa4d33693",,"Mania, K.; McNamara, A.; Polychronakis, A.","Being able to detect and to employ gaze enhances digital displays. Research on gaze-contingent or gaze-aware display devices dates back two decades. This is the time, though, that it could truly be employed for fast, low-latency gaze-based interaction and for optimization of computer graphics rendering such as in foveated rendering. Moreover, Virtual Reality (VR) is becoming ubiquitous. The widespread availability of consumer grade VR Head Mounted Displays (HMDs) transformed VR to a commodity available for everyday use. VR applications are now abundantly designed for recreation, work and communication. However, interacting with VR setups requires new paradigms of User Interfaces (UIs), since traditional 2D UIs are designed to be viewed from a static vantage point only, e.g. the computer screen. Adding to this, traditional input methods such as the keyboard and mouse are hard to manipulate when the user wears a HMD. Recently, companies such as HTC announced embedded eye-tracking in their headsets and therefore, novel, immersive 3D UI paradigms embedded in a VR setup can now be controlled via eye gaze. Gaze-based interaction is intuitive and natural the users. Tasks can be performed directly into the 3D spatial context without having to search for an out-of-view keyboard/mouse. Furthermore, people with physical disabilities, already depending on technology for recreation and basic communication, can now benefit even more from VR. This course presents timely, relevant information on how gaze-contingent displays, in general, including the recent advances of Virtual Reality (VR) eye tracking capabilities can leverage eye-tracking data to optimize the user experience and to alleviate usability issues surrounding intuitive interaction challenges. Research topics to be covered include saliency models, gaze prediction, gaze tracking, gaze direction, foveated rendering, stereo grading and 3D User Interfaces (UIs) based on gaze on any gaze-aware display technology. © 2021 Owner/Author.",,"Grading; Helmet mounted displays; Interactive computer graphics; Mammals; Rendering (computer graphics); Three dimensional computer graphics; User experience; User interfaces; Virtual reality; 3D user interface; Computer graphics rendering; Display technologies; Gaze-based interaction; Gaze-contingent displays; Head mounted displays; Intuitive interaction; Physical disability; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85113709595
"Shi L., Copot C., Vanlanduit S.","57211018738;35301588200;7004271926;","Gaze Gesture Recognition by Graph Convolutional Networks",2021,"Frontiers in Robotics and AI","8",,"709952","","",,,"10.3389/frobt.2021.709952","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113259081&doi=10.3389%2ffrobt.2021.709952&partnerID=40&md5=35811021b9930dcd36002600701e424a","InViLab, Faculty of Applied Engineering, University of Antwerp, Antwerp, Belgium","Shi, L., InViLab, Faculty of Applied Engineering, University of Antwerp, Antwerp, Belgium; Copot, C., InViLab, Faculty of Applied Engineering, University of Antwerp, Antwerp, Belgium; Vanlanduit, S., InViLab, Faculty of Applied Engineering, University of Antwerp, Antwerp, Belgium","Gaze gestures are extensively used in the interactions with agents/computers/robots. Either remote eye tracking devices or head-mounted devices (HMDs) have the advantage of hands-free during the interaction. Previous studies have demonstrated the success of applying machine learning techniques for gaze gesture recognition. More recently, graph neural networks (GNNs) have shown great potential applications in several research areas such as image classification, action recognition, and text classification. However, GNNs are less applied in eye tracking researches. In this work, we propose a graph convolutional network (GCN)–based model for gaze gesture recognition. We train and evaluate the GCN model on the HideMyGaze! dataset. The results show that the accuracy, precision, and recall of the GCN model are 97.62%, 97.18%, and 98.46%, respectively, which are higher than the other compared conventional machine learning algorithms, the artificial neural network (ANN) and the convolutional neural network (CNN). © Copyright © 2021 Shi, Copot and Vanlanduit.","eye tracking; gaze; gesture recognition; graph convolution network; graph neural network",,Article,"Final","",Scopus,2-s2.0-85113259081
"Daskalogrigorakis G., McNamara A., Mania K.","57226706831;35253845600;6602471750;","Holo-Box: Level-of-Detail Glanceable Interfaces for Augmented Reality",2021,"Special Interest Group on Computer Graphics and Interactive Techniques Conference Posters, SIGGRAPH 2021",,,"3469175","","",,,"10.1145/3450618.3469175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112689342&doi=10.1145%2f3450618.3469175&partnerID=40&md5=26ab1b016c11bd4917517a815d7674ba","Technical University of Crete, Chania, Greece; Texas AandM University, College Station, TX, United States","Daskalogrigorakis, G., Technical University of Crete, Chania, Greece; McNamara, A., Texas AandM University, College Station, TX, United States; Mania, K., Technical University of Crete, Chania, Greece","Glanceable interfaces are Augmented Reality (AR) User Interfaces (UIs) for information retrieval ""at a glance""relying on eye gaze for implicit input. While they provide rapid information retrieval, they often occlude a large part of the real-world. This is compounded as the amount of virtual information increases. Interacting with complex glanceable interfaces often results in unintentional eye gaze interaction and selections due to the Midas Touch problem. In this work, we present Holo-box, an innovative AR UI design that combines 2D compact glanceable interfaces with 3D virtual ""Holo-boxes"". We can utilize the glanceable 2D interface to provide compact information at a glance while using Holo-box for explicit input such as hand tracking activated when necessary, surpassing the Midas Touch problem and resulting in Level-of-Detail(LOD) for AR glanceable UIs. We test our proposed system inside a real-world machine shop to provide on-demand virtual information while minimizing unintentional real-world occlusion. © 2021 Owner/Author.",,"Augmented reality; Information retrieval; Interactive computer graphics; Machine shops; Eye gaze interactions; Hand tracking; Large parts; Level of detail; Midas touches; Real-world; UI designs; Virtual information; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85112689342
"Akshay S., Abhishek M.B., Sudhanshu D., Anuvaishnav C.","23466346300;57288887100;57288887200;57288715700;","Drowsy Driver Detection using Eye-Tracking through Machine Learning",2021,"Proceedings of the 2nd International Conference on Electronics and Sustainable Communication Systems, ICESC 2021",,,,"1916","1923",,,"10.1109/ICESC51422.2021.9532928","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116675031&doi=10.1109%2fICESC51422.2021.9532928&partnerID=40&md5=a4f053b16e530c8e1ebd2d7b1c8d9ea6","Amrita School of Arts and Sciences, Amrita Vishwa Vidyapeetham, Department of Computer Science, Karnataka, Mysuru, India","Akshay, S., Amrita School of Arts and Sciences, Amrita Vishwa Vidyapeetham, Department of Computer Science, Karnataka, Mysuru, India; Abhishek, M.B., Amrita School of Arts and Sciences, Amrita Vishwa Vidyapeetham, Department of Computer Science, Karnataka, Mysuru, India; Sudhanshu, D., Amrita School of Arts and Sciences, Amrita Vishwa Vidyapeetham, Department of Computer Science, Karnataka, Mysuru, India; Anuvaishnav, C., Amrita School of Arts and Sciences, Amrita Vishwa Vidyapeetham, Department of Computer Science, Karnataka, Mysuru, India","Eye tracking is one of the most useful but underutilized technologies in today's world. It can be used in a variety of ways now that the technology is available. We propose an implementation that has not been done but should be after studying several ways to process the same. In the field of Advanced Driving Assistance Systems, tracking drivers' eyes is a hot topic (ADAS). According to data from the World Health Organization (WHO), approximately 1-1.25 million people die and 20-50 million people suffer from non-fatal injuries in road accidents each year around the world. And a high majority of these collisions are caused by drowsy driving. Our paper explores a possible implementation that could help detect drowsiness given a subject, a phone camera and a single board computer. We establish a connection between the phone and the system using a network connection that streams the camera feed onto the system, which further performs computations to determine the drowsiness of the driver. © 2021 IEEE.","Adaptive Boosting; DLib; Driver Assistance; Driver Drowsiness; Driver Monitoring System; Eye Tracking; Haar Cascade Classifier; HOG; PERCLOS; SVM","Adaptive boosting; Automobile drivers; Cameras; Eye movements; Machine learning; Telephone sets; Dlib; Driver assistance; Driver drowsiness; Driver monitoring system; Drowsy driver; Eye-tracking; Haar cascade classifiers; HOG; PERCLOS; SVM; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116675031
"Jun L., Juan Y., Weiwei Y., Xiaofang K.","57289731400;57288540700;57289045700;57288712700;","Research on the Design of Primary School English Learning Resources Based on Cognitive Model",2021,"Proceedings - 2021 International Symposium on Educational Technology, ISET 2021",,,,"176","181",,,"10.1109/ISET52350.2021.00044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116668839&doi=10.1109%2fISET52350.2021.00044&partnerID=40&md5=5574188838df3686381dd44a271d134f","Sichuan Normal University, College of Computer Science, Chengdu, China","Jun, L., Sichuan Normal University, College of Computer Science, Chengdu, China; Juan, Y., Sichuan Normal University, College of Computer Science, Chengdu, China; Weiwei, Y., Sichuan Normal University, College of Computer Science, Chengdu, China; Xiaofang, K., Sichuan Normal University, College of Computer Science, Chengdu, China","Attention is one of the key cognitive factors that affect English learning. This paper analyzes the distribution and transfer of learners' attention in the process of English learning by using eye movements data, and explores the design principles of English learning resources. According to different English learning characteristics, three sets of learning resources are designed. Through the analysis of eye movements data, the design principles of picture learning resources and audio-visual multi-mode learning resources are obtained. Picture learning resources should emphasize the corresponding relationship between pictures and English vocabulary, so as to help learners establish and improve the second language system. The pictures in the learning resources should help learners form the overall cognition of English, rather than establish the corresponding Chinese meaning in English. Audio-visual multi-mode learning resources have a positive effect on the improvement of learners' English learning ability, but they need to be designed according to different English learning features. At the same time, the coordination of visual and auditory stimuli needs to be changed according to learners' learning ability. © 2021 IEEE.","attention pattern; eye-tracking; learning resources; machine learning; resource design","Eye movements; Learning systems; Attention pattern; Audio-visual; Design Principles; English Learning; Eye movement datum; Eye-tracking; Learning abilities; Learning resource; Multimodes; Resource design; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116668839
"Meng X., Du R., Jaja J.F., Varshney A.","57214988854;56612823300;35552340000;7007155280;","3D-Kernel Foveated Rendering for Light Fields",2021,"IEEE Transactions on Visualization and Computer Graphics","27","8","9007492","3350","3360",,,"10.1109/TVCG.2020.2975801","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111789253&doi=10.1109%2fTVCG.2020.2975801&partnerID=40&md5=f6885d2068f91674da5ef2fc6757394b","Computer Science, University of Maryland at College Park, College Park, MD  08018, United States; Google Llc, San Francisco, CA, United States; Electrical and Computer Engineering, University of Maryland at College Park, College Park, MD, United States","Meng, X., Computer Science, University of Maryland at College Park, College Park, MD  08018, United States; Du, R., Google Llc, San Francisco, CA, United States; Jaja, J.F., Electrical and Computer Engineering, University of Maryland at College Park, College Park, MD, United States; Varshney, A., Computer Science, University of Maryland at College Park, College Park, MD  08018, United States","Light fields capture both the spatial and angular rays, thus enabling free-viewpoint rendering and custom selection of the focal plane. Scientists can interactively explore pre-recorded microscopic light fields of organs, microbes, and neurons using virtual reality headsets. However, rendering high-resolution light fields at interactive frame rates requires a very high rate of texture sampling, which is challenging as the resolutions of light fields and displays continue to increase. In this article, we present an efficient algorithm to visualize 4D4D light fields with 3D-kernel foveated rendering (3D-KFR). The 3D-KFR scheme coupled with eye-tracking has the potential to accelerate the rendering of 4D4D depth-cued light fields dramatically. We have developed a perceptual model for foveated light fields by extending the KFR for the rendering of 3D3D meshes. On datasets of high-resolution microscopic light fields, we observe 3.47\times -7.28\times3.47×-7.28× speedup in light field rendering with minimal perceptual loss of detail. We envision that 3D-KFR will reconcile the mutually conflicting goals of visual fidelity and rendering speed for interactive visualization of light fields. © 1995-2012 IEEE.","eye tracking; foveated rendering; Light fields; microscopic light fields; visualization","Eye tracking; Rendering (computer graphics); Textures; Virtual reality; Visualization; Free viewpoint renderings; Interactive frame rates; Interactive visualizations; Light field rendering; Perceptual model; Texture samplings; Virtual-reality headsets; Visual fidelity; Three dimensional computer graphics; algorithm; article; eye tracking; velocity",Article,"Final","",Scopus,2-s2.0-85111789253
"Bjelopavlovic M., Weyhrauch M., Erbe C., Burkard F., Petrowski K., Lehmann K.M.","57222986450;55389261100;35726599400;57226475310;8904910000;36337691400;","Influencing factors on aesthetics: Highly controlled study based on eye movement and the forensic aspects in computer-based assessment of visual appeal in upper front teeth",2021,"Applied Sciences (Switzerland)","11","15","6797","","",,,"10.3390/app11156797","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111694184&doi=10.3390%2fapp11156797&partnerID=40&md5=272df03ed73de37920e5ef47cc9e0484","Department of Prosthetic Dentistry, University Medical Center of the Johannes Gutenberg-University Mainz, Augustusplatz 2, Mainz, 55131, Germany; Fliednerweg 8, Mühltal, 64367, Germany; Department of Orthodontics, University Medical Center of the Johannes Gutenberg-University Mainz, Augustusplatz 2, Mainz, 55131, Germany; Am Houiller Platz2, Friedrichsdorf, 61381, Germany; Department of Medical Psychology and Medical Sociology, University Medical Center of the Johannes Gutenberg-University Mainz, Duesbergweg 6, Mainz, 55131, Germany","Bjelopavlovic, M., Department of Prosthetic Dentistry, University Medical Center of the Johannes Gutenberg-University Mainz, Augustusplatz 2, Mainz, 55131, Germany; Weyhrauch, M., Fliednerweg 8, Mühltal, 64367, Germany; Erbe, C., Department of Orthodontics, University Medical Center of the Johannes Gutenberg-University Mainz, Augustusplatz 2, Mainz, 55131, Germany; Burkard, F., Am Houiller Platz2, Friedrichsdorf, 61381, Germany; Petrowski, K., Department of Medical Psychology and Medical Sociology, University Medical Center of the Johannes Gutenberg-University Mainz, Duesbergweg 6, Mainz, 55131, Germany; Lehmann, K.M., Department of Prosthetic Dentistry, University Medical Center of the Johannes Gutenberg-University Mainz, Augustusplatz 2, Mainz, 55131, Germany","First impressions are formed by the external appearance and, in this respect, essentially by an examination of the face. In the literature, the teeth, especially the maxillary front, are among an eye-catching and sensitive area that plays a significant role in the overall evaluation of appearance. In this study, the first eye fixation of 60 subjects with different levels of dental training (layperson, trained layperson, dental student, and dentist) is recorded using an eye-tracking system, and their subsequent evaluation of the images is recorded. Ten unedited original photographs of different maxillary anterior teeth and ten subsequently edited photographs will be used to evaluate forensic aspects such as the effect of symmetry and color on the overall evaluation. The results will be used to determine which areas of the maxillary anterior are demonstrably viewed and whether knowledge of dental esthetics influences evaluation and viewing. © 2021 by the authors.","Aesthetics; Eye-tracking; Forensics; Golden ratio",,Article,"Final","",Scopus,2-s2.0-85111694184
"Garde G., Larumbe-Bergera A., Bossavit B., Porta S., Cabeza R., Villanueva A.","57215963483;57210106737;36730794700;7005292345;36763933900;7101612861;","Low-cost eye tracking calibration: A knowledge-based study†",2021,"Sensors","21","15","5109","","",,,"10.3390/s21155109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111278274&doi=10.3390%2fs21155109&partnerID=40&md5=3ad91a130121ac4257e854d6b109d0dd","Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain; Trinity College Dublin, School of Computer Science and Statistics, The University of Dublin, College Green, Dublin 2, D02 PN40, Ireland","Garde, G., Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain; Larumbe-Bergera, A., Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain; Bossavit, B., Trinity College Dublin, School of Computer Science and Statistics, The University of Dublin, College Green, Dublin 2, D02 PN40, Ireland; Porta, S., Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain; Cabeza, R., Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain; Villanueva, A., Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain","Subject calibration has been demonstrated to improve the accuracy in high-performance eye trackers. However, the true weight of calibration in off-the-shelf eye tracking solutions is still not addressed. In this work, a theoretical framework to measure the effects of calibration in deep learning-based gaze estimation is proposed for low-resolution systems. To this end, features extracted from the synthetic U2Eyes dataset are used in a fully connected network in order to isolate the effect of specific user’s features, such as kappa angles. Then, the impact of system calibration in a real setup employing I2Head dataset images is studied. The obtained results show accuracy improvements over 50%, probing that calibration is a key process also in low-resolution gaze estimation scenarios. Furthermore, we show that after calibration accuracy values close to those obtained by high-resolution systems, in the range of 0.7◦, could be theoretically obtained if a careful selection of image features was performed, demonstrating significant room for improvement for off-the-shelf eye tracking systems. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Calibration; Gaze-estimation; Low-resolution; Theoretical analysis","Calibration; Costs; Deep learning; Image enhancement; Knowledge based systems; Accuracy Improvement; Calibration accuracy; Eye tracking systems; Fully connected networks; High-resolution systems; Low cost eye tracking; System calibration; Theoretical framework; Eye tracking; calibration; eye fixation; Calibration; Eye-Tracking Technology; Fixation, Ocular",Article,"Final","",Scopus,2-s2.0-85111278274
"Ayyagari S.S.D.P., Jones R.D., Weddell S.J.","56648224800;55568525670;23968220300;","Detection of microsleep states from the EEG: a comparison of feature reduction methods",2021,"Medical and Biological Engineering and Computing","59","7-8",,"1643","1657",,,"10.1007/s11517-021-02386-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110780300&doi=10.1007%2fs11517-021-02386-y&partnerID=40&md5=038a95ce5e89daf74b8ba39a00b1e944","Department of Electrical and Computer Engineering, University of Canterbury, Christchurch, New Zealand; Christchurch Neurotechnology Research Programme, Christchurch, New Zealand; Computational Design and Adaptation, University of Canterbury, Christchurch, New Zealand; New Zealand Brain Research Institute, Christchurch, 8011, New Zealand","Ayyagari, S.S.D.P., Department of Electrical and Computer Engineering, University of Canterbury, Christchurch, New Zealand, Christchurch Neurotechnology Research Programme, Christchurch, New Zealand, Computational Design and Adaptation, University of Canterbury, Christchurch, New Zealand; Jones, R.D., Department of Electrical and Computer Engineering, University of Canterbury, Christchurch, New Zealand, Christchurch Neurotechnology Research Programme, Christchurch, New Zealand, New Zealand Brain Research Institute, Christchurch, 8011, New Zealand; Weddell, S.J., Department of Electrical and Computer Engineering, University of Canterbury, Christchurch, New Zealand, Christchurch Neurotechnology Research Programme, Christchurch, New Zealand, Computational Design and Adaptation, University of Canterbury, Christchurch, New Zealand","Microsleeps are brief lapses in consciousness with complete suspension of performance. They are the cause of fatal accidents in many transport sectors requiring sustained attention, especially driving. A microsleep-warning device, using wireless EEG electrodes, could be used to rouse a user from an imminent microsleep. High-dimensional datasets, especially in EEG-based classification, present challenges as there are often a large number of potentially useful features for detecting the phenomenon of interest. Thus, it is often important to reduce the dimension of the original data prior to training the classifier. In this study, linear dimensionality reduction methods—principal component analysis (PCA) and probabilistic PCA (PPCA)—were compared with eight non-linear dimensionality reduction methods (kernel PCA, classical multi-dimensional scaling, isometric mapping, nearest neighbour estimation, stochastic neighbourhood embedding, autoencoder, stochastic proximity embedding, and Laplacian eigenmaps) on previously collected behavioural and EEG data from eight healthy non-sleep-deprived volunteers performing a 1D-visuomotor tracking task for 1 h. The effectiveness of the feature reduction algorithms was evaluated by visual inspection of class separation on 3D scatterplots, by trustworthiness scores, and by microsleep detection performance on a stacked-generalisation-based linear discriminant analysis (LDA) system estimating the microsleep/responsive state at 1 Hz based on the reduced features. On trustworthiness, PPCA outperformed PCA, but PCA outperformed all of the non-linear techniques. The trustworthiness score for each feature reduction method also correlated strongly with microsleep-state detection performance, providing strong validation of the ability of trustworthiness to estimate the relative effectiveness of feature reduction approaches, in terms of predicting performance, and ability to do so independently of the gold standard. [Figure not available: see fulltext.] © 2021, International Federation for Medical and Biological Engineering.","Classification; Detection; EEG; Feature reduction; Microsleeps","Dimensionality reduction; Discriminant analysis; Embeddings; Large dataset; Stochastic systems; High dimensional datasets; Laplacian eigenmaps; Linear dimensionality reduction; Linear discriminant analysis; Micro-sleep detection; Multi-dimensional scaling; Nonlinear dimensionality reduction; Nonlinear techniques; Feature extraction; adult; article; autoencoder; classifier; clinical article; controlled study; discriminant analysis; electroencephalogram; embedding; eye tracking; female; gold standard; human; human experiment; intermethod comparison; kernel method; male; neighborhood; nonlinear dimensionality reduction; principal component analysis; stochastic model; algorithm; attention; electroencephalography; principal component analysis; Algorithms; Attention; Discriminant Analysis; Electroencephalography; Humans; Principal Component Analysis",Article,"Final","",Scopus,2-s2.0-85110780300
"Whang A.J.-W., Chen Y.-Y., Tseng W.-C., Tsai C.-H., Chao Y.-P., Yen C.-H., Liu C.-H., Zhang X.","24072330600;24069801200;57204573507;57226129791;15843250800;56973279600;56080525200;57239216500;","Pupil size prediction techniques based on convolution neural network",2021,"Sensors","21","15","4965","","",,,"10.3390/s21154965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110492717&doi=10.3390%2fs21154965&partnerID=40&md5=eb0307836028e9f4afbddb568baa042b","Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei City, 106335, Taiwan; Graduate Institute of Color & Illumination Technology, National Taiwan University of Science and Technology, Taipei City, 106335, Taiwan; Graduate Institute of Electro-Optical Engineering, National Taiwan University of Science and Technology, Taipei City, 106335, Taiwan; Graduate Institute of Biomedical Engineering, Chang Gung University, Taoyuan City, 333323, Taiwan; Department of Computer Science and Information Engineering, Chang Gung University, Taoyuan City, 333323, Taiwan; Department of Neurology, Chang Gung Memorial Hospital at Linkou, Taoyuan City, 333423, Taiwan; Department of Ophthalmology, Chang Gung Memorial Hospital at Linkou, Taoyuan City, 333423, Taiwan; College of Medicine, Chang Gung University, Taoyuan City, 333323, Taiwan","Whang, A.J.-W., Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei City, 106335, Taiwan; Chen, Y.-Y., Graduate Institute of Color & Illumination Technology, National Taiwan University of Science and Technology, Taipei City, 106335, Taiwan; Tseng, W.-C., Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei City, 106335, Taiwan; Tsai, C.-H., Graduate Institute of Electro-Optical Engineering, National Taiwan University of Science and Technology, Taipei City, 106335, Taiwan; Chao, Y.-P., Graduate Institute of Biomedical Engineering, Chang Gung University, Taoyuan City, 333323, Taiwan, Department of Computer Science and Information Engineering, Chang Gung University, Taoyuan City, 333323, Taiwan, Department of Neurology, Chang Gung Memorial Hospital at Linkou, Taoyuan City, 333423, Taiwan; Yen, C.-H., Graduate Institute of Biomedical Engineering, Chang Gung University, Taoyuan City, 333323, Taiwan, Department of Ophthalmology, Chang Gung Memorial Hospital at Linkou, Taoyuan City, 333423, Taiwan, College of Medicine, Chang Gung University, Taoyuan City, 333323, Taiwan; Liu, C.-H., Department of Ophthalmology, Chang Gung Memorial Hospital at Linkou, Taoyuan City, 333423, Taiwan, College of Medicine, Chang Gung University, Taoyuan City, 333323, Taiwan; Zhang, X., Department of Electronic and Computer Engineering, National Taiwan University of Science and Technology, Taipei City, 106335, Taiwan","The size of one’s pupil can indicate one’s physical condition and mental state. When we search related papers about AI and the pupil, most studies focused on eye-tracking. This paper proposes an algorithm that can calculate pupil size based on a convolution neural network (CNN). Usually, the shape of the pupil is not round, and 50% of pupils can be calculated using ellipses as the best fitting shapes. This paper uses the major and minor axes of an ellipse to represent the size of pupils and uses the two parameters as the output of the network. Regarding the input of the network, the dataset is in video format (continuous frames). Taking each frame from the videos and using these to train the CNN model may cause overfitting since the images are too similar. This study used data augmentation and calculated the structural similarity to ensure that the images had a certain degree of difference to avoid this problem. For optimizing the network structure, this study compared the mean error with changes in the depth of the network and the field of view (FOV) of the convolution filter. The result shows that both deepening the network and widening the FOV of the convolution filter can reduce the mean error. According to the results, the mean error of the pupil length is 5.437% and the pupil area is 10.57%. It can operate in low-cost mobile embedded systems at 35 frames per second, demonstrating that low-cost designs can be used for pupil size prediction. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","And machine learning; Biomedical imaging; Computational intelligence; Engineering in medicine and biology","Convolution; Costs; Embedded systems; Errors; Eye tracking; Continuous frames; Convolution filters; Convolution neural network; Data augmentation; Frames per seconds; Network structures; Physical conditions; Structural similarity; Neural networks; algorithm; human; pupil; Algorithms; Humans; Neural Networks, Computer; Pupil",Article,"Final","",Scopus,2-s2.0-85110492717
"Shi L., Wang C.Y., Tian F., Jia H.B.","57221663452;36651948200;56895556000;7202381504;","An integrated neural network model for pupil detection and tracking",2021,"Soft Computing","25","15",,"10117","10127",,,"10.1007/s00500-021-05984-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109344681&doi=10.1007%2fs00500-021-05984-y&partnerID=40&md5=6d18c8c6120f041fa7c04c06114bd9cd","State and Provincial Joint Engineering Laboratory of Advanced Network, Monitoring and Control, Xi’an Technological University, Xi’an, China; Bournemouth University, Fern Barrow, Poole, Dorset, BH12 5BB, United Kingdom; Institute of Aviation Medicine, Military Medical University, Air Force, Beijing, China","Shi, L., State and Provincial Joint Engineering Laboratory of Advanced Network, Monitoring and Control, Xi’an Technological University, Xi’an, China; Wang, C.Y., State and Provincial Joint Engineering Laboratory of Advanced Network, Monitoring and Control, Xi’an Technological University, Xi’an, China; Tian, F., Bournemouth University, Fern Barrow, Poole, Dorset, BH12 5BB, United Kingdom; Jia, H.B., Institute of Aviation Medicine, Military Medical University, Air Force, Beijing, China","The accurate detection and tracking of pupil is important to many applications such as human–computer interaction, driver’s fatigue detection and diagnosis of brain diseases. Existing approaches however face challenges in handing low quality of pupil images. In this paper, we propose an integrated pupil tracking framework, namely LVCF, based on deep learning. LVCF consists of the pupil detection model VCF which is an end-to-end network, and the LSTM pupil motion prediction model which applies LSTM to track pupil’s position. The proposed network was trained and evaluated on 10600 images and 75 videos taken from 3 realistic datasets. Within an error threshold of 5 pixels, VCF achieves an accuracy of more than 81%, and LVCF outperforms the state of arts by 9% in terms of percentage of pupils tracked. The project of LCVF is available at https://github.com/UnderTheMangoTree/LVCF. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Convolutional neural networks; Deep learning; Eye-tracking; Long short-term memory; Pupil detection","Deep learning; Diagnosis; Human computer interaction; Motion estimation; Predictive analytics; Computer interaction; Detection and tracking; End-to-end network; Error threshold; Fatigue detection; Motion prediction; Neural network model; Pupil detection; Long short-term memory",Article,"Final","",Scopus,2-s2.0-85109344681
"Sharma H., Drukker L., Papageorghiou A.T., Noble J.A.","56272987100;36241434600;57194082999;56185660000;","Machine learning-based analysis of operator pupillary response to assess cognitive workload in clinical ultrasound imaging",2021,"Computers in Biology and Medicine","135",,"104589","","",,,"10.1016/j.compbiomed.2021.104589","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108805307&doi=10.1016%2fj.compbiomed.2021.104589&partnerID=40&md5=747b3f4cd78d2254289d4917904e493d","Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Nuffield Department of Women's and Reproductive Health, University of Oxford, Oxford, United Kingdom","Sharma, H., Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Drukker, L., Nuffield Department of Women's and Reproductive Health, University of Oxford, Oxford, United Kingdom; Papageorghiou, A.T., Nuffield Department of Women's and Reproductive Health, University of Oxford, Oxford, United Kingdom; Noble, J.A., Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, Oxford, United Kingdom","Introduction: Pupillometry, the measurement of eye pupil diameter, is a well-established and objective modality correlated with cognitive workload. In this paper, we analyse the pupillary response of ultrasound imaging operators to assess their cognitive workload, captured while they undertake routine fetal ultrasound examinations. Our experiments and analysis are performed on real-world datasets obtained using remote eye-tracking under natural clinical environmental conditions. Methods: Our analysis pipeline involves careful temporal sequence (time-series) extraction by retrospectively matching the pupil diameter data with tasks captured in the corresponding ultrasound scan video in a multi-modal data acquisition setup. This is followed by the pupil diameter pre-processing and the calculation of pupillary response sequences. Exploratory statistical analysis of the operator pupillary responses and comparisons of the distributions between ultrasonographic tasks (fetal heart versus fetal brain) and operator expertise (newly-qualified versus experienced operators) are performed. Machine learning is explored to automatically classify the temporal sequences into the corresponding ultrasonographic tasks and operator experience using temporal, spectral, and time-frequency features with classical (shallow) models, and convolutional neural networks as deep learning models. Results: Preliminary statistical analysis of the extracted pupillary response shows a significant variation for different ultrasonographic tasks and operator expertise, suggesting different extents of cognitive workload in each case, as measured by pupillometry. The best-performing machine learning models achieve receiver operating characteristic (ROC) area under curve (AUC) values of 0.98 and 0.80, for ultrasonographic task classification and operator experience classification, respectively. Conclusion: We conclude that we can successfully assess cognitive workload from pupil diameter changes measured while ultrasound operators perform routine scans. The machine learning allows the discrimination of the undertaken ultrasonographic tasks and scanning expertise using the pupillary response sequences as an index of the operators’ cognitive workload. A high cognitive workload can reduce operator efficiency and constrain their decision-making, hence, the ability to objectively assess cognitive workload is a first step towards understanding these effects on operator performance in biomedical applications such as medical imaging. © 2021 The Authors","Cognitive workload; Convolutional neural network; Deep learning; Eye-tracking; Fetal ultrasound; Machine learning; Multi-modal data; Pupillometry; Sonography data science; Time-series analysis; Ultrasound imaging","Data acquisition; Data Science; Data visualization; Decision making; Deep learning; Medical imaging; Neural networks; Time series analysis; Ultrasonic imaging; Cognitive workloads; Convolutional neural network; Deep learning; Eye-tracking; Fetal ultrasound; Machine-learning; Multi-modal data; Pupillometry; Sonography data science; Time-series analysis; Ultrasound imaging; Modal analysis; area under the curve; article; calculation; controlled study; convolutional neural network; data science; decision making; deep learning; DNA responsive element; extraction; eye tracking; fetus brain; fetus echography; fetus heart; human; human experiment; pipeline; pupil diameter; pupillometry; receiver operating characteristic; retrospective study; time series analysis; videorecording; workload; cognition; echography; machine learning; Cognition; Machine Learning; Retrospective Studies; Ultrasonography; Workload",Article,"Final","",Scopus,2-s2.0-85108805307
"Min-Allah N., Jan F., Alrashed S.","24725148500;55247968500;14026381800;","Pupil detection schemes in human eye: a review",2021,"Multimedia Systems","27","4",,"753","777",,2,"10.1007/s00530-021-00806-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106312065&doi=10.1007%2fs00530-021-00806-5&partnerID=40&md5=fac0c0fd05f4653f80672999130452bd","Department of Computer Science, College of Computer Science and Information Technology, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia; Management Information Systems Department, College of Applied Studies and Community Service, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia","Min-Allah, N., Department of Computer Science, College of Computer Science and Information Technology, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia; Jan, F., Department of Computer Science, College of Computer Science and Information Technology, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia; Alrashed, S., Management Information Systems Department, College of Applied Studies and Community Service, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia","Pupil detection in a human eyeimage or video plays a key role in many applications such as eye-tracking, diabetic retinopathy screening, smart homes, iris recognition, etc. Literature reveals pupil detection faces many complications including light reflections, cataract disease, pupil constriction/dilation moments, contact lenses, eyebrows, eyelashes, hair strips, and closed eye. To cope with these challenges, research community has been struggling to devise resilient pupil localization schemes for the image/video data collected using the near-infrared (NIR) or visible spectrum (VS) illumination. This study presents a critical review of numerous pupil detection schemes taken from standard sources. This review includes pupil localization schemes based on machine learning, histogram/thresholding, Integro-differential operator (IDO), Hough transform and among others. The probable pros and cons of each scheme are highlighted. Finally, this study offers recommendations for designing a robust pupil detection system. As scope of pupil detection is very broader, therefore this review would be a great source of information for the relevant research community. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Biocybernetics; Deep learning; Gaze detection; Pupil detection; Smart cities; Smart systems; Super resolution","Automation; Biometrics; Diagnosis; Eye protection; Hough transforms; Infrared devices; Intelligent buildings; Mathematical operators; Diabetic retinopathy screening; Integro differential operator; Iris recognition; Pupil detection; Pupil localization; Research communities; Standard sources; Visible spectra; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85106312065
"Fan N., Li X., Zhou Z., Liu Q., He Z.","57188637976;56386356300;57217077468;57189387635;53363609300;","Learning dual-margin model for visual tracking",2021,"Neural Networks","140",,,"344","354",,1,"10.1016/j.neunet.2021.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105698764&doi=10.1016%2fj.neunet.2021.04.004&partnerID=40&md5=b57d0eaa03bfe53af299daa9d29c5472","School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China","Fan, N., School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Li, X., School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Zhou, Z., School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Liu, Q., School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; He, Z., School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen, China","Existing trackers usually exploit robust features or online updating mechanisms to deal with target variations which is a key challenge in visual tracking. However, the features being robust to variations remain little spatial information, and existing online updating methods are prone to overfitting. In this paper, we propose a dual-margin model for robust and accurate visual tracking. The dual-margin model comprises an intra-object margin between different target appearances and an inter-object margin between the target and the background. The proposed method is able to not only distinguish the target from the background but also perceive the target changes, which tracks target appearance changing and facilitates accurate target state estimation. In addition, to exploit rich off-line video data and learn general rules of target appearance variations, we train the dual-margin model on a large off-line video dataset. We perform tracking under a Siamese framework using the constructed appearance set as templates. The proposed method achieves accurate and robust tracking performance on five public datasets while running in real-time. The favorable performance against the state-of-the-art methods demonstrates the effectiveness of the proposed algorithm. © 2021 Elsevier Ltd","Dual margin; Siamese network; Visual tracking","Dual margin; Learn+; Overfitting; Siamese network; Spatial informations; Target state estimations; Updating methods; Video data; Video dataset; Visual Tracking; Large dataset; algorithm; article; eye tracking; learning; running; videorecording; automated pattern recognition; image processing; machine learning; procedures; Image Processing, Computer-Assisted; Machine Learning; Pattern Recognition, Automated",Article,"Final","",Scopus,2-s2.0-85105698764
"Tadeja S.K., Lu Y., Rydlewicz M., Rydlewicz W., Bubas T., Kristensson P.O.","57209310553;57219406521;57203411370;57203414966;57219510158;6507412583;","Exploring gestural input for engineering surveys of real-life structures in virtual reality using photogrammetric 3D models",2021,"Multimedia Tools and Applications","80","20",,"31039","31058",,1,"10.1007/s11042-021-10520-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099986310&doi=10.1007%2fs11042-021-10520-z&partnerID=40&md5=019140324b2d6e1b16932a1b1807fd0d","Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Centrum Systemów Softdesk, Łódź, Poland","Tadeja, S.K., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Lu, Y., Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Rydlewicz, M., Centrum Systemów Softdesk, Łódź, Poland; Rydlewicz, W., Centrum Systemów Softdesk, Łódź, Poland; Bubas, T., Centrum Systemów Softdesk, Łódź, Poland; Kristensson, P.O., Department of Engineering, University of Cambridge, Cambridge, United Kingdom","Photogrammetry is a promising set of methods for generating photorealistic 3D models of physical objects and structures. Such methods may rely solely on camera-captured photographs or include additional sensor data. Digital twins are digital replicas of physical objects and structures. Photogrammetry is an opportune approach for generating 3D models for the purpose of preparing digital twins. At a sufficiently high level of quality, digital twins provide effective archival representations of physical objects and structures and become effective substitutes for engineering inspections and surveying. While photogrammetric techniques are well-established, insights about effective methods for interacting with such models in virtual reality remain underexplored. We report the results of a qualitative engineering case study in which we asked six domain experts to carry out engineering measurement tasks in an immersive environment using bimanual gestural input coupled with gaze-tracking. The qualitative case study revealed that gaze-supported bimanual interaction of photogrammetric 3D models is a promising modality for domain experts. It allows the experts to efficiently manipulate and measure elements of the 3D model. To better allow designers to support this modality, we report design implications distilled from the feedback from the domain experts. © 2021, The Author(s).","Digital twinning; Immersive analytics; Industrial visual analytics; Photogrammetry; Virtual reality; Virtual reality content","3D modeling; Digital twin; Eye tracking; Photogrammetry; Surveys; Bi-manual interaction; Design implications; Engineering inspections; Engineering measurements; Engineering surveys; Immersive environment; Photogrammetric technique; Qualitative case studies; Virtual reality",Article,"Final","",Scopus,2-s2.0-85099986310
"Yang B., Huang J., Sun M., Huo J., Li X., Xiong C.","57226054935;8621146500;57299165700;57207206228;57221840704;57211738191;","Head-free, Human Gaze-driven Assistive Robotic System for Reaching and Grasping",2021,"Chinese Control Conference, CCC","2021-July",,,"4138","4143",,,"10.23919/CCC52363.2021.9549800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117325986&doi=10.23919%2fCCC52363.2021.9549800&partnerID=40&md5=c915ef7753eba709d3185f0def88d93d","Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Huazhong University of Science and Technology, Sch. of Mech. Sci. and Eng. and the State Key Lab. of Digital Manufacturing Equipment and Technology, Wuhan, 430074, China","Yang, B., Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Huang, J., Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Sun, M., Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Huo, J., Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Li, X., Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Xiong, C., Huazhong University of Science and Technology, Sch. of Mech. Sci. and Eng. and the State Key Lab. of Digital Manufacturing Equipment and Technology, Wuhan, 430074, China","Patients with limb dysfunction have limited mobility, which prevents them from performing daily activities. We have developed an assistive robot system with an intuitive head free gaze interface. The system consists of multiple modules, including 3D gaze estimation, head free coordinate transformation, intention recognition, and robot trajectory planning. The robotic assistive system obtains clues from the user's gaze to decode their intentions and implement actions. This allows the user only needs to look at the objects to make the robot system reach, grasp, and bring them to the user. The 3D gaze estimation is evaluated with 5 subjects, showing an overall accuracy of 5.53±1.2 cm. The integrated system's experimental results show that the success rate is 96% in the implementation of automatic trajectory planning, and the success rate is 92% in the implementation of fixation-based trajectory planning. Finally, the results and work required to improve the system are discussed. © 2021 Technical Committee on Control Theory, Chinese Association of Automation.","3D gaze estimation; Assistive robot; Gaze-based control; Intention estimation","Robotics; Robots; Trajectories; 3d gaze estimation; Assistive robotics; Assistive robots; Daily activity; Gaze estimation; Gaze-based control; Intention estimation; Robotic systems; Robots system; Trajectory Planning; Robot programming",Conference Paper,"Final","",Scopus,2-s2.0-85117325986
"Wang X., Zhao X., Zhang Y.","57204294755;55352149700;56075029000;","Deep-learning-based reading eye-movement analysis for aiding biometric recognition",2021,"Neurocomputing","444",,,"390","398",,,"10.1016/j.neucom.2020.06.137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097802289&doi=10.1016%2fj.neucom.2020.06.137&partnerID=40&md5=b357bd5435fdd8c79fd42e4a87c388d6","National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi'an, 710072, China","Wang, X., National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi'an, 710072, China; Zhao, X., National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi'an, 710072, China; Zhang, Y., National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi'an, 710072, China","Eye-movement recognition is a new type of biometric recognition technology. Without considering the characteristics of the stimuli, the existing eye-movement recognition technology is based on eye-movement trajectory similarity measurements and uses more eye-movement features. Related studies on reading psychology have shown that when reading text, human eye-movements are different between individuals yet stable for a given individual. This paper proposes a type of technology for aiding biometric recognition based on reading eye-movement. By introducing a deep-learning framework, a computational model for reading eye-movement recognition (REMR) was constructed. The model takes the text, fixation, and text-based linguistic feature sequences as inputs and identifies a human subject by measuring the similarity distance between the predicted fixation sequence and the actual one (to be identified). The experimental results show that the fixation sequence similarity recognition algorithm obtained an equal error rate of 19.4% on the test set, and the model obtained an 86.5% Rank-1 recognition rate on the test set. © 2020 Elsevier B.V.","Biometrics; Deep-learning; Eye tracking; Eye-movement model; Identity authentication; Reading eye-movement","Biometrics; Deep learning; Linguistics; Motion estimation; Biometric recognition; Biometric recognition technology; Eye movement analysis; Learning frameworks; Movement recognition; Movement trajectories; Recognition algorithm; Similarity distance; Eye movements; algorithm; article; biometry; computer model; deep learning; eye tracking; human; molecular recognition",Article,"Final","",Scopus,2-s2.0-85097802289
"Kocejko T.","24824292600;","Using deep learning to increase accuracy of gaze controlled prosthetic arm",2021,"International Conference on Human System Interaction, HSI","2021-July",,,"","",,,"10.1109/HSI52170.2021.9538710","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116064473&doi=10.1109%2fHSI52170.2021.9538710&partnerID=40&md5=cb80d4c7530832cff2d0376be9ca5394","Gdansk University of Technology, Department of Biomedical Engineering, Gdansk, Poland","Kocejko, T., Gdansk University of Technology, Department of Biomedical Engineering, Gdansk, Poland","This paper presents how neural networks can be utilized to improve the accuracy of reach and grab functionality of hybrid prosthetic arm with eye tracing interface. The LSTM based Autoencoder was introduced to overcome the problem of lack of accuracy of the gaze tracking modality in this hybrid interface. The gaze based interaction strongly depends on the eye tracking hardware. In this paper it was presented how the overall the accuracy can be slightly improved by software solution. The cloud of points related to possible final positions of the arm was created to train Autoencoder. The trained model was next used to improve the position provided by the eye tracker. Using the LSTM based Autoencoder resulted in nearly 3% improvement of the overall accuracy. © 2021 IEEE.","deep filter; gaze tracking; HCI; human computer interaction; prosthetic arm","Human computer interaction; Long short-term memory; Prosthetics; Auto encoders; Cloud of point; Deep filter; Eye-tracking; Gaze-based interaction; Gaze-tracking; Hybrid interface; Neural-networks; Prosthetic arm; Software solution; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116064473
"Kollias K.-F., Syriopoulou-Delli C.K., Sarigiannidis P., Fragulis G.F.","57221864882;40462475200;12445587500;6602402881;","The contribution of Machine Learning and Eye-tracking technology in Autism Spectrum Disorder research: A Review Study",2021,"2021 10th International Conference on Modern Circuits and Systems Technologies, MOCAST 2021",,,"9493357","","",,,"10.1109/MOCAST52088.2021.9493357","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112197915&doi=10.1109%2fMOCAST52088.2021.9493357&partnerID=40&md5=6b58468a95b2e1310899f72425697550","University of Western, Laboratory of Robotics, Embedded and Integrated Systems, Department of Electrical and Computer Engineering, Greece; University of Macedonia, Department of Educational and Social Policy, Greece; University of Western, Department of Electrical and Computer Engineering, Greece","Kollias, K.-F., University of Western, Laboratory of Robotics, Embedded and Integrated Systems, Department of Electrical and Computer Engineering, Greece; Syriopoulou-Delli, C.K., University of Macedonia, Department of Educational and Social Policy, Greece; Sarigiannidis, P., University of Western, Department of Electrical and Computer Engineering, Greece; Fragulis, G.F., University of Western, Laboratory of Robotics, Embedded and Integrated Systems, Department of Electrical and Computer Engineering, Greece","According to Diagnostic and Statistical Manual of Mental Disorders, Autism spectrum disorder (ASD) is a developmental disorder characterised by reduced social interaction and communication, and by restricted, repetitive, and stereotyped behaviour. An important characteristic of autism, referred in several diagnostic tests, is a deficit in eye gaze. The objective of this study is to review the literature concerning machine learning and eye-tracking in ASD studies conducted since 2015. Our search on PubMed identified 18 studies which used various eye-tracking instruments, applied machine learning in different ways, distributed several tasks and had a wide range of sample sizes, age groups and functional skills of participants. There were also studies that utilised other instruments, such as Electroencephalography (EEG) and movement measures. Taken together, the results of these studies show that the combination of machine learning, and eye-tracking technology can contribute to autism identification characteristics by detecting the visual atypicalities of ASD people. In conclusion, machine learning and eye-tracking ASD studies could be considered a promising tool in autism research and future studies could involve other technological approaches, such as Internet of Things (IoT), as well. © 2021 IEEE.","ASD; Autism; Eye-tracking technology; Machine learning","Diseases; Electroencephalography; Electrophysiology; Internet of things; Machine learning; Applied machine learning; Autism spectrum disorders; Developmental disorders; Diagnostic tests; Eye tracking technologies; Internet of Things (IOT); Mental disorders; Social interactions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85112197915
"Palmero C., Sharma A., Behrendt K., Krishnakumar K., Komogortsev O.V., Talathi S.S.","57188829002;57219762538;57193013839;57219761370;6506328653;57224997923;","Openeds2020 challenge on gaze tracking for vr: Dataset and results",2021,"Sensors","21","14","4769","","",,,"10.3390/s21144769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109573720&doi=10.3390%2fs21144769&partnerID=40&md5=e4c512bb1b3a4d2607ae7716cabe7fbd","Department of Mathematics and Informatics, Universitat de Barcelona, Barcelona, 08007, Spain; Computer Vision Center, Campus UAB, Bellaterra08193, Spain; Eye Tracking Department, Facebook Reality Labs Research, Redmond, WA  98052, United States; Facebook Reality Labs, Menlo Park, CA  94025, United States; Department of Computer Science, Texas State University, San Marcos, TX  78666, United States","Palmero, C., Department of Mathematics and Informatics, Universitat de Barcelona, Barcelona, 08007, Spain, Computer Vision Center, Campus UAB, Bellaterra08193, Spain; Sharma, A., Eye Tracking Department, Facebook Reality Labs Research, Redmond, WA  98052, United States; Behrendt, K., Facebook Reality Labs, Menlo Park, CA  94025, United States; Krishnakumar, K., Facebook Reality Labs, Menlo Park, CA  94025, United States; Komogortsev, O.V., Eye Tracking Department, Facebook Reality Labs Research, Redmond, WA  98052, United States, Department of Computer Science, Texas State University, San Marcos, TX  78666, United States; Talathi, S.S., Eye Tracking Department, Facebook Reality Labs Research, Redmond, WA  98052, United States","This paper summarizes the OpenEDS 2020 Challenge dataset, the proposed baselines, and results obtained by the top three winners of each competition: (1) Gaze prediction Challenge, with the goal of predicting the gaze vector 1 to 5 frames into the future based on a sequence of previous eye images, and (2) Sparse Temporal Semantic Segmentation Challenge, with the goal of using temporal information to propagate semantic eye labels to contiguous eye image frames. Both competitions were based on the OpenEDS2020 dataset, a novel dataset of eye-image sequences captured at a frame rate of 100 Hz under controlled illumination, using a virtual-reality head-mounted display with two synchronized eye-facing cameras. The dataset, which we make publicly available for the research community, consists of 87 subjects performing several gaze-elicited tasks, and is divided into 2 subsets, one for each competition task. The proposed baselines, based on deep learning approaches, obtained an average angular error of 5.37 degrees for gaze prediction, and a mean intersection over union score (mIoU) of 84.1% for semantic segmentation. The winning solutions were able to outperform the baselines, obtaining up to 3.17 degrees for the former task and 95.2% mIoU for the latter. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Gaze estimation; Gaze prediction; Semantic segmentation; Video oculography; Virtual reality","Deep learning; Forecasting; Helmet mounted displays; Image segmentation; Semantics; Virtual reality; Angular errors; Frame rate; Gaze tracking; Head mounted displays; Learning approach; Research communities; Semantic segmentation; Temporal information; Eye tracking; human; photography; semantics; virtual reality; Eye-Tracking Technology; Humans; Photography; Semantics; Smart Glasses; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85109573720
"Yoo S., Jeong S., Jang Y.","57192084306;57192077488;36152811100;","Gaze behavior effect on gaze data visualization at different abstraction levels",2021,"Sensors","21","14","4686","","",,,"10.3390/s21144686","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109219338&doi=10.3390%2fs21144686&partnerID=40&md5=528fc84efdade44edfa7638456887897","Computer Engineering and Convergence Engineering for Intelligent Drone, Sejong University, Seoul, 05006, South Korea","Yoo, S., Computer Engineering and Convergence Engineering for Intelligent Drone, Sejong University, Seoul, 05006, South Korea; Jeong, S., Computer Engineering and Convergence Engineering for Intelligent Drone, Sejong University, Seoul, 05006, South Korea; Jang, Y., Computer Engineering and Convergence Engineering for Intelligent Drone, Sejong University, Seoul, 05006, South Korea","Many gaze data visualization techniques intuitively show eye movement together with visual stimuli. The eye tracker records a large number of eye movements within a short period. Therefore, visualizing raw gaze data with the visual stimulus appears complicated and obscured, making it difficult to gain insight through visualization. To avoid the complication, we often employ fixation identification algorithms for more abstract visualizations. In the past, many scientists have focused on gaze data abstraction with the attention map and analyzed detail gaze movement patterns with the scanpath visualization. Abstract eye movement patterns change dramatically depending on fixation identification algorithms in the preprocessing. However, it is difficult to find out how fixation identification algorithms affect gaze movement pattern visualizations. Additionally, scientists often spend much time on adjusting parameters manually in the fixation identification algorithms. In this paper, we propose a gaze behavior-based data processing method for abstract gaze data visualization. The proposed method classifies raw gaze data using machine learning models for image classification, such as CNN, AlexNet, and LeNet. Additionally, we compare the velocity-based identification (I-VT), dispersion-based identification (I-DT), density-based fixation identification, velocity and dispersion-based (I-VDT), and machine learning based and behavior-based modelson various visualizations at each abstraction level, such as attention map, scanpath, and abstract gaze movement visualization. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Gaze behavior; Gaze data visualization; Machine learning","Data handling; Dispersions; Eye movements; Eye tracking; Machine learning; Visualization; Abstraction level; Adjusting parameters; Data abstraction; Data processing methods; Eye movement patterns; Identification algorithms; Machine learning models; Visualization technique; Data visualization; algorithm; attention; eye fixation; eye movement; Algorithms; Attention; Data Visualization; Eye Movements; Fixation, Ocular",Article,"Final","",Scopus,2-s2.0-85109219338
"Spiller M., Liu Y.-H., Hossain M.Z., Gedeon T., Geissler J., Nürnberger A.","57211096099;26662786100;57212814547;24400830200;57221331539;14027288100;","Predicting Visual Search Task Success from Eye Gaze Data as a Basis for User-Adaptive Information Visualization Systems",2021,"ACM Transactions on Interactive Intelligent Systems","11","2","14","","",,1,"10.1145/3446638","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110737317&doi=10.1145%2f3446638&partnerID=40&md5=b89aa198b939f06d2316f727665dd46f","INKA-Innovation Laboratory for Image Guided Therapy, Health Campus Immunology Infectiology and Inflammation (GC-I3), Otto-von-Guericke-University, Leipziger Straße 44, Magdeburg, Germany; Oslo Metropolitan University, Pilestredet 48, Oslo, 0167, Norway; The Australi, Canberra, ACT  2600, Australia; Otto von Guericke University, Universitätsplatz 2, Magdeburg, 39106, Germany","Spiller, M., INKA-Innovation Laboratory for Image Guided Therapy, Health Campus Immunology Infectiology and Inflammation (GC-I3), Otto-von-Guericke-University, Leipziger Straße 44, Magdeburg, Germany; Liu, Y.-H., Oslo Metropolitan University, Pilestredet 48, Oslo, 0167, Norway; Hossain, M.Z., The Australi, Canberra, ACT  2600, Australia; Gedeon, T., The Australi, Canberra, ACT  2600, Australia; Geissler, J., Otto von Guericke University, Universitätsplatz 2, Magdeburg, 39106, Germany; Nürnberger, A., Otto von Guericke University, Universitätsplatz 2, Magdeburg, 39106, Germany","Information visualizations are an efficient means to support the users in understanding large amounts of complex, interconnected data; user comprehension, however, depends on individual factors such as their cognitive abilities. The research literature provides evidence that user-adaptive information visualizations positively impact the users' performance in visualization tasks. This study attempts to contribute toward the development of a computational model to predict the users' success in visual search tasks from eye gaze data and thereby drive such user-adaptive systems. State-of-the-art deep learning models for time series classification have been trained on sequential eye gaze data obtained from 40 study participants' interaction with a circular and an organizational graph. The results suggest that such models yield higher accuracy than a baseline classifier and previously used models for this purpose. In particular, a Multivariate Long Short Term Memory Fully Convolutional Network shows encouraging performance for its use in online user-adaptive systems. Given this finding, such a computational model can infer the users' need for support during interaction with a graph and trigger appropriate interventions in user-adaptive information visualization systems. This facilitates the design of such systems since further interaction data like mouse clicks is not required. © 2021 Association for Computing Machinery.","Eye tracking; individual differences; time series classification; user-adaptation","Adaptive systems; Computation theory; Computational methods; Convolutional neural networks; Data visualization; Deep learning; Digital storage; Information analysis; Information systems; Mammals; Online systems; Visualization; Cognitive ability; Computational model; Convolutional networks; Individual factors; Information visualization; State of the art; Time series classifications; User-adaptive systems; Search engines",Article,"Final","",Scopus,2-s2.0-85110737317
"Sáiz-Manzanares M.C., Pérez I.R., Rodríguez A.A., Arribas S.R., Almeida L., Martin C.F.","57217003421;57225972792;57221001924;57219529245;7102247228;57224861711;","Analysis of the learning process through eye tracking technology and feature selection techniques",2021,"Applied Sciences (Switzerland)","11","13","6157","","",,,"10.3390/app11136157","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109930004&doi=10.3390%2fapp11136157&partnerID=40&md5=d35dbf19e12f0e00ff061e8d9031c7ff","Departamento de Ciencias de la Salud, Facultad de Ciencias de la Salud, Universidad de Burgos, Research Group DATAHES, Pº Comendadores s/n, Burgos, 09001, Spain; Departamento de Ingeniería Informática, Escuela Politécnica Superior, Universidad de Burgos, Research Group ADMIRABLE, Escuela Politécnica Superior, Avda. de Cantabria s/n, Burgos, 09006, Spain; Departamento de Ingeniería Informática, Escuela Politécnica Superior, Universidad de Burgos, Research Group DATAHES, Escuela Politécnica Superior, Avda. de Cantabria s/n, Burgos, 09006, Spain; Instituto de Educação, Universidade do Minho, Research Group CIEd, Campus de Gualtar, Braga, 4710-057, Portugal; Departamento de Filología Inglesa, Universidad de Burgos, Pº Comendadores s/n, Burgos, 09001, Spain","Sáiz-Manzanares, M.C., Departamento de Ciencias de la Salud, Facultad de Ciencias de la Salud, Universidad de Burgos, Research Group DATAHES, Pº Comendadores s/n, Burgos, 09001, Spain; Pérez, I.R., Departamento de Ingeniería Informática, Escuela Politécnica Superior, Universidad de Burgos, Research Group ADMIRABLE, Escuela Politécnica Superior, Avda. de Cantabria s/n, Burgos, 09006, Spain; Rodríguez, A.A., Departamento de Ingeniería Informática, Escuela Politécnica Superior, Universidad de Burgos, Research Group ADMIRABLE, Escuela Politécnica Superior, Avda. de Cantabria s/n, Burgos, 09006, Spain; Arribas, S.R., Departamento de Ingeniería Informática, Escuela Politécnica Superior, Universidad de Burgos, Research Group DATAHES, Escuela Politécnica Superior, Avda. de Cantabria s/n, Burgos, 09006, Spain; Almeida, L., Instituto de Educação, Universidade do Minho, Research Group CIEd, Campus de Gualtar, Braga, 4710-057, Portugal; Martin, C.F., Departamento de Filología Inglesa, Universidad de Burgos, Pº Comendadores s/n, Burgos, 09001, Spain","In recent decades, the use of technological resources such as the eye tracking methodology is providing cognitive researchers with important tools to better understand the learning process. However, the interpretation of the metrics requires the use of supervised and unsupervised learning techniques. The main goal of this study was to analyse the results obtained with the eye tracking methodology by applying statistical tests and supervised and unsupervised machine learning tech-niques, and to contrast the effectiveness of each one. The parameters of fixations, saccades, blinks and scan path, and the results in a puzzle task were found. The statistical study concluded that no significant differences were found between participants in solving the crossword puzzle task; significant differences were only detected in the parameters saccade amplitude minimum and saccade velocity minimum. On the other hand, this study, with supervised machine learning techniques, provided possible features for analysis, some of them different from those used in the statistical study. Regarding the clustering techniques, a good fit was found between the algorithms used (k-means ++, fuzzy k-means and DBSCAN). These algorithms provided the learning profile of the participants in three types (students over 50 years old; and students and teachers under 50 years of age). Therefore, the use of both types of data analysis is considered complementary. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Clustering; Cognition; Eye tracking; Information processing; Instance selection; Machine learning",,Article,"Final","",Scopus,2-s2.0-85109930004
"Parra E., Chicchi Giglioli I.A., Philip J., Carrasco-Ribelles L.A., Marín-Morales J., Alcañiz Raya M.","55547606400;56994284500;57201796092;57221713820;57191977544;36921902100;","Combining virtual reality and organizational neuroscience for leadership assessment",2021,"Applied Sciences (Switzerland)","11","13","5956","","",,,"10.3390/app11135956","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109277670&doi=10.3390%2fapp11135956&partnerID=40&md5=d76c6e3fb9ea97df1a43be7345f78000","Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politécnica de Valencia, Valencia, 46022, Spain; College of Business, University of New Haven, 300 Boston Post Rd, West Haven, CT  06516, United States; Fundació Institut Universitari per a la Recerca a l’Atenció Primària de Salut Jordi Gol i Gurina (IDIAPJGol), Barcelona, 08007, Spain","Parra, E., Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politécnica de Valencia, Valencia, 46022, Spain; Chicchi Giglioli, I.A., Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politécnica de Valencia, Valencia, 46022, Spain; Philip, J., College of Business, University of New Haven, 300 Boston Post Rd, West Haven, CT  06516, United States; Carrasco-Ribelles, L.A., Fundació Institut Universitari per a la Recerca a l’Atenció Primària de Salut Jordi Gol i Gurina (IDIAPJGol), Barcelona, 08007, Spain; Marín-Morales, J., Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politécnica de Valencia, Valencia, 46022, Spain; Alcañiz Raya, M., Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politécnica de Valencia, Valencia, 46022, Spain","In this article, we introduce three-dimensional Serious Games (3DSGs) under an evidencecentered design (ECD) framework and use an organizational neuroscience-based eye-tracking measure to capture implicit behavioral signals associated with leadership skills. While ECD is a wellestablished framework used in the design and development of assessments, it has rarely been utilized in organizational research. The study proposes a novel 3DSG combined with organizational neuroscience methods as a promising tool to assess and recognize leadership-related behavioral patterns that manifest during complex and realistic social situations. We offer a research protocol for assessing task-and relationship-oriented leadership skills that uses ECD, eye-tracking measures, and machine learning. Seamlessly embedding biological measures into 3DSGs enables objective assessment methods that are based on machine learning techniques to achieve high ecological validity. We conclude by describing a future research agenda for the combined use of 3DSGs and organizational neuroscience methods for leadership and human resources. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Assessment method; Decision-making behaviors; Evidence-centered design; Leadership style; Machine learning; Serious game; Visual attention",,Article,"Final","",Scopus,2-s2.0-85109277670
"Kaczorowska M., Karczmarek P., Plechawska-Wójcik M., Tokovarov M.","57194211584;14830356600;41262115200;57194212082;","On the improvement of eye tracking-based cognitive workload estimation using aggregation functions",2021,"Sensors","21","13","4542","","",,,"10.3390/s21134542","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108970133&doi=10.3390%2fs21134542&partnerID=40&md5=563f158930d19c970ef70cde5156ca46","Department of Computer Science, Lublin University of Technology, Lublin, 20-618, Poland","Kaczorowska, M., Department of Computer Science, Lublin University of Technology, Lublin, 20-618, Poland; Karczmarek, P., Department of Computer Science, Lublin University of Technology, Lublin, 20-618, Poland; Plechawska-Wójcik, M., Department of Computer Science, Lublin University of Technology, Lublin, 20-618, Poland; Tokovarov, M., Department of Computer Science, Lublin University of Technology, Lublin, 20-618, Poland","Cognitive workload, being a quantitative measure of mental effort, draws significant interest of researchers, as it allows to monitor the state of mental fatigue. Estimation of cognitive workload becomes especially important for job positions requiring outstanding engagement and responsibility, e.g., air-traffic dispatchers, pilots, car or train drivers. Cognitive workload estimation finds its applications also in the field of education material preparation. It allows to monitor the difficulty degree for specific tasks enabling to adjust the level of education materials to typical abilities of students. In this study, we present the results of research conducted with the goal of examining the influence of various fuzzy or non-fuzzy aggregation functions upon the quality of cognitive workload estimation. Various classic machine learning models were successfully applied to the problem. The results of extensive in-depth experiments with over 2000 aggregation operators shows the applicability of the approach based on the aggregation functions. Moreover, the approach based on aggregation process allows for further improvement of classification results. A wide range of aggregation functions is considered and the results suggest that the combination of classical machine learning models and aggregation methods allows to achieve high quality of cognitive workload level recognition preserving low computational cost. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Aggregation; Classical machine learning; Cognitive workload; Fuzzy measure; Generalized Choquet integral","Driver training; Education computing; Machine learning; Mathematical operators; Aggregation functions; Aggregation methods; Aggregation operator; Classification results; Computational costs; Level of educations; Machine learning models; Quantitative measures; Eye tracking; cognition; human; machine learning; physiologic monitoring; workload; Cognition; Eye-Tracking Technology; Humans; Machine Learning; Monitoring, Physiologic; Workload",Article,"Final","",Scopus,2-s2.0-85108970133
"Golard A., Talathi S.S.","57222990964;57224997923;","Ultrasound for gaze estimation—a modeling and empirical study",2021,"Sensors","21","13","4502","","",,,"10.3390/s21134502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108896359&doi=10.3390%2fs21134502&partnerID=40&md5=287ee868e89aece27cd880904cb69569","Facebook Reality Labs, Redmond, WA  98052, United States","Golard, A., Facebook Reality Labs, Redmond, WA  98052, United States; Talathi, S.S., Facebook Reality Labs, Redmond, WA  98052, United States","Most eye tracking methods are light-based. As such, they can suffer from ambient light changes when used outdoors, especially for use cases where eye trackers are embedded in Augmented Reality glasses. It has been recently suggested that ultrasound could provide a low power, fast, light-insensitive alternative to camera-based sensors for eye tracking. Here, we report on our work on modeling ultrasound sensor integration into a glasses form factor AR device to evaluate the feasibility of estimating eye-gaze in various configurations. Next, we designed a benchtop experimental setup to collect empirical data on time of flight and amplitude signals for reflected ultrasound waves for a range of gaze angles of a model eye. We used this data as input for a low-complexity gradient-boosted tree machine learning regression model and demonstrate that we can effectively estimate gaze (gaze RMSE error of 0.965 ± 0.178 degrees with an adjusted R2 score of 90.2 ± 4.6). © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","CMUT; Comsol Modeling; Eye tracking; Gaze estimation; Gradient Boosted Regression Trees; Machine Learning; Ultrasound","Augmented reality; Glass; Regression analysis; Trees (mathematics); Ultrasonics; Camera based Sensors; Empirical data; Empirical studies; Eye tracking methods; Gaze estimation; Regression model; Ultrasound sensors; Ultrasound waves; Eye tracking; echography; eye fixation; eye movement; machine learning; Augmented Reality; Eye Movements; Fixation, Ocular; Machine Learning; Ultrasonography",Article,"Final","",Scopus,2-s2.0-85108896359
"Antonioli L., Pella A., Ricotti R., Rossi M., Fiore M.R., Belotti G., Magro G., Paganelli C., Orlandi E., Ciocca M., Baroni G.","57224921060;35776741700;56604926100;7403708768;16230235200;57214232847;56583828100;55516696500;7005698929;7003398216;7006568878;","Convolutional neural networks cascade for automatic pupil and iris detection in ocular proton therapy",2021,"Sensors","21","13","4400","","",,1,"10.3390/s21134400","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108667757&doi=10.3390%2fs21134400&partnerID=40&md5=3d0df6c7b8a3423cf7ec3047ff134ed3","Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy; Department of Electronics, Information and Bioengineering, Politecnico di Milano University, Milan, 20133, Italy; Radiotherapy Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy; Medical Physics Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy","Antonioli, L., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy; Pella, A., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy; Ricotti, R., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy; Rossi, M., Department of Electronics, Information and Bioengineering, Politecnico di Milano University, Milan, 20133, Italy; Fiore, M.R., Radiotherapy Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy; Belotti, G., Department of Electronics, Information and Bioengineering, Politecnico di Milano University, Milan, 20133, Italy; Magro, G., Medical Physics Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy; Paganelli, C., Department of Electronics, Information and Bioengineering, Politecnico di Milano University, Milan, 20133, Italy; Orlandi, E., Radiotherapy Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy; Ciocca, M., Medical Physics Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy; Baroni, G., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, 27100, Italy, Department of Electronics, Information and Bioengineering, Politecnico di Milano University, Milan, 20133, Italy","Eye tracking techniques based on deep learning are rapidly spreading in a wide variety of application fields. With this study, we want to exploit the potentiality of eye tracking techniques in ocular proton therapy (OPT) applications. We implemented a fully automatic approach based on two-stage convolutional neural networks (CNNs): the first stage roughly identifies the eye position and the second one performs a fine iris and pupil detection. We selected 707 video frames recorded during clinical operations during OPT treatments performed at our institute. 650 frames were used for training and 57 for a blind test. The estimations of iris and pupil were evaluated against the manual labelled contours delineated by a clinical operator. For iris and pupil predictions, Dice coefficient (median = 0.94 and 0.97), Szymkiewicz–Simpson coefficient (median = 0.97 and 0.98), Intersection over Union coefficient (median = 0.88 and 0.94) and Hausdorff distance (median = 11.6 and 5.0 (pixels)) were quantified. Iris and pupil regions were found to be comparable to the manually labelled ground truths. Our proposed framework could provide an automatic approach to quantitatively evaluating pupil and iris misalignments, and it could be used as an additional support tool for clinical activity, without impacting in any way with the consolidated routine. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks; Eye tracking; Iris segmentation; Ocular proton therapy; Pupil segmentation","Convolution; Deep learning; Eye tracking; Proton beam therapy; Proton beams; Application fields; Automatic approaches; Clinical operations; Dice coefficient; Hausdorff distance; Iris detection; Proton therapy; Pupil detection; Convolutional neural networks; image processing; iris; proton therapy; pupil; Image Processing, Computer-Assisted; Iris; Neural Networks, Computer; Proton Therapy; Pupil",Article,"Final","",Scopus,2-s2.0-85108667757
"Zhang Z., Lian D., Gao S.","57204289144;57203743979;35224747100;","RGB-D-based gaze point estimation via multi-column CNNs and facial landmarks global optimization",2021,"Visual Computer","37","7",,"1731","1741",,1,"10.1007/s00371-020-01934-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094638954&doi=10.1007%2fs00371-020-01934-1&partnerID=40&md5=f2e0a53b1346100007c9a82f6c40a3f1","ShanghaiTech University, Shanghai, China; Shanghai Institute of Microsystem and Information Technology, University of Chinese Academy of Sciences, Shanghai, China","Zhang, Z., ShanghaiTech University, Shanghai, China, Shanghai Institute of Microsystem and Information Technology, University of Chinese Academy of Sciences, Shanghai, China; Lian, D., ShanghaiTech University, Shanghai, China, Shanghai Institute of Microsystem and Information Technology, University of Chinese Academy of Sciences, Shanghai, China; Gao, S., ShanghaiTech University, Shanghai, China, Shanghai Institute of Microsystem and Information Technology, University of Chinese Academy of Sciences, Shanghai, China","In this work, we utilize a multi-column CNNs framework to estimate the gaze point of a person sitting in front of a display from an RGB-D image of the person. Given that gaze points are determined by head poses, eyeball poses, and 3D eye positions, we propose to infer the three components separately and then integrate them for gaze point estimation. The captured depth images, however, usually contain noises and black holes which prevent us from acquiring reliable head pose and 3D eye position estimation. Therefore, we propose to refine the raw depth for 68 facial keypoints by first estimating their relative depths from RGB face images, which along with the captured raw depths are then used to solve the absolute depth for all facial keypoints through global optimization. The refined depths will provide us reliable estimation for both head pose and 3D eye position. Given that existing publicly available RGB-D gaze tracking datasets are small, we also build a new dataset for training and validating our method. To the best of our knowledge, it is the largest RGB-D gaze tracking dataset in terms of the number of participants. Comprehensive experiments demonstrate that our method outperforms existing methods by a large margin on both our dataset and the Eyediap dataset. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Gaze tracking; Human–computer interaction; Multi-column CNNs","Global optimization; Large dataset; Depth image; Eye position; Face images; Facial landmark; Gaze point estimations; Gaze tracking; Large margins; Three component; Eye tracking",Article,"Final","",Scopus,2-s2.0-85094638954
"Oki T., Kizawa S.","56970664800;57274824000;","Evaluating visual impressions based on gaze analysis and deep learning: A case study of attractiveness evaluation of streets in densely built-up wooden residential area",2021,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2021",,"887","894",,,"10.5194/isprs-archives-XLIII-B3-2021-887-2021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115828084&doi=10.5194%2fisprs-archives-XLIII-B3-2021-887-2021&partnerID=40&md5=0da265ee673419ec59e854872ed2ec7c","Tokyo Institute of Technology, 2-12-1-M1-27 Ookayama, Meguro-ku Tokyo, 152-8550, Japan","Oki, T., Tokyo Institute of Technology, 2-12-1-M1-27 Ookayama, Meguro-ku Tokyo, 152-8550, Japan; Kizawa, S., Tokyo Institute of Technology, 2-12-1-M1-27 Ookayama, Meguro-ku Tokyo, 152-8550, Japan","This paper examines the possibility of impression evaluation based on gaze analysis of subjects and deep learning, using an example of evaluating street attractiveness in densely built-up wooden residential areas. Firstly, the relationship between the subjects' gazing tendency and their evaluation of street image attractiveness is analysed by measuring the subjects' gaze with an eye tracker. Next, we construct a model that can estimate an attractiveness evaluation result using convolutional neural networks (CNNs), combined with the method of gradient-weighted class activation mapping (Grad-CAM) - these in in visualizing which street components can contribute to evaluating attractiveness. Finally, we discuss the similarity between the subjects' gaze tendencies and activation heatmaps created by Grad-CAM. © 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.","Attractiveness; Convolutional Neural Network; Densely Built-up Wooden Residential Area; Gaze Analysis; Google Street View; Grad-CAM; Questionnaire; Semantic Segmentation","Cams; Chemical activation; Convolution; Convolutional neural networks; Deep learning; Eye tracking; Housing; Semantics; Activation mapping; Attractiveness; Convolutional neural network; Densely build-up wooden residential area; Gaze analysis; Google street view; Google+; Gradient-weighted class activation mapping; Questionnaire; Residential areas; Semantic segmentation; Semantic Segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85115828084
"Bernard V., Wannous H., Vandeborre J.-P.","57246531000;23391125300;6507497277;","Eye-Gaze Estimation using a Deep Capsule-based Regression Network",2021,"Proceedings - International Workshop on Content-Based Multimedia Indexing","2021-June",,"9461895","","",,,"10.1109/CBMI50038.2021.9461895","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114272153&doi=10.1109%2fCBMI50038.2021.9461895&partnerID=40&md5=942efad11eb6d145fb374314c2638e34","Univ. Lille, Cnrs Umr 9189 - CRIStAL, Imt Lille Douai, Lille, F-59000, France","Bernard, V., Univ. Lille, Cnrs Umr 9189 - CRIStAL, Imt Lille Douai, Lille, F-59000, France; Wannous, H., Univ. Lille, Cnrs Umr 9189 - CRIStAL, Imt Lille Douai, Lille, F-59000, France; Vandeborre, J.-P., Univ. Lille, Cnrs Umr 9189 - CRIStAL, Imt Lille Douai, Lille, F-59000, France","Eye-gaze information is used in a variety of user platforms, such as driver monitoring systems and head-mounted interfaces. In order to estimate human eye-gaze, many solutions have been proposed, using different devices and techniques. However, achieving such estimation using only cheap devices like RGB cameras would enable gaze interactions on mobile devices and therefore generalise this kind of interaction. It could also enable behavior studies based on gaze and made on every day devices. We propose in this paper a new method for eye-gaze estimation using a new deep learning architecture based on the Capsule Neural Network. Capsule Networks have shown great results so far on classification tasks, but only a few works use them for regression tasks.By taking advantage of the Capsule Network architecture and its ability to reconstruct images, we are able to recreate simplified eye images and then estimate human gaze from them. Experiments are performed on two representative datasets for the task of eye-gaze estimation. Encouraging results are obtained for both the estimation and the reconstruction. © 2021 IEEE.",,"Deep learning; Indexing (of information); Behavior studies; Classification tasks; Driver monitoring system; Eye images; Gaze interaction; Human eye; Learning architectures; RGB cameras; Network architecture",Conference Paper,"Final","",Scopus,2-s2.0-85114272153
"Hou Z., Chao X., Liang J., Yang T.","14029905400;57218488039;55892326800;57218488919;","A Multi-Modal Gaze Tracking Algorithm",2021,"Journal of Circuits, Systems and Computers","30","7","2150126","","",,,"10.1142/S0218126621501267","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095980201&doi=10.1142%2fS0218126621501267&partnerID=40&md5=f297c3eb5253a6a8eb584867bf7267bc","Hangzhou College of Commerce, Zhejiang Gongshang University, Hangzhou, China; College of Information Science and Engineering, Changzhou University, Changzhou, 213164, China","Hou, Z., Hangzhou College of Commerce, Zhejiang Gongshang University, Hangzhou, China; Chao, X., College of Information Science and Engineering, Changzhou University, Changzhou, 213164, China; Liang, J., College of Information Science and Engineering, Changzhou University, Changzhou, 213164, China; Yang, T., College of Information Science and Engineering, Changzhou University, Changzhou, 213164, China","A person's emotional information, needs and cognitive processes can be described by eye movement states and concerns, so gaze tracking was first applied in the field of psychology. With the continuous development of information technology, the application range of gaze tracking has expanded from psychology to medical, military, commercial and many other fields. Aiming at the problem of high misjudgment rate and long time-consuming of traditional iris location methods, this paper proposes a gaze tracking method based on human eye geometric characteristics to improve the tracking accuracy in 2D environment. First, the human face is located by face location algorithm and the position of human eye is estimated roughly. Then the iris template is built by iris image, and the iris center location algorithm is used to locate the iris center position. Finally, the eyes corners and iris center points are extracted to locate the eye area accurately and obtain the binocular image. The binocular images are input into the feature extraction network as multi-modal information in parallel, and the convoluted feature channels are reconstructed using the weight redistribution module in the network. Then the reconstructed features are fused in the full connection layer. Finally, the output layer is used to classify the reconstructed features. Experiments were carried out on a self-built screen block dataset. For 12 classified data, the lowest recognition error rate is 5.34%. © 2021 World Scientific Publishing Company.","Gaze tracking; geometric characteristics; multi-modal","Binoculars; Eye movements; Image processing; Location; Application range; Cognitive process; Continuous development; Emotional information; Geometric characteristics; Multi-modal information; Recognition error; Tracking accuracy; Eye tracking",Article,"Final","",Scopus,2-s2.0-85095980201
"Barz M., Sonntag D.","57189847803;12241487800;","Automatic visual attention detection for mobile eye tracking using pre-trained computer vision models and human gaze",2021,"Sensors","21","12","4143","","",,,"10.3390/s21124143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107880702&doi=10.3390%2fs21124143&partnerID=40&md5=25527013262da014bdcb9b6f7ccc8042","Interactive Machine Learning Department, German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany; Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany","Barz, M., Interactive Machine Learning Department, German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany, Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany; Sonntag, D., Interactive Machine Learning Department, German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany, Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany","Processing visual stimuli in a scene is essential for the human brain to make situation-aware decisions. These stimuli, which are prevalent subjects of diagnostic eye tracking studies, are commonly encoded as rectangular areas of interest (AOIs) per frame. Because it is a tedious manual annotation task, the automatic detection and annotation of visual attention to AOIs can accelerate and objectify eye tracking research, in particular for mobile eye tracking with egocentric video feeds. In this work, we implement two methods to automatically detect visual attention to AOIs using pre-trained deep learning models for image classification and object detection. Furthermore, we develop an evaluation framework based on the VISUS dataset and well-known performance metrics from the field of activity recognition. We systematically evaluate our methods within this framework, discuss potentials and limitations, and propose ways to improve the performance of future automatic visual attention detection methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Area of interest; Computer vision; Eye tracking; Eye tracking data analysis; Visual attention","Behavioral research; Computer vision; Deep learning; Object detection; Petroleum reservoir evaluation; Activity recognition; Automatic Detection; Evaluation framework; Eye-tracking studies; Manual annotation; Mobile eye-tracking; Performance metrics; Processing visual stimulus; Eye tracking; computer; eye movement; human; vision; Computers; Eye Movements; Eye-Tracking Technology; Humans; Vision, Ocular",Article,"Final","",Scopus,2-s2.0-85107880702
"Hussain R., Chessa M., Solari F.","57220665010;25652939100;6603393665;","Mitigating cybersickness in virtual reality systems through foveated depth-of-field blur",2021,"Sensors","21","12","4006","","",,,"10.3390/s21124006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107500635&doi=10.3390%2fs21124006&partnerID=40&md5=003c52427b73c497708e6e06f75ff91b","Department of Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Genoa, 16126, Italy","Hussain, R., Department of Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Genoa, 16126, Italy; Chessa, M., Department of Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Genoa, 16126, Italy; Solari, F., Department of Informatics, Bioengineering, Robotics and Systems Engineering, University of Genoa, Genoa, 16126, Italy","Cybersickness is one of the major roadblocks in the widespread adoption of mixed reality devices. Prolonged exposure to these devices, especially virtual reality devices, can cause users to feel discomfort and nausea, spoiling the immersive experience. Incorporating spatial blur in stereoscopic 3D stimuli has shown to reduce cybersickness. In this paper, we develop a technique to incorporate spatial blur in VR systems inspired by the human physiological system. The technique makes use of concepts from foveated imaging and depth-of-field. The developed technique can be applied to any eye tracker equipped VR system as a post-processing step to provide an artifact-free scene. We verify the usefulness of the proposed system by conducting a user study on cybersickness evaluation. We used a custom-built rollercoaster VR environment developed in Unity and an HTC Vive Pro Eye headset to interact with the user. A Simulator Sickness Questionnaire was used to measure the induced sickness while gaze and heart rate data were recorded for quantitative analysis. The experimental analysis highlighted the aptness of our foveated depth-of-field effect in reducing cybersickness in virtual environments by reducing the sickness scores by approximately 66%. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Cycbersickness; Depth-of-field; Eye-tracker; Foveation; Gaze-contingent; Shader; Spatial blur","Bioinformatics; Diseases; Eye tracking; Stereo image processing; Depth-of-field blur; Experimental analysis; Foveated imaging; Physiological systems; Post processing; Simulator sickness; Virtual reality devices; Virtual reality system; Mixed reality; computer interface; emotion; human; motion sickness; questionnaire; virtual reality; Emotions; Humans; Motion Sickness; Surveys and Questionnaires; User-Computer Interface; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85107500635
"Murthy L.R.D., Biswas P.","57205505055;14007579800;","Appearance-based gaze estimation using attention and difference mechanism",2021,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,,,"3137","3146",,,"10.1109/CVPRW53098.2021.00351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116069594&doi=10.1109%2fCVPRW53098.2021.00351&partnerID=40&md5=7eae6a6cbeba8f98d12c54f82a5d8a04","I3D Lab, CPDM, Indian Institute of Science, Bangalore, India","Murthy, L.R.D., I3D Lab, CPDM, Indian Institute of Science, Bangalore, India; Biswas, P., I3D Lab, CPDM, Indian Institute of Science, Bangalore, India","Appearance-based gaze estimation problem received wide attention over the past few years. Even though model-based approaches existed earlier, availability of large datasets and novel deep learning techniques made appearance-based methods achieve superior accuracy than model-based approaches. In this paper, we proposed two novel techniques to improve gaze estimation accuracy. Our first approach, I2D-Net uses a difference layer to eliminate any common features from left and right eyes of a participant that are not pertinent to gaze estimation task. Our second approach, AGE-Net adapted the idea of attention-mechanism and assigns weights to the features extracted from eye images. I2D-Net performed on par with the existing state-of-the-art approaches while AGE-Net reported state-of-the-art accuracy of 4.09 and 7.44 error on MPI-IGaze and RT-Gene datasets respectively. We performed ablation studies to understand the effectiveness of the proposed approaches followed by analysis of gaze error distribution with respect to various factors of MPIIGaze dataset. © 2021 IEEE.",,"Computer vision; Deep learning; Appearance based; Appearance-based methods; Attention mechanisms; Common features; Estimation problem; Gaze estimation; Large datasets; Learning techniques; Model based approach; Novel techniques; Large dataset",Conference Paper,"Final","",Scopus,2-s2.0-85116069594
"Cardoso T.V., Michelassi G.C., Franco F.O., Sumiya F.M., Portolese J., Brentani H., Machado-Lima A., Nunes F.L.S.","57226185935;57226174744;57226197184;57226191661;55955051000;57208765936;57195959173;57226183888;","Autism spectrum disorder diagnosis based on trajectories of eye tracking data",2021,"Proceedings - IEEE Symposium on Computer-Based Medical Systems","2021-June",,"9474680","50","55",,1,"10.1109/CBMS52027.2021.00016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110930990&doi=10.1109%2fCBMS52027.2021.00016&partnerID=40&md5=98ecba168899bbd4a7d44b952fc1a94a","School of Arts, Sciences and Humanities (EACH), University of Sao Paulo (USP), SP, Sao Paulo, Brazil; Institute of Mathematics and Statistics (IME), (USP), Interunit PostGraduate Program on Bioinformatics, Brazil; University of Sao Paulo's, School of Medicine (FMUSP), Department of Psychiatry, SP, Sao Paulo, Brazil","Cardoso, T.V., School of Arts, Sciences and Humanities (EACH), University of Sao Paulo (USP), SP, Sao Paulo, Brazil; Michelassi, G.C., School of Arts, Sciences and Humanities (EACH), University of Sao Paulo (USP), SP, Sao Paulo, Brazil; Franco, F.O., Institute of Mathematics and Statistics (IME), (USP), Interunit PostGraduate Program on Bioinformatics, Brazil; Sumiya, F.M., University of Sao Paulo's, School of Medicine (FMUSP), Department of Psychiatry, SP, Sao Paulo, Brazil; Portolese, J., University of Sao Paulo's, School of Medicine (FMUSP), Department of Psychiatry, SP, Sao Paulo, Brazil; Brentani, H., University of Sao Paulo's, School of Medicine (FMUSP), Department of Psychiatry, SP, Sao Paulo, Brazil; Machado-Lima, A., School of Arts, Sciences and Humanities (EACH), University of Sao Paulo (USP), SP, Sao Paulo, Brazil; Nunes, F.L.S., School of Arts, Sciences and Humanities (EACH), University of Sao Paulo (USP), SP, Sao Paulo, Brazil","The use of Eye Tracking (ET) has been investigated as an auxiliary mechanism to diagnose Autism Spectrum Disorder (ASD). One of the paradigms investigated using ET is Joint Attention (JA), which refers to moments when two individuals are focused on the same object/event so that both are aware that the focus of attention is shared. The computational tools that assist in the diagnosis of ASD have used Image Processing and Machine Learning techniques to process images, videos and ET signals. However, the JA paradigm is still little explored and presents challenges, as it requires analyzing the gaze trajectory and needs innovative approaches. The purpose of this article is to propose a model capable of extracting features from a video used as a stimulus to capture ET signals in order to verify JA and classify individuals as belonging to the ASD or Typical Development (TD) group. The main differential in relation to the approaches in the literature is the definition and implementation of the concept of floating Regions of Interest, which allows monitoring the gaze in relation to an object, considering its semantics, even if the object presents different characteristics throughout the video. A model based on ensembles of Random Forest classifiers was implemented to classify individuals as ASD or TD using the trajectory features extracted from the ET signals. The method reached 0.75 accuracy and 0.82 F1-score, indicating that the proposed approach, based on trajectory and JA, has the potential to be applied to assist in the diagnosis of ASD. © 2021 IEEE.","Autism Spectrum Disorder; Ensemble Method; Eye tracking; Image processing; Joint attention; Machine Learning; Random Forest; Trajectory; Typical Development","Biomedical signal processing; Decision trees; Diagnosis; Diseases; Image processing; Learning systems; Semantics; Trajectories; Autism spectrum disorders; Computational tools; Extracting features; Innovative approaches; Machine learning techniques; Random forest classifier; Trajectory features; Typical development; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85110930990
"Revers M.C., Oliveira J.S., Franco F.O., Portolese J., Cardoso T.V., Silva A.F., Machado-Lima A., Nunes F.L.S., Brentani H.","57223405138;57223402683;57226197184;55955051000;57226185935;57223396397;57195959173;57226183888;57208765936;","Classification of autism spectrum disorder severity using eye tracking data based on visual attention model",2021,"Proceedings - IEEE Symposium on Computer-Based Medical Systems","2021-June",,"9474657","142","147",,,"10.1109/CBMS52027.2021.00062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110909019&doi=10.1109%2fCBMS52027.2021.00062&partnerID=40&md5=e7b5f2590217a57704a8c529f9f7ca60","University of Sao Paulo's School of Medicine, Department of Psychiatry, Sao Paulo, Brazil; School of Arts, Sciences and Humanities, University of Sao Paulo, Sao Paulo, Brazil; Interunit PostGraduate Program on Bioinformatics, Institute of Mathematics and Statistics, University of Sao Paulo, Sao Paulo, Brazil","Revers, M.C., University of Sao Paulo's School of Medicine, Department of Psychiatry, Sao Paulo, Brazil; Oliveira, J.S., School of Arts, Sciences and Humanities, University of Sao Paulo, Sao Paulo, Brazil; Franco, F.O., Interunit PostGraduate Program on Bioinformatics, Institute of Mathematics and Statistics, University of Sao Paulo, Sao Paulo, Brazil; Portolese, J., University of Sao Paulo's School of Medicine, Department of Psychiatry, Sao Paulo, Brazil; Cardoso, T.V., School of Arts, Sciences and Humanities, University of Sao Paulo, Sao Paulo, Brazil; Silva, A.F., University of Sao Paulo's School of Medicine, Department of Psychiatry, Sao Paulo, Brazil; Machado-Lima, A., School of Arts, Sciences and Humanities, University of Sao Paulo, Sao Paulo, Brazil; Nunes, F.L.S., School of Arts, Sciences and Humanities, University of Sao Paulo, Sao Paulo, Brazil; Brentani, H., University of Sao Paulo's School of Medicine, Department of Psychiatry, Sao Paulo, Brazil","Computer-aided diagnosis using eye tracking data is classically based on regions of interest in the image. However, in recent years, the modeling of visual attention by saliency maps has shown better results. Wang et al., considering 3-layered saliency model that incorporated pixel-level, object-level, and semantic-level attributes, showed differences in the performance of eye tracking in autism spectrum disorder (ASD) and better characterized these differences by looking at which attributes were used, providing meaningful clinical results about the disorder. Our hypothesis is that the context interpretation would be worse according to the severity of ASD, consequently, the eye tracking data processed based on visual attention model (VAM) could be used to classify patients with ASD according to gravity. In this context, the present work proposes: 1) based on VAM, using Image Processing and Artificial Intelligence to learn a model for each group (severe and non-severe), from eye tracking data, and 2) a supervised classifier that, based on the models learned, performs the severity diagnosis. The classifier using the saliency maps was able to identify and separate the groups with an average accuracy of 88%. The most important features were the presence of face and skin color, in other words, semantic features. © 2021 IEEE.","Autism Spectrum Disorder; Classifier; Eye Tracking; Severity Level; Supervised Machine Learning","Artificial intelligence; Behavioral research; Computer aided diagnosis; Diseases; Image processing; Object tracking; Semantics; Autism spectrum disorders; Important features; Regions of interest; Saliency modeling; Semantic features; Supervised classifiers; Visual Attention; Visual attention model; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85110909019
"Gatoula P., Dimas G., Iakovidis D.K., Koulaouzidis A.","57226190947;57195485922;6603967427;14627591700;","Enhanced CNN-Based gaze estimation on wireless capsule endoscopy images",2021,"Proceedings - IEEE Symposium on Computer-Based Medical Systems","2021-June",,"9474669","189","195",,,"10.1109/CBMS52027.2021.00070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110907745&doi=10.1109%2fCBMS52027.2021.00070&partnerID=40&md5=a6a4c9abd023a734c508eb1778260169","University of Thessaly, Dept. of Computer Science and Biomedical Informatics, Lamia, Greece; Pomeranian Medical University, Dept. of Social Medicine and Public Health, Szczecin, Poland","Gatoula, P., University of Thessaly, Dept. of Computer Science and Biomedical Informatics, Lamia, Greece; Dimas, G., University of Thessaly, Dept. of Computer Science and Biomedical Informatics, Lamia, Greece; Iakovidis, D.K., University of Thessaly, Dept. of Computer Science and Biomedical Informatics, Lamia, Greece; Koulaouzidis, A., Pomeranian Medical University, Dept. of Social Medicine and Public Health, Szczecin, Poland","Wireless capsule endoscopy (WCE) is a modality used for the non-invasive examination of the gastrointestinal (GI) tract. Physicians diagnose pathologies in images derived from Capsule Endoscopy (CE) using specific gaze patterns to observe pathologically related visual cues. Lately, deep learning has advanced in the domain of human eye-fixation estimation in natural images. However, the potentials of predicting the eye related patterns, such as eye fixations, in medical images has not been thoroughly investigated. In this work, we propose a CNN auto-encoder model, that is capable of predicting saliency maps estimating the gaze-patterns, in terms of eye-fixations, of physicians in CE images. The proposed model outperforms other approaches for visual saliency estimation based on physicians' eye fixation by providing an AUC-J of 0.726 among CE images depicting various pathological and normal cases. © 2021 IEEE.","Deep Learning; Visual Saliency; Wireless Capsule Endoscopy","Deep learning; Endoscopy; Medical imaging; Capsule endoscopy; Eye fixations; Gastrointestinal tract; Gaze estimation; Natural images; Visual saliency; Wireless capsule endoscopy; Wireless capsule endoscopy image; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85110907745
"Planke L.J., Gardi A., Sabatini R., Kistan T., Ezer N.","57210715341;57198865986;56962744800;55763620100;24331325800;","Online multimodal inference of mental workload for cognitive human machine systems",2021,"Computers","10","6","81","","",,,"10.3390/computers10060081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108894248&doi=10.3390%2fcomputers10060081&partnerID=40&md5=7e7cf793e6ef439ab83874913bf521e9","School of Engineering, RMIT University, Bundoora, VIC  3038, Australia; THALES Australia—Airspace Mobility Solutions, WTC North Wharf, Melbourne, VIC  3000, Australia; Northrop Grumman Corporation, 1550 W. Nursery Rd, Linthicum HeightsMD  21090, United States","Planke, L.J., School of Engineering, RMIT University, Bundoora, VIC  3038, Australia; Gardi, A., School of Engineering, RMIT University, Bundoora, VIC  3038, Australia; Sabatini, R., School of Engineering, RMIT University, Bundoora, VIC  3038, Australia; Kistan, T., THALES Australia—Airspace Mobility Solutions, WTC North Wharf, Melbourne, VIC  3000, Australia; Ezer, N., Northrop Grumman Corporation, 1550 W. Nursery Rd, Linthicum HeightsMD  21090, United States","With increasingly higher levels of automation in aerospace decision support systems, it is imperative that the human operator maintains a high level of situational awareness in different operational conditions and a central role in the decision-making process. While current aerospace systems and interfaces are limited in their adaptability, a Cognitive Human Machine System (CHMS) aims to perform dynamic, real-time system adaptation by estimating the cognitive states of the human operator. Nevertheless, to reliably drive system adaptation of current and emerging aerospace systems, there is a need to accurately and repeatably estimate cognitive states, particularly for Mental Workload (MWL), in real-time. As part of this study, two sessions were performed during a Multi-Attribute Task Battery (MATB) scenario, including a session for offline calibration and validation and a session for online validation of eleven multimodal inference models of MWL. The multimodal inference model implemented included an Adaptive Neuro Fuzzy Inference System (ANFIS), which was used in different configurations to fuse data from an Electroencephalogram (EEG) model’s output, four eye activity features and a control input feature. The results from the online validation of the ANFIS models demonstrated that five of the ANFIS models (containing different feature combinations of eye activity and control input features) all demonstrated good results, while the best performing model (containing all four eye activity features and the control input feature) showed an average Mean Absolute Error (MAE) = 0.67 ± 0.18 and Correlation Coefficient (CC) = 0.71 ± 0.15. The remaining six ANFIS models included data from the EEG model’s output, which had an offset discrepancy. This resulted in an equivalent offset for the online multimodal fusion. Nonetheless, the efficacy of these ANFIS models could be seen with the pairwise correlation with the task level, where one model demonstrated a CC = 0.77 ± 0.06, which was the highest among all the ANFIS models tested. Hence, this study demonstrates the ability for online multimodal fusion from features extracted from EEG signals, eye activity and control inputs to produce an accurate and repeatable inference of MWL. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Adaptive automation; ANFIS; Closed loop system adaptation; Control inputs; EEG; Eye tracking; Machine learning; Mental workload; Multimodal data fusion",,Article,"Final","",Scopus,2-s2.0-85108894248
"Gu S., Wang L., He L., He X., Wang J.","36449028500;55080150600;57224909652;57188805587;57224988983;","Gaze Estimation via a Differential Eyes’ Appearances Network with a Reference Grid",2021,"Engineering","7","6",,"777","786",,1,"10.1016/j.eng.2020.08.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108090130&doi=10.1016%2fj.eng.2020.08.027&partnerID=40&md5=7a0c4690d5036f94a45536d6b2703569","Chengdu Aeronautic Polytechnic, Chengdu, 610100, China; Department of Production Engineering, KTH Royal Institute of Technology, Stockholm, 10044, Sweden","Gu, S., Chengdu Aeronautic Polytechnic, Chengdu, 610100, China; Wang, L., Department of Production Engineering, KTH Royal Institute of Technology, Stockholm, 10044, Sweden; He, L., Chengdu Aeronautic Polytechnic, Chengdu, 610100, China; He, X., Chengdu Aeronautic Polytechnic, Chengdu, 610100, China; Wang, J., Chengdu Aeronautic Polytechnic, Chengdu, 610100, China","A person's eye gaze can effectively express that person's intentions. Thus, gaze estimation is an important approach in intelligent manufacturing to analyze a person's intentions. Many gaze estimation methods regress the direction of the gaze by analyzing images of the eyes, also known as eye patches. However, it is very difficult to construct a person-independent model that can estimate an accurate gaze direction for every person due to individual differences. In this paper, we hypothesize that the difference in the appearance of each of a person's eyes is related to the difference in the corresponding gaze directions. Based on this hypothesis, a differential eyes’ appearances network (DEANet) is trained on public datasets to predict the gaze differences of pairwise eye patches belonging to the same individual. Our proposed DEANet is based on a Siamese neural network (SNNet) framework which has two identical branches. A multi-stream architecture is fed into each branch of the SNNet. Both branches of the DEANet that share the same weights extract the features of the patches; then the features are concatenated to obtain the difference of the gaze directions. Once the differential gaze model is trained, a new person's gaze direction can be estimated when a few calibrated eye patches for that person are provided. Because person-specific calibrated eye patches are involved in the testing stage, the estimation accuracy is improved. Furthermore, the problem of requiring a large amount of data when training a person-specific model is effectively avoided. A reference grid strategy is also proposed in order to select a few references as some of the DEANet's inputs directly based on the estimation values, further thereby improving the estimation accuracy. Experiments on public datasets show that our proposed approach outperforms the state-of-the-art methods. © 2021 THE AUTHORS","Cross-person evaluations; Differential gaze; Gaze estimation; Human–robot collaboration; Siamese neural network","Industrial engineering; Gaze direction; Gaze estimation; Individual Differences; Intelligent Manufacturing; Multi-stream architecture; Person-independent; Reference grids; State-of-the-art methods; Engineering",Article,"Final","",Scopus,2-s2.0-85108090130
"Feng S., Law N.","57190013027;7005934146;","Mapping Artificial Intelligence in Education Research: a Network‐based Keyword Analysis",2021,"International Journal of Artificial Intelligence in Education","31","2",,"277","303",,1,"10.1007/s40593-021-00244-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102823740&doi=10.1007%2fs40593-021-00244-4&partnerID=40&md5=d87701123ed020cf0fe8950ae02bc588","Unit of Human Communication, Development, and Information Sciences, Faculty of Education, The University of Hong Kong, Hong Kong; Unit of Teacher Education and Learning Leadership, Faculty of Education, The University of Hong Kong, Hong Kong","Feng, S., Unit of Human Communication, Development, and Information Sciences, Faculty of Education, The University of Hong Kong, Hong Kong; Law, N., Unit of Teacher Education and Learning Leadership, Faculty of Education, The University of Hong Kong, Hong Kong","In this study, we review 1830 research articles on artificial intelligence in education (AIED), with the aim of providing a holistic picture of the knowledge evolution in this interdisciplinary research field from 2010 to 2019. A novel three-step approach in the analysis of the keyword co-occurrence networks (KCN) is proposed to identify the knowledge structure, knowledge clusters and trending keywords within AIED over time. The results reveal considerable research diversity in the AIED field, centering around two sustained themes: intelligent tutoring systems (2010-19) and massive open online courses (since 2014). The focal educational concerns reflected in AIED research are: (1) online learning; (2) game-based learning; (3) collaborative learning; (4) assessment; (5) affect; (6) engagement; and (7) learning design. The highly connected keywords relevant to analytic techniques within this field include natural language processing, educational data mining, learning analytics and machine learning. Neural network, deep learning, eye tracking, and personalized learning are trending keywords in this field as they have emerged with key structural roles in the latest two-year period analyzed. This is the first article providing a systematic review of a large body of literature on artificial intelligence in education, and in it we uncover the underlying patterns of knowledge connectivity within the field, as well as provide insight into its future development. The three-step multi-scale (macro, meso, micro) framework proposed in this study can also be applied to map the knowledge development in other scientific research areas. © 2021, International Artificial Intelligence in Education Society.","Artificial intelligence in education; Keyword co‐occurrence network; Knowledge mapping; Network analysis; Systematic review","Computer aided instruction; Data handling; Data mining; Deep learning; Electronic assessment; Eye tracking; Natural language processing systems; Online systems; Artificial intelligence in education; Co-occurrence networks; Collaborative learning; Educational data mining; Intelligent tutoring system; Interdisciplinary research; Massive open online course; NAtural language processing; E-learning",Article,"Final","",Scopus,2-s2.0-85102823740
"Volonte M., Anaraky R.G., Venkatakrishnan R., Venkatakrishnan R., Knijnenburg B.P., Duchowski A.T., Babu S.V.","57203974310;57211297282;57210916095;57210916094;35225065100;6701824388;9039004700;","Empirical evaluation and pathway modeling of visual attention to virtual humans in an appearance fidelity continuum",2021,"Journal on Multimodal User Interfaces","15","2",,"109","119",,,"10.1007/s12193-020-00341-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089099124&doi=10.1007%2fs12193-020-00341-z&partnerID=40&md5=cd79b35f8452770535492ba0a517ddf4","Clemson University, Clemson, SC, United States","Volonte, M., Clemson University, Clemson, SC, United States; Anaraky, R.G., Clemson University, Clemson, SC, United States; Venkatakrishnan, R., Clemson University, Clemson, SC, United States; Venkatakrishnan, R., Clemson University, Clemson, SC, United States; Knijnenburg, B.P., Clemson University, Clemson, SC, United States; Duchowski, A.T., Clemson University, Clemson, SC, United States; Babu, S.V., Clemson University, Clemson, SC, United States","In this contribution we studied how different rendering styles of a virtual human impacted users’ visual attention in an interactive medical training simulator. In a mixed design experiment, 78 participants interacted with a virtual human representing a sample from the non-photorealistic (NPR) to the photorealistic (PR) rendering continuity. We presented five rendering style samples scenarios, namely All Pencil Shaded (APS), Pencil Shaded (PS), All Cartoon Shaded (ACT), Cartoon Shaded (CT), and Human-Like (HL), and compared how visual attention differed between groups of users. For this study, we employed an eye tracking system for collecting and analyzing users’ gaze during interaction with the virtual human in a failure to rescue medical training simulation. Results shows that users spent more total time in the APS and ACT conditions but users visually attended more to virtual humans in the PS, CT and HL appearance conditions. © 2020, Springer Nature Switzerland AG.","Eye tracking; Human computer interaction; Rendering style; Virtual Agents; Virtual humans; Virtual training simulator","Eye tracking; Design experiments; Empirical evaluations; Eye tracking systems; Medical training; Medical training simulator; Photo-realistic; Virtual humans; Visual Attention; Behavioral research",Article,"Final","",Scopus,2-s2.0-85089099124
"Gowroju S., Aarti, Kumar S.","57221499478;55904388800;57234304400;","Robust Pupil Segmentation using UNET and Morphological Image Processing",2021,"2021 International Mobile, Intelligent, and Ubiquitous Computing Conference, MIUCC 2021",,,"9447658","105","109",,,"10.1109/MIUCC52538.2021.9447658","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111361906&doi=10.1109%2fMIUCC52538.2021.9447658&partnerID=40&md5=4a95173b023de7b370926a566550ad58","Lovely Professional University, Computer Science Engineering, Punjab, India; Sreyas Institute of Engineering and Technology, Electronics Communication Engineering, Hyderabad, India","Gowroju, S., Lovely Professional University, Computer Science Engineering, Punjab, India; Aarti, Lovely Professional University, Computer Science Engineering, Punjab, India; Kumar, S., Sreyas Institute of Engineering and Technology, Electronics Communication Engineering, Hyderabad, India","The current development in image processing towards biometrics systems has opened much research on realtime applications. The deep learning algorithms are added many expectations to the researchers. The main challenges of these applications are vulnerability towards training time, detection accuracy, and accurate segmentation. In addition to this, the visual noise among various biometric systems is the main challenge. In this paper, we deployed the CNN model using modified UNet to perform the segmentation. The proposed method uses noisy images from the MMU (Multi Media University Iris database) dataset. The acquired colored eye images from the dataset exhibit specular reflections, eye gaze, off-angle images with less resolution, and occlusions caused by eyelids and eyelashes. The focus of our work is mainly to perform accurate segmentation in less training time. Compared the existing methods that uses UNet architecture, with the proposed method, we achieved an accuracy of 91.7%. © 2021 IEEE.","Accuracy; Morphological Processing; Pupil Segmentation; UNET","Biometrics; Deep learning; Learning algorithms; Physical addresses; Ubiquitous computing; Biometric systems; Detection accuracy; Morphological image processing; Pupil segmentation; Real-time application; Specular reflections; Training time; Visual noise; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85111361906
"Solbach M.D., Tsotsos J.K.","57105420400;7006624039;","Tracking Active Observers in 3D Visuo-Cognitive Tasks",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169260",,"11","","",,,"10.1145/3450341.3458496","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108019346&doi=10.1145%2f3450341.3458496&partnerID=40&md5=0bd7ba6562d2b080286264182ccc8153","Department of Electrical Engineering and Computer Science, York University, Canada","Solbach, M.D., Department of Electrical Engineering and Computer Science, York University, Canada; Tsotsos, J.K., Department of Electrical Engineering and Computer Science, York University, Canada","Most past and present research in computer vision involves passively observed data. Humans, however, are active observers in real life; they explore, search, select what and how to look. In this work, we present a psychophysical experimental setup for active, visual observation in a 3D world dubbed PESAO. The goal was to design PESAO for various active perception tasks with human subjects (active observers) capable of tracking the head and gaze. © 2021 Owner/Author.","active observer; active vision; eye tracking; head tracking; motion tracking","Active observer; Active perceptions; Cognitive task; Human subjects; Observed data; Past and present; Psychophysical; Visual observations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85108019346
"L. Miller H., Raphael Zurutuza I., Fears N., Polat S., Nielsen R.","57224585860;57224584751;57200944008;57197836242;23095747900;","Post-processing integration and semi-Automated analysis of eye-Tracking and motion-capture data obtained in immersive virtual reality environments to measure visuomotor integration",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169260",,"17","","",,,"10.1145/3450341.3458881","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108001094&doi=10.1145%2f3450341.3458881&partnerID=40&md5=d47ecf310b6435417954276b719c5efc","School of Kinesiology University of Michigan, United States; Biomedical Engineering University of North Texas, United States; University of North Texas, United States; University of North Texas, United States","L. Miller, H., School of Kinesiology University of Michigan, United States; Raphael Zurutuza, I., Biomedical Engineering University of North Texas, United States; Fears, N., School of Kinesiology University of Michigan, United States; Polat, S., University of North Texas, United States; Nielsen, R., University of North Texas, United States","Mobile eye-Tracking and motion-capture techniques yield rich, precisely quantifiable data that can inform our understanding of the relationship between visual and motor processes during task performance. However, these systems are rarely used in combination, in part because of the significant time and human resources required for post-processing and analysis. Recent advances in computer vision have opened the door for more efficient processing and analysis solutions. We developed a post-processing pipeline to integrate mobile eye-Tracking and full-body motion-capture data. These systems were used simultaneously to measure visuomotor integration in an immersive virtual environment. Our approach enables calculation of a 3D gaze vector that can be mapped to the participant's body position and objects in the virtual environment using a uniform coordinate system. This approach is generalizable to other configurations, and enables more efficient analysis of eye, head, and body movements together during visuomotor tasks administered in controlled, repeatable environments. © 2021 ACM.","computer vision; kinematic; machine learning; mobile eye tracking; motion capture; object detection; oculomotor; visuomotor integration","Eye movements; Integration; Motion capture; Motion tracking; Pipeline processing systems; Virtual reality; Analysis solution; Co-ordinate system; Efficient analysis; Immersive virtual environments; Immersive virtual reality; Mobile eye-tracking; Motion capture data; Semi-automated analysis; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85108001094
"Mulvey F.B., Mikitovic M., Sadowski M., Hou B., Rasamoel N.D., Paulin Hansen J.P., Bækgaard P.","53264358800;57224584928;57224561042;57224560067;57224581233;55465667700;56377241200;","Gaze Interactive and Attention Aware Low Vision Aids as Future Smart Glasses",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169260",,"21","","",,,"10.1145/3450341.3460769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107973473&doi=10.1145%2f3450341.3460769&partnerID=40&md5=9fec83e807b966236c06f007654a529f","Cognitive Systems, Mathematics and Computer Science Denmark Technical University, Denmark; Department of Technology, Management and Economics Denmark Technical University, Denmark","Mulvey, F.B., Cognitive Systems, Mathematics and Computer Science Denmark Technical University, Denmark; Mikitovic, M., Cognitive Systems, Mathematics and Computer Science Denmark Technical University, Denmark; Sadowski, M., Cognitive Systems, Mathematics and Computer Science Denmark Technical University, Denmark; Hou, B., Cognitive Systems, Mathematics and Computer Science Denmark Technical University, Denmark; Rasamoel, N.D., Cognitive Systems, Mathematics and Computer Science Denmark Technical University, Denmark; Paulin Hansen, J.P., Cognitive Systems, Mathematics and Computer Science Denmark Technical University, Denmark; Bækgaard, P., Department of Technology, Management and Economics Denmark Technical University, Denmark","We present a working paper on integrating eye tracking with mixed and augmented reality for the benefit of low vision aids. We outline the current state of the art and relevant research and point to further research and development required in order to adapt to individual user, environment, and current task. We outline key technical challenges and possible solutions including calibration, dealing with variant eye data quality, measuring and adapting image processing to low vision within current technical limitations, and outline an experimental approach to designing data-driven solutions using machine learning and artificial intelligence. © 2021 ACM.","eye tracking; mixed reality; virtual reality; vision aids; vision loss","Artificial intelligence; Augmented reality; Image processing; Vision aids; Data quality; Experimental approaches; Mixed and augmented realities; Research and development; State of the art; Technical challenges; Technical limitations; Working papers; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107973473
"Love K., Velisar A., Shanidze N.","57224579722;51562804200;36192170100;","Eye, Robot: Calibration Challenges and Potential Solutions for Wearable Eye Tracking in Individuals with Eccentric Fixation",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169260",,"16","","",,,"10.1145/3450341.3458489","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107966856&doi=10.1145%2f3450341.3458489&partnerID=40&md5=5950ae10bfa29cca7d9fa6e4b523ff64","Harvard College Harvard University, United States; Smith-Kettlewell Eye Research Institute, United States","Love, K., Harvard College Harvard University, United States; Velisar, A., Smith-Kettlewell Eye Research Institute, United States; Shanidze, N., Smith-Kettlewell Eye Research Institute, United States","Loss of the central retina, including the fovea, can lead to a loss of visual acuity and oculomotor deficits, and thus have profound effects on day-To-day tasks. Recent advances in head-mounted, 3D eye tracking have allowed researchers to extend studies in this population to a broader set of daily tasks and more naturalistic behaviors and settings. However, decreases in fixational stability, multiple fixational loci and their uncertain role as oculomotor references, as well as eccentric fixation all provide additional challenges for calibration and collection of eye movement data. Here we quantify reductions in calibration accuracy relative to fixation eccentricity, and suggest a robotic calibration and validation tool that will allow for future developments of calibration and tracking algorithms designed with this population in mind. © 2021 ACM.","calibration; eccentric fixation; eye tracking; macular degeneration","Calibration; End effectors; Eye movements; Wearable technology; Calibration accuracy; Calibration and tracking; Calibration and validations; Daily tasks; Eye movement datum; Visual acuity; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107966856
"Gibaldi A., Dutell V., Banks M.S.","35329375200;53979584500;7102259684;","Solving Parallax Error for 3D Eye Tracking",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169260",,"8","","",,,"10.1145/3450341.3458494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107942288&doi=10.1145%2f3450341.3458494&partnerID=40&md5=d929befd0dd309a3921caa6a30a6bda5","School of Optometry UC Berkeley, United States; Vision Science Dept, Olshausen Lab UC Berkeley, United States; Optometry UC Berkeley, United States","Gibaldi, A., School of Optometry UC Berkeley, United States; Dutell, V., Vision Science Dept, Olshausen Lab UC Berkeley, United States; Banks, M.S., Optometry UC Berkeley, United States","Head-mounted eye-Trackers allow for unrestricted behavior in the natural environment, but have calibration issues that compromise accuracy and usability. A well-known problem arises from the fact that gaze measurements suffer from parallax error due to the offset between the scene camera origin and eye position. To compensate for this error two pieces of data are required: The pose of the scene camera in head coordinates, and the three-dimensional coordinates of the fixation point in head coordinates. We implemented a method that allows for effective and accurate eye-Tracking in the three-dimensional environment. Our approach consists of a calibration procedure that allows to contextually calibrate the eye-Tracker and compute the eyes pose in the reference frame of the scene camera, and a custom stereoscopic scene camera that provides the three-dimensional coordinates of the fixation point. The resulting gaze data are free from parallax error, allowing accurate and effective use of the eye-Tracker in the natural environment. © 2021 ACM.","eye tracking calibration; parallax error; wearable eye tracking","Calibration; Cameras; Errors; Geometrical optics; Stereo image processing; Calibration procedure; Eye position; Fixation point; Natural environments; Parallax error; Reference frame; Three dimensional coordinate; Three-dimensional environment; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107942288
"Stone S.A., Boser Q.A., Dawson T.R., Vette A.H., Hebert J.S., Pilarski P.M., Chapman C.S.","57195328386;56654088600;57224579330;10939124300;8692858600;57207511605;24597673600;","Sub-centimeter 3D gaze vector accuracy on real-world tasks: An investigation of eye and motion capture calibration routines",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169260",,"13","","",,,"10.1145/3450341.3458880","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107930458&doi=10.1145%2f3450341.3458880&partnerID=40&md5=4e77eef14502001d7327f9ceea9794eb","Department of Psychology, University of Alberta, Canada; Division of Physical Medicine and Rehabilitation, Department of Medicine, University of Alberta, Canada; Department of Mechanical Engineering, University of Alberta and Glenrose Rehabilitation Hospital, Alberta Health Services, Canada; Division of Physical Medicine and Rehabilitation Department of Medicine, University of Alberta and Alberta Machine Intelligence Institute, Canada; Faculty of Kinesiology, Sport and Recreation, University of Alberta, Canada","Stone, S.A., Department of Psychology, University of Alberta, Canada; Boser, Q.A., Division of Physical Medicine and Rehabilitation, Department of Medicine, University of Alberta, Canada; Dawson, T.R., Division of Physical Medicine and Rehabilitation, Department of Medicine, University of Alberta, Canada; Vette, A.H., Department of Mechanical Engineering, University of Alberta and Glenrose Rehabilitation Hospital, Alberta Health Services, Canada; Hebert, J.S., Department of Psychology, University of Alberta, Canada; Pilarski, P.M., Division of Physical Medicine and Rehabilitation Department of Medicine, University of Alberta and Alberta Machine Intelligence Institute, Canada; Chapman, C.S., Faculty of Kinesiology, Sport and Recreation, University of Alberta, Canada","Measuring where people look in real-world tasks has never been easier but analyzing the resulting data remains laborious. One solution integrates head-mounted eye tracking with motion capture but no best practice exists regarding what calibration data to collect. Here, we compared four ∼1 min calibration routines used to train linear regression gaze vector models and examined how the coordinate system, eye data used and location of fixation changed gaze vector accuracy on three trial types: calibration, validation (static fixation to task relevant locations), and task (naturally occurring fixations during object interaction). Impressively, predicted gaze vectors show ∼1 cm of error when looking straight ahead toward objects during natural arms-length interaction. This result was achieved predicting fixations in a Spherical coordinate frame, from the best monocular data, and, surprisingly, depends little on the calibration routine. © 2021 ACM.","calibration; eye tracking; gaze vector; linear regression; motion capture","Calibration; Motion capture; Motion tracking; Vectors; 3D gaze vectors; Calibration data; Co-ordinate system; Head-mounted eye tracking; Naturally occurring; Object interactions; Real-world task; Spherical coordinate frame; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107930458
"Kurzhals K.","55390097400;","Image-Based Projection Labeling for Mobile Eye Tracking",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169256",,,"","",,,"10.1145/3448017.3457382","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107693822&doi=10.1145%2f3448017.3457382&partnerID=40&md5=9a077b0b6d9b3f62dfe80cdaa03a4d83","Eth Zürich, Switzerland","Kurzhals, K., Eth Zürich, Switzerland","The annotation of gaze data concerning investigated areas of interest (AOIs) poses a time-consuming step in the analysis procedure of eye tracking experiments. For data from mobile eye tracking glasses, the annotation effort is further increased because each recording has to be investigated individually. Automated approaches based on supervised machine learning require pre-trained categories which are hard to obtain without human interpretation, i.e., labeling ground truth data. We present an interactive visualization approach that supports efficient annotation of gaze data based on image content participants with eye tracking glasses focused on. Recordings can be segmented individually to reduce the annotation effort. Thumbnails represent segments visually and are projected on a 2D plane for a fast comparison of AOIs. Annotated scanpaths can then be interpreted directly with the timeline visualization. We showcase our approach with three different scenarios. © 2021 ACM.","dimensionality reduction; mobile eye tracking; video; Visualization","Data visualization; Glass; Supervised learning; Visualization; Automated approach; Ground truth data; Image content; Image-based; Interactive visualizations; Mobile eye-tracking; Supervised machine learning; Timeline visualizations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107693822
"Burch M., Wallner G., Broeks N., Piree L., Boonstra N., Vlaswinkel P., Franken S., Van Wijk V.","36927991800;26032727500;57224465055;57224486610;57224474083;57224464464;57224479417;57224467173;","The Power of Linked Eye Movement Data Visualizations",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169256",,,"","",,,"10.1145/3448017.3457377","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107681634&doi=10.1145%2f3448017.3457377&partnerID=40&md5=7b4fb7781513d87bca49540910f4c6a6","University of Applied Sciences Chur, Switzerland Eindhoven University of Technology, Eindhoven, Netherlands; Johannes Kepler University Linz Linz, Austria Eindhoven University of Technology, Eindhoven, Netherlands; Eindhoven University of Technology, Eindhoven, Netherlands","Burch, M., University of Applied Sciences Chur, Switzerland Eindhoven University of Technology, Eindhoven, Netherlands; Wallner, G., Johannes Kepler University Linz Linz, Austria Eindhoven University of Technology, Eindhoven, Netherlands; Broeks, N., Eindhoven University of Technology, Eindhoven, Netherlands; Piree, L., Eindhoven University of Technology, Eindhoven, Netherlands; Boonstra, N., Eindhoven University of Technology, Eindhoven, Netherlands; Vlaswinkel, P., Eindhoven University of Technology, Eindhoven, Netherlands; Franken, S., Eindhoven University of Technology, Eindhoven, Netherlands; Van Wijk, V., Eindhoven University of Technology, Eindhoven, Netherlands","In this paper we showcase several eye movement data visualizations and how they can be interactively linked to design a flexible visualization tool for eye movement data. The aim of this project is to create a user-friendly and easy accessible tool to interpret visual attention patterns and to facilitate data analysis for eye movement data. Hence, to increase accessibility and usability we provide a web-based solution. Users can upload their own eye movement data set and inspect it from several perspectives simultaneously. Insights can be shared and collaboratively be discussed with others. The currently available visualization techniques are a 2D density plot, a scanpath representation, a bee swarm, and a scarf plot, all supporting several standard interaction techniques. Moreover, due to the linking feature, users can select data in one visualization, and the same data points will be highlighted in all active visualizations for solving comparison tasks. The tool also provides functions that make it possible to upload both, private or public data sets, and can generate URLs to share the data and settings of customized visualizations. A user study showed that the tool is understandable and that providing linked customizable views is beneficial for analyzing eye movement data. © 2021 ACM.","information visualization; linked views; public transport maps","Behavioral research; Data visualization; Eye tracking; Visualization; Eye movement datum; Interaction techniques; Linking features; User friendly; Visual Attention; Visualization technique; Visualization tools; Web-based solutions; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85107681634
"Abdrabou Y., Shams A., Mantawy M.O., Ahmad Khan A., Khamis M., Alt F., Abdelrahman Y.","57200212716;57224000440;57224484542;57224479866;35243028400;27267528900;56156577200;","GazeMeter: Exploring the Usage of Gaze Behaviour to Enhance Password Assessments",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169256",,,"","",,,"10.1145/3448017.3457384","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107667421&doi=10.1145%2f3448017.3457384&partnerID=40&md5=25ca3d952f70b6fc396e4da7f3b7f277","Bundeswehr University, Munich, Germany","Abdrabou, Y., Bundeswehr University, Munich, Germany; Shams, A., Bundeswehr University, Munich, Germany; Mantawy, M.O., Bundeswehr University, Munich, Germany; Ahmad Khan, A., Bundeswehr University, Munich, Germany; Khamis, M., Bundeswehr University, Munich, Germany; Alt, F., Bundeswehr University, Munich, Germany; Abdelrahman, Y., Bundeswehr University, Munich, Germany","We investigate the use of gaze behaviour as a means to assess password strength as perceived by users. We contribute to the effort of making users choose passwords that are robust against guessing-attacks. Our particular idea is to consider also the users' understanding of password strength in security mechanisms. We demonstrate how eye tracking can enable this: by analysing people's gaze behaviour during password creation, its strength can be determined. To demonstrate the feasibility of this approach, we present a proof of concept study (N = 15) in which we asked participants to create weak and strong passwords. Our findings reveal that it is possible to estimate password strength from gaze behaviour with an accuracy of 86% using Machine Learning. Thus, we enable research on novel interfaces that consider users' understanding with the ultimate goal of making users choose stronger passwords. © 2021 ACM.","Eye-tracking; Gaze Behaviour; Password Meters; Password Strength","Authentication; Gaze behaviours; Guessing attacks; Password creation; Password strength; Proof of concept; Security mechanism; Strong password; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107667421
"Emery K.J., Zannoli M., Warren J., Xiao L., Talathi S.S.","57191309772;50263543600;57223647367;57208440163;57224997923;","OpenNEEDS: A Dataset of Gaze, Head, Hand, and Scene Signals during Exploration in Open-Ended VR Environments",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"11","","",,1,"10.1145/3448018.3457996","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107605103&doi=10.1145%2f3448018.3457996&partnerID=40&md5=545b29a7a6100c8ef4aa6ea3a4d4e4cf","The Institute for Neuroscience, The University of Nevada, Reno, United States; Facebook Reality Labs, United States; Facebook Reality Labs Research, United States","Emery, K.J., The Institute for Neuroscience, The University of Nevada, Reno, United States; Zannoli, M., Facebook Reality Labs, United States; Warren, J., Facebook Reality Labs Research, United States; Xiao, L., The Institute for Neuroscience, The University of Nevada, Reno, United States; Talathi, S.S., The Institute for Neuroscience, The University of Nevada, Reno, United States","We present OpenNEEDS, the first large-scale, high frame rate, comprehensive, and open-source dataset of Non-Eye (head, hand, and scene) and Eye (3D gaze vectors) data captured for 44 participants as they freely explored two virtual environments with many potential tasks (i.e., reading, drawing, shooting, object manipulation, etc.). With this dataset, we aim to enable research on the relationship between head, hand, scene, and gaze spatiotemporal statistics and its applications to gaze estimation. To demonstrate the power of OpenNEEDS, we show that gaze estimation models using individual non-eye sensors and an early fusion model combining all non-eye sensors outperform all baseline gaze estimation models considered, suggesting the possibility of considering non-eye sensors in the design of robust eye trackers. We anticipate that this dataset will support research progress in many areas and applications such as gaze estimation and prediction, sensor fusion, human-computer interaction, intent prediction, perceptuo-motor control, and machine learning. © 2021 ACM.","datasets; eye tracking; gaze estimation; virtual reality","Human computer interaction; Large dataset; Statistics; 3D gaze vectors; Gaze estimation; High frame rate; ITS applications; Motor control; Object manipulation; Sensor fusion; Spatio-temporal statistics; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107605103
"Chaudhary A.K., Gyawali P.K., Wang L., Pelz J.B.","57210103548;57194830715;15043491500;7007018556;","Semi-Supervised Learning for Eye Image Segmentation",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"8","","",,,"10.1145/3448018.3458009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107601224&doi=10.1145%2f3448018.3458009&partnerID=40&md5=76741e35192356bc8ae2697c2dda89be","Rochester Institute of Technology, United States; Computing and Information Sciences Rochester, Institute of Technology, United States","Chaudhary, A.K., Rochester Institute of Technology, United States; Gyawali, P.K., Computing and Information Sciences Rochester, Institute of Technology, United States; Wang, L., Rochester Institute of Technology, United States; Pelz, J.B., Rochester Institute of Technology, United States","Recent advances in appearance-based models have shown improved eye tracking performance in difficult scenarios like occlusion due to eyelashes, eyelids or camera placement, and environmental reflections on the cornea and glasses. The key reason for the improvement is the accurate and robust identification of eye parts (pupil, iris, and sclera regions). The improved accuracy often comes at the cost of labeling an enormous dataset, which is complex and time-consuming. This work presents two semi-supervised learning frameworks to identify eye-parts by taking advantage of unlabeled images where labeled datasets are scarce. With these frameworks, leveraging the domain-specific augmentation and novel spatially varying transformations for image segmentation, we show improved performance on various test cases with limited labeled samples. For instance, for a model trained on just 4 and 48 labeled images, these frameworks improved by at least 4.7% and 0.4% respectively, in segmentation performance over the baseline model, which is trained only with the labeled dataset. © 2021 ACM.","AR/VR; eye-segmentation; eye-tracking; gaze-tracking; segmentation; semi-supervised learning","Image enhancement; Image segmentation; Semi-supervised learning; Appearance-based models; Baseline models; Camera placement; Domain specific; Labeled dataset; Labeled datasets; Robust identification; Segmentation performance; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107601224
"Prinzler M.H.U., Schröder C., Al Zaidawi S.M.K., Zachmann G., Maneth S.","57203303459;55290734800;57219109682;6603062795;6601993718;","Visualizing Prediction Correctness of Eye Tracking Classifiers",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"10","","",,,"10.1145/3448018.3457997","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107591346&doi=10.1145%2f3448018.3457997&partnerID=40&md5=4f7a00eaf49b156d1da4d019b60409a3","Database Lab University of Bremen, Germany; Computer Graphics and Virtual Reality, University of Bremen, Germany","Prinzler, M.H.U., Database Lab University of Bremen, Germany; Schröder, C., Computer Graphics and Virtual Reality, University of Bremen, Germany; Al Zaidawi, S.M.K., Database Lab University of Bremen, Germany; Zachmann, G., Database Lab University of Bremen, Germany; Maneth, S., Database Lab University of Bremen, Germany","Eye tracking data is often used to train machine learning algorithms for classification tasks. The main indicator of performance for such classifiers is typically their prediction accuracy. However, this number does not reveal any information about the specific intrinsic workings of the classifier. In this paper we introduce novel visualization methods which are able to provide such information. We introduce the Prediction Correctness Value (PCV). It is the difference between the calculated probability for the correct class and the maximum calculated probability for any other class. Based on the PCV we present two visualizations: (1) coloring segments of eye tracking trajectories according to their PCV, thus indicating how beneficial certain parts are towards correct classification, and (2) overlaying similar information for all participants to produce a heatmap that indicates at which places fixations are particularly beneficial towards correct classification. Using these new visualizations we compare the performance of two classifiers (RF and RBFN). © 2021 ACM.","Explainable Artificial Intelligence; Eye Movement Biometrics; Eye Tracking; Gaze Point Visualization; Machine Learning; Prediction Visualization; User Identification;","Classification (of information); Forecasting; Learning algorithms; Machine learning; Visualization; Classification tasks; Novel visualizations; Prediction accuracy; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107591346
"Kastrati A., Plomecka M.B., Wattenhofer R., Langer N.","57222152052;57195755873;6701529043;35766494700;","Using Deep Learning to Classify Saccade Direction from Brain Activity",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"28","","",,,"10.1145/3448018.3458014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107581882&doi=10.1145%2f3448018.3458014&partnerID=40&md5=cc8b0692b172394b4cb27a6849e75bf0","Department of Electrical Engineering and Eth, Zurich, Switzerland; Department of Psychology, Methods of Plasticity Research and University, Zurich, Switzerland; Eth Zurich and Department of Electrical Engineering, Switzerland","Kastrati, A., Department of Electrical Engineering and Eth, Zurich, Switzerland; Plomecka, M.B., Department of Psychology, Methods of Plasticity Research and University, Zurich, Switzerland; Wattenhofer, R., Eth Zurich and Department of Electrical Engineering, Switzerland; Langer, N., Department of Psychology, Methods of Plasticity Research and University, Zurich, Switzerland","We present first insights into our project that aims to develop an Electroencephalography (EEG) based Eye-Tracker. Our approach is tested and validated on a large dataset of simultaneously recorded EEG and infrared video-based Eye-Tracking, serving as ground truth. We compared several state-of-the-art neural network architectures for time series classification: InceptionTime, EEGNet, and investigated other architectures such as convolutional neural networks (CNN) with Xception modules and Pyramidal CNN. We prepared and tested these architectures with our rich dataset and obtained a remarkable accuracy of the left/right saccades direction classification (94.8 %) for the InceptionTime network, after hyperparameter tuning. © 2021 Owner/Author.","gaze detection; neural networks; simultaneous Electroencephalography and Eye-tracking; time-series classification","Brain; Classification (of information); Convolutional neural networks; Electroencephalography; Electrophysiology; Eye movements; Eye tracking; Large dataset; Network architecture; Brain activity; Eye trackers; Ground truth; Hyper-parameter; Infrared video; State of the art; Time series classifications; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85107581882
"Kübler T.C., Fuhl W., Wagner E., Kasneci E.","55701951700;56770084800;57194609927;56059892600;","55 Rides: Attention annotated head and gaze data during naturalistic driving",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"17","","",,,"10.1145/3448018.3457993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107577538&doi=10.1145%2f3448018.3457993&partnerID=40&md5=c53b5831baf62f054f8cad7c939d3925","University of Tübingen, Germany; Eberhard Karls Universität Tübingen Wilhelm Schickard Institut, Germany","Kübler, T.C., University of Tübingen, Germany; Fuhl, W., Eberhard Karls Universität Tübingen Wilhelm Schickard Institut, Germany; Wagner, E., University of Tübingen, Germany; Kasneci, E., University of Tübingen, Germany","Trained eye patterns are essential for safe driving. Whether for exploration of the surrounding traffic or to make sure that a lane is clear through a shoulder check - quick and effective perception is the key to driving safety. Surprisingly though, free and open access data on gaze behavior during driving are yet extremely sparse. The environment inside a vehicle is challenging for eye-tracking technology due to rapidly changing illumination conditions, such as exiting a tunnel to brightest sunlight, proper calibration and safety. So far, available data exhibits environments that likely influence the viewing behavior, sometimes dramatically (e.g., driving simulators without mirrors, limited field of view). We propose crowd-sourced eye-tracking data collected during real-world driving using NIR-cameras and illuminators that were placed within the driver's cabin. We analyze this data using a deep learning appearance-based gaze estimation, with raw videos not being part of the data set due to legal restrictions. Our data set contains four different drivers in their habitual cars and 55 rides of an average of 30 minutes length. At least three human raters rated each ride continuously with regard to driver attention and vigilance level on a ten-point scale. From the recorded videos we extracted drivers' head and eye movements as well as eye opening angle. For this data, we apply a normalization with respect to different placement of the driver monitoring camera and demonstrate a baseline for driver attention monitoring based on eye gaze and head movement features. © 2021 ACM.","datasets; driver attention; gaze detection; neural networks","Behavioral research; Cameras; Deep learning; Eye movements; Driver attention; Driver monitoring; Driving simulator; Eye tracking technologies; Illumination conditions; Legal restriction; Real-world drivings; Vigilance levels; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107577538
"Lu W., He H., Urban A., Griffin J.","57200881818;57210982273;57211249210;57198599188;","What the Eyes Can Tell: Analyzing Visual Attention with an Educational Video Game",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"36","","",,,"10.1145/3448018.3459654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107565275&doi=10.1145%2f3448018.3459654&partnerID=40&md5=71abc061612b8813a2090954ac69ed6f","University of Missouri, United States","Lu, W., University of Missouri, United States; He, H., University of Missouri, United States; Urban, A., University of Missouri, United States; Griffin, J., University of Missouri, United States","3D video games show potential as educational tools that improve learner engagement. Integrating 3D games into school curricula, however, faces various challenges. One challenge is providing visualizations on learning dashboards for instructors. Such dashboards provide needed information so that instructors may conduct timely and appropriate interventions when students need it. Another challenge is identifying contributive learning predictors for a computational model, which can be the core algorithm used to make games more intelligent for tutoring and assessment purposes. Previous studies have found that students' visual-attention is a vital aspect of engagement during gameplay. However, few studies have examined whether attention visualization patterns can distinguish students from different performance groups. Complicating this research is the relatively nascent investigation into gaze metrics for learning-prediction models. In this exploratory study, we used eye-tracking data from an educational game, Mission HydroSci, to examine visual-attention pattern differences between low and high performers and how their self-reported demographics affect such patterns. Results showed different visual-attention patterns between low and high performers. Additionally, self-reported science, gaming, and navigational expertise levels were significantly correlated to several gaze metric features. © 2021 ACM.",,"Behavioral research; Eye tracking; Human computer interaction; Learning systems; Predictive analytics; Students; Visualization; Computational model; Educational game; Educational tools; Educational video games; Exploratory studies; Performance group; Prediction model; Visual Attention; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85107565275
"Fuhl W., Kasneci E.","56770084800;56059892600;","A Multimodal Eye Movement Dataset and a Multimodal Eye Movement Segmentation Analysis",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"16","","",,,"10.1145/3448018.3458004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107553332&doi=10.1145%2f3448018.3458004&partnerID=40&md5=c4d86963d6c9cb45680581f7f87ed7a7","Eberhard Karls Universität Tübingen Wilhelm Schickard Institut, Germany; University of Tubingen, Germany","Fuhl, W., Eberhard Karls Universität Tübingen Wilhelm Schickard Institut, Germany; Kasneci, E., University of Tubingen, Germany","We present a new dataset with annotated eye movements. The dataset consists of over 800,000 gaze points recorded during a car ride in the real world and in the simulator. In total, the eye movements of 19 subjects were annotated. In this dataset, there are several data sources including the eyelid closure, the pupil center, the optical vector, and a vector into the pupil center starting from the center of the eye corners. These different data sources are analyzed and evaluated individually as well as in combination with respect to their suitability for eye movement classification. These results will help developers of real-time systems and algorithms to find the best data sources for their application. Also, new algorithms can be trained and evaluated on this data set. Link to code and dataset https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FA%20Multimodal%20Eye%20Movement%20Dataset%20and%20...mode=list © 2021 ACM.","Classification; Data set; Driving; Eye Movements; Machine Learning; Real World; Segmentation","Eye tracking; Interactive computer systems; Motion analysis; Real time systems; Data-sources; Eye corners; Eye movement classifications; Gaze point; Multi-modal; Pupil centers; Real-world; Segmentation analysis; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85107553332
"Yan Y., Guo X., Tang J., Li C., Wang X.","57221855551;57221850465;24286986300;56699429900;57192167903;","Learning spatio-temporal correlation filter for visual tracking",2021,"Neurocomputing","436",,,"273","282",,2,"10.1016/j.neucom.2021.01.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100387936&doi=10.1016%2fj.neucom.2021.01.057&partnerID=40&md5=8cae1e932f345fc4a94c9afad8e4182e","Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, 230601, China; Peking University, Shenzhen Graduate School, Shenzhen, 518055, China; Shenzhen Raixun Information Technology Co., Ltd., China; Institute of Physical Science and Information Technology, Anhui University, Hefei, 230601, China","Yan, Y., Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, 230601, China; Guo, X., Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, 230601, China; Tang, J., Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, 230601, China; Li, C., Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, 230601, China, Institute of Physical Science and Information Technology, Anhui University, Hefei, 230601, China; Wang, X., Peking University, Shenzhen Graduate School, Shenzhen, 518055, China, Shenzhen Raixun Information Technology Co., Ltd., China","Correlation filter (CF) trackers have performed impressive performance with high frame rates. However, the limited information in both spatial and temporal domains is only used in the learning of correlation filters, which might limit the tracking performance. To handle this problem, we propose a novel spatio-temporal correlation filter approach, which employs both spatial and temporal cues in the learning, for visual tracking. In particular, we explore the spatial contexts from background whose contents are ambiguous to the target and integrate them into the correlation filter model for more discriminative learning. Moreover, to capture the appearance variations in temporal domain, we also compute a set of target templates and incorporate them into our model. At the same time, the solution of the proposed spatio-temporal correlation filter is closed-form and the tracking efficiency is thus guaranteed. Experimental experiments on benchmark datasets demonstrate the effectiveness of the proposed tracker against several CF ones. © 2021 Elsevier B.V.","Correlation filter; Spatial feature; Temporal feature; Visual tracking","Neural networks; Benchmark datasets; Correlation filters; Discriminative learning; High frame rate; Limited information; Spatial context; Spatiotemporal correlation; Tracking performance; Computer applications; Article; association; awareness; benchmarking; correlational study; discrimination learning; eye tracking; learning; model; priority journal; process optimization; spatiotemporal analysis",Article,"Final","",Scopus,2-s2.0-85100387936
"Tamim H.M., Sultana F., Tasneem N., Marzan Y., Khan M.M.","57217675545;57230311000;57220387768;57222613646;36350785300;","Class Insight: A Student Monitoring System with Real-time Updates using Face Detection and Eye Tracking",2021,"2021 IEEE World AI IoT Congress, AIIoT 2021",,,"9454176","213","220",,,"10.1109/AIIoT52608.2021.9454176","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113416864&doi=10.1109%2fAIIoT52608.2021.9454176&partnerID=40&md5=111850853aad63226ae630c745d02387","North South University, Department of Electrical and Computer Engineering, Dhaka, 1229, Bangladesh","Tamim, H.M., North South University, Department of Electrical and Computer Engineering, Dhaka, 1229, Bangladesh; Sultana, F., North South University, Department of Electrical and Computer Engineering, Dhaka, 1229, Bangladesh; Tasneem, N., North South University, Department of Electrical and Computer Engineering, Dhaka, 1229, Bangladesh; Marzan, Y., North South University, Department of Electrical and Computer Engineering, Dhaka, 1229, Bangladesh; Khan, M.M., North South University, Department of Electrical and Computer Engineering, Dhaka, 1229, Bangladesh","The student monitoring system represents a detailed description of Class Insight. It explains the purpose and the features of the system. It also interprets the interfaces, the working procedures of the system, the constraints under which it will operate and how the system will react to external stimuli. This is a machine learning-based student monitoring system that allows teachers to submit an assessment to students completely paperless. It provides tools for teachers and students to keep track of their assignments, reading materials and other tasks. The application will keep track of the students' face and eye while reading and will update progresses instantly. As a result, instructors can track real-time updates of the tasks. They will also be notified whether it is the student's face or not and how much time they spent on a single page of the reading materials. This will be generated as a report. © 2021 IEEE.","facial detection; paperless submission; pdf renderer; real-time update","Eye tracking; Face recognition; Internet of things; Students; Turing machines; External stimulus; Keep track of; Paperless; Real-time updates; Student monitoring; Working procedure; Monitoring",Conference Paper,"Final","",Scopus,2-s2.0-85113416864
"Weber T., Winiker C., Hussmann H.","57222568396;57223400388;23389275800;","A Closer Look at Machine Learning Code",2021,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,,"10.1145/3411763.3451679","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105821289&doi=10.1145%2f3411763.3451679&partnerID=40&md5=8ee0c6e685b37885eff2cf3b0ce10dec","Institute for Informatics Lmu Munich, Germany; LMU Munich, Germany","Weber, T., Institute for Informatics Lmu Munich, Germany; Winiker, C., Institute for Informatics Lmu Munich, Germany; Hussmann, H., LMU Munich, Germany","Software using Machine Learning algorithms is becoming ever more ubiquitous making it equally important to have good development processes and practices. Whether we can apply insights from software development research remains open though, since it is not yet clear, whether data-driven development has the same requirements as its traditional counterpart. We used eye tracking to investigate whether the code reading behaviour of developers differs between code that uses Machine Learning and code that does not. Our data shows that there are differences in what parts of the code people consider of interest and how they read it. This is a consequence of differences in both syntax and semantics of the code. This reading behaviour already shows that we cannot take existing solutions as universally applicable. In the future, methods that support Machine Learning must iterate on existing knowledge to meet the challenges of data-driven development. © 2021 ACM.","code reading; eye tracking; machine learning","Eye tracking; Human engineering; Learning algorithms; Semantics; Software design; Data driven; Development process; Machine learning",Conference Paper,"Final","",Scopus,2-s2.0-85105821289
"Heck M., Edinger J., Becker C.","57205548572;56178122500;55683104700;","Conditioning Gaze-Contingent Systems for the Real World: Insights from a Field Study in the Fast Food Industry",2021,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,,"10.1145/3411763.3451658","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105819571&doi=10.1145%2f3411763.3451658&partnerID=40&md5=3a377116513e849c5396beafc9e8333b","University of Mannheim, Germany; University of Hamburg, Germany; Information Systems Ii, University of Mannheim, Germany","Heck, M., University of Mannheim, Germany; Edinger, J., University of Hamburg, Germany; Becker, C., Information Systems Ii, University of Mannheim, Germany","Eye tracking can be used to infer what is relevant to a user, and adapt the content and appearance of an application to support the user in their current task. A prerequisite for integrating such adaptive user interfaces into public terminals is robust gaze estimation. Commercial eye trackers are highly accurate, but require prior person-specific calibration and a relatively stable head position. In this paper, we collect data from 26 authentic customers of a fast food restaurant while interacting with a total of 120 products on a self-order terminal. From our observations during the experiment and a qualitative analysis of the collected gaze data, we derive best practice approaches regarding the integration of eye tracking software into self-service systems. We evaluate several implicit calibration strategies that derive the user's true focus of attention either from the context of the user interface, or from their interaction with the system. Our results show that the original gaze estimates can be visibly improved by taking into account both contextual and interaction-based information. © 2021 ACM.","eye tracker calibration; eye tracking; gaze interfaces","Calibration; Human engineering; User interfaces; Adaptive user interface; Fast-food industries; Fast-food restaurants; Focus of Attention; Gaze estimation; Public terminals; Qualitative analysis; Self-service systems; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85105819571
"Sasikumar P., Collins M., Bai H., Billinghurst M.","57212406932;57223398032;55555685900;7006142663;","XRTB: A Cross Reality Teleconference Bridge to incorporate 3D interactivity to 2D Teleconferencing",2021,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,,"10.1145/3411763.3451546","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105811813&doi=10.1145%2f3411763.3451546&partnerID=40&md5=cb49922c1125517b6e9f2b2bc36a27a0","Auckland Bioengineering Institute, University of Auckland, New Zealand; Informatics University of California, Irvine, United States","Sasikumar, P., Auckland Bioengineering Institute, University of Auckland, New Zealand; Collins, M., Informatics University of California, Irvine, United States; Bai, H., Auckland Bioengineering Institute, University of Auckland, New Zealand; Billinghurst, M., Auckland Bioengineering Institute, University of Auckland, New Zealand","We present XRTeleBridge (XRTB), an application that integrates a Mixed Reality (MR) interface into existing teleconferencing solutions like Zoom. Unlike conventional webcam, XRTB provides a window into the virtual world to demonstrate and visualize content. Participants can join via webcam or via head mounted display (HMD) in a Virtual Reality (VR) environment. It enables users to embody 3D avatars with natural gestures and eye gaze. A camera in the virtual environment operates as a video feed to the teleconferencing software. An interface resembling a tablet mirrors the teleconferencing window inside the virtual environment, thus enabling the participant in the VR environment to see the webcam participants in real-time. This allows the presenter to view and interact with other participants seamlessly. To demonstrate the system's functionalities, we created a virtual chemistry lab environment and presented an example lesson using the virtual space and virtual objects and effects. © 2021 Owner/Author.","Natural hand gestures; Remote Collaboration; Teleconference; Virtual Reality","Helmet mounted displays; Human engineering; Mixed reality; Three dimensional computer graphics; 3D Avatars; Eye-gaze; Head mounted displays; Interactivity; Real time; Virtual objects; Virtual spaces; Virtual worlds; Teleconferencing",Conference Paper,"Final","",Scopus,2-s2.0-85105811813
"W hler L., Zembaty M.","57224010686;57224012546;","Towards understanding perceptual diferences between genuine and face-swapped videos",2021,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,,"10.1145/3411764.3445627","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106759542&doi=10.1145%2f3411764.3445627&partnerID=40&md5=bcb50e2ed9732475912d0f3f8df29672","Institut fur Computergraphik, Tu Braunschweig, Braunschweig, Germany","W hler, L., Institut fur Computergraphik, Tu Braunschweig, Braunschweig, Germany; Zembaty, M., Institut fur Computergraphik, Tu Braunschweig, Braunschweig, Germany","In this paper, we report on perceptual experiments indicating that there are distinct and quantitatively measurable diferences in the way we visually perceive genuine versus face-swapped videos. Recent progress in deep learning has made face-swapping techniques a powerful tool for creative purposes, but also a means for unethical forgeries. Currently, it remains unclear why people are misled, and which indicators they use to recognize potential manipulations. Here, we conduct three perceptual experiments focusing on a wide range of aspects: the conspicuousness of artifacts, the viewing behavior using eye tracking, the recognition accuracy for diferent video lengths, and the assessment of emotions. Our experiments show that responses difer distinctly when watching manipulated as opposed to original faces, from which we derive perceptual cues to recognize face swaps. By investigating physiologically measurable signals, our fndings yield valuable insights that may also be useful for advanced algorithmic detection. © 2021 ACM.","Eye tracking; Face swapping; Human perception; Video manipulation","Eye tracking; Human engineering; Face swapping; Recent progress; Recognition accuracy; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85106759542
"Sidenmark L., Potts D.","57210111157;36550598500;","Radi-eye: Hands-free radial interfaces for 3d interaction using gaze-activated head-crossing",2021,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,,"10.1145/3411764.3445697","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106758767&doi=10.1145%2f3411764.3445697&partnerID=40&md5=42304b4cb5a3dd38adc3c59f2a0bf3bd","Lancaster University, United Kingdom","Sidenmark, L., Lancaster University, United Kingdom; Potts, D., Lancaster University, United Kingdom","Eye gaze and head movement are attractive for hands-free 3D interaction in head-mounted displays, but existing interfaces aford only limited control. Radi-Eye is a novel pop-up radial interface designed to maximise expressiveness with input from only the eyes and head. Radi-Eye provides widgets for discrete and continuous input and scales to support larger feature sets. Widgets can be selected with Look and Cross, using gaze for pre-selection followed by head-crossing as trigger and for manipulation. The technique leverages natural eye-head coordination where eye and head move at an ofset unless explicitly brought into alignment, enabling interaction without risk of unintended input. We explore Radi-Eye in three augmented and virtual reality applications, and evaluate the efect of radial interface scale and orientation on performance with Look and Cross. The results show that Radi-Eye provides users with fast and accurate input while opening up a new design space for hands-free fuid interaction. © 2021 ACM.","Augmented reality; Eye tracking; Eye-head coordination; Gaze interaction; Radial interface; Virtual reality","Helmet mounted displays; Human engineering; 3D interactions; Augmented and virtual realities; Continuous input; Design spaces; Eye-head coordination; Head mounted displays; Head movements; Pre-selection; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85106758767
"Ahuja K., Shah D., Pareddy S., Xhakaj F.","57192554466;57224009634;57191975333;56989963500;","Classroom digital twins with instrumentation-free gaze tracking",2021,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,1,"10.1145/3411764.3445711","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106701387&doi=10.1145%2f3411764.3445711&partnerID=40&md5=58d8fdcfd9733a735e3f00f220678936","Carnegie Mellon University, Pittsburgh, PA, United States","Ahuja, K., Carnegie Mellon University, Pittsburgh, PA, United States; Shah, D., Carnegie Mellon University, Pittsburgh, PA, United States; Pareddy, S., Carnegie Mellon University, Pittsburgh, PA, United States; Xhakaj, F., Carnegie Mellon University, Pittsburgh, PA, United States","Classroom sensing is an important and active area of research with great potential to improve instruction. Complementing professional observers - the current best practice - automated pedagogical professional development systems can attend every class and capture fne-grained details of all occupants. One particularly valuable facet to capture is class gaze behavior. For students, certain gaze patterns have been shown to correlate with interest in the material, while for instructors, student-centered gaze patterns have been shown to increase approachability and immediacy. Unfortunately, prior classroom gaze-sensing systems have limited accuracy and often require specialized external or worn sensors. In this work, we developed a new computer-vision-driven system that powers a 3D digital twin of the classroom and enables whole-class, 6DOF head gaze vector estimation without instrumenting any of the occupants. We describe our open source implementation, and results from both controlled studies and real-world classroom deployments. © 2021 ACM.","Classroom sensing; Digital twins; Gaze tracking","Digital twin; Human engineering; Best practices; Driven system; Gaze behavior; Gaze tracking; Open source implementation; Professional development; Sensing systems; Vector estimation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85106701387
"Li F., Lee C.-H., Feng S., Trappey A., Gilani F.","57196404325;56939382300;56393590300;7003314683;57224514386;","Prospective on Eye-Tracking-based Studies in Immersive Virtual Reality",2021,"Proceedings of the 2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design, CSCWD 2021",,,"9437692","861","866",,,"10.1109/CSCWD49262.2021.9437692","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107722053&doi=10.1109%2fCSCWD49262.2021.9437692&partnerID=40&md5=e011f80ba0dda530772208559a7d5d76","Fraunhofer Singapore, Nanyang Technological University, Singapore, Singapore; School of Public Policy and Administration, Xi'an Jiaotong University, Xi'an, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; National Tsing Hua University, Department of Industrial Engineering and Engineering Management, Hsinchu, Taiwan","Li, F., Fraunhofer Singapore, Nanyang Technological University, Singapore, Singapore; Lee, C.-H., School of Public Policy and Administration, Xi'an Jiaotong University, Xi'an, China; Feng, S., Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Trappey, A., National Tsing Hua University, Department of Industrial Engineering and Engineering Management, Hsinchu, Taiwan; Gilani, F., School of Public Policy and Administration, Xi'an Jiaotong University, Xi'an, China","The current virtual reality (VR) techniques develop immersive environments via inducing illusions to our sense. Nowadays, most of VR focuses on inducing visual illusion. Hence, visual is the most important input channel for experiencing and exploring the VR environments. Recently, extensive research efforts have been put on eye-tracking studies. However, the development and growing trends of the VR-based eye-tracking studies are unrevealed due to the lack of a systematic literature review on it. In this study, we reviewed related literature from 2000 to 2019 and summarized them into two main categories, including eye tracking methods and eye-tracking-enabled applications, such as tracking gaze points to manipulate the VR environment, measuring user states, and evaluating the usability of VR based on eye-tracking data. Based on the literature review, we can find that eye-tracking can assist in developing adaptive VR systems and enhance users experience. While comparing with 2D environments, immersive VR environment still requires more deep studies in eye-tracking. © 2021 IEEE.","eye-tracking; immersive; review; virtual reality","Adaptive systems; Interactive computer systems; User experience; Virtual reality; Eye tracking methods; Eye-tracking studies; Immersive environment; Immersive virtual reality; Literature reviews; Research efforts; Systematic literature review; Visual illusions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107722053
"Moghaddasi M., Marín-Morales J., Khatri J., Guixeres J., Chicchi Giglioli I.A., Alcañiz M.","57211442960;57191977544;57218446760;26423690300;56994284500;7003335420;","Recognition of customers’ impulsivity from behavioral patterns in virtual reality",2021,"Applied Sciences (Switzerland)","11","10","4399","","",,1,"10.3390/app11104399","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106957236&doi=10.3390%2fapp11104399&partnerID=40&md5=57717575d7d3c7a7623764f97c9ccdad","Instituto de Investigación e Innovación en Bioingeniería, Universitat Politécnica de València, València, 46022, Spain","Moghaddasi, M., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politécnica de València, València, 46022, Spain; Marín-Morales, J., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politécnica de València, València, 46022, Spain; Khatri, J., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politécnica de València, València, 46022, Spain; Guixeres, J., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politécnica de València, València, 46022, Spain; Chicchi Giglioli, I.A., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politécnica de València, València, 46022, Spain; Alcañiz, M., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politécnica de València, València, 46022, Spain","Virtual reality (VR) in retailing (V-commerce) has been proven to enhance the consumer ex-perience. Thus, this technology is beneficial to study behavioral patterns by offering the opportunity to infer customers’ personality traits based on their behavior. This study aims to recognize impulsivity using behavioral patterns. For this goal, 60 subjects performed three tasks—one exploration task and two planned tasks—in a virtual market. Four noninvasive signals (eye-tracking, navigation, posture, and interactions), which are available in commercial VR devices, were recorded, and a set of features were extracted and categorized into zonal, general, kinematic, temporal, and spatial types. They were input into a support vector machine classifier to recognize the impulsivity of the subjects based on the I-8 questionnaire, achieving an accuracy of 87%. The results suggest that, while the exploration task can reveal general impulsivity, other subscales such as perseverance and sensation-seeking are more related to planned tasks. The results also show that posture and interaction are the most informative signals. Our findings validate the recognition of customer impulsivity using sensors incorporated into commercial VR devices. Such information can provide a personalized shopping experience in future virtual shops. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Consumer behavior; Machine learning; Signal processing; V-commerce; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85106957236
"Kang D., Chang H.S.","57211898992;57051740700;","Low-complexity pupil tracking for sunglasses-wearing faces for glasses-free 3d huds",2021,"Applied Sciences (Switzerland)","11","10","4366","","",,,"10.3390/app11104366","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106662184&doi=10.3390%2fapp11104366&partnerID=40&md5=ee9a25df1fb3cf0b56e2b2487781a563","Department of Electronic and Electrical Engineering, Hongik University, Seoul, 04066, South Korea; Multimedia Processing Lab, Samsung Advanced Institute of Technology, Suwon, 16678, South Korea","Kang, D., Department of Electronic and Electrical Engineering, Hongik University, Seoul, 04066, South Korea; Chang, H.S., Multimedia Processing Lab, Samsung Advanced Institute of Technology, Suwon, 16678, South Korea","This study proposes a pupil-tracking method applicable to drivers both with and without sunglasses on, which has greater compatibility with augmented reality (AR) three-dimensional (3D) head-up displays (HUDs). Performing real-time pupil localization and tracking is complicated by drivers wearing facial accessories such as masks, caps, or sunglasses. The proposed method fulfills two key requirements: low complexity and algorithm performance. Our system assesses both bare and sunglasses-wearing faces by first classifying images according to these modes and then assigning the appropriate eye tracker. For bare faces with unobstructed eyes, we applied our previous regression-algorithm-based method that uses scale-invariant feature transform features. For eyes occluded by sunglasses, we propose an eye position estimation method: our eye tracker uses nonoccluded face area tracking and a supervised regression-based pupil position estimation method to locate pupil centers. Experiments showed that the proposed method achieved high accuracy and speed, with a precision error of <10 mm in <5 ms for bare and sunglasses-wearing faces for both a 2.5 GHz CPU and a commercial 2.0 GHz CPU vehicle-embedded system. Coupled with its performance, the low CPU consumption (10%) demonstrated by the proposed algorithm highlights its promise for implementation in AR 3D HUD systems. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Augmented reality; Autostereoscopic 3D display; Eye detection; Eye tracking; Image occlusion",,Article,"Final","",Scopus,2-s2.0-85106662184
"Al Madi N., Peterson C.S., Sharif B., Maletic J.I.","57015382600;57209304815;22235542400;6603107019;","From Novice to Expert: Analysis of Token Level Effects in a Longitudinal Eye Tracking Study",2021,"IEEE International Conference on Program Comprehension","2021-May",,"9462965","172","183",,1,"10.1109/ICPC52881.2021.00025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107535745&doi=10.1109%2fICPC52881.2021.00025&partnerID=40&md5=9d629ec2d2e019fd2e1f27ed737477bc","Colby College, Department of Computer Science, Waterville, ME, United States; University of Nebraska-Lincoln, Department of Computer Science and Engineering, Lincoln, NE, United States; Kent State University, Department of Computer Science, Kent, OH, United States","Al Madi, N., Colby College, Department of Computer Science, Waterville, ME, United States; Peterson, C.S., University of Nebraska-Lincoln, Department of Computer Science and Engineering, Lincoln, NE, United States; Sharif, B., Kent State University, Department of Computer Science, Kent, OH, United States; Maletic, J.I., Colby College, Department of Computer Science, Waterville, ME, United States","Program comprehension is a vital skill in software development. This work investigates program comprehension by examining the eye movement of novice programmers as they gain programming experience over the duration of a Java course. Their eye movement behavior is compared to the eye movement of expert programmers. Eye movement studies of natural text show that word frequency and length influence eye movement duration and act as indicators of reading skill. The study uses an existing longitudinal eye tracking dataset with 20 novice and experienced readers of source code. The work investigates the acquisition of the effects of token frequency and token length in source code reading as an indication of program reading skill. The results show evidence of the frequency and length effects in reading source code and the acquisition of these effects by novices. These results are then leveraged in a machine learning model demonstrating how eye movement can be used to estimate programming proficiency and classify novices from experts with 72% accuracy. © 2021 IEEE.","empirical study; expertise; eye tracking; natural text; source code; token effects","Computer programming languages; Computer software; Eye tracking; Software design; Turing machines; Expert programmers; Eye-tracking studies; Machine learning models; Movement behavior; Novice programmer; Program comprehension; Programming experience; Programming proficiency; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85107535745
"Elbattah M., Loughnane C., Guérin J.-L., Carette R., Cilia F., Dequen G.","57163770900;57224399078;57200857001;57200860085;57200855464;23396657900;","Variational autoencoder for image-based augmentation of eye-tracking data",2021,"Journal of Imaging","7","5","83","","",,,"10.3390/jimaging7050083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107505125&doi=10.3390%2fjimaging7050083&partnerID=40&md5=973a864e8591322e5d1b31912135f2ab","Laboratoire Modélisation, Information, Systèmes (MIS), Université de Picardie Jules Verne, Amiens, 80080, France; Faculty of Science and Engineering, University of Limerick, Limerick, V94 T9PX, Ireland; Evolucare Technologies, Villers-Bretonneux, 80800, France; Laboratoire CRP-CPO, Université de Picardie Jules Verne, Amiens, 80000, France","Elbattah, M., Laboratoire Modélisation, Information, Systèmes (MIS), Université de Picardie Jules Verne, Amiens, 80080, France; Loughnane, C., Faculty of Science and Engineering, University of Limerick, Limerick, V94 T9PX, Ireland; Guérin, J.-L., Laboratoire Modélisation, Information, Systèmes (MIS), Université de Picardie Jules Verne, Amiens, 80080, France; Carette, R., Laboratoire Modélisation, Information, Systèmes (MIS), Université de Picardie Jules Verne, Amiens, 80080, France, Evolucare Technologies, Villers-Bretonneux, 80800, France; Cilia, F., Laboratoire CRP-CPO, Université de Picardie Jules Verne, Amiens, 80000, France; Dequen, G., Laboratoire Modélisation, Information, Systèmes (MIS), Université de Picardie Jules Verne, Amiens, 80080, France","Over the past decade, deep learning has achieved unprecedented successes in a diversity of application domains, given large-scale datasets. However, particular domains, such as healthcare, inherently suffer from data paucity and imbalance. Moreover, datasets could be largely inaccessible due to privacy concerns, or lack of data-sharing incentives. Such challenges have attached significance to the application of generative modeling and data augmentation in that domain. In this context, this study explores a machine learning-based approach for generating synthetic eye-tracking data. We explore a novel application of variational autoencoders (VAEs) in this regard. More specifically, a VAE model is trained to generate an image-based representation of the eye-tracking output, so-called scanpaths. Overall, our results validate that the VAE model could generate a plausible output from a limited dataset. Finally, it is empirically demonstrated that such approach could be employed as a mechanism for data augmentation to improve the performance in classification tasks. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Data augmentation; Deep learning; Eye-tracking; Variational autoencoder",,Article,"Final","",Scopus,2-s2.0-85107505125
"Angelopoulos A.N., Martel J.N.P., Kohli A.P., Conradt J., Wetzstein G.","57210646768;56352200200;57219631029;13005140400;24462821700;","Event-Based Near-Eye Gaze Tracking beyond 10,000 Hz",2021,"IEEE Transactions on Visualization and Computer Graphics","27","5","9389490","2577","2586",,,"10.1109/TVCG.2021.3067784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103798758&doi=10.1109%2fTVCG.2021.3067784&partnerID=40&md5=ed51ef2ff0e263682818d7f90ccc2408","University of California Berkeley, United States; Stanford University, United States; KTH Royal Institute of Technologyv, Sweden","Angelopoulos, A.N., University of California Berkeley, United States; Martel, J.N.P., Stanford University, United States; Kohli, A.P., University of California Berkeley, United States; Conradt, J., KTH Royal Institute of Technologyv, Sweden; Wetzstein, G., Stanford University, United States","The cameras in modern gaze-tracking systems suffer from fundamental bandwidth and power limitations, constraining data acquisition speed to 300 Hz realistically. This obstructs the use of mobile eye trackers to perform, e.g., low latency predictive rendering, or to study quick and subtle eye motions like microsaccades using head-mounted devices in the wild. Here, we propose a hybrid frame-event-based near-eye gaze tracking system offering update rates beyond 10,000 Hz with an accuracy that matches that of high-end desktop-mounted commercial trackers when evaluated in the same conditions. Our system, previewed in Figure 1, builds on emerging event cameras that simultaneously acquire regularly sampled frames and adaptively sampled events. We develop an online 2D pupil fitting method that updates a parametric model every one or few events. Moreover, we propose a polynomial regressor for estimating the point of gaze from the parametric pupil model in real time. Using the first event-based gaze dataset, we demonstrate that our system achieves accuracies of 0.45°-1.75° for fields of view from 45° to 98°. With this technology, we hope to enable a new generation of ultra-low-latency gaze-contingent rendering and display techniques for virtual and augmented reality. © 1995-2012 IEEE.","Augmented and virtual reality; Event-based camera; Eye tracking","Augmented reality; Cameras; Data acquisition; Data acquisition speed; Eye gaze tracking; Fields of views; Gaze tracking system; Gaze-contingent; Parametric modeling; Power limitations; Virtual and augmented reality; Eye tracking",Article,"Final","",Scopus,2-s2.0-85103798758
"Hu Z., Bulling A., Li S., Wang G.","57208101391;6505807414;56002421500;7407150270;","FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments",2021,"IEEE Transactions on Visualization and Computer Graphics","27","5","9382883","2681","2690",,1,"10.1109/TVCG.2021.3067779","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103251105&doi=10.1109%2fTVCG.2021.3067779&partnerID=40&md5=216cf8756fad33afadd5c6a0b28ffb3f","Peking University, China; University of Stuttgart, Germany","Hu, Z., Peking University, China; Bulling, A., University of Stuttgart, Germany; Li, S., Peking University, China; Wang, G., Peking University, China","Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction. However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications. We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments. Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities. Based on this analysis, we propose FixationNet - a novel learning-based model to forecast users' eye fixations in the near future in VR. We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93° to 2.35°) in free-viewing and of 15.1% (from 2.05° to 1.74°) in task-oriented situations. As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research. © 1995-2012 IEEE.","convolutional neural network; deep learning; Fixation forecasting; task-oriented attention; virtual reality; visual search","Behavioral research; Eye tracking; Comprehensive analysis; Gaze-based interaction; Gaze-contingent; Human visual attention; Immersive virtual reality; Learning Based Models; State of the art; Viewing conditions; Virtual reality",Article,"Final","",Scopus,2-s2.0-85103251105
"Liaqat S., Wu C., Duggirala P.R., Cheung S.-C.S., Chuah C.-N., Ozonoff S., Young G.","57222324379;57210639542;57222178800;34869344500;7004251298;7003652751;57225681104;","Predicting ASD diagnosis in children with synthetic and image-based eye gaze data",2021,"Signal Processing: Image Communication","94",,"116198","","",,4,"10.1016/j.image.2021.116198","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099252040&doi=10.1016%2fj.image.2021.116198&partnerID=40&md5=96a086cb9c784a893cb6b5ee17beefd0","University of Kentucky, United States; University of California, Davis, United States","Liaqat, S., University of Kentucky, United States; Wu, C., University of California, Davis, United States; Duggirala, P.R., University of California, Davis, United States; Cheung, S.-C.S., University of Kentucky, United States, University of California, Davis, United States; Chuah, C.-N., University of California, Davis, United States; Ozonoff, S., University of California, Davis, United States; Young, G., University of California, Davis, United States","As early intervention is highly effective for young children with autism spectrum disorder (ASD), it is imperative to make accurate diagnosis as early as possible. ASD has often been associated with atypical visual attention and eye gaze data can be collected at a very early age. An automatic screening tool based on eye gaze data that could identify ASD risk offers the opportunity for intervention before the full set of symptoms is present. In this paper, we propose two machine learning methods, synthetic saccade approach and image based approach, to automatically classify ASD given children's eye gaze data collected from free-viewing tasks of natural images. The first approach uses a generative model of synthetic saccade patterns to represent the baseline scan-path from a typical non-ASD individual and combines it with the real scan-path as well as other auxiliary data as inputs to a deep learning classifier. The second approach adopts a more holistic image-based approach by feeding the input image and a sequence of fixation maps into a convolutional or recurrent neural network. Using a publicly-accessible collection of children's gaze data, our experiments indicate that the ASD prediction accuracy reaches 67.23% accuracy on the validation dataset and 62.13% accuracy on the test dataset. © 2021 Elsevier B.V.","Autism spectrum disorders; Deep learning; Eye gaze data","Behavioral research; Eye movements; Learning systems; Statistical tests; Automatic screening; Auxiliary data; Early intervention; Generative model; Learning classifiers; Prediction accuracy; Publicly accessible; Visual Attention; Recurrent neural networks",Article,"Final","",Scopus,2-s2.0-85099252040
"Mazumdar P., Arru G., Battisti F.","57191918051;56017620500;22978219700;","Early detection of children with Autism Spectrum Disorder based on visual exploration of images",2021,"Signal Processing: Image Communication","94",,"116184","","",,3,"10.1016/j.image.2021.116184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099248007&doi=10.1016%2fj.image.2021.116184&partnerID=40&md5=fcf4dc58fa2cf0e340da5ccbd5cde508","Department of Engineering, Roma Tre UniversityRome, Italy; Department of Information Engineering, University of PadovaPadua, Italy","Mazumdar, P., Department of Engineering, Roma Tre UniversityRome, Italy; Arru, G., Department of Engineering, Roma Tre UniversityRome, Italy; Battisti, F., Department of Information Engineering, University of PadovaPadua, Italy","Autism Spectrum Disorder is a developmental disorder characterized by a deficit in social behaviour and specific interactions such as reduced eye contact and body gestures. Recent advancements in software and hardware multimedia technologies provide the tools for early detecting the presence of this disorder. In this paper we present an approach based on the combined use of machine learning and eye tracking information. More specifically, features are extracted from image content and viewing behaviour, such as the presence of objects and fixations towards the centre of a scene. Those features are used to train a machine learning-based classifier. The obtained results show that the considered features allow to identify children affected by autism spectrum disorder and typically developing ones. © 2021 Elsevier B.V.","Autism spectrum disorder; Classification; Fixations; Object detection; Visual saliency","Diseases; Machine learning; Multimedia systems; Turing machines; Autism spectrum disorders; Children with autisms; Developmental disorders; Multimedia technologies; Social behaviour; Software and hardwares; Specific interaction; Visual exploration; Eye tracking",Article,"Final","",Scopus,2-s2.0-85099248007
"Diederich M., Kang J., Kim T., Lindgren R.","57222733904;55926118000;57222734414;25621716400;","Developing an in-application shared view metric to capture collaborative learning in a multi-platform astronomy simulation",2021,"ACM International Conference Proceeding Series",,,,"173","183",,,"10.1145/3448139.3448156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103883510&doi=10.1145%2f3448139.3448156&partnerID=40&md5=41173f52eda903f3b4b39eb7af146064","Utah State University, United States; University of Illinois at Urbana-Champaign, United States","Diederich, M., Utah State University, United States; Kang, J., Utah State University, United States; Kim, T., University of Illinois at Urbana-Champaign, United States; Lindgren, R., University of Illinois at Urbana-Champaign, United States","There has been recent interest in the design of collaborative learning activities that are distributed across multiple technology devices for students to engage in scientific inquiry. Emerging research has begun to investigate students' collaborative behaviors across different device types and students' shared attention by tracking eye gaze, body posture, and their interactions with the digital environment. Using a 3D astronomy simulation that leverages a VR headset and tablet computers, this paper builds on the ideas described in eye-gaze studies by developing and implementing a metric of shared viewing across multiple devices. Preliminary findings suggest that a higher level of shared view could be related to increased conceptual discussion, as well as point to an early-stage pattern of behavior of decreased SV to prompt facilitator intervention to refocus collaborative efforts. We hope this metric will be a promising first step in further understanding and assessing the quality of collaboration across multiple device platforms in a single shared space. This paper provides an in depth look at a highly exploratory stage of a broader research trajectory to establish a robust, effective way to track screen views, including providing resources to teachers when students engage in similar learning environments, and providing insight from log data to understand how students effectively collaborate. © 2021 ACM.","Astronomy education; Immersive virtual reality; Log data; Science education; Shared view","Eye tracking; Space platforms; Students; Collaborative behavior; Collaborative learning; Collaborative learning activities; Digital environment; Learning environments; Multiple technology; Research trajectories; Scientific inquiry; Computer aided instruction",Conference Paper,"Final","",Scopus,2-s2.0-85103883510
"Hassan J., Leong J., Schneider B.","57222736691;57222733068;55051404100;","Multimodal data collection made easy: The EZ-MMLA toolkit: A data collection website that provides educators and researchers with easy access to multimodal data streams.",2021,"ACM International Conference Proceeding Series",,,,"579","585",,1,"10.1145/3448139.3448201","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103880474&doi=10.1145%2f3448139.3448201&partnerID=40&md5=785f6667267b68553ec41116a33f1abf",,"Hassan, J.; Leong, J.; Schneider, B.","While Multimodal Learning Analytics (MMLA) is becoming a popular methodology in the LAK community, most educational researchers still rely on traditional instruments for capturing learning processes (e.g., click-stream, log data, self-reports, qualitative observations). MMLA has the potential to complement and enrich traditional measures of learning by providing high frequency data on learners' behavior, cognition and affects. However, there is currently no easy-to-use toolkit for recording multimodal data streams. Existing methodologies rely on the use of physical sensors and custom-written code for accessing sensor data. In this paper, we present the EZ-MMLA toolkit. This toolkit was implemented as a website that provides easy access to the latest machine learning algorithms for collecting a variety of data streams from webcams: attention (eye-tracking), physiological states (heart rate), body posture (skeletal data), hand gestures, emotions (from facial expressions and speech), and lower-level computer vision algorithms (e.g., fiducial / color tracking). This toolkit can run from any browser and does not require special hardware or programming experience. We compare this toolkit with traditional methods and describe a case study where the EZ-MMLA toolkit was used in a classroom context. We conclude by discussing other applications of this toolkit, potential limitations, and future steps. © 2021 ACM.","Computer visions; Data collection toolkit; Multimodal analytics","Computer hardware description languages; Data acquisition; Data streams; Eye tracking; Machine learning; Websites; Computer vision algorithms; Facial Expressions; High frequency data; Multi-modal learning; Multimodal data streams; Physiological state; Programming experience; Qualitative observations; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85103880474
"Hari Krishna S.M., Pradyumna G., Aishwarya B., Gayathri C.","57223920293;57223937787;48461124900;57223932867;","Development of Personal Identification Number Authorization Algorithm Using Real- Time Eye Tracking Dynamic Keypad Generation",2021,"2021 6th International Conference for Convergence in Technology, I2CT 2021",,,"9417950","","",,,"10.1109/I2CT51068.2021.9417950","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106496830&doi=10.1109%2fI2CT51068.2021.9417950&partnerID=40&md5=b6ab7fd1db20187652635a62656e2db2","M S Ramaiah University of Applied Sciences, Department of Computer Science and Engineering, Bangalore, India","Hari Krishna, S.M., M S Ramaiah University of Applied Sciences, Department of Computer Science and Engineering, Bangalore, India; Pradyumna, G., M S Ramaiah University of Applied Sciences, Department of Computer Science and Engineering, Bangalore, India; Aishwarya, B., M S Ramaiah University of Applied Sciences, Department of Computer Science and Engineering, Bangalore, India; Gayathri, C., M S Ramaiah University of Applied Sciences, Department of Computer Science and Engineering, Bangalore, India","The digital financial transaction is on a continuous rise and going to be the order of the day. These transactions rely on the entry of the Personal Identification Number (PIN) by the user. The PIN is a common user authentication method for many applications such as ATM's, unlocking personal devices. Cyber-crimes are committed by shoulder surfing or thermal tracking. PIN entry is found vulnerable to password attacks such as shoulder surfing or thermal tracking. Intruders try to gain passwords or personal identification numbers (PIN) by glancing over the user's shoulder and observing the pattern of PIN entry (shoulder surfing). Thermal tracking is another method followed by cyber thieves, using the heat traces to decode the entered PIN by the user. This paper proposes a novel approach to authorization of PIN. To demonstrate the same, a web application is developed with trio-based authentications using machine learning techniques. Initially, the application detects and recognizes the user's face. A dynamic keypad is displayed which prompts the user to provide input PIN via eye blink. User's eye is detected and monitored by the application in order to capture the PIN and verifies the same with the existing PIN in the database. On a successful PIN verification process, the application allows the user to proceed with the transaction. The results show the proposed novel approach is better than existing approaches. © 2021 IEEE.","authorization; PIN; shoulder surfing; thermal tracking","Eye tracking; Learning systems; Security systems; Financial transactions; Machine learning techniques; Personal devices; Personal identification number; Real-time eye tracking; Shoulder surfing; User authentication; Verification process; Authentication",Conference Paper,"Final","",Scopus,2-s2.0-85106496830
"Annerer-Walcher S., Ceh S.M., Putze F., Kampen M., Körner C., Benedek M.","57203527419;57218565598;22036416700;57220055117;56225106900;13006425600;","How Reliably Do Eye Parameters Indicate Internal Versus External Attentional Focus?",2021,"Cognitive Science","45","4","e12977","","",,3,"10.1111/cogs.12977","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104532261&doi=10.1111%2fcogs.12977&partnerID=40&md5=f26b8230f026c21240a20408e6340026","Institute of Psychology, University of Graz, Austria; Department of Mathematics and Computer Science, University of Bremen, Germany","Annerer-Walcher, S., Institute of Psychology, University of Graz, Austria; Ceh, S.M., Institute of Psychology, University of Graz, Austria; Putze, F., Department of Mathematics and Computer Science, University of Bremen, Germany; Kampen, M., Department of Mathematics and Computer Science, University of Bremen, Germany; Körner, C., Institute of Psychology, University of Graz, Austria; Benedek, M., Institute of Psychology, University of Graz, Austria","Eye behavior is increasingly used as an indicator of internal versus external focus of attention both in research and application. However, available findings are partly inconsistent, which might be attributed to the different nature of the employed types of internal and external cognition tasks. The present study, therefore, investigated how consistently different eye parameters respond to internal versus external attentional focus across three task modalities: numerical, verbal, and visuo-spatial. Three eye parameters robustly differentiated between internal and external attentional focus across all tasks. Blinks, pupil diameter variance, and fixation disparity variance were consistently increased during internally directed attention. We also observed substantial attentional focus effects on other parameters (pupil diameter, fixation disparity, saccades, and microsaccades), but they were moderated by task type. Single-trial analysis of our data using machine learning techniques further confirmed our results: Classifying the focus of attention by means of eye tracking works well across participants, but generalizing across tasks proves to be challenging. Based on the effects of task type on eye parameters, we discuss what eye parameters are best suited as indicators of internal versus external attentional focus in different settings. © 2021 The Authors. Cognitive Science published by Wiley Periodicals LLC on behalf of Cognitive Science Society (CSS).","Eye behavior; Fixation disparity; Internal attentional focus; Internally directed cognition; LSTM; Machine learning; Microsaccades; Pupillometry","attention; cognition; human; saccadic eye movement; Attention; Cognition; Humans; Saccades",Article,"Final","",Scopus,2-s2.0-85104532261
"Zhang K., Liu H., Fan Z., Chen X., Leng Y., De Silva C.W., Fu C.","57193994064;57205643389;57222142973;57211101760;55376585000;7007032904;57208278421;","Foot Placement Prediction for Assistive Walking by Fusing Sequential 3D Gaze and Environmental Context",2021,"IEEE Robotics and Automation Letters","6","2","9362175","2509","2516",,,"10.1109/LRA.2021.3062003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101769592&doi=10.1109%2fLRA.2021.3062003&partnerID=40&md5=58cb8610eb78b6df75cc5b4ff9d5e615","ShenzhenKey Laboratory of Biomimetic Robotics and Intelligent Systems, Guangdong Provincial Key Laboratory of Human-Augmentation and Rehabilitation, Robotics in Universities, Southern University of Science and Technology, Shenzhen, 518055, China; Department of Mechanical and Energy Engineering, Southern University of Science and Technology, Shenzhen, 518055, China; Department of Mechanical Engineering, University of British Columbia, Vancouver, BC  V6T1Z4, Canada; School of Modern Post, Beijing University of Posts and Telecommunications, Beijing, 100876, China","Zhang, K., ShenzhenKey Laboratory of Biomimetic Robotics and Intelligent Systems, Guangdong Provincial Key Laboratory of Human-Augmentation and Rehabilitation, Robotics in Universities, Southern University of Science and Technology, Shenzhen, 518055, China, Department of Mechanical and Energy Engineering, Southern University of Science and Technology, Shenzhen, 518055, China, Department of Mechanical Engineering, University of British Columbia, Vancouver, BC  V6T1Z4, Canada; Liu, H., ShenzhenKey Laboratory of Biomimetic Robotics and Intelligent Systems, Guangdong Provincial Key Laboratory of Human-Augmentation and Rehabilitation, Robotics in Universities, Southern University of Science and Technology, Shenzhen, 518055, China, Department of Mechanical and Energy Engineering, Southern University of Science and Technology, Shenzhen, 518055, China; Fan, Z., School of Modern Post, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Chen, X., ShenzhenKey Laboratory of Biomimetic Robotics and Intelligent Systems, Guangdong Provincial Key Laboratory of Human-Augmentation and Rehabilitation, Robotics in Universities, Southern University of Science and Technology, Shenzhen, 518055, China, Department of Mechanical and Energy Engineering, Southern University of Science and Technology, Shenzhen, 518055, China; Leng, Y., ShenzhenKey Laboratory of Biomimetic Robotics and Intelligent Systems, Guangdong Provincial Key Laboratory of Human-Augmentation and Rehabilitation, Robotics in Universities, Southern University of Science and Technology, Shenzhen, 518055, China, Department of Mechanical and Energy Engineering, Southern University of Science and Technology, Shenzhen, 518055, China; De Silva, C.W., Department of Mechanical Engineering, University of British Columbia, Vancouver, BC  V6T1Z4, Canada; Fu, C., ShenzhenKey Laboratory of Biomimetic Robotics and Intelligent Systems, Guangdong Provincial Key Laboratory of Human-Augmentation and Rehabilitation, Robotics in Universities, Southern University of Science and Technology, Shenzhen, 518055, China, Department of Mechanical and Energy Engineering, Southern University of Science and Technology, Shenzhen, 518055, China","Predicting the locomotion intent of humans is important for controlling assistive robots. Previous studies have investigated assistive walking on structured terrains, but only a few studies have considered rough terrains. Human intent on rough terrains is more difficult to predict because there is a transition at every step. To predict the foot placements of humans on rough terrains, the present paper fuses sequential 3D gaze and the environmental context. The 3D gaze is assumed to be the intersection point of the line of sight as measured by an eye-tracker and the environmental point cloud as measured by an RGBD camera. The sequential 3D gaze and the environmental context are fused based on an RGBD SLAM algorithm. Then the segmented terrain that is closest to the center of sequential 3D gaze is regarded as the most possible foothold area at the next step. Six able-bodied subjects are invited to walk randomly on rough terrains. Their foot placements are labeled and compared with the predicted foot placements. Experimental results show that the proposed method can predict the foot placements of all subjects 0.5 step ahead. With environmental context and user-dependent time window, the distance error of predicting the foot placements can decrease to 0.086 m. Hence, gaze, environmental context, and time window are all important in predicting the human intent when navigating rough terrains. © 2016 IEEE.","3D gaze; foot placement prediction; intention recognition; prosthetics and exoskeletons; sensor/data fusion","Agricultural robots; Forecasting; Walking aids; Assistive robots; Classification accuracy; Environmental contexts; Foot placements; Intersection points; Prediction errors; SLAM algorithm; User-dependent; Eye tracking",Article,"Final","",Scopus,2-s2.0-85101769592
"Pfeuffer K., Abdrabou Y., Esteves A., Rivu R., Abdelrahman Y., Meitner S., Saadi A., Alt F.","36141954200;57200212716;36674891300;57217113594;56156577200;57217113931;57221786160;27267528900;","ARtention: A design space for gaze-adaptive user interfaces in augmented reality",2021,"Computers and Graphics (Pergamon)","95",,,"1","12",,6,"10.1016/j.cag.2021.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100172804&doi=10.1016%2fj.cag.2021.01.001&partnerID=40&md5=623e24e7403c6e96ec25750bc649c8ea","Bundeswehr University Munich, Munich, Germany; ITI / LARSyS, Instituto Superior Técnico, University of Lisbon, Lisbon, Portugal; LMU Munich, Munich, Germany; German University in Cairo, Cairo, Egypt","Pfeuffer, K., Bundeswehr University Munich, Munich, Germany; Abdrabou, Y., Bundeswehr University Munich, Munich, Germany; Esteves, A., ITI / LARSyS, Instituto Superior Técnico, University of Lisbon, Lisbon, Portugal; Rivu, R., Bundeswehr University Munich, Munich, Germany; Abdelrahman, Y., Bundeswehr University Munich, Munich, Germany; Meitner, S., LMU Munich, Munich, Germany; Saadi, A., German University in Cairo, Cairo, Egypt; Alt, F., Bundeswehr University Munich, Munich, Germany","Augmented Reality (AR) headsets extended with eye-tracking, a promising input technology for its natural and implicit nature, open a wide range of new interaction capabilities for everyday use. In this paper we present ARtention, a design space for gaze interaction specifically tailored for in-situ AR information interfaces. It highlights three important dimensions to consider in the UI design of such gaze-enabled applications: transitions from reality to the virtual interface, from single- to multi-layer content, and from information consumption to selection tasks. Such transitional aspects bring previously isolated gaze interaction concepts together to form a unified AR space, enabling more advanced application control seamlessly mediated by gaze. We describe these factors in detail. To illustrate how the design space can be used, we present three prototype applications and report informal user feedback obtained from different scenarios: a conversational UI, viewing a 3D visualization, and browsing items for shopping. We conclude with design considerations derived from our development and evaluation of the prototypes. We expect these to be valuable for researchers and designers investigating the use of gaze input in AR systems and applications. © 2021 Elsevier Ltd","AR; Attention; Augmented reality; Design space; Gaze interaction; Mixed reality","Augmented reality; Three dimensional computer graphics; User interfaces; 3D Visualization; Adaptive user interface; Advanced applications; Design considerations; Gaze interaction; Information interfaces; User feedback; Virtual interfaces; Eye tracking",Article,"Final","",Scopus,2-s2.0-85100172804
"Pelanis E., Teatini A., Eigl B., Regensburger A., Alzaga A., Kumar R.P., Rudolph T., Aghayan D.L., Riediger C., Kvarnström N., Elle O.J., Edwin B.","57199144936;57204610322;57204612420;57221784238;56496980600;55750778700;23010052000;57195681966;24077032600;26421699500;6602265257;7004352983;","Evaluation of a novel navigation platform for laparoscopic liver surgery with organ deformation compensation using injected fiducials",2021,"Medical Image Analysis","69",,"101946","","",,1,"10.1016/j.media.2020.101946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100147578&doi=10.1016%2fj.media.2020.101946&partnerID=40&md5=1af561003a8ed2a46ad4ab83474d32be","The Intervention Centre, Oslo University Hospital Rikshospitalet 0424Oslo, Norway; Institute of Clinical Medicine, University of Oslo, 1072Oslo, Norway; Department of Informatics, University of Oslo, 1072Oslo, Norway; Cascination AG 3008 Bern, Switzerland; Siemens HealthineersForchheim  91301, Germany; University Hospital Carl Gustav Carus, Technische Universität DresdenDresden  01307, Germany; Sahlgrenska University HospitalGoteborg  41345, Sweden; Department of Hepato-Pancreatic-Biliary surgery 0424, Oslo University HospitalOslo, Norway; Department of Surgery N1, Yerevan State Medical UniversityYerevan  0025, Armenia","Pelanis, E., The Intervention Centre, Oslo University Hospital Rikshospitalet 0424Oslo, Norway, Institute of Clinical Medicine, University of Oslo, 1072Oslo, Norway; Teatini, A., The Intervention Centre, Oslo University Hospital Rikshospitalet 0424Oslo, Norway, Department of Informatics, University of Oslo, 1072Oslo, Norway; Eigl, B., Cascination AG 3008 Bern, Switzerland; Regensburger, A., Siemens HealthineersForchheim  91301, Germany; Alzaga, A., Siemens HealthineersForchheim  91301, Germany; Kumar, R.P., The Intervention Centre, Oslo University Hospital Rikshospitalet 0424Oslo, Norway; Rudolph, T., Cascination AG 3008 Bern, Switzerland; Aghayan, D.L., The Intervention Centre, Oslo University Hospital Rikshospitalet 0424Oslo, Norway, Institute of Clinical Medicine, University of Oslo, 1072Oslo, Norway, Department of Surgery N1, Yerevan State Medical UniversityYerevan  0025, Armenia; Riediger, C., University Hospital Carl Gustav Carus, Technische Universität DresdenDresden  01307, Germany; Kvarnström, N., Sahlgrenska University HospitalGoteborg  41345, Sweden; Elle, O.J., The Intervention Centre, Oslo University Hospital Rikshospitalet 0424Oslo, Norway, Department of Informatics, University of Oslo, 1072Oslo, Norway; Edwin, B., The Intervention Centre, Oslo University Hospital Rikshospitalet 0424Oslo, Norway, Institute of Clinical Medicine, University of Oslo, 1072Oslo, Norway, Department of Hepato-Pancreatic-Biliary surgery 0424, Oslo University HospitalOslo, Norway","In laparoscopic liver resection, surgeons conventionally rely on anatomical landmarks detected through a laparoscope, preoperative volumetric images and laparoscopic ultrasound to compensate for the challenges of minimally invasive access. Image guidance using optical tracking and registration procedures is a promising tool, although often undermined by its inaccuracy. This study evaluates a novel surgical navigation solution that can compensate for liver deformations using an accurate and effective registration method. The proposed solution relies on a robotic C-arm to perform registration to preoperative CT/MRI image data and allows for intraoperative updates during resection using fluoroscopic images. Navigation is offered both as a 3D liver model with real-time instrument visualization, as well as an augmented reality overlay on the laparoscope camera view. Testing was conducted through a pre-clinical trial which included four porcine models. Accuracy of the navigation system was measured through two evaluation methods: liver surface fiducials reprojection and a comparison between planned and navigated resection margins. Target Registration Error with the fiducials evaluation shows that the accuracy in the vicinity of the lesion was 3.78±1.89 mm. Resection margin evaluations resulted in an overall median accuracy of 4.44 mm with a maximum error of 9.75 mm over the four subjects. The presented solution is accurate enough to be potentially clinically beneficial for surgical guidance in laparoscopic liver surgery. © 2020","Augmented reality; Image guided surgery; Intraoperative imaging; Laparoscopic liver resection; Navigation; Target registration error","3D modeling; Augmented reality; Deformation; Laparoscopy; Navigation systems; Three dimensional computer graphics; Transplantation (surgical); Anatomical landmarks; Fluoroscopic images; Laparoscopic ultrasound; Minimally invasive; Real-time instrument; Registration methods; Surgical navigation; Target registration errors; Computerized tomography; anatomic landmark; animal experiment; Article; comparative study; evaluation study; eye tracking; fluoroscopy; image analysis; image registration; laparoscopic surgery; liver resection; nonhuman; priority journal; animal; computer assisted surgery; diagnostic imaging; laparoscopy; liver; pig; surgery; three-dimensional imaging; Animals; Augmented Reality; Imaging, Three-Dimensional; Laparoscopy; Liver; Surgery, Computer-Assisted; Swine",Article,"Final","",Scopus,2-s2.0-85100147578
"Kratzer P., Bihlmaier S., Midlagajni N.B., Prakash R., Toussaint M., Mainprice J.","57191544765;57220959029;57219762477;57220956445;7006246144;55193484000;","MoGaze: A Dataset of Full-Body Motions that Includes Workspace Geometry and Eye-Gaze",2021,"IEEE Robotics and Automation Letters","6","2","9286421","367","373",,2,"10.1109/LRA.2020.3043167","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097954833&doi=10.1109%2fLRA.2020.3043167&partnerID=40&md5=95d9f972563ae7bf4c0fc7300ab2fd1c","Machine Learning and Robotics Lab, University of Stuttgart70174, Germany; Humans to Robots Motions Research Group, University of Stuttgart, Stuttgart, 70174, Germany; Learning and Intelligent Systems Lab, Berlin TU, Germany","Kratzer, P., Machine Learning and Robotics Lab, University of Stuttgart70174, Germany; Bihlmaier, S., Humans to Robots Motions Research Group, University of Stuttgart, Stuttgart, 70174, Germany; Midlagajni, N.B., Humans to Robots Motions Research Group, University of Stuttgart, Stuttgart, 70174, Germany; Prakash, R., Humans to Robots Motions Research Group, University of Stuttgart, Stuttgart, 70174, Germany; Toussaint, M., Learning and Intelligent Systems Lab, Berlin TU, Germany; Mainprice, J., Machine Learning and Robotics Lab, University of Stuttgart70174, Germany","As robots become more present in open human environments, it will become crucial for robotic systems to understand and predict human motion. Such capabilities depend heavily on the quality and availability of motion capture data. However, existing datasets of full-body motion rarely include 1) long sequences of manipulation tasks, 2) the 3D model of the workspace geometry, and 3) eye-gaze, which are all important when a robot needs to predict the movements of humans in close proximity. Hence, in this letter, we present a novel dataset of full-body motion for everyday manipulation tasks, which includes the above. The motion data was captured using a traditional motion capture system based on reflective markers. We additionally captured eye-gaze using a wearable pupil-tracking device. As we show in experiments, the dataset can be used for the design and evaluation of full-body motion prediction algorithms. Furthermore, our experiments show eye-gaze as a powerful predictor of human intent. The dataset includes 180 min of motion capture data with 1627 pick and place actions being performed. It is available at https://humans-to-robots-motion.github.io/mogaze/ MoGaze, Dataset and is planned to be extended to collaborative tasks with two humans in the near future. © 2016 IEEE.","Datasets for human motion; human-centered robotics; modeling and simulating humans","3D modeling; Agricultural robots; Eye movements; Forecasting; Motion capture; Social robots; Collaborative tasks; Design and evaluations; Full-body motions; Human environment; Manipulation task; Motion capture data; Motion capture system; Robotic systems; Motion estimation",Article,"Final","",Scopus,2-s2.0-85097954833
"Liu J., Chi J., Hu W., Wang Z.","57217315767;8702376200;57220187912;55880036500;","3D Model-Based Gaze Tracking Via Iris Features with a Single Camera and a Single Light Source",2021,"IEEE Transactions on Human-Machine Systems","51","2","9270574","75","86",,,"10.1109/THMS.2020.3035176","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097143340&doi=10.1109%2fTHMS.2020.3035176&partnerID=40&md5=9194f4490d10e5d50f8646e17082fbf1","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, 100083, China","Liu, J., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Chi, J., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Hu, W., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Wang, Z., School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, 100083, China","Traditional 3D gaze estimation methods are usually based on the models of pupil refraction and corneal reflection. These methods typically rely on multiple light sources. The 3D gaze can be estimated using single-camera-single-light-source systems only when certain user-dependent eye parameters are available a priori, which is rarely the case. This article proposes a 3D gaze estimation method which works based on iris features using a single camera and a single light source. User-dependent eye parameters involving the iris radius and the cornea radius are user-calibrated. The 3D line-of-sight is estimated from the optical axis and the positional relationship between the optical axis and the visual axis, and then optimized using a binocular stereo vision model. The feasibility and robustness of the proposed method are assessed by simulations and practical experiments. The system configuration required by the method is simpler than that required by the state-of-the-art methods, which shows significant potential value, especially with regard to mobile device applications. © 2013 IEEE.","3D gaze estimation; iris radius; kappa angle; single-camera-single-light-source","3D modeling; Cameras; Light sources; Stereo image processing; Stereo vision; Binocular stereo vision; Corneal reflection; Light-source systems; Mobile device applications; Multiple light source; Positional relationship; State-of-the-art methods; System configurations; Eye tracking",Article,"Final","",Scopus,2-s2.0-85097143340
"Liu L., Peng N.","57221211775;57216951697;","Evaluation of user concentration in ubiquitous and cognitive artificial intelligence-assisted English online guiding system integrating face and eye movement detection",2021,"International Journal of Communication Systems","34","6","e4580","","",,,"10.1002/dac.4580","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089483044&doi=10.1002%2fdac.4580&partnerID=40&md5=fddc642803f2ae023096627bd3077e2a","Department of Basic Teaching and Research, East University of Heilongjiang, Harbin, China","Liu, L., Department of Basic Teaching and Research, East University of Heilongjiang, Harbin, China; Peng, N., Department of Basic Teaching and Research, East University of Heilongjiang, Harbin, China","In recent years, artificial intelligence technology has made significant breakthroughs. Big data, cloud computing, speech recognition, deep learning, and so on have become new hot spots after the Internet of things. The rapid development of artificial intelligence technology promotes the innovation of foreign language teaching ideas and learning methods. The new ecology and new paradigm of foreign language guiding must be reconstructed. The biggest difference between the Internet of things and the Internet is that the Internet of things is directly connected with all kinds of sensors. It does not need people to input information through the keyboard but automatically obtain information and carry out automatic processing. The deep integration of smart classroom and English online teaching will help to promote the reform and innovation of English teaching and provide a new way for the development of English online teaching. In this paper, we analyze the fusion of face recognition technology and eye tracking system and analyze the user focus evaluation theory in the Internet of things environment. Based on the above technology, this paper proposes an online English teaching user focus evaluation system in the context of artificial intelligence. © 2020 John Wiley & Sons, Ltd.","artificial intelligence; eye movement system; face recognition; network users; online teaching","Computation theory; Deep learning; Eye movements; Eye tracking; Face recognition; Internet of things; Learning systems; Speech recognition; Teaching; Artificial intelligence technologies; Automatic processing; Deep integrations; Evaluation of users; Eye tracking systems; Face recognition technologies; Foreign language teaching; Movement detection; E-learning",Article,"Final","",Scopus,2-s2.0-85089483044
"Gapi K.T., Magbitang R.M.G., Villaverde J.F.","57266871300;57267728100;57195431920;","Classification of Attentiveness on Virtual Classrooms using Deep Learning for Computer Vision",2021,"ACM International Conference Proceeding Series",,,,"34","39",,,"10.1145/3460238.3460244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115384175&doi=10.1145%2f3460238.3460244&partnerID=40&md5=b084081ced16c4bd382d740cf3e64a30","School of Electrical, Electronics and Computer Engineering, Mapua University, Philippines","Gapi, K.T., School of Electrical, Electronics and Computer Engineering, Mapua University, Philippines; Magbitang, R.M.G., School of Electrical, Electronics and Computer Engineering, Mapua University, Philippines; Villaverde, J.F., School of Electrical, Electronics and Computer Engineering, Mapua University, Philippines","Nowadays, virtual classrooms are highly encouraged due to the COVID-19 pandemic. This could be a disadvantage because some students might not really be engaged with this kind of setup. This study presents a system for classifying level of attentiveness on virtual classrooms using deep learning for computer vision. The study confined in the development of the technology for classifying attentiveness itself, the integration of the system to virtual classrooms is not included in the scope. The criteria for the classification include the prediction of droopy corners of mouth facial cue, hanging eyelid facial cue, eye state, and eye gaze. The software of the system used the combinations of Convolutional Neural Network (CNN) models, Dlib, and OpenCV library. After evaluation, the system was able to successfully classify attentiveness of three classes with an overall accuracy of 83.33%. © 2021 ACM.","Computer vision; Convolutional Neural Network models; Deep learning; Dlib library; Facial cue; OpenCV","Computer aided instruction; Convolution; Convolutional neural networks; Deep learning; Digital libraries; E-learning; Convolutional neural network; Convolutional neural network model; Deep learning; Dlib library; Eye-gaze; Facial cue; Neural network model; Opencv; Overall accuracies; Virtual Classroom; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85115384175
"Heck M., Edinger J., Bünemann J., Becker C.","57205548572;56178122500;57222469151;55683104700;","Exploring Gaze-Based Prediction Strategies for Preference Detection in Dynamic Interface Elements",2021,"CHIIR 2021 - Proceedings of the 2021 Conference on Human Information Interaction and Retrieval",,,,"129","139",,3,"10.1145/3406522.3446013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102755581&doi=10.1145%2f3406522.3446013&partnerID=40&md5=51848aba23d2f13a5b203c563efefa39","University of Mannheim, Mannheim, Germany; University of Hamburg, Hamburg, Germany","Heck, M., University of Mannheim, Mannheim, Germany; Edinger, J., University of Hamburg, Hamburg, Germany; Bünemann, J., University of Mannheim, Mannheim, Germany; Becker, C., University of Mannheim, Mannheim, Germany","Digitization is currently infiltrating all daily processes, forcing casual computer users to become acquainted with unfamiliar tools. In order to avoid overstraining these users, simplified interfaces that are reduced to the functionality and content which are relevant to the individual userare imperative. Gaze-contingent systems thus monitor viewing behavior during natural system interactions to predict relevant interface elements. The prediction performance is highly dependent on theunderlying features and algorithm, especially when the interface consist of dynamic elements such as videos. In this paper, we conduct two studies with a total of 233 subjects in which we record theviewers' gaze while watching videos. We then compare the quality of preference predictions for video elements of majority voting to the performance of machine learning. Our results indicate that (1)majority voting can predict preferences with an accuracy of up to 73% (66%) for two (four) elements, (2) machine learning improves the performance to 82% (74%), (3) prediction accuracy depends on the strength of the user's preference for an element, and (4) we can rank preferences for individual elements. © 2021 ACM.","adaptive user interfaces; eye tracking; preference prediction","Machine learning; Dynamic elements; Dynamic interface; Gaze-contingent; Interface elements; Natural systems; Prediction accuracy; Prediction performance; User's preferences; Forecasting",Conference Paper,"Final","",Scopus,2-s2.0-85102755581
"Pawar P., McManus B., Anthony T., Stavrinos D.","57215929340;56442149600;24922913500;24177608100;","Hazard detection in driving simulation using deep learning",2021,"Conference Proceedings - IEEE SOUTHEASTCON","2021-March",,"9401875","","",,,"10.1109/SoutheastCon45413.2021.9401875","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105008858&doi=10.1109%2fSoutheastCon45413.2021.9401875&partnerID=40&md5=2537856ab37c2727c3d351bea32c9cee","University of Alabama at Birmingham, Dept. of Psychology, Birmingham, United States; University of Alabama at Birmingham, Dept. of Electrical and Computer Engineering, Birmingham, United States","Pawar, P., University of Alabama at Birmingham, Dept. of Psychology, Birmingham, United States; McManus, B., University of Alabama at Birmingham, Dept. of Psychology, Birmingham, United States; Anthony, T., University of Alabama at Birmingham, Dept. of Electrical and Computer Engineering, Birmingham, United States; Stavrinos, D., University of Alabama at Birmingham, Dept. of Psychology, Birmingham, United States","The advancement in Big Data and Analytics has led to a data driven approach in research and development. Many traffic research studies use experimental simulators for data acquisition. This enables researchers to collect large amounts of data from human test subjects. Some research practices involve manual procedures to process participant data. On a large scale this is very labor intensive, highly time consuming and expensive. In these cases, automated solutions to perform these tasks can be implemented using deep learning techniques. This paper considers alternative approaches to process data from a longitudinal research study that uses a high-fidelity driving simulator with integrated eye tracking technology. In the simulation, specific roadway hazard scenarios are programmed to replicate realistic driving hazards. These hazard elements are then annotated by a researcher for behavioral analysis. This process is typically done manually on a frame-by-frame level with the help of software. The process is then repeated for every participant. It is a long and tedious task requiring long hours and high labor costs to be completed reliably. A deep learning solution is implemented to automate the hazard detection and annotation process. This method is tested across a small subset of participant data to assess the performance of this solution. With this method, the labor intensive process can be mitigated and improve the overall time and labor efficiency and to ensure timely dissemination of results from this research study. © 2021 IEEE.","Deep Learning; Driving Hazard detection; Driving Simulation; Translational Research","Advanced Analytics; Behavioral research; Data acquisition; Eye tracking; Hazards; Wages; Behavioral analysis; Data-driven approach; Eye tracking technologies; Labor intensive process; Large amounts of data; Learning techniques; Longitudinal research; Research and development; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85105008858
"Dang T., Bhattacharya S., Crumbley J.","57223129879;56611149100;57223133006;","A review study on the use of oculometry in the assessment of driver cognitive states",2021,"Conference Proceedings - IEEE SOUTHEASTCON","2021-March",,"9401905","","",,,"10.1109/SoutheastCon45413.2021.9401905","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104965043&doi=10.1109%2fSoutheastCon45413.2021.9401905&partnerID=40&md5=7c4d3364d2d6a7b1118c444a8085b6c7","Electrical Engineering Technology, Kennesaw State University, Marietta, United States","Dang, T., Electrical Engineering Technology, Kennesaw State University, Marietta, United States; Bhattacharya, S., Electrical Engineering Technology, Kennesaw State University, Marietta, United States; Crumbley, J., Electrical Engineering Technology, Kennesaw State University, Marietta, United States","Roadway fatalities are increasing with a growing population and need for reliable transportation. These fatalities can be mitigated by incorporating driver state information with current Driver Safety Systems (DSS). There are primarily two driver cognitive states: Focused and Distracted. These states can be predicted using Machine Learning algorithms (ML) such as Support Vector Machines (SVM), Adaptive Boosting (AdaBoost), and Artificial Neural Networks (ANN) using extracted biomedical features like Electroencephalography (EEG), Electromyography (EMG), Heart Rate and Eye Tracking. This literature review summarizes all biomedical signals that are used in the assessment of driver cognitive states. A thorough literature review in this field identifies eye tracking as the most efficient and quick technique of real time driver state identification. Hence, this paper outlines the latest techniques in eye tracking using oculometry. This review paper also highlights unique ocular feature extraction techniques that can be extremely useful for future researches conducted in the field of driver state recognition. © 2021 IEEE.","AdaBoost; Artificial Neural Network; Driver state; Eye-tracking; Oculometry; PERCLOS; Pupillometry; Support Vector Machine","Adaptive boosting; Artificial heart; Bioelectric phenomena; Bioinformatics; Electroencephalography; Electrophysiology; Neural networks; Support vector machines; Biomedical signal; Cognitive state; Current drivers; Feature extraction techniques; Literature reviews; State identification; State information; State recognition; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85104965043
"Liu S., Zhou X.-D., Jiang X., Wu H., Shi Y.","57221491395;57191432161;57251548800;57221706335;57213417833;","Face Shows Your Intention: Visual Search Based on Full-face Gaze Estimation with Channel-spatial Attention",2021,"ACM International Conference Proceeding Series","PartF171546",,,"76","81",,,"10.1145/3461353.3461362","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114557535&doi=10.1145%2f3461353.3461362&partnerID=40&md5=43f48a076d704969d9a690f351f65ff5","Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Chongqing University of Education, China","Liu, S., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Zhou, X.-D., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Jiang, X., Chongqing University of Education, China; Wu, H., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Shi, Y., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China","Visual search is the process that humans use visual perception to recognize targets of interest among multitudinous objects, which is a challenging research topic in computer vision. In contrast to previous works that take the overt gaze signal as input to predict the target of visual search with computational models, we proposed a visual search network based on full-face gaze estimation with channel-spatial mechanism, which can directly predict the user's search objects from the full-face images without extra obtaining the prohibitive intermediate gaze data. We seamlessly integrate the gaze information generated by the full-face gaze estimation module and the semantic information of the scene image into the visual search network that can directly infer the user's search intention. We demonstrate the effectiveness of our method for visual search task in real-world settings, and illustrate that directions for future research on full-face based human visual cognition. © 2021 Association for Computing Machinery. All rights reserved.","Attention mechanism; Eye tracking; Gaze estimation; Machine learning; Visual search","Semantics; Computational model; Real world setting; Search intentions; Semantic information; Spatial attention; Spatial mechanism; Targets of interest; Visual perception; Artificial intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85114557535
"Kapp S., Barz M., Mukhametov S., Sonntag D., Kuhn J.","57195483291;57189847803;57211779287;12241487800;55984409000;","Arett: Augmented reality eye tracking toolkit for head mounted displays",2021,"Sensors","21","6","2234","","",,5,"10.3390/s21062234","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102790558&doi=10.3390%2fs21062234&partnerID=40&md5=44063cb55782a4842be02ef315ce4c64","Department of Physics, Technische Universität Kaiserslautern, Erwin-Schrödinger-Str. 46, Kaiserslautern, 67663, Germany; German Research Center for Artificial Intelligence (DFKI), Interactive Machine Learning Department, Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany; Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany","Kapp, S., Department of Physics, Technische Universität Kaiserslautern, Erwin-Schrödinger-Str. 46, Kaiserslautern, 67663, Germany; Barz, M., German Research Center for Artificial Intelligence (DFKI), Interactive Machine Learning Department, Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany, Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany; Mukhametov, S., Department of Physics, Technische Universität Kaiserslautern, Erwin-Schrödinger-Str. 46, Kaiserslautern, 67663, Germany; Sonntag, D., German Research Center for Artificial Intelligence (DFKI), Interactive Machine Learning Department, Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany, Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany; Kuhn, J., Department of Physics, Technische Universität Kaiserslautern, Erwin-Schrödinger-Str. 46, Kaiserslautern, 67663, Germany","Currently an increasing number of head mounted displays (HMD) for virtual and augmented reality (VR/AR) are equipped with integrated eye trackers. Use cases of these integrated eye trackers include rendering optimization and gaze-based user interaction. In addition, visual attention in VR and AR is interesting for applied research based on eye tracking in cognitive or educational sciences for example. While some research toolkits for VR already exist, only a few target AR scenarios. In this work, we present an open-source eye tracking toolkit for reliable gaze data acquisition in AR based on Unity 3D and the Microsoft HoloLens 2, as well as an R package for seamless data analysis. Furthermore, we evaluate the spatial accuracy and precision of the integrated eye tracker for fixation targets with different distances and angles to the user (n = 21). On average, we found that gaze estimates are reported with an angular accuracy of 0.83 degrees and a precision of 0.27 degrees while the user is resting, which is on par with state-of-the-art mobile eye trackers. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Accuracy; Augmented reality; Eye tracking; Precision; Toolkit","Augmented reality; Behavioral research; Data acquisition; Helmet mounted displays; Educational science; Head mounted displays; Rendering optimizations; Spatial accuracy; State of the art; User interaction; Virtual and augmented reality; Visual Attention; Eye tracking; virtual reality; Augmented Reality; Eye-Tracking Technology; Smart Glasses; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85102790558
"Hu Z.","57208101391;","[DC] Eye fixation forecasting in task-oriented virtual reality",2021,"Proceedings - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2021",,,"9419161","707","708",,,"10.1109/VRW52623.2021.00236","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105966888&doi=10.1109%2fVRW52623.2021.00236&partnerID=40&md5=f1c5e70c67e30187665a531cc1408f81","Peking University, China","Hu, Z., Peking University, China","In immersive virtual reality (VR), users' visual attention is crucial for many important applications, including VR content design, gaze-based interaction, and gaze-contingent rendering. Especially, information on users' future eye fixations is key for intelligent user interfaces and has significant relevance for many areas, such as visual attention enhancement, dynamic event triggering, and human-computer interaction. However, previous works typically focused on free-viewing conditions and paid less attention to task-oriented attention. This paper aims at forecasting users' eye fixations in task-oriented virtual reality. To this end, a VR eye tracking dataset that corresponds to different users performing a visual search task in immersive virtual environments is built. A comprehensive analysis of users' eye fixations is performed based on the collected data. The analysis reveals that eye fixations are correlated with users' historical gaze positions, task-related objects, saliency information of the VR content, and head rotation velocities. Based on this analysis, a novel learning-based model is proposed to forecast users' eye fixations in the near future in immersive virtual environments. © 2021 IEEE.","Convolutional neural network; Eye tracking; Deep learning; Fixation forecasting; Virtual reality; Visual attention; Visual search","Abstracting; Behavioral research; Eye tracking; Forecasting; Human computer interaction; User interfaces; Comprehensive analysis; Gaze-based interaction; Immersive virtual environments; Immersive virtual reality; Intelligent User Interfaces; Learning Based Models; Viewing conditions; Visual Attention; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85105966888
"Ge S., Zhang C., Li S., Zeng D., Tao D.","56226219300;57210580772;57215343722;10538969100;57218623508;","Cascaded Correlation Refinement for Robust Deep Tracking",2021,"IEEE Transactions on Neural Networks and Learning Systems","32","3","9069312","1276","1288",,,"10.1109/TNNLS.2020.2984256","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102302367&doi=10.1109%2fTNNLS.2020.2984256&partnerID=40&md5=4fcac7f645908d3ca6593341931e5ed0","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication, Shanghai Institute of Advanced Communication and Data Science, Shanghai University, Shanghai, China; UBTECH Sydney Artificial Intelligence Centre, The University of Sydney, Darlington, NSW, Australia; School of Cyber Security, University of Chinese Academy of Sciences, Beijing, 100049, China; Faculty of Engineering, School of Computer Science, The University of Sydney, Darlington, NSW  2008, Australia","Ge, S., Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Zhang, C., Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China, School of Cyber Security, University of Chinese Academy of Sciences, Beijing, 100049, China; Li, S., Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China, School of Cyber Security, University of Chinese Academy of Sciences, Beijing, 100049, China; Zeng, D., Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication, Shanghai Institute of Advanced Communication and Data Science, Shanghai University, Shanghai, China; Tao, D., UBTECH Sydney Artificial Intelligence Centre, The University of Sydney, Darlington, NSW, Australia, Faculty of Engineering, School of Computer Science, The University of Sydney, Darlington, NSW  2008, Australia","Recent deep trackers have shown superior performance in visual tracking. In this article, we propose a cascaded correlation refinement approach to facilitate the robustness of deep tracking. The core idea is to address accurate target localization and reliable model update in a collaborative way. To this end, our approach cascades multiple stages of correlation refinement to progressively refine target localization. Thus, the localized object could be used to learn an accurate on-the-fly model for improving the reliability of model update. Meanwhile, we introduce an explicit measure to identify the tracking failure and then leverage a simple yet effective look-back scheme to adaptively incorporate the initial model and on-the-fly model to update the tracking model. As a result, the tracking model can be used to localize the target more accurately. Extensive experiments on OTB2013, OTB2015, VOT2016, VOT2018, UAV123, and GOT-10k demonstrate that the proposed tracker achieves the best robustness against the state of the arts. © 2012 IEEE.","Cascaded refinement; correlation filter; deep learning; visual tracking","Artificial intelligence; Model updates; Multiple stages; Reliable models; State of the art; Target localization; Tracking failure; Tracking models; Visual Tracking; Computer networks; article; eye tracking; reliability",Article,"Final","",Scopus,2-s2.0-85102302367
"Nezami F.N., Wächter M.A., Maleki N., Spaniol P., Kühne L.M., Haas A., Pingel J.M., Tiemann L., Nienhaus F., Keller L., König S.U., König P., Pipa G.","56208678400;57200320147;57221907767;57221918105;57221913722;57221910901;57221905698;57221915192;57221907060;57221915644;56274699800;57193206133;7801435498;","Westdrive x loopar: An open‐access virtual reality project in unity for evaluating user interaction methods during takeover requests",2021,"Sensors","21","5","1879","1","13",,1,"10.3390/s21051879","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102113135&doi=10.3390%2fs21051879&partnerID=40&md5=6d50b709f28c1717ed67104b99e320e3","Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; Center of Experimental Medicine, Department of Neurophysiology and Pathophysiology, University Medical Center Hamburg‐Eppendorf, Hamburg, 20251, Germany","Nezami, F.N., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; Wächter, M.A., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; Maleki, N., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; Spaniol, P., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; Kühne, L.M., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; Haas, A., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; Pingel, J.M., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; Tiemann, L., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; Nienhaus, F., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; Keller, L., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; König, S.U., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany; König, P., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany, Center of Experimental Medicine, Department of Neurophysiology and Pathophysiology, University Medical Center Hamburg‐Eppendorf, Hamburg, 20251, Germany; Pipa, G., Institute of Cognitive Science, University of Osnabrück, Osnabrück, 49090, Germany","With the further development of highly automated vehicles, drivers will engage in non-related tasks while being driven. Still, drivers have to take over control when requested by the car. Here, the question arises, how potentially distracted drivers get back into the control‐loop quickly and safely when the car requests a takeover. To investigate effective human–machine interactions, a mobile, versatile, and cost‐efficient setup is needed. Here, we describe a virtual reality toolkit for the Unity 3D game engine containing all the necessary code and assets to enable fast adaptations to various human–machine interaction experiments, including closely monitoring the subject. The presented project contains all the needed functionalities for realistic traffic behavior, cars, pedestrians, and a large, open‐source, scriptable, and modular VR environment. It covers roughly 25 km2, a package of 125 animated pedestrians, and numerous vehicles, including motorbikes, trucks, and cars. It also contains all the needed nature assets to make it both highly dynamic and realistic. The presented repository contains a C++ library made for LoopAR that enables force feedback for gaming steering wheels as a fully supported component. It also includes all necessary scripts for eye‐tracking in the used devices. All the main functions are integrated into the graphical user interface of the Unity® editor or are available as prefab variants to ease the use of the embedded functionalities. This project’s primary purpose is to serve as an open‐access, cost‐efficient toolkit that enables interested researchers to conduct realistic virtual reality research studies without costly and immobile simulators. To ensure the accessibility and usability of the mentioned toolkit, we performed a user experience report, also included in this paper. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Human– machine interaction; Out‐of‐the‐loop unfamiliarity (OOTLU) autonomous driving; Takeover request (ToR); VR research","C++ (programming language); Graphical user interfaces; Patient monitoring; Pedestrian safety; User experience; 3D game engines; Automated vehicles; Experience report; Fast adaptations; Realistic traffics; Research studies; User interaction; Virtual reality toolkits; Virtual reality; adaptation; car; human; motor vehicle; pedestrian; virtual reality; Adaptation, Physiological; Automobiles; Humans; Motor Vehicles; Pedestrians; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85102113135
"Kim H., Kwon S., Lee S.","57215855670;54415852000;57211724385;","Nra-net—neg-region attention network for salient object detection with gaze tracking",2021,"Sensors","21","5","1753","1","18",,1,"10.3390/s21051753","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101902574&doi=10.3390%2fs21051753&partnerID=40&md5=2dafbf3080812ab1a40cea74aaebf731","Department of Plasma Bio Display, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea; Department of Smart Convergence, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea; Ingenium College of Liberal Arts, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea","Kim, H., Department of Plasma Bio Display, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea; Kwon, S., Department of Smart Convergence, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea; Lee, S., Ingenium College of Liberal Arts, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea","In this paper, we propose a detection method for salient objects whose eyes are focused on gaze tracking; this method does not require a device in a single image. A network was constructed using Neg-Region Attention (NRA), which predicts objects with a concentrated line of sight using deep learning techniques. The existing deep learning-based method has an autoencoder structure, which causes feature loss during the encoding process of compressing and extracting features from the image and the decoding process of expanding and restoring. As a result, a feature loss occurs in the area of the object from the detection results, or another area is detected as an object. The proposed method, that is, NRA, can be used for reducing feature loss and emphasizing object areas with encoders. After separating positive and negative regions using the exponential linear unit activation function, converted attention was performed for each region. The attention method provided without using the backbone network emphasized the object area and suppressed the background area. In the experimental results, the proposed method showed higher detection results than the conventional methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Autoencoder; Convolutional neural network; Deep learning; Gaze tracking; Image process-ing; Salient object detection","Deep learning; Eye tracking; Learning systems; Object tracking; Signal encoding; Activation functions; Back-bone network; Conventional methods; Detection methods; Extracting features; Learning techniques; Learning-based methods; Salient object detection; Object detection; article; attention network; autoencoder; convolutional neural network; deep learning; gaze; human; human experiment; image processing; Eye-Tracking Technology; Neural Networks, Computer",Article,"Final","",Scopus,2-s2.0-85101902574
"Katzakis N., Chen L., Ariza O., Teather R.J., Steinicke F.","35786220900;36019872900;57197830700;24588246800;8883314100;","Evaluation of 3D Pointing Accuracy in the Fovea and Periphery in Immersive Head-Mounted Display Environments",2021,"IEEE Transactions on Visualization and Computer Graphics","27","3","8869741","1929","1936",,,"10.1109/TVCG.2019.2947504","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100433915&doi=10.1109%2fTVCG.2019.2947504&partnerID=40&md5=f37f32030807fe81477f66826b5ea05b","Informatics, University of Hamburg, Hamburg, 20146, Germany; Department of Psychology, Peking University, Beijing, 100871, China; School of Information Technology, Carleton University, Ottawa, ON  K1S 5B6, Canada","Katzakis, N., Informatics, University of Hamburg, Hamburg, 20146, Germany; Chen, L., Department of Psychology, Peking University, Beijing, 100871, China; Ariza, O., Informatics, University of Hamburg, Hamburg, 20146, Germany; Teather, R.J., School of Information Technology, Carleton University, Ottawa, ON  K1S 5B6, Canada; Steinicke, F., Informatics, University of Hamburg, Hamburg, 20146, Germany","The coupling between perception and action has seldom been explored in sophisticated motor behaviour such as 3D pointing. In this study, we investigated how 3D pointing accuracy, measured by a depth estimation task, could be affected by the target appearing in different visual eccentricities. Specifically, we manipulated the visual eccentricity of the target and its depth in virtual reality. Participants wore a head-mounted-display with an integrated eye-tracker and docked a cursor into a target. We adopted a within-participants factorial design with three variables. The first variable is Eccentricity: the location of the target on one of five horizontal eccentricities (left far periphery, left near periphery, foveal, right near periphery and right far periphery). The second variable is Depth at three levels and the third variable is Feedback Loop with two levels: open/closed. Eccentricity is refactored into Motion Correspondence between the starting location of the cursor and the target location with four levels: periphery to fovea, fovea to periphery, periphery to periphery, fovea to fovea. The results showed that the pointing accuracy is modulated mainly by the target locations rather than the initial locations of the effector (hand). Visible feedback during pointing improved performance. © 2020 IEEE.","3D; periphery; pointing; Virtual reality; vision; visual feedback","Eye tracking; Helmet mounted displays; Location; Depth Estimation; Factorial design; Head mounted displays; Motion correspondences; Motor behaviours; Perception and actions; Pointing accuracy; Target location; Three dimensional displays",Article,"Final","",Scopus,2-s2.0-85100433915
"Dmitriev K., Marino J., Baker K., Kaufman A.E.","57190619141;22734901400;55258421100;34769848900;","Visual Analytics of a Computer-Aided Diagnosis System for Pancreatic Lesions",2021,"IEEE Transactions on Visualization and Computer Graphics","27","3","8868209","2174","2185",,2,"10.1109/TVCG.2019.2947037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100400095&doi=10.1109%2fTVCG.2019.2947037&partnerID=40&md5=956f2562027d2af04fe78b4dc79dfcc7","Computer Science Department, Stony Brook University, Stony Brook, NY  11794, United States; Radiology Department, Stony Brook Medicine, Stony Brook, NY  11794, United States","Dmitriev, K., Computer Science Department, Stony Brook University, Stony Brook, NY  11794, United States; Marino, J., Computer Science Department, Stony Brook University, Stony Brook, NY  11794, United States; Baker, K., Radiology Department, Stony Brook Medicine, Stony Brook, NY  11794, United States; Kaufman, A.E., Computer Science Department, Stony Brook University, Stony Brook, NY  11794, United States","Machine learning is a powerful and effective tool for medical image analysis to perform computer-aided diagnosis (CAD). Having great potential in improving the accuracy of a diagnosis, CAD systems are often analyzed in terms of the final accuracy, leading to a limited understanding of the internal decision process, impossibility to gain insights, and ultimately to skepticism from clinicians. We present a visual analytics approach to uncover the decision-making process of a CAD system for classifying pancreatic cystic lesions. This CAD algorithm consists of two distinct components: random forest (RF), which classifies a set of predefined features, including demographic features, and a convolutional neural network (CNN), which analyzes radiological (imaging) features of the lesions. We study the class probabilities generated by the RF and the semantical meaning of the features learned by the CNN. We also use an eye tracker to better understand which radiological features are particularly useful for a radiologist to make a diagnosis and to quantitatively compare with the features that lead the CNN to its final classification decision. Additionally, we evaluate the effects and benefits of supplying the CAD system with a case-based visual aid in a second-reader setting. © 2020 IEEE.","abdominal imaging; cancer; computer-aided diagnosis; machine learning; pancreas; Radiomics","Computer aided analysis; Computer aided instruction; Convolutional neural networks; Decision making; Decision trees; Eye tracking; Medical imaging; Visualization; Class probabilities; Classification decision; Computer aided diagnosis systems; Computer Aided Diagnosis(CAD); Decision making process; Demographic features; Pancreatic lesions; Radiological features; Computer aided diagnosis",Article,"Final","",Scopus,2-s2.0-85100400095
"Pan Y., Mitchell K.","55549956700;7202163186;","Improving VIP viewer gaze estimation and engagement using adaptive dynamic anamorphosis",2021,"International Journal of Human Computer Studies","147",,"102563","","",,,"10.1016/j.ijhcs.2020.102563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096183137&doi=10.1016%2fj.ijhcs.2020.102563&partnerID=40&md5=1452cd94a94e501cc3a9c516c00e8799","Disney Research, Los Angeles, United States; Edinburgh Napier University, 10 Colinton Road, Edinburgh, EH10 5DT, United Kingdom","Pan, Y., Disney Research, Los Angeles, United States; Mitchell, K., Disney Research, Los Angeles, United States, Edinburgh Napier University, 10 Colinton Road, Edinburgh, EH10 5DT, United Kingdom","Anamorphosis for 2D displays can provide viewer centric perspective viewing, enabling 3D appearance, eye contact and engagement, by adapting dynamically in real time to a single moving viewer's viewpoint, but at the cost of distorted viewing for other viewers. We present a method for constructing non-linear projections as a combination of anamorphic rendering of selective objects whilst reverting to normal perspective rendering of the rest of the scene. Our study defines a scene consisting of five characters, with one of these characters selectively rendered in anamorphic perspective. We conducted an evaluation experiment and demonstrate that the tracked viewer centric imagery for the selected character results in an improved gaze and engagement estimation. Critically, this is performed without sacrificing the other viewers’ viewing experience. In addition, we present findings on the perception of gaze direction for regularly viewed characters located off-center to the origin, where perceived gaze shifts from being aligned to misalignment increasingly as the distance between viewer and character increases. Finally, we discuss different viewpoints and the spatial relationship between objects. © 2020","Display; Dynamic anamorphosis; Gaze","Image enhancement; Adaptive dynamics; Evaluation experiments; Eye contact; Gaze direction; Gaze estimation; Gaze shifts; Nonlinear projections; Spatial relationships; Rendering (computer graphics)",Article,"Final","",Scopus,2-s2.0-85096183137
"Tang C., Qin P., Zhang J.","57222002578;57222007264;37361641100;","Robust template adjustment siamese network for object visual tracking",2021,"Sensors","21","4","1466","1","17",,,"10.3390/s21041466","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100920305&doi=10.3390%2fs21041466&partnerID=40&md5=275c1d986861a930ce603343ea94e7b4","Key Laboratory of Optical Engineering, Chinese Academy of Sciences, Chengdu, 610200, China; Institute of Optics and Electronics, Chinese Academy of Sciences, Chengdu, 610200, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 100049, China","Tang, C., Key Laboratory of Optical Engineering, Chinese Academy of Sciences, Chengdu, 610200, China, Institute of Optics and Electronics, Chinese Academy of Sciences, Chengdu, 610200, China, School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 100049, China; Qin, P., Institute of Optics and Electronics, Chinese Academy of Sciences, Chengdu, 610200, China, School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 100049, China; Zhang, J., Institute of Optics and Electronics, Chinese Academy of Sciences, Chengdu, 610200, China","Most of the existing trackers address the visual tracking problem by extracting an appearance template from the first frame, which is used to localize the target in the current frame. Unfor-tunately, they typically face the model degeneration challenge, which easily results in model drift and target loss. To address this issue, a novel Template Adjustment Siamese Network (TA-Siam) is proposed in this paper. The proposed framework TA-Siam consists of two simple subnetworks: The template adjustment subnetwork for feature extraction and the classification-regression subnetwork for bounding box prediction. The template adjustment module adaptively uses the feature of sub-sequent frames to adjust the current template. It makes the template adapt to the target appearance variation of long-term sequence and effectively overcomes model drift problem of Siamese net-works. In order to reduce classification errors, the rhombus labels are proposed in our TA-Siam. For more efficient learning and faster convergence, our proposed tracker uses a more effective regression loss in the training process. Extensive experiments and comparisons with trackers are con-ducted on the challenging benchmarks including VOT2016, VOT2018, OTB50, OTB100, GOT-10K, and LaSOT. Our TA-Siam achieves state-of-the-art performance at the speed of 45 FPS. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Anchor-free regression; Classification labels; Siamese network; Template adjustment; Visual tracking","Classification errors; Classification regression; Current frame; Efficient learning; Faster convergence; State-of-the-art performance; Training process; Visual Tracking; Object tracking; article; eye tracking; feature extraction; learning; prediction; velocity",Article,"Final","",Scopus,2-s2.0-85100920305
"Yeamkuan S., Chamnongthai K.","57221865355;57202765861;","3d point-of-intention determination using a multimodal fusion of hand pointing and eye gaze for a 3d display",2021,"Sensors (Switzerland)","21","4","1155","1","31",,2,"10.3390/s21041155","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100516851&doi=10.3390%2fs21041155&partnerID=40&md5=85b3e58079dcc84285ca42ef36bb5936","Department of Electronic and Telecommunication Engineering, Faculty of Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, 10140, Thailand","Yeamkuan, S., Department of Electronic and Telecommunication Engineering, Faculty of Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, 10140, Thailand; Chamnongthai, K., Department of Electronic and Telecommunication Engineering, Faculty of Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, 10140, Thailand","This paper proposes a three-dimensional (3D) point-of-intention (POI) determination method using multimodal fusion between hand pointing and eye gaze for a 3D virtual display. In the method, the finger joint forms of the pointing hand sensed by a Leap Motion sensor are first detected as pointing intention candidates. Subsequently, differences with neighboring frames, which should be during hand pointing period, are checked by AND logic with the hand-pointing intention candidates. A crossing point between the eye gaze and hand pointing lines is finally decided by the closest distance concept. In order to evaluate the performance of the proposed method, experiments with ten participants, in which they looked at and pointed at nine test points for approximately five second each, were performed. The experimental results show the proposed method measures 3D POIs at 75 cm, 85 cm, and 95 cm with average distance errors of 4.67%, 5.38%, and 5.71%, respectively. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Eye tracking; Hand recognition; Hand tracking; Multimodal systems; Sensor fusion","Motion sensors; Average Distance; Closest distance; Crossing point; Determination methods; Finger joints; Multi-modal fusion; Threedimensional (3-d); Virtual displays; Three dimensional displays; behavior; eye fixation; hand; human; Fixation, Ocular; Hand; Humans; Intention",Article,"Final","",Scopus,2-s2.0-85100516851
"Sevil M., Rashid M., Hajizadeh I., Askari M.R., Hobbs N., Brandt R., Park M., Quinn L., Cinar A.","57190946349;57125488800;16745191800;57211430055;57197779873;57195105079;57192005806;57203250702;57203200789;","Discrimination of simultaneous psychological and physical stressors using wristband biosignals",2021,"Computer Methods and Programs in Biomedicine","199",,"105898","","",,2,"10.1016/j.cmpb.2020.105898","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098080023&doi=10.1016%2fj.cmpb.2020.105898&partnerID=40&md5=2e1e047d671dd8a7e5c959deddf8eefb","Department of Biomedical Engineering, Illinois Institute of Technology, Chicago, IL, 60616, United States; Department of Chemical and Biological Engineering, Illinois Institute of Technology, Chicago, IL, 60616, United States; College of Nursing, University of Illinois at ChicagoIL, 60616, United States","Sevil, M., Department of Biomedical Engineering, Illinois Institute of Technology, Chicago, IL, 60616, United States; Rashid, M., Department of Chemical and Biological Engineering, Illinois Institute of Technology, Chicago, IL, 60616, United States; Hajizadeh, I., Department of Chemical and Biological Engineering, Illinois Institute of Technology, Chicago, IL, 60616, United States; Askari, M.R., Department of Chemical and Biological Engineering, Illinois Institute of Technology, Chicago, IL, 60616, United States; Hobbs, N., Department of Biomedical Engineering, Illinois Institute of Technology, Chicago, IL, 60616, United States; Brandt, R., Department of Biomedical Engineering, Illinois Institute of Technology, Chicago, IL, 60616, United States; Park, M., College of Nursing, University of Illinois at ChicagoIL, 60616, United States; Quinn, L., College of Nursing, University of Illinois at ChicagoIL, 60616, United States; Cinar, A., Department of Biomedical Engineering, Illinois Institute of Technology, Chicago, IL, 60616, United States, Department of Chemical and Biological Engineering, Illinois Institute of Technology, Chicago, IL, 60616, United States","Background and objective: In this work, we address the problem of detecting and discriminating acute psychological stress (APS) in the presence of concurrent physical activity (PA) using wristband biosignals. We focused on signals available from wearable devices that can be worn in daily life because the ultimate objective of this work is to provide APS and PA information in real-time management of chronic conditions such as diabetes by automated personalized insulin delivery. Monitoring APS noninvasively throughout free-living conditions remains challenging because the responses to APS and PA of many physiological variables measured by wearable devices are similar. Methods: Various classification algorithms are compared to simultaneously detect and discriminate the PA (sedentary state, treadmill running, and stationary bike) and the type of APS (non-stress state, mental stress, and emotional anxiety). The impact of APS inducements is verified with commonly used self-reported questionnaires (The State-Trait Anxiety Inventory (STAI)). To aid the classification algorithms, novel features are generated from the physiological variables reported by a wristband device during 117 hours of experiments involving simultaneous APS inducement and PA. We also translate the APS assessment into a quantitative metric for use in predicting the adverse outcomes. Results: An accurate classification of the concurrent PA and APS states is achieved with an overall classification accuracy of 99% for PA and 92% for APS. The average accuracy of APS detection during sedentary state, treadmill running, and stationary bike is 97.3, 94.1, and 84.5%, respectively. Conclusions: The simultaneous assessment of APS and PA throughout free-living conditions from a convenient wristband device is useful for monitoring the factors contributing to an elevated risk of acute events in people with chronic diseases like cardiovascular complications and diabetes. © 2020","Acute psychological stress; Discrimination of physical and psychological stressors; Machine learning; Physical activity; Wearable devices","Physiological models; Physiology; Risk assessment; Sporting goods; Surveys; Chronic conditions; Classification accuracy; Classification algorithm; Physical activity; Physical stressors; Psychological stress; Quantitative metric; Real-time management; Wearable technology; hydrocortisone; insulin; lactic acid; acute stress; adverse outcome; anxiety; Article; Bayesian learning; blood volume; chronic disease; classification algorithm; decision tree; deep learning; diabetes mellitus; discriminant analysis; electrodermal response; ensemble learning; eye tracking; heart rate; human; k nearest neighbor; learning algorithm; machine learning; mental stress; physical activity; physical stress; psychological aspect; pulse rate; questionnaire; sedentary time; self report; skin temperature; speech; State Trait Anxiety Inventory; stationary bike; support vector machine; treadmill exercise; algorithm; electronic device; exercise; mental stress; Algorithms; Anxiety; Exercise; Humans; Stress, Psychological; Wearable Electronic Devices",Article,"Final","",Scopus,2-s2.0-85098080023
"Fotouhi J., Mehrfard A., Song T., Johnson A., Osgood G., Unberath M., Armand M., Navab N.","56352068600;57215412467;57207819588;57193741558;6602692951;56893868600;35236429300;7003458998;","Development and Pre-Clinical Analysis of Spatiotemporal-Aware Augmented Reality in Orthopedic Interventions",2021,"IEEE Transactions on Medical Imaging","40","2","9252943","765","778",,1,"10.1109/TMI.2020.3037013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096823556&doi=10.1109%2fTMI.2020.3037013&partnerID=40&md5=be5245e708a54737862d41ad2a8a2dcd","Laboratory for Computational Sensing and Robotics, Johns Hopkins University, Baltimore, MD, United States; Laboratory for Computer-Aided Medical Procedures, Technical University of Munich, Munich, 80333, Germany; Department of Orthopaedic Surgery, Johns Hopkins Hospital, Baltimore, MD, United States","Fotouhi, J., Laboratory for Computational Sensing and Robotics, Johns Hopkins University, Baltimore, MD, United States; Mehrfard, A., Laboratory for Computational Sensing and Robotics, Johns Hopkins University, Baltimore, MD, United States, Laboratory for Computer-Aided Medical Procedures, Technical University of Munich, Munich, 80333, Germany; Song, T., Laboratory for Computational Sensing and Robotics, Johns Hopkins University, Baltimore, MD, United States; Johnson, A., Department of Orthopaedic Surgery, Johns Hopkins Hospital, Baltimore, MD, United States; Osgood, G., Department of Orthopaedic Surgery, Johns Hopkins Hospital, Baltimore, MD, United States; Unberath, M., Laboratory for Computational Sensing and Robotics, Johns Hopkins University, Baltimore, MD, United States; Armand, M., Laboratory for Computational Sensing and Robotics, Johns Hopkins University, Baltimore, MD, United States, Department of Orthopaedic Surgery, Johns Hopkins Hospital, Baltimore, MD, United States; Navab, N., Laboratory for Computational Sensing and Robotics, Johns Hopkins University, Baltimore, MD, United States, Laboratory for Computer-Aided Medical Procedures, Technical University of Munich, Munich, 80333, Germany","Suboptimal interaction with patient data and challenges in mastering 3D anatomy based on ill-posed 2D interventional images are essential concerns in image-guided therapies. Augmented reality (AR) has been introduced in the operating rooms in the last decade; however, in image-guided interventions, it has often only been considered as a visualization device improving traditional workflows. As a consequence, the technology is gaining minimum maturity that it requires to redefine new procedures, user interfaces, and interactions. The main contribution of this paper is to reveal how exemplary workflows are redefined by taking full advantage of head-mounted displays when entirely co-registered with the imaging system at all times. The awareness of the system from the geometric and physical characteristics of X-ray imaging allows the exploration of different human-machine interfaces. Our system achieved an error of 4.76 ± 2.91mm for placing K-wire in a fracture management procedure, and yielded errors of 1.57 ± 1.16° and 1.46 ± 1.00° in the abduction and anteversion angles, respectively, for total hip arthroplasty (THA). We compared the results with the outcomes from baseline standard operative and non-immersive AR procedures, which had yielded errors of [4.61mm, 4.76°, 4.77°] and [5.13mm, 1.78°, 1.43°], respectively, for wire placement, and abduction and anteversion during THA. We hope that our holistic approach towards improving the interface of surgery not only augments the surgeon's capabilities but also augments the surgical team's experience in carrying out an effective intervention with reduced complications and provide novel approaches of documenting procedures for training purposes. © 1982-2012 IEEE.","Augmented reality; frustum; interaction; surgery; visualization; X-ray","Arthroplasty; Augmented reality; Errors; Helmet mounted displays; Hospital data processing; Transplantation (surgical); User interfaces; Fracture managements; Head mounted displays; Human Machine Interface; Image guided therapy; Image-guided Intervention; Physical characteristics; Total hip arthroplasty; Visualization devices; Image enhancement; abduction; acetabulum; Article; augmented reality; bone anteversion; clinical outcome; comparative study; computer vision; controlled study; eye tracking; geometry; human; orthopedic surgeon; orthopedic surgery; osteosynthesis; preclinical study; pubis symphysis; quaternion; radiation dose; radiography; spatiotemporal analysis; total hip replacement; computer assisted surgery; Augmented Reality; Humans; Surgery, Computer-Assisted",Article,"Final","",Scopus,2-s2.0-85096823556
"Ahmed N.Y.","24779349300;","Real-time accurate eye center localization for low-resolution grayscale images",2021,"Journal of Real-Time Image Processing","18","1",,"193","220",,2,"10.1007/s11554-020-00955-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083370073&doi=10.1007%2fs11554-020-00955-2&partnerID=40&md5=223d7af55c9cfaba28d30548ec205cc4","Department of radiation engineering, National Center for Radiation Research and Technology (NCRRT), Atomic Energy Authority in Egypt, Cairo, Egypt","Ahmed, N.Y., Department of radiation engineering, National Center for Radiation Research and Technology (NCRRT), Atomic Energy Authority in Egypt, Cairo, Egypt","Eye center localization is considered a crucial step for many human–computer interaction (HCI) real-time applications. Detecting the center of eye (COE), accurately and in real time, is very challenging due to the wide variation of poses, eye appearance and specular reflection, especially in low-resolution images. In this paper, an accurate real-time detection algorithm of the COE is proposed. The proposed approach depends on the image gradient to detect the COE. The computational complexity is minimized and the accuracy is improved by down sampling the face resolution and applying a rough-to-fine algorithms, to reduce the search area, in accordance with the Eye Region Of Interest (EROI) and the number of COE candidates, tested by the proposed algorithm. Also, the detection algorithm is applied on a limited number of pixels that represent the iris boundary of the COE candidates. The Look Up Tables (LUTs) are implemented to, initially, store the invariant elements of the proposed image gradient-based algorithm, to reduce the detection time. Before applying the proposed COE detection approach, a modified specular reflection method is used to improve the detection accuracy. The performance of the proposed algorithm has been evaluated by applying it to three benchmark databases: the BIOID, GI4E and Talking Face video datasets, at different face resolutions. Experimental results revealed that the accuracy of the proposed algorithm is up to 91.68% and 96.7% for BIOID and GI4E datasets, respectively, while the minimum achieved average detection time is 2.7 ms. The promising results highlight the potential of the proposed algorithm to be used in some eye gaze-based real-time applications. Comparing the proposed method with the most state-of-the-art approaches showed that the system outperforms most of them and has a comparable performance with the others, in terms of the COE localization accuracy and detection speed. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Center of eye detection; Human–computer interaction; Image gradient; Real-time image processing; Specular highlight removal","Human computer interaction; Image segmentation; Signal detection; Table lookup; Computer interaction; Detection algorithm; Localization accuracy; Low resolution images; Real-time application; Real-time detection; Specular reflections; State-of-the-art approach; Benchmarking",Article,"Final","",Scopus,2-s2.0-85083370073
"Zontone P., Affanni A., Bernardini R., Del Linz L., Piras A., Rinaldo R.","12805791200;55957389200;7005276670;57216836480;57202664963;7003771007;","Emotional response analysis using electrodermal activity, electrocardiogram and eye tracking signals in drivers with various car setups",2021,"European Signal Processing Conference","2021-January",,"9287446","1160","1164",,1,"10.23919/Eusipco47968.2020.9287446","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099294479&doi=10.23919%2fEusipco47968.2020.9287446&partnerID=40&md5=8b5219a606346459d2df29e4d54bb366","Polytechnic Department of Engineering and Architecture, University of Udine, Udine, Italy; VI-grade Srl Via G., Galilei 42, Tavagnacco, Udine, Italy","Zontone, P., Polytechnic Department of Engineering and Architecture, University of Udine, Udine, Italy; Affanni, A., Polytechnic Department of Engineering and Architecture, University of Udine, Udine, Italy; Bernardini, R., Polytechnic Department of Engineering and Architecture, University of Udine, Udine, Italy; Del Linz, L., VI-grade Srl Via G., Galilei 42, Tavagnacco, Udine, Italy; Piras, A., Polytechnic Department of Engineering and Architecture, University of Udine, Udine, Italy; Rinaldo, R., Polytechnic Department of Engineering and Architecture, University of Udine, Udine, Italy","In the automotive industry, it is important to evaluate different car setups in order to match a professional driver's preference or to match the most acceptable setup for most drivers. Therefore, it is of great significance to devise objective and automatic procedures to assess a driver's response to different car settings. In this work, we analyze different physiological signals in order to evaluate how a particular car setup can be more or less stressful than others. In detail, we record an endosomatic Electrodermal Activity (EDA) signal, called Skin Potential Response (SPR), the Electrocardiogram (ECG) signal, and eye tracking coordinates. We eliminate motion artifacts by processing two SPR signals, one from each hand of the driver. Tests are carried out in a company that designs driving simulators, where the tested individuals had to drive along a straight highway with several lane changes. Three different car setups have been tested (neutral, understeering, and oversteering). We apply a statistical test to the data extracted from the cleaned SPR signal, and we then compare the results with the ones obtained using a Machine Learning algorithm. We show that we are able to discriminate the drivers' response to each setup, and, in particular, that the base car setup generates the least intense emotional response when compared to the understeering and the oversteering car setups. © 2021 European Signal Processing Conference, EUSIPCO. All rights reserved.","Electrocardiogram; Eye Tracking; Skin Potential Response; Stress Detection; Supervised Machine Learning Algorithm","Digital storage; Electrocardiography; Electrodes; Learning algorithms; Machine learning; Signal processing; Surface plasmon resonance; Turing machines; Automatic procedures; Driving simulator; Electrocardiogram signal; Electrodermal activity; Emotional response; Physiological signals; Professional drivers; Skin potential response; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85099294479
"Saeki M., Matsuyama Y., Kobashikawa S., Ogawa T., Kobayashi T.","57211139813;26427734700;14018096800;55755369100;7408541127;","Analysis of Multimodal Features for Speaking Proficiency Scoring in an Interview Dialogue",2021,"2021 IEEE Spoken Language Technology Workshop, SLT 2021 - Proceedings",,,"9383590","629","635",,,"10.1109/SLT48900.2021.9383590","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103928400&doi=10.1109%2fSLT48900.2021.9383590&partnerID=40&md5=aa69228b9336c8be6173620d2bb8e564","Waseda University, Department of Communications and Computer Engineering, Japan","Saeki, M., Waseda University, Department of Communications and Computer Engineering, Japan; Matsuyama, Y., Waseda University, Department of Communications and Computer Engineering, Japan; Kobashikawa, S., Waseda University, Department of Communications and Computer Engineering, Japan; Ogawa, T., Waseda University, Department of Communications and Computer Engineering, Japan; Kobayashi, T., Waseda University, Department of Communications and Computer Engineering, Japan","This paper analyzes the effectiveness of different modalities in automated speaking proficiency scoring in an online dialogue task of non-native speakers. Conversational competence of a language learner can be assessed through the use of multimodal behaviors such as speech content, prosody, and visual cues. Although lexical and acoustic features have been widely studied, there has been no study on the usage of visual features, such as facial expressions and eye gaze. To build an automated speaking proficiency scoring system using multi-modal features, we first constructed an online video interview dataset of 210 Japanese English-learners with annotations of their speaking proficiency. We then examined two approaches for incorporating visual features and compared the effectiveness of each modality. Results show the end-to-end approach with deep neural networks achieves a higher correlation with human scoring than one with handcrafted features. Modalities are effective in the order of lexical, acoustic, and visual features. © 2021 IEEE.","BERT (Bidirectional Encoder Representations from Transformers); multi-modal machine learning; Speaking proficiency assessment","Deep neural networks; Acoustic features; Facial Expressions; Multimodal features; Non-native speakers; Online video; Scoring systems; Speech content; Visual feature; Visual languages",Conference Paper,"Final","",Scopus,2-s2.0-85103928400
"Ou W.-L., Kuo T.-L., Chang C.-C., Fan C.-P.","55786494400;57216125874;57221691595;7402656929;","Deep-learning-based pupil center detection and tracking technology for visible-light wearable gaze tracking devices",2021,"Applied Sciences (Switzerland)","11","2","851","1","21",,1,"10.3390/app11020851","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099837702&doi=10.3390%2fapp11020851&partnerID=40&md5=13bd2196fd9991fadfa0781320ebc0e9","Department of Electrical Engineering, National Chung Hsing University, 145 Xingda Rd., South Dist., Taichung City, 402, Taiwan","Ou, W.-L., Department of Electrical Engineering, National Chung Hsing University, 145 Xingda Rd., South Dist., Taichung City, 402, Taiwan; Kuo, T.-L., Department of Electrical Engineering, National Chung Hsing University, 145 Xingda Rd., South Dist., Taichung City, 402, Taiwan; Chang, C.-C., Department of Electrical Engineering, National Chung Hsing University, 145 Xingda Rd., South Dist., Taichung City, 402, Taiwan; Fan, C.-P., Department of Electrical Engineering, National Chung Hsing University, 145 Xingda Rd., South Dist., Taichung City, 402, Taiwan","In this study, for the application of visible-light wearable eye trackers, a pupil tracking methodology based on deep-learning technology is developed. By applying deep-learning object detection technology based on the You Only Look Once (YOLO) model, the proposed pupil tracking method can effectively estimate and predict the center of the pupil in the visible-light mode. By using the developed YOLOv3-tiny-based model to test the pupil tracking performance, the detection accuracy is as high as 80%, and the recall rate is close to 83%. In addition, the average visible-light pupil tracking errors of the proposed YOLO-based deep-learning design are smaller than 2 pixels for the training mode and 5 pixels for the cross-person test, which are much smaller than those of the previous ellipse fitting design without using deep-learning technology under the same visible-light conditions. After the combination of calibration process, the average gaze tracking errors by the proposed YOLOv3-tiny-based pupil tracking models are smaller than 2.9 and 3.5 degrees at the training and testing modes, respectively, and the proposed visible-light wearable gaze tracking system performs up to 20 frames per second (FPS) on the GPU-based software embedded platform. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep-learning; Gaze tracker; Pupil tracking; Visible-light; Wearable eye tracker; YOLOv3-tiny",,Article,"Final","",Scopus,2-s2.0-85099837702
"De-Juan-Ripoll C., Llanes-Jurado J., Giglioli I.A.C., Marín-Morales J., Alcañiz M.","57204604322;57218430050;56994284500;57191977544;7003335420;","An immersive virtual reality game for predicting risk taking through the use of implicit measures",2021,"Applied Sciences (Switzerland)","11","2","825","1","21",,2,"10.3390/app11020825","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099828068&doi=10.3390%2fapp11020825&partnerID=40&md5=a2bc84e5ed8a0697aa116171fa5a231c","Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politècnica de València, Valencia, 46022, Spain","De-Juan-Ripoll, C., Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politècnica de València, Valencia, 46022, Spain; Llanes-Jurado, J., Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politècnica de València, Valencia, 46022, Spain; Giglioli, I.A.C., Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politècnica de València, Valencia, 46022, Spain; Marín-Morales, J., Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politècnica de València, Valencia, 46022, Spain; Alcañiz, M., Instituto de Investigación e Innovación en Bioingeniería (i3B), Universitat Politècnica de València, Valencia, 46022, Spain","Risk taking (RT) measurement constitutes a challenge for researchers and practitioners and has been addressed from different perspectives. Personality traits and temperamental aspects such as sensation seeking and impulsivity influence the individual’s approach to RT, prompting risk-seeking or risk-aversion behaviors. Virtual reality has emerged as a suitable tool for RT measurement, since it enables the exposure of a person to realistic risks, allowing embodied interactions, the application of stealth assessment techniques and physiological real-time measurement. In this article, we present the assessment on decision making in risk environments (AEMIN) tool, as an enhanced version of the spheres and shield maze task, a previous tool developed by the authors. The main aim of this article is to study whether it is possible is to discriminate participants with high versus low scores in the measures of personality, sensation seeking and impulsivity, through their behaviors and physiological responses during playing AEMIN. Applying machine learning methods to the dataset we explored: (a) if through these data it is possible to discriminate between the two populations in each variable; and (b) which parameters better discriminate between the two populations in each variable. The results support the use of AEMIN as an ecological assessment tool to measure RT, since it brings to light behaviors that allow to classify the subjects into high/low risk-related psychological constructs. Regarding physiological measures, galvanic skin response seems to be less salient in prediction models. © 2021 by the authors.","Eye tracking; Galvanic skin response; Implicit measures; Impulsivity; Personality; Risk taking; Sensation seeking; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85099828068
"Lang M., Schieder C.","57300509800;26424794800;","Exploring the impact of personality traits and technical affinity on the appearance of technostress",2021,"14th IADIS International Conference Information Systems 2021, IS 2021",,,,"145","152",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117371272&partnerID=40&md5=8804d6ec468774b558e3e6354fde42b7","Ostbayerische Technische Hochschule, Amberg-Weiden, Germany","Lang, M., Ostbayerische Technische Hochschule, Amberg-Weiden, Germany; Schieder, C., Ostbayerische Technische Hochschule, Amberg-Weiden, Germany","Information and communication technologies, such as instant messengers, have become an essential part of every person's work and private life. Undesirable side effects, such as technostress accompany this trend. The present study examines the relationship between the Big Five personality traits and the technical affinity to the appearance of technostress and its effect on the general task fulfillment. The experimental design's central component was an online memory game, combined with the NEO Five-Factor Inventory and a technical affinity questionnaire. The experiment with 13 participants was monitored with a gaze tracking device and corresponding software. This study showed that people with certain personality traits perceive technostress at a higher or lower level. Furthermore, technostress lowers task performance. However, this effect can be mitigated by a higher level of technical affinity. This paper is ongoing research. Therefore, future research should consider a higher number of participants and a variation of stressors. © 14th IADIS International Conf. Infor. Sys. 2021. All rights reserved.","Personality Traits; Task Performance; Technical Affinity; Technostress","Computer vision; Information use; Big five; Central component; Gaze-tracking; Information and Communication Technologies; Instant messengers; Personality traits; Side effect; Task performance; Technical affinity; Technostress; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85117371272
"Li X., Wang Y.","57213267610;56032563800;","Low Crosstalk Multi-view 3D Display Based on Parallax Barrier with Dimmed Subpixel",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12890 LNCS",,,"490","500",,,"10.1007/978-3-030-87361-5_40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117126635&doi=10.1007%2f978-3-030-87361-5_40&partnerID=40&md5=1988b96616f7933b8d01c55658f94da9","School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China","Li, X., School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China; Wang, Y., School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China","Multi-view three-dimentional display system stands out for its easy realization and flexible viewing effect among all kinds of autostereoscopic display techniques. However, traditional multi-view display system suffers from severe crosstalk. Here a five-view display system with dimmed subpixel is presented to reduce crosstalk, and at the same time ensures fine viewing experience. The dimmed subpixel is arranged interlocked to make sure that the full color formation is intact and also to balance the distribution of the exit pupil. The crosstalk is reduced from 16% to 12.5%. There are three synthetic modes. Incorporated with the eye tracking device, the main viewer is able to get the best viewing experience by changing the image synthetic mode according to the viewer’s position. © 2021, Springer Nature Switzerland AG.","Autostereoscopic display; Low crosstalk; Multi-view","Eye tracking; Flexible displays; Geometrical optics; Pixels; Stereo image processing; Three dimensional displays; 3-D displays; 3D-displays; Auto-stereoscopic display; Display system; Full color; Low crosstalk; Multi-views; Multiview displays; Parallax barriers; Sub-pixels; Crosstalk",Conference Paper,"Final","",Scopus,2-s2.0-85117126635
"Li R., Ma H., Wang R., Ding J.","57296544600;56406620700;57225154146;57296896900;","Device-Adaptive 2D Gaze Estimation: A Multi-Point Differential Framework",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12889 LNCS",,,"485","497",,,"10.1007/978-3-030-87358-5_39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117124096&doi=10.1007%2f978-3-030-87358-5_39&partnerID=40&md5=b34f2238396f037f29952634d0adde4d","Tsinghua University, Beijing, 100084, China; University of Science and Technology Beijing, Beijing, 100083, China","Li, R., Tsinghua University, Beijing, 100084, China; Ma, H., University of Science and Technology Beijing, Beijing, 100083, China; Wang, R., University of Science and Technology Beijing, Beijing, 100083, China; Ding, J., University of Science and Technology Beijing, Beijing, 100083, China","Eye tracking system on mobile devices is important for many interactive applications. However, since models are usually customized with limited types of devices and new devices have totally different physical parameters, it is hard to generalize over unseen devices. In this paper, we present a device-adaptive 2D gaze estimation algorithm based on differential prediction. We reformulate the gaze estimation as a relative position prediction problem between the input image and calibration images, which skips the estimation for camera parameters and makes models easily generalize over devices. To tackle the new challenge, this work proposes a framework which jointly trains a differential prediction module and an aggregation module for ensembling the predictions from multiple calibration points. Experiments show that the framework outperforms baseline models constantly on open datasets with only 3–5 calibration points. © 2021, Springer Nature Switzerland AG.","Adaptive gaze estimation; Differential prediction; Neural networks","Calibration; Computer vision; Eye tracking; Adaptive gaze estimation; Calibration points; Differential prediction; Eye tracking systems; Gaze estimation; Interactive applications; Multi-points; Neural-networks; New devices; System on mobile devices; Forecasting",Conference Paper,"Final","",Scopus,2-s2.0-85117124096
"Cao S., Zhao X., Qin B., Li J., Xiang Z.","57219797003;55352149700;57221152419;57218474951;57296381600;","A Monocular Reflection-Free Head-Mounted 3D Eye Tracking System",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12890 LNCS",,,"659","672",,,"10.1007/978-3-030-87361-5_54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117076445&doi=10.1007%2f978-3-030-87361-5_54&partnerID=40&md5=4368d46cfaf8759e17022a665fa88524","School of Computer Science, Northwestern Polytechnical University, Xi’an, 710129, China; School of Software, Northwestern Polytechnical University, Xi’an, 710129, China; Ningbo Institute of Northwestern Polytechnical University, Ningbo, 315103, China","Cao, S., School of Computer Science, Northwestern Polytechnical University, Xi’an, 710129, China; Zhao, X., School of Computer Science, Northwestern Polytechnical University, Xi’an, 710129, China, Ningbo Institute of Northwestern Polytechnical University, Ningbo, 315103, China; Qin, B., School of Computer Science, Northwestern Polytechnical University, Xi’an, 710129, China; Li, J., School of Computer Science, Northwestern Polytechnical University, Xi’an, 710129, China; Xiang, Z., School of Software, Northwestern Polytechnical University, Xi’an, 710129, China","Head-mounted eye tracking has significant potential for gaze baesd application such as consumer attention monitoring, human-computer interaction, or virtual reality (VR). Existing methods, however, either use pupil center-corneal reflection (PCCR) vectors as gaze directions or require complex hardware setups and use average physiological parameters of the eye to obtain gaze directions. In view of this situation, we propose a novel method which uses only a single camera to obtain gaze direction by fitting a 3D eye model based on the motion trajectory of pupil contour. Then a 3D to 2D mapping model is proposed based on the fitting model, so the complex structure of hardware and the use of average parameters for the eyes are avoided. The experimental results show that the method can improve the gaze accuracy and simplify the hardware structure. © 2021, Springer Nature Switzerland AG.","3D gaze estimation; Head-mounted device; Mapping model; Pupil contour; Single camera","3D modeling; Cameras; Computer hardware; Eye movements; Eye tracking; Human computer interaction; Mapping; Physiological models; 3d gaze estimation; Eye tracking systems; Free-head; Gaze direction; Gaze estimation; Head-mounted device; Head-mounted eye tracking; Mapping modeling; Pupil contour; Single cameras; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85117076445
"Li X.-S., Fan Z.-Z., Ren Y.-Y., Zheng X.-L., Yang R.","16744214900;57295398300;35220576200;36462110400;57222516532;","Classification of Eye Movement and Its Application in Driving Based on a Refined Pre-Processing and Machine Learning Algorithm",2021,"IEEE Access","9",,,"136164","136181",,,"10.1109/ACCESS.2021.3115961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117011843&doi=10.1109%2fACCESS.2021.3115961&partnerID=40&md5=0b8ba4089693c1349f5c65dc0354fa3e","Transportation College, Jilin University, Changchun, 130022, China; Kingfar International Inc., Beijing, 100089, China","Li, X.-S., Transportation College, Jilin University, Changchun, 130022, China; Fan, Z.-Z., Transportation College, Jilin University, Changchun, 130022, China; Ren, Y.-Y., Transportation College, Jilin University, Changchun, 130022, China; Zheng, X.-L., Transportation College, Jilin University, Changchun, 130022, China; Yang, R., Kingfar International Inc., Beijing, 100089, China","The eyes are the first channel used by humans to obtain various types of visual information from the outside world and, especially when driving, 80-90% of information is received through the eyes. Eye movement behaviors are generally divided into six types, but attention is often paid to fixation, saccade, and smooth pursuit. Due to their importance, it is essential to classify eye movement behaviors accurately. The classification of eye movements should be a complete process, including the three steps of pre-processing, classification, and post-processing. However, it is very uncommon for all of these steps to be included in the eye-tracking literature when eye movement classification is discussed. Therefore, first, this paper proposes a refined eye movement data pre-processing framework and an improved method consisting of three steps is introduced. Second, an eye movement classification algorithm based on an improved decision tree that is independent of the threshold setting and application environment is proposed, and a post-processing consisting of merging adjacent fixations and discarding short fixations is described. Finally, the application of the classified eye movement behavior in the driving field is described, including the estimation of preview time using fixation and the estimation of time-to-collision using smooth pursuit. Two important results are obtained in this paper. One concerns the classification accuracy of eye movement behavior, the F1-scores of fixation, saccade, and smooth pursuit being respectively 92.63%, 93.46%, and 65.29%, which are higher than the scores of other algorithms. The other relates to the application to driving. On the one hand, the preview time calculated by fixation is mostly distributed around 1-6s, which is closer to reality than the traditional setting of 1s. At the same time, the regression relationship between the preview time and the road turning radius is also quantitatively analyzed and their regression function is obtained. On the other hand, the average estimated error of time-to-collision used by smooth pursuit is 7.37%. These results can play an important role in the development of ADAS and the improvement of traffic safety. © 2013 IEEE.","behavior classification; feature construction; improved decision tree; Keywords eye movement data; refined pre-processing","Classification (of information); Data handling; Decision trees; Eye tracking; Learning algorithms; Machine learning; Behaviour classification; Eye movement datum; Feature construction; Improved decision tree; Keyword eye movement data; Movement behaviour; Post-processing; Pre-processing; Refined pre-processing; Smooth pursuit; Eye movements",Article,"Final","",Scopus,2-s2.0-85117011843
"Alhanaee K., Alhammadi M., Almenhali N., Shatnawi M.","57291327200;57223022664;57292231800;55303347400;","Face recognition smart attendance system using deep transfer learning",2021,"Procedia Computer Science","192",,,"4093","4102",,,"10.1016/j.procs.2021.09.184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116923008&doi=10.1016%2fj.procs.2021.09.184&partnerID=40&md5=639cea743a9176abf3b716b9a536a164","Deprtment of Electrical Engineering Technology, Higher Colleges of Technology, Abu Dhabi, United Arab Emirates","Alhanaee, K., Deprtment of Electrical Engineering Technology, Higher Colleges of Technology, Abu Dhabi, United Arab Emirates; Alhammadi, M., Deprtment of Electrical Engineering Technology, Higher Colleges of Technology, Abu Dhabi, United Arab Emirates; Almenhali, N., Deprtment of Electrical Engineering Technology, Higher Colleges of Technology, Abu Dhabi, United Arab Emirates; Shatnawi, M., Deprtment of Electrical Engineering Technology, Higher Colleges of Technology, Abu Dhabi, United Arab Emirates","Face identification has been considered an interesting research domain in the past few years as it plays a major biometric authentication role in several applications including attendance management and access control systems. Attendance management systems are very important to all organization though they are complex and time-consuming for managing regular attendance log. There are many automated human identification techniques such as biometrics, RFID, eye tracking, voice recognition. Face is one of the most broadly used biometrics for human identity authentication. This paper presents a facial recognition attendance system based on deep learning convolutional neural networks. We utilize transfer learning by using three pre-trained convolutional neural networks and trained them on our data. The three networks showed very high performance in terms of high prediction accuracy and reasonable training time. © 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.","Here; Separated by semicolons","Authentication; Convolution; Convolutional neural networks; Deep learning; Eye tracking; Face recognition; Radio frequency identification (RFID); Access control systems; Attendance systems; Biometric authentication; Convolutional neural network; Face identification; Here; Management control system; Research domains; Separated by semicolons; Transfer learning; Biometrics",Conference Paper,"Final","",Scopus,2-s2.0-85116923008
"Anisimov V., Chernozatonsky K., Pikunov A., Raykhrud M., Revazov A., Shedenko K., Zhigulskaya D., Zuev S.","56386799900;57292040300;57292957500;57292040400;57291809100;57223425284;57291362800;57292501000;","OkenReader: ML-based classification of the reading patterns using an Apple iPad",2021,"Procedia Computer Science","192",,,"1944","1953",,,"10.1016/j.procs.2021.08.200","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116881604&doi=10.1016%2fj.procs.2021.08.200&partnerID=40&md5=1523494b0696c8a4b4cca284e25b803b","Oken Technologies Inc., Verhnyaya Krasnoselskaya, 3/2, Moscow, 107140, Russian Federation","Anisimov, V., Oken Technologies Inc., Verhnyaya Krasnoselskaya, 3/2, Moscow, 107140, Russian Federation; Chernozatonsky, K., Oken Technologies Inc., Verhnyaya Krasnoselskaya, 3/2, Moscow, 107140, Russian Federation; Pikunov, A., Oken Technologies Inc., Verhnyaya Krasnoselskaya, 3/2, Moscow, 107140, Russian Federation; Raykhrud, M., Oken Technologies Inc., Verhnyaya Krasnoselskaya, 3/2, Moscow, 107140, Russian Federation; Revazov, A., Oken Technologies Inc., Verhnyaya Krasnoselskaya, 3/2, Moscow, 107140, Russian Federation; Shedenko, K., Oken Technologies Inc., Verhnyaya Krasnoselskaya, 3/2, Moscow, 107140, Russian Federation; Zhigulskaya, D., Oken Technologies Inc., Verhnyaya Krasnoselskaya, 3/2, Moscow, 107140, Russian Federation; Zuev, S., Oken Technologies Inc., Verhnyaya Krasnoselskaya, 3/2, Moscow, 107140, Russian Federation","Digital learning and professional training require processing of large volumes of information, mostly in the text format. The new information environment requires the development of new studying methods and, most importantly, a new tool for assessing its quality in order to adjust the strategy of compiling the educational content. Management and control of perception in reading can be performed with the use of technology, assessing attention, engagement, understanding, cognitive load and tiredness of readers. The article describes an early effort towards creation of a commercial technology based on a machine-learning (ML) algorithm that uses readers' eye-tracking recording as a proxy for their cognitive state. The solution was realized on Ipad Pro with a standard iOS operating system, which provides opportunities for mass adoption in educational and training settings. © 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.","Cognitive state; Machine learning algorithms; Neurophysiological data; Reading; Text perception","Hand held computers; Learning algorithms; Machine learning; Text processing; Cognitive state; Digital-learning; Large volumes; Machine learning algorithms; Neurophysiological data; Professional training; Reading; Reading patterns; Text format; Text perception; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116881604
"Jogeshwar A.K., Pelz J.B.","57193727303;7007018556;","Gazeenviz4D: 4-D gaze-in-environment visualization pipeline",2021,"Procedia Computer Science","192",,,"2952","2961",,,"10.1016/j.procs.2021.09.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116856498&doi=10.1016%2fj.procs.2021.09.067&partnerID=40&md5=b0aec5e73516d8f5edacad835d2ed628","Rochester Institute of Technology, Rochester, NY  14623, United States","Jogeshwar, A.K., Rochester Institute of Technology, Rochester, NY  14623, United States; Pelz, J.B., Rochester Institute of Technology, Rochester, NY  14623, United States","Eye-tracking data visualization and analysis is often performed in three dimensions (x,y,t). It involves overlaying the gaze point (x,y) on scene-camera images and creating a pipeline to process the gaze-overlaid spatio-temporal data. In this project, we present a pipeline (called GazeEnViz4D) to extract 3D data from raw 2D eye-tracking data, and we have developed a custom ENvironment VIsualiZer (EnViz4D§) to allow researchers to visualize the pipeline-processed data in four dimensions (x,y,z,t) for extensive, interactive analysis. GazeEnViz4D consists of creating a 3D point cloud of the environment, calculating the observer motion, locating the 2D gaze obtained from the eye-tracker in the 3D model, and visualizing the data over time using EnViz4D. EnViz4D allows a researcher to zoom in the environment at any instance, pause or play the 3D data, speed up or slow down, forward or reverse, essentially recreating the data collection episode in four dimensions from an arbitrary modifiable viewpoint. © 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.","4-dimensional analysis; Environmental model; Eye-tracking; Gaze visualization; Interactive tool; Motion tracking; Structure-from-motion","3D modeling; Data visualization; Eye movements; Motion analysis; Pipelines; Visualization; 3D data; 4-dimensional analyse; Dimensional analysis; Environmental model; Eye-tracking; Four dimensions; Gaze visualization; Interactive tool; Structure from motion; Tracking data; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116856498
"Teng C., Sharma H., Drukker L., Papageorghiou A.T., Noble J.A.","57285766600;56272987100;36241434600;6603569987;57226264208;","Towards Scale and Position Invariant Task Classification Using Normalised Visual Scanpaths in Clinical Fetal Ultrasound",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12967 LNCS",,,"129","138",,,"10.1007/978-3-030-87583-1_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116487070&doi=10.1007%2f978-3-030-87583-1_13&partnerID=40&md5=08db1893e8e80e12ad705f73574be415","Institute of Biomedical Engineering, University of Oxford, Oxford, United Kingdom; Nuffield Department of Women’s and Reproductive Health, University of Oxford, Oxford, United Kingdom","Teng, C., Institute of Biomedical Engineering, University of Oxford, Oxford, United Kingdom; Sharma, H., Institute of Biomedical Engineering, University of Oxford, Oxford, United Kingdom; Drukker, L., Nuffield Department of Women’s and Reproductive Health, University of Oxford, Oxford, United Kingdom; Papageorghiou, A.T., Nuffield Department of Women’s and Reproductive Health, University of Oxford, Oxford, United Kingdom; Noble, J.A., Institute of Biomedical Engineering, University of Oxford, Oxford, United Kingdom","We present a method for classifying tasks in fetal ultrasound scans using the eye-tracking data of sonographers. The visual attention of a sonographer captured by eye-tracking data over time is defined by a scanpath. In routine fetal ultrasound, the captured standard imaging planes are visually inconsistent due to fetal position, movements, and sonographer scanning experience. To address this challenge, we propose a scale and position invariant task classification method using normalised visual scanpaths. We describe a normalisation method that uses bounding boxes to provide the gaze with a reference to the position and scale of the imaging plane and use the normalised scanpath sequences to train machine learning models for discriminating between ultrasound tasks. We compare the proposed method to existing work considering raw eye-tracking data. The best performing model achieves the F1-score of 84% and outperforms existing models. © 2021, Springer Nature Switzerland AG.","Eye-tracking; Fetal ultrasound; Time-series classification; Visual scanpath","Behavioral research; Medical computing; Medical imaging; Ultrasonics; Eye-tracking; Fetal ultrasound; Imaging plane; Scan path; Sonographers; Task classification; Time series classifications; Tracking data; Ultrasound scans; Visual scanpath; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116487070
"Wang Y., Duan F., Wang Y.","57286292100;24537140100;57213689160;","Multi-point surface FES hand rehabilitation system for stroke patients based on eye movement control",2021,"Advanced Robotics",,,,"","",,,"10.1080/01691864.2021.1982405","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116481546&doi=10.1080%2f01691864.2021.1982405&partnerID=40&md5=d11bdf3299308f39944c7541758e9567","The College of Artificial Intelligence, Nankai University, Tianjin, China","Wang, Y., The College of Artificial Intelligence, Nankai University, Tianjin, China; Duan, F., The College of Artificial Intelligence, Nankai University, Tianjin, China; Wang, Y., The College of Artificial Intelligence, Nankai University, Tianjin, China","Recently, the number of stroke patients has increased greatly. Most of them suffer from hand motor impairment, which creates the need for effective rehabilitation systems. Functional electrical stimulation (FES) is a neurorehabilitation method based on brain plasticity, which is widely accepted because it is non-invasive and convenient. However, it is still a challenge to achieve fine finger control by using surface FES. Here, we use multi-point surface FES to achieve individual finger motion. Besides, simple FES does not engage the subjects' attention, and an optimal stimulation position is difficult to identify. Hence, we try to integrate FES with sensory control by presenting a multi-point FES system based on eye movement control. By programming the electrodes display interface and establishing communication with the FES device and the eye tracker, subjects can change stimulation points via their eye movements. Experiments were performed to test the validity and safety of the proposed system. Feedback data was obtained using a 3D motion capture device. The results indicate that achieving individual finger motion is possible and stimulation points can be changed via eye movements. The system is practical and can be used in hand motor training for stroke patients to shorten the rehabilitation period. © 2021 Informa UK Limited, trading as Taylor & Francis Group and The Robotics Society of Japan.","eye movements; Functional electrical stimulation; joint range of motion; motion capture","Display devices; Eye tracking; Functional electric stimulation; Eye movement control; Finger motion; Functional electri-cal stimulations; Hand rehabilitation; Motion capture; Motor impairments; Multi-points; Rehabilitation System; Stroke patients; Surface functional; Eye movements",Article,"Article in Press","",Scopus,2-s2.0-85116481546
"Saab K., Hooper S.M., Sohoni N.S., Parmar J., Pogatchnik B., Wu S., Dunnmon J.A., Zhang H.R., Rubin D., Ré C.","57193841679;57216149541;57202708705;57285170100;36176344300;57286547400;36767226100;57285862700;7202307112;10739281400;","Observational Supervision for Medical Image Classification Using Gaze Data",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12902 LNCS",,,"603","614",,,"10.1007/978-3-030-87196-3_56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116417040&doi=10.1007%2f978-3-030-87196-3_56&partnerID=40&md5=f7971d1e96ed98855e586d37ca72908f","Department of Electrical Engineering, Stanford University, Stanford, United States; Institute for Computational and Mathematical Engineering, Stanford University, Stanford, United States; Department of Computer Science, Stanford University, Stanford, United States; Department of Radiology, Stanford University, Stanford, United States; Khoury College of Computer Sciences, Northeastern University, Boston, United States; Department of Biomedical Data Science, Stanford University, Stanford, United States","Saab, K., Department of Electrical Engineering, Stanford University, Stanford, United States; Hooper, S.M., Department of Electrical Engineering, Stanford University, Stanford, United States; Sohoni, N.S., Institute for Computational and Mathematical Engineering, Stanford University, Stanford, United States; Parmar, J., Department of Computer Science, Stanford University, Stanford, United States; Pogatchnik, B., Department of Radiology, Stanford University, Stanford, United States; Wu, S., Department of Computer Science, Stanford University, Stanford, United States; Dunnmon, J.A., Department of Computer Science, Stanford University, Stanford, United States; Zhang, H.R., Khoury College of Computer Sciences, Northeastern University, Boston, United States; Rubin, D., Department of Biomedical Data Science, Stanford University, Stanford, United States; Ré, C., Department of Computer Science, Stanford University, Stanford, United States","Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinician’s workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines. © 2021, Springer Nature Switzerland AG.","Eye tracking; Medical image diagnosis; Weak supervision","Computer aided instruction; Deep learning; Image classification; Linearization; Magnetic resonance imaging; Medical imaging; Classification tasks; Eye-tracking; Labeled dataset; Learning models; Medical image classification; Medical image diagnosis; Performance; Precision point; Weak supervision; Work-flows; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116417040
"Koonsanit K., Tsunajima T., Nishiuchi N.","26654161500;57218452799;6602390273;","Evaluation of Strong and Weak Signifiers in a Web Interface Using Eye-Tracking Heatmaps and Machine Learning",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12883 LNCS",,,"203","213",,,"10.1007/978-3-030-84340-3_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115860382&doi=10.1007%2f978-3-030-84340-3_16&partnerID=40&md5=84a2fbd0d139b0a01e55644ecc050fbc","Department of Computer Science, Graduate School of Systems Design, Tokyo Metropolitan University, Tokyo, Japan","Koonsanit, K., Department of Computer Science, Graduate School of Systems Design, Tokyo Metropolitan University, Tokyo, Japan; Tsunajima, T., Department of Computer Science, Graduate School of Systems Design, Tokyo Metropolitan University, Tokyo, Japan; Nishiuchi, N., Department of Computer Science, Graduate School of Systems Design, Tokyo Metropolitan University, Tokyo, Japan","The eye-tracking heatmap is a quantitative research tool that shows the user’s gaze points. Most of the eye-tracking heatmap is a 2D visualization comprising different colors. The heatmap colors indicate gaze duration, and the color cell’s position indicates gaze position. The eye-tracking heatmap has often been used to evaluate the usability of web interfaces to understand user behavior. For example, web designers have used heatmaps to obtain actual evidence for how users use their website. Further, the collection of eye-tracking heatmap data during website viewing facilitates measurement of improvements in site usability. However, although the eye-tracking heatmap provides rich information about how users watch, focus, and interact with a site, the high informational requirements substantially increase computational burden. In many cases, the distribution of gaze points in an eye-tracking heatmap may not be easily understood and interpreted. Accordingly, manual evaluation of heatmaps is inefficient. This study aimed to evaluate web usability by focusing on signifiers as an interface element using eye-tracking heatmaps and machine learning algorithms. We also used the dimensionality reduction technique to reduce the complexity of heatmap data. The results showed that the proposed classification model that combined the decision tree and PCA technique provided more than 90% accuracy when compared with the other nine classical machine learning methods. This finding indicated that the machine learning process reached the correct decision about the interface’s usability. © 2021, Springer Nature Switzerland AG.","Classification; Eye-tracking Heatmap; Machine learning; Signifier; Usability; Web interface","Behavioral research; Color; Decision trees; Learning algorithms; Machine learning; Usability engineering; Web Design; 2-D visualizations; Eye-tracking; Eye-tracking heatmap; Gaze point; Heatmaps; Machine-learning; Quantitative research; Research tools; Signifi; Web interface; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85115860382
"Amadori P.V., Fischer T., Wang R., Demiris Y.","56703112800;57190126084;57189039099;6506125343;","Predicting Secondary Task Performance: A Directly Actionable Metric for Cognitive Overload Detection",2021,"IEEE Transactions on Cognitive and Developmental Systems",,,,"","",,,"10.1109/TCDS.2021.3114162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115685349&doi=10.1109%2fTCDS.2021.3114162&partnerID=40&md5=6977027aed41eb30e44b70baa33f49f4","Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K. (e-mail: pierluigi.amadori@gmail.com); Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.","Amadori, P.V., Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K. (e-mail: pierluigi.amadori@gmail.com); Fischer, T., Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.; Wang, R., Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.; Demiris, Y., Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.","In this paper, we address cognitive overload detection from unobtrusive physiological signals for users in dual-tasking scenarios. Anticipating cognitive overload is a pivotal challenge in interactive cognitive systems and could lead to safer shared-control between users and assistance systems. Our framework builds on the assumption that decision mistakes on the cognitive secondary task of dual-tasking users correspond to cognitive overload events, wherein the cognitive resources required to perform the task exceed the ones available to the users. We propose DecNet, an end-to-end sequence-to-sequence deep learning model that infers in real-time the likelihood of user mistakes on the secondary task, i.e., the practical impact of cognitive overload, from eye-gaze and head-pose data. We train and test DecNet on a dataset collected in a simulated driving setup from a cohort of 20 users on two dual-tasking decision-making scenarios, with either visual or auditory decision stimuli. DecNet anticipates cognitive overload events in both scenarios and can perform in time-constrained scenarios, anticipating cognitive overload events up to 2s before they occur. We show that DecNet&#x2019;s performance gap between audio and visual scenarios is consistent with user perceived difficulty. This suggests that single modality stimulation induces higher cognitive load on users, hindering their decision-making abilities. Crown","Cognitive Workload; Data models; Decision Anticipation; Feature extraction; Load modeling; Simulated Driving.; Solid modeling; Task analysis; User Monitoring; Vehicles; Visualization","Cognitive systems; Deep learning; Job analysis; Statistical tests; Cognitive overload; Cognitive workloads; Decision anticipation; Features extraction; Load modeling; Simulated driving; Simulated driving.; Solid modelling; Task analysis; User monitoring; Decision making",Article,"Article in Press","",Scopus,2-s2.0-85115685349
"Gomez Cubero C., Rehm M.","57226718958;10039896700;","Intention Recognition in Human Robot Interaction Based on Eye Tracking",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12934 LNCS",,,"428","437",,,"10.1007/978-3-030-85613-7_29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115269762&doi=10.1007%2f978-3-030-85613-7_29&partnerID=40&md5=7a573dc68dc95a721f6de4147a18bd82","Technical Faculty of IT and Design, Aalborg University, Aalborg, Denmark","Gomez Cubero, C., Technical Faculty of IT and Design, Aalborg University, Aalborg, Denmark; Rehm, M., Technical Faculty of IT and Design, Aalborg University, Aalborg, Denmark","In human robot interaction any input that might help the robot to understand the human behaviour is valuable, and the eyes and their movement undoubtedly hold valuable information. In this paper we propose a novel algorithm for intention recognition using eye tracking in human robot collaboration. We first explore how the Cascade Effect hypothesis and a LSTM-based machine learning model perform to classify intent from gaze. Second, an algorithm is proposed, which can be used in a real time interaction to infer intention from the human user with a small uncertainty. A data collection with 30 participants was conducted in virtual reality to train and test the algorithm. The algorithm allows to detect the user intention up to two seconds before any user action with a success rate of up to 75%. These results open the possibility to study human robot interaction, where the robot can take the initiative based on the intention recognition. © 2021, IFIP International Federation for Information Processing.","Eye tracking; Human-robot interaction; Intention recognition","Behavioral research; Classification (of information); Eye movements; Eye tracking; Human computer interaction; Long short-term memory; Cascade effects; Data collection; Human behaviours; Human-robot collaboration; Intention recognition; Machine learning models; Novel algorithm; Real time interactions; Social robots",Conference Paper,"Final","",Scopus,2-s2.0-85115269762
"Islam M.R., Nawa S., Vargo A., Iwata M., Matsubara M., Morishima A., Kise K.","55765000561;57264127600;57191334988;7402168530;55608696000;36829985300;16178222100;","Quality Assessment of Crowdwork via Eye Gaze: Towards Adaptive Personalized Crowdsourcing",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12933 LNCS",,,"104","113",,,"10.1007/978-3-030-85616-8_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115205931&doi=10.1007%2f978-3-030-85616-8_8&partnerID=40&md5=7a3e875a57068de209ae9dae636b75f2","Osaka Prefecture University, Sakai, Japan; University of Tsukuba, Tsukuba, Japan","Islam, M.R., Osaka Prefecture University, Sakai, Japan; Nawa, S., Osaka Prefecture University, Sakai, Japan; Vargo, A., Osaka Prefecture University, Sakai, Japan; Iwata, M., Osaka Prefecture University, Sakai, Japan; Matsubara, M., University of Tsukuba, Tsukuba, Japan; Morishima, A., University of Tsukuba, Tsukuba, Japan; Kise, K., Osaka Prefecture University, Sakai, Japan","A significant challenge for creating efficient and fair crowdsourcing platforms is in rapid assessment of the quality of crowdwork. If a crowdworker lacks the skill, motivation, or understanding to provide adequate quality task completion, this reduces the efficacy of a platform. While this would seem like only a problem for task providers, the reality is that the burden of this problem is increasingly leveraged on crowdworkers. For example, task providers may not pay crowdworkers for their work after the evaluation of the task results has been completed. In this paper, we propose methods for quickly evaluating the quality of crowdwork using eye gaze information by estimating the correct answer rate. We find that the method with features generated by self-supervised learning (SSL) provides the most efficient result with a mean absolute error of 0.09. The results exhibit the potential of using eye gaze information to facilitate adaptive personalized crowdsourcing platforms. © 2021, IFIP International Federation for Information Processing.","Crowdsourcing; Eye gaze; Machine learning; Self-supervised learning","Crowdsourcing; Crowdsourcing platforms; Eye-gaze; Mean absolute error; Quality assessment; Rapid assessment; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85115205931
"Chang Y., He C., Zhao Y., Luy T., Gu N.","57224667373;57262788100;56451804400;57263506500;57262788200;","High-Frame-Rate Eye-Tracking Framework for Mobile Devices",2021,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2021-June",,,"1445","1449",,,"10.1109/ICASSP39728.2021.9414624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115165423&doi=10.1109%2fICASSP39728.2021.9414624&partnerID=40&md5=fd5062d0ee1fa069488c090a888a438f","School of Computer Science, Fudan University; Shanghai Key Laboratory of Data Science, Fudan University","Chang, Y., School of Computer Science, Fudan University; He, C., School of Computer Science, Fudan University; Zhao, Y., School of Computer Science, Fudan University; Luy, T., Shanghai Key Laboratory of Data Science, Fudan University; Gu, N., School of Computer Science, Fudan University","Gaze-on-screen tracking, an appearance-based eye-tracking task, has drawn significant interest in recent years. While learning-based high-precision eye-tracking methods have been designed in the past, the complex pre-training and high computation in neural network-based deep models restrict their applicability in mobile devices. Moreover, as the display frame rate of mobile devices has steadily increased to 120 fps, high-frame-rate eye tracking becomes increasingly challenging. In this work, we tackle the tracking efficiency challenge and introduce GazeHFR, a biologic-inspired eyetracking model specialized for mobile devices, offering both high accuracy and efficiency. Specifically, GazeHFR classifies the eye movement into two distinct phases, i.e., saccade and smooth pursuit, and leverages inter-frame motion information combined with lightweight learning models tailored to each movement phase to deliver high-efficient eye tracking without affecting accuracy. Compared to prior art, Gaze- HFR achieves approximately 7x speedup and 15% accuracy improvement on mobile devices. ©2021 IEEE.","Applications of machine learning; Biomedical video analysis; Gaze estimation; Mobile imaging","Display devices; Efficiency; Eye movements; Learning systems; Motion tracking; Accuracy Improvement; Appearance based; Eye tracking methods; High frame rate; High-precision; Learning models; Motion information; Smooth pursuit; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85115165423
"Kang D., Ma L.","57211898992;57255718500;","Real-Time Eye Tracking for Bare and Sunglasses-Wearing Faces for Augmented Reality 3D Head-Up Displays",2021,"IEEE Access","9",,,"125508","125522",,,"10.1109/ACCESS.2021.3110644","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114750359&doi=10.1109%2fACCESS.2021.3110644&partnerID=40&md5=60b918124b7babc2e650355e12d58846","Department of Electronic and Electrical Engineering, Hongik University, Seoul, 04066, South Korea; Sait China Lab, SRC-Beijing, Samsung Electronics, Beijing, 100007, China","Kang, D., Department of Electronic and Electrical Engineering, Hongik University, Seoul, 04066, South Korea; Ma, L., Sait China Lab, SRC-Beijing, Samsung Electronics, Beijing, 100007, China","Eye pupil tracking is important for augmented reality (AR) three-dimensional (3D) head-up displays (HUDs). Accurate and fast eye tracking is still challenging due to multiple driving conditions with eye occlusions, such as wearing sunglasses. In this paper, we propose a system for commercial use that can handle practical driving conditions. Our system classifies human faces into bare faces and sunglasses faces, which are treated differently. For bare faces, our eye tracker regresses the pupil area in a coarse-to-fine manner based on a revised Supervised Descent Method based eye-nose alignment. For sunglasses faces, because the eyes are occluded, our eye tracker uses whole face alignment with a revised Practical Facial Landmark Detector for pupil center tracking. Furthermore, we propose a structural inference-based re-weight network to predict eye position from non-occluded areas, such as the nose and mouth. The proposed re-weight sub-network revises the importance of different feature map positions and predicts the occluded eye positions by non-occluded parts. The proposed eye tracker is robust via a tracker-checker and a small model size. Experiments show that our method achieves high accuracy and speed, approximately 1.5 and 6.5 mm error for bare and sunglasses faces, respectively, at less than 10 ms on a 2.0GHz CPU. The evaluation dataset was captured indoors and outdoors to reflect multiple sunlight conditions. Our proposed method, combined with AR 3D HUDs, shows promising results for commercialization with low crosstalk 3D images. © 2013 IEEE.","augmented reality (AR) display; autostereoscopic three-dimensional display; eye position estimation; Eye tracking; head-up displays (HUDs); iris regression","Alignment; Augmented reality; Head-up displays; Three dimensional displays; Wear of materials; Coarse to fine; Descent method; Driving conditions; Face alignment; Facial landmark; Pupil tracking; Real-time eye tracking; Threedimensional (3-d); Eye tracking",Article,"Final","",Scopus,2-s2.0-85114750359
"Luo W., Cao J., Ishikawa K., Ju D.","57222567991;57248184300;57222574869;7005382458;","A human-computer control system based on intelligent recognition of eye movements and its application in wheelchair driving",2021,"Multimodal Technologies and Interaction","5","9","50","","",,,"10.3390/mti5090050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114364002&doi=10.3390%2fmti5090050&partnerID=40&md5=bd09e38bf69e158fd3f8da13c1891753","Graduate School of Engineering, Saitama Institute of Technology, Fukaya, 369-0217, Japan; Ningbo Haizhi Institute of Materials Industry Innovation, Ningbo, 315033, China; Tokyo Green Power Electric Research Institute Co., Ltd, Tokyo, 111-0022, Japan","Luo, W., Graduate School of Engineering, Saitama Institute of Technology, Fukaya, 369-0217, Japan, Ningbo Haizhi Institute of Materials Industry Innovation, Ningbo, 315033, China; Cao, J., Graduate School of Engineering, Saitama Institute of Technology, Fukaya, 369-0217, Japan; Ishikawa, K., Tokyo Green Power Electric Research Institute Co., Ltd, Tokyo, 111-0022, Japan; Ju, D., Graduate School of Engineering, Saitama Institute of Technology, Fukaya, 369-0217, Japan, Ningbo Haizhi Institute of Materials Industry Innovation, Ningbo, 315033, China, Tokyo Green Power Electric Research Institute Co., Ltd, Tokyo, 111-0022, Japan","This paper presents a practical human-computer interaction system for wheelchair motion through eye tracking and eye blink detection. In this system, the pupil in the eye image has been extracted after binarization, and the center of the pupil was localized to capture the trajectory of eye movement and determine the direction of eye gaze. Meanwhile, convolutional neural networks for feature extraction and classification of open-eye and closed-eye images have been built, and machine learning was performed by extracting features from multiple individual images of open-eye and closed-eye states for input to the system. As an application of this human-computer interaction control system, experimental validation was carried out on a modified wheelchair and the proposed method proved to be effective and reliable based on the experimental results. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Binarization; Convolutional neural networks; Human-computer interaction; Machine learning",,Article,"Final","",Scopus,2-s2.0-85114364002
"Zhou Z., Wang L., Popescu V.","57221494106;57207491164;7103266698;","A Partially-Sorted Concentric Layout for Efficient Label Localization in Augmented Reality",2021,"IEEE Transactions on Visualization and Computer Graphics",,,,"","",,,"10.1109/TVCG.2021.3106492","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113901056&doi=10.1109%2fTVCG.2021.3106492&partnerID=40&md5=db0ef761e53f09dd8494cd3fbd8f2e5c","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China, (e-mail: 418845530@qq.com); State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China, (e-mail: wanglily@buaa.edu.cn); Purdue University, U.S., (e-mail: popescu@purdue.edu)","Zhou, Z., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China, (e-mail: 418845530@qq.com); Wang, L., State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China, (e-mail: wanglily@buaa.edu.cn); Popescu, V., Purdue University, U.S., (e-mail: popescu@purdue.edu)","A common approach for Augmented Reality labeling is to display the label text on a flag planted into the real world element at a 3D anchor point. When there are more than just a few labels, the efficiency of the interface decreases as the user has to search for a given label sequentially. The search can be accelerated by sorting the labels alphabetically, but sorting all labels results in long and intersecting leader lines from the anchor points to the labels. This paper proposes a partially-sorted concentric label layout that leverages the search efficiency of sorting while avoiding the label display problems of long or intersecting leader lines. The labels are partitioned into a small number of sorted sequences displayed on circles of increasing radii. Since the labels on a circle are sorted, the user can quickly search each circle. A tight upper bound derived from circular permutation theory limits the number of circles and thereby the complexity of the label layout. For example, 12 labels require at most three circles. When the application allows it, the labels are presorted to further reduce the number of circles in the layout. The layout was tested in a user study where it significantly reduced the label searching time compared to a conventional single-circle layout. IEEE","Annotations; Augmented reality; Augmented Reality; Fast label finding; Gaze tracking; Label layout; Labeling; Layout; Search problems; Sorting","Efficiency; Anchor point; Circular permutation; Label layouts; Real-world; Search efficiency; Searching time; Upper Bound; User study; Augmented reality",Article,"Article in Press","",Scopus,2-s2.0-85113901056
"Stember J.N., Celik H., Gutman D., Swinburne N., Young R., Eskreis-Winkler S., Holodny A., Jambawalikar S., Wood B.J., Chang P.D., Krupinski E., Bagci U.","18538253000;57212691442;57222346959;55481511100;57232771700;51963422000;7004827467;6507408536;7401873523;57192687394;26643320200;57225322215;","Integrating eye tracking and speech recognition accurately annotates mr brain images for deep learning: Proof of principle",2021,"Radiology: Artificial Intelligence","3","1","e200047","","",,2,"10.1148/ryai.2020200047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113654183&doi=10.1148%2fryai.2020200047&partnerID=40&md5=29a50f1dc39340c03a8d08b8499b12d3","Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; The National Institutes of Health Clinical Center, BethesdaMD, United States; Department of Radiology, Columbia University Medical Center, New York, NY, United States; Department of Radiology, University of California–Irvine, Irvine, CA, United States; Department of Radiology & Imaging Sciences, Emory University, Atlanta, GA, United States; Center for Research in Computer Vision, University of Central Florida, Orlando, FLA, United States","Stember, J.N., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Celik, H., The National Institutes of Health Clinical Center, BethesdaMD, United States; Gutman, D., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Swinburne, N., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Young, R., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Eskreis-Winkler, S., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Holodny, A., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Jambawalikar, S., Department of Radiology, Columbia University Medical Center, New York, NY, United States; Wood, B.J., The National Institutes of Health Clinical Center, BethesdaMD, United States; Chang, P.D., Department of Radiology, University of California–Irvine, Irvine, CA, United States; Krupinski, E., Department of Radiology & Imaging Sciences, Emory University, Atlanta, GA, United States; Bagci, U., Center for Research in Computer Vision, University of Central Florida, Orlando, FLA, United States","Purpose: To generate and assess an algorithm combining eye tracking and speech recognition to extract brain lesion location labels automatically for deep learning (DL). Materials and Methods: In this retrospective study, 700 two-dimensional brain tumor MRI scans from the Brain Tumor Segmentation database were clinically interpreted. For each image, a single radiologist dictated a standard phrase describing the lesion into a microphone, simulating clinical interpretation. Eye-tracking data were recorded simultaneously. Using speech recognition, gaze points corresponding to each lesion were obtained. Lesion locations were used to train a keypoint detection convolutional neural network to find new lesions. A network was trained to localize lesions for an independent test set of 85 images. The statistical measure to evaluate our method was percent accuracy. Results: Eye tracking with speech recognition was 92% accurate in labeling lesion locations from the training dataset, thereby demonstrating that fully simulated interpretation can yield reliable tumor location labels. These labels became those that were used to train the DL network. The detection network trained on these labels predicted lesion location of a separate testing set with 85% accuracy. Conclusion: The DL network was able to locate brain tumors on the basis of training data that were labeled automatically from simulated clinical image interpretation. © RSNA, 2020.",,"Article; back propagation; brain damage; brain tumor; convolutional neural network; deep learning; human; image analysis; image segmentation; nuclear magnetic resonance imaging; prediction; retrospective study; speech discrimination",Article,"Final","",Scopus,2-s2.0-85113654183
"Baceviciute S., Lucas G., Terkildsen T., Makransky G.","55441702500;57228027800;57205338475;50361371800;","Investigating the redundancy principle in immersive virtual reality environments: An eye-tracking and EEG study",2021,"Journal of Computer Assisted Learning",,,,"","",,,"10.1111/jcal.12595","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113339050&doi=10.1111%2fjcal.12595&partnerID=40&md5=8f4d9464f0a90b67cf4820baba6e6b11","Department of Psychology, University of Copenhagen, Copenhagen, Denmark","Baceviciute, S., Department of Psychology, University of Copenhagen, Copenhagen, Denmark; Lucas, G., Department of Psychology, University of Copenhagen, Copenhagen, Denmark; Terkildsen, T., Department of Psychology, University of Copenhagen, Copenhagen, Denmark; Makransky, G., Department of Psychology, University of Copenhagen, Copenhagen, Denmark","Background: The increased availability of immersive virtual reality (IVR) has led to a surge of immersive technology applications in education. Nevertheless, very little is known about how to effectively design instruction for this new media, so that it would benefit learning and associated cognitive processing. Objectives: This experiment explores if and how traditional instructional design principles from 2D media translate to IVR. Specifically, it focuses on studying the underlying mechanisms of the redundancy-principle, which states that presenting the same information concurrently in two different sensory channels can cause cognitive overload and might impede learning. Methods: A total of 73 participants learned through a specifically-designed educational IVR application in three versions: (1) auditory representation format, (2) written representation format, and (3) a redundancy format (i.e. both written and auditory formats). The study utilized advanced psychophysiological methods of Electroencephalography (EEG) and eye-tracking (ET), learning measures and self-report scales. Results and Conclusions: Results show that participants in the redundancy condition performed equally well on retention and transfer post-tests. Similarly, results from the subjective measures, EEG and ET suggest that redundant content was not found to be more cognitively demanding than written content alone. Implications: Findings suggest that the redundancy effect might not generalize to VR as originally anticipated in 2D media research, providing direct implications to the design of IVR tools for education. © 2021 John Wiley & Sons Ltd","EEG; eye-tracking; immersive virtual reality; learning; redundancy principle",,Article,"Article in Press","",Scopus,2-s2.0-85113339050
"Khellat-Kihel S., Sun Z., Tistarelli M.","56582558200;8081773300;7003853982;","An Hybrid Attention-Based System for the Prediction of Facial Attributes",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12339 LNCS",,,"116","127",,,"10.1007/978-3-030-82427-3_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113290786&doi=10.1007%2f978-3-030-82427-3_9&partnerID=40&md5=3673bb5c51f215750be2bf1ecb6542c3","Computer Vision Laboratory, University of Sassari, Viale Italia 39, Sassari, 07100, Italy; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Room 1605, Intelligence Bulding, 95 Zhongguancun East Road, Beijing, 100190, China; Computer Vision Laboratory, Department of Biomedical Sciences and Information Technology, University of Sassari, Viale S. Pietro 43/b, Sassari, 07100, Italy","Khellat-Kihel, S., Computer Vision Laboratory, University of Sassari, Viale Italia 39, Sassari, 07100, Italy; Sun, Z., Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Room 1605, Intelligence Bulding, 95 Zhongguancun East Road, Beijing, 100190, China; Tistarelli, M., Computer Vision Laboratory, Department of Biomedical Sciences and Information Technology, University of Sassari, Viale S. Pietro 43/b, Sassari, 07100, Italy","Recent research on face analysis has demonstrated the richness of information embedded in feature vectors extracted from a deep convolutional neural network. Even though deep learning achieved a very high performance on several challenging visual tasks, such as determining the identity, age, gender and race, it still lacks a well grounded theory which allows to properly understand the processes taking place inside the network layers. Therefore, most of the underlying processes are unknown and not easy to control. On the other hand, the human visual system follows a well understood process in analyzing a scene or an object, such as a face. The direction of the eye gaze is repeatedly directed, through purposively planned saccadic movements, towards salient regions to capture several details. In this paper we propose to capitalize on the knowledge of the saccadic human visual processes to design a system to predict facial attributes embedding a biologically-inspired network architecture, the HMAX. The architecture is tailored to predict attributes with different textural information and conveying different semantic meaning, such as attributes related and unrelated to the subject’s identity. Salient points on the face are extracted from the outputs of the S2 layer of the HMAX architecture and fed to a local texture characterization module based on LBP (Local Binary Pattern). The resulting feature vector is used to perform a binary classification on a set of pre-defined visual attributes. The devised system allows to distill a very informative, yet robust, representation of the imaged faces, allowing to obtain high performance but with a much simpler architecture as compared to a deep convolutional neural network. Several experiments performed on publicly available, challenging, large datasets demonstrate the validity of the proposed approach. © 2021, The Author(s).",,"Biomimetics; Brain; Computation theory; Computer architecture; Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Eye movements; Forecasting; Large dataset; Network layers; Semantics; Textures; Binary classification; Biologically inspired networks; Human Visual System; Local binary patterns; Recent researches; Salient regions; Textural information; Visual attributes; Network architecture",Conference Paper,"Final","",Scopus,2-s2.0-85113290786
"Souza K.E.S.D., Aviz I.L.D., Mello H.D.D., Figueiredo K., Vellasco M.M.B.R., Costa F.A.R., Seruffo M.C.D.R.","57210323990;57211989161;57226889032;9639556600;56200580000;57226164970;23006319400;","An Evaluation Framework for User Experience Using Eye Tracking, Mouse Tracking, Keyboard Input, and Artificial Intelligence: A Case Study",2021,"International Journal of Human-Computer Interaction",,,,"","",,,"10.1080/10447318.2021.1960092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113227350&doi=10.1080%2f10447318.2021.1960092&partnerID=40&md5=53e9f51230565b8ba84afc08a0c7b263","Anthropic Studies in the Amazon Graduate Program, Federal University of Pará, Castanhal, Brazil; Institute of Technology, Federal University of Pará, Belem, Brazil; Electrical Engineering Department, Rio de Janeiro State University, Rio de Janeiro, Brazil; Department of Informatics and Computer Science, Rio de Janeiro State University, Rio de Janeiro, Brazil; Electrical Engineering Department, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Center for High Amazon Studies, Federal University of Pará, Belém, Brazil","Souza, K.E.S.D., Anthropic Studies in the Amazon Graduate Program, Federal University of Pará, Castanhal, Brazil; Aviz, I.L.D., Institute of Technology, Federal University of Pará, Belem, Brazil; Mello, H.D.D., Electrical Engineering Department, Rio de Janeiro State University, Rio de Janeiro, Brazil; Figueiredo, K., Department of Informatics and Computer Science, Rio de Janeiro State University, Rio de Janeiro, Brazil; Vellasco, M.M.B.R., Electrical Engineering Department, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Costa, F.A.R., Center for High Amazon Studies, Federal University of Pará, Belém, Brazil; Seruffo, M.C.D.R., Anthropic Studies in the Amazon Graduate Program, Federal University of Pará, Castanhal, Brazil","User eXperience (UX) has been used to achieve improvements in digital information systems based on how people perceive them. In particular, this paper establishes a framework that employs methods for eye and mouse tracking, keyboard input, self-assessment questionnaire and artificial intelligence algorithms to evaluate user experience and categorize users in terms of performance profiles. The results obtained with this framework are artifacts that can be used to support customizations of the User Interface (UI) on the websites. Moreover, the established framework is generic and flexible and can be applied to any information system, such as the case study shown in the website of the Federal Revenue of Brazil (RFB). The main objectives of this paper are as follows: (i) to set out a powerful UX framework based on three tracking techniques–the AIT2-UX; (ii) to provide the T2-UXT to collect, collate, process and visualize data obtained from users’ interactions (iii) to use and compare machine learning algorithms with the classification of user performance profiles; (iv) to use the artifacts generated by the framework to manually customize the UI with the website. © 2021 Taylor & Francis Group, LLC.",,"Information systems; Information use; Learning algorithms; Machine learning; Mammals; User experience; User interfaces; Websites; Artificial intelligence algorithms; Digital information systems; Evaluation framework; Performance profile; Self assessment; Tracking techniques; User experiences (ux); User performance; Eye tracking",Article,"Article in Press","",Scopus,2-s2.0-85113227350
"Xu Y., Zhang Z., Gao S.","57192081433;57204289144;35224747100;","Spherical DNNs and Their Applications in 360<formula><tex>$^\circ$</tex></formula> Images and Videos",2021,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,,,"","",,,"10.1109/TPAMI.2021.3100259","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112654967&doi=10.1109%2fTPAMI.2021.3100259&partnerID=40&md5=cbbc4fa5a4d96220edd0af2944da363d","Institute of High Performance Computing (IHPC), A*STAR, 54759 Singapore, Singapore, Singapore, (e-mail: xuyy2@shanghaitech.edu.cn); School of Information Science and Techology, ShanghaiTech University, 387433 Shanghai, Shanghai, China, (e-mail: zhangzh@shanghaitech.edu.cn); ShanghaiTech University, ShanghaiTech University, Shanghai, Shanghai, China, (e-mail: gaoshh@shanghaitech.edu.cn)","Xu, Y., Institute of High Performance Computing (IHPC), A*STAR, 54759 Singapore, Singapore, Singapore, (e-mail: xuyy2@shanghaitech.edu.cn); Zhang, Z., School of Information Science and Techology, ShanghaiTech University, 387433 Shanghai, Shanghai, China, (e-mail: zhangzh@shanghaitech.edu.cn); Gao, S., ShanghaiTech University, ShanghaiTech University, Shanghai, Shanghai, China, (e-mail: gaoshh@shanghaitech.edu.cn)","Spherical images or videos, as typical non-Euclidean data, are usually stored in the form of 2D panoramas obtained through an equirectangular projection, which is neither equal area nor conformal. The distortion caused by the projection limits the performance of vanilla Deep Neural Networks (DNNs) designed for traditional Euclidean data. In this paper, we design a novel Spherical Deep Neural Network (DNN) to deal with the distortion caused by the equirectangular projection. Specifically, we customize a set of components, including a spherical convolution, a spherical pooling, a spherical ConvLSTM cell and a spherical MSE loss, as the replacements of their counterparts in vanilla DNNs for spherical data. The core idea is to change the identical behavior of the conventional operations in vanilla DNNs across different feature patches so that they will be adjusted to the distortion caused by the variance of sampling rate among different feature patches. We demonstrate the effectiveness of our Spherical DNNs for saliency detection and gaze estimation in <formula><tex>$360^\circ$</tex></formula> videos. To facilitate the study of the 360 video saliency detection, we further construct a large-scale <formula><tex>$360^\circ$</tex></formula> video saliency detection dataset. Comprehensive experiments validate the effectiveness of our proposed Spherical DNNs for spherical handwritten digit classification and sport classification, saliency detection and gaze tracking in <formula><tex>$360^\circ$</tex></formula> videos. IEEE","360&#x00B0; Videos; Convolution; Distortion; Feature extraction; Gaze Prediction; Kernel; Saliency Detection; Saliency detection; Spherical Deep Neural Networks; Task analysis; Videos","Character recognition; Deep neural networks; Eye tracking; Large dataset; Neural networks; Gaze estimation; Gaze tracking; Handwritten digit classification; Non-Euclidean; Saliency detection; Sampling rates; Spherical images; Video saliencies; Spheres",Article,"Article in Press","",Scopus,2-s2.0-85112654967
"Joseph A.W., Vaiz J.S., Murugesh R.","57222028440;57226679244;57200213550;","Modeling Cognitive Load in Mobile Human Computer Interaction Using Eye Tracking Metrics",2021,"Lecture Notes in Networks and Systems","271",,,"99","106",,,"10.1007/978-3-030-80624-8_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112302323&doi=10.1007%2f978-3-030-80624-8_13&partnerID=40&md5=a24a07df0e3cc64265accbfb7ffa61cc","IT-Integrated Design, National Institute of Design, Bengaluru, Karnataka, India; Department of Computer Application, Madurai Kamaraj University, Madurai, India","Joseph, A.W., IT-Integrated Design, National Institute of Design, Bengaluru, Karnataka, India; Vaiz, J.S., Department of Computer Application, Madurai Kamaraj University, Madurai, India; Murugesh, R., Department of Computer Application, Madurai Kamaraj University, Madurai, India","Modeling cognitive load of user interaction based on ocular parameters have become a dominant method for exploring usability evaluation of interfaces for systems and applications. Growing importance of Artificial Intelligence in Human Computer Interaction (HCI) has proposed many approaches to understand users’ need and enhance human centric method for interface design. In particular, machine learning-based cognitive modeling, using eye tracking parameters have received more attention in the context of smart devices and applications. In this context, this paper aims to model the estimated cognitive load values for each user into different levels of cognition like very high, high, moderate, low, very low etc., while performing different tasks on a smart phone. The study focuses on the use behavioural measures, ocular parameters along with eight traditional machine learning classification algorithms like Decision Tree, Linear Discriminant Analysis, Random Forest, Support Vector Machine, Naïve Bayes, Neural Network, Fuzzy Rules with Weight Factor and K-Nearest Neighbor to model different levels of estimated cognitive load for each participant. The data set for modeling consisted of 250 records, 11 ocular parameters as prediction variables including age and type of task; and three types of classes (2-class, 3-class, 5-class) for classifying the estimated cognitive load for each participant. We noted that, Age, Fixation Count, Saccade Count, Saccade Rate, Average Pupil Dilation are the most important parameters contributing to modeling the estimated cognitive load levels. Further, we observed that, the Decision Tree algorithm achieved highest accuracy for classifying estimated cognitive load values into 2-class (86.8%), 3-class (74%) and 5-class (62.8%) respectively. Finally, from our study, it may be noted that, machine learning is an effective method for predicting 2-class-based (Low and High) cognitive load levels using ocular parameters. The outcome of the study also provides the fact that ageing affects users’ cognitive workload while performing tasks on smartphone. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Classification; Cognitive load levels; Eye tracking metrics; Human-computer interaction; Machine learning; Modeling cognitive load; Ocular parameters",,Conference Paper,"Final","",Scopus,2-s2.0-85112302323
"Breen M., McClarty J., Langley C., Farzidayeri J., Trevethan K., Swenson B., Sarkar M., Wade J., Sarkar N.","57226641558;57226647326;57226645471;57195959710;57226641722;57226650194;14054951000;55803508600;7201361624;","2D and 3D Visualization of Eye Gaze Patterns in a VR-Based Job Interview Simulator: Application in Educating Employers on the Gaze Patterns of Autistic Candidates",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12768 LNCS",,,"533","544",,,"10.1007/978-3-030-78092-0_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112227029&doi=10.1007%2f978-3-030-78092-0_36&partnerID=40&md5=240ff22982498b73f7ff8ea8c8c8e28b","Robotics and Autonomous Systems Lab, Vanderbilt University, Nashville, TN  37212, United States; Computer Science, Middle Tennessee State University, Murfreesboro, TN  37132, United States; Mechanical Engineering, Vanderbilt University, Nashville, TN  37212, United States","Breen, M., Robotics and Autonomous Systems Lab, Vanderbilt University, Nashville, TN  37212, United States; McClarty, J., Robotics and Autonomous Systems Lab, Vanderbilt University, Nashville, TN  37212, United States; Langley, C., Robotics and Autonomous Systems Lab, Vanderbilt University, Nashville, TN  37212, United States; Farzidayeri, J., Computer Science, Middle Tennessee State University, Murfreesboro, TN  37132, United States; Trevethan, K., Computer Science, Middle Tennessee State University, Murfreesboro, TN  37132, United States; Swenson, B., Computer Science, Middle Tennessee State University, Murfreesboro, TN  37132, United States; Sarkar, M., Computer Science, Middle Tennessee State University, Murfreesboro, TN  37132, United States; Wade, J., Robotics and Autonomous Systems Lab, Vanderbilt University, Nashville, TN  37212, United States, Mechanical Engineering, Vanderbilt University, Nashville, TN  37212, United States; Sarkar, N., Robotics and Autonomous Systems Lab, Vanderbilt University, Nashville, TN  37212, United States, Mechanical Engineering, Vanderbilt University, Nashville, TN  37212, United States","Employment of autistic individuals is strikingly low in relation to the skill level and capabilities of this population. Roughly 65% of autistic adults are either unemployed or underemployed relative to their abilities but there is increasing recognition that this number could be greatly improved through empowering autistic individuals while simultaneously providing a boost to the economy. Much of this disparity can be attributed in part to the lack of awareness and understanding among employers regarding behavior of autistic individuals during the hiring process. Most notably, the job interview—where strong eye contact is traditionally expected but can be extremely uncomfortable for autistic individuals—presents an unreasonable initial barrier to employment for many. The current work presents a data visualization dashboard that is populated with quantitative data (including eye tracking data) captured during simulated job interviews using a novel interview simulator called Career Interview Readiness in Virtual Reality (CIRVR). We conducted a brief series of case studies wherein autistic individuals who took part in a CIRVR interview and other key stakeholders provided lived experiences and qualitative insights into the most effective design and application of such data visualization dashboard. We conclude with a discussion of the role of information related to visual attention in job interviews with an emphasis on the importance of descriptive rather than prescriptive interpretation. © 2021, Springer Nature Switzerland AG.","Autism; Eye gaze; Inclusive employment; Job interview","Behavioral research; Data visualization; Employment; Eye tracking; Human computer interaction; Three dimensional computer graphics; User experience; Visualization; 2D and 3D visualization; Case-studies; Design and application; Hiring process; Job interviews; Quantitative data; Skill levels; Visual Attention; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85112227029
"Krishnaswamy N., Pustejovsky J.","57197865998;6602448845;","The Role of Embodiment and Simulation in Evaluating HCI: Experiments and Evaluation",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12777 LNCS",,,"220","232",,,"10.1007/978-3-030-77817-0_17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112226860&doi=10.1007%2f978-3-030-77817-0_17&partnerID=40&md5=2610302747437f965373e8f1205ba088","Colorado State University, Fort Collins, CO  80523, United States; Brandeis University, Waltham, MA  02453, United States","Krishnaswamy, N., Colorado State University, Fort Collins, CO  80523, United States; Pustejovsky, J., Brandeis University, Waltham, MA  02453, United States","In this paper series, we argue for the role embodiment plays in the evaluation of systems developed for Human Computer Interaction. We use a simulation platform, VoxWorld, for building Embodied Human Computer Interactions (EHCI). VoxWorld enables multimodal dialogue systems that communicate through language, gesture, action, facial expressions, and gaze tracking, in the context of task-oriented interactions. A multimodal simulation is an embodied 3D virtual realization of both the situational environment and the co-situated agents, as well as the most salient content denoted by communicative acts in a discourse. It is built on the modeling language VoxML, which encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextually salient inferences and decisions in the environment. Through simulation experiments in VoxWorld, we can begin to identify and then evaluate the diverse parameters involved in multimodal communication between agents. In this second part of this paper series, we discuss the consequences of embodiment and common ground, and how they help evaluate parameters of the interaction between humans and agents, and compare and contrast evaluation schemes enabled by different levels of embodied interaction. © 2021, Springer Nature Switzerland AG.","Common ground; Embodiment; HCI; Multimodal dialogue; VoxML","Computer simulation languages; Ergonomics; Eye tracking; Health risks; Modeling languages; Risk management; Safety engineering; Semantics; Speech processing; Communicative acts; Contrast evaluation; Embodied interaction; Facial Expressions; Multi-modal simulation; Multimodal communications; Multimodal dialogue systems; Virtual realization; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85112226860
"Pustejovsky J., Krishnaswamy N.","6602448845;57197865998;","The Role of Embodiment and Simulation in Evaluating HCI: Theory and Framework",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12777 LNCS",,,"288","303",,,"10.1007/978-3-030-77817-0_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112220152&doi=10.1007%2f978-3-030-77817-0_21&partnerID=40&md5=f13a4783ace036f025a0e7008c5f547a","Brandeis University, Waltham, MA  02453, United States; Colorado State University, Fort Collins, CO  80523, United States","Pustejovsky, J., Brandeis University, Waltham, MA  02453, United States; Krishnaswamy, N., Colorado State University, Fort Collins, CO  80523, United States","In this paper, we argue that embodiment can play an important role in the evaluation of systems developed for Human Computer Interaction. To this end, we describe a simulation platform for building Embodied Human Computer Interactions (EHCI). This system, VoxWorld, enables multimodal dialogue systems that communicate through language, gesture, action, facial expressions, and gaze tracking, in the context of task-oriented interactions. A multimodal simulation is an embodied 3D virtual realization of both the situational environment and the co-situated agents, as well as the most salient content denoted by communicative acts in a discourse. It is built on the modeling language VoxML, which encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextually salient inferences and decisions in the environment. Through simulation experiments in VoxWorld, we can begin to identify and then evaluate the diverse parameters involved in multimodal communication between agents. VoxWorld enables an embodied HCI by situating both human and computational agents within the same virtual simulation environment, where they share perceptual and epistemic common ground. In this first part of this paper series, we discuss the consequences of embodiment and common ground, and how they help evaluate parameters of the interaction between humans and agents, and demonstrate different behaviors and types of interactions on different classes of agents. © 2021, Springer Nature Switzerland AG.","Common ground; Embodiment; HCI; Multimodal dialogue; VoxML","Computation theory; Computer simulation languages; Ergonomics; Eye tracking; Health risks; Modeling languages; Risk management; Safety engineering; Semantics; Speech processing; Communicative acts; Computational agents; Facial Expressions; Multi-modal simulation; Multimodal communications; Multimodal dialogue systems; Virtual realization; Virtual simulation environments; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85112220152
"Savochkina E., Lee L.H., Drukker L., Papageorghiou A.T., Noble J.A.","57226651419;57217029182;36241434600;6603569987;57226264208;","First Trimester Gaze Pattern Estimation Using Stochastic Augmentation Policy Search for Single Frame Saliency Prediction",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12722 LNCS",,,"361","374",,,"10.1007/978-3-030-80432-9_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112210242&doi=10.1007%2f978-3-030-80432-9_28&partnerID=40&md5=72246c163f65073924e746a627f904af","Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Nuffield Department of Women’s and Reproductive Health, University of Oxford, Oxford, United Kingdom","Savochkina, E., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Lee, L.H., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Drukker, L., Nuffield Department of Women’s and Reproductive Health, University of Oxford, Oxford, United Kingdom; Papageorghiou, A.T., Nuffield Department of Women’s and Reproductive Health, University of Oxford, Oxford, United Kingdom; Noble, J.A., Department of Engineering Science, University of Oxford, Oxford, United Kingdom","While performing an ultrasound (US) scan, sonographers direct their gaze at regions of interest to verify that the correct plane is acquired and to interpret the acquisition frame. Predicting sonographer gaze on US videos is useful for identification of spatio-temporal patterns that are important for US scanning. This paper investigates utilizing sonographer gaze, in the form of gaze-tracking data, in a multi-modal imaging deep learning framework to assist the analysis of the first trimester fetal ultrasound scan. Specifically, we propose an encoder-decoder convolutional neural network with skip connections to predict the visual gaze for each frame using 115 first trimester ultrasound videos; 29,250 video frames for training, 7,290 for validation and 9,126 for testing. We find that the dataset of our size benefits from automated data augmentation, which in turn, alleviates model overfitting and reduces structural variation imbalance of US anatomical views between the training and test datasets. Specifically, we employ a stochastic augmentation policy search method to improve segmentation performance. Using the learnt policies, our models outperform the baseline: KLD, SIM, NSS and CC (2.16, 0.27, 4.34 and 0.39 versus 3.17, 0.21, 2.92 and 0.28). © 2021, Springer Nature Switzerland AG.","Data augmentation; Fetal ultrasound; First trimester; Gaze tracking; Single frame saliency prediction; U-Net","Convolutional neural networks; Deep learning; Forecasting; Image analysis; Image understanding; Medical imaging; Statistical tests; Stochastic systems; Ultrasonics; Fetal ultrasound; Learning frameworks; Multi-modal imaging; Regions of interest; Segmentation performance; Spatiotemporal patterns; Structural variations; Ultrasound videos; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85112210242
"Chakraborty P., Ahmed S., Yousuf M.A., Azad A., Alyami S.A., Moni M.A.","57216817907;57225877868;57188663159;36087132900;57190760454;35119094400;","A Human-Robot Interaction System Calculating Visual Focus of Human's Attention Level",2021,"IEEE Access","9",,"9462086","93409","93421",,3,"10.1109/ACCESS.2021.3091642","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112114960&doi=10.1109%2fACCESS.2021.3091642&partnerID=40&md5=42c7e47f1cd68060b7d6b4891ef7acf5","Department of Computer Science and Engineering, Comilla University, Cumilla, Bangladesh; Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh; School of Biotechnology and Biomolecular Sciences, University of New South Wales Sydney (UNSW Sydney), Sydney, NSW, Australia; Department of Mathematics and Statistics, Faculty of Science, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, Saudi Arabia; WHO Collaborating Centre on EHealth, UNSW Digital Health, University of New South Wales Sydney (UNSW Sydney), Sydney, NSW, Australia","Chakraborty, P., Department of Computer Science and Engineering, Comilla University, Cumilla, Bangladesh; Ahmed, S., Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh; Yousuf, M.A., Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh; Azad, A., School of Biotechnology and Biomolecular Sciences, University of New South Wales Sydney (UNSW Sydney), Sydney, NSW, Australia; Alyami, S.A., Department of Mathematics and Statistics, Faculty of Science, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, Saudi Arabia; Moni, M.A., WHO Collaborating Centre on EHealth, UNSW Digital Health, University of New South Wales Sydney (UNSW Sydney), Sydney, NSW, Australia","Attention is the mental awareness of human on a particular object or a piece of information. The level of attention indicates how intense the focus is on an object or an instance. In this study, several types of human attention level have been observed. After introducing image segmentation and detection technique for facial features, eyeball movement and gaze estimation were measured. Eye movement were assessed using the video data, and a total of 10197 data instances were manually labelled for the attention level. Then Artificial Neural Network (ANN) and Recurrent Neural Network-Long Short Term Memory (LSTM) based Deep learning (DL) architectures have been proposed for analysing the data. Next, the trained DL model has been implanted into a robotic system that is capable of detecting various features; ultimately leading to the calculation of visual attention for reading, browsing, and writing purposes. This system is capable of checking the attention level of the participants and also can detect if participants are present or not. Based on a certain level of visual focus of attention (VFOA), this system interacts with the person, generates awareness and establishes verbal or visual communication with that person. The proposed ML techniques have achieved almost 99.24% validation accuracy and 99.43% test accuracy. It is also shown in the comparative study that, since the dataset volumes are limited, ANN is more suitable for attention level calculation than RNN-LSTM. We hope that the implemented robotic structure manifests the real-world implication of the proposed method. © 2013 IEEE.","ANN; attention level; concentration; Human-robot interaction; RNN-LSTM; visual focus of attention","Behavioral research; Deep learning; Eye movements; Feature extraction; Image segmentation; Long short-term memory; Robotics; Visual communication; Attention level; Comparative studies; Eyeball movements; Gaze estimation; Human attention; Robotic structures; Visual Attention; Visual focus of attentions; Social robots",Article,"Final","",Scopus,2-s2.0-85112114960
"Chen S., Zhao Y., Wu T., Li Y.","57218312392;57194688959;57190308770;56919769400;","Exploring Relationships Between Distractibility and Eye Tracking During Online Learning",2021,"Lecture Notes in Networks and Systems","259",,,"254","266",,,"10.1007/978-3-030-80285-1_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112063689&doi=10.1007%2f978-3-030-80285-1_31&partnerID=40&md5=19be129a4d9b6dd3bfae0a23e7ab9912","School of Degisn Art and Media, Nanjing University of Science and Technology, Xiaoling 200#, Xuanwu District, Jiangsu, Nanjing, China","Chen, S., School of Degisn Art and Media, Nanjing University of Science and Technology, Xiaoling 200#, Xuanwu District, Jiangsu, Nanjing, China; Zhao, Y., School of Degisn Art and Media, Nanjing University of Science and Technology, Xiaoling 200#, Xuanwu District, Jiangsu, Nanjing, China; Wu, T., School of Degisn Art and Media, Nanjing University of Science and Technology, Xiaoling 200#, Xuanwu District, Jiangsu, Nanjing, China; Li, Y., School of Degisn Art and Media, Nanjing University of Science and Technology, Xiaoling 200#, Xuanwu District, Jiangsu, Nanjing, China","More than half of students think their attention is easily shifted when they’re learning online. Distractibility, to a certain extent caused by visual stimuli is the main impact to decrease their academic performance. In addition, eye-tracking technology has been widely applied to explore distractibility in many “look” tasks, such as reading, viewing advertisements, and watching online videos as well as measure the efficiency of visual cognition. Therefore, this paper aimed to discuss the relationship between distractibility with eye movement indices and academic performance. Fifty high school students (30 girls) were recruited to complete experiment that was divided into two groups, which are the experimental group with distractions and controls with no one. The result showed that three of traditional eye movement indices were significantly correlated with distractibility (p&amp;lt; 0.05 ). Then we introduced the network accessibility model and the gaze transformation entropy to create two composite indexes according to the complexity and directivity of distractibility characteristics. The result revealed that the two composite indexes are significantly correlated with distractibility (p&amp;lt; 0.05 ). Finally, we constructed the mapping model about eye movement metrics about distractibility and online learning performance with a machine learning algorithm. The result ration was R2= 0.799, and the error was Re&amp;lt; 0.1, which proved the model was feasible and accessible. The research from the perspective of distractibility can provide valuable support for physiological indicators testing tools of academic performance and highlights the applications of eye movement dynamics. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Distractibility; Entropy; Eye tracking; Interactive design; Machine learning algorithm; Online learning",,Conference Paper,"Final","",Scopus,2-s2.0-85112063689
"Paletta L., Ganster H., Schneeberger M., Pszeida M., Lodron G., Pechstädt K., Spitzer M., Reischl C.","6602696802;6603211377;57188694703;56160335300;36168421000;57219436429;57226402013;57226398569;","Towards large-scale evaluation of mental stress and biomechanical strain in manufacturing environments using 3D-referenced gaze and wearable-based analytics",2021,"IS and T International Symposium on Electronic Imaging Science and Technology","2021","6","310","","",,,"10.2352/ISSN.2470-1173.2021.6.IRIACV-310","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111470852&doi=10.2352%2fISSN.2470-1173.2021.6.IRIACV-310&partnerID=40&md5=dc7460e57ca8eccabec9fe6094b817df","JOANNEUM RESEARCH, Graz, Austria; FH JOANNEUM, Bad Gleichenberg, Graz, Austria","Paletta, L., JOANNEUM RESEARCH, Graz, Austria; Ganster, H., JOANNEUM RESEARCH, Graz, Austria; Schneeberger, M., JOANNEUM RESEARCH, Graz, Austria; Pszeida, M., JOANNEUM RESEARCH, Graz, Austria; Lodron, G., JOANNEUM RESEARCH, Graz, Austria; Pechstädt, K., FH JOANNEUM, Bad Gleichenberg, Graz, Austria; Spitzer, M., FH JOANNEUM, Bad Gleichenberg, Graz, Austria; Reischl, C., FH JOANNEUM, Bad Gleichenberg, Graz, Austria","In future manufacturing human-machine interaction will evolve towards flexible and smart collaboration. It will meet requirements from the optimization of assembly processes as well as from motivated and skilled human behavior. Recently, human factors engineering has substantially progressed by means of detailed task analysis. However, there is still a lack in precise measuring cognitive and sensorimotor patterns for the analysis of long-term mental and physical strain. This work presents a novel methodology that enables real-time measurement of cognitive load based on executive function analyses as well as biomechanical strain from non-obtrusive wearable sensors. The methodology works on 3D information recovery of the working cell using a precise stereo measurement device. The worker is equipped with eye tracking glasses and a set of wearable accelerometers. Wireless connectivity transmits the sensor-based data to a nearby PC for monitoring. Data analytics then recovers the 3D geometry of gaze and viewing frustum within the working cell and furthermore extracts the worker's task switching rate as well as a skeleton-based approximation of worker's posture associated with an estimation of biomechanical strain of muscles and joints. First results enhanced by AI-based estimators demonstrate a good match with the results of an activity analysis performed by occupational therapists. © 2021, Society for Imaging Science and Technology.",,"Behavioral research; Biomechanics; Data Analytics; Eye tracking; Human engineering; Industrial robots; Intelligent robots; Job analysis; Manufacture; Molecular biology; Stereo image processing; Switching frequency; Executive function; Human machine interaction; Manufacturing environments; Precise measuring; Real time measurements; Sensor based data; Stereo measurements; Wireless connectivities; Wearable sensors",Conference Paper,"Final","",Scopus,2-s2.0-85111470852
"Nourrit V., Poilane R., De Bougrenet J.-L.","56108541100;57226407965;54404725400;","Custom on-axis head-mounted eye tracker for 3D active glasses",2021,"IS and T International Symposium on Electronic Imaging Science and Technology","2021","2","055","","",,,"10.2352/ISSN.2470-1173.2021.2.SDA-055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111456581&doi=10.2352%2fISSN.2470-1173.2021.2.SDA-055&partnerID=40&md5=79bd0cbbf24bc98bffce07607e12fa8f","IMT Atlantique, Brest, France; Eye Triple Shut, Brest, France","Nourrit, V., IMT Atlantique, Brest, France; Poilane, R., Eye Triple Shut, Brest, France; De Bougrenet, J.-L., IMT Atlantique, Brest, France","Currently, no low cost commercial 3D active glasses with embedded eye tracker are available despite the importance of 3D and eye tracking for numerous applications. In this context, a simple low cost eye tracker for 3D glasses with liquid crystal shutters is presented and tested for orthoptics applications. By using a beam splitter to better align the camera with the line of sight when the subject looks at a target in front of him at far range, the new design allows recording high quality images with limited pupil deformation when compared to other commercial eye trackers where the cameras can be far from this axis (head mounted or fixed). Such a design could be useful for various applications from orthoptics to virtual reality. © 2021, Society for Imaging Science and Technology.",,"Cameras; Costs; Glass; Liquid crystals; Medicine; Stereo image processing; Active glass; Eye trackers; High quality images; Line of Sight; Liquid crystal shutter; Low costs; On-axis; Orthoptics; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85111456581
"Lee S., Park J., Nam D.","35778392800;50961546500;43761314400;","Crosstalk minimization method for eye-tracking-based 3D display",2021,"IS and T International Symposium on Electronic Imaging Science and Technology","2021","2","060407","","",,,"10.2352/J.ImagingSci.Technol.2020.64.6.060407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111413299&doi=10.2352%2fJ.ImagingSci.Technol.2020.64.6.060407&partnerID=40&md5=1fe55847ac1c12d1c49441d4c6ace55f","Computer Vision Lab, Samsung Advanced Institute of Technology, Yeongtong-gu, Gyeonggi-do, Suwon-si, South Korea","Lee, S., Computer Vision Lab, Samsung Advanced Institute of Technology, Yeongtong-gu, Gyeonggi-do, Suwon-si, South Korea; Park, J., Computer Vision Lab, Samsung Advanced Institute of Technology, Yeongtong-gu, Gyeonggi-do, Suwon-si, South Korea; Nam, D., Computer Vision Lab, Samsung Advanced Institute of Technology, Yeongtong-gu, Gyeonggi-do, Suwon-si, South Korea","In this article, the authors present an image processing method to reduce three-dimensional (3D) crosstalk for eye-tracking-based 3D display. Specifically, they considered 3D pixel crosstalk and offset crosstalk and applied different approaches based on its characteristics. For 3D pixel crosstalk which depends on the viewer's relative location, they proposed output pixel value weighting scheme based on viewer's eye position, and for offset crosstalk they subtracted luminance of crosstalk components according to the measured display crosstalk level in advance. By simulations and experiments using the 3D display prototypes, the authors evaluated the effectiveness of proposed method. © 2020 Society for Imaging Science and Technology.",,"Crosstalk; Eye tracking; Pixels; Stereo image processing; Cross-talk levels; Crosstalk minimization; Eye position; Image processing - methods; Pixel values; Relative location; Threedimensional (3-d); Weighting scheme; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85111413299
"Gerbasi A., Groznik V., Georgiev D., Sacchi L., Sadikov A.","57226379586;52463560700;36542322400;57192331744;55883901600;","Detecting Mild Cognitive Impairment Using Smooth Pursuit and a Modified Corsi Task",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12721 LNAI",,,"168","172",,,"10.1007/978-3-030-77211-6_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111388619&doi=10.1007%2f978-3-030-77211-6_19&partnerID=40&md5=ca1508e8fe4b76376ef452a986a9cef7","Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Pavia, Italy; Faculty of Computer and Information Science, University of Ljubljana, Ljubljana, Slovenia; NEUS Diagnostics, d.o.o., Ljubljana, Slovenia; Faculty of Mathematics, University of Primorska, Natural Sciences and Information Technologies, Koper, Slovenia; Department of Neurology, University Medical Centre Ljubljana, Ljubljana, Slovenia","Gerbasi, A., Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Pavia, Italy, Faculty of Computer and Information Science, University of Ljubljana, Ljubljana, Slovenia; Groznik, V., Faculty of Computer and Information Science, University of Ljubljana, Ljubljana, Slovenia, NEUS Diagnostics, d.o.o., Ljubljana, Slovenia, Faculty of Mathematics, University of Primorska, Natural Sciences and Information Technologies, Koper, Slovenia; Georgiev, D., Faculty of Computer and Information Science, University of Ljubljana, Ljubljana, Slovenia, Department of Neurology, University Medical Centre Ljubljana, Ljubljana, Slovenia; Sacchi, L., Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Pavia, Italy; Sadikov, A., Faculty of Computer and Information Science, University of Ljubljana, Ljubljana, Slovenia, NEUS Diagnostics, d.o.o., Ljubljana, Slovenia","Over 50 million people today live with some form of dementia as it is the most common neurodegenerative disease in the world. Mild cognitive impairment (MCI) is a stage before dementia symptoms overtly manifest. An estimated 10–15% of patients diagnosed with MCI annually convert to Alzheimer’s dementia. Early detection of MCI is imperative as disease-modifying therapies in development could have the potential to significantly delay disease progression before dementia symptoms develop. There is evidence that observing oculomotor movements during different neuropsychological tasks can serve as a biomarker for MCI. A clinical study with 105 participants was performed at several centres in Ljubljana, Slovenia. All the participants underwent an extensive neurological and psychological evaluation and were, on the basis of this evaluation, divided into two groups: cognitively impaired and healthy controls. At the same time the participants performed several short tasks on the computer screen, including smooth pursuit dot tracking and a modified version of the Corsi block-tapping test. During the tasks, performed using their gaze alone, their eye movements were recorded with an eye-tracker. The eye-tracking data was analysed and a number of features describing the gaze behaviour was proposed. These features were used to construct several machine learning models to predict whether a person exhibits signs of cognitive impairment or not. A model based on random forest classifier achieved the best performance with 80% classification accuracy and an area under the ROC curve of 85%. © 2021, Springer Nature Switzerland AG.","Corsi block-tapping test; Early detection; Eye-tracking; Machine learning; Mild cognitive impairment (MCI); Smooth pursuit","Artificial intelligence; Decision trees; Disease control; Eye movements; Neurodegenerative diseases; Patient monitoring; Area under the ROC curve; Classification accuracy; Cognitively impaired; Machine learning models; Mild cognitive impairments; Mild cognitive impairments (MCI); Psychological evaluation; Random forest classifier; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85111388619
"Sakya G., Singh S., Mishra S., Tiwari S.M.","55786972300;57226390360;57226389199;57226381894;","Intelligent Invigilation Using Video Surveillance",2021,"Lecture Notes in Networks and Systems","197 LNNS",,,"401","411",,,"10.1007/978-981-16-0980-0_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111379750&doi=10.1007%2f978-981-16-0980-0_37&partnerID=40&md5=9dc68c916d7b6cda2654377f8e8676d7","JSS Academy of Technical Education, Noida, India","Sakya, G., JSS Academy of Technical Education, Noida, India; Singh, S., JSS Academy of Technical Education, Noida, India; Mishra, S., JSS Academy of Technical Education, Noida, India; Tiwari, S.M., JSS Academy of Technical Education, Noida, India","At the examination center, examinees daily face difficulties in finding their seating position that consumes a lot of time. Also in COVID-19 pandemic, we need to maintain social distancing while taking examination offline. So, this paper proposes system that can help examinees in knowing their seat number in their respective examination hall with the help of online Web portal. It can help in eliminating the crowd at the notice board and can also save their time. With the help of an admit card generated to which a barcode be attached, an examinee can scan that code each day and know their seating location as per a daily basis. Another system is also proposed for face recognition that can help the examiner in identifying the identity of candidates that can also alert them if they are not found in the database using deep learning. Within the examination hall, the orientation of the head and movement of the mouth of an examinee provide us the clue of suspicious behaviors. Nowadays, most of the suspicious behaviors monitoring procedures are done manually that involves a lot of invigilators in each examination hall. In the pandemic situation, we proposed an intelligent invigilation using video surveillance that can autonomously detect and track examinee’s eye gaze, head orientation, and mouth movement to robustly detect their cheating activities. Algorithms which are implemented independently include eigen-face, fisherface, and linear binary pattern histograms. With the help of webcam installed at each examinee’s desk, if any suspicious activity is detected, the system will generate an alarm indicating such behaviors. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Deep learning; Faces recognition; Intelligent invigilation; Video surveillance; Web development",,Conference Paper,"Final","",Scopus,2-s2.0-85111379750
"Gite S., Pradhan B., Alamri A., Kotecha K.","56656365900;12753037900;57215408871;6506676097;","ADMT: Advanced driver's movement tracking system using spatio-temporal interest points and maneuver anticipation using deep neural networks",2021,"IEEE Access","9",,"9478887","99312","99326",,2,"10.1109/ACCESS.2021.3096032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111162872&doi=10.1109%2fACCESS.2021.3096032&partnerID=40&md5=09edc144bbb551bbd59b0d2ee4598179","Department of Computer Science and Information Technology, Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, 412115, India; Symbiosis Centre of Applied A.I. (SCAAI), Symbiosis International, Deemed University, Pune, 412115, India; Centre for Advanced Modelling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and I.T., University of Technology Sydney, Ultimo, NSW  2007, Australia; Earth Observation Center, Institute of Climate Change, Universiti Kebangsaan Malaysia, Selangor, Bangi, 43600, Malaysia; Department of Geology and Geophysics, College of Science, King Saud University, Riyadh, 11451, Saudi Arabia","Gite, S., Department of Computer Science and Information Technology, Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, 412115, India, Symbiosis Centre of Applied A.I. (SCAAI), Symbiosis International, Deemed University, Pune, 412115, India; Pradhan, B., Centre for Advanced Modelling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and I.T., University of Technology Sydney, Ultimo, NSW  2007, Australia, Earth Observation Center, Institute of Climate Change, Universiti Kebangsaan Malaysia, Selangor, Bangi, 43600, Malaysia; Alamri, A., Department of Geology and Geophysics, College of Science, King Saud University, Riyadh, 11451, Saudi Arabia; Kotecha, K., Department of Computer Science and Information Technology, Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, 412115, India, Symbiosis Centre of Applied A.I. (SCAAI), Symbiosis International, Deemed University, Pune, 412115, India","Assistive driving is a complex engineering problem and is influenced by several factors such as the sporadic nature of the quality of the environment, the response of the driver, and the standard of the roads on which the vehicle is being driven. The authors track the driver's anticipation based on his head movements using Spatio-Temporal Interest Point (STIP) extraction and enhance the anticipation of action accuracy well before using the RNN-LSTM framework. This research tackles a fundamental problem of lane change assistance by developing a novel model called Advanced Driver's Movement Tracking (ADMT). ADMT uses customized convolution-based deep learning networks by using Recurrent Convolutional Neural Network (RCNN). STIP with eye gaze extraction and RCNN performed in ADMT on brain4cars dataset for driver movement tracking. Its performance is compared with the traditional machine learning and deep learning models, namely Support Vector Machines (SVM), Hidden Markov Model (HMM), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and provided an increment of almost 12% in the prediction accuracy and 44% in the anticipation time. Furthermore, ADMT systems outperformed all of the models in terms of both the accuracy of the system and the previously mentioned time of anticipation that is discussed at length in the paper. Thus it assists the driver with additional anticipation time to access the typical reaction time for better preparedness to respond to undesired future behavior. The driver is then assured of a safe and assisted driving experience with the proposed system. © 2013 IEEE.","Advanced driver movement tracking system; Deep neural networks; Eye gaze tracking; RCNN; Spatio-temporal interest points","Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Extraction; Eye movements; Hidden Markov models; Learning systems; Motion analysis; Support vector machines; Tracking (position); Assisted drivings; Complex engineering problems; Learning models; Learning network; Movement tracking systems; Prediction accuracy; Recurrent neural network (RNN); Spatio-temporal interest points; Long short-term memory",Article,"Final","",Scopus,2-s2.0-85111162872
"Chihara T., Sakamoto J.","24069807600;57226308452;","Effect of Time Length of Eye Movement Data Analysis on the Accuracy of Mental Workload Estimation During Automobile Driving",2021,"Lecture Notes in Networks and Systems","221 LNNS",,,"593","599",,,"10.1007/978-3-030-74608-7_72","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111080269&doi=10.1007%2f978-3-030-74608-7_72&partnerID=40&md5=686e990858d5c42a22d7569665d088a9","Kanazawa University, Ishikawa, Japan","Chihara, T., Kanazawa University, Ishikawa, Japan; Sakamoto, J., Kanazawa University, Ishikawa, Japan","We investigated the appropriate time window duration for calculating eye and head movement parameters in mental workload (MWL) estimation during automobile driving. Participants performed driving tasks on a driving simulator, and eye and head movements were measured by controlling their MWL using the N-back task, which required them to keep answering aloud the N-th previous digit in a sequence of digits. The eye and head movement parameters were calculated by changing a time window from 30 s to 150 s in increments of 30 s. An anomaly detector of MWL was constructed using the one-class support vector machine (OCSVM) with the no N-back task (“None”) data. In each window length condition, we calculated the area under curve (AUC) for the binary classification between None and the highest MWL condition, the percentage of anomaly data, and the distance from the decision boundary. The results showed that a time window of 30 s had significantly lower AUC compared with other time windows. In addition, the correlation coefficient between the subjective MWL score and the distance of each eye movement parameter data from the decision boundary monotonically increased in the time window 30 s to 120 s and decreased at 150 s. Therefore, we concluded that 60 s to 120 s is an appropriate time window duration for MWL evaluation. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Anomaly detection; Driver monitoring; Eye tracking; Machine learning; Mental workload; One-class support vector machine",,Conference Paper,"Final","",Scopus,2-s2.0-85111080269
"Bruno A., Lancette S., Zhang J., Moore M., Ward V.P., Chang J.","7102246682;57220102595;56349080600;57220101758;57220105073;55514990300;","A saliency-based technique for advertisement layout optimisation to predict customers’ behaviour",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12662 LNCS",,,"495","507",,,"10.1007/978-3-030-68790-8_39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110771235&doi=10.1007%2f978-3-030-68790-8_39&partnerID=40&md5=f5fc2dea375f9a2c1336c3cd1cb85245","National Centre for Computer Animation, Bournemouth University, Poole, BH12 5BB, United Kingdom; Shoppar Ltd., Plexal, 14 East Bay Lane, Stratford London, E20 3BS, United Kingdom","Bruno, A., National Centre for Computer Animation, Bournemouth University, Poole, BH12 5BB, United Kingdom; Lancette, S., National Centre for Computer Animation, Bournemouth University, Poole, BH12 5BB, United Kingdom; Zhang, J., National Centre for Computer Animation, Bournemouth University, Poole, BH12 5BB, United Kingdom; Moore, M., National Centre for Computer Animation, Bournemouth University, Poole, BH12 5BB, United Kingdom; Ward, V.P., Shoppar Ltd., Plexal, 14 East Bay Lane, Stratford London, E20 3BS, United Kingdom; Chang, J., National Centre for Computer Animation, Bournemouth University, Poole, BH12 5BB, United Kingdom","Customer retail environments represent an exciting and challenging context to develop and put in place cutting-edge computer vision techniques for more engaging customer experiences. Visual attention is one of the aspects that play such a critical role in the analysis of customers behaviour on advertising campaigns continuously displayed in shops and retail environments. In this paper, we approach the optimisation of advertisement layout content, aiming to grab the audience’s visual attention more effectively. We propose a fully automatic method for the delivery of the most effective layout content configuration using saliency maps out of each possible set of images with a given grid layout. Visual Saliency deals with the identification of the most critical regions out of pictures from a perceptual viewpoint. We want to assess the feasibility of saliency maps as a tool for the optimisation of advertisements considering all possible permutations of images which compose the advertising campaign itself. We start by analysing advertising campaigns consisting of a given spatial layout and a certain number of images. We run a deep learning-based saliency model over all permutations. Noticeable differences among global and local saliency maps occur over different layout content out of the same images. The latter aspect suggests that each image gives its contribution to the global visual saliency because of its content and location within the given layout. On top of this consideration, we employ some advertising images to set up a graphical campaign with a given design. We extract relative variance values out the local saliency maps of all permutations. We hypothesise that the inverse of relative variance can be used as an Effectiveness Score (ES) to catch those layout content permutations showing the more balanced spatial distribution of salient pixel. A group of 20 participants have run some eye-tracking sessions over the same advertising layouts to validate the proposed method. © Springer Nature Switzerland AG 2021.","Computer vision; Deep learning; Layout optimisation; Retail environment; Visual saliency","Behavioral research; Deep learning; Eye tracking; Inverse problems; Pattern recognition; Visualization; Advertising campaign; Automatic method; Computer vision techniques; Critical region; Customer experience; Saliency modeling; Visual Attention; Visual saliency; Sales",Conference Paper,"Final","",Scopus,2-s2.0-85110771235
"Sinthanayothin C., Bholsithi W., Wongwaen N.","25032111600;8561444900;36607051500;","Morph targets for 3D facial animation with webcam using facemesh, Jeeliz-transfer APIs and Three.js",2021,"Proceedings of SPIE - The International Society for Optical Engineering","11878",,"1187820","","",,,"10.1117/12.2600859","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109369647&doi=10.1117%2f12.2600859&partnerID=40&md5=f64f134246feade84a7bb5c5196eb691","National Electronics and Computer Technology Center, National Science and Technology Development Agency (NSTDA), 112, Thailand Science Park, Klong 1, Pathum Thani, Klong Luang, 12120, Thailand","Sinthanayothin, C., National Electronics and Computer Technology Center, National Science and Technology Development Agency (NSTDA), 112, Thailand Science Park, Klong 1, Pathum Thani, Klong Luang, 12120, Thailand; Bholsithi, W., National Electronics and Computer Technology Center, National Science and Technology Development Agency (NSTDA), 112, Thailand Science Park, Klong 1, Pathum Thani, Klong Luang, 12120, Thailand; Wongwaen, N., National Electronics and Computer Technology Center, National Science and Technology Development Agency (NSTDA), 112, Thailand Science Park, Klong 1, Pathum Thani, Klong Luang, 12120, Thailand","This paper presents a face control system for 3D avatar with webcam using the Facemesh API for face tracking and Jeeliz-transfer API for eye tracking. 3D avatar face animation is developed as a responsive web application. It starts with face detection and tracking through the webcam. Face coordinate data is normalized to a vertical face view where the distance between the eyes and the level of the eyes are the same for each video frame. Then new face coordinates are calculated in both 2D and 3D to study the change of specific coordinates such as mouth shape and face shape. In addition, the coordinates of face structures in the video platform have also been added to the 3D model platform. Specific coordinates are studied to analyze distance changes to be applied in 3D avatar manipulation. The 3D models are designed and created in multiple blend-shapes or basic character facial features. However, due to the limitations of web browser-based 3D morphing, which Three.js is used for morph target displays, it allows only eight combination shapes to be displayed at the same time. Therefore, the required blending geometry must be pre-assembled. Blend-shape factors are based on an analysis of the coordinates of each moving face in order to eliminate the limitations. Our 3D facial animations with a webcam generate results of high quality, real-time and online simulation. Therefore, our work is a fundamental technology that can be applied to animate other 3D characters in blend-shape format. © 2021 SPIE.","3D face animation; Blendshape; Face tracking; Facemesh; Jeeliz-transfer; Three.js","3D modeling; Animation; Eye tracking; Three dimensional computer graphics; 3d facial animations; Face detection and tracking; Face structure; Facial feature; Model platform; Online simulation; Video-platforms; WEB application; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85109369647
"Amer S.G., Kamh S.A., Elshahed M.A., Ramadan R.A.","57225110585;6602245911;56494900800;15058265600;","Wheelchair Control System based Eye Gaze",2021,"International Journal of Advanced Computer Science and Applications","12","6",,"895","900",,,"10.14569/IJACSA.2021.01206104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109153735&doi=10.14569%2fIJACSA.2021.01206104&partnerID=40&md5=add1e75daee5009f355b7d29ff8199b5","Department of Physics, Faculty of Women, Ain Shames University, Cairo, Egypt; Computer Engineering Department, Faculty of Engineering, Cairo University, Cairo, Egypt","Amer, S.G., Department of Physics, Faculty of Women, Ain Shames University, Cairo, Egypt; Kamh, S.A., Department of Physics, Faculty of Women, Ain Shames University, Cairo, Egypt; Elshahed, M.A., Department of Physics, Faculty of Women, Ain Shames University, Cairo, Egypt; Ramadan, R.A., Computer Engineering Department, Faculty of Engineering, Cairo University, Cairo, Egypt","The inability to control the limbs is the main reason that affects the daily activities of the disabled which causes social restrictions and isolation. More studies were performed to help disabilities for easy communication with the outside world and others. Various techniques are designed to help the disabled in carrying out daily activities easily. Among these technologies is the Smart Wheelchair. This research aims to develop a smart eye-controlled wheelchair whose movement depends on eye movement tracking. The proposed Wheelchair is simple in design and easy to use with low cost compared with previous Wheelchairs. The eye movement was detected through a camera fixed on the chair. The user’s gaze direction is obtained from the captured image after some processing and analysis. The order is sent to the Arduino Uno board which controls the wheelchair movement. The Wheelchair performance was checked using different volunteers and its accuracy reached 94.4% with a very short response time compared with the other existing chairs. © 2021. All Rights Reserved.","deep learning; Dilip; facial landmarks points; gaze ratio; numpy",,Article,"Final","",Scopus,2-s2.0-85109153735
"Vortmann L.-M., Schwenke L., Putze F.","57211492447;57194583933;22036416700;","Using brain activity patterns to differentiate real and virtual attended targets during augmented reality scenarios",2021,"Information (Switzerland)","12","6","226","","",,1,"10.3390/info12060226","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107880732&doi=10.3390%2finfo12060226&partnerID=40&md5=5d540cb6cef80f8700259993d18bcdaa","Cognitive Systems Lab, Department of Mathematics and Computer Science, University of Bremen, Bremen, 28359, Germany","Vortmann, L.-M., Cognitive Systems Lab, Department of Mathematics and Computer Science, University of Bremen, Bremen, 28359, Germany; Schwenke, L., Cognitive Systems Lab, Department of Mathematics and Computer Science, University of Bremen, Bremen, 28359, Germany; Putze, F., Cognitive Systems Lab, Department of Mathematics and Computer Science, University of Bremen, Bremen, 28359, Germany","Augmented reality is the fusion of virtual components and our real surroundings. The simultaneous visibility of generated and natural objects often requires users to direct their selective attention to a specific target that is either real or virtual. In this study, we investigated whether this target is real or virtual by using machine learning techniques to classify electroencephalographic (EEG) and eye tracking data collected in augmented reality scenarios. A shallow convolutional neural net classified 3 second EEG data windows from 20 participants in a person-dependent manner with an average accuracy above 70% if the testing data and training data came from different trials. This accuracy could be significantly increased to 77% using a multimodal late fusion approach that included the recorded eye tracking data. Person-independent EEG classification was possible above chance level for 6 out of 20 participants. Thus, the reliability of such a brain–computer interface is high enough for it to be treated as a useful input mechanism for augmented reality applications. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Attention; Augmented reality; Classification; EEG; Eye tracking; Neural networks","Augmented reality; Brain; Convolutional neural networks; Electroencephalography; Learning systems; Augmented reality applications; Brain activity patterns; EEG classification; Electroencephalographic (EEG); Machine learning techniques; Person-independent; Selective attention; Virtual components; Eye tracking",Article,"Final","",Scopus,2-s2.0-85107880732
"Pustejovsky J., Krishnaswamy N.","6602448845;57197865998;","Embodied Human Computer Interaction",2021,"KI - Kunstliche Intelligenz",,,,"","",,2,"10.1007/s13218-021-00727-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107495926&doi=10.1007%2fs13218-021-00727-5&partnerID=40&md5=dff6d880e31530edd42b8bdd30abc799","Brandeis University, Waltham, MA, United States; Colorado State University, Fort Collins, CO, United States","Pustejovsky, J., Brandeis University, Waltham, MA, United States; Krishnaswamy, N., Colorado State University, Fort Collins, CO, United States","In this paper, we argue that embodiment can play an important role in the design and modeling of systems developed for Human Computer Interaction. To this end, we describe a simulation platform for building Embodied Human Computer Interactions (EHCI). This system, VoxWorld, enables multimodal dialogue systems that communicate through language, gesture, action, facial expressions, and gaze tracking, in the context of task-oriented interactions. A multimodal simulation is an embodied 3D virtual realization of both the situational environment and the co-situated agents, as well as the most salient content denoted by communicative acts in a discourse. It is built on the modeling language VoxML (Pustejovsky and Krishnaswamy in VoxML: a visualization modeling language, proceedings of LREC, 2016), which encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextually salient inferences and decisions in the environment. VoxWorld enables an embodied HCI by situating both human and artificial agents within the same virtual simulation environment, where they share perceptual and epistemic common ground. We discuss the formal and computational underpinnings of embodiment and common ground, how they interact and specify parameters of the interaction between humans and artificial agents, and demonstrate behaviors and types of interactions on different classes of artificial agents. © 2021, Gesellschaft für Informatik e.V. and Springer-Verlag GmbH Germany, part of Springer Nature.","Artificial agent; Multimodal embodiment; Simulation; Situated grounding",,Article,"Article in Press","",Scopus,2-s2.0-85107495926
"Imaoka H., Hashimoto H., Takahashi K., Ebihara A.F., Liu J., Hayasaka A., Morishita Y., Sakurai K.","7003746327;57214231120;57224105998;57219621812;34976887400;7004539705;36617695400;7402174170;","The future of biometrics technology: From face recognition to related applications",2021,"APSIPA Transactions on Signal and Information Processing",,,,"","",,,"10.1017/ATSIP.2021.8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106991714&doi=10.1017%2fATSIP.2021.8&partnerID=40&md5=04f8a862d8e13f41f5ffb857bc7b9a3c","NEC Corporation, Tokyo, Minato-ku, Japan","Imaoka, H., NEC Corporation, Tokyo, Minato-ku, Japan; Hashimoto, H., NEC Corporation, Tokyo, Minato-ku, Japan; Takahashi, K., NEC Corporation, Tokyo, Minato-ku, Japan; Ebihara, A.F., NEC Corporation, Tokyo, Minato-ku, Japan; Liu, J., NEC Corporation, Tokyo, Minato-ku, Japan; Hayasaka, A., NEC Corporation, Tokyo, Minato-ku, Japan; Morishita, Y., NEC Corporation, Tokyo, Minato-ku, Japan; Sakurai, K., NEC Corporation, Tokyo, Minato-ku, Japan","Biometric recognition technologies have become more important in the modern society due to their convenience with the recent informatization and the dissemination of network services. Among such technologies, face recognition is one of the most convenient and practical because it enables authentication from a distance without requiring any authentication operations manually. As far as we know, face recognition is susceptible to the changes in the appearance of faces due to aging, the surrounding lighting, and posture. There were a number of technical challenges that need to be resolved. Recently, remarkable progress has been made thanks to the advent of deep learning methods. In this position paper, we provide an overview of face recognition technology and introduce its related applications, including face presentation attack detection, gaze estimation, person re-identification and image data mining. We also discuss the research challenges that still need to be addressed and resolved. Copyright © The Author(s), 2021 published by Cambridge University Press in association with Asia Pacific Signal and Information Processing Association.","Biometrics; Deep Learning; Face Recognition","Authentication; Biometrics; Data mining; Deep learning; Learning systems; Pattern recognition systems; Biometric recognition technology; Biometrics technology; Face recognition technologies; Image data minings; Learning methods; Person re identifications; Research challenges; Technical challenges; Face recognition",Article,"Article in Press","",Scopus,2-s2.0-85106991714
"Sluka T., Kvasov A., Kubes T., Masson J., Fotinos A., Smolik G., Suruceanu G., Ergunay S., Michoud A., Hirt G., Kabengera P., Comminot J.","24554186900;56531775300;57219700519;57225705248;57224086817;57225247496;6701718546;55364961300;57224095267;57224081992;57224086767;57224078408;","Light-field brings Augmented Reality to the personal space",2021,"Proceedings of SPIE - The International Society for Optical Engineering","11765",,"117650S","","",,,"10.1117/12.2584091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106926575&doi=10.1117%2f12.2584091&partnerID=40&md5=d6b3c492f3df4d8fc7c24e1ccbed2788","Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland","Sluka, T., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Kvasov, A., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Kubes, T., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Masson, J., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Fotinos, A., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Smolik, G., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Suruceanu, G., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Ergunay, S., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Michoud, A., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Hirt, G., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Kabengera, P., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland; Comminot, J., Creal, Chemin de la Dent d'Oche 1A, Ecublens, 1024, Switzerland","The state-of-The-Art Virtual and Augmented Reality (VR/AR) hardware fails to deliver satisfying visual experience due to missing or conflicting focus cues. The absence of natural focal depth in digital 3D imagery causes the so-called vergence-Accommodation conflict, focal rivalry, and possibly damage the eye-sight, especially during prolonged viewing of virtual objects within the arm's reach. It remains one of the most challenging and market-blocking problems in the VR/AR arena today. This talk will introduce CREAL's unique near-To-eye light-field projection system that provides high-resolution 3D imagery with fully natural focus cues. The system operates without eye-Tracking or severe penalty on image quality, rendering load, power consumption, data bandwidth, form-factor, production cost, or complexity. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Augmented Reality; focal rivalry; light-field; near-To-eye display; ocular parallax; vergence-Accommodation conict","Augmented reality; Eye tracking; Data bandwidth; High resolution; Personal spaces; Production cost; State of the art; Virtual and augmented reality; Virtual objects; Visual experiences; Mixed reality",Conference Paper,"Final","",Scopus,2-s2.0-85106926575
"Nagamatsu T., Hiroe M., Arai H.","23398000100;57202892342;57224004474;","Extending the measurement angle of a gaze estimation method using an eye model expressed by a revolution about the optical axis of the eye",2021,"IEICE Transactions on Information and Systems","E104.D","5",,"729","738",,,"10.1587/transinf.2020EDP7072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106705057&doi=10.1587%2ftransinf.2020EDP7072&partnerID=40&md5=6a7d22411b47d3945f834cff92dbe781","Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658–0022, Japan","Nagamatsu, T., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658–0022, Japan; Hiroe, M., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658–0022, Japan; Arai, H., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658–0022, Japan","SUMMARY An eye model expressed by a revolution about the optical axis of the eye is one of the most accurate models for use in a 3D gaze estimation method. The measurement range of the previous gaze estimation method that uses two cameras based on the eye model is limited by the larger of the two angles between the gaze and the optical axes of two cameras. The previous method cannot calculate the gaze when exceeding a certain limit of the rotation angle of the eye. In this paper, we show the characteristics of reflections on the surface of the eye from two light sources, when the eye rotates. Then, we propose a method that extends the rotation angle of the eye for a 3D gaze estimation based on this model. The proposed method uses reflections that were not used in the previous method. We developed an experimental gaze tracking system for a wide projector screen and experimentally validated the proposed method with 20 participants. The result shows that the proposed method can measure the gaze of more number of people with increased accuracy compared with the previous method. Copyright © 2021 The Institute of Electronics, Information and Communication Engineers","Aspherical eye model; Eye tracking; Gaze tracking","Cameras; Light sources; Eye model; Gaze estimation; Gaze tracking system; Measurement range; Number of peoples; Optical axes; Optical axis; Rotation angles; Eye tracking",Article,"Final","",Scopus,2-s2.0-85106705057
"Prasetyo Y.T., Widyaningrum R.","57204827000;57222252302;","Error Rate as Mediators of the Relationships Among 2D/3D TV Environment, Eye Gaze Accuracy, and Symptoms",2021,"Lecture Notes in Networks and Systems","220",,,"756","761",,,"10.1007/978-3-030-74605-6_96","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106424061&doi=10.1007%2f978-3-030-74605-6_96&partnerID=40&md5=79d2cdd7181c610c1f52ff4feb6f55ea","School of Industrial Engineering and Engineering Management, Mapúa University, Manila, Philippines; Department of Industrial Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","Prasetyo, Y.T., School of Industrial Engineering and Engineering Management, Mapúa University, Manila, Philippines; Widyaningrum, R., Department of Industrial Engineering, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","3D TV is a new platform to enjoy the stereoscopic environment in the home and eye tracking technology has been extensively utilized to evaluate the 3D TV. This study was mainly intended to explore an additional eye movement parameter that can predict the eye gaze accuracy and symptoms while perceiving the image in the 3D TV. A total of 12 graduate students were asked to perform tapping task in the 2D and 3D TV using within-subject design under 6 different levels of index of difficulty (ID). Structural equation modeling (SEM) was applied to analyze the causal relationship between 2D/3D environment, a new eye movement parameter, eye gaze accuracy, and symptoms. The result showed that error rate was found as a significant mediator of the relationships. In addition, the SEM approach was also found as a new significant and reliable approach in the visual ergonomics particularly for bridging the objective and subjective measures. Finally, the new eye movement parameter can be an important key for predicting eye gaze accuracy in the stereoscopic display. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Error rate; Eye movement parameter; Eye tracking; Structural equation modeling; Visual ergonomics",,Conference Paper,"Final","",Scopus,2-s2.0-85106424061
"Richard A., Lea C., Ma S., Gall J., La Torre F.D., Sheikh Y.","35756429900;57219786689;26424307700;23396675200;7007150700;9437184000;","Audio- And gaze-driven facial animation of codec avatars",2021,"Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021",,,,"41","50",,1,"10.1109/WACV48630.2021.00009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106203436&doi=10.1109%2fWACV48630.2021.00009&partnerID=40&md5=640dc91e49d71d2648810ef6c91ce955","Facebook Reality Labs; University of Bonn, Germany","Richard, A., Facebook Reality Labs; Lea, C., Facebook Reality Labs; Ma, S., Facebook Reality Labs; Gall, J., University of Bonn, Germany; La Torre, F.D., Facebook Reality Labs; Sheikh, Y., Facebook Reality Labs","Codec Avatars are a recent class of learned, photorealistic face models that accurately represent the geometry and texture of a person in 3D (i.e., for virtual reality), and are almost indistinguishable from video [28]. In this paper we describe the first approach to animate these parametric models in real-time which could be deployed on commodity virtual reality hardware using audio and/or eye tracking. Our goal is to display expressive conversations between individuals that exhibit important social signals such as laughter and excitement solely from la-tent cues in our lossy input signals. To this end we collected over 5 hours of high frame rate 3D face scans across three participants including traditional neutral speech as well as expressive and conversational speech. We investigate a multimodal fusion approach that dynamically identifies which sensor encoding should animate which parts of the face at any time. See the supplemental video which demonstrates our ability to generate full face motion far beyond the typically neutral lip articulations seen in competing work: https://research.fb.com/videos/audio-and-gaze-driven-facial-animation-of-codec-avatars/ © 2021 IEEE.",,"Animation; Eye tracking; Textures; Three dimensional computer graphics; 3D faces; Expressive-speech; Eye-tracking; Face models; Facial animation; High frame rate; Parametric models; Photo-realistic; Real- time; Social signals; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85106203436
"Lee J.-H., Yanusik I., Choi Y., Kang B., Hwang C., Malinovskaya E., Park J., Nam D., Lee C., Kim C., Min T., Hong S.","55675810300;57192644105;7404776991;36620258400;57219332810;56081393200;50961546500;43761314400;57211093908;57223622926;57223592721;57219331120;","Optical design of automotive augmented reality 3D head-up display with light-field rendering",2021,"Proceedings of SPIE - The International Society for Optical Engineering","11708",,"117080I","","",,,"10.1117/12.2576660","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105964355&doi=10.1117%2f12.2576660&partnerID=40&md5=9f05642b2743e8e1655d0a01fac79ed6","Multimedia Processing Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea; Samsung RandD Institute Russia, Office 1500, 12 bldg.1, Dvintsev str., Moscow, 127018, Russian Federation; Computer Vision Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea","Lee, J.-H., Multimedia Processing Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea; Yanusik, I., Samsung RandD Institute Russia, Office 1500, 12 bldg.1, Dvintsev str., Moscow, 127018, Russian Federation; Choi, Y., Multimedia Processing Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea; Kang, B., Multimedia Processing Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea; Hwang, C., Multimedia Processing Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea; Malinovskaya, E., Samsung RandD Institute Russia, Office 1500, 12 bldg.1, Dvintsev str., Moscow, 127018, Russian Federation; Park, J., Computer Vision Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea; Nam, D., Computer Vision Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea; Lee, C., Multimedia Processing Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea; Kim, C., Multimedia Processing Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea; Min, T., Multimedia Processing Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea; Hong, S., Multimedia Processing Lab, Samsung Advanced Institute of Technology, 130, Samsung-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 16678, South Korea","Although head-up displays (HUDs) have already been installed in some commercial vehicles, their application to augmented reality (AR) is limited owing to the resulting narrow field of view (FoV) and fixed virtual-image distance. The matching of depth between AR information and real objects across wide FoVs is a key feature of AR HUDs to provide a safe driving experience. Meanwhile, current approaches based on the integration of two-plane virtual images and computer-generated holography suffer from problems such as partial depth control and high computational complexity, respectively, which makes them unsuitable for application in fast-moving vehicles. To bridge this gap, here, we propose a light-field-based 3D display technology with eye-tracking. We begin by matching the HUD optics with the light-field display view formation. First, we design mirrors to deliver high-quality virtual images with an FoV of 10 × 5° for a total eyebox size of 140 × 120 mm and compensate for the curved windshield shape. Next, we define the procedure to translate the driver eye position, obtained via eye-tracking, to the plane of the light-field display views. We further implement a lenticular-lens design and the corresponding sub-pixel-allocation-based rendering, for which we construct a simplified model to substitute for the freeform mirror optics. Finally, we present a prototyped device that affords the desired image quality, 3D image depth up to 100 m, and crosstalk level of <1.5%. Our findings indicate that such 3D HUDs can form the mainstream technology for AR HUDs. © 2021 SPIE.","3D head-up display; augmented reality; eye-tracking; light-field rendering","Aircraft windshields; Augmented reality; Commercial vehicles; Eye tracking; Field emission displays; Head-up displays; Light; Mirrors; Optical design; Rendering (computer graphics); Three dimensional computer graphics; 3D display technologies; Cross-talk levels; Desired image qualities; Field of views; Lenticular lens; Light field displays; Light field rendering; Moving vehicles; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85105964355
"Arjun S., Saluja K.P., Biswas P.","57193513088;57204033632;14007579800;","Analyzing Ocular Parameters for Web Browsing and Graph Visualization",2021,"Smart Innovation, Systems and Technologies","221",,,"327","337",,,"10.1007/978-981-16-0041-8_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105955941&doi=10.1007%2f978-981-16-0041-8_28&partnerID=40&md5=dc28f9c7d7f878b1dcdb7c8bd6cc341f","Centre for Product Design and Manufacturing, Indian Institute of Science, Bengaluru, India","Arjun, S., Centre for Product Design and Manufacturing, Indian Institute of Science, Bengaluru, India; Saluja, K.P., Centre for Product Design and Manufacturing, Indian Institute of Science, Bengaluru, India; Biswas, P., Centre for Product Design and Manufacturing, Indian Institute of Science, Bengaluru, India","This paper proposes a set of techniques to investigate eye gaze and fixation patterns while users interact with electronic user interfaces. In particular, two case studies are presented—one on analyzing eye gaze while interacting with deceptive materials in web pages and another on analyzing graphs in standard computer monitor and virtual reality displays. We analyzed spatial and temporal distributions of eye gaze fixations and sequence of eye gaze movements. We used this information to propose new design guidelines to avoid deceptive materials in web and user-friendly representation of data in 2D graphs. In 2D graph study, we identified that area graph has the lowest number of clusters for user's gaze fixations and lowest average response time. The results of 2D graph study were implemented in virtual and mixed reality environment. Along with this, it was observed that the duration while interacting with deceptive materials in web pages is independent of the number of fixations. Furthermore, web-based data visualization tool for analyzing eye tracking data from single and multiple users was developed. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Eye tracking; Graph visualization; User interfaces; Virtual reality","Data visualization; Display devices; Eye movements; Mixed reality; User interfaces; Visualization; Websites; Graph visualization; Mixed-reality environment; Number of clusters; Number of fixations; Spatial and temporal distribution; User friendly; Virtual-reality display; Web-based data visualizations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85105955941
"Richter M., Blankenbach K., Reichel S.","57223608125;25623015000;6701426415;","New approaches for multi-view displays by circular display",2021,"Proceedings of SPIE - The International Society for Optical Engineering","11708",,"117080Q","","",,,"10.1117/12.2576972","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105942832&doi=10.1117%2f12.2576972&partnerID=40&md5=d8ec3255019034fac5d871919670a72f","Pforzheim University, Display Lab, Tiefenbronner Str. 65, Pforzheim, 75175, Germany","Richter, M., Pforzheim University, Display Lab, Tiefenbronner Str. 65, Pforzheim, 75175, Germany; Blankenbach, K., Pforzheim University, Display Lab, Tiefenbronner Str. 65, Pforzheim, 75175, Germany; Reichel, S., Pforzheim University, Display Lab, Tiefenbronner Str. 65, Pforzheim, 75175, Germany","Multi-view displays reproduce more than one view of an object. Classical 3D displays allow only a single user. Today's multi-view displays are flat and reproduce two or more images. They base on e.g. parallax barrier, lenticular lenses, light field displays and projection with reduced effective resolution. Another method is projection on rotating mirrors. This approach requires highest frame rates (∼5,000 Hz), so only prototypes without color and grey are realized so far. Our approach base on a rotating (prototype 60 rps) periscope-like mirror system (with magnification) in the center of a 360° circular display. For prototyping, we used large high-resolution flat displays. One simple method is to use a fixed position on a single display for every corresponding view. To avoid motion blur and ghosting one mirror is equipped with a vertical slit screen to block light from neighboring areas. We implemented eye tracking for efficient rendering in real time and reproduce only relevant views according the corresponding angular position of the viewer's eyes. So a standard high resolution display can be used to generate thousands of different perspectives (at full color and 60 Hz). Possible applications of our multi-view display are collaborative work for e.g. several designers, which can see the object from very different locations and interactively improve design, (science) museums and entertainment. © 2021 SPIE.","3D; eye tracking; motion blur; multi-view display; rotating mirror","Eye tracking; Geometrical optics; Mirrors; Three dimensional displays; Angular positions; Collaborative Work; Effective resolutions; High resolution display; Light field displays; Multi-view displays; Parallax barriers; Rotating mirrors; Field emission displays",Conference Paper,"Final","",Scopus,2-s2.0-85105942832
"Hu T., Jha S., Busso C.","57221705332;57193014012;35742852700;","Temporal Head Pose Estimation From Point Cloud in Naturalistic Driving Conditions",2021,"IEEE Transactions on Intelligent Transportation Systems",,,,"","",,1,"10.1109/TITS.2021.3075350","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105844898&doi=10.1109%2fTITS.2021.3075350&partnerID=40&md5=a1fc0876bcf359bbb6fa098ab8746d25","Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA.; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA (e-mail: busso@utdallas.edu).","Hu, T., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA.; Jha, S., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA.; Busso, C., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA (e-mail: busso@utdallas.edu).","Head pose estimation is an important problem as it facilitates tasks such as gaze estimation and attention modeling. In the automotive context, head pose provides crucial information about the driver's mental state, including drowsiness, distraction and attention. It can also be used for interaction with in-vehicle infotainment systems. While computer vision algorithms using RGB cameras are reliable in controlled environments, head pose estimation is a challenging problem in the car due to sudden illumination changes, occlusions and large head rotations that are common in a vehicle. These issues can be partially alleviated by using depth cameras. Head rotation trajectories are continuous with important temporal dependencies. Our study leverages this observation, proposing a novel temporal deep learning model for head pose estimation from point cloud. The approach extracts discriminative feature representation directly from point cloud data, leveraging the 3D spatial structure of the face. The frame-based representations are then combined with bidirectional long short term memory (BLSTM) layers. We train this model on the newly collected multimodal driver monitoring (MDM) dataset, achieving better results compared to non-temporal algorithms using point cloud data, and state-of-the-art models using RGB images. We further show quantitatively and qualitatively that incorporating temporal information provides large improvements not only in accuracy, but also in the smoothness of the predictions. CCBYNCND","Cameras; deep learning; Deep learning; Driver head pose estimation; Feature extraction; Magnetic heads; point cloud; Pose estimation; temporal modeling.; Three-dimensional displays; Vehicles","Cameras; Driver training; 3D spatial structure; Computer vision algorithms; Controlled environment; Discriminative features; Head Pose Estimation; Illumination changes; Temporal information; Vehicle infotainment; Deep learning",Article,"Article in Press","",Scopus,2-s2.0-85105844898
"Yao S., Han X., Zhang H., Wang X., Cao X.","57219796710;55451013500;57188759418;55979164000;8920951000;","Learning Deep Lucas-Kanade Siamese Network for Visual Tracking",2021,"IEEE Transactions on Image Processing","30",,"9423558","4814","4827",,,"10.1109/TIP.2021.3076272","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105659480&doi=10.1109%2fTIP.2021.3076272&partnerID=40&md5=98369ef18151dbdbb7b64b26c6884ac7","Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, 100093, China; Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen, 518172, China","Yao, S., Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, 100093, China; Han, X., Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen, 518172, China; Zhang, H., Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, 100093, China; Wang, X., Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, 100093, China; Cao, X., Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, 100093, China","In most recent years, Siamese trackers have drawn great attention because of their well-balanced accuracy and efficiency. Although these approaches have achieved great success, the discriminative power of the conventional Siamese trackers is still limited by the insufficient template-candidate representation. Most of the existing approaches take non-aligned features to learn a similarity function for template-candidate matching, while the target object's geometrical transformation is seldom explored. To address this problem, we propose a novel Siamese tracking framework, which enables to dynamically transform the template-candidate features to a more discriminative viewpoint for similarity matching. Specifically, we reformulate the template-candidate matching problem of the conventional Siamese tracker from the perspective of Lucas-Kanade (LK) image alignment approach. A Lucas-Kanade network (LKNet) is proposed and incorporated to the Siamese architecture to learn aligned feature representations in data-driven trainable manner, which is able to enhance the model adaptability in challenging scenarios. Within this framework, we propose two Siamese trackers named LK-Siam and LK-SiamRPN to validate the effectiveness. Extensive experiments conducted on the prevalent datasets show that the proposed method is more competitive over a number of state-of-the-art methods. © 1992-2012 IEEE.","dynamic template-candidate matching; Lucas-Kanade algorithm; object tracking; Siamese network","Deep learning; Mathematical transformations; Discriminative power; Feature representation; Geometrical transformation; Matching problems; Number of state; Similarity functions; Similarity-matching; Visual Tracking; Template matching; article; eye tracking; image alignment; learning",Article,"Final","",Scopus,2-s2.0-85105659480
"Norouzifard M., Nemati A., Mollaee S., GholamHosseini H., Black J., Thompson B., Turuwhenua J., on behalf of the hPOD Study Team","24824760100;57205121255;57219335084;55903330400;55421025300;57217451109;6506988000;","A Comparison of Approaches for Synchronizing Events in Video Streams Using Audio",2021,"Communications in Computer and Information Science","1386 CCIS",,,"262","272",,,"10.1007/978-3-030-72073-5_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104843847&doi=10.1007%2f978-3-030-72073-5_20&partnerID=40&md5=f9967475526eb14644352965dd3d8a82","Auckland Bioengineering Institute, University of Auckland, Auckland, New Zealand; Institute for Health & Equity, Medical College of Wisconsin, Milwaukee, United States; School of Engineering, Computer, and Mathematical Sciences, Auckland University of Technology (AUT), Auckland, New Zealand; School of Optometry and Vision Science, University of Auckland, Auckland, New Zealand; School of Optometry and Vision Science, University of Waterloo, Waterloo, Canada; Centre for Eye and Vision Research, Hong Kong","Norouzifard, M., Auckland Bioengineering Institute, University of Auckland, Auckland, New Zealand; Nemati, A., Institute for Health & Equity, Medical College of Wisconsin, Milwaukee, United States; Mollaee, S., Auckland Bioengineering Institute, University of Auckland, Auckland, New Zealand; GholamHosseini, H., School of Engineering, Computer, and Mathematical Sciences, Auckland University of Technology (AUT), Auckland, New Zealand; Black, J., School of Optometry and Vision Science, University of Auckland, Auckland, New Zealand; Thompson, B., School of Optometry and Vision Science, University of Auckland, Auckland, New Zealand, School of Optometry and Vision Science, University of Waterloo, Waterloo, Canada, Centre for Eye and Vision Research, Hong Kong; Turuwhenua, J., Auckland Bioengineering Institute, University of Auckland, Auckland, New Zealand, School of Optometry and Vision Science, University of Auckland, Auckland, New Zealand; on behalf of the hPOD Study Team","A common scenario found in experimentation is to synchronize events, such as breaks between visual stimulus, with the video record taken of an experiment made of participants as they undertake the task. In our case, we recently synchronized a protocol of stimulus presentations shown on a laptop display, with webcam video made of participants’ (who were two year old children) facial and eye movements as they were shown trials of stimulus containing moving dots (a random dot kinematogram or RDK). The purpose was to assess eye movements in response to these RDK stimulus as a part of a potential neurological assessment for children. The video contained audio signals such as “beeps” and musical interludes that indicated the start and end of trials, thereby providing a convenient opportunity to align these audio events with the timing of known events in the video record. The process of alignment can be performed manually, but this is a tedious and time consuming task when considering, for example, large databases of videos. In this paper, we tested two alternate methods for synchronizing known audio events using: 1) a deep learning based model, and a 2) standard template matching algorithm. These methods were used to synchronize the known protocol of stimulus events in videos by processing the audio contents of the recording. The deep learning approach utilized simple mel-spectrum audio signal feature extraction, whilst we adopted a cross-correlation algorithm that detected an audio template in the time domain. We found that whilst correlation was not effective as a means of beep detection; but our machine learning-based technique was robust with 90% accuracy in the testing dataset and did not the same amount of remediation required of the correlation approach. © 2021, Springer Nature Switzerland AG.","Audio signal processing; Deep learning; Eye tracking; Template matching; Video synchronization","Correlation detectors; Deep learning; Eye movements; Feature extraction; Learning algorithms; Learning systems; Statistical tests; Synchronization; Template matching; Time domain analysis; Video streaming; Alternate method; Cross-correlation algorithm; Facial and eye movement; Learning approach; Learning Based Models; Template-matching algorithms; Time-consuming tasks; Visual stimulus; Audio acoustics",Conference Paper,"Final","",Scopus,2-s2.0-85104843847
"Wells C., Schnabel M.A., Moleta T., Brown A.","57223100818;44861619400;56039524400;55147021900;","Beauty is in the eye of the beholder: Improving the human-computer interface within vrad by the active and two-way employment of our visual senses",2021,"Projections - Proceedings of the 26th International Conference of the Association for Computer-Aided Architectural Design Research in Asia, CAADRIA 2021","2",,,"355","364",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104840533&partnerID=40&md5=5eabb46b68f99cb4272a304fc57a9184","Victoria University of Wellington, New Zealand","Wells, C., Victoria University of Wellington, New Zealand; Schnabel, M.A., Victoria University of Wellington, New Zealand; Moleta, T., Victoria University of Wellington, New Zealand; Brown, A., Victoria University of Wellington, New Zealand","Whether it is via traditional methods with pen and paper or contemporary techniques such as 3D digital modelling and VR drawing, the eye typically plays a mostly passive or consuming role within the design process. By incorporating eye-tracking deeper within these methods, we can begin to discern this technology's possibilities as a method that encompasses the visual experience as an active input. Our research, however, developed the Eye-Tracking Voxel Environment Sculptor (EVES) that incorporates eye-tracking as there design actor. Through EVES we can extend eye-tracking as an active design medium. The eye-tracking data garnered from the designer within EVES is directly utilised as an input within a modelling environment to manipulate and sculpt voxels. In addition to modelling input, eye-tracking is also explored in its usability in the Virtual Reality User Interface. Eye-tracking is implemented within EVES to this extent to test the limits and possibilities of eye-tracking and the Human-Computer Interface within the realm of Virtual Reality Aided Design. © 2021 and published by the Association for Computer-Aided Architectural Design Research in Asia (CAADRIA), Hong Kong.","Eye-Tracking; Human-Computer interface (HCI); Modelling; Sketching; Virtual reality","3D modeling; Architectural design; User interfaces; Virtual reality; Aided designs; Contemporary techniques; Design process; Human computer interfaces; Modelling environment; Two ways; Visual experiences; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85104840533
"Ghadekar P., Korpal P., Chendake P., Bansal R., Pawar A., Bhor S.","56637017300;57223082205;57223089268;57223095882;57171929600;57223083671;","Real-Time Hands-Free Mouse Control for Disabled",2021,"Advances in Intelligent Systems and Computing","1311 AISC",,,"161","170",,,"10.1007/978-981-33-4859-2_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104748095&doi=10.1007%2f978-981-33-4859-2_16&partnerID=40&md5=e471d089f901a3531f7aa4abb06d3c5a","Department of Information Technology, Vishwakarma Institute of Technology, Pune, India","Ghadekar, P., Department of Information Technology, Vishwakarma Institute of Technology, Pune, India; Korpal, P., Department of Information Technology, Vishwakarma Institute of Technology, Pune, India; Chendake, P., Department of Information Technology, Vishwakarma Institute of Technology, Pune, India; Bansal, R., Department of Information Technology, Vishwakarma Institute of Technology, Pune, India; Pawar, A., Department of Information Technology, Vishwakarma Institute of Technology, Pune, India; Bhor, S., Department of Information Technology, Vishwakarma Institute of Technology, Pune, India","In this paper, a human–computer interface system using eye motion is implemented. In traditional methods, human–computer interfaces use keyboard, mouse as input devices. A hand-free interface between computer and human is represented in this paper. The system is developed using template matching and is a real time, fast and affordable technique for tracking facial features and eye gestures. The traditional computer screen pointing devices can be replaced by this technology, for the use of disabled people. The paper presents computer mouse cursor movement with human eyes. Wherever the eyesight focuses, accordingly the mouse is controlled. The proposed vision-based virtual interface controls the system by various eye movements such as eye blinking, winking of eye. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Eye tracking; Eye-blinking detection; Mouse movement","Eye movements; Machine learning; Template matching; Computer mouse; Disabled people; Facial feature; Free interface; Input devices; Pointing devices; Traditional computers; Virtual interfaces; Mammals",Conference Paper,"Final","",Scopus,2-s2.0-85104748095
"Mandal R., Becken S., Connolly R.M., Stantic B.","54410932900;55917272700;7101819604;23394040900;","Residual Attention Network vs Real Attention on Aesthetic Assessment",2021,"Communications in Computer and Information Science","1371 CCIS",,,"310","320",,,"10.1007/978-981-16-1685-3_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104734560&doi=10.1007%2f978-981-16-1685-3_26&partnerID=40&md5=42f84624af9e407a2037065cec0027eb","School of Information and Communication Technology, Griffith University, Brisbane, Australia; Australian Rivers Institute, Griffith University, Brisbane, Australia; Griffith Institute for Tourism, Brisbane, Australia","Mandal, R., School of Information and Communication Technology, Griffith University, Brisbane, Australia; Becken, S., Griffith Institute for Tourism, Brisbane, Australia; Connolly, R.M., Australian Rivers Institute, Griffith University, Brisbane, Australia; Stantic, B., School of Information and Communication Technology, Griffith University, Brisbane, Australia","Photo aesthetics assessment is a challenging problem. Deep Convolutional Neural Network (CNN)-based algorithms have achieved promising results for aesthetics assessment in recent times. Lately, few efficient and effective attention-based CNN architectures are proposed that improve learning efficiency by adaptively adjusts the weight of each patch during the training process. In this paper, we investigate how real human attention affects instead of CNN-based synthetic attention network architecture in image aesthetic assessment. A dataset consists of a large number of images along with eye-tracking information has been developed using an eye-tracking device (https://www.tobii.com/group/about/this-is-eye-tracking/ ) power by sensor technology for our research, and it will be the first study of its kind in image aesthetic assessment. We adopted a Residual Attention Network and ResNet architectures which achieve state-of-the-art performance image recognition tasks on benchmark datasets. We report our findings on photo aesthetics assessment with two sets of datasets consist of original images and images with masked attention patches, which demonstrates higher accuracy when compared to the state-of-the-art methods. © 2021, Springer Nature Singapore Pte Ltd.","Aesthetic scoring; Deep learning; Great Barrier Reef; Image aesthetic evaluation; Photo aesthetic assessment","Benchmarking; Convolutional neural networks; Database systems; Deep neural networks; Eye tracking; Image recognition; Large dataset; Benchmark datasets; Eye tracking devices; Image Aesthetics; Learning efficiency; Sensor technologies; State-of-the-art methods; State-of-the-art performance; Training process; Network architecture",Conference Paper,"Final","",Scopus,2-s2.0-85104734560
"Milanova M., Aldaeif F.","7003785945;57223096486;","Markerless 3D Virtual Glasses Try-On System",2021,"Smart Innovation, Systems and Technologies","216",,,"99","111",,,"10.1007/978-981-33-4676-5_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104693626&doi=10.1007%2f978-981-33-4676-5_7&partnerID=40&md5=24cd1b1990170b2cbef7c1dab1a92c54","University of Arkansas at Little Rock, Little Rock, AR, United States","Milanova, M., University of Arkansas at Little Rock, Little Rock, AR, United States; Aldaeif, F., University of Arkansas at Little Rock, Little Rock, AR, United States","This paper presents the implementation of a markerless mobile augmented reality application called a virtual eye glasses try-on system. The system first detects and tracks human face and eyes. Then, the system overlays the 3D virtual glasses over the face in real time. This system helps the consumer to select any style of glasses available on the virtual space saving both time and effort when shopping online. A method based on local-invariant descriptors is implemented to extract image feature points for eyes detection and tracking. A new approach for camera pose estimation is proposed to augment real images with virtual graphics. Experiments are conducted using Haar cascade and speeded up robust features (SURF) cascade. The system is optimized and adapted for a mobile architecture. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Augmented reality; Eye tracking; Face tracking; Haar cascade; Jones; Pose estimation; SURF cascade; Viola-","Augmented reality; Glass; Signal processing; Camera pose estimation; Image feature points; Invariant descriptors; Mobile architecture; Mobile augmented reality; Speeded up robust features; System overlays; Virtual spaces; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85104693626
"Zhuang Y., Zhang Y., Zhao H.","57223040112;57209691865;16320268200;","Appearance-based gaze estimation using separable convolution neural networks",2021,"IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)",,,"9390807","609","612",,,"10.1109/IAEAC50856.2021.9390807","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104620141&doi=10.1109%2fIAEAC50856.2021.9390807&partnerID=40&md5=2f2da48739995189086935afa77c0b95","North China University of Technology, Information Institute, Beijing, China","Zhuang, Y., North China University of Technology, Information Institute, Beijing, China; Zhang, Y., North China University of Technology, Information Institute, Beijing, China; Zhao, H., North China University of Technology, Information Institute, Beijing, China","Gaze estimation is one of the current important research contents of computer vision. For the current situation where the gaze estimation neural network has a large amount of parameters but the accuracy is not greatly improved and the head pose is difficult to handle, this paper proposes a simplified gaze estimation network model SLeNet based on the LeNet neural network. The deep separable convolution in the Xception network is used to reduce the amount of parameters in the convolution part and improve the computational performance of the network model. The method of splicing head posture features is retained, but another branch neural network is designed to learn head posture based on eye image and mouth corner information, and no additional module is required to obtain head posture separately. The improved network model is used to compare experiments with the original network and VGG-16 on the MPIIGaze dataset. The results show that the improved SLeNet network model performs better on the MPIIGaze dataset than LeNet and VGG-16 and has fewer parameters. © 2021 IEEE.","depth separable convolution; gaze estimation; head pose; neural network","Convolution; Appearance based; Computational performance; Convolution neural network; Current situation; Gaze estimation; Head posture; Large amounts; Network modeling; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85104620141
"Lin W., Kotakehara Y., Hirota Y., Murakami M., Kakusho K., Yueh H.-P.","35211059800;57213069205;57223049196;42561762300;6602076578;13808330900;","Modeling Reading Behaviors: An Automatic Approach to Eye Movement Analytics",2021,"IEEE Access","9",,"9410616","63580","63590",,,"10.1109/ACCESS.2021.3074913","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104604253&doi=10.1109%2fACCESS.2021.3074913&partnerID=40&md5=0ea7a5a42edf1cd26706642902de4b6e","Department of Library and Information Science, National Taiwan University, Taipei, 10617, Taiwan; Department of Human System Interaction, Kwansei Gakuin University, Nishinomiya, 662-8501, Japan; Department of Teaching and Learning Support, Osaka University, Osaka, 565-0871, Japan; Department of Psychology, National Taiwan University, Taipei, 10617, Taiwan; Department of Bio-Industry Communication and Development, National Taiwan University, Taipei, 10617, Taiwan","Lin, W., Department of Library and Information Science, National Taiwan University, Taipei, 10617, Taiwan; Kotakehara, Y., Department of Human System Interaction, Kwansei Gakuin University, Nishinomiya, 662-8501, Japan; Hirota, Y., Department of Human System Interaction, Kwansei Gakuin University, Nishinomiya, 662-8501, Japan; Murakami, M., Department of Teaching and Learning Support, Osaka University, Osaka, 565-0871, Japan; Kakusho, K., Department of Human System Interaction, Kwansei Gakuin University, Nishinomiya, 662-8501, Japan; Yueh, H.-P., Department of Psychology, National Taiwan University, Taipei, 10617, Taiwan, Department of Bio-Industry Communication and Development, National Taiwan University, Taipei, 10617, Taiwan","Critical reading plays an important role in science learning, and previous studies have endeavored to objectively and precisely capture readers' cognitive processing in reading scientific texts. Since many factors affect readers' initiation and comprehension of scientific texts, studying the interactions of these factors was technically challenging for earlier studies. Recently, the use of artificial intelligent techniques for analyzing physiological signals has gained significant research attention, but exploitation of the educational data for proactive instructional use is still limited. This study proposed and evaluated an automatic approach incorporating the K-means++ clustering method for eye movement analytics. In this study, 64 undergraduate and graduate students read a multi-page popular science text while their eye movements were recorded. The results of the cluster analysis identified three patterns of reading behavior that were consistent and comparable to those of previous studies using self-reported measures and post-analysis analytics. Findings of the study support the potential and validity of a bottom-up, data-driven approach that can directly examine and analyze reading behaviors without interruption, and the contribution of the study to research and practice is outlined. © 2013 IEEE.","Computer aided analysis; gaze tracking; machine learning; scientific reading","Cluster analysis; K-means clustering; Motion analysis; Students; Artificial intelligent techniques; Automatic approaches; Cognitive processing; Data-driven approach; Graduate students; Physiological signals; Science learning; Scientific texts; Eye movements",Article,"Final","",Scopus,2-s2.0-85104604253
"Dondi P., Lombardi L., Malagodi M., Licchelli M.","35408915200;57104711900;35362367600;7003790948;","Stylistic Classification of Historical Violins: A Deep Learning Approach",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12667 LNCS",,,"112","125",,,"10.1007/978-3-030-68787-8_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104461045&doi=10.1007%2f978-3-030-68787-8_8&partnerID=40&md5=2a2fd9a2927dbb6578aac7232f4dece5","Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Via Ferrata 5, Pavia, 27100, Italy; CISRiC - Arvedi Laboratory of Non-invasive Diagnostics, University of Pavia, Via Bell’Aspa 3, Cremona, 26100, Italy; Department of Musicology and Cultural Heritage, University of Pavia, Corso Garibaldi 178, Cremona, 26100, Italy; Department of Chemistry, University of Pavia, via Taramelli 12, Pavia, 27100, Italy","Dondi, P., Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Via Ferrata 5, Pavia, 27100, Italy, CISRiC - Arvedi Laboratory of Non-invasive Diagnostics, University of Pavia, Via Bell’Aspa 3, Cremona, 26100, Italy; Lombardi, L., Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Via Ferrata 5, Pavia, 27100, Italy; Malagodi, M., CISRiC - Arvedi Laboratory of Non-invasive Diagnostics, University of Pavia, Via Bell’Aspa 3, Cremona, 26100, Italy, Department of Musicology and Cultural Heritage, University of Pavia, Corso Garibaldi 178, Cremona, 26100, Italy; Licchelli, M., CISRiC - Arvedi Laboratory of Non-invasive Diagnostics, University of Pavia, Via Bell’Aspa 3, Cremona, 26100, Italy, Department of Chemistry, University of Pavia, via Taramelli 12, Pavia, 27100, Italy","Stylistic study of artworks is a well-known problem in the Cultural Heritage field. Traditional artworks, such as statues and paintings, have been extensively studied by art experts, producing standard methodologies to analyze and recognize the style of an artist. In this context, the case of historical violins is peculiar. Even if the main stylistic features of a violin are known, only few experts are capable to attribute a violin to its maker with a high degree of certainty. This paper presents a study about the use of deep learning to discriminate a violin style. Firstly, we collected images of 17th–18th century violins held, or in temporary loan, at “Museo del Violino” of Cremona (Italy) to be used as reference dataset. Then, we tested the performances of three state-of-the-art CNNs (VGG16, ResNet50 and InceptionV3) on a binary classification (Stradivari vs. NotStradivari). The best performing model was able to achieve 77.27% accuracy and 0.72 F1 score. A promising result, keeping in mind the limited amount of data and the complexity of the task, even for human experts. Finally, we compared the regions of interest identified by the network with the regions of interest identified in a previous eye tracking study conducted on expert luthiers, to highlight similarity and differences between the two behaviors. © 2021, Springer Nature Switzerland AG.","CNN; Cultural Heritage; Data augmentation; Deep learning; Historical violins; Transfer learning","Eye tracking; Musical instruments; Pattern recognition; Binary classification; Cultural heritage field; Degree of certainty; Eye-tracking studies; Human expert; Learning approach; Regions of interest; State of the art; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85104461045
"Elmadjian C., Gonzales C., Morimoto C.H.","57202983651;57207696774;7102275798;","Eye Movement Classification with Temporal Convolutional Networks",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12663 LNCS",,,"390","404",,,"10.1007/978-3-030-68796-0_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104349808&doi=10.1007%2f978-3-030-68796-0_28&partnerID=40&md5=6cca28e28dc38f3bc28823948d508c14","University of São Paulo, São Paulo, Brazil","Elmadjian, C., University of São Paulo, São Paulo, Brazil; Gonzales, C., University of São Paulo, São Paulo, Brazil; Morimoto, C.H., University of São Paulo, São Paulo, Brazil","Recently, deep learning approaches have been proposed to detect eye movements such as fixations, saccades, and smooth pursuits from eye tracking data. These are end-to-end methods that have shown to surpass traditional ones, requiring no ad hoc parameters. In this work we propose the use of temporal convolutional networks (TCNs) for automated eye movement classification and investigate the influence of feature space, scale, and context window sizes on the classification results. We evaluated the performance of TCNs against a state-of-the-art 1D-CNN-BLSTM model using GazeCom, a public available dataset. Our results show that TCNs can outperform the 1D-CNN-BLSTM, achieving an F-score of 94.2% for fixations, 89.9% for saccades, and 73.7% for smooth pursuits on sample level, and 89.6%, 94.3%, and 60.2% on event level. We also state the advantages of TCNs over sequential networks for this problem, and how these scores can be further improved by feature space extension. © 2021, Springer Nature Switzerland AG.","Eye movement classification; Feature selection; Temporal convolutional networks","Convolution; Convolutional neural networks; Deep learning; Eye tracking; Motion analysis; Pattern recognition; Robustness (control systems); Ad hoc parameters; Classification results; Context window; Convolutional networks; Eye movement classifications; Learning approach; Smooth pursuit; State of the art; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85104349808
"Muddamsetty S.M., Jahromi M.N.S., Moeslund T.B.","56103912200;57225379672;6507267791;","Expert Level Evaluations for Explainable AI (XAI) Methods in the Medical Domain",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12663 LNCS",,,"35","46",,,"10.1007/978-3-030-68796-0_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104334113&doi=10.1007%2f978-3-030-68796-0_3&partnerID=40&md5=bab865277c8a2aee9efe375bc7faee6c","Visual Analysis of People Laboratory (VAP), Aalborg University, Rendsburggade 14, Aalborg, 9000, Denmark","Muddamsetty, S.M., Visual Analysis of People Laboratory (VAP), Aalborg University, Rendsburggade 14, Aalborg, 9000, Denmark; Jahromi, M.N.S., Visual Analysis of People Laboratory (VAP), Aalborg University, Rendsburggade 14, Aalborg, 9000, Denmark; Moeslund, T.B., Visual Analysis of People Laboratory (VAP), Aalborg University, Rendsburggade 14, Aalborg, 9000, Denmark","The recently emerged field of explainable artificial intelligence (XAI) attempts to shed lights on ‘black box’ Machine Learning (ML) models in understandable terms for human. As several explanation methods are developed alongside different applications for a black box model, the need for expert-level evaluation in inspecting their effectiveness becomes inevitable. This is significantly important for sensitive domains such as medical applications where evaluation of experts is essential to better understand how accurate the results of complex ML are and debug the models if necessary. The aim of this study is to experimentally show how the expert-level evaluation of XAI methods in a medical application can be utilized and aligned with the actual explanations generated by the clinician. To this end, we collect annotations from expert subjects equipped with an eye-tracker while they classify medical images and devise an approach for comparing the results with those obtained from XAI methods. We demonstrate the effectiveness of our approach in several experiments. © 2021, Springer Nature Switzerland AG.","Deep learning; Expert-level explanation; Explainable AI (XAI); Eye-tracker; Retinal Images; XAI evaluation","Eye tracking; Medical applications; Medical imaging; Pattern recognition; Program debugging; Black boxes; Black-box model; Eye trackers; Medical domains; Artificial intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85104334113
"Szalma J., Amora K.K., Vidnyánszky Z., Weiss B.","57209807819;57222991404;55989457800;34168497800;","Investigating the Effect of Inter-letter Spacing Modulation on Data-Driven Detection of Developmental Dyslexia Based on Eye-Movement Correlates of Reading: A Machine Learning Approach",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12663 LNCS",,,"467","481",,,"10.1007/978-3-030-68796-0_34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104306697&doi=10.1007%2f978-3-030-68796-0_34&partnerID=40&md5=d3d57d983b1a45f8df1ab8e4beabdfed","Brain Imaging Centre, Research Centre for Natural Sciences, Budapest, Hungary; Multilingualism Doctoral School, University of Pannonia, Veszprém, Hungary","Szalma, J., Brain Imaging Centre, Research Centre for Natural Sciences, Budapest, Hungary; Amora, K.K., Brain Imaging Centre, Research Centre for Natural Sciences, Budapest, Hungary, Multilingualism Doctoral School, University of Pannonia, Veszprém, Hungary; Vidnyánszky, Z., Brain Imaging Centre, Research Centre for Natural Sciences, Budapest, Hungary; Weiss, B., Brain Imaging Centre, Research Centre for Natural Sciences, Budapest, Hungary","Developmental dyslexia is a reading disability estimated to affect between 5 to 10% of the population. However, current screening methods are limited as they tell very little about the oculomotor processes underlying natural reading. Accordingly, investigating the eye-movement correlates of reading in a machine learning framework could potentially enhance the detection of poor readers. Here, the capability of eye-movement measures in classifying dyslexic and control young adults (24 dyslexic, 24 control) was assessed on eye-tracking data acquired during reading of isolated sentences presented at five inter-letter spacing levels. The set of 65 eye-movement features included properties of fixations, saccades and glissades. Classification accuracy and importance of features were assessed for all spacing levels by aggregating the results of five feature selection methods. Highest classification accuracy (73.25%) was achieved for an increased spacing level, while the worst classification performance (63%) was obtained for the minimal spacing condition. However, the classification performance did not differ significantly between these two spacing levels (p = 0.28). The most important features contributing to the best classification performance across the spacing levels were as follows: median of progressive and all saccade amplitudes, median of fixation duration and interquartile range of forward glissade duration. Selection frequency was even for the median of fixation duration, while the median amplitude of all and forward saccades measures exhibited complementary distributions across the spacing levels. The results suggest that although the importance of features may vary with the size of inter-letter spacing, the classification performance remains invariant. © 2021, Springer Nature Switzerland AG.","Developmental dyslexia; Eye-movement features; Feature selection; Inter-letter spacing; Machine learning; Reading; Support vector machine","Classification (of information); Eye tracking; Machine learning; Pattern recognition; Turing machines; Classification accuracy; Classification performance; Developmental dyslexia; Eye-movement measures; Feature selection methods; Important features; Inter quartile ranges; Machine learning approaches; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85104306697
"Ricotti R., Pella A., Elisei G., Tagaste B., Bello F., Fontana G., Fiore M.R., Ciocca M., Mastella E., Orlandi E., Baroni G.","56604926100;35776741700;57215381991;11240633000;57222156659;55794880800;16230235200;7003398216;54795586300;7005698929;7006568878;","Gaze Stability During Ocular Proton Therapy: Quantitative Evaluation Based on Eye Surface Surveillance Videos",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12663 LNCS",,,"440","452",,1,"10.1007/978-3-030-68796-0_32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104293402&doi=10.1007%2f978-3-030-68796-0_32&partnerID=40&md5=f3e1ba7b425fbcbf8d6246dc03a7983a","Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Strada Campeggi, 53, Pavia, 27100, Italy; Radiation Oncology, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, Italy; Medical Physics Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, Italy; Department of Electronics, Information and Bioengineering, Politecnico di Milano University, Milan, Italy","Ricotti, R., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Strada Campeggi, 53, Pavia, 27100, Italy; Pella, A., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Strada Campeggi, 53, Pavia, 27100, Italy; Elisei, G., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Strada Campeggi, 53, Pavia, 27100, Italy; Tagaste, B., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Strada Campeggi, 53, Pavia, 27100, Italy; Bello, F., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Strada Campeggi, 53, Pavia, 27100, Italy; Fontana, G., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Strada Campeggi, 53, Pavia, 27100, Italy; Fiore, M.R., Radiation Oncology, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, Italy; Ciocca, M., Medical Physics Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, Italy; Mastella, E., Medical Physics Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, Italy; Orlandi, E., Radiation Oncology, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Pavia, Italy; Baroni, G., Bioengineering Unit, Clinical Department, National Center for Oncological Hadrontherapy (CNAO), Strada Campeggi, 53, Pavia, 27100, Italy, Department of Electronics, Information and Bioengineering, Politecnico di Milano University, Milan, Italy","Ocular proton therapy (OPT) is acknowledged as a therapeutic option for the treatment of ocular melanomas. OPT clinical workflow is deeply based on x-ray image guidance procedures, both for treatment planning and patient setup verification purposes. An optimized eye orientation relative to the proton beam axis is determined during treatment planning and it is reproduced during treatment by focusing the patient gaze on a fixation light conveniently positioned in space. Treatment geometry verification is routinely performed through stereoscopic radiographic images while real time patient gaze reproducibility is qualitatively monitored by visual control of eye surface images acquired by dedicated optical cameras. We described an approach to quantitatively evaluate the stability of patients’ gaze direction over an OPT treatment course at the National Centre of Oncological Hadrontherapy (Centro Nazionale di Adroterapia Oncologica, CNAO, Pavia, Italy). Pupil automatic segmentation procedure was implemented on eye surveillance videos of five patients recorded during OPT. Automatic pupil detection performance was benchmarked against manual pupil contours of four different clinical operators. Stability of patients’ gaze direction was quantified. 2D distances were expressed as percentage of the reference pupil radius. Valuable approximation between circular fitting and manual contours was observed. Inter-operator manual contours 2D distances were in median (interquartile range) 3.3% (3.6%) of the of the reference pupil radius. The median (interquartile range) of 2D distances between the automatic segmentations and the manual contours was 5.0% (5.3) of the of the reference pupil radius. Stability of gaze direction varied across patients with median values ranging between 6.6% and 16.5% of reference pupil radius. The measured pupil displacement on the camera field of view were clinically acceptable. Further developments are necessary to reach a real-time clip-less quantification of eye during OPT. © 2021, Springer Nature Switzerland AG.","Eye tracking; Gaze detection; Ocular proton therapy","Cameras; Pattern recognition; Proton beam therapy; Proton beams; Stability; Stereo image processing; X ray radiography; Automatic segmentations; Further development; Geometry Verification; Inter quartile ranges; Quantitative evaluation; Radiographic images; Surface surveillance; Treatment planning; Security systems",Conference Paper,"Final","",Scopus,2-s2.0-85104293402
"Garde G., Larumbe-Bergera A., Porta S., Cabeza R., Villanueva A.","57215963483;57210106737;7005292345;36763933900;7101612861;","Synthetic Gaze Data Augmentation for Improved User Calibration",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12663 LNCS",,,"377","389",,1,"10.1007/978-3-030-68796-0_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104281273&doi=10.1007%2f978-3-030-68796-0_27&partnerID=40&md5=e404f7e913eebea8d638b9a00a0de72f","Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain","Garde, G., Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain; Larumbe-Bergera, A., Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain; Porta, S., Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain; Cabeza, R., Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain; Villanueva, A., Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain","In this paper, we focus on the calibration possibilitiesó of a deep learning based gaze estimation process applying transfer learning, comparing its performance when using a general dataset versus when using a gaze specific dataset in the pretrained model. Subject calibration has demonstrated to improve gaze accuracy in high performance eye trackers. Hence, we wonder about the potential of a deep learning gaze estimation model for subject calibration employing fine-tuning procedures. A pretrained Resnet-18 network, which has great performance in many computer vision tasks, is fine-tuned using user’s specific data in a few shot adaptive gaze estimation approach. We study the impact of pretraining a model with a synthetic dataset, U2Eyes, before addressing the gaze estimation calibration in a real dataset, I2Head. The results of the work show that the success of the individual calibration largely depends on the balance between fine-tuning and the standard supervised learning procedures and that using a gaze specific dataset to pretrain the model improves the accuracy when few images are available for calibration. This paper shows that calibration is feasible in low resolution scenarios providing outstanding accuracies below 1.5 ∘ of error. © 2021, Springer Nature Switzerland AG.","Calibration; Gaze estimation; Transfer learning","Deep learning; Image enhancement; Learning systems; Pattern recognition; Transfer learning; Data augmentation; Eye trackers; Fine tuning; Gaze estimation; Individual calibrations; Low resolution; Pre-training; User calibration; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-85104281273
"Golard A., Talathi S.S.","57222990964;57224997923;","Ultrasound for Gaze Estimation",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12663 LNCS",,,"369","376",,,"10.1007/978-3-030-68796-0_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104277085&doi=10.1007%2f978-3-030-68796-0_26&partnerID=40&md5=0cb0652f9e6014dbb3b879276e1455b4","Facebook Reality Labs, Redmond, WA  98052, United States","Golard, A., Facebook Reality Labs, Redmond, WA  98052, United States; Talathi, S.S., Facebook Reality Labs, Redmond, WA  98052, United States","Most eye tracking methods are light-based. As such they can suffer from ambient light changes when used outdoors. It has been suggested that ultrasound could provide a low power, fast, light-insensitive alternative to camera based sensors for eye tracking. We designed a bench top experimental setup to investigate the utility of ultrasound for eye tracking, and collected time of flight and amplitude data for a range of gaze angles of a model eye. We used this data as input for a machine learning model and demonstrate that we can effectively estimate gaze (gaze RMSE error of 1.021 ± 0.189 ∘ with an adjusted R2 score of 89.92 ± 4.9). © 2021, Springer Nature Switzerland AG.","CMUT; Eye tracking; Machine learning; Ultrasound","Pattern recognition; Turing machines; Ultrasonics; Ambient light; Camera based Sensors; Eye tracking methods; Gaze estimation; Low Power; Machine learning models; Range of gaze; Time of flight; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85104277085
"Zhou F., Yang X.J., de Winter J.C.F.","56640372700;23975212000;8311196100;","Using Eye-Tracking Data to Predict Situation Awareness in Real Time During Takeover Transitions in Conditionally Automated Driving",2021,"IEEE Transactions on Intelligent Transportation Systems",,,,"","",,2,"10.1109/TITS.2021.3069776","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103890643&doi=10.1109%2fTITS.2021.3069776&partnerID=40&md5=d07268958241160352217617e01d1804","Department of Industrial and Manufacturing Systems Engineering, University of Michigan-Dearborn, Dearborn, MI 48128 USA (e-mail: fezhou@umich.edu); Department of Industrial and Operations Engineering, University of Michigan, Ann Arbor, MI 48109 USA.; Cognitive Robotics Department, Faculty of Mechanical, Maritime and Materials Engineering, Delft University of Technology, 2628 CD Delft, The Netherlands.","Zhou, F., Department of Industrial and Manufacturing Systems Engineering, University of Michigan-Dearborn, Dearborn, MI 48128 USA (e-mail: fezhou@umich.edu); Yang, X.J., Department of Industrial and Operations Engineering, University of Michigan, Ann Arbor, MI 48109 USA.; de Winter, J.C.F., Cognitive Robotics Department, Faculty of Mechanical, Maritime and Materials Engineering, Delft University of Technology, 2628 CD Delft, The Netherlands.","Situation awareness (SA) is critical to improving takeover performance during the transition period from automated driving to manual driving. Although many studies measured SA during or after the driving task, few studies have attempted to predict SA in real time in automated driving. In this work, we propose to predict SA during the takeover transition period in conditionally automated driving using eye-tracking and self-reported data. First, a tree ensemble machine learning model, named LightGBM (Light Gradient Boosting Machine), was used to predict SA. Second, in order to understand what factors influenced SA and how, SHAP (SHapley Additive exPlanations) values of individual predictor variables in the LightGBM model were calculated. These SHAP values explained the prediction model by identifying the most important factors and their effects on SA, which further improved the model performance of LightGBM through feature selection. We standardized SA between 0 and 1 by aggregating three performance measures (i.e., placement, distance, and speed estimation of vehicles with regard to the ego-vehicle) of SA in recreating simulated driving scenarios, after 33 participants viewed 32 videos with six lengths between 1 and 20 s. Using only eye-tracking data, our proposed model outperformed other selected machine learning models, having a root-mean-squared error (RMSE) of 0.121, a mean absolute error (MAE) of 0.096, and a 0.719 correlation coefficient between the predicted SA and the ground truth. The code is available at https://github.com/refengchou/Situation-awareness-prediction. Our proposed model provided important implications on how to monitor and predict SA in real time in automated driving using eye-tracking data. IEEE","Atmospheric measurements; automated driving; explainability.; eye-tracking measures; Particle measurements; Predictive models; Real-time situation awareness prediction; Real-time systems; takeover; Task analysis; Time measurement; Vehicles","Adaptive boosting; Automation; Forecasting; Machine learning; Mean square error; Predictive analytics; Correlation coefficient; Machine learning models; Mean absolute error; Performance measure; Predictor variables; Root mean squared errors; Simulated driving; Situation awareness; Eye tracking",Article,"Article in Press","",Scopus,2-s2.0-85103890643
"Li X., Shan Y., Chen W., Wu Y., Hansen P., Perrault S.","57202025304;57222640348;57210164314;57211682110;8862059000;39362211300;","Predicting user visual attention in virtual reality with a deep learning model",2021,"Virtual Reality","25","4",,"1123","1136",,,"10.1007/s10055-021-00512-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103552320&doi=10.1007%2fs10055-021-00512-7&partnerID=40&md5=af96679c1af24a3c9ee1da006204e00d","College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China; Department of Computer Science and Systems, Stockholm University, Stockholm, Sweden; ISTD, Singapore University of Technology and Design, Singapore, Singapore","Li, X., College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China; Shan, Y., College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China; Chen, W., College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China; Wu, Y., College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China; Hansen, P., Department of Computer Science and Systems, Stockholm University, Stockholm, Sweden; Perrault, S., ISTD, Singapore University of Technology and Design, Singapore, Singapore","Recent studies show that user’s visual attention during virtual reality museum navigation can be effectively estimated with deep learning models. However, these models rely on large-scale datasets that usually are of high structure complexity and context specific, which is challenging for nonspecialist researchers and designers. Therefore, we present the deep learning model, ALRF, to generalise on real-time user visual attention prediction in virtual reality context. The model combines two parallel deep learning streams to process the compact dataset of temporal–spatial salient features of user’s eye movements and virtual object coordinates. The prediction accuracy outperformed the state-of-the-art deep learning models by reaching record high 91.03%. Importantly, with quick parametric tuning, the model showed flexible applicability across different environments of the virtual reality museum and outdoor scenes. Implications for how the proposed model may be implemented as a generalising tool for adaptive virtual reality application design and evaluation are discussed. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Deep learning model; Eye tracking; Virtual reality; Visual attention","Behavioral research; E-learning; Eye movements; Forecasting; Large dataset; Learning systems; Virtual reality; Application design; Large-scale datasets; Parametric tunings; Prediction accuracy; Salient features; State of the art; Structure complexity; Visual Attention; Deep learning",Article,"Final","",Scopus,2-s2.0-85103552320
"Wang X., Zhang J., Zhang H., Zhao S., Liu H.","57219425011;57214900779;57222558640;57222556510;57218402201;","Vision-based Gaze Estimation: A Review",2021,"IEEE Transactions on Cognitive and Developmental Systems",,,,"","",,,"10.1109/TCDS.2021.3066465","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103172218&doi=10.1109%2fTCDS.2021.3066465&partnerID=40&md5=2a953fb3a71db63522a4fcb7982a7621","school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China. (e-mail: 57652761@qq.com); School of Computer Science and Engineering, Tianjin University of Technology, Tianjin 300384, China.; school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China.; School of Computing, University of Portsmouth, Portsmouth PO13HE, U.K.","Wang, X., school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China. (e-mail: 57652761@qq.com); Zhang, J., School of Computer Science and Engineering, Tianjin University of Technology, Tianjin 300384, China.; Zhang, H., school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China.; Zhao, S., School of Computing, University of Portsmouth, Portsmouth PO13HE, U.K.; Liu, H., school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China.","Eye gaze is an important natural behavior in social interaction as it delivers complex exchanges between observer and observed, by building up the geometric constraints and relation of the exchanges. These inter-person exchanges can be modeled based on gaze direction estimated using computer vision. Despite significant progresses in vision-based gaze estimation in last 10 years, it is still nontrivial since the accuracy of gaze estimation is significantly affected by such intrinsic factors as head pose variance, individual bias between optical axis and visual axis, eye blink, occlusion and image blur, degrade gaze features, lead to inaccurate gaze-involved human social interaction analysis. This paper aims to review and discuss existing methods addressing above-mentioned problems, gaze involved applications and datasets against the state-of-the-arts in vision-based gaze estimation. It also points out future research directions and challenges of gaze estimation in terms of meta learning, causal inference, disentangled representation, and social gaze behaviour for unconstrained gaze estimation. IEEE","3D Gaze Estimation; Computer Vision.; Estimation; Faces; Feature extraction; Head pose; Iris; Optical axis; Solid modeling; Three-dimensional displays; Visual axis; Visualization","Causal inferences; Future research directions; Gaze estimation; Geometric constraint; Human social interactions; Intrinsic factors; Social interactions; State of the art; Arts computing",Article,"Article in Press","",Scopus,2-s2.0-85103172218
"Varley P.A.C., Cristina S., Bonnici A., Camilleri K.P.","26649183800;49963155000;35748631300;8301303700;","As plain as the nose on your face?",2021,"VISIGRAPP 2021 - Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","4",,,"471","479",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103023415&partnerID=40&md5=9b5282e4647035dfe5c1ba98b377abc9","Department of Systems and Control Engineering, University of Malta, Msida, MSD, 2080, Malta","Varley, P.A.C., Department of Systems and Control Engineering, University of Malta, Msida, MSD, 2080, Malta; Cristina, S., Department of Systems and Control Engineering, University of Malta, Msida, MSD, 2080, Malta; Bonnici, A., Department of Systems and Control Engineering, University of Malta, Msida, MSD, 2080, Malta; Camilleri, K.P., Department of Systems and Control Engineering, University of Malta, Msida, MSD, 2080, Malta","We present an investigation into locating nose tips in 2D images of human faces. Our objective is conferenceroom gaze-tracking, in which a presenter can control a presentation or demonstration by gaze from a distance in the range 2m to 10m. In a first step towards this, we here consider faces in the range 150cm to 300cm. Head pose is the major contributing component of gaze direction, and nose tip position within the image of the face is a strong clue to head pose. To facilitate detection of nose tips, we have implemented a combination of two Haar cascades (one for frontal noses and one for profile noses) with a lower failure rate than existing cascades, and we have examined a number of ""hand-crafted ferns""for their potential to locate the nose tip within the nose-like regions returned by our Haar cascades. Copyright © 2021 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","Gaze Tracking; Haar Cascades; Head Pose; Noses","Computer graphics; Eye tracking; Failure analysis; 2D images; Failure rate; Gaze direction; Gaze tracking; Head pose; Human faces; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85103023415
"Ajenaghughrure I.B., Da Costa Sousa S.C., Lamas D.","57205723162;54883324200;36666606200;","Psychophysiological modelling of trust in technology: Comparative analysis of psychophysiological signals",2021,"VISIGRAPP 2021 - Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","2",,,"161","173",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102969589&partnerID=40&md5=11b74d885ec69ed852f97d9c3bb951d6","School of Digital Technologies, Tallinn University, Narva Mnt 25, Tallinn, 10120, Estonia","Ajenaghughrure, I.B., School of Digital Technologies, Tallinn University, Narva Mnt 25, Tallinn, 10120, Estonia; Da Costa Sousa, S.C., School of Digital Technologies, Tallinn University, Narva Mnt 25, Tallinn, 10120, Estonia; Lamas, D., School of Digital Technologies, Tallinn University, Narva Mnt 25, Tallinn, 10120, Estonia","Measuring users trust with psychophysiological signals during interaction (real-time) with autonomous systems that incorporates artificial intelligence has been widely researched with several psychophysiological signals. However, it is unclear what psychophysiological is most reliable for real-time trust assessment during user’s interaction with an autonomous system. This study investigates what psychophysiological signal is most suitable for assessing trust in real-time. A within-subject four condition experiment was implemented with a virtual reality autonomous vehicle driving game that involved 31 carefully selected participants, while electroencephalogram, electrodermal activity, eletrocardiogram, eye-tracking and facial electromyogram psychophysiological signals were acquired. We applied hybrid feature selection methods on the features extracted from the psychophysiological signals. Using training and testing datasets containing only the resulting features from the feature selection methods, for each individual and multi-modal (combined) psychophysiological signals, we trained and tested six stack ensemble trust classifier models. The results of the model’s performance indicate that the EEG is most reliable, while the multimodal psychophysiological signals remain promising. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Artificial intelligence; Autonomous vehicle; Machine learning; Psychophysiology; Trust","Artificial intelligence; Classification (of information); Computer graphics; Computer vision; Electroencephalography; Eye tracking; Feature extraction; Real time systems; Autonomous systems; Comparative analysis; Electrodermal activity; Feature selection methods; Hybrid feature selections; Psychophysiological signals; Training and testing; Trust in technologies; Signal analysis",Conference Paper,"Final","",Scopus,2-s2.0-85102969589
"Romaguera T.V., Romaguera L.V., Piñol D.C., Seisdedos C.R.V.","57219764017;57201665413;57219766953;52464634100;","Pupil center detection approaches: A comparative analysis",2021,"Computacion y Sistemas","25","1",,"67","81",,,"10.13053/CYS-25-1-3385","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102489031&doi=10.13053%2fCYS-25-1-3385&partnerID=40&md5=460864bfec5ad61cd5817f65cf2ec794","Universidad de Oriente, Center for Neuroscience Studies, Images and Signals Processing, Cuba; École Polytechnique de Montréal, Electrical Engineering Department, Canada; Universidad de Oriente, Department of Telecommunications, Cuba","Romaguera, T.V., Universidad de Oriente, Center for Neuroscience Studies, Images and Signals Processing, Cuba; Romaguera, L.V., École Polytechnique de Montréal, Electrical Engineering Department, Canada; Piñol, D.C., Universidad de Oriente, Department of Telecommunications, Cuba; Seisdedos, C.R.V., Universidad de Oriente, Center for Neuroscience Studies, Images and Signals Processing, Cuba","In the last decade, the development of technologies and tools for eye tracking has been a constantly growing area. Detecting the center of the pupil using image processing techniques has been an essential step in this process. A large number of techniques have been proposed for pupil center detection using both traditional image processing and machine learning-based methods. Despite the large number of methods proposed, no comparative work on their performance was found, using the same images and performance metrics. In this work, we aim at comparing four of the most frequently cited traditional methods for pupil center detection in terms of accuracy, robustness, and computational cost. These methods are based on the circular Hough transform, ellipse fitting, Daugman's integro-differential operator and radial symmetry transform. The comparative analysis was performed with 800 infrared images from the CASIAIrisV3 and CASIA-IrisV4 databases containing various types of disturbances. The best performance was obtained by the method based on the radial symmetry transform with an accuracy and average robustness higher than 94%. The shortest processing time, obtained with the ellipse fitting method, was 0.06 s. © 2021 Instituto Politecnico Nacional. All rights reserved.","Ellipse fitting; Hough Daugman; Pupil detection; Radial symmetry",,Article,"Final","",Scopus,2-s2.0-85102489031
"Krishnan S., Amudha J., Tejwani S.","57222997680;35766448700;55988859900;","Gaze Fusion-Deep Neural Network Model for Glaucoma Detection",2021,"Communications in Computer and Information Science","1366",,,"42","53",,,"10.1007/978-981-16-0419-5_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102276918&doi=10.1007%2f978-981-16-0419-5_4&partnerID=40&md5=883e0113ea30be750a0f2d3e809c7f1f","Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bangalore, India; Narayana Nethralaya, Bommasandra, Bengaluru, India","Krishnan, S., Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bangalore, India; Amudha, J., Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bangalore, India; Tejwani, S., Narayana Nethralaya, Bommasandra, Bengaluru, India","The proposed system, Gaze Fusion - Deep Neural Network Model (GFDM) has utilized transfer learning approach to discriminate subject’s eye tracking data in the form of fusion map into two classes: glaucoma and normal. We have fed eye tracking data in the form of fusion maps of different participants to Deep Neural Network (DNN) model which is pretrained with ImageNet weights. The experimental results of the GFDM show that fusion map dissimilar to pretrained model’s dataset can give better understanding of glaucoma. The model also show the part of the screen where participants has the difficulty in viewing. GFDM has compared with traditional machine learning models such as Support Vector Classifier, Decision Tree classifier and ensemble classifier and shown that the proposed model outperforms other classifiers. The model has Area Under ROC Curve (AUC) score 0.75. The average sensitivity of correctly identifying glaucoma patients is 100% with specificity value 83%. © 2021, Springer Nature Singapore Pte Ltd.","Deep neural network; Eye tracking; Fusion map; Glaucoma; Transfer learning","Decision trees; Deep learning; Deep neural networks; Eye tracking; Heuristic algorithms; Learning algorithms; Learning systems; Ophthalmology; Transfer learning; Area under roc curve (AUC); Average sensitivities; Decision tree classifiers; Ensemble classifiers; Glaucoma detection; Machine learning models; Neural network model; Support vector classifiers; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85102276918
"Lu Y., Wang Y., Xin Y., Wu D., Lu G.","57212483262;57221296847;57215128937;57215128978;55872294200;","Unsupervised Gaze: Exploration of Geometric Constraints for 3D Gaze Estimation",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12573 LNCS",,,"121","133",,,"10.1007/978-3-030-67835-7_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101602875&doi=10.1007%2f978-3-030-67835-7_11&partnerID=40&md5=d14dd2caf1b26a176b82c14b1f79c6b3","Intelligent Vision and Sensing Lab, Rochester Institute of Technology, Rochester, United States; Tecent Deep Sea Lab, Shenzhen, China","Lu, Y., Intelligent Vision and Sensing Lab, Rochester Institute of Technology, Rochester, United States; Wang, Y., Intelligent Vision and Sensing Lab, Rochester Institute of Technology, Rochester, United States; Xin, Y., Tecent Deep Sea Lab, Shenzhen, China; Wu, D., Tecent Deep Sea Lab, Shenzhen, China; Lu, G., Intelligent Vision and Sensing Lab, Rochester Institute of Technology, Rochester, United States","Eye gaze estimation can provide critical evidence for people attention, which has extensive applications on cognitive science and computer vision areas, such as human behavior analysis and fake user identification. Existing typical methods mostly place the eye-tracking sensors directly in front of the eyeballs, which is hard to be utilized in the wild. And recent learning-based methods require prior ground truth annotations of gaze vector for training. In this paper, we propose an unsupervised learning-based method for estimating the eye gaze in 3D space. Building on top of the existing unsupervised approach to regress shape parameters and initialize the depth, we propose to apply geometric spectral photometric consistency constraint and spatial consistency constraints across multiple views in video sequences to refine the initial depth values on the detected iris landmark. We demonstrate that our method is able to learn gaze vector in the wild scenes more robust without ground truth gaze annotations or 3D supervision, and show our system leads to a competitive performance compared with existing supervised methods. © 2021, Springer Nature Switzerland AG.","3D gaze estimation; Geometric constraints; Unsupervised learning","Behavioral research; Learning systems; Competitive performance; Consistency constraints; Eye-tracking sensors; Geometric constraint; Human behavior analysis; Learning-based methods; Unsupervised approaches; User identification; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85101602875
"Zhang L., Zhang X., Xu M., Shao L.","35231925400;35232030000;55703591000;55643855000;","Massive-Scale Aerial Photo Categorization by Cross-Resolution Visual Perception Enhancement",2021,"IEEE Transactions on Neural Networks and Learning Systems",,,,"","",,,"10.1109/TNNLS.2021.3055548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100933709&doi=10.1109%2fTNNLS.2021.3055548&partnerID=40&md5=28c2444de3939af39b1af8406754e51c","College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou 325035, China.; College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou 325035, China (e-mail: zhangxiaoqinnan@gmail.com); Center for Interdisciplinary Information Science Research, Zhengzhou University, Zhengzhou 450000, China.; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE.","Zhang, L., College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou 325035, China.; Zhang, X., College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou 325035, China (e-mail: zhangxiaoqinnan@gmail.com); Xu, M., Center for Interdisciplinary Information Science Research, Zhengzhou University, Zhengzhou 450000, China.; Shao, L., Inception Institute of Artificial Intelligence, Abu Dhabi, UAE.","Categorizing aerial photographs with varied weather/lighting conditions and sophisticated geomorphic factors is a key module in autonomous navigation, environmental evaluation, and so on. Previous image recognizers cannot fulfill this task due to three challenges: 1) localizing visually/semantically salient regions within each aerial photograph in a weakly annotated context due to the unaffordable human resources required for pixel-level annotation; 2) aerial photographs are generally with multiple informative attributes (e.g., clarity and reflectivity), and we have to encode them for better aerial photograph modeling; and 3) designing a cross-domain knowledge transferal module to enhance aerial photograph perception since multiresolution aerial photographs are taken asynchronistically and are mutually complementary. To handle the above problems, we propose to optimize aerial photograph's feature learning by leveraging the low-resolution spatial composition to enhance the deep learning of perceptual features with a high resolution. More specifically, we first extract many BING-based object patches (Cheng et al., 2014) from each aerial photograph. A weakly supervised ranking algorithm selects a few semantically salient ones by seamlessly incorporating multiple aerial photograph attributes. Toward an interpretable aerial photograph recognizer indicative to human visual perception, we construct a gaze shifting path (GSP) by linking the top-ranking object patches and, subsequently, derive the deep GSP feature. Finally, a cross-domain multilabel SVM is formulated to categorize each aerial photograph. It leverages the global feature from low-resolution counterparts to optimize the deep GSP feature from a high-resolution aerial photograph. Comparative results on our compiled million-scale aerial photograph set have demonstrated the competitiveness of our approach. Besides, the eye-tracking experiment has shown that our ranking-based GSPs are over 92&#x0025; consistent with the real human gaze shifting sequences. IEEE","Aerial photograph; cross domain; Image recognition; Kernel; machine learning; perception enhancement; ranking; Semantics; Support vector machines; Training; transfer learning; Visual perception; Visualization","Air navigation; Antennas; Deep learning; Eye tracking; Image enhancement; Photographic equipment; Vision; Aerial Photographs; Autonomous navigation; Environmental evaluation; Human visual perception; Informative attributes; Perceptual feature; Spatial composition; Visual perception; Aerial photography",Article,"Article in Press","",Scopus,2-s2.0-85100933709
"Amadori P.V., Fischer T., Demiris Y.","56703112800;57190126084;6506125343;","HammerDrive: A Task-Aware Driving Visual Attention Model",2021,"IEEE Transactions on Intelligent Transportation Systems",,,,"","",,,"10.1109/TITS.2021.3055120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100869852&doi=10.1109%2fTITS.2021.3055120&partnerID=40&md5=265f423a47285f84b96887e03323c2d0","Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K. (e-mail: p.amadori@imperial.ac.uk); Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K..","Amadori, P.V., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K. (e-mail: p.amadori@imperial.ac.uk); Fischer, T., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K..; Demiris, Y., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K..","We introduce HammerDrive, a novel architecture for task-aware visual attention prediction in driving. The proposed architecture is learnable from data and can reliably infer the current focus of attention of the driver in real-time, while only requiring limited and easy-to-access telemetry data from the vehicle. We build the proposed architecture on two core concepts: 1) driving can be modeled as a collection of sub-tasks (maneuvers), and 2) each sub-task affects the way a driver allocates visual attention resources, i.e., their eye gaze fixation. HammerDrive comprises two networks: a hierarchical monitoring network of forward-inverse model pairs for sub-task recognition and an ensemble network of task-dependent convolutional neural network modules for visual attention modeling. We assess the ability of HammerDrive to infer driver visual attention on data we collected from 20 experienced drivers in a virtual reality-based driving simulator experiment. We evaluate the accuracy of our monitoring network for sub-task recognition and show that it is an effective and light-weight network for reliable real-time tracking of driving maneuvers with above 90&#x0025; accuracy. Our results show that HammerDrive outperforms a comparable state-of-the-art deep learning model for visual attention prediction on numerous metrics with ~13&#x0025; improvement for both Kullback-Leibler divergence and similarity, and demonstrate that task-awareness is beneficial for driver visual attention prediction. Crown","Advanced driver-assistance systems; Computational modeling; Computer architecture; HAMMER.; Predictive models; Real-time systems; simulated driving; Task analysis; task recognition; Vehicles; visual attention; Visualization","Convolutional neural networks; Deep learning; Forecasting; Inverse problems; Network architecture; Virtual reality; Driving simulator; Focus of Attention; Kullback Leibler divergence; Monitoring network; Novel architecture; Proposed architectures; Real time tracking; Visual attention model; Behavioral research",Article,"Article in Press","",Scopus,2-s2.0-85100869852
"De Cock L., Van de Weghe N., Ooms K., Vanhaeren N., Ridolfi M., De Poorter E., De Maeyer P.","57204432138;8973773700;35176370900;57196039159;57193089591;23396863400;8973773500;","Taking a closer look at indoor route guidance; usability study to compare an adapted and non-adapted mobile prototype",2021,"Spatial Cognition and Computation",,,,"","",,1,"10.1080/13875868.2021.1885411","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100860514&doi=10.1080%2f13875868.2021.1885411&partnerID=40&md5=eb8f19adf5bc899cbca5ff42f2631805","Department of Geography, Ghent University, Ghent, Belgium; IDLab, Department of Information Technology, Ghent University - Imec, Ghent, Belgium","De Cock, L., Department of Geography, Ghent University, Ghent, Belgium; Van de Weghe, N., Department of Geography, Ghent University, Ghent, Belgium; Ooms, K., Department of Geography, Ghent University, Ghent, Belgium; Vanhaeren, N., Department of Geography, Ghent University, Ghent, Belgium; Ridolfi, M., IDLab, Department of Information Technology, Ghent University - Imec, Ghent, Belgium; De Poorter, E., IDLab, Department of Information Technology, Ghent University - Imec, Ghent, Belgium; De Maeyer, P., Department of Geography, Ghent University, Ghent, Belgium","As indoor wayfinding can be very challenging, adapted systems, which adapt the route instruction type, are being developed to facilitate more supportive indoor route guidance. In this study, such a system has been developed based on the results of an online survey. This adapted system was compared with a non-adapted system by use of eye tracking, position tracking, an orientation test and a questionnaire. The results revealed that using symbols instead of photos reduced the imposed cognitive load, while using 3D-simulations instead of photos improved the environmental awareness. This resulted in less wayfinding errors with the adapted system, compared to the non-adapted system. Therefore, the present study provides additional evidence on the benefits of adapted systems for indoor route guidance. © 2021 Taylor & Francis.","Adapted route guidance; eye tracking; indoor wayfinding; route instruction type; UWB",,Article,"Article in Press","",Scopus,2-s2.0-85100860514
"Li T.-H., Suzuki H., Ohtake Y.","57218311579;36068450200;7005292455;","Visualization of user's attention on objects in 3d environment using only eye tracking glasses",2021,"Journal of Computational Design and Engineering","7","2",,"228","237",,2,"10.1093/JCDE/QWAA019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100857330&doi=10.1093%2fJCDE%2fQWAA019&partnerID=40&md5=a3a1835bb96e336bd6474682b945abf6","Department of Precision Engineering, The University of Tokyo, Tokyo, 113-8656, Japan","Li, T.-H., Department of Precision Engineering, The University of Tokyo, Tokyo, 113-8656, Japan; Suzuki, H., Department of Precision Engineering, The University of Tokyo, Tokyo, 113-8656, Japan; Ohtake, Y., Department of Precision Engineering, The University of Tokyo, Tokyo, 113-8656, Japan","Eye tracking technology is widely applied to detect user's attention in a 2D field, such as web page design, package design, and shooting games. However, because our surroundings primarily consist of 3D objects, applications will be expanded if there is an effective method to obtain and display user's 3D gaze fixation. In this research, a methodology is proposed to demonstrate the user's 3D gaze fixation on a digital model of a scene using only a pair of eye tracking glasses. The eye tracking glasses record user's gaze data and scene video. Thus, using image-based 3D reconstruction, a 3D model of the scene can be reconstructed from the frame images; simultaneously, the transformation matrix of each frame image can be evaluated to find 3D gaze fixation on the 3D model. In addition, a method that demonstrates multiple users' 3D gaze fixation on the same digital model is presented to analyze gaze distinction between different subjects. With this preliminary development, this approach shows potential to be applied to a larger environment and conduct a more reliable investigation. © 2020 Society for Computational Design and Engineering. All rights reserved.","3D gaze visualization; Eye tracking technology; Gaze distinction; Image-based 3D reconstruction","3D modeling; Eye tracking; Glass; Linear transformations; Object tracking; Websites; 3-D environments; 3D reconstruction; Digital model; Eye tracking technologies; Multiple user; Package designs; Transformation matrices; Web page design; Three dimensional computer graphics; digital image; electronic equipment; eye; image analysis; three-dimensional modeling; tracking; visualization; detection method; experimental study; methodology; model; perception",Article,"Final","",Scopus,2-s2.0-85100857330
"Hijazi H., Couceiro R., Castelhano J., De Carvalho P., Castelo-Branco M., Madeira H.","57221983221;23501459000;55470748800;26535630100;6701423628;7004929011;","Intelligent biofeedback augmented content comprehension (tellback)",2021,"IEEE Access","9",,"9352725","28393","28406",,,"10.1109/ACCESS.2021.3058664","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100836531&doi=10.1109%2fACCESS.2021.3058664&partnerID=40&md5=cd0a87a14948e745081d42f92173be2f","Centre of Informatics and Systems, University of Coimbra (CISUC), Coimbra, 3000-214, Portugal; Coimbra Institute for Biomedical Imaging and Translational Research (CIBIT), Institute of Nuclear Sciences Applied to Health (ICNAS), University of Coimbra, Coimbra, 3000-214, Portugal","Hijazi, H., Centre of Informatics and Systems, University of Coimbra (CISUC), Coimbra, 3000-214, Portugal; Couceiro, R., Centre of Informatics and Systems, University of Coimbra (CISUC), Coimbra, 3000-214, Portugal; Castelhano, J., Coimbra Institute for Biomedical Imaging and Translational Research (CIBIT), Institute of Nuclear Sciences Applied to Health (ICNAS), University of Coimbra, Coimbra, 3000-214, Portugal; De Carvalho, P., Centre of Informatics and Systems, University of Coimbra (CISUC), Coimbra, 3000-214, Portugal; Castelo-Branco, M., Coimbra Institute for Biomedical Imaging and Translational Research (CIBIT), Institute of Nuclear Sciences Applied to Health (ICNAS), University of Coimbra, Coimbra, 3000-214, Portugal; Madeira, H., Centre of Informatics and Systems, University of Coimbra (CISUC), Coimbra, 3000-214, Portugal","Assessing comprehension difficulties requires the ability to assess cognitive load. Changes in cognitive load induced by comprehension difficulties could be detected with an adequate time resolution using different biofeedback measures (e.g., changes in the pupil diameter). However, identifying the Spatiooral sources of content comprehension difficulties (i.e., when, and where exactly the difficulty occurs in content regions) with a fine granularity is a big challenge that has not been explicitly addressed in the state-of-the-art. This paper proposes and evaluates an innovative approach named Intelligent BiofeedbackAugmented Content Comprehension (TellBack) to explicitly address this challenge. The goal is to autonomously identify regions of digital content that cause user's comprehension difficulty, opening the possibility to provide real-time comprehension support to users. TellBack is based on assessing the cognitive load associated with content comprehension through non-intrusive cheap biofeedback devices that acquire measures such as pupil response or Heart Rate Variability (HRV). To identify when exactly the difficulty in comprehension occurs, physiological manifestations of the Autonomic Nervous System (ANS) such as the pupil diameter variability and the modulation of HRV are exploited, whereas the fine spatial resolution (i.e., the region of content where the user is looking at) is provided by eye-tracking. The evaluation results of this approach show an accuracy of 83.00% ± 0.75 in classifying regions of content as difficult or not difficult using Support Vector Machine (SVM), and precision, recall, and micro F1-score of 0.89, 0.79, and 0.83, respectively. Results obtained with 4 other classifiers, namely Random Forest, k-nearest neighbor, Decision Tree, and Gaussian Naive Bayes, showed a slightly lower precision. TellBack outperforms the state-of-the-art in precision & recall by 23% and 17% respectively. © 2013 IEEE.","Biomedical measurement; Cognitive load; Content comprehension; Eye-tracking; Heart rate variability; Machine learning","Biofeedback; Decision trees; Eye tracking; Nearest neighbor search; Autonomic nervous system; Digital contents; Evaluation results; Heart rate variability; Innovative approaches; K-nearest neighbors; Spatial resolution; State of the art; Support vector machines",Article,"Final","",Scopus,2-s2.0-85100836531
"Akter T., Ali M.H., Khan M.I., Satu M.S., Moni M.A.","57202700495;55470937400;57219312827;57189219909;35119094400;","Machine Learning Model to Predict Autism Investigating Eye-Tracking Dataset",2021,"International Conference on Robotics, Electrical and Signal Processing Techniques",,,"9331152","383","387",,3,"10.1109/ICREST51555.2021.9331152","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100707616&doi=10.1109%2fICREST51555.2021.9331152&partnerID=40&md5=efdee004c16effb464b57c062dd9d161","Jahangirnagar University, Dept. of Cse, Dhaka, Bangladesh; Gono Bishwabidyalay, Dept. of Cse, Dhaka, Bangladesh; NSTU, Dept. of Mis, Noakhali, Bangladesh; University of New South Wales, Unsw Digital Health, Sydney, NSW  2052, Australia","Akter, T., Jahangirnagar University, Dept. of Cse, Dhaka, Bangladesh; Ali, M.H., Jahangirnagar University, Dept. of Cse, Dhaka, Bangladesh; Khan, M.I., Gono Bishwabidyalay, Dept. of Cse, Dhaka, Bangladesh; Satu, M.S., NSTU, Dept. of Mis, Noakhali, Bangladesh; Moni, M.A., University of New South Wales, Unsw Digital Health, Sydney, NSW  2052, Australia","Autism spectrum disorder is a neurodevelopmental disorder that characterizes by reducing concentration on social activities and improving interest in non-social tasks. The aim of this work is to investigate eye gazing images and identify autism applying various machine learning techniques. Therefore, we collected eye-tracking data from the Figshare data repository. But, these scanpath images were almost similar for normal and autistic children. To obtain similar groups, k-means clustering method was used and generated four clusters. Further, several classifiers were applied into primary data and these clusters and evaluated the performance of them using various metrics. After the assessment of overall results, MLP shows the highest 87% accuracy in cluster 1. In addition, it shows the best area under curve, f-measure, g-mean, sensitivity, specificity, fall out and miss rate respectively. This predictive model could notably useful to forecast ASD status at early stages. © 2021 IEEE.","ASD; Classifier; Eye Tracking; K-Means Clustering; Machine Learning","Agricultural robots; Diseases; K-means clustering; Machine learning; Predictive analytics; Robotics; Signal processing; Autism spectrum disorders; Autistic children; Data repositories; K-means clustering method; Machine learning models; Machine learning techniques; Predictive modeling; Social activities; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85100707616
"Mathis F., Williamson J.H., Vaniea K., Khamis M.","57201295637;8678439600;23037455300;35243028400;","Fast and secure authentication in virtual reality using coordinated 3D manipulation and pointing",2021,"ACM Transactions on Computer-Human Interaction","28","1","6","","",,6,"10.1145/3428121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100428835&doi=10.1145%2f3428121&partnerID=40&md5=4d91f9d4e4a7683be1ab0890a891dd10","School of Computing Science, University of Glasgow, 18 Lilybank Gardens, Glasgow, G12 8RZ, United Kingdom; School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh, EH8 9AB, United Kingdom","Mathis, F., School of Computing Science, University of Glasgow, 18 Lilybank Gardens, Glasgow, G12 8RZ, United Kingdom; Williamson, J.H., School of Computing Science, University of Glasgow, 18 Lilybank Gardens, Glasgow, G12 8RZ, United Kingdom; Vaniea, K., School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh, EH8 9AB, United Kingdom; Khamis, M., School of Computing Science, University of Glasgow, 18 Lilybank Gardens, Glasgow, G12 8RZ, United Kingdom","There is a growing need for usable and secure authentication in immersive virtual reality (VR). Established concepts (e.g., 2D authentication schemes) are vulnerable to observation attacks, and most alternatives are relatively slow. We present RubikAuth, an authentication scheme for VR where users authenticate quickly and secure by selecting digits from a virtual 3D cube that leverages coordinated 3D manipulation and pointing. We report on results from three studies comparing how pointing using eye gaze, head pose, and controller tapping impact RubikAuth's usability, memorability, and observation resistance under three realistic threat models. We found that entering a four-symbol RubikAuth password is fast: 1.69-3.5 s using controller tapping, 2.35-4.68 s using head pose and 2.39 -4.92 s using eye gaze, and highly resilient to observations: 96-99.55% of observation attacks were unsuccessful. RubikAuth also has a large theoretical password space: 45n for an n-symbols password. Our work underlines the importance of considering novel but realistic threat models beyond standard one-time attacks to fully assess the observation-resistance of authentication schemes. We conclude with an in-depth discussion of authentication systems for VR and outline five learned lessons for designing and evaluating authentication schemes. © 2021 Copyright held by the owner/author(s).","Authentication; Head-mounted displays; Observation; Threat modeling; Usable security; Virtual reality","Cryptography; Software architecture; Virtual reality; 3D manipulation; Authentication scheme; Authentication systems; Highly resilient; Immersive virtual reality; Observation attacks; Password spaces; Secure authentications; Authentication",Article,"Final","",Scopus,2-s2.0-85100428835
"Xu M., Yang L., Tao X., Duan Y., Wang Z.","55703599800;57195075538;57208894708;57195315827;24170127500;","Saliency Prediction on Omnidirectional Image with Generative Adversarial Imitation Learning",2021,"IEEE Transactions on Image Processing","30",,"9328187","2087","2102",,,"10.1109/TIP.2021.3050861","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099728936&doi=10.1109%2fTIP.2021.3050861&partnerID=40&md5=50db1ade53536dcf2bd938d4fd6fffbd","School of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","Xu, M., School of Electronic and Information Engineering, Beihang University, Beijing, China; Yang, L., School of Electronic and Information Engineering, Beihang University, Beijing, China; Tao, X., Department of Electronic Engineering, Tsinghua University, Beijing, China; Duan, Y., Department of Electronic Engineering, Tsinghua University, Beijing, China; Wang, Z., School of Electronic and Information Engineering, Beihang University, Beijing, China","When watching omnidirectional images (ODIs), subjects can access different viewports by moving their heads. Therefore, it is necessary to predict subjects' head fixations on ODIs. Inspired by generative adversarial imitation learning (GAIL), this paper proposes a novel approach to predict saliency of head fixations on ODIs, named SalGAIL. First, we establish a dataset for attention on ODIs (AOI). In contrast to traditional datasets, our AOI dataset is large-scale, which contains the head fixations of 30 subjects viewing 600 ODIs. Next, we mine our AOI dataset and discover three findings: (1) the consistency of head fixations are consistent among subjects, and it grows alongside the increased subject number; (2) the head fixations exist with a front center bias (FCB); and (3) the magnitude of head movement is similar across the subjects. According to these findings, our SalGAIL approach applies deep reinforcement learning (DRL) to predict the head fixations of one subject, in which GAIL learns the reward of DRL, rather than the traditional human-designed reward. Then, multi-stream DRL is developed to yield the head fixations of different subjects, and the saliency map of an ODI is generated via convoluting predicted head fixations. Finally, experiments validate the effectiveness of our approach in predicting saliency maps of ODIs, significantly better than 11 state-of-The-Art approaches. Our AOI dataset and code of SalGAIL are available online at https://github.com/yanglixiaoshen/SalGAIL. © 1992-2012 IEEE.","deep reinforcement learning; imitation learning; large-scale dataset; Omnidirectional images","Deep learning; Forecasting; Image segmentation; Large dataset; Center bias; Head movements; Imitation learning; Multi-stream; Omnidirectional image; Saliency map; State-of-the-art approach; Reinforcement learning; adolescent; adult; eye fixation; factual database; female; head movement; human; image processing; male; physiology; procedures; young adult; Adolescent; Adult; Databases, Factual; Deep Learning; Eye-Tracking Technology; Female; Fixation, Ocular; Head Movements; Humans; Image Processing, Computer-Assisted; Male; Young Adult",Article,"Final","",Scopus,2-s2.0-85099728936
"Averbukh V.L.","6603950562;","Evolution of human computer interaction",2021,"Scientific Visualization","12","5",,"130","164",,,"10.26583/SV.12.5.11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099121969&doi=10.26583%2fSV.12.5.11&partnerID=40&md5=672ca60668b828aa8f5379c98bcb2a64","IMM, UrB, RAS; Ural Federal University, Russian Federation","Averbukh, V.L., IMM, UrB, RAS, Ural Federal University, Russian Federation","The work is devoted to the review of the development the human-computer interaction. In the first sections the history of computing in the ""pre-computer"" era is briefly described and then the early history of modern computing, methods of the first computers controlling and the tasks of programmers at this stage are described. It describes the methods of interaction with the first -generation computers using the remote control elements, punched cards and punched tapes. The section, devoted to the second generation computers, describes the emergence of high-level operating systems and programming languages. At this point, there are such means of interaction with the computer as the displays and, respectively, such programming tools as interactive languages and interactive debuggers. Research is also beginning on principles of human-computer interaction the infancy of the discipline ""computer graphics"", the development of computer graphics packages and the emergence of interactive computer graphics standards are considered. In the section “Revolutions in computer science” describes the appearance of a large number of the same series computers and the first super-computers in the context of human-computer interaction. Revolutionary changes are considered in computer graphics and emerging of the science discipline “computer visualization” with its parts “scientific visualization”, “software visualization”, “information visualization” and also “programming by demonstration”. The information about the attempt to create a fifth generation computer based on logical programming is given. It is told about the initial period of teaching programming. The creation of computer networks and the emergence of personal computing as well as the creation the tools of modern parallel computing have become the important stages in the development of modern computing. The virtual reality becomes an important computer visualization tool. The modern state of human-computer interfaces is characterized primarily by emerging of natural interfaces which can be attributed Brain-Computer Interface (Neurocomputer interface, Brain-Computer Interfaces), interfaces based on the direct use of nerve impulses, speech recognition, recognition of lip movement, mimic recognition and eye tracking (Eye Gaze or Eye Tracking), haptic interfaces and also interfaces giving tactile feedback (allowing you to feel the touch),motion capture interfaces the entire human body or individual organs (head, entire arm, hands, fingers, legs), motion capture toolkits,in particular, interfaces based on leg movements (foot-operated computer interfaces), sign interfaces, sign languages. We briefly describe the activity approach to the design of interfaces and also some problems concerning the problem of mass interfaces. Finally, we discuss a number of problems arising from the increasing capabilities of modern computers. The work is in the nature of a popular science article and it largely reflects the subjective impressions of the author. © 2020 National Research Nuclear University. All rights reserved.","Computer graphics; Computer networks; Computer visualization; History of human-computer interaction; Natural interfaces; Personal computing","Biofeedback; Brain computer interface; Computer operating systems; Computer systems programming; Eye movements; Eye tracking; Haptic interfaces; Information systems; Interactive computer graphics; Interface states; Motion capture; Motion tracking; Personal computers; Personal computing; Problem oriented languages; Program debugging; Remote control; Speech recognition; Visualization; Computer visualization; Human computer interfaces; Information visualization; Programming by demon-stration; Revolutionary changes; Software visualization; Subjective impressions; Teaching programming; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85099121969
"Yang T., Chan A.B.","55470243200;14015159100;","Visual Tracking via Dynamic Memory Networks",2021,"IEEE Transactions on Pattern Analysis and Machine Intelligence","43","1","8770289","360","374",,9,"10.1109/TPAMI.2019.2929034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097570841&doi=10.1109%2fTPAMI.2019.2929034&partnerID=40&md5=490eb45d9eac7a820a29dfb5a0a543fe","Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong","Yang, T., Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; Chan, A.B., Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong","Template-matching methods for visual tracking have gained popularity recently due to their good performance and fast speed. However, they lack effective ways to adapt to changes in the target object's appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adapt the template to the target's appearance variations during tracking. The reading and writing process of the external memory is controlled by an LSTM network with the search feature map as input. A spatial attention mechanism is applied to concentrate the LSTM input on the potential target as the location of the target is at first unknown. To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template. In order to alleviate the drift problem, we also design a 'negative' memory unit that stores templates for distractors, which are used to cancel out wrong responses from the object template. To further boost the tracking performance, an auxiliary classification loss is added after the feature extractor part. Unlike tracking-by-detection methods where the object's information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target's appearance changes by updating the external memory. Moreover, the capacity of our model is not determined by the network size as with other trackers - the capacity can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Extensive experiments on the OTB and VOT datasets demonstrate that our trackers perform favorably against state-of-the-art tracking methods while retaining real-time speed. © 1979-2012 IEEE.","distractor template canceling; Dynamic memory networks; gated residual template learning; spatial attention","Long short-term memory; Object detection; Target tracking; Template matching; Memory requirements; Object information; Potential targets; Template learning; Template matching method; Tracking by detections; Tracking performance; Weight parameters; Object tracking; article; attention; eye tracking; human; human experiment; learning; memory; velocity; writing",Article,"Final","",Scopus,2-s2.0-85097570841
"Wang W., Shen J., Xie J., Cheng M.-M., Ling H., Borji A.","56103221800;13605783600;56029337700;57200622074;57191091290;23395793600;","Revisiting Video Saliency Prediction in the Deep Learning Era",2021,"IEEE Transactions on Pattern Analysis and Machine Intelligence","43","1","8744328","220","237",,44,"10.1109/TPAMI.2019.2924417","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097570489&doi=10.1109%2fTPAMI.2019.2924417&partnerID=40&md5=912068e0696c009c54e29c5c22e280e9","Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Hikvision Research Institute, Industry, CA, United States; College of Computer Science, Nankai University, Nankai, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, United States; MarkableAI, New York, NY  11201, United States","Wang, W., Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Shen, J., Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing, China; Xie, J., Hikvision Research Institute, Industry, CA, United States; Cheng, M.-M., College of Computer Science, Nankai University, Nankai, China; Ling, H., Department of Computer and Information Sciences, Temple University, Philadelphia, PA, United States; Borji, A., MarkableAI, New York, NY  11201, United States","Predicting where people look in static scenes, a.k.a visual saliency, has received significant research interest recently. However, relatively less effort has been spent in understanding and modeling visual attention over dynamic scenes. This work makes three contributions to video saliency research. First, we introduce a new benchmark, called DHF1K (Dynamic Human Fixation 1K), for predicting fixations during dynamic scene free-viewing, which is a long-time need in this field. DHF1K consists of 1K high-quality elaborately-selected video sequences annotated by 17 observers using an eye tracker device. The videos span a wide range of scenes, motions, object types and backgrounds. Second, we propose a novel video saliency model, called ACLNet (Attentive CNN-LSTM Network), that augments the CNN-LSTM architecture with a supervised attention mechanism to enable fast end-to-end saliency learning. The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning a more flexible temporal saliency representation across successive frames. Such a design fully leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance. Third, we perform an extensive evaluation of the state-of-the-art saliency models on three datasets : DHF1K, Hollywood-2, and UCF sports. An attribute-based analysis of previous saliency models and cross-dataset generalization are also presented. Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that ACLNet outperforms other contenders and has a fast processing speed (40 fps using a single GPU). Our code and all the results are available at https://github.com/wenguanwang/DHF1K. © 1979-2012 IEEE.","benchmark; deep learning; dynamic visual attention; Video saliency","Behavioral research; Dynamics; Eye tracking; Forecasting; Large dataset; Long short-term memory; Object recognition; Attention mechanisms; Research interests; Saliency representation; State of the art; Testing performance; Training efficiency; Video saliencies; Visual Attention; Deep learning",Article,"Final","",Scopus,2-s2.0-85097570489
"Wang Z., Chai J., Xia S.","57211979182;7202678260;7202893266;","Realtime and Accurate 3D Eye Gaze Capture with DCNN-Based Iris and Pupil Segmentation",2021,"IEEE Transactions on Visualization and Computer Graphics","27","1","8818661","190","203",,2,"10.1109/TVCG.2019.2938165","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097348765&doi=10.1109%2fTVCG.2019.2938165&partnerID=40&md5=9db4e141aff2b762d10f930f048e167a","Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese, Academy of Sciences, Beijing, 100049, China; Texas Am University, College Station, TX  77843, United States","Wang, Z., Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China, University of Chinese, Academy of Sciences, Beijing, 100049, China; Chai, J., Texas Am University, College Station, TX  77843, United States; Xia, S., Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China, University of Chinese, Academy of Sciences, Beijing, 100049, China","This paper presents a realtime and accurate method for 3D eye gaze tracking with a monocular RGB camera. Our key idea is to train a deep convolutional neural network(DCNN) that automatically extracts the iris and pupil pixels of each eye from input images. To achieve this goal, we combine the power of Unet [1] and Squeezenet [2] to train an efficient convolutional neural network for pixel classification. In addition, we track the 3D eye gaze state in the Maximum A Posteriori (MAP) framework, which sequentially searches for the most likely state of the 3D eye gaze at each frame. When eye blinking occurs, the eye gaze tracker can obtain an inaccurate result. We further extend the convolutional neural network for eye close detection in order to improve the robustness and accuracy of the eye gaze tracker. Our system runs in realtime on desktop PCs and smart phones. We have evaluated our system on live videos and Internet videos, and our results demonstrate that the system is robust and accurate for various genders, races, lighting conditions, poses, shapes and facial expressions. A comparison against Wang et al. [3] shows that our method advances the state of the art in 3D eye tracking using a single RGB camera. © 1995-2012 IEEE.","3D eye gaze tracking; convolutional neural network; facial capture","Cameras; Convolution; Convolutional neural networks; Deep neural networks; Pixels; Smartphones; Eye close detections; Eye gaze trackers; Eye gaze tracking; Facial Expressions; Lighting conditions; Maximum a posteriori; Pixel classification; Pupil segmentation; Eye tracking",Article,"Final","",Scopus,2-s2.0-85097348765
"Pu S., Song Y., Ma C., Zhang H., Yang M.-H.","57194196194;55613370800;57203342753;57007784200;7404927015;","Learning recurrent memory activation networks for visual tracking",2021,"IEEE Transactions on Image Processing","30",,"9269487","725","738",,,"10.1109/TIP.2020.3038356","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097156907&doi=10.1109%2fTIP.2020.3038356&partnerID=40&md5=3b64af66da4f5e84ca6a8a16cd6bd25a","Tencent Ai Lab, Beijing, 100193, China; Tencent Ai Lab, Shenzhen, 518057, China; Ai Institute, Shanghai Jiao Tong University, Shanghai, 200240, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Computer Science and Engineering, University of California Merced, Merced, CA  95343, United States","Pu, S., Tencent Ai Lab, Beijing, 100193, China; Song, Y., Tencent Ai Lab, Shenzhen, 518057, China; Ma, C., Ai Institute, Shanghai Jiao Tong University, Shanghai, 200240, China; Zhang, H., School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, 100876, China; Yang, M.-H., Computer Science and Engineering, University of California Merced, Merced, CA  95343, United States","Facilitated by deep neural networks, numerous tracking methods have made significant advances. Existing deep trackers mainly utilize independent frames to model the target appearance, while paying less attention to its temporal coherence. In this paper, we propose a recurrent memory activation network (RMAN) to exploit the untapped temporal coherence of the target appearance for visual tracking. We build the RMAN on top of the long short-term memory network (LSTM) with an additional memory activation layer. Specifically, we first use the LSTM to model the temporal changes of the target appearance. Then we selectively activate the memory blocks via the activation layer to produce a temporally coherent representation. The recurrent memory activation layer enriches the target representations from independent frames and reduces the background interference through temporal consistency. The proposed RMAN is fully differentiable and can be optimized end-to-end. To facilitate network training, we propose a temporal coherence loss together with the original binary classification loss. Extensive experimental results on standard benchmarks demonstrate that our method performs favorably against the state-of-the-art approaches. © 1992-2012 IEEE.","recurrent memory activation; representation; temporal coherence; Visual tracking","Chemical activation; Deep learning; Deep neural networks; Target tracking; Binary classification; Coherent representations; Network training; Short term memory; State-of-the-art approach; Target representation; Temporal coherence; Temporal consistency; Long short-term memory; article; binary classification; eye tracking; long short term memory network",Article,"Final","",Scopus,2-s2.0-85097156907
"Naqvi R.A., Hussain D., Loh W.-K.","55975847900;57196185189;7102037423;","Artificial intelligence-based semantic segmentation of ocular regions for biometrics and healthcare applications",2021,"Computers, Materials and Continua","66","1",,"715","732",,4,"10.32604/cmc.2020.013249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096491110&doi=10.32604%2fcmc.2020.013249&partnerID=40&md5=db5d16f300603be75615fef21ab7d8e3","Department of Unmanned Vehicle Engineering, Sejong University, Seoul, 05006, South Korea; School of Computational Science, Korea Institute for Advanced Study (KIAS), Seoul, 02455, South Korea; Department of Software, Gachon University, Seongnam, 13120, South Korea","Naqvi, R.A., Department of Unmanned Vehicle Engineering, Sejong University, Seoul, 05006, South Korea; Hussain, D., School of Computational Science, Korea Institute for Advanced Study (KIAS), Seoul, 02455, South Korea; Loh, W.-K., Department of Software, Gachon University, Seongnam, 13120, South Korea","Multiple ocular region segmentation plays an important role in different applications such as biometrics, liveness detection, healthcare, and gaze estimation. Typically, segmentation techniques focus on a single region of the eye at a time. Despite the number of obvious advantages, very limited research has focused on multiple regions of the eye. Similarly, accurate segmentation of multiple eye regions is necessary in challenging scenarios involving blur, ghost effects low resolution, off-angles, and unusual glints. Currently, the available segmentation methods cannot address these constraints. In this paper, to address the accurate segmentation of multiple eye regions in unconstrainted scenarios, a lightweight outer residual encoder-decoder network suitable for various sensor images is proposed. The proposed method can determine the true boundaries of the eye regions from inferior-quality images using the high-frequency information flow from the outer residual encoder-decoder deep convolutional neural network (called ORED-Net). Moreover, the proposed ORED-Net model does not improve the performance based on the complexity, number of parameters or network depth. The proposed network is considerably lighter than previous state-of-theart models. Comprehensive experiments were performed, and optimal performance was achieved using SBVPI and UBIRIS.v2 datasets containing images of the eye region. The simulation results obtained using the proposed OREDNet, with the mean intersection over union score (mIoU) of 89.25 and 85.12 on the challenging SBVPI and UBIRIS.v2 datasets, respectively. © 2020 Tech Science Press. All rights reserved.","Biometric for healthcare; Deep learning; Ocular regions; Semantic segmentation; Sensors","Biometrics; Convolutional neural networks; Decoding; Deep neural networks; Health care; Semantics; Signal encoding; Health care application; High-frequency informations; Liveness detection; Optimal performance; Region segmentation; Segmentation methods; Segmentation techniques; Semantic segmentation; Image segmentation",Article,"Final","",Scopus,2-s2.0-85096491110
"Liu D., Peng X., Liu X., Li Y., Bao Y., Xu J., Bian X., Xue W., Qian D.","7410097184;57214524235;57221422548;57213610452;57218352130;57219599692;57219935934;57215009554;57188691661;","A real-time system using deep learning to detect and track ureteral orifices during urinary endoscopy",2021,"Computers in Biology and Medicine","128",,"104104","","",,,"10.1016/j.compbiomed.2020.104104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096200170&doi=10.1016%2fj.compbiomed.2020.104104&partnerID=40&md5=547f7def41187bb587999e5fc5aebb50","Department of Urology, Shanghai Punan Hospital of Pudong New District, Shanghai, 200215, China; School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China; Deepwise Artificial Intelligence Laboratory, Beijing, 100080, China; Department of Urology, Ren Ji Hospital Affiliated to Shanghai Jiao Tong University, School of Medicine, Shanghai, 200025, China; Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, 200240, China","Liu, D., Department of Urology, Shanghai Punan Hospital of Pudong New District, Shanghai, 200215, China; Peng, X., School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China; Liu, X., Deepwise Artificial Intelligence Laboratory, Beijing, 100080, China; Li, Y., Deepwise Artificial Intelligence Laboratory, Beijing, 100080, China; Bao, Y., School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China; Xu, J., School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China; Bian, X., Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, 200240, China; Xue, W., Department of Urology, Ren Ji Hospital Affiliated to Shanghai Jiao Tong University, School of Medicine, Shanghai, 200025, China; Qian, D., Institute of Medical Robotics, Shanghai Jiao Tong University, Shanghai, 200240, China","Background and objective: To automatically identify and locate various types and states of the ureteral orifice (UO) in real endoscopy scenarios, we developed and verified a real-time computer-aided UO detection and tracking system using an improved real-time deep convolutional neural network and a robust tracking algorithm. Methods: The single-shot multibox detector (SSD) was refined to perform the detection task. We trained both the SSD and Refined-SSD using 447 resectoscopy images with UO and tested them on 818 ureteroscopy images. We also evaluated the detection performance on endoscopy video frames, which comprised 892 resectoscopy frames and 1366 ureteroscopy frames. UOs could not be identified with certainty because sometimes they appeared on the screen in a closed state of peristaltic contraction. To mitigate this problem and mimic the inspection behavior of urologists, we integrated the SSD and Refined-SSD with five different tracking algorithms. Results: When tested on 818 ureteroscopy images, our proposed UO detection network, Refined-SSD, achieved an accuracy of 0.902. In the video sequence analysis, our detection model yielded test sensitivities of 0.840 and 0.922 on resectoscopy and ureteroscopy video frames, respectively. In addition, by testing Refined-SSD on 1366 ureteroscopy video frames, the sensitivity achieved a value of 0.922, and a lowest false positive per image of 0.049 was obtained. For UO tracking performance, our proposed UO detection and tracking system (Refined-SSD integrated with CSRT) performed the best overall. At an overlap threshold of 0.5, the success rate of our proposed UO detection and tracking system was greater than 0.95 on 17 resectoscopy video clips and achieved nearly 0.95 on 40 ureteroscopy video clips. Conclusions: We developed a deep learning system that could be used for detecting and tracking UOs in endoscopy scenarios in real time. This system can simultaneously maintain high accuracy. This approach has great potential to serve as an excellent learning and feedback system for trainees and new urologists in clinical settings. © 2020 Elsevier Ltd","Deep learning; Detection; Tracking; Ureteral orifice","Convolutional neural networks; Deep neural networks; Endoscopy; Interactive computer systems; Learning systems; Orifices; Real time systems; Tracking (position); Video cameras; Clinical settings; Detection and tracking; Detection networks; Detection performance; Real-time computer; Tracking algorithm; Tracking performance; Video sequence analysis; Deep learning; article; convolutional neural network; deep learning; eye tracking; feedback system; human; peristalsis; sequence analysis; ureteroscopy; urologist; videorecording",Article,"Final","",Scopus,2-s2.0-85096200170
"Félix I., Raposo C., Antunes M., Rodrigues P., Barreto J.P.","57219448190;55893962200;53163159600;56428557100;7004946748;","Towards markerless computer-aided surgery combining deep segmentation and geometric pose estimation: application in total knee arthroplasty",2021,"Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization","9","3",,"271","278",,1,"10.1080/21681163.2020.1835554","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092782860&doi=10.1080%2f21681163.2020.1835554&partnerID=40&md5=199ef07b5fc1f6bed7c122a693eeb82a","Faculty of Sciences and Technology, University of Coimbra, Coimbra, Portugal; Perceive 3D, Coimbra, Portugal","Félix, I., Faculty of Sciences and Technology, University of Coimbra, Coimbra, Portugal; Raposo, C., Perceive 3D, Coimbra, Portugal; Antunes, M., Perceive 3D, Coimbra, Portugal; Rodrigues, P., Faculty of Sciences and Technology, University of Coimbra, Coimbra, Portugal; Barreto, J.P., Faculty of Sciences and Technology, University of Coimbra, Coimbra, Portugal, Perceive 3D, Coimbra, Portugal","Total knee arthroplasty (TKA) is a surgical procedure performed in patients suffering from knee arthritis. The correct positioning of the implants is strongly related to multiple surgical variables that have a tremendous impact on the success of the surgery. Computer-based navigation systems have been investigated and developed in order to assist the surgeon in accurately controlling those surgical variables. The existing technologies are very costly, require additional bone incisions for fixing markers to be tracked, and these markers are usually bulky, interfering with the standard surgical flow. This work presents a markerless navigation system that supports the surgeon in accurately performing the TKA procedure. The proposed system uses a mobile RGB-D camera for replacing the existing optical tracking systems and does not require markers to be tracked. We combine an effective deep learning-based approach for accurately segmenting the bone surface with a robust geometry-based algorithm for registering the bones with pre-operative models. The favourable performance of our pipeline is achieved by (1) employing a semi-supervised labelling approach for generating training data from real TKA surgery data, (2) using effective data augmentation techniques for improving the generalisation capability and (3) using appropriate depth data cleaning strategies. The construction of this complete markerless registration prototype that generalises for unseen intra-operative data is non-obvious, and relevant insights and future research directions can be derived. The experimental results show encouraging performance for video-based TKA. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","deep learning; image segmentation; knee surgery; Markerless navigation; pose estimation","algorithm; article; bone; cleaning; computer assisted surgery; deep learning; eye tracking; geometry; human; image segmentation; incision; pipeline; surgeon; total knee arthroplasty; videorecording",Article,"Final","",Scopus,2-s2.0-85092782860
"Jiang L., Xu M., Wang Z., Sigal L.","57188642646;55703599800;24170127500;7103067039;","DeepVS2.0: A Saliency-Structured Deep Learning Method for Predicting Dynamic Visual Attention",2021,"International Journal of Computer Vision","129","1",,"203","224",,,"10.1007/s11263-020-01371-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089908260&doi=10.1007%2fs11263-020-01371-6&partnerID=40&md5=671598882cb063d959b89e98691c8715","School of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Computer Science, University of British Columbia, Vancouver, BC, Canada","Jiang, L., School of Electronic and Information Engineering, Beihang University, Beijing, China, Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; Xu, M., School of Electronic and Information Engineering, Beihang University, Beijing, China; Wang, Z., School of Electronic and Information Engineering, Beihang University, Beijing, China; Sigal, L., Department of Computer Science, University of British Columbia, Vancouver, BC, Canada","Deep neural networks (DNNs) have exhibited great success in image saliency prediction. However, few works apply DNNs to predict the saliency of generic videos. In this paper, we propose a novel DNN-based video saliency prediction method, called DeepVS2.0. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which provides sufficient data to train the DNN models for predicting video saliency. Through the statistical analysis of LEDOV, we find that human attention is normally attracted by objects, particularly moving objects or the moving parts of objects. Accordingly, we propose an object-to-motion convolutional neural network (OM-CNN) in DeepVS2.0 to learn spatio-temporal features for predicting the intra-frame saliency via exploring the information of both objectness and object motion. We further find from our database that human attention has a temporal correlation with a smooth saliency transition across video frames. Therefore, a saliency-structured convolutional long short-term memory network (SS-ConvLSTM) is developed in DeepVS2.0 to predict inter-frame saliency, using the extracted features of OM-CNN as the input. Moreover, the center-bias dropout and sparsity-weighted loss are embedded in SS-ConvLSTM, to consider the center-bias and sparsity of human attention maps. Finally, the experimental results show that our DeepVS2.0 method advances the state-of-the-art video saliency prediction. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional LSTM; Deep neural networks; Eye-tracking database; Saliency prediction; Video; Video database","Behavioral research; Convolution; Convolutional neural networks; Deep neural networks; Eye tracking; Forecasting; Learning systems; Image saliencies; Prediction methods; Short term memory; Spatio temporal features; State of the art; Temporal correlations; Video saliencies; Visual Attention; Deep learning",Article,"Final","",Scopus,2-s2.0-85089908260
"Shotton T., Kim J.H.","57218201731;55863428900;","Assessing Differences on Eye Fixations by Attention Levels in an Assembly Environment",2021,"Advances in Intelligent Systems and Computing","1201 AISC",,,"417","423",,2,"10.1007/978-3-030-51041-1_55","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088247173&doi=10.1007%2f978-3-030-51041-1_55&partnerID=40&md5=72238835ab2c4d35e921a41feffb14f4","Department of Industrial and Manufacturing Systems Engineering, University of Missouri, Columbia, United States","Shotton, T., Department of Industrial and Manufacturing Systems Engineering, University of Missouri, Columbia, United States; Kim, J.H., Department of Industrial and Manufacturing Systems Engineering, University of Missouri, Columbia, United States","The purpose of this study is to see how eye movement can be used to determine how much attention a person is paying to a task in an assembly setting. The study uses the Dikablis Eye Tracking Glasses to analyze differences in length and number of eye fixations. Two groups of participants; the high and low attention level groups were compared to understand differences between workers who give full attention and those who do not. The participants who were assigned in the low attention level group had to memorize some given numbers throughout their assembly task while the high attention level group only had to complete the assembly task. According to our results, three of the six areas of interest locations had significantly different eye fixation lengths and one of the six locations had a significantly different number of eye fixations. The findings show that analyzing eye fixation lengths could be a way to measure a worker’s attention level during an assembly task. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.","3D; AOI; Attention levels; Eye tracking; Fixations; Manufacturing","Ergonomics; Eye tracking; Assembly tasks; Attention level; Eye fixations; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85088247173
"Zhao Z., Nishi Y., Arima S.","57217114613;57208575322;24723398900;","Interaction effects of environment and defect features on human cognitions and skills in visual inspections",2021,"Smart Innovation, Systems and Technologies","189",,,"431","448",,,"10.1007/978-981-15-5784-2_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086305718&doi=10.1007%2f978-981-15-5784-2_35&partnerID=40&md5=307d93a39d21d4617407f42e59d74f10","University of Tsukuba, Ibaraki, Tsukuba, Japan","Zhao, Z., University of Tsukuba, Ibaraki, Tsukuba, Japan; Nishi, Y., University of Tsukuba, Ibaraki, Tsukuba, Japan; Arima, S., University of Tsukuba, Ibaraki, Tsukuba, Japan","This paper discussed the external environmental conditions and humans’ internal cognition relating to the visual inspection process of actual mass productions. In visual inspection, human perception and recognition are indispensable to the detection and pass-fail discrimination of the defects, because only those form the discipline of the inspection. First, the effects of the external environment on the defect detection rate were evaluated based on the experimental results. Both defects’ features and environmental factors such as the display luminance and defects’ contrast and size are significant for the peripheral visual inspection. Some of the main effects reported in previous studies were verified again, and new interaction effects and whole factors became clear by quantitative analysis. The second was experiments and analyses of human perception and cognition of actual visual inspection targets. A wearable eye-tracker was used to observe experts and a beginner. The visual inspections by the experts were highly efficient because of their skilled perception at first glance and discrimination based on various industrial knowledge in addition to the defect’s appearance. The experts could stably detect a tiny and low-contrast defect on product images including much disturbing stimulus, and it is thought that their sensitivity and resolution were improved based on “attention” because they answered with high confidence. On the other hand, under the same situation, the number of the beginner’s focal points much increases, and processing speed deteriorates remarkably. Finally, some suggestions were summarized based on the results of the first and second topics about the environment of the visual inspection to raise the efficiency of defect detection and pass-fail judgement. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2020.","Attention; Contrast; Defect features; Environment factors; Interaction effect; Luminance; Main effect; Peripheral vision; Visual inspection","Eye tracking; Image enhancement; Inspection; Intelligent systems; Defect detection; Display luminances; Environmental conditions; Environmental factors; External environments; Interaction effect; Processing speed; Visual inspection; Defects",Conference Paper,"Final","",Scopus,2-s2.0-85086305718
"Xu M., Chen F., Li L., Shen C., Lv P., Zhou B., Ji R.","55703591000;56939478900;57225065167;57206995261;36348724100;55823863000;23134935200;","Bio-Inspired Deep Attribute Learning towards Facial Aesthetic Prediction",2021,"IEEE Transactions on Affective Computing","12","1","8454803","227","238",,19,"10.1109/TAFFC.2018.2868651","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052889354&doi=10.1109%2fTAFFC.2018.2868651&partnerID=40&md5=0bdec4cf0d308df5dd6b82ed330f475c","Center for Interdisciplinary Information Science Research, Zhengzhou University, Zhengzhou, China; Department of Cognitive Science, School of Information Science and Engineering, Xiamen University, Xiamen, China","Xu, M., Center for Interdisciplinary Information Science Research, Zhengzhou University, Zhengzhou, China; Chen, F., Department of Cognitive Science, School of Information Science and Engineering, Xiamen University, Xiamen, China; Li, L., Center for Interdisciplinary Information Science Research, Zhengzhou University, Zhengzhou, China; Shen, C., Department of Cognitive Science, School of Information Science and Engineering, Xiamen University, Xiamen, China; Lv, P., Center for Interdisciplinary Information Science Research, Zhengzhou University, Zhengzhou, China; Zhou, B., Center for Interdisciplinary Information Science Research, Zhengzhou University, Zhengzhou, China; Ji, R., Department of Cognitive Science, School of Information Science and Engineering, Xiamen University, Xiamen, China","Computational prediction of facial aesthetics has attracted ever-increasing research focus, which has wide range of prospects in multimedia applications. The key challenge lies in extracting discriminative and perception-Aware features to characterize the facial beautifulness. To this end, the existing schemes simply adopt a direct feature mapping, which relies on handcraft-designed low-level features that cannot reflect human-level aesthetic perception. In this paper, we present a systematic framework towards designing biology-inspired, discriminative representation for facial aesthetic prediction. First, we design a group of biological experiments that adopt eye tracker to identify spatial regions of interest during the facial aesthetic judgments of subjects, which forms a Bio-inspired Facial Aesthetic Ontology (Bio-FAO) and is made public available. Second, we adopt the cutting-edge convolutional neural network to train a set of Bio-inspired Attribute features, termed Bio-AttriBank, which forms a mid-level interpretable representation corresponding to the aforementioned Bio-FAO. For a given image, the facial aesthetic prediction is then formulated as a classification problem over the Bio-AttriBank descriptor responses, which well bridges the affective gap, and provides explainable evidences on why/how a face is beautiful or not. We have carried out extensive experiments on both JAFFE and FaceWarehouse datasets, with comparisons to a set of state-of-The-Art and alternative approaches. Superior performance gains in the experiments have demonstrated the merits of the proposed scheme. © 2010-2012 IEEE.","aesthetic concept; bio-inspired attention; deep learning; Facial aesthetic","Biomimetics; Bridges; Convolutional neural networks; Detectors; Eye tracking; Feature extraction; Flow visualization; Forecasting; Learning systems; Ontology; Aesthetic concepts; Bio-inspired attention; Biological experiments; Computational predictions; Face; Facial aesthetic; Interpretable representation; Systematic framework; Deep learning",Article,"Final","",Scopus,2-s2.0-85052889354
"Fang W., Zhang K.","55435889400;57226713170;","Real-time object detection of retail products for eye tracking",2020,"2020 8th International Conference on Orange Technology, ICOT 2020",,,"9468806","","",,,"10.1109/ICOT51877.2020.9468806","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112435021&doi=10.1109%2fICOT51877.2020.9468806&partnerID=40&md5=7e73ede7f03d41d6713bf3c0a2b059e9","Suzhou Institute of Trade and Commerce, Information Technology Department, Suzhou, China","Fang, W., Suzhou Institute of Trade and Commerce, Information Technology Department, Suzhou, China; Zhang, K., Suzhou Institute of Trade and Commerce, Information Technology Department, Suzhou, China","Object detection is one important task in automatically analyzing eye tracking video data. This paper presents a real-time object detection method of retail products based on deep learning for eye tracking system. In the proposed approach, an eye tracking based Convolutional Neural Networks is constructed to obtain the feature of original images. Then, a weighted bounding box selection strategy based on gaze location is used for object detection. Besides, parameters are adjusted according to gaze location of eye tracking. Experimental results show that our method can achieve better accuracy for eye tracking than other existing methods in the detection of retail products. © 2020 IEEE.","Convolutional Neural Networks; Eye Tracking; Gaze Location; Object Detection","Citrus fruits; Convolutional neural networks; Deep learning; Object detection; Object recognition; Object tracking; Sales; Bounding box; Eye tracking systems; Object detection method; Original images; Real time; Video data; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85112435021
"Ruensuk M., Cheon E., Hong H., Oakley I.","57191225329;57217076548;34881804800;6602434341;","How do you feel online? Exploiting smartphone sensors to detect transitory emotions during social media use",2020,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","4","4","3432223","","",,,"10.1145/3432223","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098160008&doi=10.1145%2f3432223&partnerID=40&md5=0081b93593ee364c0b59be7c4d1f036f","Unist, Ulsan, South Korea; Seoul National University, Seoul, South Korea","Ruensuk, M., Unist, Ulsan, South Korea; Cheon, E., Unist, Ulsan, South Korea; Hong, H., Seoul National University, Seoul, South Korea; Oakley, I., Unist, Ulsan, South Korea","Emotions are an intrinsic part of the social media user experience that can evoke negative behaviors such as cyberbullying and trolling. Detecting the emotions of social media users may enable responding to and mitigating these problems. Prior work suggests this may be achievable on smartphones: emotions can be detected via built-in sensors during prolonged input tasks. We extend these ideas to a social media context featuring sparse input interleaved with more passive browsing and media consumption activities. To achieve this, we present two studies. In the first, we elicit participant's emotions using images and videos and capture sensor data from a mobile device, including data from a novel passive sensor: its built-in eye-Tracker. Using this data, we construct machine learning models that predict self-reported binary affect, achieving 93.20% peak accuracy. A follow-up study extends these results to a more ecologically valid scenario in which participants browse their social media feeds the study yields high accuracies for both self-reported binary valence (94.16%) and arousal (92.28%). We present a discussion of the sensors, features and study design choices that contribute to this high performance and that future designers and researchers can use to create effective and accurate smartphone-based affect detection systems. © 2020 ACM.","affective computing; classification; emotion detection; smartphones; social media","Eye tracking; Smartphones; Social networking (online); User experience; Affect detection; Built-in sensors; Cyber bullying; Follow-up Studies; Machine learning models; Media consumption; Passive sensor; Study design; Sentiment analysis",Article,"Final","",Scopus,2-s2.0-85098160008
"Anden R., Linstead E.","57208584465;16307496400;","Predicting eye movement and fixation patterns on scenic images using Machine Learning for Children with Autism Spectrum Disorder",2020,"Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020",,,"9313278","2563","2569",,1,"10.1109/BIBM49941.2020.9313278","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100357264&doi=10.1109%2fBIBM49941.2020.9313278&partnerID=40&md5=2f63de8dc905f037a9d41713d899789b","Chapman University, Schmid College of Science and Technology, Machine Learning and Assistive Technology (MLAT) Lab, Orange, CA, United States","Anden, R., Chapman University, Schmid College of Science and Technology, Machine Learning and Assistive Technology (MLAT) Lab, Orange, CA, United States; Linstead, E., Chapman University, Schmid College of Science and Technology, Machine Learning and Assistive Technology (MLAT) Lab, Orange, CA, United States","This study uses eye-tracking experiment data to predict the fixation points for children with Autism Spectrum Disorder (ASD) and Typically Developing (TD) for 14 ASD and 14 TD subjects for 300 scenic images [1]. Based on explanatory Logistic Regression models, it is evident that fixation patterns for both ASD and TD subjects focus on the center of each scenic image. Using gradient boosting the researchers successfully identify 31.7% and 39.5% of all fixation points in the top decile of predicted fixation points for ASD and TD subjects respectively. Results conclude that TD subjects have less variability in their eye movement and fixation points leading to increased accuracy in predicting where they will look. © 2020 IEEE.","Autism Spectrum Disorder; Gradient Boosting; image recognition; Machine Learning; Random Forest; Supervised Learning","Bioinformatics; Diseases; Eye tracking; Forecasting; Logistic regression; Machine learning; Motion estimation; Children with autisms; Experiment data; Fixation point; Gradient boosting; Logistic regression models; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85100357264
"Jatmiko A.S., Ginalih C.T., Darmakusuma R.","57222130270;57222123275;35174204500;","Evaluation of Emotional Meaning of Eyelid Position on a 3D Animatronic Eyes",2020,"6th International Conference on Interactive Digital Media, ICIDM 2020",,,"9339629","","",,,"10.1109/ICIDM51048.2020.9339629","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101515729&doi=10.1109%2fICIDM51048.2020.9339629&partnerID=40&md5=95d36fa3554d1beb63fb1ff6673de007","Institut Teknologi Bandung, School of Electrical Engineering and Informatics, Bandung, Indonesia","Jatmiko, A.S., Institut Teknologi Bandung, School of Electrical Engineering and Informatics, Bandung, Indonesia; Ginalih, C.T., Institut Teknologi Bandung, School of Electrical Engineering and Informatics, Bandung, Indonesia; Darmakusuma, R., Institut Teknologi Bandung, School of Electrical Engineering and Informatics, Bandung, Indonesia","Emotion is a complex thing in humans, there are many ways to express emotions including eye gaze. Ekman, et al. describing abstractions regarding core features on human faces adopted by Onchi E, et al. through single-eyed 2D avatar that is designed that only move the upper and lower eyelids only. Adopting a 2D single-eye avatar, this study evaluated the similarity of human emotions expressed using 3D two-eye animatronic models with stiff eyelids shared by Will Cogley. Simulation in the form of a survey to evaluate the degree of similarity of the seven types of emotions described by Ekman, et al. i.e. neutral, happy, surprised, sad, scared, angry and disgusted, given to 40 participants with a variety of different backgrounds. The result is that the sample of emotions displayed had a significant effect on the participants' perceptions, each sample was also able to display the meaning of emotions well because there was a significant effect on the interaction between the sample and emotions (p = 5.63 × 10-17). Based on these results, the participants had almost similar perceptions of the eyelids with physical embodiment and virtual agents so that these results could be mutually reinforcing. Compared with the results of research using Probo, emotions can be expressed better when facial features such as eyebrows, eyelids and mouth are present. The conclusion of this research is expected to be a step to find out the function of each facial feature that contributes to express emotions. © 2020 IEEE.","3D model; animatronic eyes; emotion; eye gaze","Digital storage; Human computer interaction; Three dimensional computer graphics; Core features; Degree of similarity; Express emotions; Eye-gaze; Facial feature; Human emotion; Human faces; Virtual agent; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85101515729
"Yarlagadda V., Koolagudi S.G., Kumar M V M., Donepudi S.","57222128414;23397105900;57222120082;57203576561;","Driver Drowsiness Detection Using Facial Parameters and RNNs with LSTM",2020,"2020 IEEE 17th India Council International Conference, INDICON 2020",,,"9342348","","",,,"10.1109/INDICON49873.2020.9342348","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101518691&doi=10.1109%2fINDICON49873.2020.9342348&partnerID=40&md5=99499259aa0c4ed56cc5b2f6407d1345","Vit Vellore, School of Cse, Tamil Nadu, 632 014, India","Yarlagadda, V., Vit Vellore, School of Cse, Tamil Nadu, 632 014, India; Koolagudi, S.G., Vit Vellore, School of Cse, Tamil Nadu, 632 014, India; Kumar M V, M., Vit Vellore, School of Cse, Tamil Nadu, 632 014, India; Donepudi, S., Vit Vellore, School of Cse, Tamil Nadu, 632 014, India","The drowsiness is an intermediate state between awake and sleep, in which the observation and analysis of a conductor is very small. The lack of concentration due to the driver fatigue is a major cause that leads to the high number of accidents. In this work, an effort has been put to detect the state of drowsiness using facial parameters obtained using facial points. Moreover, the parameters related to eye and mouth organs have also been extracted. Deep neural networks are outperforming when compared to many state-of-the art algorithms. Hence, recurrent neural networks (RNNs) and long short-term memory (LSTM) units are considered to estimate the drowsiness level of a driver. It is found that they are very appropriate in processing of sequential multimedia data. An accuracy of 97.25% is obtained with the proposed approach. © 2020 IEEE.","Computer vision; Deep learning; Driver drowsiness detection; Eye tracking; Face detection; Long-short term memory; Machine learning; Recurrent neural networks","Deep neural networks; Face recognition; Driver drowsiness; Driver fatigue; Intermediate state; Multimedia data; Recurrent neural network (RNNs); State-of-the-art algorithms; Long short-term memory",Conference Paper,"Final","",Scopus,2-s2.0-85101518691
"Prava Roy A., Kumar Koley S., Garain U.","57208867238;57222121533;6602234041;","Eyes speak out Mind: Deep models for Gaze-based Analysis of Bilingual and Monolingual Reading",2020,"2020 IEEE 17th India Council International Conference, INDICON 2020",,,"9342182","","",,,"10.1109/INDICON49873.2020.9342182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101518207&doi=10.1109%2fINDICON49873.2020.9342182&partnerID=40&md5=f8ef81c87410b13ffc01f671c6bed412","National Inst. of Technology, Dept. of Ece, Durgapur, India; Indian Statistical Institute, Comp. Vis. Patt. Recog. (CVPR) Unit, Kolkata, India; Indian Statistical Institute, Cvpr Unit and Centre for Aiml (CAIML), Kolkata, India","Prava Roy, A., National Inst. of Technology, Dept. of Ece, Durgapur, India; Kumar Koley, S., Indian Statistical Institute, Comp. Vis. Patt. Recog. (CVPR) Unit, Kolkata, India; Garain, U., Indian Statistical Institute, Cvpr Unit and Centre for Aiml (CAIML), Kolkata, India","This paper presents an approach that attempts to explore effects of bilingualism in reading by analysis of eye-gaze data. As readers are more comfortable in reading first language (L1) compared to second language (L2), their eyes move differently in these two cases. Being motivated by this hypothesis, a deep learning model is developed to predict whether a reader is reading first or second language by analyzing the reading behavior given by the eye-tracking data. Exposure to different languages simultaneously may influence the proficiency in native language reading and this matter is also investigated by using deep learning model which can differentiate monolingual readers from bilingual readers analyzing their eye movement during reading in their respective native language. Experiments are conducted on GECO Corpus [1] which provides eye-tracking data of 19 readers reading a novel in L1 and L2 along with eye-tracking data of several monolingual readers reading same text in their native language. Experimental results show that the proposed models are quite efficient in capturing the differences in eye gaze pattern for reading L1 and L2 for a reader as well as in identifying monolingual and bilingual readers by processing eye-gaze data. © 2020 IEEE.","Bilingual; Deep Learning; Eye-tracking Data; Feature attribution; First Language; Monolingual; Reading Behavior; Second Language","Data handling; Deep learning; Eye movements; Learning systems; Eye-gaze; Learning models; Native language; Second language; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85101518207
"Ahsan Z., Obaidellah U.","57222010346;25646168000;","Predicting expertise among novice programmers with prior knowledge on programming tasks",2020,"2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2020 - Proceedings",,,"9306263","1008","1016",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100945995&partnerID=40&md5=eb461fc88f3277273a90880d034add81","University Malaya, Faculty of Computer Science and Information Technology, Dept of Artificial Intelligence, Kuala Lumpur, Malaysia","Ahsan, Z., University Malaya, Faculty of Computer Science and Information Technology, Dept of Artificial Intelligence, Kuala Lumpur, Malaysia; Obaidellah, U., University Malaya, Faculty of Computer Science and Information Technology, Dept of Artificial Intelligence, Kuala Lumpur, Malaysia","The studies on program comprehension have seen developments over the years from the cognitive science perspective. As eye-tracking technology has proven to analyze visual attention and gaze-performance, it has then been largely used in the program comprehension studies to help understand the underlying cognitive processes among the participants. In this research work, we conducted an experiment using common fundamental programming questions on 66 undergraduate computer science students to study the gaze-behavior among the high and low-performing participants on programming comprehension. We aim to better understand the differences in the time taken by the individuals in terms of their performance with existing prior knowledge and use machine learning to predict their expertise. Findings from this study suggest that mental schemas do play a role as the high performers demonstrated less time taken to attempt the questions than the low performers and machine learning algorithms were able to successfully predict their expertise. The conclusions drawn are supported by eye-tracking metrics across individual- and group-levels. © 2020 APSIPA.","classification; eye-tracking; machine learning; novice programmers; problem-solving","Behavioral research; Eye tracking; Forecasting; Learning algorithms; Machine learning; Students; Cognitive process; Cognitive science; Computer science students; Eye tracking technologies; Novice programmer; Program comprehension; Programming tasks; Visual Attention; Computer programming",Conference Paper,"Final","",Scopus,2-s2.0-85100945995
"Melesse D., Khalil M., Kagabo E., Ning T., Huang K.","57207925565;57207939901;57221800260;7101956756;41761666000;","Appearance-Based Gaze Tracking through Supervised Machine Learning",2020,"International Conference on Signal Processing Proceedings, ICSP","2020-December",,"9321075","467","471",,,"10.1109/ICSP48669.2020.9321075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100262989&doi=10.1109%2fICSP48669.2020.9321075&partnerID=40&md5=afb7af6471358038badbf941ae4e9a41","Trinity College, Department of Engineering, Hartford, CT, United States","Melesse, D., Trinity College, Department of Engineering, Hartford, CT, United States; Khalil, M., Trinity College, Department of Engineering, Hartford, CT, United States; Kagabo, E., Trinity College, Department of Engineering, Hartford, CT, United States; Ning, T., Trinity College, Department of Engineering, Hartford, CT, United States; Huang, K., Trinity College, Department of Engineering, Hartford, CT, United States","Applications that use human gaze have become increasingly more popular in the domain of human-computer interfaces, and advances in eye gaze tracking technology over the past few decades have led to the development of promising gaze estimation techniques. In this paper, a low-cost, in-house video camera-based gaze tracking system was developed, trained and evaluated. Seminal gaze detection methods constrained the application space to indoor conditions, and in most cases techniques required intrusive hardware. More modern gaze detection techniques try to eliminate the use of any additional hardware to reduce monetary cost as well as undue burden to the user, all the while maintaining accuracy of detection. In this work, image acquisition was achieved using a low-cost USB web camera mounted at a fixed position on the viewing screen or laptop. In order to determine the point of gaze, the Viola Jones face detection algorithm is used to extract facial features from the image frame. The gaze is then calculated using image processing techniques to extract gaze features, namely related to the image position of the pupil. Thousands of images are classified and labeled to form an in-house database. A multi-class Support Vector Machine (SVM) was trained and tested on this data set to distinguish point of gaze from input face image. Cross validation was used to train the model. Confusion matrices, accuracy, precision, and recall are used to evaluate the performance of the classification model. Evaluation of the proposed appearance-based technique using two different kernel functions is also assessed in detail. 2020 IEEE.","automatic gaze tracking; face detection; human computer interface; support vector machine","Computer hardware; Costs; Face recognition; Support vector machines; Video cameras; Classification models; Confusion matrices; Face detection algorithm; Gaze tracking system; Human computer interfaces; Image processing technique; Multi-class support vector machines; Supervised machine learning; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85100262989
"Zhang Y., Yang X., Ma Z.","57219185145;55683790100;57219766050;","Driver's Gaze Zone Estimation Method: A Four-channel Convolutional Neural Network Model",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"20","24",,,"10.1145/3440054.3440058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100661674&doi=10.1145%2f3440054.3440058&partnerID=40&md5=ce2974e73c0d52d726a184348e421734","The University of Jinan, China; The University of Jinan","Zhang, Y., The University of Jinan, China; Yang, X., The University of Jinan, China; Ma, Z., The University of Jinan","Driver's gaze has become an important indicator to analysis driving state. By estimating the gaze zone of drivers, we can further judge their fatigue state and even predict their driving intention in the next step. In this paper, we propose a four-channel gaze estimation model based on Convolutional Neural Network (CNN), which is used to estimate the gaze zones of the driver. In the proposed method, the images of the right eye, the left eye, the face, and the head are used as the input data of the multi-channel CNN. Then, the features of different channels are fused to estimate the gaze zone. Finally, we compared our method with several existing methods, and the experimental results show that the accuracy of our method is 96%. © 2020 ACM.","convolution neural network; deep learning; driver gaze zone estimation; eye tracking; Gaze estimation","Big data; Convolution; Intelligent computing; Driving state; Estimation methods; Four-channel; Gaze estimation; Input datas; Multi channel; Convolutional neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85100661674
"Li S., Wang J., Meng X., Ji B.","57267154200;57268429200;57268007900;57267373400;","A Contrastive Analysis on Effect Evaluation of 3D-to-2D Rendering Technologies for Animated Characters",2020,"Proceedings - 2020 International Conference on Innovation Design and Digital Technology, ICIDDT 2020",,,,"387","392",,,"10.1109/ICIDDT52279.2020.00077","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115369398&doi=10.1109%2fICIDDT52279.2020.00077&partnerID=40&md5=618c0aafae7e793d0f25c66a025c9f86","Art School of Jiangsu University, Department of Digital Media and Animation, Zhenjiang, China; Nanjing Vocational College of Information Technology, Nanjing, China","Li, S., Art School of Jiangsu University, Department of Digital Media and Animation, Zhenjiang, China; Wang, J., Nanjing Vocational College of Information Technology, Nanjing, China; Meng, X., Art School of Jiangsu University, Department of Digital Media and Animation, Zhenjiang, China; Ji, B., Art School of Jiangsu University, Department of Digital Media and Animation, Zhenjiang, China","With the rapid development of computer technologies and arts, the 3D-to-2D rendering technologies have been extensively applied to the animation industry. This thesis makes a contrastive analysis on the applications and their effects of two 3D-to-2D rendering technologies in the performance of animated characters. The eye movement data of the subjects were recorded by Tobii Pro XL eye tracker, and the subjective evaluation scores of the two kinds of 3D-to-2D rendering images of animated characters, the characteristics of eye movement data and the relationship between the two were analyzed. The results showed that: there were significant differences in subjective evaluation scores between the two technical schemes; subjective evaluation scores were closely related to the concentration trend of fixation points; the scores of each factor were significantly different; eye movement data such as fixation time and fixation points in the region of interest were significantly correlated with subjective scoring behavior of subjects. These subjective and objective indicators can be integrated to promote effect evaluation more objectively and guide the creation direction of 3D-to-2D rendering technical modeling of animated characters. © 2020 IEEE.","3D-to-2D rendering technology; contrastive analysis; effect evaluation; eye movement experiment","Animation; Eye movements; Image segmentation; Rendering (computer graphics); Three dimensional computer graphics; 3d-to-2d rendering technology; Animated characters; Computer art; Computer technology; Contrastive analysis; Effect evaluation; Eye movement datum; Eye movement experiment; Fixation point; Subjective evaluations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85115369398
"Delvigne V., Wannous H., Vandeborre J.-P., Ris L., Dutoit T.","57219434009;23391125300;6507497277;6602720245;36022249200;","Attention Estimation in Virtual Reality with EEG based Image Regression",2020,"Proceedings - 2020 IEEE International Conference on Artificial Intelligence and Virtual Reality, AIVR 2020",,,"9319046","10","16",,,"10.1109/AIVR50618.2020.00012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100002706&doi=10.1109%2fAIVR50618.2020.00012&partnerID=40&md5=5df4cb7f8d0060642d7143ae3d7ba48d","University of Mons, Isia Lab, Faculty of Engineering, Mons, Belgium; Imt Lille Douai, CRIStAL, Umr, Cnrs 9189, Villeneuve d'Ascq, France; University of Mons, Neuroscience Department Lab, Faculty of Medicine and Pharmacy, Mons, Belgium","Delvigne, V., University of Mons, Isia Lab, Faculty of Engineering, Mons, Belgium, Imt Lille Douai, CRIStAL, Umr, Cnrs 9189, Villeneuve d'Ascq, France; Wannous, H., Imt Lille Douai, CRIStAL, Umr, Cnrs 9189, Villeneuve d'Ascq, France; Vandeborre, J.-P., Imt Lille Douai, CRIStAL, Umr, Cnrs 9189, Villeneuve d'Ascq, France; Ris, L., University of Mons, Neuroscience Department Lab, Faculty of Medicine and Pharmacy, Mons, Belgium; Dutoit, T., University of Mons, Isia Lab, Faculty of Engineering, Mons, Belgium","Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental disorder affecting a certain amount of children and their way of living. A novel method to treat this disorder is to use Brain-Computer Interfaces (BCI) throughout the patient learns to self-regulate his symptoms by herself. In this context, researches have led to tools aiming to estimate the attention toward these interfaces. In parallel, the democratization of virtual reality (VR) headset, and the fact that it produces valid environments for several aspects: safe, flexible and ecologically valid have led to an increase of its use for BCI application. Another point is that Artificial Intelligence (AI) is more and more developed in different domain among which medical application. In this paper, we present an innovative method aiming to estimate attention from the measurement of physiological signals: Electroencephalogram (EEG), gaze direction and head movement. This framework is developed to assess attention in VR environments. We propose a novel approach for feature extraction and a dedicated Machine Learning model. The pilot study has been applied on a set of volunteer and our approach presents a lower error rate in comparison with the state of the art methods. © 2020 IEEE.","Brain-Compute Interface; Eye-tracking; Machine Learning; Virtual Reality","Artificial intelligence; Brain computer interface; Electroencephalography; Medical applications; Medical computing; Attention deficit hyperactivity disorder; Attention estimations; Different domains; Electro-encephalogram (EEG); Innovative method; Machine learning models; Physiological signals; State-of-the-art methods; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85100002706
"Elbattah M., Guerin J.-L., Carette R., Cilia F., Dequen G.","57163770900;57200857001;57200860085;57200855464;23396657900;","NLP-Based Approach to Detect Autism Spectrum Disorder in Saccadic Eye Movement",2020,"2020 IEEE Symposium Series on Computational Intelligence, SSCI 2020",,,"9308238","1581","1587",,1,"10.1109/SSCI47803.2020.9308238","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099697871&doi=10.1109%2fSSCI47803.2020.9308238&partnerID=40&md5=16fd4afb4a2792397be6052d68c00b8e","Université de Picardie Jules Verne, Laboratoire Mis, Amiens, France; Evolucare Technologies, Villers-Bretonneux, France","Elbattah, M., Université de Picardie Jules Verne, Laboratoire Mis, Amiens, France; Guerin, J.-L., Université de Picardie Jules Verne, Laboratoire Mis, Amiens, France; Carette, R., Evolucare Technologies, Villers-Bretonneux, France; Cilia, F., Université de Picardie Jules Verne, Laboratoire Mis, Amiens, France; Dequen, G., Université de Picardie Jules Verne, Laboratoire Mis, Amiens, France","Autism Spectrum Disorder (ASD) is a lifelong condition generally characterized by social and communication impairments. The early diagnosis of ASD is highly desirable, yet it could be complicated by several factors. Standard tests typically require intensive efforts and experience, which calls for developing assistive tools. In this respect, this study aims to develop a Machine Learning-based approach to assist the diagnosis process. Our approach is based on learning the sequence-based patterns in the saccadic eye movements. The key idea is to represent eye-tracking records as textual strings describing the sequences of fixations and saccades. As such, the study could borrow Natural Language Processing (NLP) methods for transforming the raw eye-tracking data. The NLP-based transformation could yield interesting features for training classification models. The experimental results demonstrated that such representation could be beneficial in this regard. With standard ConvNet models, our approach could realize a promising accuracy of classification (ROC-AUC up to 0.84). © 2020 IEEE.","Autism Spectrum Disorder; Eye-Tracking; Machine Learning; NLP; Sequence Modeling","Convolutional neural networks; Diagnosis; Diseases; Eye tracking; Intelligent computing; Metadata; Natural language processing systems; Turing machines; Accuracy of classifications; Assistive tool; Autism spectrum disorders; Classification models; Early diagnosis; NAtural language processing; Saccadic eye movements; Standard tests; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85099697871
"Mehta P., Malviya M., McComb C., Manogharan G., Berdanier C.G.P.","57214442586;57211927873;56400780700;56439976600;56301150400;","Mining design heuristics for additive manufacturing via eye-tracking methods and hidden Markov modeling",2020,"Journal of Mechanical Design, Transactions of the ASME","142","12","124502","","",,3,"10.1115/1.4048410","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099381285&doi=10.1115%2f1.4048410&partnerID=40&md5=ed976e678a21e245c9e6c5ace7bba603","Additive Manufacturing and Design, The Pennsylvania State University, University Park, PA  16802, United States; Department of Mechanical Engineering, The Pennsylvania State University, University Park, PA  16802, United States; School of Engineering Design, Technology, and Professional Programs, The Pennsylvania State University, University Park, PA  16802, United States","Mehta, P., Additive Manufacturing and Design, The Pennsylvania State University, University Park, PA  16802, United States; Malviya, M., Department of Mechanical Engineering, The Pennsylvania State University, University Park, PA  16802, United States; McComb, C., School of Engineering Design, Technology, and Professional Programs, The Pennsylvania State University, University Park, PA  16802, United States; Manogharan, G., Department of Mechanical Engineering, The Pennsylvania State University, University Park, PA  16802, United States; Berdanier, C.G.P., Department of Mechanical Engineering, The Pennsylvania State University, University Park, PA  16802, United States","In this research, we collected eye-tracking data from nine engineering graduate students as they redesigned a traditionally manufactured part for additive manufacturing (AM). Final artifacts were assessed for manufacturability and quality of final design, and design behaviors were captured via the eye-tracking data. Statistical analysis of design behavior duration shows that participants with more than 3 years of industry experience spend significantly less time removing material and revising than those with less experience. Hidden Markov modeling (HMM) analysis of the design behaviors gives insight to the transitions between behaviors through which designers proceed. Findings show that high-performing designers proceeded through four behavioral states, smoothly transitioning between states. In contrast, low-performing designers roughly transitioned between states, with moderate transition probabilities back and forth between multiple states. Copyright © 2020 by ASME.","Design process; Design visualization; Machine learning","3D printers; Additives; Engineering education; Heuristic methods; Hidden Markov models; Industrial research; Students; Behavioral state; Design heuristics; Engineering graduate students; Eye tracking methods; Hidden markov modeling(HMM); Industry experience; Manufacturability; Transition probabilities; Eye tracking",Article,"Final","",Scopus,2-s2.0-85099381285
"Hernandez-Matas C., Zabulis X., Argyros A.A.","56648186900;22952585700;7003791908;","REMPE: Registration of Retinal Images through Eye Modelling and Pose Estimation",2020,"IEEE Journal of Biomedical and Health Informatics","24","12","9055005","3362","3373",,3,"10.1109/JBHI.2020.2984483","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097571535&doi=10.1109%2fJBHI.2020.2984483&partnerID=40&md5=e7121ad252578321be3d9e264f1d17b2","Foundation for Research and Technology-Hellas, Heraklion, Greece","Hernandez-Matas, C., Foundation for Research and Technology-Hellas, Heraklion, Greece; Zabulis, X., Foundation for Research and Technology-Hellas, Heraklion, Greece; Argyros, A.A., Foundation for Research and Technology-Hellas, Heraklion, Greece","Objective: In-vivo assessment of small vessels can promote accurate diagnosis and monitoring of diseases related to vasculopathy, such as hypertension and diabetes. The eye provides a unique, open, and accessible window for directly imaging small vessels in the retina with non-invasive techniques, such as fundoscopy. In this context, accurate registration of retinal images is of paramount importance in the comparison of vessel measurements from original and follow-up examinations, which is required for monitoring the disease and its treatment. At the same time, retinal registration exhibits a range of challenges due to the curved shape of the retina and the modification of imaged tissue across examinations. Thereby, the objective is to improve the state-of-the-art in the accuracy of retinal image registration. Method: In this work, a registration framework that simultaneously estimates eye pose and shape is proposed. Corresponding points in the retinal images are utilized to solve the registration as a 3D pose estimation. Results: The proposed framework is evaluated quantitatively and shown to outperform state-of-the-art methods in retinal image registration for fundoscopy images. Conclusion: Retinal image registration methods based on eye modelling allow to perform more accurate registration than conventional methods. Significance: This is the first method to perform retinal image registration combined with eye modelling. The method improves the state-of-the-art in accuracy of retinal registration for fundoscopy images, quantitatively evaluated in benchmark datasets annotated with ground truth. The implementation of registration method has been made publicly available. © 2013 IEEE.","Image registration; medical image registration; medical imaging; retinal image registration; retinal imaging","Diagnosis; Image registration; Ophthalmology; 3D pose estimation; Accurate registration; Conventional methods; Noninvasive technique; Registration methods; Retinal image registrations; State-of-the-art methods; Vessel measurements; Image enhancement; accuracy; algorithm; Article; calibration; computer model; diagnostic imaging; eye tracking; geometry; histogram; human; image quality; image reconstruction; image registration; image segmentation; ophthalmoscopy; optical coherence tomography; prevalence; quantitative analysis; retina blood vessel; retina image; retinal pigment epithelium; task performance; training; visual acuity; body position; physiology; procedures; retina; three-dimensional imaging; Algorithms; Humans; Imaging, Three-Dimensional; Posture; Retina",Article,"Final","",Scopus,2-s2.0-85097571535
"Chesley B., Barbour D.L.","57220761647;7003532294;","Visual Field Estimation by Probabilistic Classification",2020,"IEEE Journal of Biomedical and Health Informatics","24","12","9106830","3499","3506",,2,"10.1109/JBHI.2020.2999567","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097571229&doi=10.1109%2fJBHI.2020.2999567&partnerID=40&md5=2c9ac33ebae08b4823d4966195945833","Department of Mathematics and Statistics, Washington University in St. Louis, Saint Louis, MO, United States; Department of Biomedical Engineering, Laboratory of Sensory Neuroscience and Neuroengineering, Washington University in St. Louis, Saint Louis, MO, United States","Chesley, B., Department of Mathematics and Statistics, Washington University in St. Louis, Saint Louis, MO, United States; Barbour, D.L., Department of Biomedical Engineering, Laboratory of Sensory Neuroscience and Neuroengineering, Washington University in St. Louis, Saint Louis, MO, United States","The gold standard clinical tool for evaluating visual dysfunction in cases of glaucoma and other disorders of vision remains the visual field or threshold perimetry exam. Administration of this exam has evolved over the years into a sophisticated, standardized, automated algorithm that relies heavily on specifics of disease processes particular to common retinal disorders. The purpose of this study is to evaluate the utility of a novel general estimator applied to visual field testing. A multidimensional psychometric function estimation tool was applied to visual field estimation. This tool is built on semiparametric probabilistic classification rather than multiple logistic regression. It combines the flexibility of nonparametric estimators and the efficiency of parametric estimators. Simulated visual fields were generated from human patients with a variety of diagnoses, and the errors between simulated ground truth and estimated visual fields were quantified. Error rates of the estimates were low, typically within 2 dB units of ground truth on average. The greatest threshold errors appeared to be confined to the portions of the threshold function with the highest spatial frequencies. This method can accurately estimate a variety of visual field profiles with continuous threshold estimates, potentially using a relatively small number of stimuli. © 2013 IEEE.","Active machine learning; diagnostics; psychophysics; retina; threshold perimetry; visual fields","Diagnosis; Errors; Function evaluation; Logistic regression; Ophthalmology; Automated algorithms; Multiple logistic regression; Nonparametric estimators; Probabilistic classification; Psychometric function; Spatial frequency; Threshold functions; Visual dysfunctions; Vision; accuracy; adult; aged; algorithm; Article; audiometry; classification algorithm; clinical article; computer assisted tomography; curriculum; entropy; eye fundus; eye movement; eye tracking; glaucoma; human; hypertension; learning; learning algorithm; machine learning; optic disk; perimetry; psychophysics; refraction error; retina; retina ganglion cell; retinal nerve fiber layer thickness; retinal pigment epithelium; scanning laser ophthalmoscopy; simulation; stimulus; visual field; intraocular hypertension; machine learning; middle aged; pathophysiology; physiology; procedures; psychophysics; statistical model; Adult; Aged; Algorithms; Humans; Machine Learning; Middle Aged; Models, Statistical; Ocular Hypertension; Psychophysics; Visual Field Tests; Visual Fields",Article,"Final","",Scopus,2-s2.0-85097571229
"Juang L.-H., Wu M.-N., Lin C.-H.","57144103600;57220200062;57220198111;","Affective computing study of attention recognition for the 3D guide system",2020,"CAAI Transactions on Intelligence Technology","5","4",,"268","275",,,"10.1049/trit.2020.0068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097212411&doi=10.1049%2ftrit.2020.0068&partnerID=40&md5=c49dbb65bc8d4e1f9a72eca19f4b52b3","School of Electrical Engineering and Automation, Xiamen University of Technology, No. 600, Ligong Road, Jimei, Xiamen, 361024, China","Juang, L.-H., School of Electrical Engineering and Automation, Xiamen University of Technology, No. 600, Ligong Road, Jimei, Xiamen, 361024, China; Wu, M.-N., School of Electrical Engineering and Automation, Xiamen University of Technology, No. 600, Ligong Road, Jimei, Xiamen, 361024, China; Lin, C.-H., School of Electrical Engineering and Automation, Xiamen University of Technology, No. 600, Ligong Road, Jimei, Xiamen, 361024, China","The eye-tracking has been widely used in multiple discipline studies in recent years. However, most of the studies focused on the analysis for static images or text but less for highly interactive operation application. In addition, the affective computing rose and development in recent years have changed completely the design of thinking pattern for the human-computer interaction. Therefore, this study hopes to integrate the affective computing into the 3D guide system which was developed for the real campus through the eye-tracking technology. The analysing user's gaze position and recognising attention emotion are according to the interest region which is stetted into the environment, and shows the feedback content corresponds to the area and the emotion. Through the most intuitive gaze analysis, the operation burden can be reduced and the user's interactive experience can be improved to achieve intuitive and user-friendly experience. The results can also apply for medical therapy on human attention training. © IET 2020. All rights reserved.",,"Human computer interaction; User experience; Affective Computing; Attention recognition; Eye tracking technologies; Human attention; Interactive operations; Interest regions; Medical therapy; Multiple disciplines; Eye tracking",Article,"Final","",Scopus,2-s2.0-85097212411
"Biener V., Schneider D., Gesslein T., Otte A., Kuth B., Kristensson P.O., Ofek E., Pahud M., Grubert J.","57204625703;57197779079;57210914914;57210911290;57211261370;6507412583;10139546600;6602493330;35114754100;","Breaking the Screen: Interaction across Touchscreen Boundaries in Virtual Reality for Mobile Knowledge Workers",2020,"IEEE Transactions on Visualization and Computer Graphics","26","12","9212653","3490","3502",,6,"10.1109/TVCG.2020.3023567","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095498999&doi=10.1109%2fTVCG.2020.3023567&partnerID=40&md5=e6ae27554d1ee15602728c2060ecf9b7","Coburg University of Applied Sciences, Germany; University of Cambridge, United Kingdom; Microsoft Research","Biener, V., Coburg University of Applied Sciences, Germany; Schneider, D., Coburg University of Applied Sciences, Germany; Gesslein, T., Coburg University of Applied Sciences, Germany; Otte, A., Coburg University of Applied Sciences, Germany; Kuth, B., Coburg University of Applied Sciences, Germany; Kristensson, P.O., University of Cambridge, United Kingdom; Ofek, E., Microsoft Research; Pahud, M., Microsoft Research; Grubert, J., Coburg University of Applied Sciences, Germany","Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations, such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices, such as tablets, limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces. In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR; 2) design and evaluate a set of interaction concepts; and 3) build example applications and gather user feedback on those applications. © 1995-2012 IEEE.","eye tracking; knowledge work; mobile office; multimodal interaction; virtual reality; window management","Helmet mounted displays; Knowledge management; Constrained input; Head mounted displays; Immersive VR; Interaction concepts; Knowledge work; Knowledge workers; Small inputs; User feedback; Virtual reality",Article,"Final","",Scopus,2-s2.0-85095498999
"Odekhe R., Cao Q., Jing S.M.","57208510928;7202487579;57219419578;","Gaze Teleoperation of a Surgical Robot Endoscope for Minimal Invasive Surgery",2020,"ICARM 2020 - 2020 5th IEEE International Conference on Advanced Robotics and Mechatronics",,,"9195348","163","165",,,"10.1109/ICARM49381.2020.9195348","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092632730&doi=10.1109%2fICARM49381.2020.9195348&partnerID=40&md5=be7e00ff6af142503c9dd8069e0f8e74","Intelligent Robotics Lab, Shanghai Jiaotong University, Shanghai, 200240, China","Odekhe, R., Intelligent Robotics Lab, Shanghai Jiaotong University, Shanghai, 200240, China; Cao, Q., Intelligent Robotics Lab, Shanghai Jiaotong University, Shanghai, 200240, China; Jing, S.M., Intelligent Robotics Lab, Shanghai Jiaotong University, Shanghai, 200240, China","This paper presents an innovative approach for controlling an endoscope during robot assisted minimally invasive surgery through the gaze of the surgeon. This system incorporates an eye tracking device which captures the 2D gaze fixation of the surgeon. An event detection algorithm is used in discriminating the fixations and these are sent as UDP to an Arduino microcontroller device where it is used to control two servo motors. The endoscope is attached to the two servos which generates it motion in two different planes which corresponds to the target gaze coordinates of the endoscope fixation point. Experimental results show the effectiveness and robustness of the gaze-based system in intuitively controlling the endoscope. © 2020 IEEE.",,"Agricultural robots; Endoscopy; Eye tracking; Robotics; Robots; Surgical equipment; Transplantation (surgical); Different planes; Event detection algorithm; Eye tracking devices; Fixation point; Innovative approaches; Microcontroller devices; Minimal invasive surgery; Minimally invasive surgery; Robotic surgery",Conference Paper,"Final","",Scopus,2-s2.0-85092632730
"Li X., Liu Q., Fan N., Zhou Z., He Z., Jing X.-Y.","56386356300;57189387635;57188637976;57217077468;53363609300;7202420489;","Dual-regression model for visual tracking",2020,"Neural Networks","132",,,"364","374",,4,"10.1016/j.neunet.2020.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091632933&doi=10.1016%2fj.neunet.2020.09.011&partnerID=40&md5=5cb13dea2c0029c9069a418ba8176be8","School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; The State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China","Li, X., School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Liu, Q., School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Fan, N., School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Zhou, Z., School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; He, Z., School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Jing, X.-Y., The State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China","Existing regression based tracking methods built on correlation filter model or convolution model do not take both accuracy and robustness into account at the same time. In this paper, we propose a dual-regression framework comprising a discriminative fully convolutional module and a fine-grained correlation filter component for visual tracking. The convolutional module trained in a classification manner with hard negative mining ensures the discriminative ability of the proposed tracker, which facilitates the handling of several challenging problems, such as drastic deformation, distractors, and complicated backgrounds. The correlation filter component built on the shallow features with fine-grained features enables accurate localization. By fusing these two branches in a coarse-to-fine manner, the proposed dual-regression tracking framework achieves a robust and accurate tracking performance. Extensive experiments on the OTB2013, OTB2015, and VOT2015 datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods. © 2020 Elsevier Ltd","Full convolutional network; Object tracking; Regression tracking model","Convolution; Regression analysis; Accurate tracking; Convolution model; Correlation filters; Discriminative ability; Negative minings; Regression model; State-of-the-art methods; Visual Tracking; Filtration; accuracy; algorithm; Article; convolutional neural network; deep learning; eye tracking; human; priority journal; quantitative analysis; semantics; support vector machine; task performance; image processing; procedures; Algorithms; Image Processing, Computer-Assisted",Article,"Final","",Scopus,2-s2.0-85091632933
"Zheng L.J., Mountstephens J., Teo J.","57216398510;36915612500;57201882145;","Four-class emotion classification in virtual reality using pupillometry",2020,"Journal of Big Data","7","1","43","","",,5,"10.1186/s40537-020-00322-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087400515&doi=10.1186%2fs40537-020-00322-9&partnerID=40&md5=60cf1a17a02ca1f0d275d0f20d33cb25","Faculty of Computing and Informatics, Universiti Malaysia Sabah, Jalan UMS, Kota Kinabalu, Sabah  88400, Malaysia","Zheng, L.J., Faculty of Computing and Informatics, Universiti Malaysia Sabah, Jalan UMS, Kota Kinabalu, Sabah  88400, Malaysia; Mountstephens, J., Faculty of Computing and Informatics, Universiti Malaysia Sabah, Jalan UMS, Kota Kinabalu, Sabah  88400, Malaysia; Teo, J., Faculty of Computing and Informatics, Universiti Malaysia Sabah, Jalan UMS, Kota Kinabalu, Sabah  88400, Malaysia","Background: Emotion classification remains a challenging problem in affective computing. The large majority of emotion classification studies rely on electroencephalography (EEG) and/or electrocardiography (ECG) signals and only classifies the emotions into two or three classes. Moreover, the stimuli used in most emotion classification studies utilize either music or visual stimuli that are presented through conventional displays such as computer display screens or television screens. This study reports on a novel approach to recognizing emotions using pupillometry alone in the form of pupil diameter data to classify emotions into four distinct classes according to Russell’s Circumplex Model of Emotions, utilizing emotional stimuli that are presented in a virtual reality (VR) environment. The stimuli used in this experiment are 360° videos presented using a VR headset. Using an eye-tracker, pupil diameter is acquired as the sole classification feature. Three classifiers were used for the emotion classification which are Support Vector Machine (SVM), k-Nearest Neighbor (KNN), and Random Forest (RF). Findings: SVM achieved the best performance for the four-class intra-subject classification task at an average of 57.05% accuracy, which is more than twice the accuracy of a random classifier. Although the accuracy can still be significantly improved, this study reports on the first systematic study on the use of eye-tracking data alone without any other supplementary sensor modalities to perform human emotion classification and demonstrates that even with a single feature of pupil diameter alone, emotions could be classified into four distinct classes to a certain level of accuracy. Moreover, the best performance for recognizing a particular class was 70.83%, which was achieved by the KNN classifier for Quadrant 3 emotions. Conclusion: This study presents the first systematic investigation on the use of pupillometry as the sole feature to classify emotions into four distinct classes using VR stimuli. The ability to conduct emotion classification using pupil data alone represents a promising new approach to affective computing as new applications could be developed using readily-available webcams on laptops and other mobile devices that are equipped with cameras without the need for specialized and costly equipment such as EEG and/or ECG as the sensor modality. © 2020, The Author(s).","Emotion classification; Eye-tracking; Machine learning; Pupil diameter; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85087400515
"Sharma K., Giannakos M., Dillenbourg P.","55903734200;36462343600;8912010400;","Eye-tracking and artificial intelligence to enhance motivation and learning",2020,"Smart Learning Environments","7","1","13","","",,6,"10.1186/s40561-020-00122-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084034865&doi=10.1186%2fs40561-020-00122-x&partnerID=40&md5=eb86fcffa7c05cc87ef11df26336cfa5","Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway; Department of Computer Science, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","Sharma, K., Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway; Giannakos, M., Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway; Dillenbourg, P., Department of Computer Science, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","The interaction with the various learners in a Massive Open Online Course (MOOC) is often complex. Contemporary MOOC learning analytics relate with click-streams, keystrokes and other user-input variables. Such variables however, do not always capture users’ learning and behavior (e.g., passive video watching). In this paper, we present a study with 40 students who watched a MOOC lecture while their eye-movements were being recorded. We then proposed a method to define stimuli-based gaze variables that can be used for any kind of stimulus. The proposed stimuli-based gaze variables indicate students’ content-coverage (in space and time) and reading processes (area of interest based variables) and attention (i.e., with-me-ness), at the perceptual (following teacher’s deictic acts) and conceptual levels (following teacher discourse). In our experiment, we identified a significant mediation effect of the content coverage, reading patterns and the two levels of with-me-ness on the relation between students’ motivation and their learning performance. Such variables enable common measurements for the different kind of stimuli present in distinct MOOCs. Our long-term goal is to create student profiles based on their performance and learning strategy using stimuli-based gaze variables and to provide students gaze-aware feedback to improve overall learning process. One key ingredient in the process of achieving a high level of adaptation in providing gaze-aware feedback to the students is to use Artificial Intelligence (AI) algorithms for prediction of student performance from their behaviour. In this contribution, we also present a method combining state-of-the-art AI technique with the eye-tracking data to predict student performance. The results show that the student performance can be predicted with an error of less than 5%. © 2020, The Author(s).","Deep learning; Eye-tracking; Learning; Massive open online courses; MOOCs; Motivation; Multimodal analytics; Video based learning",,Article,"Final","",Scopus,2-s2.0-85084034865
"Lan G., Heit B., Scargill T., Gorlatova M.","57189242558;57220747706;57219272275;24775927000;","GazeGraph: Graph-based few-shot cognitive context sensing from human visual behavior",2020,"SenSys 2020 - Proceedings of the 2020 18th ACM Conference on Embedded Networked Sensor Systems",,,,"422","435",,1,"10.1145/3384419.3430774","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097524234&doi=10.1145%2f3384419.3430774&partnerID=40&md5=49ede4803830b1b298c151601c622781","Duke University, Durham, NC, United States","Lan, G., Duke University, Durham, NC, United States; Heit, B., Duke University, Durham, NC, United States; Scargill, T., Duke University, Durham, NC, United States; Gorlatova, M., Duke University, Durham, NC, United States","In this work, we present GazeGraph, a system that leverages human gazes as the sensing modality for cognitive context sensing. GazeGraph is a generalized framework that is compatible with different eye trackers and supports various gaze-based sensing applications. It ensures high sensing performance in the presence of heterogeneity of human visual behavior, and enables quick system adaptation to unseen sensing scenarios with few-shot instances. To achieve these capabilities, we introduce the spatial-temporal gaze graphs and the deep learning-based representation learning method to extract powerful and generalized features from the eye movements for context sensing. Furthermore, we develop a few-shot gaze graph learning module that adapts the 'learning to learn' concept from meta-learning to enable quick system adaptation in a data-efficient manner. Our evaluation demonstrates that GazeGraph outperforms the existing solutions in recognition accuracy by 45% on average over three datasets. Moreover, in few-shot learning scenarios, GazeGraph outperforms the transfer learning-based approach by 19% to 30%, while reducing the system adaptation time by 80%. © 2020 ACM.","cognitive context sensing; eye tracking; few-shot learning","Behavioral research; Deep learning; Embedded systems; Eye movements; Graphic methods; Transfer learning; Learning scenarios; Learning to learn; Learning-based approach; Recognition accuracy; Sensing applications; Sensing modalities; Sensing performance; Spatial temporals; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85097524234
"Bourguet M.-L., Xu M., Zhang S., Urakami J., Venture G.","6603288437;57216349825;57210743976;6603109617;15623574000;","The Impact of a Social Robot Public Speaker on Audience Attention",2020,"HAI 2020 - Proceedings of the 8th International Conference on Human-Agent Interaction",,,,"60","68",,1,"10.1145/3406499.3415073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096616233&doi=10.1145%2f3406499.3415073&partnerID=40&md5=4bc8f0e388280c1ef8803bdede9114a1","School of Electronic Eng. and Computer Sci., Queen Mary U. of London, London, United Kingdom; Beijing U. of Posts and Telecommunication, Beijing, China; School of Engineering, Tokyo Institute of Technology, Tokyo, Japan; GVLab, Tokyo U. of Agriculture and Technology, Tokyo, Japan","Bourguet, M.-L., School of Electronic Eng. and Computer Sci., Queen Mary U. of London, London, United Kingdom; Xu, M., Beijing U. of Posts and Telecommunication, Beijing, China; Zhang, S., Beijing U. of Posts and Telecommunication, Beijing, China; Urakami, J., School of Engineering, Tokyo Institute of Technology, Tokyo, Japan; Venture, G., GVLab, Tokyo U. of Agriculture and Technology, Tokyo, Japan","Social robots acting as stand-ins for speakers or teachers would enable them to reach large audiences from anywhere in the world, increasing the options for distant learning. They would need to be endowed with effective public speaking skills though, in order to deliver their message, entertain, and maintain audience attention. In this paper, we report two user studies to understand the impact of a social robot public speaker on its audience and compare it to a skilled human speaker. The first study uses an in-house audience attention monitoring system based on computer vision and machine learning; the second study uses eye tracking technology. For both studies, we programmed the social robot Pepper to deliver a short speech in its robotic voice while mimicking the behaviour of a skilled human speaker (gestures and body movements). We found that, when the audience has a genuine interest in the speech, the robot manages to maintain audience attention level as well as the human speaker but fails to arouse interest as much. When the audience is not particularly interested in the speech, the human speaker is better at maintaining audience attention: the novelty of the robot does not compensate for the lack of interest in the speech content, and the robot's behaviour is found to be distracting. Finally, understanding of the speech content is significantly lower for the robot audience. It could be linked to lower audience attention levels, to the robot's lack of facial expressions and failure to convey enthusiasm, or to a feeling that the robot is not legitimate to speak about the topics touched on in the speeches. © 2020 ACM.","audience attention; computer-vision; eye tracking; machine learning; public speaking; social robots","Computer vision; Distance education; Economic and social effects; Eye tracking; Speech; Attention level; Body movements; Distant learning; Eye tracking technologies; Facial Expressions; Monitoring system; Public speaking; Speech content; Social robots",Conference Paper,"Final","",Scopus,2-s2.0-85096616233
"Abdeltawab A., Ahmad A.","57204789984;55364133600;","Classification of motor imagery EEG signals using machine learning",2020,"2020 IEEE 10th International Conference on System Engineering and Technology, ICSET 2020 - Proceedings",,,"09265364","196","201",,,"10.1109/ICSET51301.2020.9265364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098271611&doi=10.1109%2fICSET51301.2020.9265364&partnerID=40&md5=3b0eb61dd9c02e2018e038530de3e530","School of Electrical Engineering, Faculty of Engineering, Universiti Teknologi Malaysia (UTM), Johor, Malaysia","Abdeltawab, A., School of Electrical Engineering, Faculty of Engineering, Universiti Teknologi Malaysia (UTM), Johor, Malaysia; Ahmad, A., School of Electrical Engineering, Faculty of Engineering, Universiti Teknologi Malaysia (UTM), Johor, Malaysia","Brain Computer Interface (BCI) is a term that was first introduced by Jacques Vidal in the 1970s when he created a system that can determine the human eye gaze direction, making the system able to determine the direction a person want to go or move something to using scalp-recorded visual evoked potential (VEP) over the visual cortex. Ever since that time, many researchers where captivated by the huge potential and list of possibilities that can be achieved if simply a digital machine can interpret human thoughts. In this work, we explore electroencephalography (EEG) signal classification, specifically for motor imagery (MI) tasks. Classification of MI tasks can be carried out by using machine learning and deep learning models, yet there is a trade between accuracy and computation time that needs to be maintained. The objective is to create a machine learning model that can be optimized for real-time classification while having a relatively acceptable classification accuracy. The proposed model relies on common spatial patter (CSP) for feature extraction as well as linear discriminant analysis (LDA) for classification. With simple pre-processing stage and a proper selection of data for training the model proved to have a balanced accuracy of +80% while maintaining small run-time (milliseconds) that is opted for real-time classifications. © 2020 IEEE","Common Spatial Pattern (CSP); EEG; Linear Discriminant Analysis (LDA); Machine learning; Motor Imagery Classification","Biomedical signal processing; Brain computer interface; Deep learning; Discriminant analysis; Electroencephalography; Electrophysiology; Eye movements; Image classification; Systems engineering; Turing machines; Classification accuracy; Computation time; Digital machines; Linear discriminant analysis; Machine learning models; Motor imagery eeg signals; Signal classification; Visual evoked potential; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85098271611
"Dueñas K., Naderi E., Kwon J.","57220103432;57209283547;57207223658;","Investigating the Influence of 2-D presentation versus 3-D rotation presentation formats on user perception",2020,"Proceedings - Web3D 2020: 25th ACM Conference on 3D Web Technology",,,"3424716","","",,,"10.1145/3424616.3424716","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096930756&doi=10.1145%2f3424616.3424716&partnerID=40&md5=d7f1c2de8ae5f44f7f2b528b4dccc1a9","Department of Design Housing and Apparel, University of Minnesota, United States; Samsung, South Korea","Dueñas, K., Department of Design Housing and Apparel, University of Minnesota, United States; Naderi, E., Department of Design Housing and Apparel, University of Minnesota, United States; Kwon, J., Samsung, South Korea","When presenting a product through digital media, both designers and retailers aim to communicate with their audience in the most effective way using visual perception. Previous studies have found that a users' perception and behavior are affected by the visual presentation. The purpose of this study is to discover how a users' perception and evaluation of a product change depending on how the product is presented. In this study, we specifically look at the presentation formats of 2 - D orthographic views and 3 - D simulated format (360 rotation). This study employs a between-subject design with surveys and an eye - tracking test to determine if 2 - D presentation or 3 - D presentation resulted in better user evaluation and higher approval to the product design. We found that users have a better understanding of product aesthetics, usefulness, and product ease of use when they were viewing 2 - D orthographic images of products versus when they were viewing the 3 - D 360 rotation presentation format. Future marketing, design, and theoretical implications as well as future research directions are discussed. © 2020 Owner/Author.","2D Presentation; 3D Presentation; Eye Tracking; Orthographic Views,; Presentation Format; Product Design; User Cognitive Response","Digital storage; Eye tracking; Web services; 3-D presentations; Future research directions; Orthographic image; Orthographic views; Presentation formats; Product aesthetics; Visual perception; Visual presentation; Product design",Conference Paper,"Final","",Scopus,2-s2.0-85096930756
"Li Y., Niu J., Cai Z.","57212459947;57222010812;7402905297;","Evaluating Focused Attention Level Using Mouse Operation Data",2020,"Proceedings - 2020 Chinese Automation Congress, CAC 2020",,,"9327066","3679","3684",,,"10.1109/CAC51589.2020.9327066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100944272&doi=10.1109%2fCAC51589.2020.9327066&partnerID=40&md5=97860c17cc97d08c275ebeb2d9bacf8f","Xi'an Jiaotong University, Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi'an, China","Li, Y., Xi'an Jiaotong University, Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi'an, China; Niu, J., Xi'an Jiaotong University, Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi'an, China; Cai, Z., Xi'an Jiaotong University, Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi'an, China","Focused Attention Level (FAL) is important in many tasks such as driving, online learning, and many computer-related operations. Previous approaches to evaluation of FAL usually involve eye-tracking and electroencephalogram which require dedicated devices to track the movements of eyes or record the signals from the brain. In this paper, we propose a novel method of evaluation of FAL only using a user's mouse operation data which are easily available in computers equipped with a mouse device. We perform our study using the Schulte Table which is a popular task designed to improve the focused attention levels of children and students. We extract features from mouse operation data in completing a Schulte Table task and establish a machine learning model to predict the focused attention level of the subject. Experimental results show that the accuracy of our method can reach 80.9%. © 2020 IEEE.","feature selection; focused attention level; machine learning; mouse operation; the Schulte Table","Eye movements; Eye tracking; Turing machines; Attention level; Machine learning models; Mouse operations; Online learning; Mammals",Conference Paper,"Final","",Scopus,2-s2.0-85100944272
"Bublea A., Caleanu C.D.","57221596015;14049898400;","Deep Learning based Eye Gaze Tracking for Automotive Applications: An Auto-Keras Approach",2020,"2020 14th International Symposium on Electronics and Telecommunications, ISETC 2020 - Conference Proceedings",,,"9301091","","",,1,"10.1109/ISETC50328.2020.9301091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099600654&doi=10.1109%2fISETC50328.2020.9301091&partnerID=40&md5=069911eeca07718f176b34859e8823bc","Politehnica University Timioara, Department of Applied Electronics Faculty of Electronics, Telecommunications and Information Technologies, Timisoara, Romania","Bublea, A., Politehnica University Timioara, Department of Applied Electronics Faculty of Electronics, Telecommunications and Information Technologies, Timisoara, Romania; Caleanu, C.D., Politehnica University Timioara, Department of Applied Electronics Faculty of Electronics, Telecommunications and Information Technologies, Timisoara, Romania","We propose a deep neural network-based gaze sensing method in which the design of the neural architecture is performed automatically, through a network architecture search algorithm called Auto-Keras. First, the neural model is generated using the Columbia Gaze Data Set. Then, the performance of the solution is estimated on an online scenario and proves the generalization ability of our model. In comparison to a geometrical approach, which uses dlib facial landmarks, filtering and morphological operators for gaze estimation, the proposed method provides superior results and certain advantages. © 2020 IEEE.","automotive; deep learning; eye gaze tracking","Deep neural networks; Eye tracking; Network architecture; Automotive applications; Eye gaze tracking; Generalization ability; Geometrical approaches; Morphological operator; Neural architectures; Neural modeling; Search Algorithms; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85099600654
"Famadas J., Madadi M., Palmero C., Escalera S.","57222114562;56524375700;57188829002;22634035000;","Generative Video Face Reenactment by AUs and Gaze Regularization",2020,"Proceedings - 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2020",,,"9320223","444","451",,,"10.1109/FG47880.2020.00125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101483294&doi=10.1109%2fFG47880.2020.00125&partnerID=40&md5=8012814bd38a67dd4537c483c8751f1b","Universitat de Barcelona, Department of Mathematics and Informatics, Barcelona, Spain; Universitat Autonòma de Barcelona, Computer Vision Center, Barcelona, Spain","Famadas, J., Universitat de Barcelona, Department of Mathematics and Informatics, Barcelona, Spain; Madadi, M., Universitat Autonòma de Barcelona, Computer Vision Center, Barcelona, Spain; Palmero, C., Universitat de Barcelona, Department of Mathematics and Informatics, Barcelona, Spain; Escalera, S., Universitat de Barcelona, Department of Mathematics and Informatics, Barcelona, Spain","In this work, we propose an encoder-decoder-like architecture to perform face reenactment in image sequences. Our goal is to transfer the training subject identity to a given test subject. We regularize face reenactment by facial action unit intensity and 3D gaze vector regression. This way, we enforce the network to transfer subtle facial expressions and eye dynamics, providing a more lifelike result. The proposed encoder-decoder receives as input the previous sequence frame stacked to the current frame image of facial landmarks. Thus, the generated frames benefit from appearance and geometry, while keeping temporal coherence for the generated sequence. At test stage, a new target subject with the facial performance of the source subject and the appearance of the training subject is reenacted. Principal component analysis is applied to project the test subject geometry to the closest training subject geometry before reenactment. Evaluation of our proposal shows faster convergence, and more accurate and realistic results in comparison to other architectures without action units and gaze regularization. © 2020 IEEE.","Eye gaze; Face alignment; Face reenactment; Facial expressions; Facial Synthesis","Decoding; Geometry; Network architecture; Signal encoding; 3D gaze vectors; Encoder-decoder; Facial Expressions; Facial landmark; Faster convergence; Generative video; Image sequence; Temporal coherence; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-85101483294
"Yvinec E., Dapogny A., Bailly K.","57219733813;56916151100;25633701200;","DeeSCo: Deep heterogeneous ensemble with Stochastic Combinatory loss for gaze estimation",2020,"Proceedings - 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2020",,,"9320196","146","152",,,"10.1109/FG47880.2020.00039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101476670&doi=10.1109%2fFG47880.2020.00039&partnerID=40&md5=b0e15487eb82e3f0f70685084c83a657","Datakalab, 114 Boulevard Malesherbes, Paris, 75017, France","Yvinec, E., Datakalab, 114 Boulevard Malesherbes, Paris, 75017, France; Dapogny, A., Datakalab, 114 Boulevard Malesherbes, Paris, 75017, France; Bailly, K., Datakalab, 114 Boulevard Malesherbes, Paris, 75017, France","From medical research to gaming applications, gaze estimation is becoming a valuable tool. While there exists a number of hardware-based solutions, recent deep learningbased approaches, coupled with the availability of large-scale databases, have allowed to provide a precise gaze estimate using only consumer sensors. However, there remains a number of questions, regarding the problem formulation, architectural choices and learning paradigms for designing gaze estimation systems in order to bridge the gap between geometry-based systems involving specific hardware and approaches using consumer sensors only. In this paper, we introduce a deep, end-toend trainable ensemble of heatmap-based weak predictors for 2D/3D gaze estimation. We show that, through heterogeneous architectural design of these weak predictors, we can improve the decorrelation between the latter predictors to design more robust deep ensemble models. Furthermore, we propose a stochastic combinatory loss that consists in randomly sampling combinations of weak predictors at train time. This allows to train better individual weak predictors, with lower correlation between them. This, in turns, allows to significantly enhance the performance of the deep ensemble. We show that our Deep heterogeneous ensemble with Stochastic Combinatory loss (DeeSCo) outperforms state-of-the-art approaches for 2D/3D gaze estimation on multiple datasets. © 2020 IEEE.","deep learning; Ensemble methods; Gaze estimation","Bridges; Gesture recognition; Gaming applications; Heterogeneous ensembles; Large-scale database; Learning paradigms; Learning-based approach; Multiple data sets; Problem formulation; State-of-the-art approach; Stochastic systems",Conference Paper,"Final","",Scopus,2-s2.0-85101476670
"Wang J., Liang H.-N., Monteiro D.V., Xu W., Chen H., Chen Q.","57207048907;8636386200;57144011000;57202312016;57221155309;57221157960;","Real-Time Detection of Simulator Sickness in Virtual Reality Games Based on Players' Psychophysiological Data during Gameplay",2020,"Adjunct Proceedings of the 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2020",,,"9288444","247","248",,1,"10.1109/ISMAR-Adjunct51615.2020.00071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099539370&doi=10.1109%2fISMAR-Adjunct51615.2020.00071&partnerID=40&md5=8b23144731e82415ecd8fa0612e4baab","Xi'an Jiaotong-Liverpool University, China","Wang, J., Xi'an Jiaotong-Liverpool University, China; Liang, H.-N., Xi'an Jiaotong-Liverpool University, China; Monteiro, D.V., Xi'an Jiaotong-Liverpool University, China; Xu, W., Xi'an Jiaotong-Liverpool University, China; Chen, H., Xi'an Jiaotong-Liverpool University, China; Chen, Q., Xi'an Jiaotong-Liverpool University, China","Virtual Reality (VR) technology has been proliferating in the last decade, especially in the last few years. However, Simulator Sickness (SS) still represents a significant problem for its wider adoption. Currently, the most common way to detect SS is using the Simulator Sickness Questionnaire (SSQ). SSQ is a subjective measurement and is inadequate for real-Time applications such as VR games. This research aims to investigate how to use machine learning techniques to detect SS based on in-game characters' and users' physiological data during gameplay in VR games. To achieve this, we designed an experiment to collect such data with three types of games. We trained a Long Short-Term Memory neural network with the dataset eye-Tracking and character movement data to detect SS in real-Time. Our results indicate that, in VR games, our model is an accurate and efficient way to detect SS in real-Time. © 2020 IEEE.","EEG; Eye-Tracking; Gaming; Machine Learning; Simulator Sickness; Virtual Reality","Augmented reality; Diseases; Eye movements; Eye tracking; Learning systems; Signal detection; Simulators; Machine learning techniques; Movement datum; Physiological data; Real time; Real-time application; Real-time detection; Simulator sickness; Subjective measurements; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85099539370
"Li R., Whitmire E., Stengel M., Boudaoud B., Kautz J., Luebke D., Patel S., Aksit K.","57207821076;55902853100;42162165500;57204188866;7006458237;57204337568;8450420300;57203835080;","Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors",2020,"Proceedings - 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2020",,,"9284794","117","126",,,"10.1109/ISMAR50242.2020.00033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099278346&doi=10.1109%2fISMAR50242.2020.00033&partnerID=40&md5=d3a308e0b818f7681b2c217abcc97d97","University of Washington, United States; Nvidia Research","Li, R., University of Washington, United States; Whitmire, E., University of Washington, United States; Stengel, M., Nvidia Research; Boudaoud, B., University of Washington, United States; Kautz, J., University of Washington, United States; Luebke, D., University of Washington, United States; Patel, S., University of Washington, United States; Aksit, K., University of Washington, United States","Gaze tracking is an essential component of next generation displays for virtual reality and augmented reality applications. Traditional camera-based gaze trackers used in next generation displays are known to be lacking in one or multiple of the following metrics: power consumption, cost, computational complexity, estimation accuracy, latency, and form-factor. We propose the use of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to traditional camera-based gaze tracking approaches while taking all of these metrics into consideration. We begin by developing a rendering-based simulation framework for understanding the relationship between light sources and a virtual model eyeball. Findings from this framework are used for the placement of LEDs and photodiodes. Our first prototype uses a neural network to obtain an average error rate of 2. 67^{\circ} at 400 Hz while demanding only 16 mW. By simplifying the implementation to using only LEDs, duplexed as light transceivers, and more minimal machine learning model, namely a light-weight supervised Gaussian process regression algorithm, we show that our second prototype is capable of an average error rate of 1. 57^{\circ} at 250 Hz using 800 mW. © 2020 IEEE.","Computer systems organization; Embedded and cyber-physical systems; Human-centered computing; Sensors and actuators; Ubiquitous and mobile computing; Ubiquitous and mobile devices","Augmented reality; Cameras; Light emitting diodes; Machine learning; Photodiodes; Augmented reality applications; Average errors; Gaussian process regression; Gaze tracking; Machine learning models; Simulation framework; Single pixel; Virtual modeling; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85099278346
"Taib R., Berkovsky S., Koprinska I., Wang E., Zeng Y., Li J.","17347338100;8945336100;6506616260;57209399486;57209396959;57205694120;","Personality Sensing: Detection of Personality Traits Using Physiological Responses to Image and Video Stimuli",2020,"ACM Transactions on Interactive Intelligent Systems","10","3","3357459","","",,,"10.1145/3357459","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097241147&doi=10.1145%2f3357459&partnerID=40&md5=7a22313602cb173228d2e89ad5cbcced","Data61, CSIRO, 13 Garden St, Eveleigh, NSW  2015, Australia; Australian Institute of Health Innovation, Macquarie University, 75 Talavera Rd, North Ryde, NSW  2113, Australia; School of Computer Science, University of SydneyNSW  2006, Australia; School of Psychology, University of SydneyNSW  2006, Australia; Wisconsin Embedded Systems and Computing Lab, University of Wisconsin-Madison, 4613 Engineering Dr, Madison, WI  536706, United States","Taib, R., Data61, CSIRO, 13 Garden St, Eveleigh, NSW  2015, Australia; Berkovsky, S., Australian Institute of Health Innovation, Macquarie University, 75 Talavera Rd, North Ryde, NSW  2113, Australia; Koprinska, I., School of Computer Science, University of SydneyNSW  2006, Australia; Wang, E., School of Computer Science, University of SydneyNSW  2006, Australia; Zeng, Y., School of Psychology, University of SydneyNSW  2006, Australia; Li, J., Wisconsin Embedded Systems and Computing Lab, University of Wisconsin-Madison, 4613 Engineering Dr, Madison, WI  536706, United States","Personality detection is an important task in psychology, as different personality traits are linked to different behaviours and real-life outcomes. Traditionally it involves filling out lengthy questionnaires, which is time-consuming, and may also be unreliable if respondents do not fully understand the questions or are not willing to honestly answer them. In this article, we propose a framework for objective personality detection that leverages humans' physiological responses to external stimuli. We exemplify and evaluate the framework in a case study, where we expose subjects to affective image and video stimuli, and capture their physiological responses using non-invasive commercial-grade eye-tracking and skin conductivity sensors. These responses are then processed and used to build a machine learning classifier capable of accurately predicting a wide range of personality traits. We investigate and discuss the performance of various machine learning methods, the most and least accurately predicted traits, and also assess the importance of the different stimuli, features, and physiological signals. Our work demonstrates that personality traits can be accurately detected, suggesting the applicability of the proposed framework for robust personality detection and use by psychology practitioners and researchers, as well as designers of personalised interactive systems. © 2020 ACM.","eye tracking; field study; framework; GSR; Personality detection","Eye tracking; Machine learning; Physiology; Surveys; Turing machines; Conductivity sensors; External stimulus; Interactive system; Machine learning methods; Personality detections; Personality traits; Physiological response; Physiological signals; Physiological models",Article,"Final","",Scopus,2-s2.0-85097241147
"Pfeuffer K., Mecke L., Delgado Rodriguez S., Hassib M., Maier H., Alt F.","36141954200;57190245974;57212167536;56150903100;57219863654;27267528900;","Empirical Evaluation of Gaze-enhanced Menus in Virtual Reality",2020,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"","",,3,"10.1145/3385956.3418962","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095819416&doi=10.1145%2f3385956.3418962&partnerID=40&md5=37861db4cbabe1e28ede7cf4e2361ae2","Bundeswehr University, Munich, Germany; LMU Munich, Germany","Pfeuffer, K., Bundeswehr University, Munich, Germany; Mecke, L., Bundeswehr University, Munich, Germany, LMU Munich, Germany; Delgado Rodriguez, S., Bundeswehr University, Munich, Germany, LMU Munich, Germany; Hassib, M., Bundeswehr University, Munich, Germany; Maier, H., LMU Munich, Germany; Alt, F., Bundeswehr University, Munich, Germany","Many user interfaces involve attention shifts between primary and secondary tasks, e.g., when changing a mode in a menu, which detracts the user from their main task. In this work, we investigate how eye gaze input affords exploiting the attention shifts to enhance the interaction with handheld menus. We assess three techniques for menu selection: dwell time, gaze button, and cursor. Each represents a different multimodal balance between gaze and manual input. We present a user study that compares the techniques against two manual baselines (dunk brush, pointer) in a compound colour selection and line drawing task. We show that user performance with the gaze techniques is comparable to pointer-based menu selection, with less physical effort. Furthermore, we provide an analysis of the trade-off as each technique strives for a unique balance between temporal, manual, and visual interaction properties. Our research points to new opportunities for integrating multimodal gaze in menus and bimanual interfaces in 3D environments. © 2020 ACM.","Design; Gaze; Manual input; Menu; Pointing; Virtual Reality","Economic and social effects; Virtual reality; 3-D environments; Attention shifts; Bi-manual interface; Empirical evaluations; Menu selection; Secondary tasks; User performance; Visual interaction; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85095819416
"Kacur J., Polec J., Smolejova E., Heretik A.","24437886100;6506086664;57188726141;57193699469;","An analysis of eye-tracking features and modelling methods for free-viewed standard stimulus: Application for schizophrenia detection",2020,"IEEE Journal of Biomedical and Health Informatics","24","11","9115864","3055","3065",,3,"10.1109/JBHI.2020.3002097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095799843&doi=10.1109%2fJBHI.2020.3002097&partnerID=40&md5=904ed254d01b21fd1fbc4074744fa4f4","Faculty of Electrical Engineering and Information Technology, Slovak University of Technology, Bratislava, 811 07, Slovakia; Slovak University of Technology, Bratislava, 811 07, Slovakia; Department of Psychology, Comenius University, Bratislava, 814 99, Slovakia","Kacur, J., Faculty of Electrical Engineering and Information Technology, Slovak University of Technology, Bratislava, 811 07, Slovakia; Polec, J., Slovak University of Technology, Bratislava, 811 07, Slovakia; Smolejova, E., Department of Psychology, Comenius University, Bratislava, 814 99, Slovakia; Heretik, A., Department of Psychology, Comenius University, Bratislava, 814 99, Slovakia","Currently psychiatry is a medical field lacking an automated diagnostic process. The presence of a mental disorder is established by observing its typical symptoms. Eye-movement specifics have already been established as an 'endophenotype' for schizophrenia, but an automated diagnostic process of eye-movement analysis is still lacking. This article presents several novel approaches for the automatic detection of a schizophrenic disorder based on a free-view image test using a Rorschach inkblot and an eye tracker. Several features that enabled us to analyse the eye-tracker signal as a whole as well as its specific parts were tested. The variety of features spans global (heat maps, gaze plots), sequences of features (means, variances, and spectra), static (x and y signals as 2D images), dynamic (velocities), and model-based (limiting probabilities and transition matrices) categories. For each set of features, a proper modelling and classification method was designed (convolutional, recurrent, fully connected and combined neural networks; Hidden Markov models). By doing so, it was possible to find the importance of each feature and its physical representation using k-fold cross validation and a paired t-test. The dataset was sampled on 22 people with schizophrenia and 22 healthy individuals. The most successful approach was based on heat maps using all data and convolutional networks, reaching a 78.8% accuracy, which is a 10.5% improvement over the reference method. From all tested methods, there are two in an 85% accuracy range and over fifteen others in a 75% accuracy range at a 10% significance level. © 2013 IEEE.","CNN; eye tracking; gaze plot; GMMs; heat map; HMMs; LSTM; Markov chain; schizophrenia","Classification (of information); Convolution; Convolutional neural networks; Diagnosis; Diseases; Eye movements; Hidden Markov models; Recurrent neural networks; Automated diagnostics; Automatic Detection; Classification methods; Combined neural networks; Convolutional networks; Eye movement analysis; K fold cross validations; Transition matrices; Eye tracking; adult; algorithm; Article; classification algorithm; clinical article; computer model; electroencephalography; emergency ward; endophenotype; eye tracking; facial recognition; female; genetic algorithm; head movement; human; learning algorithm; machine learning; male; nerve cell network; perception; pharmacy (shop); polysomnography; prevalence; psychiatry; receiver operating characteristic; scanning laser ophthalmoscopy; schizophrenia; sensitivity and specificity; signal noise ratio; signal processing; speech discrimination; speech intelligibility; stimulus; support vector machine; eye movement; schizophrenia; Eye Movements; Eye-Tracking Technology; Humans; Neural Networks, Computer; Schizophrenia",Article,"Final","",Scopus,2-s2.0-85095799843
"Mengoudi K., Ravi D., Yong K.X.X., Primativo S., Pavisic I.M., Brotherhood E., Lu K., Schott J.M., Crutch S.J., Alexander D.C.","57210202731;57201696886;57204880602;55831755300;57195351982;56079056500;57205113051;7103177641;6602191607;7402830766;","Augmenting dementia cognitive assessment with instruction-less eye-tracking tests",2020,"IEEE Journal of Biomedical and Health Informatics","24","11","9124654","3066","3075",,4,"10.1109/JBHI.2020.3004686","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095799060&doi=10.1109%2fJBHI.2020.3004686&partnerID=40&md5=ff169d1227903ea0592bdbe2a2893704","Department of Computer Science, Centre for Medical Image Computing, University College London, London, WC1E 6BT, United Kingdom; Department of Neurodegenerative Disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, WC1E 6BT, United Kingdom; Department of Human Science, LUMSA University, Rome, 00193, Italy; UK Dementia Research Institute, University College London, London, WC1E 6BT, United Kingdom","Mengoudi, K., Department of Computer Science, Centre for Medical Image Computing, University College London, London, WC1E 6BT, United Kingdom; Ravi, D., Department of Neurodegenerative Disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, WC1E 6BT, United Kingdom; Yong, K.X.X., Department of Computer Science, Centre for Medical Image Computing, University College London, London, WC1E 6BT, United Kingdom; Primativo, S., Department of Human Science, LUMSA University, Rome, 00193, Italy; Pavisic, I.M., Department of Neurodegenerative Disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, WC1E 6BT, United Kingdom, UK Dementia Research Institute, University College London, London, WC1E 6BT, United Kingdom; Brotherhood, E., Department of Neurodegenerative Disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, WC1E 6BT, United Kingdom; Lu, K., Department of Neurodegenerative Disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, WC1E 6BT, United Kingdom; Schott, J.M., Department of Neurodegenerative Disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, WC1E 6BT, United Kingdom; Crutch, S.J., Department of Neurodegenerative Disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, WC1E 6BT, United Kingdom; Alexander, D.C., Department of Computer Science, Centre for Medical Image Computing, University College London, London, WC1E 6BT, United Kingdom","Eye-tracking technology is an innovative tool that holds promise for enhancing dementia screening. In this work, we introduce a novel way of extracting salient features directly from the raw eye-tracking data of a mixed sample of dementia patients during a novel instruction-less cognitive test. Our approach is based on self-supervised representation learning where, by training initially a deep neural network to solve a pretext task using well-defined available labels (e.g. recognising distinct cognitive activities in healthy individuals), the network encodes high-level semantic information which is useful for solving other problems of interest (e.g. dementia classification). Inspired by previous work in explainable AI, we use the Layer-wise Relevance Propagation (LRP) technique to describe our network's decisions in differentiating between the distinct cognitive activities. The extent to which eye-tracking features of dementia patients deviate from healthy behaviour is then explored, followed by a comparison between self-supervised and handcrafted representations on discriminating between participants with and without dementia. Our findings not only reveal novel self-supervised learning features that are more sensitive than handcrafted features in detecting performance differences between participants with and without dementia across a variety of tasks, but also validate that instruction-less eye-tracking tests can detect oculomotor biomarkers of dementia-related cognitive dysfunction. This work highlights the contribution of self-supervised representation learning techniques in biomedical applications where the small number of patients, the non-homogenous presentations of the disease and the complexity of the setting can be a challenge using state-of-the-art feature extraction methods. © 2013 IEEE.","cognition; deep-learning; dementia; Eye-tracking; representation learning","Backpropagation; Classification (of information); Deep learning; Deep neural networks; Diagnosis; Feature extraction; Learning systems; Medical applications; Network coding; Neurodegenerative diseases; Semantics; Biomedical applications; Cognitive activities; Cognitive assessments; Dementia screenings; Eye tracking technologies; Feature extraction methods; High level semantics; Learning techniques; Eye tracking; biological marker; adult; aged; Article; behavior change; body weight loss; classification algorithm; clinical article; cognitive defect; computer assisted tomography; controlled study; deep learning; deep neural network; dementia; dementia assessment; electroencephalography; emotionality; episodic memory; eye tracking; feature extraction; female; frontal variant frontotemporal dementia; functional magnetic resonance imaging; human; human experiment; human tissue; image quality; learning algorithm; machine learning; male; memory disorder; middle aged; mild cognitive impairment; Mini Mental State Examination; social cognition; social interaction test; stimulus; support vector machine; task performance; very elderly; visual field; visual stimulation; cognition; cognitive defect; dementia; neuropsychological test; Cognition; Cognitive Dysfunction; Dementia; Eye-Tracking Technology; Humans; Neuropsychological Tests",Article,"Final","",Scopus,2-s2.0-85095799060
"Koujan M.R., Doukas M.C., Roussos A., Zafeiriou S.","57202381654;57219525377;21733592400;8883680000;","ReenactNet: Real-time Full Head Reenactment",2020,"Proceedings - 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2020",,,"9320178","918","",,,"10.1109/FG47880.2020.00049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095327423&doi=10.1109%2fFG47880.2020.00049&partnerID=40&md5=901281c475da1a3ec695ac5a365206f3","University of Exeter, College of Engineering Mathematics and Physical Sciences, United Kingdom; Imperial College London, Department of Computing, United Kingdom; Institute of Computer Science (ICS), Foundation for Research and Technology-Hellas (FORTH), Greece","Koujan, M.R., University of Exeter, College of Engineering Mathematics and Physical Sciences, United Kingdom; Doukas, M.C., Imperial College London, Department of Computing, United Kingdom; Roussos, A., Institute of Computer Science (ICS), Foundation for Research and Technology-Hellas (FORTH), Greece; Zafeiriou, S., Imperial College London, Department of Computing, United Kingdom","video-to-video synthesis is a challenging problem aiming at learning a translation function between a sequence of semantic maps and a photo-realistic video depicting the characteristics of a driving video. We propose a head-to-head system of our own implementation capable of fully transferring the human head 3D pose, facial expressions and eye gaze from a source to a target actor, while preserving the identity of the target actor. Our system produces high-fidelity, temporally-smooth and photo-realistic synthetic videos faithfully transferring the human time-varying head attributes from the source to the target actor. Our proposed implementation: 1) works in real time (20 fps), 2) runs on a commodity laptop with a webcam as the only input, 3) is interactive, allowing the participant to drive a target person, e.g. a celebrity, politician, etc, instantly by varying their expressions, head pose, and eye gaze, and visualising the synthesised video concurrently. © 2020 IEEE.",,"Semantics; Facial Expressions; High-fidelity; Photo-realistic; Photo-realistic video; Semantic map; Time varying; Translation functions; Video to videos; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-85095327423
"Batliner M., Hess S., Ehrlich-Adám C., Lohmeyer Q., Meboldt M.","57204901071;57200655398;57219417447;43461718100;22835485400;","Automated areas of interest analysis for usability studies of tangible screen-based user interfaces using mobile eye tracking",2020,"Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM","34","4",,"505","514",,1,"10.1017/S0890060420000372","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092604652&doi=10.1017%2fS0890060420000372&partnerID=40&md5=8deadaeb109470a96a6eacf07a852f45","Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland","Batliner, M., Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland; Hess, S., Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland; Ehrlich-Adám, C., Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland; Lohmeyer, Q., Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland; Meboldt, M., Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland","The user's gaze can provide important information for human-machine interaction, but the analysis of manual gaze data is extremely time-consuming, inhibiting wide adoption in usability studies. Existing methods for automated areas of interest (AOI) analysis cannot be applied to tangible products with a screen-based user interface (UI), which have become ubiquitous in everyday life. The objective of this paper is to present and evaluate a method to automatically map the user's gaze to dynamic AOIs on tangible screen-based UIs based on computer vision and deep learning. This paper presents an algorithm for automated Dynamic AOI Mapping (aDAM), which allows the automated mapping of gaze data recorded with mobile eye tracking to the predefined AOIs on tangible screen-based UIs. The evaluation of the algorithm is performed using two medical devices, which represent two extreme examples of tangible screen-based UIs. The different elements of aDAM are examined for accuracy and robustness, as well as the time saved compared to manual mapping. The break-even point for an analyst's effort for aDAM compared to manual analysis is found to be 8.9 min gaze data time. The accuracy and robustness of both the automated gaze mapping and the screen matching indicate that aDAM can be applied to a wide range of products. aDAM allows, for the first time, automated AOI analysis of tangible screen-based UIs with AOIs that dynamically change over time. The algorithm requires some additional initial input for the setup and training, but analyzed gaze data duration and effort is only determined by computation time and does not require any additional manual work thereafter. The efficiency of the approach has the potential for a broader adoption of mobile eye tracking in usability testing for the development of new products and may contribute to a more data-driven usability engineering process in the future. Copyright © The Author(s), 2020. Published by Cambridge University Press.","Computer vision; convolutional neural networks; mobile eye tracking; usability testing","Automation; Deep learning; Mapping; Usability engineering; User interfaces; Automated mapping; Computation time; Human machine interaction; Mobile eye-tracking; Tangible product; Usability engineering process; Usability studies; Usability testing; Eye tracking",Article,"Final","",Scopus,2-s2.0-85092604652
"Sun Q., Mai Y., Yang R., Ji T., Jiang X., Chen X.","57218569445;57218566091;57188853612;8973090800;7404628260;55739098400;","Fast and accurate online calibration of optical see-through head-mounted display for AR-based surgical navigation using Microsoft HoloLens",2020,"International Journal of Computer Assisted Radiology and Surgery","15","11",,"1907","1919",,7,"10.1007/s11548-020-02246-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089566585&doi=10.1007%2fs11548-020-02246-4&partnerID=40&md5=75c4d83e2bbb69e7229bb654b4cd7932","Institute of Biomedical Manufacturing and Life Quality Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Oral Maxillofacial - Headneck Oncology, Shanghai Ninth People’s Hospital, College of Stomatology, Shanghai Jiao Tong University School of Medicine, Shanghai, China; Faculty of Mathematics and Computer Science, University of Münster, Einsteinstrasse 62, Münster, 48149, Germany","Sun, Q., Institute of Biomedical Manufacturing and Life Quality Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Mai, Y., Institute of Biomedical Manufacturing and Life Quality Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Yang, R., Department of Oral Maxillofacial - Headneck Oncology, Shanghai Ninth People’s Hospital, College of Stomatology, Shanghai Jiao Tong University School of Medicine, Shanghai, China; Ji, T., Department of Oral Maxillofacial - Headneck Oncology, Shanghai Ninth People’s Hospital, College of Stomatology, Shanghai Jiao Tong University School of Medicine, Shanghai, China; Jiang, X., Faculty of Mathematics and Computer Science, University of Münster, Einsteinstrasse 62, Münster, 48149, Germany; Chen, X., Institute of Biomedical Manufacturing and Life Quality Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China","Purpose:: The use of an optical see-through head-mounted display (OST-HMD) in augmented reality (AR) has significantly increased in recent years, but the alignment between the virtual scene and physical reality is still a challenge. A fast and accurate calibration method of OST-HMD is important for augmented reality in the medical field. Methods:: We proposed a fast online calibration procedure for OST-HMD with the aid of an optical tracking system. Two 3D datasets are collected in this procedure: the virtual points rendered in front of the observer’s eyes and the corresponding points in optical tracking space. The transformation between these two 3D coordinates is solved to build the connection between virtual and real space. An AR-based surgical navigation system is developed with the help of this procedure, which is used for experiment verification and clinical trial. Results:: Phantom experiment based on the 3D-printed skull is performed, and the average root-mean-square error of control points between rendered object and skull model is 1.30 ± 0.39 mm, and the time consumption of the calibration procedure can be less than 30 s. A clinical trial is also conducted to show the feasibility in real surgery theatre. Conclusions:: The proposed calibration method does not rely on the camera of the OST-HMD and is very easy to operate. Phantom experiment and clinical case demonstrated the feasibility of our AR-based surgical navigation system and indicated it has the potential in clinical application. © 2020, CARS.","Augmented reality; Calibration; Microsoft HoloLens; Surgical navigation","algorithm; Article; augmented reality; calculation; calibration; clinical trial (topic); eye tracking; feedback system; hand movement; human; image processing; image registration; image segmentation; mandible reconstruction; online system; priority journal; quaternion; three dimensional printing; treatment planning; calibration; computer assisted surgery; computer interface; imaging phantom; procedures; Augmented Reality; Calibration; Humans; Phantoms, Imaging; Surgery, Computer-Assisted; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-85089566585
"Liu C., Orlosky J., Plopski A.","57207047627;55641218100;56023098100;","Eye Gaze-based Object Rotation for Head-mounted Displays",2020,"Proceedings - SUI 2020: ACM Symposium on Spatial User Interaction",,,,"","",,1,"10.1145/3385959.3418444","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096596954&doi=10.1145%2f3385959.3418444&partnerID=40&md5=880526225836d096fe57d8d85ec981cd","Osaka University, Suita, Osaka, Japan; Otago Business School, University of Otago, Dunedin, New Zealand","Liu, C., Osaka University, Suita, Osaka, Japan; Orlosky, J., Osaka University, Suita, Osaka, Japan; Plopski, A., Otago Business School, University of Otago, Dunedin, New Zealand","Hands-free manipulation of 3D objects has long been a challenge for augmented and virtual reality (AR/VR). While many methods use eye gaze to assist with hand-based manipulations, interfaces cannot yet provide completely gaze-based 6 degree-of-freedom (DoF) manipulations in an efficient manner. To address this problem, we implemented three methods to handle rotations of virtual objects using gaze, including RotBar: a method that maps line-of-sight eye gaze onto per-axis rotations, RotPlane: a method that makes use of orthogonal planes to achieve per-axis angular rotations, and RotBall: a method that combines a traditional arcball with an external ring to handle user-perspective roll manipulations. We validated the efficiency of each method by conducting a user study involving a series of orientation tasks along different axes with each method. Experimental results showed that users could accomplish single-axis orientation tasks with RotBar and RotPlane significantly faster and more accurate than RotBall. On the other hand for multi-axis orientation tasks, RotBall significantly outperformed RotBar and RotPlane in terms of speed and accuracy. © 2020 ACM.","eye gaze; head-mounted display; object rotation; user interface","Degrees of freedom (mechanics); Helmet mounted displays; Rotation; Virtual addresses; 6 degree of freedom; Angular rotations; Augmented and virtual realities; Axis rotation; Head mounted displays; Orthogonal plane; User perspectives; Virtual objects; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85096596954
"Liu S., Liu D., Wu H.","57221491395;57218528195;57221706335;","Gaze estimation with multi-scale channel and spatial attention",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"3437438","303","309",,,"10.1145/3436369.3437438","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099880091&doi=10.1145%2f3436369.3437438&partnerID=40&md5=87c63e8d4dc7823cccacdb790b3ead52","Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; School of Microelectronics and Communication Engineering, Chongqing University, China","Liu, S., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Liu, D., School of Microelectronics and Communication Engineering, Chongqing University, China; Wu, H., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China","Gaze estimation is well established as a significant research topic in computer vision given its importance for different applications. Recent studies demonstrate that other regions of the face beyond the two eyes contain valuable information for gaze estimation. Motivated by these works, we propose a novel and powerful deep convolutional network with multi-scale channel and spatial attention, which only takes the full-face image as input without additional modules to detect the eyes and estimate the head pose, to handle the gaze estimation task. It uses multi-scale channel and spatial information to adaptively select and increase important features and suppress some unnecessary facial regions which may not contribute to estimate gaze. By rigorously evaluating our module, we show that our method significantly outperforms the state-of-the-art for 3D gaze estimation on multiple public datasets. © 2020 ACM.","Appearance-based gaze estimation; Attention mechanism; Eye tracking; Machine learning","Convolutional neural networks; Convolutional networks; Facial regions; Gaze estimation; Important features; Research topics; Spatial attention; Spatial informations; State of the art; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85099880091
"Chauhan H., Prasad A., Shukla J.","57221469442;57221461581;57027180600;","Engagement analysis of ADHD students using visual cues from eye tracker",2020,"ICMI 2020 Companion - Companion Publication of the 2020 International Conference on Multimodal Interaction",,,,"27","31",,1,"10.1145/3395035.3425256","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099266318&doi=10.1145%2f3395035.3425256&partnerID=40&md5=24c0ddcc341826e3487d9762025d4d6e","Indraprastha Institution of Information Technology Delhi, HMI Lab, New Delhi, India","Chauhan, H., Indraprastha Institution of Information Technology Delhi, HMI Lab, New Delhi, India; Prasad, A., Indraprastha Institution of Information Technology Delhi, HMI Lab, New Delhi, India; Shukla, J., Indraprastha Institution of Information Technology Delhi, HMI Lab, New Delhi, India","In this paper, we focus on finding the correlation between visual attention and engagement of ADHD students in one-on-one sessions with specialized educators using visual cues and eye-tracking data. Our goal is to investigate the extent to which observations of eye-gaze, posture, emotion and other physiological signals can be used to model the cognitive state of subjects and to explore the integration of multiple sensor modalities to improve the reliability of detection of human displays of awareness and emotion in the context of ADHD affected children. This is a novel problem since no previous studies have aimed to identify markers of attentiveness in the context of students affected with ADHD. The experiment has been designed to collect data in a controlled environment and later on can be used to generate Machine Learning models to assist real-world educators. Additionally, we propose a novel approach for AOI (Area of Interest) detection for eye-tracking analysis in dynamic scenarios using existing deep learning-based saliency prediction and fixation prediction models. We aim to use the processed data to extract the features from a subject's eye-movement patterns and use Machine Learning models to classify the attention levels. © 2020 ACM.","Eye tracking; Gaze detection; Machine learning; Saliency prediction","Behavioral research; Deep learning; Eye movements; Interactive computer systems; Learning systems; Physiological models; Predictive analytics; Students; Controlled environment; Dynamic scenarios; Engagement analysis; Eye movement patterns; Eye-tracking analysis; Machine learning models; Multiple sensors; Physiological signals; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85099266318
"Al Zaidawi S.M.K., Prinzler M.H.U., Schröder C., Zachmann G., Maneth S.","57219109682;57203303459;55290734800;6603062795;6601993718;","Gender classification of prepubescent children via eye movements with reading stimuli",2020,"ICMI 2020 Companion - Companion Publication of the 2020 International Conference on Multimodal Interaction",,,,"1","6",,1,"10.1145/3395035.3425261","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099212951&doi=10.1145%2f3395035.3425261&partnerID=40&md5=1b55a1d73c38dcd03dfa3dcb694ad0cb","University of Bremen, Bremen, Germany","Al Zaidawi, S.M.K., University of Bremen, Bremen, Germany; Prinzler, M.H.U., University of Bremen, Bremen, Germany; Schröder, C., University of Bremen, Bremen, Germany; Zachmann, G., University of Bremen, Bremen, Germany; Maneth, S., University of Bremen, Bremen, Germany","We present a new study of gender prediction using eye movements of prepubescent children aged 9 - 10. Despite previous research indicating that gender differences in eye movements are observed only in adults, we are able to predict gender with accuracies of up to 64%. Our method segments gaze point trajectories into saccades and fixations. It then computes a small number of features and classifies saccades and fixations separately using statistical methods. The used dataset contains non-dyslexic and dyslexic children. In mixed groups, the accuracy of our classifiers drops dramatically. To address this challenge, we construct a hierarchical classifier that makes use of dyslexia prediction to improve significantly the accuracy of gender prediction in mixed groups. © 2020 ACM.","Eye tracking; Feature selection; Gender difference; Gender prediction; Machine learning classifiers","Forecasting; Interactive computer systems; Gaze point; Gender classification; Gender differences; Gender predictions; Hierarchical classifiers; New study; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85099212951
"Liu M., Fu Li Y., Liu H.","57217199438;57222350602;57224917932;","3D gaze estimation for head-mounted devices based on visual saliency",2020,"IEEE International Conference on Intelligent Robots and Systems",,,"9341755","10611","10616",,,"10.1109/IROS45743.2020.9341755","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102408756&doi=10.1109%2fIROS45743.2020.9341755&partnerID=40&md5=30db035319a4768cba781c58b7413817","City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Central China Normal University, National Engineering Research Center for E-Learning, Wuhan, China","Liu, M., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Fu Li, Y., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Liu, H., Central China Normal University, National Engineering Research Center for E-Learning, Wuhan, China","Compared with the maturity of 2D gaze tracking technology, 3D gaze tracking has gradually become a research hotspot in recent years. The head-mounted gaze tracker has shown great potential for gaze estimation in 3D space due to its appealing flexibility and portability. The general challenge for 3D gaze tracking algorithms is that calibration is necessary before the usage, and calibration targets cannot be easily applied in some situations or might be blocked by moving human and objects. Besides, the accuracy on depth direction has always come to be a crucial problem. Regarding the issues mentioned above, a 3D gaze estimation with auto-calibration method is proposed in this study. We use an RGBD camera as the scene camera to acquire the accurate 3D structure of the environment. The automatic calibration is achieved by uniting gaze vectors with saliency maps of the scene which aligned depth information. Finally, we determine the 3D gaze point through a point cloud generated from the RGBD camera. The experiment result demonstrates that our proposed method achieves 4.34 of average angle error in the field from 0.5m to 3m and the average depth error is 23.22mm, which is sufficient for 3D gaze estimation in the real scene. © 2020 IEEE.",,"Agricultural robots; Calibration; Cameras; Intelligent robots; Object tracking; Auto-calibration method; Automatic calibration; Average angle; Calibration targets; Depth information; Gaze estimation; Rgb-d cameras; Visual saliency; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85102408756
"Olawale O.P., Dimililer K.","57220811417;14624921500;","Individual Eye Gaze Prediction with the Effect of Image Enhancement Using Deep Neural Networks",2020,"4th International Symposium on Multidisciplinary Studies and Innovative Technologies, ISMSIT 2020 - Proceedings",,,"9254786","","",,1,"10.1109/ISMSIT50672.2020.9254786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097667477&doi=10.1109%2fISMSIT50672.2020.9254786&partnerID=40&md5=3ca49e16582cd84676f17a4fe8d34ad4","Near East University, Faculty of Engineering, Research Center for AI and IoT, Software Engineering Department, via Mersin 10, Nicosia Cyprus, Turkey; Near East University, Faculty of Engineering, Applied Artificial Intelligence Research Center, Electrical and Electronic Engineering Department, via Mersin 10, Nicosia, Turkey","Olawale, O.P., Near East University, Faculty of Engineering, Research Center for AI and IoT, Software Engineering Department, via Mersin 10, Nicosia Cyprus, Turkey; Dimililer, K., Near East University, Faculty of Engineering, Applied Artificial Intelligence Research Center, Electrical and Electronic Engineering Department, via Mersin 10, Nicosia, Turkey","The prediction of individual eye gaze is a research topic that has gained the interest of researchers with its wide range of applications because neural networks majorly increase the rate of accuracy of individual gaze. In this research work, MPIIGaze dataset has been employed for the prediction of individual gaze and the direction of individual gaze was grouped into down view, left view, right view and lastly centre view. A CNN model was used to train and validate a random selection of images. Firstly, the ordinary images were trained and validated, after which image enhancement processing technique was applied. With the image brightness enhancement technique, a higher rate of gaze prediction accuracy was achieved. Hence, it can be deduced that image enhancement has proved its purpose by providing image interpretation with better quality. © 2020 IEEE.","deep learning; Gaze detection; gaze direction; image enhancement; individual eyes",,Conference Paper,"Final","",Scopus,2-s2.0-85097667477
"Sims S.D., Conati C.","57198861395;6602976668;","A Neural Architecture for Detecting User Confusion in Eye-tracking Data",2020,"ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction",,,,"15","23",,1,"10.1145/3382507.3418828","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096716453&doi=10.1145%2f3382507.3418828&partnerID=40&md5=55fc571f5c41a4647e22bdfec63a20ed","University of British ColumbiaVancouver, Vancouver, BC, Canada","Sims, S.D., University of British ColumbiaVancouver, Vancouver, BC, Canada; Conati, C., University of British ColumbiaVancouver, Vancouver, BC, Canada","Encouraged by the success of deep learning in a variety of domains, we investigate the effectiveness of a novel application of such methods for detecting user confusion with eye-tracking data. We introduce an architecture that uses RNN and CNN sub-models in parallel, to take advantage of the temporal and visuospatial aspects of our data. Experiments with a dataset of user interactions with the ValueChart visualization tool show that our model outperforms an existing model based on a Random Forest classifier, resulting in a 22% improvement in combined confused and not confused class accuracies. © 2020 ACM.","classification; eye-tracking; neural networks; user affect; user model","Classification (of information); Decision trees; Interactive computer systems; Recurrent neural networks; Model-based OPC; Neural architectures; Novel applications; Random forest classifier; Submodels; User interaction; Visualization tools; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85096716453
"Emerson A., Henderson N., Rowe J., Min W., Lee S., Minogue J., Lester J.","57203432942;57201721223;57203082620;55790560900;36194869400;12791055300;57203179695;","Early Prediction of Visitor Engagement in Science Museums with Multimodal Learning Analytics",2020,"ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction",,,,"107","116",,1,"10.1145/3382507.3418890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096695474&doi=10.1145%2f3382507.3418890&partnerID=40&md5=d71d22f1ff038b8bcf869b251ae7c590","North Carolina State University, Raleigh, NC  27695, United States","Emerson, A., North Carolina State University, Raleigh, NC  27695, United States; Henderson, N., North Carolina State University, Raleigh, NC  27695, United States; Rowe, J., North Carolina State University, Raleigh, NC  27695, United States; Min, W., North Carolina State University, Raleigh, NC  27695, United States; Lee, S., North Carolina State University, Raleigh, NC  27695, United States; Minogue, J., North Carolina State University, Raleigh, NC  27695, United States; Lester, J., North Carolina State University, Raleigh, NC  27695, United States","Modeling visitor engagement is a key challenge in informal learning environments, such as museums and science centers. Devising predictive models of visitor engagement that accurately forecast salient features of visitor behavior, such as dwell time, holds significant potential for enabling adaptive learning environments and visitor analytics for museums and science centers. In this paper, we introduce a multimodal early prediction approach to modeling visitor engagement with interactive science museum exhibits. We utilize multimodal sensor data including eye gaze, facial expression, posture, and interaction log data captured during visitor interactions with an interactive museum exhibit for environmental science education, to induce predictive models of visitor dwell time. We investigate machine learning techniques (random forest, support vector machine, Lasso regression, gradient boosting trees, and multi-layer perceptron) to induce multimodal predictive models of visitor engagement with data from 85 museum visitors. Results from a series of ablation experiments suggest that incorporating additional modalities into predictive models of visitor engagement improves model accuracy. In addition, the models show improved predictive performance over time, demonstrating that increasingly accurate predictions of visitor dwell time can be achieved as more evidence becomes available from visitor interactions with interactive science museum exhibits. These findings highlight the efficacy of multimodal data for modeling museum exhibit visitor engagement. © 2020 ACM.","early prediction; multimodal learning analytics; museum-based learning; visitor modeling","Adaptive boosting; Computer aided instruction; Decision trees; Exhibitions; Forecasting; Interactive computer systems; Learning systems; Multilayer neural networks; Museums; Random forests; Recreation centers; Support vector machines; Support vector regression; Ablation experiments; Adaptive learning environment; Environmental science; Informal learning environments; Machine learning techniques; Multi layer perceptron; Multi-modal learning; Predictive performance; Predictive analytics",Conference Paper,"Final","",Scopus,2-s2.0-85096695474
"Yu Z., Huang X., Zhang X., Shen H., Li Q., Deng W., Tang J., Yang Y., Ye J.","57220059228;57211979633;57218452564;57202803726;57221243465;8905974100;55713937200;57221524455;7403237682;","A Multi-Modal Approach for Driver Gaze Prediction to Remove Identity Bias",2020,"ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction",,,,"768","776",,,"10.1145/3382507.3417961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096677530&doi=10.1145%2f3382507.3417961&partnerID=40&md5=8c3d340a52efddc895a3f5cb7d06d323","Beijing University of Posts and Telecommunications, DiDi Chuxing, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; DiDi Chuxing, Beijing, China","Yu, Z., Beijing University of Posts and Telecommunications, DiDi Chuxing, Beijing, China; Huang, X., Beijing University of Posts and Telecommunications, Beijing, China; Zhang, X., DiDi Chuxing, Beijing, China; Shen, H., DiDi Chuxing, Beijing, China; Li, Q., DiDi Chuxing, Beijing, China; Deng, W., Beijing University of Posts and Telecommunications, Beijing, China; Tang, J., DiDi Chuxing, Beijing, China; Yang, Y., DiDi Chuxing, Beijing, China; Ye, J., DiDi Chuxing, Beijing, China","Driver gaze prediction is an important task in Advanced Driver Assistance System (ADAS). Although the Convolutional Neural Network (CNN) can greatly improve the recognition ability, there are still several unsolved problems due to the challenge of illumination, pose and camera placement. To solve these difficulties, we propose an effective multi-model fusion method for driver gaze estimation. Rich appearance representations, i.e. holistic and eyes regions, and geometric representations, i.e. landmarks and Delaunay angles, are separately learned to predict the gaze, followed by a score-level fusion system. Moreover, pseudo-3D appearance supervision and identity-adaptive geometric normalization are proposed to further enhance the prediction accuracy. Finally, the proposed method achieves state-of-the-art accuracy of 82.5288% on the test data, which ranks 1st at the EmotiW2020 driver gaze prediction sub-challenge. © 2020 ACM.","appearance representation; driver gaze prediction; geometric representation; identity bias; model fusion","Automobile drivers; Convolutional neural networks; Forecasting; Interactive computer systems; Geometric representation; Multi-modal approach; Multi-model fusion; Prediction accuracy; Recognition abilities; Score-level fusion; State of the art; Unsolved problems; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85096677530
"Putze F., Küster D., Urban T., Zastrow A., Kampen M.","22036416700;15064259400;57220055588;57220046354;57220055117;","Attention Sensing through Multimodal User Modeling in an Augmented Reality Guessing Game",2020,"ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction",,,,"33","40",,,"10.1145/3382507.3418865","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096675360&doi=10.1145%2f3382507.3418865&partnerID=40&md5=b31fbf3ac59f0288b7cb189846204695","University of Bremen, Bremen, Germany","Putze, F., University of Bremen, Bremen, Germany; Küster, D., University of Bremen, Bremen, Germany; Urban, T., University of Bremen, Bremen, Germany; Zastrow, A., University of Bremen, Bremen, Germany; Kampen, M., University of Bremen, Bremen, Germany","We developed an attention-sensitive system that is capable of playing the children's guessing game ""I spy with my litte eye""with a human user. In this game, the user selects an object from a given scene and provides the system with a single-sentence clue about it. For each trial, the system tries to guess the target object. Our approach combines top-down and bottom-up machine learning for object and color detection, automatic speech recognition, natural language processing, a semantic database, eye tracking, and augmented reality. Our evaluation demonstrates performance significantly above chance level, and results for most of the individual machine learning components are encouraging. Participants reported very high levels of satisfaction and curiosity about the system. The collected data shows that our guessing game generates a complex and rich data set. We discuss the capabilities and challenges of our system and its components with respect to multimodal attention sensing. © 2020 ACM.","attention; augmented reality; gamification; top-down and bottom-up modeling","Augmented reality; Eye tracking; Interactive computer systems; Machine learning; Natural language processing systems; Object detection; Semantics; Speech recognition; Automatic speech recognition; Color detection; Multi-modal; NAtural language processing; Semantic database; Sensitive systems; Target object; User Modeling; Object tracking",Conference Paper,"Final","",Scopus,2-s2.0-85096675360
"Stappen L., Rizos G., Schuller B.","57210120403;57194224622;6603767415;","X-AWARE: ConteXt-AWARE Human-Environment Attention Fusion for Driver Gaze Prediction in the Wild",2020,"ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction",,,,"858","867",,2,"10.1145/3382507.3417967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096659210&doi=10.1145%2f3382507.3417967&partnerID=40&md5=80b9660cab9367c61c270f5360bc1719","University of Augsburg, Chair of Embedded Intelligence for Health Care and Wellbeing, Augsburg, Germany; Imperial College London, Group on Language, Audio, and Music, London, United Kingdom","Stappen, L., University of Augsburg, Chair of Embedded Intelligence for Health Care and Wellbeing, Augsburg, Germany; Rizos, G., Imperial College London, Group on Language, Audio, and Music, London, United Kingdom; Schuller, B., Imperial College London, Group on Language, Audio, and Music, London, United Kingdom","Reliable systems for automatic estimation of the driver's gaze are crucial for reducing the number of traffic fatalities and for many emerging research areas aimed at developing intelligent vehicle-passenger systems. Gaze estimation is a challenging task, especially in environments with varying illumination and reflection properties. Furthermore, there is wide diversity with respect to the appearance of drivers' faces, both in terms of occlusions (e.g. vision aids) and cultural/ethnic backgrounds. For this reason, analysing the face along with contextual information - for example, the vehicle cabin environment - adds another, less subjective signal towards the design of robust systems for passenger gaze estimation. In this paper, we present an integrated approach to jointly model different features for this task. In particular, to improve the fusion of the visually captured environment with the driver's face, we have developed a contextual attention mechanism, X-AWARE, attached directly to the output convolutional layers of InceptionResNetV2 networks. In order to showcase the effectiveness of our approach, we use the Driver Gaze in the Wild dataset, recently released as part of the Eighth Emotion Recognition in the Wild Challenge (EmotiW) challenge. Our best model outperforms the baseline by an absolute of 15.03% in accuracy on the validation set, and improves the previously best reported result by an absolute of 8.72% on the test set. © 2020 ACM.","attention fusion; context aware; gaze detection; in the wild","Interactive computer systems; Network layers; Attention mechanisms; Automatic estimation; Contextual information; Emotion recognition; Human environment; Integrated approach; Reflection properties; Traffic fatalities; Vision aids",Conference Paper,"Final","",Scopus,2-s2.0-85096659210
"Kangas J., Koskinen O., Raisamo R.","7005151992;56024691000;35610443700;","Gaze Tracker Accuracy and Precision Measurements in Virtual Reality Headsets",2020,"ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction",,,,"640","644",,,"10.1145/3382507.3418816","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096634313&doi=10.1145%2f3382507.3418816&partnerID=40&md5=7fc4b5b29de18bae716e015dd7463521","Tampere University, Tampere, Finland","Kangas, J., Tampere University, Tampere, Finland; Koskinen, O., Tampere University, Tampere, Finland; Raisamo, R., Tampere University, Tampere, Finland","To effectively utilize a gaze tracker in user interaction it is important to know the quality of the gaze data that it is measuring. We have developed a method to evaluate the accuracy and precision of gaze trackers in virtual reality headsets. The method consists of two software components. The first component is a simulation software that calibrates the gaze tracker and then performs data collection by providing a gaze target that moves around the headset's field-of-view. The second component makes an off-line analysis of the logged gaze data and provides a number of measurement results of the accuracy and precision. The analysis results consist of the accuracy and precision of the gaze tracker in different directions inside the virtual 3D space. Our method combines the measurements into overall accuracy and precision. Visualizations of the measurements are created to see possible trends over the display area. Results from selected areas in the display are analyzed to find out differences between the areas (for example, the middle/outer edge of the display or the upper/lower part of display). © 2020 ACM.","accuracy measurement; eye tracking; precision; virtual reality","Computer software; Interactive computer systems; Virtual reality; Accuracy and precision; Data collection; Off-line analysis; Overall accuracies; Simulation software; Software component; User interaction; Virtual-reality headsets; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85096634313
"Pustejovsky J., Krishnaswamy N.","6602448845;57197865998;","Embodied Human-Computer Interactions through Situated Grounding",2020,"Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents, IVA 2020",,,,"","",,2,"10.1145/3383652.3423910","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096988598&doi=10.1145%2f3383652.3423910&partnerID=40&md5=b2c2f4d57ea986717478d15c3379cd30","Brandeis University, Waltham, MA, United States; Colorado State University, Fort Collins, United States","Pustejovsky, J., Brandeis University, Waltham, MA, United States; Krishnaswamy, N., Colorado State University, Fort Collins, United States","In this paper, we introduce a simulation platform for modeling and building Embodied Human-Computer Interactions (EHCI). This system, VoxWorld, is a multimodal dialogue system enabling communication through language, gesture, action, facial expressions, and gaze tracking, in the context of task-oriented interactions. A multimodal simulation is an embodied 3D virtual realization of both the situational environment and the co-situated agents, as well as the most salient content denoted by communicative acts in a discourse. It is built on the modeling language VoxML [7], which encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextu-ally salient inferences and decisions in the environment. VoxWorld enables an embodied HCI by situating both human and computational agents within the same virtual simulation environment, where they share perceptual and epistemic common ground. © 2020 Owner/Author.","multimodal embodiment; simulation; situated grounding; virtual agent","Computer simulation languages; Eye tracking; Intelligent virtual agents; Modeling languages; Semantics; Speech processing; Communicative acts; Computational agents; Facial Expressions; Multi-modal simulation; Multimodal dialogue systems; Situated agents; Virtual realization; Virtual simulation environments; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85096988598
"Bing Z., Lemke C., Morin F.O., Jiang Z., Cheng L., Huang K., Knoll A.","57194493806;57217274465;57219725161;57197827224;57141287900;56535700000;7102250639;","Perception-Action Coupling Target Tracking Control for a Snake Robot via Reinforcement Learning",2020,"Frontiers in Neurorobotics","14",,"591128","","",,4,"10.3389/fnbot.2020.591128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094954753&doi=10.3389%2ffnbot.2020.591128&partnerID=40&md5=abdb9535d5217c0d2c66d3c133198a22","Department of Informatics, Technical University of Munich, Munich, Germany; Department of Informatics, Ludwig Maximilian University of Munich, Munich, Germany; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; †Long Cheng, College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou, China","Bing, Z., Department of Informatics, Technical University of Munich, Munich, Germany; Lemke, C., Department of Informatics, Ludwig Maximilian University of Munich, Munich, Germany; Morin, F.O., Department of Informatics, Technical University of Munich, Munich, Germany; Jiang, Z., Department of Informatics, Technical University of Munich, Munich, Germany; Cheng, L., School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China, †Long Cheng, College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou, China; Huang, K., School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Knoll, A., Department of Informatics, Technical University of Munich, Munich, Germany","Visual-guided locomotion for snake-like robots is a challenging task, since it involves not only the complex body undulation with many joints, but also a joint pipeline that connects the vision and the locomotion. Meanwhile, it is usually difficult to jointly coordinate these two separate sub-tasks as this requires time-consuming and trial-and-error tuning. In this paper, we introduce a novel approach for solving target tracking tasks for a snake-like robot as a whole using a model-free reinforcement learning (RL) algorithm. This RL-based controller directly maps the visual observations to the joint positions of the snake-like robot in an end-to-end fashion instead of dividing the process into a series of sub-tasks. With a novel customized reward function, our RL controller is trained in a dynamically changing track scenario. The controller is evaluated in four different tracking scenarios and the results show excellent adaptive locomotion ability to the unpredictable behavior of the target. Meanwhile, the results also prove that the RL-based controller outperforms the traditional model-based controller in terms of tracking accuracy. © Copyright © 2020 Bing, Lemke, Morin, Jiang, Cheng, Huang and Knoll.","motion planning; reinforcement learning; snake robot; target tracking; visual perception","Clutter (information theory); Controllers; Reinforcement learning; Robots; Adaptive locomotion; Perception-action; Reward function; Snake-like robot; Target-tracking control; Tracking accuracy; Traditional models; Visual observations; Target tracking; article; eye tracking; locomotion; motion; nonhuman; reinforcement learning (machine learning); reward; robotics; snake; vision",Article,"Final","",Scopus,2-s2.0-85094954753
"Xia Y., Liang B.","57216709662;56643806300;","Gaze Estimation Based on Deep Learning Method",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"3425003","","",,1,"10.1145/3424978.3425003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094900908&doi=10.1145%2f3424978.3425003&partnerID=40&md5=1105a91d7d328980fdd84477d6af5529","Institute of Medical Technology, Peking University, Beijing, China; Department of Biostatistics, Peking University, Beijing, China","Xia, Y., Institute of Medical Technology, Peking University, Beijing, China; Liang, B., Department of Biostatistics, Peking University, Beijing, China","Many mature methods of gaze estimation are available in various scenarios. Relying on additional hardware or platforms with professional equipment to tackle intensive computation tasks is a prominent problem of traditional methods, which usually involves high costs and is relatively tedious. Besides, the implementation of traditional gaze estimation method is typically complex. Traditional gaze estimation approaches require systematic prior knowledge or expertise for practical operations, and the gaze is estimated through the representation of pupil and iris, so high-quality images shot in special environments are required. This paper proposes a data-driven method for gaze estimation. It can be applied to various mobile platforms with deep learning methods instead of additional hardware devices or systematic prior knowledge. When collecting gaze data set, the paper designs a set of automatic and fast data collection mechanism on the mobile platform. Beyond that, the paper proposes an annotation method on collected gaze dataset that improves the predicted accuracy. The results demonstrate that the deep learning method performs well and can satisfy the task need of different applications. © 2020 ACM.","Annotation method; Deep learning; Gaze estimation","Computer hardware; Data acquisition; Drilling platforms; Learning systems; Annotation methods; Computation tasks; Data collection mechanism; Data-driven methods; Hardware devices; High quality images; Learning methods; Professional equipment; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85094900908
"Abid M., Perreira Da Silva M., Le Callet P.","57212027972;24337440300;57200770358;","Perceptual Characterization of 3D Graphical Contents based on Attention Complexity Measures",2020,"QoEVMA 2020 - Proceedings of the 1st Workshop on Quality of Experience (QoE) in Visual Multimedia Applications",,,,"31","36",,,"10.1145/3423328.3423498","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096092286&doi=10.1145%2f3423328.3423498&partnerID=40&md5=4826a9444dad42f9e0498b876a28503e","Universite de Nantes, LS2N, Cnrs, Nantes, France","Abid, M., Universite de Nantes, LS2N, Cnrs, Nantes, France; Perreira Da Silva, M., Universite de Nantes, LS2N, Cnrs, Nantes, France; Le Callet, P., Universite de Nantes, LS2N, Cnrs, Nantes, France","This paper provides insights on how to perceptually characterize colored 3D Graphical Contents (3DGC). In this study, pre-defined viewpoints were considered to render static graphical objects. For perceptual characterization, we used visual attention complexity (VAC) measures. Considering a view-based approach to exploit the perceived information, an eye-tracking experiment was conducted using colored graphical objects. Based on the collected gaze data, we revised the VAC measure, suggested in 2D imaging context, and adapted it to 3DGC. We also provided an objective predictor that highly mimics the experimental attentional complexity information. This predictor can be useful in Quality of Experience (QoE) studies: to balance content selection when benchmarking 3DGC processing techniques (e.g., rendering, coding, streaming, etc.) for human panel studies or ad hoc key performance indicator, and also to optimize the user's QoE when rendering such contents. © 2020 ACM.","3d graphical objects; attentional complexity; content characterization; visual attention","Behavioral research; Benchmarking; Eye tracking; Object tracking; Rendering (computer graphics); Complexity measures; Graphical objects; Key performance indicators; Panel studies; Processing technique; Quality of experience (QoE); View-based approach; Visual Attention; Quality of service",Conference Paper,"Final","",Scopus,2-s2.0-85096092286
"Otsu K., Seo M., Kitajima T., Chen Y.-W.","57221536630;35280835900;56810615800;56036268200;","Automatic generation of eye gaze corrected video using recursive conditional generative adversarial networks",2020,"2020 IEEE 9th Global Conference on Consumer Electronics, GCCE 2020",,,"9291784","674","677",,,"10.1109/GCCE50665.2020.9291784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099408812&doi=10.1109%2fGCCE50665.2020.9291784&partnerID=40&md5=9a59eee985d358a7f60cdb44968c0567","Ritsumeikan University, Graduate School of Information and Engineering, Shiga, Japan; Osaka Institute of Technology, Osaka, Japan; Samsung Japan Research Institute, Kanagawa, Japan","Otsu, K., Ritsumeikan University, Graduate School of Information and Engineering, Shiga, Japan; Seo, M., Osaka Institute of Technology, Osaka, Japan; Kitajima, T., Samsung Japan Research Institute, Kanagawa, Japan; Chen, Y.-W., Ritsumeikan University, Graduate School of Information and Engineering, Shiga, Japan","Eye contact plays an important role in conversations. However, it is difficult to maintain eye contact while using current popular video calling systems due to the different positions of the camera and display. To solve this problem, we introduce convolutional long short-term memory, which captures the features of frames up to the previous instant, into a deep learning generation model - conditional generative adversarial networks (GANs), which is recursive GANs - to generate eye gaze corrected video. By extending this network, the generator generates an image that takes the previous frame into account and the discriminator identifies the image by considering the previous frame. Thus, we aimed to achieve a consistent video conversion. © 2020 IEEE.","Convolutional LSTM; deep learning; face; generative adversarial networks; image transformation; Video","Electronics engineering; Electronics industry; Adversarial networks; Automatic Generation; Eye contact; Eye-gaze; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85099408812
"Kurono H., Shibuya Y.","57215823394;17346932700;","Non-contact digital signage allowing users to change contents with their face orientation",2020,"2020 IEEE 9th Global Conference on Consumer Electronics, GCCE 2020",,,"9291871","747","751",,,"10.1109/GCCE50665.2020.9291871","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099368679&doi=10.1109%2fGCCE50665.2020.9291871&partnerID=40&md5=fc01552dd4789833d04ff6ec82971b35","Kyoto Institute of Technology, Master's Program of Information Science Graduate School of Science and Technology, Kyoto, Japan; Kyoto Institute of Technology, Faculty of Information and Human Sciences, Kyoto, Japan","Kurono, H., Kyoto Institute of Technology, Master's Program of Information Science Graduate School of Science and Technology, Kyoto, Japan; Shibuya, Y., Kyoto Institute of Technology, Faculty of Information and Human Sciences, Kyoto, Japan","In today's society, digital signage is widely used in various places such as commercial facilities and train stations. Most digital signages use a liquid crystal display or LED panel. One of the advantages of digital signage is the ability to dynamically change the content of the advertisement. However, the method of change is not due to the viewer's intention, but rather to elapsed time to show, and it is merely a sequential display of static advertisements. In this study, we propose and implement a non-contact interactive digital signage system using a web camera that allows users to change the content according to their face orientation. In the implementation, we use OpenCV for showing the content and image processing, and the machine learning library, named dlib, is used for detecting user's face and identifying its orientation. The proposed system changes the layout and enlarges the content in which the user is interested. The user does not need to wear any device and to do any special action. Furthermore, the proposed system offers an intuitive interaction manner to the user. In this paper, some experimental designs are also described. © 2020 IEEE.","advertising effectiveness; digital signage; eye tracking; machine learning; OpenCV","Image processing; Liquid crystal displays; Commercial facilities; Digital signage; Face orientation; Intuitive interaction; Non-contact; System change; Train stations; Web camera; Liquid crystals",Conference Paper,"Final","",Scopus,2-s2.0-85099368679
"Liu M., Li Y.F., Liu H.","57217199438;8589964900;57224917932;","Towards Robust Auto-calibration for Head-mounted Gaze Tracking Systems",2020,"2020 IEEE International Conference on Mechatronics and Automation, ICMA 2020",,,"9233571","588","593",,1,"10.1109/ICMA49215.2020.9233571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096604606&doi=10.1109%2fICMA49215.2020.9233571&partnerID=40&md5=c69806bebb22c81f28b42a425b6808aa","City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Central China Normal University, National Engineering Research Center for E-Learning, Wuhan, Hubei Province, China","Liu, M., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Li, Y.F., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Liu, H., Central China Normal University, National Engineering Research Center for E-Learning, Wuhan, Hubei Province, China","Removing explicit user calibration is indeed an appealing goal for gaze tracking systems. In this paper, a novel auto-calibration method is proposed to achieve the 3D point of regard (PoR) prediction for the head-mounted gaze tracker. Our method chooses an RGBD sensor as the scene camera to capture 3D structures of the environment and treats salient regions as possible 3D calibration targets. In order to improve efficiency, the bag of words (BoW) algorithm is applied to calculate scene images' similarity and eliminate redundant maps. After elimination, the translation relationship between eye cameras and the scene camera can be determined by uniting calibration targets with gaze vectors, and 3D gaze points are obtained by transformed gaze vectors and the point cloud of environment. The experiment results indicate that our method achieves effective performance on 3D gaze estimation for head-mounted gaze trackers, which can promote engineering applications of human-computer interaction technology in many areas. © 2020 IEEE.","3D gaze estimation; auto-calibration; head-mounted gaze tracker; human-computer interaction","Calibration; Cameras; Human computer interaction; Image enhancement; Auto calibration; Auto-calibration method; Calibration targets; Effective performance; Engineering applications; Gaze tracking system; Point of regards; User calibration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85096604606
"Bermejo C., Chatzopoulos D., Hui P.","57192959451;56890943900;14029922900;","EyeShopper: Estimating Shoppers' Gaze using CCTV Cameras",2020,"MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia",,,,"2765","2774",,1,"10.1145/3394171.3413683","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106964693&doi=10.1145%2f3394171.3413683&partnerID=40&md5=31cc50e3f51eaf7b26e45acb4712dc30","The Hong Kong University of Science and Technology, Hong Kong; University of Helsinki, Finland","Bermejo, C., The Hong Kong University of Science and Technology, Hong Kong; Chatzopoulos, D., The Hong Kong University of Science and Technology, Hong Kong; Hui, P., The Hong Kong University of Science and Technology, Hong Kong, University of Helsinki, Finland","Recent advances in machine and deep learning allow for enhanced retail analytics by applying object detection techniques. However, existing approaches either require laborious installation processes to function or lack precision when the customers turn their back in the installed cameras. In this paper, we present EyeShopper, an innovative system that tracks the gaze of shoppers when facing away from the camera and provides insights about their behavior in physical stores. EyeShopper is readily deployable in existing surveillance systems and robust against low-resolution video inputs. At the same time, its accuracy is comparable to state-of-the-art gaze estimation frameworks that require high-resolution and continuous video inputs to function. Furthermore, EyeShopper is more robust than state-of-the-art gaze tracking techniques for back head images. Extensive evaluation with different real video datasets and a synthetic dataset we produced shows that EyeShopper estimates with high accuracy the gaze of customers. © 2020 ACM.","camera; convolutional neural networks; gaze estimator; indoor location; retail stores","Cameras; Deep learning; Object detection; Sales; Security systems; Gaze estimation; High resolution; Innovative systems; Low resolution video; Physical stores; State of the art; Surveillance systems; Video datasets; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85106964693
"HemaMalini B.H., Supritha R.C., Venkatesh Prasad N.K., Vandana R., Yadav R.","57222114799;57210205399;57222111282;57222106570;54893773800;","Eye and Voice Controlled Wheel Chair",2020,"Proceedings of B-HTC 2020 - 1st IEEE Bangalore Humanitarian Technology Conference",,,"9297845","","",,,"10.1109/B-HTC50970.2020.9297845","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101460232&doi=10.1109%2fB-HTC50970.2020.9297845&partnerID=40&md5=2f65c87aabf20f0266b252b6b14ebacc","BMS Institute of Technology and Management, Dept. of CSE, Bengaluru, India","HemaMalini, B.H., BMS Institute of Technology and Management, Dept. of CSE, Bengaluru, India; Supritha, R.C., BMS Institute of Technology and Management, Dept. of CSE, Bengaluru, India; Venkatesh Prasad, N.K., BMS Institute of Technology and Management, Dept. of CSE, Bengaluru, India; Vandana, R., BMS Institute of Technology and Management, Dept. of CSE, Bengaluru, India; Yadav, R., BMS Institute of Technology and Management, Dept. of CSE, Bengaluru, India","The smart wheelchair developed for people with inabilities dependent on the eye-Tracking and by utilizing the voice assistant module. The smart wheelchair contains two modules, including a module for processing the image, a wheelchair-controlled voice associate module and constrained by appliances. The module for picture processing comprises a camera mounted on the wheelchair that can capture the picture and handling those images. The captured picture moved into a Raspberry Pie micro controller process using Open CV to get pupil movement in the 2D bearing. The movement of pupil is then moved remotely to the module for managing a wheelchair. The eyeball movement is also used as the controller to regulate the operations. Speech recognition technology is a technology that provides a way of interaction for human with the wheel chair. Hence, the problems faced by the people can be easily solved by using this technology for controlling the wheel chair. This can be implemented by using the smart phone or smart assistant enabled device as an interface between the human and the wheel chair. © 2020 IEEE.","Eye Tracking; Image Processing; Speech Reorganization; Voice Assistant","Eye tracking; Image processing; Smartphones; Wheelchairs; Wheels; Eyeball movements; Smart wheelchairs; Speech recognition technology; Voice-controlled; Speech recognition",Conference Paper,"Final","",Scopus,2-s2.0-85101460232
"Alfaroby E. M., Wibirama S., Ardiyanto I.","57221293375;26654457700;36069000500;","Accuracy Improvement of Object Selection in Gaze Gesture Application using Deep Learning",2020,"ICITEE 2020 - Proceedings of the 12th International Conference on Information Technology and Electrical Engineering",,,"9271771","307","311",,,"10.1109/ICITEE49829.2020.9271771","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098691974&doi=10.1109%2fICITEE49829.2020.9271771&partnerID=40&md5=87133a2a10d7b49530c07a6224d0b0ac","Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia","Alfaroby E., M., Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia; Wibirama, S., Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia; Ardiyanto, I., Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia","Gaze-based interaction is a crucial research area. Gaze gesture provides faster interaction between a user and a computer application because people naturally look at the object of interest before taking any other actions. Spontaneous gaze-gesture-based application uses gaze-gesture as an input modality without performing any calibration. The conventional eye tracking systems have a problem with low accuracy. In general, data captured by eye tracker contains errors and noise within gaze position signal. The errors and noise affect the performance of object selection in gaze gesture based application that controls digital contents on the display using smooth-pursuit eye movement. The conventional object selection method suffers from low accuracy (<80%). In this paper, we addressed this accuracy problem with a novel approach using deep learning. We exploited deep learning power to recognize the pattern of eye-gaze data. Long Short Term Memory (LSTM) is a deep learning architecture based on recurrent neural network (RNN). We used LSTM to perform object selection task. The dataset consisted of 34 participants taken from previous study of object selection technique of gaze gesture-based application. Our experimental results show that the proposed method achieved 96.17% of accuracy. In future, our result may be used as a guidance for developing gaze gesture application. © 2020 IEEE.","Deep Learning; Eye Tracking; Gaze Gesture; LSTM","Eye movements; Eye tracking; Long short-term memory; Accuracy Improvement; Accuracy problems; Eye tracking systems; Gaze-based interaction; Input modalities; Learning architectures; Recurrent neural network (RNN); Smooth pursuit eye movement; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85098691974
"Sun Y., Prabhushankar M., Alregib G.","57219795910;57191443851;6506443965;","Implicit Saliency in Deep Neural Networks",2020,"Proceedings - International Conference on Image Processing, ICIP","2020-October",,"9191186","2915","2919",,,"10.1109/ICIP40778.2020.9191186","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098671128&doi=10.1109%2fICIP40778.2020.9191186&partnerID=40&md5=fc532ae5c14701bbe1a2e0e5bb5d67c7","Georgia Institute of Technology, Olives at the Center for Signal and Information Processing, Chool of Electrical and Computer Engineering, Atlanta, GA  30332-0250, United States","Sun, Y., Georgia Institute of Technology, Olives at the Center for Signal and Information Processing, Chool of Electrical and Computer Engineering, Atlanta, GA  30332-0250, United States; Prabhushankar, M., Georgia Institute of Technology, Olives at the Center for Signal and Information Processing, Chool of Electrical and Computer Engineering, Atlanta, GA  30332-0250, United States; Alregib, G., Georgia Institute of Technology, Olives at the Center for Signal and Information Processing, Chool of Electrical and Computer Engineering, Atlanta, GA  30332-0250, United States","In this paper, we show that existing recognition and localization deep architectures, that have not been exposed to eye tracking data or any saliency datasets, are capable of predicting the human visual saliency. We term this as implicit saliency in deep neural networks. We calculate this implicit saliency using expectancy-mismatch hypothesis in an unsupervised fashion. Our experiments show that extracting saliency in this fashion provides comparable performance when measured against the state-of-art supervised algorithms. Additionally, the robustness outperforms those algorithms when we add large noise to the input images. Also, we show that semantic features contribute more than low-level features for human visual saliency detection. Based on these properties and performances, our proposed method greatly lowers the threshold for saliency detection in terms of required data and bridges the gap between human visual saliency and model saliency. © 2020 IEEE.","Deep Learning; Expectation Mismatch; Implicit Saliency; Recognition; Saliency","Deep neural networks; Eye tracking; Image processing; Semantics; Visualization; Deep architectures; Exposed to; Human visual; Input image; Low-level features; Saliency detection; Semantic features; Supervised algorithm; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85098671128
"Mitsuzum Y., Irie G., Kimura A., Nakazawa A.","57221269355;35173001300;7402712235;35807510800;","A Generative Self-Ensemble Approach to Simulated+Unsupervised Learning",2020,"Proceedings - International Conference on Image Processing, ICIP","2020-October",,"9191100","2151","2155",,,"10.1109/ICIP40778.2020.9191100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098668992&doi=10.1109%2fICIP40778.2020.9191100&partnerID=40&md5=0b6ebce36ac553b05d14d030e173ee8c","Ntt Corporation; Kyoto University, Japan","Mitsuzum, Y., Ntt Corporation; Irie, G., Ntt Corporation; Kimura, A., Ntt Corporation; Nakazawa, A., Kyoto University, Japan","In this paper, we consider Simulated and Unsupervised (S+U) learning which is a problem of learning from labeled synthetic and unlabeled real images. After translating the synthetic images to real ones, existing S+U learning methods use only the labeled synthetic images for training a predictor (e.g., a regression function) and ignore the target real images, which may result in unsatisfactory prediction performance. Our approach utilizes both synthetic and real images to train the predictor. The main idea of ours is to involve a self-ensemble learning framework into S+U learning. More specifically, we require the prediction results for an unlabeled real image to be consistent between 'teacher' and 'student' predictors, even after some perturbations are added to the image. Furthermore, aiming at generating diverse perturbations along the underlying data manifold, we introduce one-to-many image translation between synthetic and real images. Evaluation experiments on an appearance-based gaze estimation task demonstrate that the proposed ideas can improve the prediction accuracy and our full method can outperform existing S+U learning methods. © 2020 IEEE.","Image-to-Image Translation; SemiSupervised Learning; Simulated+Unsupervised Learning","Forecasting; Learning systems; Ensemble approaches; Ensemble learning; Evaluation experiments; Image translation; Prediction accuracy; Prediction performance; Regression function; Synthetic images; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85098668992
"Chen Q., Yu X., Liu N., Yuan X., Wang Z.","57211439388;55231004100;55871863700;57205728585;57218401950;","Personalized course recommendation based on eye-tracking technology and deep learning",2020,"Proceedings - 2020 IEEE 7th International Conference on Data Science and Advanced Analytics, DSAA 2020",,,"9260031","692","698",,1,"10.1109/DSAA49011.2020.00079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098011550&doi=10.1109%2fDSAA49011.2020.00079&partnerID=40&md5=0712b4e5523fe3fe15a99a46d0d45756","Shandong Normal University, School of Information Science and Engineering, Jinan, China","Chen, Q., Shandong Normal University, School of Information Science and Engineering, Jinan, China; Yu, X., Shandong Normal University, School of Information Science and Engineering, Jinan, China; Liu, N., Shandong Normal University, School of Information Science and Engineering, Jinan, China; Yuan, X., Shandong Normal University, School of Information Science and Engineering, Jinan, China; Wang, Z., Shandong Normal University, School of Information Science and Engineering, Jinan, China","With the rapid development of online courses, the requirements of personalized course recommendation have been increasing. The traditional collaborative filtering algorithm confronts with the challenge of cold start, which is difficult to settle on online course recommendation effectively. In this paper, we propose a novel click through rate (CTR) model for personalized online course recommendation, with discriminative user features, item features and cross features. The feature representation ability of the CTR model is improved and the serious challenge of cold start is alleviated. Furthermore, transfer learning is introduced to deal with the problem of insufficient data in models training. More specially, eye tracking technology is applied to capture the users' cognitive styles, which are visualized with the heat map and fixation point trajectory. Finally, the recommendation interface sent to the learners, according to the user's cognitive style. The experiments show that the novel CTR model improves the performance of the personalized online course recommendation. © 2020 IEEE.","Course recommendation; CTR prediction; Deep transfer learning; Personalization","Advanced Analytics; Collaborative filtering; Curricula; E-learning; Eye tracking; Transfer learning; Click-through rate; Cognitive styles; Collaborative filtering algorithms; Eye tracking technologies; Feature representation; Fixation point; Online course; Personalized course; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85098011550
"von Reumont F., Budke A.","55831264300;36080339800;","Strategies for successful learning with geographical comics: An eye-tracking study with young learners",2020,"Education Sciences","10","10","293","1","27",,,"10.3390/educsci10100293","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094570669&doi=10.3390%2feducsci10100293&partnerID=40&md5=e894b7d33b3e44fd0f61c585a4f8dca2","Department Didactics of Mathematics and Natural Sciences, Institute of Geography Education, University of Cologne, Gronewaldstr. 2, Cologne, 50931, Germany","von Reumont, F., Department Didactics of Mathematics and Natural Sciences, Institute of Geography Education, University of Cologne, Gronewaldstr. 2, Cologne, 50931, Germany; Budke, A., Department Didactics of Mathematics and Natural Sciences, Institute of Geography Education, University of Cologne, Gronewaldstr. 2, Cologne, 50931, Germany","Many studies report that comics are useful as learning material. However, there is little known about how learning with comics works. Based on previously established theories about multimedia learning, we conducted an eye-tracking experiment to examine learning about geography with a specially designed combination of comic and map which we call geo-comic. In our experiment, we show that our geo-comic fulfills many prerequisites for promoting deep learning. Thus, we establish guidelines for an effective design of geo-comics and recommend deploying comics in combination with maps in geography classes. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Comics; Eye-tracking; Geography education; Multimedia learning; Multiple perspectives; Text-picture combination",,Article,"Final","",Scopus,2-s2.0-85094570669
"Aresta G., Ferreira C., Pedrosa J., Araujo T., Rebelo J., Negrao E., Morgado M., Alves F., Cunha A., Ramos I., Campilho A.","57190949419;56204792400;57159246600;57190949608;57204512245;57212081287;57219445643;57219444857;7103392597;57191864412;57215190972;","Automatic Lung Nodule Detection Combined with Gaze Information Improves Radiologists' Screening Performance",2020,"IEEE Journal of Biomedical and Health Informatics","24","10","9007735","2894","2901",,3,"10.1109/JBHI.2020.2976150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092750056&doi=10.1109%2fJBHI.2020.2976150&partnerID=40&md5=c12e6ad9d618e413006fc4bf2c719240","Faculty of Engineering of University of Porto (FEUP), Porto, 4099-002, Portugal; Feup, Inesc Tec, Porto, Portugal; Inesc Tec, Porto, Portugal; Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Inesc Tec, University of Trás-os-Montes e Alto Douro (UTAD), Vila Real, 5001-801, Portugal","Aresta, G., Faculty of Engineering of University of Porto (FEUP), Porto, 4099-002, Portugal; Ferreira, C., Feup, Inesc Tec, Porto, Portugal; Pedrosa, J., Inesc Tec, Porto, Portugal; Araujo, T., Feup, Inesc Tec, Porto, Portugal; Rebelo, J., Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Negrao, E., Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Morgado, M., Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Alves, F., Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Cunha, A., Inesc Tec, University of Trás-os-Montes e Alto Douro (UTAD), Vila Real, 5001-801, Portugal; Ramos, I., Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Campilho, A., Inesc Tec, Porto, Portugal","Early diagnosis of lung cancer via computed tomography can significantly reduce the morbidity and mortality rates associated with the pathology. However, searching lung nodules is a high complexity task, which affects the success of screening programs. Whilst computer-aided detection systems can be used as second observers, they may bias radiologists and introduce significant time overheads. With this in mind, this study assesses the potential of using gaze information for integrating automatic detection systems in the clinical practice. For that purpose, 4 radiologists were asked to annotate 20 scans from a public dataset while being monitored by an eye tracker device, and an automatic lung nodule detection system was developed. Our results show that radiologists follow a similar search routine and tend to have lower fixation periods in regions where finding errors occur. The overall detection sensitivity of the specialists was 0.67\pm 0.07, whereas the system achieved 0.69. Combining the annotations of one radiologist with the automatic system significantly improves the detection performance to similar levels of two annotators. Filtering automatic detection candidates only for low fixation regions still significantly improves the detection sensitivity without increasing the number of false-positives. © 2013 IEEE.","clinical environment; computer-aided diagnosis; deep learning; eye-tracking; Lung cancer","Biological organs; Computerized tomography; Diagnosis; Diseases; Automatic Detection; Automatic detection systems; Clinical practices; Computer aided detection systems; Detection performance; Detection sensitivity; Lung nodule detection; Screening performance; Eye tracking; accuracy; algorithm; Article; artifact; cancer screening; clinical practice; communication skill; computer assisted tomography; computer simulation; deep learning; echography; electroencephalography; false positive result; filtration; gait; gaze; human; human experiment; image segmentation; joint function; lung nodule; lung volume; morbidity; mortality; quality of life; radiologist; signal noise ratio; thorax radiography; training; computer assisted diagnosis; diagnostic imaging; eye fixation; lung tumor; physiology; procedures; radiologist; x-ray computed tomography; Deep Learning; Eye-Tracking Technology; Fixation, Ocular; Humans; Lung Neoplasms; Radiographic Image Interpretation, Computer-Assisted; Radiologists; Tomography, X-Ray Computed",Article,"Final","",Scopus,2-s2.0-85092750056
"Southwell R., Gregg J., Bixler R., D'Mello S.K.","57216258729;57057103000;55789867400;14053463100;","What Eye Movements Reveal About Later Comprehension of Long Connected Texts",2020,"Cognitive Science","44","10","e12905","","",,,"10.1111/cogs.12905","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092684528&doi=10.1111%2fcogs.12905&partnerID=40&md5=0d704df949d22f8a7bba66651e51067d","Institute of Cognitive Science, University of Colorado, Boulder, United States; Department of Computer Science and Engineering, University of Notre Dame, United States","Southwell, R., Institute of Cognitive Science, University of Colorado, Boulder, United States; Gregg, J., Institute of Cognitive Science, University of Colorado, Boulder, United States; Bixler, R., Department of Computer Science and Engineering, University of Notre Dame, United States; D'Mello, S.K., Institute of Cognitive Science, University of Colorado, Boulder, United States","We know that reading involves coordination between textual characteristics and visual attention, but research linking eye movements during reading and comprehension assessed after reading is surprisingly limited, especially for reading long connected texts. We tested two competing possibilities: (a) the weak association hypothesis: Links between eye movements and comprehension are weak and short-lived, versus (b) the strong association hypothesis: The two are robustly linked, even after a delay. Using a predictive modeling approach, we trained regression models to predict comprehension scores from global eye movement features, using participant-level cross-validation to ensure that the models generalize across participants. We used data from three studies in which readers (Ns = 104, 130, 147) answered multiple-choice comprehension questions ~30 min after reading a 6,500-word text, or after reading up to eight 1,000-word texts. The models generated accurate predictions of participants' text comprehension scores (correlations between observed and predicted comprehension: 0.384, 0.362, 0.372, ps <.001), in line with the strong association hypothesis. We found that making more, but shorter fixations, consistently predicted comprehension across all studies. Furthermore, models trained on one study's data could successfully predict comprehension on the others, suggesting generalizability across studies. Collectively, these findings suggest that there is a robust link between eye movements and subsequent comprehension of a long connected text, thereby connecting theories of low-level eye movements with those of higher order text processing during reading. © 2020 Cognitive Science Society, Inc","Eye movements; Machine learning; Naturalistic text reading; Predictive modeling; Reading comprehension","comprehension; eye movement; female; human; male; reading; reproducibility; young adult; Comprehension; Eye Movements; Eye-Tracking Technology; Female; Humans; Male; Reading; Reproducibility of Results; Young Adult",Article,"Final","",Scopus,2-s2.0-85092684528
"Shi Y., Du J., Zhu Q.","57189999728;57219889677;57209806243;","The impact of engineering information format on task performance: Gaze scanning pattern analysis",2020,"Advanced Engineering Informatics","46",,"101167","","",,3,"10.1016/j.aei.2020.101167","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090331127&doi=10.1016%2fj.aei.2020.101167&partnerID=40&md5=7a3324aaff6899572e7cee45665daaa7","Department of Civil and Coastal Engineering, Engineering School of Sustainable Infrastructure and Environment (ESSIE), Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States","Shi, Y., Department of Civil and Coastal Engineering, Engineering School of Sustainable Infrastructure and Environment (ESSIE), Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States; Du, J., Department of Civil and Coastal Engineering, Engineering School of Sustainable Infrastructure and Environment (ESSIE), Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States; Zhu, Q., Department of Civil and Coastal Engineering, Engineering School of Sustainable Infrastructure and Environment (ESSIE), Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States","The emergence of new visualization technologies such as Virtual Reality (VR) and Augmented Reality (AR) had been widely implemented in the Architecture, Engineering, and Construction (AEC) industry. Although cumulative evidence pointed out a positive impact of these visualization technologies on construction task performance, there is still an obvious disagreement on the benefits or implications of these new visualization technologies, due to the lack of understanding of the mechanisms in which the visualization affects cognitive processes related to information processing. To obtain more evidence, this paper presents a human-subject experiment (n = 90) to investigate the impact of information format on the performance of an industrial pipeline maintenance task. The investigation centers around how different engineering information formats affect the attention patterns as a potential explanation for the changes in performance. A between-group experiment design was used where the participants were randomly assigned to one of the three groups (2D group, 3D group, and VR group) depending on what type of information was given to review the pipe operation instruction. After the review session, the participants were asked to perform the operation task in the virtual environment based on their memory. The results showed that the 3D and VR groups outperformed the 2D group in task performance. The analysis of eye-tracking data further indicated that the information format significantly changed the gaze scanning pattern when participants were reviewing the operational instructions. We also found that the task performance was correlated with eye-tracking features including gaze movement and pupil dilation. Our findings provided more evidence about the mechanisms in which new visualization technologies affect the attention patterns, helped resolve the current disagreement within the literature. In addition, a prediction model was proposed to use eye-tracking features to predict construction task performance. © 2020 Elsevier Ltd","Eye-tracking; Information format; Virtual reality; Working memory","Augmented reality; Eye movements; Predictive analytics; Virtual reality; Visualization; Architecture , engineering , and constructions; Cumulative evidence; Engineering information; Experiment design; Human subject experiments; Information format; Pipeline maintenance; Visualization technologies; Eye tracking",Article,"Final","",Scopus,2-s2.0-85090331127
"Shi Y., Zhu Y., Mehta R.K., Du J.","57189999728;57209917661;55413685300;57219889677;","A neurophysiological approach to assess training outcome under stress: A virtual reality experiment of industrial shutdown maintenance using Functional Near-Infrared Spectroscopy (fNIRS)",2020,"Advanced Engineering Informatics","46",,"101153","","",,10,"10.1016/j.aei.2020.101153","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089076628&doi=10.1016%2fj.aei.2020.101153&partnerID=40&md5=3c3d85c3f17b118a4868f9dc1d207db9","Department of Civil and Coastal Engineering, Engineering School of Sustainable Infrastructure and Environment (ESSIE), Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States; Department of Industrial & Systems Engineering, Texas A&M University, College Station, TX  77843, United States","Shi, Y., Department of Civil and Coastal Engineering, Engineering School of Sustainable Infrastructure and Environment (ESSIE), Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States; Zhu, Y., Department of Industrial & Systems Engineering, Texas A&M University, College Station, TX  77843, United States; Mehta, R.K., Department of Industrial & Systems Engineering, Texas A&M University, College Station, TX  77843, United States; Du, J., Department of Civil and Coastal Engineering, Engineering School of Sustainable Infrastructure and Environment (ESSIE), Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States","Shutdown maintenance, i.e., turning off a facility for a short period for renewal or replacement operations is a highly stressful task. With the limited time and complex operation procedures, human stress is a leading risk. Especially shutdown maintenance workers often need to go through excessive and stressful on-site trainings to digest complex operation information in limited time. The challenge is that workers’ stress status and task performance are hard to predict, as most trainings are only assessed after the shutdown maintenance operation is finished. A proactive assessment or intervention is needed to evaluate workers’ stress status and task performance during the training to enable early warning and interventions. This study proposes a neurophysiological approach to assess workers’ stress status and task performance under different virtual training scenarios. A Virtual Reality (VR) system integrated with the eye-tracking function was developed to simulate the power plant shutdown maintenance operations of replacing a heat exchanger in both normal and stressful scenarios. Meanwhile, a portable neuroimaging device – Functional Near-Infrared Spectroscopy (fNIRS) was also utilized to collect user's brain activities by measuring hemodynamic responses associated with neuron behavior. A human–subject experiment (n = 16) was conducted to evaluate participants’ neural activity patterns and physiological metrics (gaze movement) related to their stress status and final task performance. Each participant was required to review the operational instructions for a pipe maintenance task for a short period and then perform the task based on their memory in both normal and stressful scenarios. Our experiment results indicated that stressful training had a strong impact on participants’ neural connectivity patterns and final performance, suggesting the use of stressors during training to be an important and useful control factors. We further found significant correlations between gaze movement patterns in review phase and final task performance, and between the neural features and final task performance. In summary, we proposed a variety of supervised machine learning classification models that use the fNIRS data in the review session to estimate individual's task performance. The classification models were validated with the k-fold (k = 10) cross-validation method. The Random Forest classification model achieved the best average classification accuracy (80.38%) in classifying participants’ task performance compared to other classification models. The contribution of our study is to help establish the knowledge and methodological basis for an early warning and estimating system of the final task performance based on the neurophysiological measures during the training for industrial operations. These findings are expected to provide more evidence about an early performance warning and prediction system based on a hybrid neurophysiological measure method, inspiring the design of a cognition-driven personalized training system for industrial workers. © 2020 Elsevier Ltd","Eye-tracking; fNIRS; Shutdown maintenance training; Virtual reality","Behavioral research; Brain; Decision trees; E-learning; Eye tracking; Infrared devices; Maintenance; Near infrared spectroscopy; Neurons; Occupational risks; Plant shutdowns; Supervised learning; Virtual reality; Classification accuracy; Cross-validation methods; Functional near-infrared spectroscopy (fnirs); Maintenance operations; Neural activity patterns; Neurophysiological measures; Random forest classification; Supervised machine learning; Functional neuroimaging",Article,"Final","",Scopus,2-s2.0-85089076628
"Shi P., Billeter M., Eisemann E.","55858760100;23092768600;35304781200;","SalientGaze: Saliency-based gaze correction in virtual reality",2020,"Computers and Graphics (Pergamon)","91",,,"83","94",,2,"10.1016/j.cag.2020.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088662850&doi=10.1016%2fj.cag.2020.06.007&partnerID=40&md5=89fbc20657a13b144a09e2373fbbfca1","Computer Graphics and Visualization Group, Delft University of Technology, Delft, 2628 XE, Netherlands; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, 410073, China","Shi, P., Computer Graphics and Visualization Group, Delft University of Technology, Delft, 2628 XE, Netherlands, Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, 410073, China; Billeter, M., Computer Graphics and Visualization Group, Delft University of Technology, Delft, 2628 XE, Netherlands; Eisemann, E., Computer Graphics and Visualization Group, Delft University of Technology, Delft, 2628 XE, Netherlands","Eye-tracking with gaze estimation is a key element in many applications, ranging from foveated rendering and user interaction to behavioural analysis and usage metrics. For virtual reality, eye-tracking typically relies on near-eye cameras that are mounted in the VR headset. Such methods usually involve an initial calibration to create a mapping from eye features to a gaze position. However, the accuracy based on the initial calibration degrades when the position of the headset relative to the users’ head changes; this is especially noticeable when users readjust the headset for comfort or even completely remove it for a short while. We show that a correction of such shifts can be achieved via 2D drift vectors in eye space. Our method estimates these drifts by extracting salient cues from the shown virtual environment to determine potential gaze directions. Our solution can compensate for HMD shifts, even those arising from taking off the headset, which enables us to eliminate reinitialization steps. © 2020 Elsevier Ltd","Drift estimation; Eye-tracking; Headsets shifts; Saliency; Stereo; Virtual reality","Behavioral research; Calibration; Eye tracking; Vector spaces; Eye camera; Gaze direction; Gaze estimation; Key elements; Reinitialization; User interaction; Virtual reality",Article,"Final","",Scopus,2-s2.0-85088662850
"Liu F., Liu D., Tian J., Xie X., Yang X., Wang K.","57089319900;57211581942;7401636162;57216303850;56967210500;57202811601;","Cascaded one-shot deformable convolutional neural networks: Developing a deep learning model for respiratory motion estimation in ultrasound sequences",2020,"Medical Image Analysis","65",,"101793","","",,4,"10.1016/j.media.2020.101793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088385162&doi=10.1016%2fj.media.2020.101793&partnerID=40&md5=93bacf08d7558e6b28d9a6d409ded8eb","CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of SciencesBeijing  100190, China; Department of the Artificial Intelligence Technology, University of Chinese Academy of SciencesBeijing  100049, China; Department of Medical Ultrasonics, Institute of Diagnostic and Interventional Ultrasound, The First Affiliated Hospital of Sun Yat-sen University, Guangzhou, 510080, China; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang UniversityBeijing  100191, China","Liu, F., CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of SciencesBeijing  100190, China, Department of the Artificial Intelligence Technology, University of Chinese Academy of SciencesBeijing  100049, China; Liu, D., Department of Medical Ultrasonics, Institute of Diagnostic and Interventional Ultrasound, The First Affiliated Hospital of Sun Yat-sen University, Guangzhou, 510080, China; Tian, J., CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of SciencesBeijing  100190, China, Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang UniversityBeijing  100191, China; Xie, X., Department of Medical Ultrasonics, Institute of Diagnostic and Interventional Ultrasound, The First Affiliated Hospital of Sun Yat-sen University, Guangzhou, 510080, China; Yang, X., CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of SciencesBeijing  100190, China; Wang, K., CAS Key Laboratory of Molecular Imaging, Institute of Automation, Chinese Academy of SciencesBeijing  100190, China, Department of the Artificial Intelligence Technology, University of Chinese Academy of SciencesBeijing  100049, China","Improving the quality of image-guided radiation therapy requires the tracking of respiratory motion in ultrasound sequences. However, the low signal-to-noise ratio and the artifacts in ultrasound images make it difficult to track targets accurately and robustly. In this study, we propose a novel deep learning model, called a Cascaded One-shot Deformable Convolutional Neural Network (COSD-CNN), to track landmarks in real time in long ultrasound sequences. Specifically, we design a cascaded Siamese network structure to improve the tracking performance of CNN-based methods. We propose a one-shot deformable convolution module to enhance the robustness of the COSD-CNN to appearance variation in a meta-learning manner. Moreover, we design a simple and efficient unsupervised strategy to facilitate the network's training with a limited number of medical images, in which many corner points are selected from raw ultrasound images to learn network features with high generalizability. The proposed COSD-CNN has been extensively evaluated on the public Challenge on Liver UltraSound Tracking (CLUST) 2D dataset and on our own ultrasound image dataset from the First Affiliated Hospital of Sun Yat-sen University (FSYSU). Experiment results show that the proposed model can track a target through an ultrasound sequence with high accuracy and robustness. Our method achieves new state-of-the-art performance on the CLUST 2D benchmark set, indicating its strong potential for application in clinical practice. © 2020","Cascaded Siamese network; One-shot deformable convolution; Respiratory motion estimation; Ultrasound sequence","Benchmarking; Convolution; Convolutional neural networks; Deformation; Image enhancement; Learning systems; Medical imaging; Motion estimation; Motion tracking; Respiratory mechanics; Signal to noise ratio; Ultrasonics; Clinical practices; Image-guided radiation therapy; Low signal-to-noise ratio; Network structures; Respiratory motions; State-of-the-art performance; Tracking performance; Ultrasound images; Deep learning; article; clinical practice; convolutional neural network; deep learning; diagnostic test accuracy study; eye tracking; liver; motion; sun; ultrasound; echography; human; image guided radiotherapy; motion; Deep Learning; Humans; Motion; Neural Networks, Computer; Radiotherapy, Image-Guided; Ultrasonography",Article,"Final","",Scopus,2-s2.0-85088385162
"Jothi Prabha A., Bhargavi R.","57204137234;36661898100;","Predictive Model for Dyslexia from Fixations and Saccadic Eye Movement Events",2020,"Computer Methods and Programs in Biomedicine","195",,"105538","","",,4,"10.1016/j.cmpb.2020.105538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086067963&doi=10.1016%2fj.cmpb.2020.105538&partnerID=40&md5=b6f508f89e2f579d3bda760991a2b5d8","School of Computing Sciences and Engineering Department, Vellore Institute of Technology, Chennai, 127, India","Jothi Prabha, A., School of Computing Sciences and Engineering Department, Vellore Institute of Technology, Chennai, 127, India; Bhargavi, R., School of Computing Sciences and Engineering Department, Vellore Institute of Technology, Chennai, 127, India","Background: Dyslexia is a disorder characterized by difficulty in reading such as poor speech and sound recognition. They have less capability to relate letters and form words and exhibit poor reading comprehension. Eye-tracking methodologies play a major role in analyzing human cognitive processing. Dyslexia is not a visual impairment disorder but it's a difficulty in phonological processing and word decoding. These difficulties are reflected in their eye movement patterns during reading. Objective: The disruptive eye movement helps us to use eye-tracking methodologies for identifying dyslexics. Methods: In this paper, a small set of eye movement features have been proposed that contribute more to distinguish between dyslexics and non-dyslexics by machine learning models. Features related to eye movement events such as fixations and saccades are detected using statistical measures, dispersion threshold identification (I-DT) and velocity threshold identification (I-VT) algorithms. These features were further analyzed using various machine learning algorithms such as Particle Swarm Optimization (PSO) based SVM Hybrid Kernel (Hybrid SVM – PSO), Support Vector Machine (SVM), Random Forest classifier (RF), Logistic Regression (LR) and K-Nearest Neighbor (KNN) for classification of dyslexics and non-dyslexics. Results: The accuracy achieved using the Hybrid SVM –PSO model is 95.6 %. The best set of features that gave high accuracy are average no of fixations, average fixation gaze duration, average saccadic movement duration, total number of saccadic movements, and average number of fixations. Conclusion: It is observed that eye movement features detected using velocity-based algorithms performed better than those detected by dispersion-based algorithms and statistical measures. © 2020 Elsevier B.V.","Dispersion-threshold identification algorithms; Hybrid SVM–PSO; K-Nearest Neighbor; Logistic Regression; Random Forest Classifier; Statistical features; Support Vector Machine; Velocity-threshold identification algorithm","Decision trees; Dispersions; Eye tracking; Feature extraction; Learning algorithms; Learning systems; Logistic regression; Nearest neighbor search; Particle swarm optimization (PSO); Speech; Speech recognition; Support vector machines; Support vector regression; Eye movement patterns; K nearest neighbor (KNN); Machine learning models; Phonological processing; Random forest classifier; Reading comprehension; Saccadic eye movements; Statistical measures; Eye movements; Article; child; cross validation; detection algorithm; diagnostic accuracy; diagnostic test accuracy study; differential diagnosis; dispersion threshold identification algorithm; dyslexia; external validity; eye fixation; eye position; eye tracking; feature detection; feature extraction; feature selection; gaze; human; instrument validation; intermethod comparison; k nearest neighbor; logistic regression analysis; major clinical study; particle swarm optimization; predictive value; principal component analysis; random forest; recursive feature elimination; saccadic eye movement; sensitivity and specificity; statistical analysis; support vector machine; velocity threshold identification algorithm; algorithm; eye movement; reading; Algorithms; Dyslexia; Eye Movements; Humans; Reading; Saccades",Article,"Final","",Scopus,2-s2.0-85086067963
"Xia C., Chen K., Li K., Li H.","56102195500;57222360543;57222365001;57222355593;","Identification of autism spectrum disorder via an eye-tracking based representation learning model",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"3440078","59","65",,,"10.1145/3440067.3440078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102414939&doi=10.1145%2f3440067.3440078&partnerID=40&md5=1e87b1055b9095d0fc16b893ef555aca","School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Department of Engineering, King's College London, London, WC2R 2LS, United Kingdom; Children Rehabilitation Center, Shaanxi Rehabilitation Hospital, Xi'an, 710065, China","Xia, C., School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Chen, K., Department of Engineering, King's College London, London, WC2R 2LS, United Kingdom; Li, K., Children Rehabilitation Center, Shaanxi Rehabilitation Hospital, Xi'an, 710065, China; Li, H., Children Rehabilitation Center, Shaanxi Rehabilitation Hospital, Xi'an, 710065, China","Autism spectrum disorder (ASD) is a lifelong developmental disorder characterized by repetitive, restricted behavior and deficits in communication and social interactions. Early diagnosis and intervention can significantly reduce the hazards of the disease. However, the lack of effective clinical resources for early diagnosis has been a long-standing problem. In response to this problem, we apply the recent advances in deep neural networks on eye-tracking data in this study to classify children with and without ASD. First, we record the eye movement data of 31 children with ASD and 43 typically developing children on four categories of stimuli to construct an eye-tracking data set for ASD identification. Based on the collected eye movement data, we extract the dynamic saccadic scanpath on each image for all subjects. Then, we utilize the hierarchical features learned from a convolutional neural network and multidimensional visual salient features to encode the scanpaths. Next, we adopt the support vector machine to learn the relationship between encoded pieces of scanpaths and the labels from the two classes via supervised learning. Finally, we derive the scores of each scanpath and make the final judgment for each subject according to the scores on all scanpaths. The experimental results have shown that the proposed model has a maximum classification accuracy of 94.28% in the diagnostic tests. Based on existing research and calculation models, dynamic saccadic scanpaths can provide promising findings and implications for ASD early detection. Furthermore, integrating more information of the scanpaths into the model and developing a more in-depth description of scanpaths can improve the recognition accuracy. We hope our work can contribute to the development of multimodal approaches in the early detection and diagnosis of ASD. © 2020 ACM.","Autism Spectrum Disorder (ASD); Eye-Tracking; Machine Learning; Representation Learning; Saccadic Scanpath; Support Vector Machine (SVM)","Bioinformatics; Convolutional neural networks; Deep learning; Deep neural networks; Diagnosis; Diseases; Eye movements; Learning systems; Support vector machines; Autism spectrum disorders; Classification accuracy; Detection and diagnosis; Developmental disorders; Hierarchical features; Multi-modal approach; Recognition accuracy; Social interactions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85102414939
"Boutros F., Damer N., Raja K., Ramachandra R., Kirchbuchner F., Kuijper A.","57205379838;50861109400;57188866050;57190835798;57031859600;56131137100;","On benchmarking iris recognition within a head-mounted display for AR/VR applications",2020,"IJCB 2020 - IEEE/IAPR International Joint Conference on Biometrics",,,"9304919","","",,4,"10.1109/IJCB48548.2020.9304919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099719694&doi=10.1109%2fIJCB48548.2020.9304919&partnerID=40&md5=7cec54f18d42f7d2a183113d643747ad","Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; The Norwegian Colour and Visual Computing Laboratory, NTNU, Gjovik, Norway; Norwegian Biometrics Laboratory, NTNU, Gjovik, Norway","Boutros, F., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Damer, N., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Raja, K., The Norwegian Colour and Visual Computing Laboratory, NTNU, Gjovik, Norway, Norwegian Biometrics Laboratory, NTNU, Gjovik, Norway; Ramachandra, R., Norwegian Biometrics Laboratory, NTNU, Gjovik, Norway; Kirchbuchner, F., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Kuijper, A., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany","Augmented and virtual reality is being deployed in different fields of applications. Such applications might involve accessing or processing critical and sensitive information, which requires strict and continuous access control. Given that Head-Mounted Displays (HMD) developed for such applications commonly contains internal cameras for gaze tracking purposes, we evaluate the suitability of such setup for verifying the users through iris recognition. In this work, we first evaluate a set of iris recognition algorithms suitable for HMD devices by investigating three well-established handcrafted feature extraction approaches, and to complement it, we also present the analysis using four deep learning models. While taking into consideration the minimalistic hardware requirements of stand-alone HMD, we employ and adapt a recently developed miniature segmentation model (EyeMMS) for segmenting the iris. Further, to account for non-ideal and non-collaborative capture of iris, we define a new iris quality metric that we termed as Iris Mask Ratio (IMR) to quantify the iris recognition performance. Motivated by the performance of iris recognition, we also propose the continuous authentication of users in a non-collaborative capture setting in HMD. Through the experiments on a publicly available OpenEDS dataset, we show that performance with EER = 5% can be achieved using deep learning methods in a general setting, along with high accuracy for continuous user authentication. © 2020 IEEE.",,"Authentication; Benchmarking; Biometrics; Deep learning; Eye tracking; Learning systems; Street traffic control; Augmented and virtual realities; Continuous authentications; Head mounted displays; Iris recognition algorithm; Quality metrices; Segmentation models; Sensitive informations; User authentication; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85099719694
"Lee K.F., Chen Y.L., Yu C.W., Wu C.H.","56413130800;35322122400;54379325600;57203205774;","The Eye Tracking and Gaze Estimation System by Low Cost Wearable Devices",2020,"2020 IEEE International Conference on Consumer Electronics - Taiwan, ICCE-Taiwan 2020",,,"9258009","","",,,"10.1109/ICCE-Taiwan49838.2020.9258009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098465558&doi=10.1109%2fICCE-Taiwan49838.2020.9258009&partnerID=40&md5=21208bd82f557dc7aab55340bdbd1fcd","National Taipei University of Technology, Taipei, Taiwan","Lee, K.F., National Taipei University of Technology, Taipei, Taiwan; Chen, Y.L., National Taipei University of Technology, Taipei, Taiwan; Yu, C.W., National Taipei University of Technology, Taipei, Taiwan; Wu, C.H., National Taipei University of Technology, Taipei, Taiwan","This study develop a wearable eye tracking and gaze estimation low cost devices. This devices use infrared camera and design by integration of elastic mechanism adaptable. The use of cheap endoscope camera for mobile and 3D print technique for building up the devices, which results in be a low cost solution. This device can effectively extract and estimate pupil ellipse from few camera-captured samples of an eye and compute the corresponding 3D eye model. And this device use multiple point's calibration method to solve the related polynomial formula for future angle-to-gaze mapping. This wearable device is a low-cost which can be used for virtual reality and auxiliary equipment. © 2020 IEEE.",,"3D modeling; 3D printers; Auxiliary equipment; Cameras; Cost estimating; Wearable technology; Calibration method; Elastic mechanism; Gaze estimation; Infra-red cameras; Low-cost devices; Low-cost solution; Multiple points; Wearable devices; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85098465558
"Yeh S.-C., Lin C.-H., Lin S.-K., Wu E.H., Tsai M.-C.","14619911100;57141487400;57221164030;55366211300;57221166436;","A Virtual Reality Based System for Drug Addiction and Diagnosis",2020,"2020 IEEE International Conference on Consumer Electronics - Taiwan, ICCE-Taiwan 2020",,,"9258161","","",,,"10.1109/ICCE-Taiwan49838.2020.9258161","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098460440&doi=10.1109%2fICCE-Taiwan49838.2020.9258161&partnerID=40&md5=c55a3ebd9b8b14a6712f9cfba016ef46","Csie, National Central University, Taipei, Taiwan; Kaohsiung Chang Gung Memorial Hospital","Yeh, S.-C., Csie, National Central University, Taipei, Taiwan; Lin, C.-H., Csie, National Central University, Taipei, Taiwan; Lin, S.-K., Csie, National Central University, Taipei, Taiwan; Wu, E.H., Csie, National Central University, Taipei, Taiwan; Tsai, M.-C., Kaohsiung Chang Gung Memorial Hospital","By integrating virtual reality (VR) technique with flavor simulator, our research intended to create an immersed virtual environment (VE) that is composed of multiple VR tasks with varied intensities of drug-temptation therefore to gradually induce the carving of addiction patients. Moreover, multi-model sensors, including electrocardiography (ECG), electroencephalography (EEG), galvanic skin response (GSR) and eye tracking, were synchronized with the VE system and utilized to measure the neuro-behavior of the addiction patients before, during and after the exposure to the virtual environment. Further, data of multi-model neuro behaviors and VR task performance were analyzed by statistics and machine learning method in order to evaluate the intensity of craving induced and judge the dependence on the drug so as to make an assessment to the addiction patients. © 2020 IEEE.",,"Electrocardiography; Electroencephalography; Electrophysiology; Eye tracking; Learning systems; Drug addiction; Galvanic skin response; Machine learning methods; Multi model; Task performance; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85098460440
"Musabini A., Chetitah M.","57215655070;57219502077;","Heatmap-based method for estimating drivers’ cognitive distraction",2020,"Proceedings of 2020 IEEE 19th International Conference on Cognitive Informatics and Cognitive Computing, ICCI*CC 2020",,,"9450216","179","186",,,"10.1109/ICCICC50026.2020.9450216","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112865911&doi=10.1109%2fICCICC50026.2020.9450216&partnerID=40&md5=1320dd53f7c89835e2dbfb458057b5cc","InnoCoRe, Valeo, Comfort and Driving Assistance, Bobigny, France","Musabini, A., InnoCoRe, Valeo, Comfort and Driving Assistance, Bobigny, France; Chetitah, M., InnoCoRe, Valeo, Comfort and Driving Assistance, Bobigny, France","In order to increase road safety, among the visual ? Cognitive distraction: taking the mind off the driving and manual distractions, modern intelligent vehicles need also to detect cognitive distracted driving (i.e., the driver’s mind task. wandering). In this study, the influence of cognitive processes Passive safety systems to combat visual and manual dis-on the driver’s gaze behavior is explored. A novel image-based traction are already widely used in commercial vehicles. representation of the driver’s eye-gaze dispersion is proposed These systems track the driver’s eye-gaze. Once the driver to estimate cognitive distraction. Data are collected on open highway roads, with a tailored protocol to create cognitive looks anywhere other than the road, they are judged to be distraction. The visual difference of created shapes shows that distracted [4]. The downside of this is that if the driver is a driver explores a wider area in neutral driving compared looking at the road but daydreaming (a phenomenon known to distracted driving. Support vector machine (SVM)-based as the mind wandering [5]), they are misjudged as attentive. classifiers are trained, and 85.2% of accuracy is achieved for Cognitive distracted driving is a dangerous situation which a two-class problem, even with a small dataset. Thus, the proposed method has the discriminative power to recognize vehicles should be able to detect to increase road safety. It cognitive distraction using gaze information. Finally, this work has been highlighted as one of the issues to resolve in the details how this image-based representation could be useful for European New Car Assessment Programme (Euro NCAP) other cases of distracted driving detection. 2022 requirements (driver inattentiveness) [6]. ©2020 IEEE","Affective computing; Cognitive distraction; Computer vision; Distracted driving; Eye-gaze; Human-centered artificial intelligence; Machine learning; Pattern recognition","Accident prevention; Behavioral research; Classification (of information); Commercial vehicles; Motor transportation; Roads and streets; Support vector machines; Cognitive distractions; Cognitive process; Dangerous situations; Discriminative power; Gaze behavior; Image-based representation; Passive safety systems; Visual differences; Cognitive systems",Conference Paper,"Final","",Scopus,2-s2.0-85112865911
"Li F., Chang D., Liu Y., Cui J., Feng S., Huang N., Chen C.-H., Sourina O.","57196404325;55987550100;36809950300;56479735300;56393590300;57219454003;25921980900;57204345367;","Evaluation of humanoid robot design based on global eye-tracking metrics",2020,"Advances in Transdisciplinary Engineering","12",,,"241","249",,,"10.3233/ATDE200082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092779875&doi=10.3233%2fATDE200082&partnerID=40&md5=7ecd00d54b24e0fe2bc4e1c4973a0443","Fraunhofer Singapore, Singapore; School of Design, Shanghai Jiao Tong University, Shanghai, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Huazhong University of Science and Technology, China; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore","Li, F., Fraunhofer Singapore, Singapore; Chang, D., School of Design, Shanghai Jiao Tong University, Shanghai, China; Liu, Y., Fraunhofer Singapore, Singapore; Cui, J., Fraunhofer Singapore, Singapore; Feng, S., Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Huang, N., Huazhong University of Science and Technology, China; Chen, C.-H., School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; Sourina, O., Fraunhofer Singapore, Singapore","The first impression of robot appearance normally affects the interaction with physical robots. Hence, it is critically important to evaluate the humanoid robot appearance design. This study towards evaluating humanoid robot design based on global eye-tracking metrics. Two methods are selected to extract global eye-tracking metrics, including bin-analysis-based entropy and approximate entropy. The data are collected from an eye-tracking experiment, where 20 participants evaluate 12 humanoid robot appearance designs with their eye movements recorded. The humanoid robots are evaluated from five aspects, namely smartness, friendliness, pleasure, arousal, and dominance. The results show that the entropy of fixation duration and velocity, approximate entropy of saccades amplitude are positively associated with the subjective feelings induced by robot appearance. These findings can aid in better understanding the first impression of human-robot interaction and enable the eye-tracking-based evaluation of humanoid robot design. By combining the theory of design and bio-signals analysis, the study contributes to the field of Transdisciplinary Engineering. © 2020 The authors and IOS Press.","Evaluation; Eye-tracking; Global metrics; Humanoid robot design","Anthropomorphic robots; Entropy; Eye movements; Human robot interaction; Machine design; Approximate entropy; Biosignals; First impressions; Fixation duration; Humanoid robot; Physical robots; Robot appearance; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85092779875
"Amadori P.V., Fischer T., Wang R., Demiris Y.","56703112800;57190126084;57189039099;6506125343;","Decision Anticipation for Driving Assistance Systems",2020,"2020 IEEE 23rd International Conference on Intelligent Transportation Systems, ITSC 2020",,,"9294216","","",,2,"10.1109/ITSC45102.2020.9294216","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099667660&doi=10.1109%2fITSC45102.2020.9294216&partnerID=40&md5=0167e13a074b83f53d015c6c28c6e443","Imperial College London, Personal Robotics Laboratory, Department of Electrical and Electronic EngineeringSW7 2BT, United Kingdom","Amadori, P.V., Imperial College London, Personal Robotics Laboratory, Department of Electrical and Electronic EngineeringSW7 2BT, United Kingdom; Fischer, T., Imperial College London, Personal Robotics Laboratory, Department of Electrical and Electronic EngineeringSW7 2BT, United Kingdom; Wang, R., Imperial College London, Personal Robotics Laboratory, Department of Electrical and Electronic EngineeringSW7 2BT, United Kingdom; Demiris, Y., Imperial College London, Personal Robotics Laboratory, Department of Electrical and Electronic EngineeringSW7 2BT, United Kingdom","Anticipating the correctness of imminent driver decisions is a crucial challenge in advanced driving assistance systems and has the potential to lead to more reliable and safer human-robot interactions. In this paper, we address the task of decision correctness prediction in a driver-in-the-loop simulated environment using unobtrusive physiological signals, namely, eye gaze and head pose. We introduce a sequence-to-sequence based deep learning model to infer the driver's likelihood of making correct/wrong decisions based on the corresponding cognitive state. We provide extensive experimental studies over multiple baseline classification models on an eye gaze pattern and head pose dataset collected from simulated driving. Our results show strong correlates between the physiological data and decision correctness, and that the proposed sequential model reliably predicts decision correctness from the driver with 80% precision and 72% recall. We also demonstrate that our sequential model performs well in scenarios where early anticipation of correctness is critical, with accurate predictions up to two seconds before a decision is performed. © 2020 IEEE.",,"Behavioral research; Classification (of information); Deep learning; Intelligent systems; Man machine systems; Physiological models; Physiology; Social robots; Accurate prediction; Classification models; Driving assistance systems; Physiological data; Physiological signals; Sequential model; Simulated driving; Simulated environment; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85099667660
"Fan D., Liu Y., Chen X., Meng F., Liu X., Ullah Z., Cheng W., Liu Y., Huang Q.","57191521115;57218958175;55739132200;36621115700;54405655300;57218956419;57214750875;57203136059;24448166100;","Eye gaze based 3d triangulation for robotic bionic eyes",2020,"Sensors (Switzerland)","20","18","5271","1","24",,,"10.3390/s20185271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090877690&doi=10.3390%2fs20185271&partnerID=40&md5=0b4a08c09fb0108fbd31ed61b8896d4b","School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, 100081, China; AIPARK, Zhangjiakou, 075000, China; Beijing Advanced Innovation Center for Intelligent Robots and Systems, Beijing Institute of Technology, Beijing, 100081, China; Key Laboratory of Biomimetic Robots and Systems, Beijing Institute of Technology, Ministry of Education, Beijing, 100081, China; Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Shatin, NT  999077, Hong Kong","Fan, D., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, 100081, China; Liu, Y., AIPARK, Zhangjiakou, 075000, China; Chen, X., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, 100081, China, Beijing Advanced Innovation Center for Intelligent Robots and Systems, Beijing Institute of Technology, Beijing, 100081, China; Meng, F., Beijing Advanced Innovation Center for Intelligent Robots and Systems, Beijing Institute of Technology, Beijing, 100081, China, Key Laboratory of Biomimetic Robots and Systems, Beijing Institute of Technology, Ministry of Education, Beijing, 100081, China; Liu, X., Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; Ullah, Z., Beijing Advanced Innovation Center for Intelligent Robots and Systems, Beijing Institute of Technology, Beijing, 100081, China; Cheng, W., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, 100081, China; Liu, Y., Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Shatin, NT  999077, Hong Kong; Huang, Q., Beijing Advanced Innovation Center for Intelligent Robots and Systems, Beijing Institute of Technology, Beijing, 100081, China, Key Laboratory of Biomimetic Robots and Systems, Beijing Institute of Technology, Ministry of Education, Beijing, 100081, China","Three-dimensional (3D) triangulation based on active binocular vision has increasing amounts of applications in computer vision and robotics. An active binocular vision system with non-fixed cameras needs to calibrate the stereo extrinsic parameters online to perform 3D triangulation. However, the accuracy of stereo extrinsic parameters and disparity have a significant impact on 3D triangulation precision. We propose a novel eye gaze based 3D triangulation method that does not use stereo extrinsic parameters directly in order to reduce the impact. Instead, we drive both cameras to gaze at a 3D spatial point P at the optical center through visual servoing. Subsequently, we can obtain the 3D coordinates of P through the intersection of the two optical axes of both cameras. We have performed experiments to compare with previous disparity based work, named the integrated two-pose calibration (ITPC) method, using our robotic bionic eyes. The experiments show that our method achieves comparable results with ITPC. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","3D coordinates estimation; 3D triangulation; Active binocular vision; Eye gaze; Robotic bionic eyes","Binocular vision; Bionics; Cameras; Robotics; Stereo vision; Triangulation; Visual servoing; 3-d triangulations; 3D coordinates; Binocular vision systems; Extrinsic parameter; Optical center; Spatial points; Threedimensional (3-d); Triangulation-based; Stereo image processing; binocular vision; eye fixation; robotics; three-dimensional imaging; visual prosthesis; Fixation, Ocular; Imaging, Three-Dimensional; Robotics; Vision, Binocular; Visual Prosthesis",Article,"Final","",Scopus,2-s2.0-85090877690
"Lin Z., Xue P., Wang C.","57220786737;57201987957;36651948200;","Research on data fusion of pilot's los and target information",2020,"Proceedings - 2020 International Conference on Computer Network, Electronic and Automation, ICCNEA 2020",,,"9239767","43","48",,,"10.1109/ICCNEA50255.2020.00019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097625661&doi=10.1109%2fICCNEA50255.2020.00019&partnerID=40&md5=bda806ab37c1bf55ec1b168a285c61fa","School of Computer Science and Engineering, Xi'An Technological University, Xi'an, China","Lin, Z., School of Computer Science and Engineering, Xi'An Technological University, Xi'an, China; Xue, P., School of Computer Science and Engineering, Xi'An Technological University, Xi'an, China; Wang, C., School of Computer Science and Engineering, Xi'An Technological University, Xi'an, China","Aiming at the problem that the precision of gaze information data extraction is not high in the process of pilot's; flight, this paper puts forward the research on the fusion of the data of the landing point of sight and the target information, and feeds back the information through human-computer interaction to obtain the accurate target information. First, the non wearable eye tracker is used to track the line of sight, estimate the fixation point, obtain the fixation point coordinates, and match with the position coordinates of the instrument panel to get the dial where the fixation point is. Then, data fusion is carried out between the instrument panel where the fixation point is located and the flight parameters to obtain the information displayed on the instrument panel. Finally, the fixation information of the instrument panel is used as the input data to support the identification of the pilot's intention. Secondly, data fusion is carried out between the pilot's attention information and reaction time information, and a large amount of data is used as the input data. Machine learning is adopted Methods after information fusion, the decision- making of pilots was predicted. © 2020 IEEE.","Data Fusion; Gaze Information; Human Computer Interaction; Line of Sight Landing; Reaction Time Information","Computer networks; Data fusion; Decision making; Eye tracking; Input output programs; Instrument panels; Fixation point; Flight parameters; Information data; Landing points; Large amounts; Position coordinates; Target information; Time information; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85097625661
"Wibirama S., Sidhawara A.G.P., Lukhayu Pritalia G., Adji T.B.","26654457700;57205337529;57204724943;24734043700;","A Survey of Learning Style Detection Method using Eye-Tracking and Machine Learning in Multimedia Learning",2020,"2020 International Symposium on Community-Centric Systems, CcS 2020",,,"9231447","","",,,"10.1109/CcS49175.2020.9231447","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096517365&doi=10.1109%2fCcS49175.2020.9231447&partnerID=40&md5=f6029c9bce0befd08b35b48a535af72a","Faculty of Engineering, Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia","Wibirama, S., Faculty of Engineering, Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia; Sidhawara, A.G.P., Faculty of Engineering, Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia; Lukhayu Pritalia, G., Faculty of Engineering, Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia; Adji, T.B., Faculty of Engineering, Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia","Current utilization of multimedia learning environment focuses on student-centered approach. This approach is based on a theory stating that learning styles affect individuals in information processing. Based on prior works, there are three main approaches to distinguish learning styles: conventional approach - such as interview and self-reporting, artificial-intelligence-based approach, and sensor-based approach. Unfortunately, there is no comparative analysis that addresses strengths and limitations of these approaches. Thus, there is no information on how and when to use these approaches appropriately. To address this limitation, we present a brief literature review of several studies in distinguishing learning styles, including their strengths and limitations. We also present insights on potential methods of detecting learning styles in multimedia learning based on eye movement data and machine learning algorithms. Our paper is useful as a guideline for developing intelligent e-learning systems based on eye tracking and machine learning. © 2020 IEEE.","cognitive style; eye-tracking; learning style; machine learning; multimedia learning","Computer aided instruction; E-learning; Eye movements; Eye tracking; Machine learning; Comparative analysis; Conventional approach; Detection methods; Eye movement datum; Intelligent e-learning systems; Literature reviews; Multi-media learning; Student-centered approach; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85096517365
"Shimauchi T., Sakurai K., Tate L., Tamura H.","57219557177;56909413100;57219560650;35600682500;","Gaze-based vehicle driving evaluation of system with an actual vehicle at an intersection with a traffic light",2020,"Electronics (Switzerland)","9","9","1408","1","15",,,"10.3390/electronics9091408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093910250&doi=10.3390%2felectronics9091408&partnerID=40&md5=ff33d91890b044bbd383fa12f771fb94","Faculty of Engineering, University of Miyazaki, 1-1, Gakuen Kibanadai-nishi, Miyazaki-shi, 889-2192, Japan","Shimauchi, T., Faculty of Engineering, University of Miyazaki, 1-1, Gakuen Kibanadai-nishi, Miyazaki-shi, 889-2192, Japan; Sakurai, K., Faculty of Engineering, University of Miyazaki, 1-1, Gakuen Kibanadai-nishi, Miyazaki-shi, 889-2192, Japan; Tate, L., Faculty of Engineering, University of Miyazaki, 1-1, Gakuen Kibanadai-nishi, Miyazaki-shi, 889-2192, Japan; Tamura, H., Faculty of Engineering, University of Miyazaki, 1-1, Gakuen Kibanadai-nishi, Miyazaki-shi, 889-2192, Japan","Due to the population aging in Japan, more elderly people are retaining their driver’s licenses and the increase in the number of car accidents by elderly drivers is a social problem. To address this problem, an objective data-based method to evaluate whether elderly drivers can continue driving is needed. In this paper, we propose a car driving evaluation system based on gaze as calculated by eye and head angles. We used an eye tracking device (TalkEye Lite) made by the Takei Scientific Instruments Cooperation. For our image processing technique, we propose a gaze fixation condition using deep learning (YOLOv2-tiny). By using an eye tracking device and the proposed gaze fixation condition, we built a system where drivers could be evaluated during actual car operation. We describe our system in this paper. In order to evaluate our proposed method, we conducted experiments from November 2017 to November 2018 where elderly people were evaluated by our system while driving an actual car. The subjects were 22 general drivers (two were 80–89 years old, four were 70–79 years old, six were 60–69 years old, three were 50–59 years old, five were 40–49 years old and two were 30–39 years old). We compared the subjects’ gaze information with the subjective evaluation by a professional driving instructor. As a result, we confirm that the subjects’ gaze information is related to the subjective evaluation by the instructor. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Car driving evaluation system; Eye tracking device; Gaze fixation; Subjective evaluation; YOLOv2-tiny",,Article,"Final","",Scopus,2-s2.0-85093910250
"Hu Y., Wang W., Liu H., Liu L.","57202627400;55778099100;57195532958;57214860542;","Reinforcement Learning Tracking Control for Robotic Manipulator with Kernel-Based Dynamic Model",2020,"IEEE Transactions on Neural Networks and Learning Systems","31","9","8890006","3570","3578",,4,"10.1109/TNNLS.2019.2945019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090252250&doi=10.1109%2fTNNLS.2019.2945019&partnerID=40&md5=a64b48efa0dc6f66f1b3fffb993a3fec","State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110169, China; Department of Mathematics, Georgia Institute of Technology, Atlanta, GA  30332, United States","Hu, Y., State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110169, China; Wang, W., State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110169, China; Liu, H., Department of Mathematics, Georgia Institute of Technology, Atlanta, GA  30332, United States; Liu, L., State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110169, China","Reinforcement learning (RL) is an efficient learning approach to solving control problems for a robot by interacting with the environment to acquire the optimal control policy. However, there are many challenges for RL to execute continuous control tasks. In this article, without the need to know and learn the dynamic model of a robotic manipulator, a kernel-based dynamic model for RL is proposed. In addition, a new tuple is formed through kernel function sampling to describe a robotic RL control problem. In this algorithm, a reward function is defined according to the features of tracking control in order to speed up the learning process, and then an RL tracking controller with a kernel-based transition dynamic model is proposed. Finally, a critic system is presented to evaluate the policy whether it is good or bad to the RL control tasks. The simulation results illustrate that the proposed method can fulfill the robotic tracking tasks effectively and achieve similar and even better tracking performance with much smaller inputs of force/torque compared with other learning algorithms, demonstrating the effectiveness and efficiency of the proposed RL algorithm. © 2012 IEEE.","Kernel function; reinforcement learning (RL); reward function; robotics tracking control","Dynamic models; Flexible manipulators; Learning systems; Navigation; Reinforcement learning; Robotics; Continuous control; Effectiveness and efficiencies; Efficient learning; Optimal control policy; Robotic manipulators; Tracking controller; Tracking performance; Transition dynamics; Learning algorithms; algorithm; article; eye tracking; reinforcement learning (machine learning); reward; robotics; simulation; torque; velocity",Article,"Final","",Scopus,2-s2.0-85090252250
"Javanmardi M., Qi X.","57210049808;7202430358;","Appearance variation adaptation tracker using adversarial network",2020,"Neural Networks","129",,,"334","343",,6,"10.1016/j.neunet.2020.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086830118&doi=10.1016%2fj.neunet.2020.06.011&partnerID=40&md5=c31487cca07fe106362427dbae679859","Utah State University, LoganUT, United States","Javanmardi, M., Utah State University, LoganUT, United States; Qi, X., Utah State University, LoganUT, United States","Visual trackers using deep neural networks have demonstrated favorable performance in object tracking. However, training a deep classification network using overlapped initial target regions may lead an overfitted model. To increase the model generalization, we propose an appearance variation adaptation (AVA) tracker that aligns the feature distributions of target regions over time by learning an adaptation mask in an adversarial network. The proposed adversarial network consists of a generator and a discriminator network that compete with each other over optimizing a discriminator loss in a mini-max optimization problem. Specifically, the discriminator network aims to distinguish recent target regions from earlier ones by minimizing the discriminator loss, while the generator network aims to produce an adaptation mask to maximize the discriminator loss. We incorporate a gradient reverse layer in the adversarial network to solve the aforementioned mini-max optimization in an end-to-end manner. We compare the performance of the proposed AVA tracker with the most recent state-of-the-art trackers by doing extensive experiments on OTB50, OTB100, and VOT2016 tracking benchmarks. Among the compared methods, AVA yields the highest area under curve (AUC) score of 0.712 and the highest average precision score of 0.951 on the OTB50 tracking benchmark. It achieves the second best AUC score of 0.688 and the best precision score of 0.924 on the OTB100 tracking benchmark. AVA also achieves the second best expected average overlap (EAO) score of 0.366, the best failure rate of 0.68, and the second best accuracy of 0.53 on the VOT2016 tracking benchmark. © 2020 Elsevier Ltd","Adversarial learning; Convolutional neural network; Visual tracking","Benchmarking; Deep neural networks; Discriminators; Failure analysis; Adversarial networks; Deep classifications; Failure rate; Feature distribution; Model generalization; Optimization problems; Recent state; Target regions; Object tracking; area under the curve; article; convolutional neural network; eye tracking; learning; adaptation; automated pattern recognition; human; photostimulation; procedures; Adaptation, Physiological; Humans; Neural Networks, Computer; Pattern Recognition, Automated; Photic Stimulation",Article,"Final","",Scopus,2-s2.0-85086830118
"Abdallah A.S., Elliott L.J., Donley D.","34881398100;24331126400;57220896912;","Toward Smart Internet of Things (IoT) Devices: Exploring the Regions of Interest for Recognition of Facial Expressions using Eye-gaze Tracking",2020,"Canadian Conference on Electrical and Computer Engineering","2020-August",,"9255696","","",,,"10.1109/CCECE47787.2020.9255696","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097819091&doi=10.1109%2fCCECE47787.2020.9255696&partnerID=40&md5=7b1fed58404710015afd9bf21690f2cd","School of Engineering, Penn State Erie - the Behrend College, Erie, PA, United States; Penn State Erie - the Behrend College, Department of Psychology, Erie, PA  16563, United States","Abdallah, A.S., School of Engineering, Penn State Erie - the Behrend College, Erie, PA, United States; Elliott, L.J., Penn State Erie - the Behrend College, Department of Psychology, Erie, PA  16563, United States; Donley, D., Penn State Erie - the Behrend College, Department of Psychology, Erie, PA  16563, United States","A significant portion of the internet of things (IoT) devices will become reliable products in our daily life if and only if they are equipped with strong human computer interaction (HCI) technologies, specifically visual interaction with users through affective computing. One of the major challenges faced in affective computing is recognizing facial expressions and the true emotions behind them. Despite numerous studies performed, current detection systems are ineffective at correctly identifying facial expressions with reliable accuracy, especially in case of negative expressions. Several research projects attempted to extract the recognition process that humans follow to identify facial expressions in order to replicate in smart machines without a significant success. This paper describes our interdisciplinary project whose goal is to extract and define the recognition process that humans follow when identifying the facial expressions of others. We monitor this process by identifying and analyzing the regions of interest participants look at when they are shown static emotions samples under a specific experimental setup. This paper reports the current status of data collection, experimental setup, and initial data visualization. © 2020 IEEE.","Emotions Detection; Eye-gaze tracking; Facial Expressions Recognition; Machine learning","Data visualization; Eye tracking; Human computer interaction; Internet of things; Affective Computing; Human computer interaction (HCI); Interdisciplinary project; Internet of thing (IOT); Internet of Things (IOT); Recognition of facial expressions; Recognition process; Regions of interest; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85097819091
"Mancas M., Kong P., Gosselin B.","8569088000;55929232800;57162610800;","Visual Attention: Deep Rare Features",2020,"2020 Joint 9th International Conference on Informatics, Electronics and Vision and 2020 4th International Conference on Imaging, Vision and Pattern Recognition, ICIEV and icIVPR 2020",,,"9306639","","",,,"10.1109/ICIEVicIVPR48672.2020.9306639","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099882566&doi=10.1109%2fICIEVicIVPR48672.2020.9306639&partnerID=40&md5=a2327d36cc4501eb87914f50ccf804f6","University of Mons Information, Signal and Artificial Intelligence Lab; Institute of Technology of Cambodia, Department of Information and Communication Engineering","Mancas, M., University of Mons Information, Signal and Artificial Intelligence Lab; Kong, P., Institute of Technology of Cambodia, Department of Information and Communication Engineering; Gosselin, B., University of Mons Information, Signal and Artificial Intelligence Lab","Human visual system is modeled in engineering field providing feature-engineered methods which detect contrasted/surprising/unusual data into images. This data is 'in-teresting' for humans and leads to numerous applications. Deep learning (DNNs) drastically improved the algorithms efficiency on the main benchmark datasets. However, DNN-based models are counter-intuitive: surprising or unusual data is by definition difficult to learn because of its low occurrence probability. In reality, DNNs models mainly learn top-down features such as faces, text, people, or animals which usually attract human attention, but they have low efficiency in extracting surprising or unusual data in the images. In this paper, we propose a model called DeepRare2019 (DR) which uses the power of DNNs feature extraction and the genericity of feature-engineered algorithms. DR 1) does not need any training, 2) it takes less than a second per image on CPU only and 3) our tests on three very different eye-tracking datasets show that DR is generic and is always in the top-3 models on all datasets and metrics while no other model exhibits such a regularity and genericity. DeepRare2019 code can be found at https://gilhub.com/numediart/VisualAttention-RareFamily. Contribution-DeepRare2019 (DR) uses the power of DNNs feature extraction and the genericity of feature-engineered algorithms to provide accurate visual attention prediction in any situation. © 2020 IEEE.","Deep Features; Eye Tracking; Odd out One; Rarity; Saliency; Visibility; Visual Attention Prediction","Behavioral research; Deep learning; Efficiency; Extraction; Eye tracking; Image processing; Benchmark datasets; Engineering fields; Genericity; Human attention; Human Visual System; Occurrence probability; Topdown; Visual Attention; Feature extraction",Conference Paper,"Final","",Scopus,2-s2.0-85099882566
"Li T., Huang Q., Alfaro S., Supikov A., Ratcliff J., Grover G., Azuma R.","54944442800;57219610035;54882671100;13007702500;55370068300;57200737969;56276832800;","Light-Field Displays: A View-Dependent Approach",2020,"ACM SIGGRAPH 2020 Emerging Technologies, SIGGRAPH 2020",,,"3407293","","",,1,"10.1145/3388534.3407293","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094860201&doi=10.1145%2f3388534.3407293&partnerID=40&md5=0e41eb173ceed56ab03ecfab222103e0","Intel Labs","Li, T., Intel Labs; Huang, Q., Intel Labs; Alfaro, S., Intel Labs; Supikov, A., Intel Labs; Ratcliff, J., Intel Labs; Grover, G., Intel Labs; Azuma, R., Intel Labs","Most 3D displays suffer from the vergence-accommodation conflict, which is a significant contributor to eyestrain. Light-field displays avoid this conflict by directly supporting accommodation but they are viewed as requiring too much resolution to be practical, due to the tradeoff between spatial and angular resolution. We demo three light-field display prototypes that show a view-dependent approach which sacrifices viewer independence to achieve acceptable performance with reasonable display resolutions. When combined with a directional backlight and eye tracking system, this approach can provide a 3D volume from which a viewer can see 3D objects with accommodation, without wearing special glasses. © 2020 Owner/Author.","3D display; compute shader; integral imaging; super multi view display; vergence accommodation conflict","Eye tracking; Interactive computer graphics; Object tracking; Three dimensional displays; 3-D displays; Acceptable performance; Angular resolution; Display resolutions; Eye tracking systems; Light field displays; Special glass; View-dependent; Field emission displays",Conference Paper,"Final","",Scopus,2-s2.0-85094860201
"Li D., Wen G., Kuai Y., Zhu L., Porikli F.","56226705300;7102510641;57194695138;57200856282;6603528219;","Robust visual tracking with channel attention and focal loss",2020,"Neurocomputing","401",,,"295","307",,4,"10.1016/j.neucom.2019.10.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083118628&doi=10.1016%2fj.neucom.2019.10.041&partnerID=40&md5=3e447a63029ae63fef202f678b029967","National University of Defense Technology, Changsha, China; Australian National University, Canberra, Australia","Li, D., National University of Defense Technology, Changsha, China; Wen, G., National University of Defense Technology, Changsha, China; Kuai, Y., National University of Defense Technology, Changsha, China; Zhu, L., National University of Defense Technology, Changsha, China; Porikli, F., Australian National University, Canberra, Australia","Recently, the tracking community leads a fashion of end-to-end feature representation learning for visual tracking. Previous works treat all feature channels and training samples equally during training. This ignores channel interdependencies and foreground–background data imbalance, thus limiting the tracking performance. To tackle these problems, we introduce channel attention and focal loss into the network design to enhance feature representation learning. Specifically, a Squeeze-and-Excitation (SE) block is coupled to each convolutional layer to generate channel attention. Channel attention reflects the channel-wise importance of each feature channel and is used for feature weighting in online tracking. To alleviate the foreground–background data imbalance, we propose a focal logistic loss by adding a modulating factor to the logistic loss, with two tunable focusing parameters. The focal logistic loss down-weights the loss assigned to easy examples in the background area. Both the SE block and focal logistic loss are computationally lightweight and impose only a slight increase in model complexity. Extensive experiments are performed on three challenging tracking datasets including OTB100, UAV123 and TC128. Experimental results demonstrate that the enhanced tracker achieves significant performance improvement while running at a real-time frame-rate of 66 fps. © 2019","Channel attention; Focal logistic loss; Visual tracking","Computer applications; Neural networks; Feature representation; Feature weighting; Model complexity; Modulating factor; On-line tracking; Real-time frame rates; Tracking performance; Visual Tracking; Aircraft detection; article; attention; body weight; excitation; eye tracking; feature learning (machine learning); running",Article,"Final","",Scopus,2-s2.0-85083118628
"Lee-Cultura S., Sharma K., Papavlasopoulou S., Giannakos M.","57203855458;55903734200;57063398000;36462343600;","Motion-Based Educational Games: Using Multi-Modal Data to Predict Player's Performance",2020,"IEEE Conference on Computatonal Intelligence and Games, CIG","2020-August",,"9231892","17","24",,4,"10.1109/CoG47356.2020.9231892","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096894140&doi=10.1109%2fCoG47356.2020.9231892&partnerID=40&md5=2f407882e5e0479b70f82e507d901666","Norwegian University of Science and Technology, Institutt for Dataeknologi Og Informatikk, Norway","Lee-Cultura, S., Norwegian University of Science and Technology, Institutt for Dataeknologi Og Informatikk, Norway; Sharma, K., Norwegian University of Science and Technology, Institutt for Dataeknologi Og Informatikk, Norway; Papavlasopoulou, S., Norwegian University of Science and Technology, Institutt for Dataeknologi Og Informatikk, Norway; Giannakos, M., Norwegian University of Science and Technology, Institutt for Dataeknologi Og Informatikk, Norway","Multi-Modal Data (MMD) can help educational games researchers understand the synergistic relationship between player's movement and their learning experiences, and consequently uncover insights that may lead to improved design of movement-based game technologies for learning. Predicting player performance fosters opportunities to cultivate heightened educational experiences and outcomes. However, predicting player's performance utilising player-generated MMD during their interactions with educational Motion-Based Touchless Games (MBTG) is challenging. To bridge this gap, we implemented an in-situ study where 26 users, age 11, played 2 maths MBTGs in a single 20-30 minute session. We collected player's MMD (i.e., gaze data from eye-tracking glasses, physiological data from wristbands, and skeleton data from Kinect) produced during game-play. To investigate the potential of MMD for predicting player's academic performance, we used machine learning techniques and MMD derived from player's game-play. This allowed us to identify the MMD features that drive rapid highly accurate predictions of players' academic performance in educational MBTGs. This might allow us to provide real-time proactive feedback to the player to support them through their educational gaming experience. Our analysis compared two data lengths corresponding to half and full duration of the player's question solving time. We showed that all combinations of extracted features associated with gaze, physiological, and skeleton data, predicted student performance more accurately than the majority baseline. Additionally, the most accurate prediction of player's performance derived from the combination of gaze and physiological data for both full and half data lengths. Our findings emphasise the significance of using MMD for real-time performance prediction in educational MBTG and offer implications for practice. © 2020 IEEE.","education; educational games; machine learning; motion-based games; multi-modal data; prediction","Display devices; Eye tracking; Forecasting; Learning systems; Modal analysis; Musculoskeletal system; Academic performance; Accurate prediction; Educational experiences; Learning experiences; Machine learning techniques; Movement-based games; Real time performance; Student performance; Digital storage",Conference Paper,"Final","",Scopus,2-s2.0-85096894140
"Minegishi T., Osawa H.","57219854449;24484018400;","Do You Move Unconsciously? Accuracy of Social Distance and Line of Sight between a Virtual Robot Head and Humans",2020,"29th IEEE International Conference on Robot and Human Interactive Communication, RO-MAN 2020",,,"9223506","503","508",,,"10.1109/RO-MAN47096.2020.9223506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095744966&doi=10.1109%2fRO-MAN47096.2020.9223506&partnerID=40&md5=65b1b3747d3f0d1e11be1985cc89e2d7","University of Tsukuba","Minegishi, T., University of Tsukuba; Osawa, H., University of Tsukuba","In this paper, we examine the effectiveness of the social distance between a virtual agent and users, and the gaze instruction using a display that can be viewed stereoscopically without using any wearable devices. An actual robot cannot always maintain an appropriate interpersonal distance, through nonverbal gestures owing to its limited range of motion. Because large movements may harm humans, the nonverbal gestures of robots are limited in the real world. In this work, 14 participants were asked how far they wanted to move from a robot posing as a museum guide agent. They were also asked to identify the point at which they felt the agent was gazing. There was a significant distance between the initial position and the position to which the participants moved in the first task under two-dimensional (2D) and three-dimensional (3D) scenarios. The participants moved a significant distance in the first task. In the gaze estimation task, however, the error between the 3D and 2D evaluations was significantly lesser, at a point far from the agent. © 2020 IEEE.",,"Agricultural robots; Display devices; Stereo image processing; Wearable technology; Gaze estimation; Range of motions; Social distance; Threedimensional (3-d); Two Dimensional (2 D); Virtual agent; Virtual robots; Wearable devices; Social robots",Conference Paper,"Final","",Scopus,2-s2.0-85095744966
"Akshay S., Megha Y.J., Shetty C.B.","23466346300;57219702115;57220390989;","Machine learning algorithm to identify eye movement metrics using raw eye tracking data",2020,"Proceedings of the 3rd International Conference on Smart Systems and Inventive Technology, ICSSIT 2020",,,"9214290","949","955",,2,"10.1109/ICSSIT48917.2020.9214290","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094809209&doi=10.1109%2fICSSIT48917.2020.9214290&partnerID=40&md5=de3f811ce232adc668af9bac046a6b7c","Science Amrita School of Arts and Sciences, Mysuru Amrita Vishwa Vidyapeetham, Department of Computer, India","Akshay, S., Science Amrita School of Arts and Sciences, Mysuru Amrita Vishwa Vidyapeetham, Department of Computer, India; Megha, Y.J., Science Amrita School of Arts and Sciences, Mysuru Amrita Vishwa Vidyapeetham, Department of Computer, India; Shetty, C.B., Science Amrita School of Arts and Sciences, Mysuru Amrita Vishwa Vidyapeetham, Department of Computer, India","Eye-tracking studies in software engineering are becoming more prevalent and also in the areas like medical, gaming and commercial fields. Researchers may use the same metrics but it is majorly used to give a different name for same field that cause the difficulties in comparing studies, so in this work, a model is developed to reduce the existing challenges. Many existing algorithms are available to apply on eye tracking data but machine learning is one of the best algorithms, for example random forest is one the machine learning algorithms, which helps to hold the test set. In the eye movement metrics, the dataset will be divided into two sets they are: test set and training set. This paper reports on the eye-tracking metries using raw eye-tracking data. The proposed research work has used random forest, decision tree, KNN and SVM for experimentation in order to understand the dataset. The objective of this study is two-fold. First, the identification of various eye movement metrics events and Second, Apply visualization technique. It can be applied in medical field. Here first we will identify the accuracy, recall, precision and f-measure between KNN classifier and SVM, then identifying the eye movement metrics using machine learning algorithm. We give in this research a brief description of the eye movement metrics and which machine algorithm would give the best result, with its applications. © 2020 IEEE.","Eye tracking; Fixation; Metrics; Saccades; Visual perception component","Decision trees; Engineering research; Eye movements; Learning systems; Random forests; Software engineering; Statistical tests; Support vector machines; Eye-tracking studies; F measure; ITS applications; K-NN classifier; Machine algorithm; Medical fields; Training sets; Visualization technique; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85094809209
"Ahrens M.","57169945700;","Towards Automatic Capturing of Traceability Links by Combining Eye Tracking and Interaction Data",2020,"Proceedings of the IEEE International Conference on Requirements Engineering","2020-August",,"9218195","434","439",,,"10.1109/RE48521.2020.00064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093931100&doi=10.1109%2fRE48521.2020.00064&partnerID=40&md5=158e36c79549f9a1cf9da480ad7ca38d","Leibniz Universität Hannover, Software Engineering Group, Hannover, Germany","Ahrens, M., Leibniz Universität Hannover, Software Engineering Group, Hannover, Germany","Despite its numerous scientifically cited benefits, traceability is still rarely established in industrial settings. Years of research in the area have brought several different approaches to create traceability links including Information Retrieval (IR) and Machine Learning approaches. However, their accuracy and overall traceability support is not sufficient yet to be properly applied in practice. In my research, I want to investigate the usage of eye tracking and interaction data in the field of traceability. By tracking how software engineers interact with documents, where they focus on and recording gaze links between those documents, an algorithm is designed to obtain trace links between artifacts from these data. Eye tracking and interaction data have the advantage that they can be recorded in an automatic, non-intrusive way without requiring manual effort. They give detailed insight about where people focus on when working on tasks. However, software support of eye tracking is still limited, especially in the context of dynamic content such as switching between, scrolling or editing documents. Therefore, one essential step of my research is to provide software support for recording eye tracking data in dynamic document environments. By combining eye tracking data with additionally recorded metadata such as interactions, this eye tracking framework shall enable the automatic capturing of gaze links and gaze durations during software engineering tasks. The approach of using eye tracking in the context of traceability will be evaluated in several usage scenarios such as requirements coverage assessment. © 2020 IEEE.","eye tracking; interaction data; software traceability; trace links","Requirements engineering; Software engineering; Dynamic content; Dynamic documents; Industrial settings; Machine learning approaches; Non-intrusive; Software support; Traceability links; Usage scenarios; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85093931100
"Delvigne V., Ris L., Dutoit T., Wannous H., Vandeborre J.-P.","57219434009;6602720245;36022249200;23391125300;6507497277;","VERA: Virtual Environments Recording Attention",2020,"2020 IEEE 8th International Conference on Serious Games and Applications for Health, SeGAH 2020",,,"9201699","","",,,"10.1109/SeGAH49190.2020.9201699","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092717391&doi=10.1109%2fSeGAH49190.2020.9201699&partnerID=40&md5=fe6ece87a0a6839b21da34a964d0e4c4","IMT Lille Douai, CRIStAL UMR CNRS 9189, Villeneuve d'Ascq, France; Faculty of Medicine and Pharmacy, University of Mons, Neuroscience Department Lab, Mons, Belgium; ISIA Lab, Faculty of Engineering, University of Mons, Mons, Belgium","Delvigne, V., IMT Lille Douai, CRIStAL UMR CNRS 9189, Villeneuve d'Ascq, France; Ris, L., Faculty of Medicine and Pharmacy, University of Mons, Neuroscience Department Lab, Mons, Belgium; Dutoit, T., ISIA Lab, Faculty of Engineering, University of Mons, Mons, Belgium; Wannous, H., IMT Lille Douai, CRIStAL UMR CNRS 9189, Villeneuve d'Ascq, France; Vandeborre, J.-P., IMT Lille Douai, CRIStAL UMR CNRS 9189, Villeneuve d'Ascq, France","Children with Attention Deficit Hyperactivity Disorder (ADHD), present different symptoms binding for everyday life, e.g. difficulty to be focused, impulsiveness, difficulty to regulate motor functions, etc. The most commonly prescribed treatment is the medication that can present side effects. Another solution is behavioural treatment that does not seem to present better results than medication for a higher cost. A novel method with growing interest is the use of neurofeedback (NF) to teach the patient to self-regulate symptoms by herself, through the visualisation of the brain activity in an understandable form. Moreover, virtual reality (VR) is a supportive environment for NF in the context of ADHD. However, before proceeding the NF, it is important to determine the features of the physiological signals corresponding to the symptoms' appearance. We present here a novel framework based on the joint measurement of electroencephalogram (EEG) and sight direction by equipment that can be embedded in VR headset, the goals being to estimate attentional state. In parallel to the signal acquisition, attentional tasks are performed to label the physiological signals. Features have been extracted from the signals and machine learning (ML) models have been applied to retrieve the attentional state. Encouraging results have been provided from the pilot study with the ability to make the right classification in multiple scenarios. Moreover, a dataset with the labelled physiological signals is under development. It will help to have a better understanding of the mechanism behind ADHD symptoms. © 2020 IEEE.","Brain-Computer Interface; Eye-tracking; Machine Learning; Virtual-Reality","Brain; Electroencephalography; Serious games; Signal processing; Attention deficit hyperactivity disorder; Brain activity; Electro-encephalogram (EEG); Joint measurement; Motor function; Physiological signals; Pilot studies; Signal acquisitions; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85092717391
"Wang Y., Kotha A., Hong P.-H., Qiu M.","36715604600;57219338228;57219339163;57216130886;","Automated Student Engagement Monitoring and Evaluation during Learning in the Wild",2020,"Proceedings - 2020 7th IEEE International Conference on Cyber Security and Cloud Computing and 2020 6th IEEE International Conference on Edge Computing and Scalable Cloud, CSCloud-EdgeCom 2020",,,"9170958","270","275",,3,"10.1109/CSCloud-EdgeCom49738.2020.00054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092294640&doi=10.1109%2fCSCloud-EdgeCom49738.2020.00054&partnerID=40&md5=364e01d22b9e80b959c92d7f8d3daa06","Texas Am University-Commerce, Department of Computer Science, Commerce, TX  75428, United States","Wang, Y., Texas Am University-Commerce, Department of Computer Science, Commerce, TX  75428, United States; Kotha, A., Texas Am University-Commerce, Department of Computer Science, Commerce, TX  75428, United States; Hong, P.-H., Texas Am University-Commerce, Department of Computer Science, Commerce, TX  75428, United States; Qiu, M., Texas Am University-Commerce, Department of Computer Science, Commerce, TX  75428, United States","With the explosive growth of edge computing and massive open online courses (MOOCs), there is an urgent need to enable pervasive learning so that students could study with high efficiency at any comfortable places at their own pace. Although there have been a number of studies and applications for student engagement monitoring and evaluation in the pervasive learning, most of the existing works are either supported by commercial eye tracking devices/software or designed for off-line studies on the basis of questionnaires, self-reports, checklists, quizzes, teacher introspective evaluations, and assignments. In this work, we investigate the feasibility of real-Time student engagement monitoring and evaluation with low-cost off-The-shelf web-cameras in realistic learning scenarios. To recognizing and evaluating student engagement, a new model is developed and trained by a deep learning Convolutional Neural Network (CNN) with an open source dataset. The quantitative experimental results demonstrate that the deep learning CNN and our model work well and efficiently when monitoring student learning and detecting student engagement in real time. © 2020 IEEE.","cost efficient; engagement evaluation; engagement recognition; Pervasive learning; student engagement","Convolutional neural networks; Deep neural networks; Distance education; Edge computing; Eye tracking; Learning systems; Security of data; Students; Surveys; Explosive growth; Eye tracking devices; Learning scenarios; Massive open online course; Monitoring and evaluations; Off-line studies; Pervasive learning; Student engagement; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85092294640
"Tao Y., Coltey E., Wang T., Alonso M., Shyu M.-L., Chen S.-C., Alhaffar H., Elias A., Bogosian B., Vassigh S.","55752990600;57219305928;57208718842;57220821501;7005365168;7410254440;57193616742;57193609294;57203502639;57203133538;","Confidence Estimation Using Machine Learning in Immersive Learning Environments",2020,"Proceedings - 3rd International Conference on Multimedia Information Processing and Retrieval, MIPR 2020",,,"9175522","247","252",,2,"10.1109/MIPR49039.2020.00058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092135997&doi=10.1109%2fMIPR49039.2020.00058&partnerID=40&md5=62d74cee990af7877fe35a0ad0d32d8a","Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL  33124, United States; School of Computing and Information Sciences, Florida International University, Miami, FL  33199, United States; College of Communication, Architecture and the Arts, Florida International University, Miami, FL  33199, United States","Tao, Y., Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL  33124, United States; Coltey, E., School of Computing and Information Sciences, Florida International University, Miami, FL  33199, United States; Wang, T., School of Computing and Information Sciences, Florida International University, Miami, FL  33199, United States; Alonso, M., School of Computing and Information Sciences, Florida International University, Miami, FL  33199, United States; Shyu, M.-L., Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL  33124, United States; Chen, S.-C., School of Computing and Information Sciences, Florida International University, Miami, FL  33199, United States; Alhaffar, H., College of Communication, Architecture and the Arts, Florida International University, Miami, FL  33199, United States; Elias, A., College of Communication, Architecture and the Arts, Florida International University, Miami, FL  33199, United States; Bogosian, B., College of Communication, Architecture and the Arts, Florida International University, Miami, FL  33199, United States; Vassigh, S., College of Communication, Architecture and the Arts, Florida International University, Miami, FL  33199, United States","As the development of Virtual Reality and Augmented Reality (VR/AR) technology rapidly advances, learning in an artificial immersive environment becomes increasingly feasible. Such emerging technology not only facilitates and promotes an efficient learning process, but also reduces the cost of access to learning materials and environments. Current research mainly focuses on the development of immersive learning environments and the adaptive learning methods based on interactions between trainees and the environment. However, valuable human biometric data available in immersive environments, such as eye gaze and controller pose, have not been explored and utilized to help understand the affective state of the trainees. In this paper, we propose a machine-learning based research framework to estimate trainees' confidence about their decisions in immersive learning environments. Using this framework, we designed an experiment to collect biometric data from a multiple-choice question and answer session in an immersive learning environment. This includes collecting answers from 10 participants on 35 questions and their self-reported confidence in their answers. A Long Short-Term Memory neural network model was used to analyze the data and estimate the confidence with 85.6% accuracy. © 2020 IEEE.","confidence estimation; deep neural network; immersive environment; immersive learning; machine learning","Augmented reality; Biometrics; Machine learning; Turing machines; Virtual reality; Adaptive learning method; Confidence estimation; Efficient learning; Emerging technologies; Immersive environment; Multiple choice questions; Neural network model; Research frameworks; Computer aided instruction",Conference Paper,"Final","",Scopus,2-s2.0-85092135997
"Deng Q., Wang J., Hillebrand K., Benjamin C.R., Soffker D.","57204710623;55888840800;57218643568;57218643784;57213716988;","Prediction Performance of Lane Changing Behaviors: A Study of Combining Environmental and Eye-Tracking Data in a Driving Simulator",2020,"IEEE Transactions on Intelligent Transportation Systems","21","8","8821577","3561","3570",,7,"10.1109/TITS.2019.2937287","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089875423&doi=10.1109%2fTITS.2019.2937287&partnerID=40&md5=75373ce8ef5f62934813c2a9db1310a0","Chair of Dynamics and Control, University of Duisburg-Essen, Duisburg, Germany","Deng, Q., Chair of Dynamics and Control, University of Duisburg-Essen, Duisburg, Germany; Wang, J., Chair of Dynamics and Control, University of Duisburg-Essen, Duisburg, Germany; Hillebrand, K., Chair of Dynamics and Control, University of Duisburg-Essen, Duisburg, Germany; Benjamin, C.R., Chair of Dynamics and Control, University of Duisburg-Essen, Duisburg, Germany; Soffker, D., Chair of Dynamics and Control, University of Duisburg-Essen, Duisburg, Germany","Advanced Driver Assistance Systems (ADAS) are systems developed to assist the human driver and therefore to make driving safer and better. Understanding and predicting human driving behavior play an important role in the development of assistance systems. In this contribution, the development of a driver assistance system is based on the prediction of driving behaviors. The driving patterns of three different behaviors are modeled including left/right lane change and lane keeping. A driving simulator is used to simulate a highway scene. The implementation of a prediction system based on different machine learning approaches such as Hidden Markov Model (HMM), Support Vector Machine (SVM), Convolutional neural networks (CNNs), and Random Forest (RF) is accomplished. In addition, eye-tracking information is integrated. The task is to predict behaviors based on the measurement. As test, a 10-fold cross-validation is used based on data sets from driving simulator and applied to compare the performance of different algorithms. In combination with related results in terms of accuracy (ACC), detection rate (DR), and false alarm rate (FAR), the performance and effectiveness of the developed prediction systems are evaluated. The results show that the performance of RF algorithm is the best of all four algorithms compared. Combining environmental and eye-tracking data the RF algorithm achieved the best results. All ACC values are larger than 99 %. Afterwards, two RF-based prediction models with and without eye-tracking data are developed for online test. Finally, some application samples are suggested for driver assistance. The results calculated by the proposed model are shown on a user interface to help the drivers to see when it is suitable to turn left, to turn right, or to keep the direction. © 2000-2011 IEEE.","and advanced driver assistance systems; driving behaviors prediciton; Machine learning methods; state and intent recognition","Alarm systems; Automobile drivers; Automobile simulators; Behavioral research; Convolutional neural networks; Decision trees; Eye tracking; Forecasting; Hidden Markov models; Predictive analytics; Support vector machines; User interfaces; 10-fold cross-validation; Driver assistance; Driver assistance system; Human driving behavior; Lane-changing behaviors; Machine learning approaches; Prediction performance; Prediction systems; Advanced driver assistance systems",Article,"Final","",Scopus,2-s2.0-85089875423
"Baranova-Bolotova V.","57209221823;","Multi-Document Answer Generation for Non-Factoid Questions",2020,"SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval",,,,"2477","",,,"10.1145/3397271.3401449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090122041&doi=10.1145%2f3397271.3401449&partnerID=40&md5=800cca58830f3def36aeba54fd812da0","Rmit University, Melbourne, Australia","Baranova-Bolotova, V., Rmit University, Melbourne, Australia","The current research will be devoted to the challenging and under-investigated task of multi-source answer generation for complex non-factoid questions. We will start with experimenting with generative models on one particular type of non-factoid questions-instrumental/procedural questions which often start with ""how-to"". For this, a new dataset, comprised of more than 100,000 QA-pairs which were crawled from a dedicated web-resource where each answer has a set of references to the articles it was written upon, will be used. We will also compare different ways of model evaluation to choose a metric which better correlates with human assessment. To be able to do this, the way people evaluate answers to non-factoid questions and set some formal criteria of what makes a good quality answer is needed to be understood. Eye-tracking and crowdsourcing methods will be employed to study how users interact with answers and evaluate them, and how the answer features correlate with task complexity. We hope that our research will help to redefine the way users interact and work with search engines so as to transform IR finally into the answer retrieval systems that users have always desired. © 2020 Owner/Author.","deep learning; eye-tracking; non-factoid questions; question answering; user study","Eye tracking; Information retrieval; Search engines; Factoid questions; Generative model; Human assessment; Model evaluation; Multi-document; Multi-Sources; Retrieval systems; Task complexity; Quality control",Conference Paper,"Final","",Scopus,2-s2.0-85090122041
"Zheng Z., Ruan L., Zhu M., Guo X.","35757181600;57204551822;55252727500;55252365200;","Reinforcement learning control for underactuated surface vessel with output error constraints and uncertainties",2020,"Neurocomputing","399",,,"479","490",,7,"10.1016/j.neucom.2020.03.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082424477&doi=10.1016%2fj.neucom.2020.03.021&partnerID=40&md5=e435ecbb1b6c263992fa4e426a3f396c","Seventh Research Division, School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China; CETHIK Research Institute, Hangzhou, 310012, China; Institute of Unmanned System, Beihang University, Beijing, 100191, China; Research Institute of Frontier Science, Beihang University, Beijing, 100191, China","Zheng, Z., Seventh Research Division, School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China; Ruan, L., CETHIK Research Institute, Hangzhou, 310012, China; Zhu, M., Institute of Unmanned System, Beihang University, Beijing, 100191, China; Guo, X., Research Institute of Frontier Science, Beihang University, Beijing, 100191, China","This study investigates the trajectory tracking control problem of an underactuated marine vessel in the presence of output constraints, model uncertainties and environmental disturbances. The error transformation technique can ensure that the tracking errors remain within the predefined constraint boundaries. The controller is designed in combination with the critic function and the reinforcement learning (RL) algorithm based on actor-critic neural networks. The RL method is applied to solve model uncertainties and disturbances, and the critic function modifies the control action to supervise the system performance. Based on Lyapunov's direct method, a stability analysis is proposed to prove that the boundedness of system signals and the desired tracking performance can be guaranteed. Finally, the simulation illustrates the effectiveness and feasibility of the proposed controller. © 2020 Elsevier B.V.","Actor-Critic (AC); Neural networks; Output constraints; Reinforcement learning; Trajectory tracking; Underactuated marine vessel","Controllers; Errors; Neural networks; Uncertainty analysis; Actor critic; Actor-critic neural network; Marine vessels; Output constraints; Reinforcement learning control; Trajectory tracking; Trajectory tracking control; Underactuated surface vessels; Reinforcement learning; article; artificial neural network; eye tracking; feasibility study; reinforcement learning (machine learning); simulation; uncertainty",Article,"Final","",Scopus,2-s2.0-85082424477
"Dai M., Xiao G., Cheng S., Wang D., He X.","56457485500;57201647250;8691666600;9133885500;7404409118;","Structural correlation filters combined with a Gaussian particle filter for hierarchical visual tracking",2020,"Neurocomputing","398",,,"235","246",,3,"10.1016/j.neucom.2020.02.095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081348121&doi=10.1016%2fj.neucom.2020.02.095&partnerID=40&md5=27c6e3a97fdc32d0b8cf709c4e03c9d6","College of Physics and Information Engineering, Fuzhou University, Fuzhou, Fujian  350108, China; Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, NSW  2007, Australia; Commonwealth Scientific and Industrial Research Organisation, Sydney, NSW 2122, Australia; Division of Engineering in Medicine, Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, Cambridge, MA  02139, United States; John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA  02138, United States; Wyss Institute for Biologically Inspired Engineering, Harvard University, Boston, MA  02115, United States; College of Environment and Resources, Fuzhou University, Fuzhou, Fujian  350108, China","Dai, M., College of Physics and Information Engineering, Fuzhou University, Fuzhou, Fujian  350108, China, Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, NSW  2007, Australia, Commonwealth Scientific and Industrial Research Organisation, Sydney, NSW 2122, Australia, Division of Engineering in Medicine, Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, Cambridge, MA  02139, United States; Xiao, G., Division of Engineering in Medicine, Department of Medicine, Brigham and Women's Hospital, Harvard Medical School, Cambridge, MA  02139, United States, John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA  02138, United States, Wyss Institute for Biologically Inspired Engineering, Harvard University, Boston, MA  02115, United States, College of Environment and Resources, Fuzhou University, Fuzhou, Fujian  350108, China; Cheng, S., College of Physics and Information Engineering, Fuzhou University, Fuzhou, Fujian  350108, China; Wang, D., Commonwealth Scientific and Industrial Research Organisation, Sydney, NSW 2122, Australia; He, X., Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, NSW  2007, Australia","Visual tracking is a key problem for many computer vision applications such as human-computer interaction, intelligent medical diagnosis, navigation and traffic control management. Most of the existing tracking methods are mainly based on correlation filters. However, boundary effect, scale estimation and template updating have not been fully resolved. Herein, this paper presents a new hierarchical tracking method combining structural correlation filters with a Gaussian Particle Filter (GPF), named KCF-GPF. Weak KCF classifiers are constructed via a Lukas-Kanade (LK) method and the preliminary target location is presented as a weighted sum of these classifiers. Specially, a facile weight strategy is implemented to estimate the reliability of each weak classifier. On the basis of the preliminary target location, the GPF using features from a Convolutional Neural Network (CNN) is employed to predict the location and scale of a target. Extensive experiments with the OTB-2013 and the OTB-2015 databases demonstrate that the proposed algorithm performs favourably against state-of-the-art trackers. © 2020","Convolutional Neural Network (CNN); Gaussian Particle Filter (GPF); Lukas-Kanade (LK); Reliability estimation; Structural correlation filter","Convolution; Diagnosis; Gaussian distribution; Human computer interaction; Location; Monte Carlo methods; Traffic control; Computer vision applications; Control management; Correlation filters; Gaussian particle filter; Lukas-Kanade (LK); Lukas-Kanade methods; Reliability estimation; Structural correlation; Convolutional neural networks; Article; comparative study; convolutional neural network; correlation analysis; eye tracking; human; human computer interaction; image analysis; image processing; kernel method; machine learning; measurement precision; nonhuman; priority journal",Article,"Final","",Scopus,2-s2.0-85081348121
"Aivaliotis P.-E., Grivokostopoulou F., Perikos I., Daramouskas I., Hatziligeroudis I.","57221474593;54412205200;27667764000;57207760262;25924984600;","Eye Gaze Analysis of Students in Educational Systems",2020,"11th International Conference on Information, Intelligence, Systems and Applications, IISA 2020",,,"9284374","","",,,"10.1109/IISA50023.2020.9284374","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099188196&doi=10.1109%2fIISA50023.2020.9284374&partnerID=40&md5=de1597d3eb866d829d1ebc98e54e95b9","University of Patras, Department of Computer Engineering and Informatics, Patras, Greece","Aivaliotis, P.-E., University of Patras, Department of Computer Engineering and Informatics, Patras, Greece; Grivokostopoulou, F., University of Patras, Department of Computer Engineering and Informatics, Patras, Greece; Perikos, I., University of Patras, Department of Computer Engineering and Informatics, Patras, Greece; Daramouskas, I., University of Patras, Department of Computer Engineering and Informatics, Patras, Greece; Hatziligeroudis, I., University of Patras, Department of Computer Engineering and Informatics, Patras, Greece","Eye gaze provides indicative information about the status and the behavior of a person and can be very assistive in human-computer interaction. Eye-gaze analysis is very helpful in a variety of applications in order to understand the interest of the users, their behavior or even to unveil distractions. However, the accurate eye-gaze estimation is a very challenging process. In this paper, we present an eye gaze estimation work that relies on convolutional neural networks which imitate the LeNet's architecture. They analyze eye gaze and provide a 2D vector that concerns the coordinates of the specific pixel inside the 2D screen's space, in which the user is looking at. Also, a system capable of working under various real-world conditions such as light, angle and distance differentiations was designed and developed. An evaluation study was performed and the results are quite promising pointing out that the system is scalable and accurate in estimating the eye gaze of the users. © 2020 IEEE.","Convolutional Neural Networks; Deep Learning; Eye Gaze estimation; Human-Computer Interaction","Convolutional neural networks; Vector spaces; Assistive; Educational systems; Evaluation study; Eye-gaze; Real-world; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85099188196
"Dahmani M., Chowdhury M.E.H., Khandakar A., Rahman T., Al-Jayyousi K., Hefny A., Kiranyaz S.","57214653802;8964151000;36053012700;57216883687;57218098738;57214813088;7801632948;","An intelligent and low-cost eye-tracking system for motorized wheelchair control",2020,"Sensors (Switzerland)","20","14","3936","1","27",,7,"10.3390/s20143936","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087912991&doi=10.3390%2fs20143936&partnerID=40&md5=78582708851ec19bcd9e47bb9e1a1b23","School of Engineering, University of Maryland, College Park, MD  20742, United States; Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar; Department of Biomedical Physics and Technology, University of Dhaka, Dhaka, 1000, Bangladesh","Dahmani, M., School of Engineering, University of Maryland, College Park, MD  20742, United States; Chowdhury, M.E.H., Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar; Khandakar, A., Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar; Rahman, T., Department of Biomedical Physics and Technology, University of Dhaka, Dhaka, 1000, Bangladesh; Al-Jayyousi, K., Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar; Hefny, A., Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar; Kiranyaz, S., Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar","In the 34 developed and 156 developing countries, there are ~132 million disabled people who need a wheelchair, constituting 1.86% of the world population. Moreover, there are millions of people suffering from diseases related to motor disabilities, which cause inability to produce controlled movement in any of the limbs or even head. This paper proposes a system to aid people with motor disabilities by restoring their ability to move effectively and effortlessly without having to rely on others utilizing an eye-controlled electric wheelchair. The system input is images of the user’s eye that are processed to estimate the gaze direction and the wheelchair was moved accordingly. To accomplish such a feat, four user-specific methods were developed, implemented, and tested; all of which were based on a benchmark database created by the authors. The first three techniques were automatic, employ correlation, and were variants of template matching, whereas the last one uses convolutional neural networks (CNNs). Different metrics to quantitatively evaluate the performance of each algorithm in terms of accuracy and latency were computed and overall comparison is presented. CNN exhibited the best performance (i.e., 99.3% classification accuracy), and thus it was the model of choice for the gaze estimator, which commands the wheelchair motion. The system was evaluated carefully on eight subjects achieving 99% accuracy in changing illumination conditions outdoor and indoor. This required modifying a motorized wheelchair to adapt it to the predictions output by the gaze estimation algorithm. The wheelchair control can bypass any decision made by the gaze estimator and immediately halt its motion with the help of an array of proximity sensors, if the measured distance goes below a well-defined safety margin. This work not only empowers any immobile wheelchair user, but also provides low-cost tools for the organization assisting wheelchair users. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks (CNNs); Eye tracking; Machine learning; Motorized wheelchair; Ultrasonic proximity sensors","Convolutional neural networks; Costs; Developing countries; Eye movements; Template matching; Wheelchairs; Benchmark database; Classification accuracy; Electric wheelchair; Illumination conditions; Low cost eye tracking; Motor disability; Motorized wheelchairs; Wheelchair control; Eye tracking; algorithm; disabled person; human; wheelchair; Algorithms; Disabled Persons; Eye-Tracking Technology; Humans; Neural Networks, Computer; Wheelchairs",Article,"Final","",Scopus,2-s2.0-85087912991
"Dari S., Kadrileev N., Hullermeier E.","57211024441;57219548736;6701552637;","A Neural Network-Based Driver Gaze Classification System with Vehicle Signals",2020,"Proceedings of the International Joint Conference on Neural Networks",,,"9207709","","",,2,"10.1109/IJCNN48605.2020.9207709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093868851&doi=10.1109%2fIJCNN48605.2020.9207709&partnerID=40&md5=f538277641862acf6ee248b9d085dd55","BMW Goup, Research and Development, Munich, Germany; Technical University Munich, Department of Electrical Engineering, Munich, Germany; Technical University Munich, Department of Computer Science, Munich, Germany","Dari, S., BMW Goup, Research and Development, Munich, Germany; Kadrileev, N., Technical University Munich, Department of Electrical Engineering, Munich, Germany; Hullermeier, E., Technical University Munich, Department of Computer Science, Munich, Germany","Driver monitoring can play an essential part in avoiding accidents by warning the driver and shifting the driver's attention to the traffic scenery in time during critical situations. This may apply for the different levels of automated driving, for take-over requests as well as for driving in manual mode. A great proxy for this purpose has always been the driver's gazing direction. The aim of this work is to introduce a robust gaze detection system. In this regard, we make several contributions that are novel in the area of gaze detection systems. In particular, we propose a deep learning approach to predict gaze regions, which is based on informative features such as eye landmarks and head pose angles of the driver. Moreover, we introduce different post-processing techniques that improve the accuracy by exploiting temporal information from videos and the availability of other vehicle signals. Last but not least, we confirm our method with a leave-one-driver-out cross-validation. Unlike previous studies, we do not use gazes to predict maneuver changes, but we consider the human-computer-interaction aspect and use vehicle signals to improve the performance of the estimation. The proposed system is able to achieve an accuracy of 92.3% outperforming earlier landmark-based gaze estimators. © 2020 IEEE.","Autonomous Driving; Driver distraction; Driver Gaze Estimation; Driver Monitoring; Driver State Recognition; Naturalistic Driving Study","Behavioral research; Deep learning; Human computer interaction; Vehicles; Automated driving; Classification system; Cross validation; Driver monitoring; Gaze detection; Learning approach; Post-processing techniques; Temporal information; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85093868851
"Riad Saboundji R., Adrian Rill R.","57222514796;57191287116;","Predicting Human Errors from Gaze and Cursor Movements",2020,"Proceedings of the International Joint Conference on Neural Networks",,,"9207189","","",,,"10.1109/IJCNN48605.2020.9207189","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093849077&doi=10.1109%2fIJCNN48605.2020.9207189&partnerID=40&md5=f3db94bf08711a83fd24202293b7e95e","Eotvos Lorand University, Faculty of Informatics, Budapest, Hungary; Babes-Bolyai University, Faculty of Mathematics and Computer Science, Cluj-Napoca, Romania","Riad Saboundji, R., Eotvos Lorand University, Faculty of Informatics, Budapest, Hungary; Adrian Rill, R., Babes-Bolyai University, Faculty of Mathematics and Computer Science, Cluj-Napoca, Romania","Intelligent interfaces are increasingly integrated into diverse technological areas. In complex high-risk environments, where humans represent a crucial part of the system and their attention is often divided between simultaneous activities, imminent human errors may have serious consequences. Enhancing interfaces with predictive capabilities promotes the safe and reliable operation of such systems. In this work, we employ a data-driven approach to predict human errors in a special divided attention task involving timing constraints and requiring focused concentration and frequent shifts of attention. We performed a longitudinal study with 10 subjects, and constructed time series from the experimental data using gaze movement and mouse cursor motion features in order to classify successful and failed actions. We evaluate classical machine learning algorithms, compare them with a more traditional temporal modeling approach and a deep learning based LSTM model. Employing a leave-one- subject-out cross-validation procedure we achieve a classification accuracy of up to 86%, with LSTM presenting the highest performance. Furthermore, we investigate the trade-off between evaluation metrics and anticipation window, i.e. the time remaining until the correct action can still be performed. We conclude that prediction is feasible and accuracy and F1-score increases, despite the training dataset becoming greatly imbalanced. Investigating the anticipation window allows to understand how far in advance human errors need to be predicted in order to initiate preventive measures. Our efforts have implications for the design of predictive interfaces involving decision making under time pressure in dynamic divided attention environments. © 2020 IEEE.","anticipation window; gaze tracking; human error; LSTM; predictive interface; time series","Decision making; Deep learning; Economic and social effects; Errors; Forecasting; Learning algorithms; Long short-term memory; Mammals; Classification accuracy; Data-driven approach; Evaluation metrics; High risk environment; Intelligent interface; Longitudinal study; Predictive capabilities; Preventive measures; Time and motion study",Conference Paper,"Final","",Scopus,2-s2.0-85093849077
"Vismaya U.K., Saritha E.","57219120655;56875441300;","A Review on Driver Distraction Detection Methods",2020,"Proceedings of the 2020 IEEE International Conference on Communication and Signal Processing, ICCSP 2020",,,"9182316","483","487",,1,"10.1109/ICCSP48568.2020.9182316","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091341366&doi=10.1109%2fICCSP48568.2020.9182316&partnerID=40&md5=abbba3489d1e3550998f6dcea1a7763d","Government College of Engineering Kannur, Department of Electronics and Communication, Kerala, India","Vismaya, U.K., Government College of Engineering Kannur, Department of Electronics and Communication, Kerala, India; Saritha, E., Government College of Engineering Kannur, Department of Electronics and Communication, Kerala, India","One of the most relevant reason for death is road crashes. Road crashes may occur for many reasons. One of the causes is distracted driving. Distracted driving is the deviation of the driver's consciousness from driving tasks due to some secondary tasks like eating talking etc. It may cause death, injuries and economic losses. The alert given to the driver on time can avoid most of these problems and improve transportation safety. However, the development of such systems can solve many difficulties related to fast and proper recognition of a driver's behavior. Different methods have been used to determine the forms of driver disturbances and to explain the consequences in terms of driving performance and participation in crashes. There are three different types of distractions present, visual distraction, manual distraction, and cognitive distraction. Head, eye movements are one of the strong indicators of driver's distraction. We know remote eye tracking has emerged recently, which gives an opportunity of real-time identification of visual distraction. In this paper discussed visual along with the manual distractions. © 2020 IEEE.","Artificial neural network; drowsiness detection; eye blink detection; head pose estimation; homomorphic filter; machine learning and semi-supervised learning","Automobile drivers; Eye movements; Eye tracking; Highway accidents; Losses; Roads and streets; Cognitive distractions; Detection methods; Driver distractions; Driver's behavior; Driving performance; Real-time identification; Transportation safety; Visual distractions; Signal processing",Conference Paper,"Final","",Scopus,2-s2.0-85091341366
"Putra P., Shima K., Shimatani K.","57203001552;24068050800;16314367300;","Catchicken: A serious game based on the go/nogo task to estimate inattentiveness and impulsivity symptoms",2020,"Proceedings - IEEE Symposium on Computer-Based Medical Systems","2020-July",,"9182952","152","157",,,"10.1109/CBMS49503.2020.00036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091140124&doi=10.1109%2fCBMS49503.2020.00036&partnerID=40&md5=93e3ccdbf7e40401088ec89cda89db34","Yokohama National University, Graduate School of Engineering, Japan; Yokohama National University, Faculty of Engineering, Japan; Prefectural University of Hiroshima, Faculty of Health and Welfare, Japan","Putra, P., Yokohama National University, Graduate School of Engineering, Japan; Shima, K., Yokohama National University, Faculty of Engineering, Japan; Shimatani, K., Prefectural University of Hiroshima, Faculty of Health and Welfare, Japan","We present a Go/NoGo 3D game equipped with an eye tracker that records subjects' responses and his gaze position on the monitor over time. The proposed system consists of two functions: training that allows an instructor to modify the game's parameters and make a customized test; and evaluation in which the instructor can fix the parameters to create a standardized test. During the experiment, subjects were required to respond only to Go character by pressing a spacebar. The experimental results from 59 participants demonstrated that one's response time and its variability correlated with one's gaze behavior. Subjects with higher gaze modulation tended to respond faster and more stable. We also observed that utilizing the proposed system we could monitor the improvements in an Autism Spectrum Disorder child during his rehabilitation: his gaze modulation increased and his response time became more steady. In brief, utilizing the proposed system, we could effectively measure participants' response time variability of NoGo errors and their gaze trajectory area, which previous studies found to have a strong relationship with symptoms of mental disorders. © 2020 IEEE.","Gaze behavior; Go/NoGo task; High-risk disordered children; Serious games","Eye tracking; Modulation; Patient rehabilitation; Response time (computer systems); 3D games; Autism spectrum disorders; Eye trackers; Gaze behavior; Mental disorders; On The Go; Response time variability; Standardized tests; Serious games",Conference Paper,"Final","",Scopus,2-s2.0-85091140124
"Jiang M., Francis S.M., Tseng A., Srishyla D., Dubois M., Beard K., Conelea C., Zhao Q., Jacob S.","56027704500;55979295000;55920911000;57213596359;57221231170;57216864368;13402971200;55743334300;7202574761;","Predicting Core Characteristics of ASD Through Facial Emotion Recognition and Eye Tracking in Youth",2020,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","2020-July",,"9176843","871","875",,2,"10.1109/EMBC44109.2020.9176843","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091001196&doi=10.1109%2fEMBC44109.2020.9176843&partnerID=40&md5=0aacbaf3a6b436a6c3adf26382650168","University of Minnesota, Department of Computer Science and Engineering, Minneapolis, MN, United States","Jiang, M., University of Minnesota, Department of Computer Science and Engineering, Minneapolis, MN, United States; Francis, S.M., University of Minnesota, Department of Computer Science and Engineering, Minneapolis, MN, United States; Tseng, A., University of Minnesota, Department of Computer Science and Engineering, Minneapolis, MN, United States; Srishyla, D., University of Minnesota, Department of Computer Science and Engineering, Minneapolis, MN, United States; Dubois, M., University of Minnesota, Department of Computer Science and Engineering, Minneapolis, MN, United States; Beard, K., University of Minnesota, Department of Computer Science and Engineering, Minneapolis, MN, United States; Conelea, C., University of Minnesota, Department of Computer Science and Engineering, Minneapolis, MN, United States; Zhao, Q., University of Minnesota, Department of Computer Science and Engineering, Minneapolis, MN, United States; Jacob, S., University of Minnesota, Department of Computer Science and Engineering, Minneapolis, MN, United States","Autism Spectrum Disorder (ASD) is a heterogeneous neurodevelopmental disorder (NDD) with a high rate of comorbidity. The implementation of eye-tracking methodologies has informed behavioral and neurophysiological patterns of visual processing across ASD and comorbid NDDs. In this study, we propose a machine learning method to predict measures of two core ASD characteristics: impaired social interactions and communication, and restricted, repetitive, and stereotyped behaviors and interests. Our method extracts behavioral features from task performance and eye-tracking data collected during a facial emotion recognition paradigm. We achieved high regression accuracy using a Random Forest regressor trained to predict scores on the SRS-2 and RBS-R assessments; this approach may serve as a classifier for ASD diagnosis. © 2020 IEEE.",,"Decision trees; Diagnosis; Face recognition; Forecasting; Learning systems; Speech recognition; Turing machines; Autism spectrum disorders; Behavioral features; Co morbidities; Facial emotions; Machine learning methods; Social interactions; Task performance; Visual-processing; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85091001196
"Khan W., Hussain A., Kuru K., Al-Askar H.","55319932200;56212648400;35100500500;56288742400;","Pupil localisation and eye centre estimation using machine learning and computer vision",2020,"Sensors (Switzerland)","20","13","3785","1","18",,8,"10.3390/s20133785","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087442183&doi=10.3390%2fs20133785&partnerID=40&md5=e09aa8b9fc7e23ff1f7e6b7a9a1d0f99","Computer Science Department, Liverpool John Moores University, Liverpool, L33AF, United Kingdom; School of Engineering, University of Central Lancashire, Preston, PR12HE, United Kingdom; Computer Science Department, College of Engineering and Computer Sciences, Prince Sattam Bin Abdulaziz University, Al-Kharj, 11942, Saudi Arabia","Khan, W., Computer Science Department, Liverpool John Moores University, Liverpool, L33AF, United Kingdom; Hussain, A., Computer Science Department, Liverpool John Moores University, Liverpool, L33AF, United Kingdom; Kuru, K., School of Engineering, University of Central Lancashire, Preston, PR12HE, United Kingdom; Al-Askar, H., Computer Science Department, College of Engineering and Computer Sciences, Prince Sattam Bin Abdulaziz University, Al-Kharj, 11942, Saudi Arabia","Various methods have been used to estimate the pupil location within an image or a real-time video frame in many fields. However, these methods lack the performance specifically in low-resolution images and varying background conditions. We propose a coarse-to-fine pupil localisation method using a composite of machine learning and image processing algorithms. First, a pre-trained model is employed for the facial landmark identification to extract the desired eye frames within the input image. Then, we use multi-stage convolution to find the optimal horizontal and vertical coordinates of the pupil within the identified eye frames. For this purpose, we define an adaptive kernel to deal with the varying resolution and size of input images. Furthermore, a dynamic threshold is calculated recursively for reliable identification of the best-matched candidate. We evaluated our method using various statistical and standard metrics along with a standardised distance metric that we introduce for the first time in this study. The proposed method outperforms previous works in terms of accuracy and reliability when benchmarked on multiple standard datasets. The work has diverse artificial intelligence and industrial applications including human computer interfaces, emotion recognition, psychological profiling, healthcare, and automated deception detection. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Deep eye; Eye centre localisation; Eye gaze; Facial analysis; Image convolution; Iris detection; Machine intelligence; Pupil detection; Pupil segmentation","Computer vision; Medical computing; Automated deception detections; Emotion recognition; Human computer interfaces; Image processing algorithm; Low resolution images; Multiple standards; Varying background; Vertical coordinates; Machine learning; algorithm; artificial intelligence; computer; human; machine learning; pupil; reproducibility; Algorithms; Artificial Intelligence; Computers; Humans; Machine Learning; Pupil; Reproducibility of Results",Article,"Final","",Scopus,2-s2.0-85087442183
"De Lope J., Graña M.","55888684400;7005388617;","Behavioral Activity Recognition Based on Gaze Ethograms",2020,"International Journal of Neural Systems","30","7","2050025","","",,3,"10.1142/S0129065720500252","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087280688&doi=10.1142%2fS0129065720500252&partnerID=40&md5=4f3d515386b2bc6698182183e09a3ed7","Department of Artificial Intelligence, Universidad Politécnica de Madrid (UPM), Madrid, Spain; Computational Intelligence Group, University of the Basque Country (UPV/EHU), San-Sebastian, Spain","De Lope, J., Department of Artificial Intelligence, Universidad Politécnica de Madrid (UPM), Madrid, Spain; Graña, M., Computational Intelligence Group, University of the Basque Country (UPV/EHU), San-Sebastian, Spain","Noninvasive behavior observation techniques allow more natural human behavior assessment experiments with higher ecological validity. We propose the use of gaze ethograms in the context of user interaction with a computer display to characterize the user's behavioral activity. A gaze ethogram is a time sequence of the screen regions the user is looking at. It can be used for the behavioral modeling of the user. Given a rough partition of the display space, we are able to extract gaze ethograms that allow discrimination of three common user behavioral activities: reading a text, viewing a video clip, and writing a text. A gaze tracking system is used to build the gaze ethogram. User behavioral activity is modeled by a classifier of gaze ethograms able to recognize the user activity after training. Conventional commercial gaze tracking for research in the neurosciences and psychology science are expensive and intrusive, sometimes impose wearing uncomfortable appliances. For the purposes of our behavioral research, we have developed an open source gaze tracking system that runs on conventional laptop computers using their low quality cameras. Some of the gaze tracking pipeline elements have been borrowed from the open source community. However, we have developed innovative solutions to some of the key issues that arise in the gaze tracker. Specifically, we have proposed texture-based eye features that are quite robust to low quality images. These features are the input for a classifier predicting the screen target area, the user is looking at. We report comparative results of several classifier architectures carried out in order to select the classifier to be used to extract the gaze ethograms for our behavioral research. We perform another classifier selection at the level of ethogram classification. Finally, we report encouraging results of user behavioral activity recognition experiments carried out over an inhouse dataset. © 2020 The Author(s).","activity recognition; gaze ethogram; gaze tracking; Neuroethology; noninvasive eye tracker; screen-based eye tracker","Eye tracking; Laptop computers; Open systems; Pattern recognition; Textures; Activity recognition; Behavior observation; Classifier selection; Ecological validity; Gaze tracking system; Innovative solutions; Open source communities; User interaction; Behavioral research; automated pattern recognition; eye fixation; human; machine learning; motor activity; pattern recognition; physiology; procedures; reading; support vector machine; writing; Eye-Tracking Technology; Fixation, Ocular; Humans; Machine Learning; Motor Activity; Pattern Recognition, Automated; Pattern Recognition, Visual; Reading; Support Vector Machine; Writing",Article,"Final","",Scopus,2-s2.0-85087280688
"Kim H., Ohmura Y., Kuniyoshi Y.","57217147980;7006439860;7005371269;","Using Human Gaze to Improve Robustness against Irrelevant Objects in Robot Manipulation Tasks",2020,"IEEE Robotics and Automation Letters","5","3","9103290","4415","4422",,1,"10.1109/LRA.2020.2998410","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086446787&doi=10.1109%2fLRA.2020.2998410&partnerID=40&md5=69b51097c9866b61f4d0f71c21f95757","Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, University of Tokyo, Tokyo, Japan","Kim, H., Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, University of Tokyo, Tokyo, Japan; Ohmura, Y., Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, University of Tokyo, Tokyo, Japan; Kuniyoshi, Y., Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, University of Tokyo, Tokyo, Japan","Deep imitation learning enables the learning of complex visuomotor skills from raw pixel inputs. However, this approach suffers from the problem of overfitting to the training images. The neural network can easily be distracted by task-irrelevant objects. In this letter, we use the human gaze measured by a head-mounted eye tracking device to discard task-irrelevant visual distractions. We propose a mixture density network-based behavior cloning method that learns to imitate the human gaze. The model predicts gaze positions from raw pixel images and crops images around the predicted gazes. Only these cropped images are used to compute the output action. This cropping procedure can remove visual distractions because the gaze is rarely fixated on task-irrelevant objects. This robustness against irrelevant objects can improve the manipulation performance of robots in scenarios where task-irrelevant objects are present. We evaluated our model on four manipulation tasks designed to test the robustness of the model to irrelevant objects. The results indicate that the proposed model can predict the locations of task-relevant objects from gaze positions, is robust to task-irrelevant objects, and exhibits impressive manipulation performance especially in multi-object handling. © 2016 IEEE.","computer vision for automation; Deep learning in grasping and manipulation; learning from demonstration; telerobotics and teleoperation; visual servoing","Agricultural robots; Clone cells; Deep learning; Pixels; Head-mounted eye tracking; Imitation learning; Manipulation task; Mixture density; Robot manipulation; Task relevant; Training image; Visual distractions; Eye tracking",Article,"Final","",Scopus,2-s2.0-85086446787
"Wang C., Komninos C., Andersen S., D’Ettorre C., Dwyer G., Maneas E., Edwards P., Desjardins A., Stilli A., Stoyanov D.","57216740613;57216752775;57216756302;57201490650;56742945400;56038489100;7401881894;57204519454;56423623500;57203105770;","Ultrasound 3D reconstruction of malignant masses in robotic-assisted partial nephrectomy using the PAF rail system: a comparison study",2020,"International Journal of Computer Assisted Radiology and Surgery","15","7",,"1147","1155",,1,"10.1007/s11548-020-02149-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084457669&doi=10.1007%2fs11548-020-02149-4&partnerID=40&md5=89cccd8bda3300e19f8bd775b39c3269","Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, 43-45 Foley St., Fitzrovia, London, W1W 7EJ, United Kingdom; Department of Electrical and Computer Engineering, University of Patras, Rio, Patras, 26504, Greece; Department of Computer Science, Stanford University, 353 Serra Mall, Stanford, CA  94305, United States","Wang, C., Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, 43-45 Foley St., Fitzrovia, London, W1W 7EJ, United Kingdom; Komninos, C., Department of Electrical and Computer Engineering, University of Patras, Rio, Patras, 26504, Greece; Andersen, S., Department of Computer Science, Stanford University, 353 Serra Mall, Stanford, CA  94305, United States; D’Ettorre, C., Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, 43-45 Foley St., Fitzrovia, London, W1W 7EJ, United Kingdom; Dwyer, G., Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, 43-45 Foley St., Fitzrovia, London, W1W 7EJ, United Kingdom; Maneas, E., Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, 43-45 Foley St., Fitzrovia, London, W1W 7EJ, United Kingdom; Edwards, P., Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, 43-45 Foley St., Fitzrovia, London, W1W 7EJ, United Kingdom; Desjardins, A., Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, 43-45 Foley St., Fitzrovia, London, W1W 7EJ, United Kingdom; Stilli, A., Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, 43-45 Foley St., Fitzrovia, London, W1W 7EJ, United Kingdom; Stoyanov, D., Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, 43-45 Foley St., Fitzrovia, London, W1W 7EJ, United Kingdom","Purpose: In robotic-assisted partial nephrectomy (RAPN), the use of intraoperative ultrasound (IOUS) helps to localise and outline the tumours as well as the blood vessels within the kidney. The aim of this work is to evaluate the use of the pneumatically attachable flexible (PAF) rail system for US 3D reconstruction of malignant masses in RAPN. The PAF rail system is a novel device developed and previously presented by the authors to enable track-guided US scanning. Methods: We present a comparison study between US 3D reconstruction of masses based on: the da Vinci Surgical System kinematics, single- and stereo-camera tracking of visual markers embedded on the probe. An US-realistic kidney phantom embedding a mass is used for testing. A new design for the US probe attachment to enhance the performance of the kinematic approach is presented. A feature extraction algorithm is proposed to detect the margins of the targeted mass in US images. Results: To evaluate the performance of the investigated approaches the resulting 3D reconstructions have been compared to a CT scan of the phantom. The data collected indicates that single camera reconstruction outperformed the other approaches, reconstructing with a sub-millimetre accuracy the targeted mass. Conclusions: This work demonstrates that the PAF rail system provides a reliable platform to enable accurate US 3D reconstruction of masses in RAPN procedures. The proposed system has also the potential to be employed in other surgical procedures such as hepatectomy or laparoscopic liver resection. © 2020, The Author(s).","3D ultrasound; Laparoscopy; Soft robotics; Surgical robotics","tenidap; accuracy; Article; calibration; comparative study; computer assisted tomography; controlled study; eye tracking; feature extraction algorithm; human; image reconstruction; kidney tumor; kinematics; partial nephrectomy; peroperative echography; priority journal; robot assisted surgery; three dimensional echography; interventional ultrasonography; laparoscopy; nephrectomy; procedures; robot assisted surgery; three-dimensional imaging; treatment outcome; x-ray computed tomography; Humans; Imaging, Three-Dimensional; Laparoscopy; Nephrectomy; Robotic Surgical Procedures; Tomography, X-Ray Computed; Treatment Outcome; Ultrasonography, Interventional",Article,"Final","",Scopus,2-s2.0-85084457669
"Fabiano D., Canavan S., Agazzi H., Hinduja S., Goldgof D.","57207798431;24490527800;36175638700;57210364585;35572085000;","Gaze-based classification of autism spectrum disorder",2020,"Pattern Recognition Letters","135",,,"204","212",,1,"10.1016/j.patrec.2020.04.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084440615&doi=10.1016%2fj.patrec.2020.04.028&partnerID=40&md5=176bb71be3fcedee77aa020eb34dd1a5","University of South Florida, 4202 E. Fowler Ave, Tampa, FL  33620, United States","Fabiano, D., University of South Florida, 4202 E. Fowler Ave, Tampa, FL  33620, United States; Canavan, S., University of South Florida, 4202 E. Fowler Ave, Tampa, FL  33620, United States; Agazzi, H., University of South Florida, 4202 E. Fowler Ave, Tampa, FL  33620, United States; Hinduja, S., University of South Florida, 4202 E. Fowler Ave, Tampa, FL  33620, United States; Goldgof, D., University of South Florida, 4202 E. Fowler Ave, Tampa, FL  33620, United States","People with autism spectrum disorder (ASD) display impairments in social interaction and communication skills, as well as restricted interests and repetitive behaviors, which greatly affect daily life functioning. Current identification of ASD involves a lengthy process that requires an experienced clinician to assess multiple domains of functioning. Considering this, we propose a method for classifying multiple levels of risk of ASD using eye gaze and demographic feature descriptors such as a subject's age and gender. We construct feature descriptors that incorporate the subject's age and gender, as well as features based on eye gaze patterns. We also present an analysis of eye gaze patterns validating the use of the selected hand-crafted features. We assess the efficacy of our descriptors to classify ASD on a National Database for Autism Research dataset, using multiple classifiers including a random forest, C4.5 decision tree, PART, and a deep feedforward neural network. © 2020 Elsevier B.V.","Autism spectrum disorder; Classification; Gaze; Machine learning","Decision trees; Diseases; Feedforward neural networks; Autism spectrum disorders; C4.5 decision trees; Communication skills; Demographic features; Feature descriptors; Multiple classifiers; National database; Social interactions; Classification (of information)",Article,"Final","",Scopus,2-s2.0-85084440615
"Zhu J., Wang Z., Gong T., Zeng S., Li X., Hu B., Li J., Sun S., Zhang L.","57189245082;57211256510;57216614233;57206483657;55718091300;56709128700;57191420467;57189250412;56412291000;","An Improved Classification Model for Depression Detection Using EEG and Eye Tracking Data",2020,"IEEE Transactions on Nanobioscience","19","3","9079496","527","537",,3,"10.1109/TNB.2020.2990690","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084093346&doi=10.1109%2fTNB.2020.2990690&partnerID=40&md5=b0025d9fc7c13dc75307681de6855130","Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Lanzhou University Second Hospital, Lanzhou, 730030, China","Zhu, J., Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Wang, Z., Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Gong, T., Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Zeng, S., Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Li, X., Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Hu, B., Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Li, J., Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Sun, S., Gansu Provincial Key Laboratory of Wearable Computing, School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China; Zhang, L., Lanzhou University Second Hospital, Lanzhou, 730030, China","At present, depression has become a main health burden in the world. However, there are many problems with the diagnosis of depression, such as low patient cooperation, subjective bias and low accuracy. Therefore, reliable and objective evaluation method is needed to achieve effective depression detection. Electroencephalogram (EEG) and eye movements (EMs) data have been widely used for depression detection due to their advantages of easy recording and non-invasion. This research proposes a content based ensemble method (CBEM) to promote the depression detection accuracy, both static and dynamic CBEM were discussed. In the proposed model, EEG or EMs dataset was divided into subsets by the context of the experiments, and then a majority vote strategy was used to determine the subjects' label. The validation of the method is testified on two datasets which included free viewing eye tracking and resting-state EEG, and these two datasets have 36,34 subjects respectively. For these two datasets, CBEM achieves accuracies of 82.5% and 92.65% respectively. The results show that CBEM outperforms traditional classification methods. Our findings provide an effective solution for promoting the accuracy of depression identification, and provide an effective method for identificationof depression, which in the future could be used for the auxiliary diagnosis of depression. © 2002-2011 IEEE.","Affective computing; Depression detection; EEG; Ensemble method; Eye tracking","Eye movements; Eye tracking; Classification methods; Classification models; Detection accuracy; Effective solution; Electro-encephalogram (EEG); Ensemble methods; Objective evaluation; Resting state; Electroencephalography; algorithm; computer assisted diagnosis; depression; electroencephalography; female; human; machine learning; male; procedures; signal processing; Algorithms; Depression; Diagnosis, Computer-Assisted; Electroencephalography; Eye-Tracking Technology; Female; Humans; Machine Learning; Male; Signal Processing, Computer-Assisted",Article,"Final","",Scopus,2-s2.0-85084093346
"Couture Bue A.C.","57211001390;","The looking glass selfie: Instagram use frequency predicts visual attention to high-anxiety body regions in young women",2020,"Computers in Human Behavior","108",,"106329","","",,5,"10.1016/j.chb.2020.106329","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081131592&doi=10.1016%2fj.chb.2020.106329&partnerID=40&md5=144433147e50e84f4896a3f5079cf516","Communication Studies, University of Michigan, 5344 North Quad, 105 S. State Street, Ann Arbor, MI  48109, United States","Couture Bue, A.C., Communication Studies, University of Michigan, 5344 North Quad, 105 S. State Street, Ann Arbor, MI  48109, United States","Existing research links social media use with body dissatisfaction and increased appearance comparison, demonstrating that photo-based behaviors such as taking and posting ‘selfies’ may lead to increased risk of body image disturbance (Cohen, Newton-John, & Slater, 2017). Social media users frequently interact with photos of themselves online while using social media platforms. When looking at a self-photo, individuals can selectively focus attention on body regions that are self-reported as attractive or unattractive; attention to unattractive regions can lead to increased body dissatisfaction (Smeets, Jansen, & Roefs, 2011). Likewise, body dissatisfaction is a predictor of selective attention to self-reported unattractive regions (Glashouwer, Jonker, Thomassen, & de Jong, 2016), creating a reinforcing cycle. The present study used eye-tracking methods to examine how 157 women aged 18–35 visually processed a self-photo, measuring attention to self-reported high- and low-anxiety body regions. Even accounting for baseline body dissatisfaction, Instagram but not Facebook use frequency predicted greater visual attention to high-anxiety body regions, with physical appearance comparison and body dissatisfaction serving as serial mediators. Discussion focuses on social comparison theory, suggesting that Instagram (a highly visual platform) encourages upward social comparisons when self-evaluating, prompting attention to body regions perceived as less attractive. © 2020 Elsevier Ltd","Body dissatisfaction; Eye tracking; Instagram; Selfies; Social comparison theory; Social media","Behavioral research; Social networking (online); Body dissatisfaction; Instagram; Selfies; Social comparison theory; Social media; Eye tracking",Article,"Final","",Scopus,2-s2.0-85081131592
"Neumann A., Strenge B., Uhlich J.C., Schlicher K.D., Maier G.W., Schalkwijk L., Waßmuth J., Essig K., Schack T.","57196857875;55804522900;57205445265;57204504714;7201409088;57218221543;57218222301;6701685019;22635455800;","AVIKOM: Towards a mobile audiovisual cognitive assistance system for modern manufacturing and logistics",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"3389191","1","8",,3,"10.1145/3389189.3389191","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088367709&doi=10.1145%2f3389189.3389191&partnerID=40&md5=5cb410c36be01f88305b275691b5d3bf","Bielefeld University, Bielefeld, Germany; Bielefeld University of Applied Sciences, Bielefeld, Germany; Rhein-Waal University of Applied Sciences Communication and Environment, Kamp-Lintfort, Germany","Neumann, A., Bielefeld University, Bielefeld, Germany; Strenge, B., Bielefeld University, Bielefeld, Germany; Uhlich, J.C., Bielefeld University, Bielefeld, Germany; Schlicher, K.D., Bielefeld University, Bielefeld, Germany; Maier, G.W., Bielefeld University, Bielefeld, Germany; Schalkwijk, L., Bielefeld University of Applied Sciences, Bielefeld, Germany; Waßmuth, J., Bielefeld University of Applied Sciences, Bielefeld, Germany; Essig, K., Rhein-Waal University of Applied Sciences Communication and Environment, Kamp-Lintfort, Germany; Schack, T., Bielefeld University, Bielefeld, Germany","This paper introduces the novel Augmented Reality (AR) assistance system AVIKOM, a joint endeavour of three research groups together with four small and medium-sized enterprises (SME) as well as network partners and a diaconal institution. In particular, we investigate how AR-enabled assistance systems can be tailored to individual requirements of workers with diverse cognitive and physical capabilities for today's real-world industrial applications in the areas of (manual) assembly, logistics and operation of industrial machinery. We combine best practices from the domains of artificial intelligence, machine learning, user experience engineering, ethics research, and cognitive science with state-of-the-art insights for multi-modal system development to create a cognitive action assistance system that recognizes and adapts to individual users in various situational contexts, such as picking and training. Proven work and organizational psychology methods and worth-related evaluations will accompany the system introduction into working environments. Using user- and worth-centred system design and change management strategies (e.g. information and participation) right from the beginning of such a technological development facilitates proper involvement of future users in the development process. This can lead to better congruence of technology features with workers' requirements and positively shape future users' attitudes towards the system. © 2020 ACM.","assistive systems; augmented reality (AR); eye tracking; individualized feedback; scene and action understanding","Artificial intelligence; Augmented reality; Cognitive systems; Machinery; User experience; Cognitive assistance; Development process; Industrial machinery; Organizational psychology; Physical capabilities; Situational context; Small- and medium-sized enterprise; Technological development; Information management",Conference Paper,"Final","",Scopus,2-s2.0-85088367709
"Cha X., Yang X., Zhang Y., Feng Z., Xu T., Fan X.","57214798390;55683790100;57219185145;7403443516;56683649700;57197729874;","Eye Tracking in Driving Environment Based on Multichannel Convolutional Neural Network",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"141","144",,,"10.1145/3408127.3408179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091591611&doi=10.1145%2f3408127.3408179&partnerID=40&md5=0cb1e0445980a08f9b7f5b983c4561a9","School of Information Science and Engineering, University of Jinan, Jinan, China; Shandong Provincial Key Laboratory of Network Based Intelligent Computing, Jinan, China","Cha, X., School of Information Science and Engineering, University of Jinan, Jinan, China; Yang, X., School of Information Science and Engineering, University of Jinan, Jinan, China; Zhang, Y., School of Information Science and Engineering, University of Jinan, Jinan, China; Feng, Z., Shandong Provincial Key Laboratory of Network Based Intelligent Computing, Jinan, China; Xu, T., School of Information Science and Engineering, University of Jinan, Jinan, China; Fan, X., School of Information Science and Engineering, University of Jinan, Jinan, China","Gaze is the most important way for human to obtain information from the outside world, and it is the most direct and significant cue to analysis human behavior and intention. In driving environment, eye tracking is usually applied to model driver's fixations and gaze allocations, which is important in advanced driver assistance system (ADAS). In this paper, we have proposed a new eye tracking method in driving environment, which is based on multichannel convolutional neural network. Firstly, we establish the dataset for driver's eye tracking, which includes the left eye region image, the right eye region image and the face region image. After that, the multi-channel convolutional neural network is training using the dataset. Finally, the driver's gaze zone will be estimated using the pre-trained network. Experimental results show that the accuracy of the proposed method is 94.60% for seven gaze zone estimation, and it can be used in ADAS to analysis the driver's behavior and detect driver distraction. © 2020 ACM.","advanced driver assistance system; Deep learning; driver distraction; eye tracking; gaze zone estimation","Advanced driver assistance systems; Automobile drivers; Behavioral research; Convolution; Convolutional neural networks; Digital signal processing; Driver distractions; Driver's behavior; Driving environment; Eye tracking methods; Face regions; Human behaviors; Model drivers; Multichannel; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85091591611
"Song H., Yang M., Li T., Chen S.","57219178937;57210466465;57210468511;24491760700;","Fixation Points Estimation Based on Binocular Stereo Vision",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"71","74",,,"10.1145/3408127.3408198","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091575432&doi=10.1145%2f3408127.3408198&partnerID=40&md5=2e058f149a077d566bccdde7ae813d58","School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China","Song, H., School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; Yang, M., School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; Li, T., School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; Chen, S., School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China","This paper proposes a method of fixation points estimation combining the two-dimension mapping model with three-dimension stereo vision. The purpose of our study is to design an easy-to-use non-contact gaze tracking system under natural light. The method of two-dimension mapping is easy to achieve, however, it needs the user to keep the head being fixed. It could achieve higher estimation accuracy though it is still not easy for users to use the algorithm. To solve this problem, we have introduced the binocular cameras to calculate the pose of head and then add the related result into the result of 2D mapping to compensate the movement of head. The average error angles of gaze estimation in the case head fixed and head moving are 5.1°and 8.5°, respectively. This proposed method is easy to achieve and the experiment device is easy to mounted. © 2020 ACM.","binocular cameras; estimation of fixation points; Gaze estimation; Iris center location; Stereo imaging","Digital signal processing; Eye tracking; Mapping; Stereo image processing; Average errors; Binocular camera; Binocular stereo vision; Fixation point; Gaze estimation; Gaze tracking system; Three dimensions; Two-dimension; Stereo vision",Conference Paper,"Final","",Scopus,2-s2.0-85091575432
"Wu H., Feng J., Tian X., Sun E., Liu Y., Dong B., Xu F., Zhong S.","57202218745;57217515048;57191916901;57213811954;57203674247;57218159448;57205067497;7202152664;","EMO: Real-time emotion recognition from single-eye images for resource-constrained eyewear devices",2020,"MobiSys 2020 - Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services",,,,"448","461",,4,"10.1145/3386901.3388917","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088116516&doi=10.1145%2f3386901.3388917&partnerID=40&md5=a9a42f59b7ef97475fc74ea4ca30b300","Nanjing University, China; Microsoft Research, United States; Kitware, United States","Wu, H., Nanjing University, China; Feng, J., Nanjing University, China; Tian, X., Nanjing University, China; Sun, E., Nanjing University, China; Liu, Y., Microsoft Research, United States; Dong, B., Kitware, United States; Xu, F., Nanjing University, China; Zhong, S., Nanjing University, China","Real-time user emotion recognition is highly desirable for many applications on eyewear devices like smart glasses. However, it is very challenging to enable this capability on such devices due to tightly constrained image contents (only eye-area images available from the on-device eye-tracking camera) and computing resources of the embedded system. In this paper, we propose and develop a novel system called EMO that can recognize, on top of a resource-limited eyewear device, real-time emotions of the user who wears it. Unlike most existing solutions that require whole-face images to recognize emotions, EMO only utilizes the single-eye-area images captured by the eye-tracking camera of the eyewear. To achieve this, we design a customized deep-learning network to effectively extract emotional features from input single-eye images and a personalized feature classifier to accurately identify a user's emotions. EMO also exploits the temporal locality and feature similarity among consecutive video frames of the eye-tracking camera to further reduce the recognition latency and system resource usage. We implement EMO on two hardware platforms and conduct comprehensive experimental evaluations. Our results demonstrate that EMO can continuously recognize seven-type emotions at 12.8 frames per second with a mean accuracy of 72.2%, significantly outperforming the state-of-the-art approach, and consume much fewer system resources. © 2020 ACM.","Deep learning; Emotion recognition; Eyewear devices; Single-eye images; Visual sensing","Cameras; Deep learning; Speech recognition; Computing resource; Experimental evaluation; Feature classifiers; Feature similarities; Frames per seconds; Real-time emotion recognition; State-of-the-art approach; Temporal locality; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85088116516
"Koorathota S.C., Thakoor K., Adelman P., Mao Y., Liu X., Sajda P.","57203854483;55938126400;57217114663;57212456758;57190950885;57204342918;","Sequence Models in Eye Tracking: Predicting Pupil Diameter during Learning",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3391653","","",,1,"10.1145/3379157.3391653","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086315562&doi=10.1145%2f3379157.3391653&partnerID=40&md5=170deff4b2e170a13e3c4a77ead588db","Columbia University, Fractal Media, United States","Koorathota, S.C., Columbia University, Fractal Media, United States; Thakoor, K., Columbia University, Fractal Media, United States; Adelman, P., Columbia University, Fractal Media, United States; Mao, Y., Columbia University, Fractal Media, United States; Liu, X., Columbia University, Fractal Media, United States; Sajda, P., Columbia University, Fractal Media, United States","A deep learning framework for predicting pupil diameter using eye tracking data is described. Using a variety of input, such as fixation positions, durations, saccades and blink-related information, we assessed the performance of a sequence model in predicting future pupil diameter in a student population as they watched educational videos in a controlled setting. Through assessing student performance on a post-viewing test, we report that deep learning sequence models may be useful for separating components of pupil responses that are linked to luminance and accommodation from those that are linked to cognition and arousal. © 2020 ACM.","attention; embeddings; eye tracking; neural networks","Deep learning; Forecasting; Learning systems; Educational videos; Learning frameworks; Learning sequences; Pupil response; Sequence modeling; Sequence models; Student performance; Student populations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086315562
"Jogeshwar A.K.","57193727303;","Analysis and visualization tool for motion and gaze",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3391996","","",,1,"10.1145/3379157.3391996","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086313667&doi=10.1145%2f3379157.3391996&partnerID=40&md5=9e0e9a76cad89f064712192fd000eeb3","Rochester Institute of Technology, Rochester, NY, United States","Jogeshwar, A.K., Rochester Institute of Technology, Rochester, NY, United States","Observers' gaze is studied as a marker of attention, and by tracking the eyes, one can obtain gaze data. Attention of an individual performing natural tasks such as making a sandwich, playing squash, or teaching a class can be studied with the help of eye-tracking. Data analysis of real world interaction is challenging and time-consuming as it consists of varying or undefined environments, massive amounts of video data and unrestricted movement. To approach these challenges, my research aims to create an interactive four-dimensional (x,y,z,t) tool for the analysis and visualization of observer motion and gaze data, of one or more observers performing natural day-to-day tasks. Three solutions are necessary to achieve this goal: Simulation of the environment with the ability to vary viewpoint, gaze visualization from two-dimensional scene to three-dimensions, and tracing of the observer(s) motion. The approaches to these challenges are described in the following sections. © 2020 Owner/Author.","3D analysis; Gaze visualization; modeled environment; motion tracking; natural task","Data visualization; Visualization; Natural tasks; Real-world; Three dimensions; Three solutions; Video data; Visualization tools; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086313667
"Lohr D.J., Aziz S., Komogortsev O.","57189846143;57216313731;6506328653;","Eye Movement Biometrics Using a New Dataset Collected in Virtual Reality",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3391420","","",,4,"10.1145/3379157.3391420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086309204&doi=10.1145%2f3379157.3391420&partnerID=40&md5=c4cfba9899899977a1e3801d4b4b6937","Texas State University, San Marcos, TX, United States","Lohr, D.J., Texas State University, San Marcos, TX, United States; Aziz, S., Texas State University, San Marcos, TX, United States; Komogortsev, O., Texas State University, San Marcos, TX, United States","This paper introduces a novel eye movement dataset collected in virtual reality (VR) that contains both 2D and 3D eye movement data from over 400 subjects. We establish that this dataset is suitable for biometric studies by evaluating it with both statistical and machine learning-based approaches. For comparison, we also include results from an existing, similarly constructed dataset. © 2020 Owner/Author.","authentication; biometrics; eye tracking; virtual reality","Biometrics; Eye tracking; Virtual reality; Eye movement datum; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85086309204
"Jayawardena G.","57206894319;","RAEMAP: Real-Time Advanced Eye Movements Analysis Pipeline",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3391992","","",,2,"10.1145/3379157.3391992","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086305317&doi=10.1145%2f3379157.3391992&partnerID=40&md5=ca6e7c18263174a0003a23c734959247","Old Dominion University, Norfolk, VA, United States","Jayawardena, G., Old Dominion University, Norfolk, VA, United States","Eye-tracking measures enable means to understand the underlying covert processes engaged during inhibitory tasks which rely on attention allocation. We propose Real-Time Advanced Eye Movements Analysis Pipeline (RAEMAP) to utilize eye tracking measures as a valid psychophysiological measure. RAEMAP will include realtime analysis of the traditional positional gaze metrics as well as advanced metrics such as ambient/focal coefficient κ, gaze transition entropy, and index of pupillary activity (IPA). RAEMAP will also provide visualizations of calculated eye gaze metrics, heatmaps, and dynamic AOI generation in real-time. This paper will outline the proposed architecture of RAEMAP in terms of distributed computing, incorporation of machine learning models, and the evaluation to prove the utility of RAEMAP to diagnose ADHD in real-time. © 2020 Owner/Author.","ADHD; Distributed-Computing; Eye-Tracking; Health Analytics","Eye movements; Pipelines; Eye-gaze; Heatmaps; Machine learning models; Proposed architectures; Psychophysiological measures; Real time; Real time analysis; Transition entropies; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086305317
"Kim J.-H., Jeong J.-W.","57205720618;57205722835;","Gaze Estimation in the Dark with Generative Adversarial Networks",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3391654","","",,,"10.1145/3379157.3391654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086303847&doi=10.1145%2f3379157.3391654&partnerID=40&md5=5647fe5a5546555b6c69055260bf2e2b","Kumoh National Institute of Technology, South Korea","Kim, J.-H., Kumoh National Institute of Technology, South Korea; Jeong, J.-W., Kumoh National Institute of Technology, South Korea","In this paper, we propose to utilize generative adversarial networks (GANs) to achieve successful gaze estimation in interactive multimedia environments with low light conditions such as a digital museum or exhibition hall. The proposed approach utilizes a GAN to enhance user images captured under low-light conditions, thereby recovering missing information for gaze estimation. The recovered images are fed into the CNN architecture to estimate the direction of user gaze. The preliminary experimental results on the modified MPIIGaze dataset demonstrated an average performance improvement of 6.6 under various low light conditions, which is a promising step for further research. © 2020 ACM.","deep learning; GAN; Gaze estimation; low-light environment","Exhibition buildings; Image enhancement; Interactive computer systems; Multimedia systems; Adversarial networks; Digital museums; Exhibition halls; Gaze estimation; Interactive multimedia; Low light conditions; Missing information; User images; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086303847
"Nair N., Chaudhary A.K., Kothari R.S., Diaz G.J., Pelz J.B., Bailey R.","57215969974;57210103548;56533410500;55436582600;7007018556;16641965200;","RIT-Eyes: Realistically rendered eye images for eye-tracking applications",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3391990","","",,1,"10.1145/3379157.3391990","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086303721&doi=10.1145%2f3379157.3391990&partnerID=40&md5=3dedebcff3d7d8764fa8501b66aa21bd","Rochester Institute of Technology, Rochester, NY, United States","Nair, N., Rochester Institute of Technology, Rochester, NY, United States; Chaudhary, A.K., Rochester Institute of Technology, Rochester, NY, United States; Kothari, R.S., Rochester Institute of Technology, Rochester, NY, United States; Diaz, G.J., Rochester Institute of Technology, Rochester, NY, United States; Pelz, J.B., Rochester Institute of Technology, Rochester, NY, United States; Bailey, R., Rochester Institute of Technology, Rochester, NY, United States","Convolutional neural network-based solutions for video oculography require large quantities of accurately labeled eye images acquired under a wide range of image quality, surrounding environmental reflections, feature occlusion, and varying gaze orientations. Manually annotating such a dataset is challenging, time-consuming, and error-prone. To alleviate these limitations, this work introduces an improved eye image rendering pipeline designed in Blender. RIT-Eyes provides access to realistic eye imagery with error-free annotations in 2D and 3D which can be used for developing gaze estimation algorithms. Furthermore, RIT-Eyes is capable of generating novel temporal sequences with realistic blinks and mimicking eye and head movements derived from publicly available datasets. © 2020 Owner/Author.","computer graphics; Datasets; eyetracking; rendering; segmentation","Convolutional neural networks; Eye movements; Image enhancement; Rendering (computer graphics); Error prones; Eye images; Gaze estimation; Head movements; Temporal sequences; Video oculography; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086303721
"Hildebrandt M., Langstrand J.-P., Nguyen H.T.","8209590400;57209102920;57209108684;","Synopticon: Sensor Fusion for Automated Gaze Analysis",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3391986","","",,,"10.1145/3379157.3391986","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086302459&doi=10.1145%2f3379157.3391986&partnerID=40&md5=cc76e585d0e469a89eccc8442309b4fb","Institute for Energy Technology, Halden, Norway","Hildebrandt, M., Institute for Energy Technology, Halden, Norway; Langstrand, J.-P., Institute for Energy Technology, Halden, Norway; Nguyen, H.T., Institute for Energy Technology, Halden, Norway","This demonstration presents Synopticon, an open-source software system for automatic, real-time gaze object detection for mobile eye tracking. The system merges gaze data from eye tracking glasses with position data from a motion capture system and projects the resulting gaze vector onto a 3D model of the environment. © 2020 Owner/Author.","automated analysis; data fusion; eye tracking; position tracking","3D modeling; Motion tracking; Object detection; Object tracking; Open source software; Open systems; Gaze analysis; Mobile eye-tracking; Motion capture system; Open source software systems; Position data; Real time; Sensor fusion; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086302459
"Li Y., Zhan Y., Yang Z.","57205654781;14030938900;57203791621;","Evaluation of appearance-based eye tracking calibration data selection",2020,"Proceedings of 2020 IEEE International Conference on Artificial Intelligence and Computer Applications, ICAICA 2020",,,"9181854","222","224",,1,"10.1109/ICAICA50127.2020.9181854","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092146609&doi=10.1109%2fICAICA50127.2020.9181854&partnerID=40&md5=37d95106606ab91f001382339f7866c9","School of Computers, Guangdong University of Technology, Guangzhou, China","Li, Y., School of Computers, Guangdong University of Technology, Guangzhou, China; Zhan, Y., School of Computers, Guangdong University of Technology, Guangzhou, China; Yang, Z., School of Computers, Guangdong University of Technology, Guangzhou, China","Eye tracking is a valuable topic in computer vision. Appearance-based eye tracking is a promising research direction in recent years. Convolutional neural networks (CNN) had been used in gaze estimation, which cover the significant variability in eye appearance caused by unconstrained head motion. With computation capability of consumer devices rapidly evolving, accurate and efficient appearance-based eye tracking has the potential for multipurpose applications. Person-independent networks have limit in improving gaze estimation accuracy. Person-specific network with calibration is more effective than person-independent approaches. Unlike classical eye tracking methods, appearance-based eye tracking has not a clear way to calibration. Our goal is to analyze the impact of calibration data selection and calibration target distribution on person-specific gaze estimation accuracy. We trained person-independent network and use SVR to calibration. We choose two kind of typical distribution targets to evaluation. Use different distribution targets to calibration achieves different accuracy. © 2020 IEEE.","calibration; eye tracking; gaze; human computer interaction","Calibration; Convolutional neural networks; Data reduction; Appearance based; Calibration data; Calibration targets; Consumer devices; Different distributions; Eye tracking methods; Gaze estimation; Person-independent; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85092146609
"Caporusso N., Zhang K., Carlson G.","21734204500;57210235168;36607389200;","Using Eye-tracking to Study the Authenticity of Images Produced by Generative Adversarial Networks",2020,"2nd International Conference on Electrical, Communication and Computer Engineering, ICECCE 2020",,,"9179472","","",,,"10.1109/ICECCE49384.2020.9179472","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091942527&doi=10.1109%2fICECCE49384.2020.9179472&partnerID=40&md5=05df71780d732155fcdf81ac356800a3","Northern Kentucky University, Department of Computer Science, Highland Heights, United States; Fort Hays State University, Department of Informatics, Hays, United States; Fort Hays State University, Department of Communication Studies, Hays, United States","Caporusso, N., Northern Kentucky University, Department of Computer Science, Highland Heights, United States; Zhang, K., Fort Hays State University, Department of Informatics, Hays, United States; Carlson, G., Fort Hays State University, Department of Communication Studies, Hays, United States","Nowadays, Machine Learning algorithms, such as Generative Adversarial Networks (GANs), enable generating content, and especially images, featuring people, objects, or landscapes, with unprecedented levels of accuracy and fidelity. As a result, it is becoming challenging for a viewer to distinguish a picture of a fake profile from one that has a real human in it. In this paper, we present the results of an experimental study in which we investigated the perception of images produced by GANs. Specifically, we focused on the individuals' ability to discriminate between fake and real profiles. Furthermore, we utilized eye-tracking technology to identify the presence of patterns in subjects' gaze, which, in turn, can be useful to optimize the output of GANs and, simultaneously, provide insight on the underlying cognitive dynamics. © 2020 IEEE.","Cybersecurity; Eye tracking; Faceforensics; Generative Adversarial Networks","Learning algorithms; Machine learning; Adversarial networks; Cognitive dynamics; Eye tracking technologies; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85091942527
"Kocejko T., Weglerski R., Zubowicz T., Ruminski J., Wtorek J., Arminski K.","24824292600;57219132476;55556170400;6603237944;6701488611;55556788100;","Design aspects of a low-cost prosthetic arm for people with severe movement disabilities",2020,"International Conference on Human System Interaction, HSI","2020-June",,"9142647","295","299",,,"10.1109/HSI49210.2020.9142647","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091394167&doi=10.1109%2fHSI49210.2020.9142647&partnerID=40&md5=0c3696742fd34e70db7b910fc8100737","Faculty of Electronics, Telecommunication and Informatics, Gdansk University of Technology, Department of Biomedical Engineering, Gdansk, Poland; Faculty of Electrical and Control Engineering, Gdansk University of Technology, Department of Control Engineering, Gdansk, Poland; Control Systems and Informatics, Faculty of Electrical and Control Engineering Gdansk University of Technology, Department of Electrical Engineering, Gdansk, Poland","Kocejko, T., Faculty of Electronics, Telecommunication and Informatics, Gdansk University of Technology, Department of Biomedical Engineering, Gdansk, Poland; Weglerski, R., Faculty of Electronics, Telecommunication and Informatics, Gdansk University of Technology, Department of Biomedical Engineering, Gdansk, Poland; Zubowicz, T., Faculty of Electrical and Control Engineering, Gdansk University of Technology, Department of Control Engineering, Gdansk, Poland; Ruminski, J., Faculty of Electronics, Telecommunication and Informatics, Gdansk University of Technology, Department of Biomedical Engineering, Gdansk, Poland; Wtorek, J., Faculty of Electronics, Telecommunication and Informatics, Gdansk University of Technology, Department of Biomedical Engineering, Gdansk, Poland; Arminski, K., Control Systems and Informatics, Faculty of Electrical and Control Engineering Gdansk University of Technology, Department of Electrical Engineering, Gdansk, Poland","In this paper the main aspects of mechanical design behind the low-cost prosthetic arm are presented. The fundamentals of a proper design has been defined to obtain functional 3D printed 5 degree of freedom (DOF) prosthesis. The designed prosthetic arm is a part of the hybrid interface with eye tracking movement control. The main focus was to create affordable but usable prosthesis which corresponds in size and weights to the human arm. The iterative process (starting from the final segment of the arm) was used to design fully functioning arm. All the elements were evaluated regarding total weight and the maximum load that can be carried by the arm. The result of this work is a prototype that weighs below 6kg and has a range of motion comparable to the human's arm. Final product is able to freely move an object of a total weight of 1 kg. All the mechanical parts of the designed arm were 3D printed which therefore presented construction can be adopted by people with different disabilities and (when connected to interfaces like EEG, EMG or eye tracking) provide support in everyday life activities. © 2020 IEEE.","CAD; Eye tracking; Prosthetic arm; Prosthetics","Costs; Degrees of freedom (mechanics); Eye movements; Eye tracking; Prosthetics; Degree of freedom (dof); Hybrid interface; Iterative process; Mechanical design; Mechanical parts; Movement control; Prosthetic arm; Range of motions; 3D printers",Conference Paper,"Final","",Scopus,2-s2.0-85091394167
"Hu L., Gao J.","57220914676;57219025059;","Research on real-time distance measurement of mobile eye tracking system based on neural network",2020,"Proceedings of 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference, ITOEC 2020",,,"9141800","1561","1565",,,"10.1109/ITOEC49072.2020.9141800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091123679&doi=10.1109%2fITOEC49072.2020.9141800&partnerID=40&md5=6ae86befa0ac32311ad0f30d9bb29801","University of Chinese Academy of Sciences, Beijing, China; Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, China","Hu, L., University of Chinese Academy of Sciences, Beijing, China, Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, China; Gao, J., University of Chinese Academy of Sciences, Beijing, China","With the development and application of eye-tracking technology, mobile eye-tracking systems have become more widely used due to their safety and portability. We combine eye-tracking systems with real-time object detection using machine learning. We propose a method of wearing an eye tracker in daily life to obtain the distance between the eye tracking system and the gaze target in real time. During the visual interaction of the eye tracking system, in order to obtain the distance from the eyeball fixation target to the eyeball in real time, the world camera of the mobile eye tracking system pupil labs first collects the position and scale information of the detected target image in real time, and uses camera calibration principle, pinhole camera model and camera distortion model to establish a ranging equation, and then the feasibility of the real-time ranging equation is verified through a specified distance experiment. The total average relative error after de-distortion at the position of 50cm-75cm is reduced to 1.25%, and the highest accuracy-0.9182cm distance measurement can be achieved within the effective distance. © 2020 IEEE.","distance measurement; mobile eye-tracking system; neural network; pinhole camera","Neural networks; Object detection; Object tracking; Pinhole cameras; Real time systems; Target tracking; Average relative error; Development and applications; Distance experiment; Eye tracking systems; Eye tracking technologies; Mobile eye-tracking; Pin-hole camera models; Visual interaction; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85091123679
"Liu C., Plopski A., Orlosky J.","57207047627;56023098100;55641218100;","OrthoGaze: Gaze-based three-dimensional object manipulation using orthogonal planes",2020,"Computers and Graphics (Pergamon)","89",,,"1","10",,3,"10.1016/j.cag.2020.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084676499&doi=10.1016%2fj.cag.2020.04.005&partnerID=40&md5=c4c7731e155cf4b0818dee97105e57f4","Graduate School of Information Science and Technology, Osaka University, Japan; Department of Information Science, University of Otago, Dunedin, New Zealand; Cybermedia Center, Osaka University, Japan","Liu, C., Graduate School of Information Science and Technology, Osaka University, Japan; Plopski, A., Department of Information Science, University of Otago, Dunedin, New Zealand; Orlosky, J., Cybermedia Center, Osaka University, Japan","In virtual and augmented reality, gaze-based methods have been explored for decades as effective user interfaces for hands-free interaction. Though several well-known gaze-based methods exist for simple interactions such as selection, no solutions exist for 3D manipulation tasks requiring a higher degree of freedom (DoF). In this paper, we introduce OrthoGaze, a novel user interface that allows users to intuitively manipulate the three-dimensional position of a virtual object using only their eye or head gaze. Our approach makes use of three selectable, orthogonal planes, where each plane not only helps guide the user's gaze in an arbitrary virtual space, but also allows for 2-DoF manipulations of object position. To evaluate our method, we conducted two user studies involving aiming and docking tasks in virtual reality to evaluate the fundamental characteristics of sustained gaze aiming and to determine which type of gaze-based control performs best when combined with OrthoGaze. Results showed that eye gaze was more accurate than head gaze for sustained aiming. Additionally, eye and head gaze-based control for 3D manipulations achieved 78% and 96% performance, respectively, in comparison with a hand-held controller. Subjective results also suggest that gaze-based manipulation can comprehensively cause more fatigue than controller-based. From the experimental results, we expect OrthoGaze to become an effective method for pure hands-free object manipulation in head-mounted displays. © 2020 Elsevier Ltd","Eye tracking; Human-computer interaction; Object manipulation; User interface","Augmented reality; Controllers; Degrees of freedom (mechanics); Helmet mounted displays; User interfaces; 3d manipulation tasks; Degree of freedom (dof); Dimensional position; Fundamental characteristics; Hands-free interactions; Head mounted displays; Three-dimensional object; Virtual and augmented reality; Virtual reality",Article,"Final","",Scopus,2-s2.0-85084676499
"Deng C., Han Y., Zhao B.","25958671000;57195983797;7403059245;","High-Performance Visual Tracking with Extreme Learning Machine Framework",2020,"IEEE Transactions on Cybernetics","50","6","8600730","2781","2792",,8,"10.1109/TCYB.2018.2886580","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084524438&doi=10.1109%2fTCYB.2018.2886580&partnerID=40&md5=f7c8c0c5549e2736e5323a60002e5193","School of Information and Electronics, Beijing Institute of Technology, Beijing, China","Deng, C., School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Han, Y., School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Zhao, B., School of Information and Electronics, Beijing Institute of Technology, Beijing, China","In real-time applications, a fast and robust visual tracker should generally have the following important properties: 1) feature representation of an object that is not only efficient but also has a good discriminative capability and 2) appearance modeling which can quickly adapt to the variations of foreground and backgrounds. However, most of the existing tracking algorithms cannot achieve satisfactory performance in both of the two aspects. To address this issue, in this paper, we advocate a novel and efficient visual tracker by exploiting the excellent feature learning and classification capabilities of an emerging learning technique, that is, extreme learning machine (ELM). The contributions of the proposed work are as follows: 1) motivated by the simplicity and learning ability of the ELM autoencoder (ELM-AE), an ELM-AE-based feature extraction model is presented, and this model can provide a compact and discriminative representation of the inputs efficiently and 2) due to the fast learning speed of an ELM classifier, an ELM-based appearance model is developed for feature classification, and is able to rapidly distinguish the object of interest from its surroundings. In addition, in order to cope with the visual changes of the target and its backgrounds, the online sequential ELM is used to incrementally update the appearance model. Plenty of experiments on challenging image sequences demonstrate the effectiveness and robustness of the proposed tracker. © 2013 IEEE.","extreme learning machine (ELM); Extreme learning machine autoencoder (ELM-AE); feature classification; feature learning; online sequential ELM (OS-ELM); robust visual tracking","Classification (of information); Knowledge acquisition; Machine learning; Appearance modeling; Extreme learning machine; Feature classification; Feature representation; Learning abilities; Learning techniques; Real-time application; Tracking algorithm; Object tracking; article; autoencoder; classifier; eye tracking; feature extraction; machine learning; velocity",Article,"Final","",Scopus,2-s2.0-85084524438
"Knoop-van Campen C.A.N., Segers E., Verhoeven L.","57193771349;8856236500;6603731251;","Effects of audio support on multimedia learning processes and outcomes in students with dyslexia",2020,"Computers and Education","150",,"103858","","",,6,"10.1016/j.compedu.2020.103858","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079872077&doi=10.1016%2fj.compedu.2020.103858&partnerID=40&md5=bf9701146c178e361412e4053b5d3f85","Behavioural Science Institute, Radboud University, Netherlands; Behavioural Science Institute, Radboud University, Department of Instructional Technology, University of Twente, Netherlands","Knoop-van Campen, C.A.N., Behavioural Science Institute, Radboud University, Netherlands; Segers, E., Behavioural Science Institute, Radboud University, Department of Instructional Technology, University of Twente, Netherlands; Verhoeven, L., Behavioural Science Institute, Radboud University, Netherlands","Adding audio to written text may cause redundancy effects, but could be beneficial for students with dyslexia for whom it supports their reading. Studying both learning process and learning outcomes in students with and without dyslexia can shed light on this issue and helps to find out whether there are constraints to the redundancy effect as proposed in the Cognitive Theory of Multimedia Learning. We examined to what extent adding -redundant- audio affects multimedia learning in 42 university students with dyslexia and 44 typically developing students. Participants studied two user-paced multimedia lessons (text-picture, text-audio-picture) with retention and transfer post-tests. An SMI RED-500 eye-tracker captured eye-movements during learning. Regarding process measures, students had longer study times, with more focus on pictures, and more transitions between text and pictures in the text-audio-picture condition. Regarding learning outcomes, negative redundancy effects on transfer knowledge (deep learning), but not on (factual) retention knowledge were found across both groups. When relating learning processes to learning outcomes, longer study time predicted higher transfer knowledge in both groups in the text-audio-picture condition, whereas in the text-picture condition, more study time predicted lower transfer knowledge in typically developing students only. To conclude, adding audio seems to have a negative effect on the quality of knowledge and leads to less efficient learning across the two groups. Reading ability does not impact the universality of the redundancy effect, but students with dyslexia should only use audio support when aiming to learn factual knowledge and should be aware that it increases study time. © 2020 The Authors","Dyslexia; Eye-tracking; Learning processes; Multimedia learning; Redundancy effect","Deep learning; E-learning; Eye movements; Eye tracking; Image processing; Redundancy; Students; Cognitive theory of multimedia learning; Dyslexia; Efficient learning; Factual knowledge; Learning process; Multi-media learning; Reading abilities; University students; Transfer learning",Article,"Final","",Scopus,2-s2.0-85079872077
"Joo H.-J., Jeong H.-Y.","54784711400;8865324700;","A study on eye-tracking-based Interface for VR/AR education platform",2020,"Multimedia Tools and Applications","79","23-24",,"16719","16730",,1,"10.1007/s11042-019-08327-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077082981&doi=10.1007%2fs11042-019-08327-0&partnerID=40&md5=7f571bc559675d9e67dc1ecf7bb55967","Department of Computer Science & Engineering, Dongguk University, Seoul, South Korea; Humanitas College, Kyung Hee University, Seoul, South Korea","Joo, H.-J., Department of Computer Science & Engineering, Dongguk University, Seoul, South Korea; Jeong, H.-Y., Humanitas College, Kyung Hee University, Seoul, South Korea","In recent years, a platform providing a Visual Programming development environment capable of 3D editing and interaction editing in an In-VR environment to quickly prototype VR/AR contents for education of VR and AR for general users and children. In the past, VR contents were mostly viewed by users. However, thanks to the rapid development of recent computing technologies, VR contents interacting with users have emerged as a device capable of tracking user behavior in a small size It was able to appear. In addition, because VR is extended to AR and MR, it can be used in all three virtual environments and requires efficient user interface (UI). In this paper, we propose UI based on eye tracking. Eye-tracking-based UI not only reduces the amount of time the user directly manipulates the controller, but also dramatically lowers the time spent on simple operations, while reducing the need for a dedicated controller by allowing multiple types of controllers to be used in combination. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Augmented reality; Eye tracking; Virtual reality; VR/AR education; VR/AR user Interface","Augmented reality; Behavioral research; Computer programming; Controllers; User interfaces; Virtual reality; Computing technology; Dedicated controllers; Simple operation; Time spent; User behaviors; Visual programming; Eye tracking",Article,"Final","",Scopus,2-s2.0-85077082981
"Mahanama B., Jayawardana Y., Jayarathna S.","57206891362;57206898577;36052654200;","Gaze-Net: Appearance-based gaze estimation using capsule networks",2020,"ACM International Conference Proceeding Series",,,,"","",,,"10.1145/3396339.3396393","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086144457&doi=10.1145%2f3396339.3396393&partnerID=40&md5=b555d0d82529dbd9f74e6624913ef774","Department of Computer Science, Old Dominion University, Norfolk, VA  23529, United States","Mahanama, B., Department of Computer Science, Old Dominion University, Norfolk, VA  23529, United States; Jayawardana, Y., Department of Computer Science, Old Dominion University, Norfolk, VA  23529, United States; Jayarathna, S., Department of Computer Science, Old Dominion University, Norfolk, VA  23529, United States","Recent studies on appearance based gaze estimation indicate the ability of Neural Networks to decode gaze information from facial images encompassing pose information. In this paper, we propose Gaze-Net: A capsule network capable of decoding, representing, and estimating gaze information from ocular region images. We evaluate our proposed system using two publicly available datasets, MPIIGaze (200,000+ images in the wild) and Columbia Gaze (5000+ images of users with 21 gaze directions observed at 5 camera angles/positions). Our model achieves a Mean Absolute Error (MAE) of 2.84° for Combined angle error estimate within dataset for MPIIGaze dataset. Further, model achieves a MAE of 10.04° for across dataset gaze estimation error for Columbia gaze dataset. Through transfer learning, the error is reduced to 5.9°. The results show this approach is promising with implications towards using commodity webcams to develop low-cost multi-user gaze tracking systems. © 2020 Association for Computing Machinery.","Capsule networks; Deep learning; Gaze estimation; Gaze tracking; Transfer learning","Decoding; Errors; Transfer learning; Appearance based; Camera angles; Facial images; Gaze direction; Gaze estimation; Gaze tracking system; Mean absolute error; Pose information; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086144457
"Zhou X., Lin J., Zhang Z., Shao Z., Chen S., Liu H.","55743240400;57210574380;57191632915;55921729700;57192606393;54958434200;","Improved itracker combined with bidirectional long short-term memory for 3D gaze estimation using appearance cues",2020,"Neurocomputing","390",,,"217","225",,2,"10.1016/j.neucom.2019.04.099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074493405&doi=10.1016%2fj.neucom.2019.04.099&partnerID=40&md5=0eb3cdfe3754fcc13e8cbf14b40f297b","Zhejiang University of Technology, Hangzhou, 210023, China; Quzhou University, Quzhou, 324000, China; Tianjin University of Technology, Tianjin, 300384, China; University of Portsmouth, Portsmouth, United Kingdom","Zhou, X., Zhejiang University of Technology, Hangzhou, 210023, China, Quzhou University, Quzhou, 324000, China; Lin, J., Zhejiang University of Technology, Hangzhou, 210023, China; Zhang, Z., Zhejiang University of Technology, Hangzhou, 210023, China; Shao, Z., Zhejiang University of Technology, Hangzhou, 210023, China; Chen, S., Zhejiang University of Technology, Hangzhou, 210023, China, Tianjin University of Technology, Tianjin, 300384, China; Liu, H., University of Portsmouth, Portsmouth, United Kingdom","Gaze is an important non-verbal cue for speculating human's attention, which has been widely employed in many human–computer interaction-based applications. In this paper, we propose an improved Itracker to predict the subject's gaze for a single image frame, as well as employ a many-to-one bidirectional Long Short-Term Memory (bi-LSTM) to fit the temporal information between frames to estimate gaze for video sequence. For single image frame gaze estimation, we improve the conventional Itracker by removing the face-grid and reducing one network branch via concatenating the two-eye region images. Experimental results show that our improved Itracker obtains 11.6% significant improvement over the state-of-the-art methods on MPIIGaze dataset and has robust estimation accuracy for different image resolutions under the premise of greatly reducing network complexity. For video sequence gaze estimation, by employing the bi-LSTM to fit the temporal information between frames, experimental results on EyeDiap dataset further demonstrate 3% accuracy improvement. © 2019","CNN; Gaze estimation; LSTM; RNN","Brain; Human computer interaction; Image enhancement; Image resolution; Video recording; Accuracy Improvement; Computer interaction; Gaze estimation; LSTM; Network complexity; Robust estimation; State-of-the-art methods; Temporal information; Long short-term memory; article; gaze; human; human experiment; short term memory; videorecording",Article,"Final","",Scopus,2-s2.0-85074493405
"Li J., Zhong Y., Han J., Ouyang G., Li X., Liu H.","56272795400;57205359088;57051695600;7005498402;57192497946;54958434200;","Classifying ASD children with LSTM based on raw videos",2020,"Neurocomputing","390",,,"226","238",,9,"10.1016/j.neucom.2019.05.106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074363799&doi=10.1016%2fj.neucom.2019.05.106&partnerID=40&md5=db9933d6d92b05cf6e69f760315ab3fa","School of Information Engineering, Nanchang University, Nanchang, 330031, China; State Key Laboratory of Cognitive Neuroscience and Learning & IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, 100875, China; State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China","Li, J., School of Information Engineering, Nanchang University, Nanchang, 330031, China; Zhong, Y., School of Information Engineering, Nanchang University, Nanchang, 330031, China; Han, J., State Key Laboratory of Cognitive Neuroscience and Learning & IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, 100875, China; Ouyang, G., State Key Laboratory of Cognitive Neuroscience and Learning & IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, 100875, China; Li, X., State Key Laboratory of Cognitive Neuroscience and Learning & IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, 100875, China; Liu, H., State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China","Autism spectrum disorder (ASD) is a serious neurodevelopmental disorder that impairs a child's ability to communicate and interact with others. Usually, recognizing a child with ASD needs the diagnosis by professional doctors. However, it is not only expensive and time-consuming, but also the results are influenced by subjective factors, such as the experience of a doctor. Recently, some methods which identify ASD based on biomarkers have been developed, but there are rarely works specific to raw video data. This paper is the first attempt to help diagnose the children with ASD in raw video data using a deep learning technique. Firstly, in order to investigate different gaze patterns between ASD children and typically developing (TD) children, we track the eye movement in each video by the tracking-learning-detection method. Secondly, we divide these tracking trajectories into two components: (i) the length; and (ii) the angle. Afterwards, we calculate an accumulative histogram for each component. Finally, we adopt three-layer Long Short-Term Memory (LSTM) network for classification. Experimental results on our extended dataset (Ext-Dataset) containing 272 videos captured from 136 ASD children and 136 TD children show the LSTM network outperforms the traditional machine learning methods, e.g., Support Vector Machine, with the improvement of accuracy by 6.2% (from 86.4% to 92.6%). Especially, for ASD, we obtain the sensitivity (the true positive rate, TPR) of 91.9% and the specificity (the true negative rate, TNR) of 93.4%, which demonstrates the effectiveness of our method. © 2019","Accumulative histogram; Autism spectrum disorder; Deep learning; Long Short-Term Memory (LSTM); Tracking-learning-detection (TLD)","Brain; Deep learning; Diseases; Eye movements; Graphic methods; Support vector machines; Video recording; Accumulative histogram; Autism spectrum disorders; Detection methods; Learning techniques; Machine learning methods; Tracking trajectory; True negative rates; True positive rates; Long short-term memory; Article; autism; child; deep learning; eye tracking; feature extraction; gaze; human; long short term memory; major clinical study; memory; priority journal; sensitivity and specificity; support vector machine",Article,"Final","",Scopus,2-s2.0-85074363799
"Kyritsis M., Gulliver S.R., Feredoes E.","14019633900;6603891003;15753701000;","Visual Search Fixation Strategies in a 3D Image Set: An Eye-Tracking Study",2020,"Interacting with Computers","32","3",,"246","256",,,"10.1093/iwc/iwaa018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097497065&doi=10.1093%2fiwc%2fiwaa018&partnerID=40&md5=0852dbc52a90c4ca93357d60c42e99d4","Henley Business School, Business Informatics Systems and Accounting, Informatics Research Centre, University of Reading, Reading, Berkshire, RG6 6UD, United Kingdom; School of Psychology and Clinical Language Sciences, University of Reading, Whiteknights Road, Reading, Berkshire, RG6 6AL, United Kingdom","Kyritsis, M., Henley Business School, Business Informatics Systems and Accounting, Informatics Research Centre, University of Reading, Reading, Berkshire, RG6 6UD, United Kingdom; Gulliver, S.R., Henley Business School, Business Informatics Systems and Accounting, Informatics Research Centre, University of Reading, Reading, Berkshire, RG6 6UD, United Kingdom; Feredoes, E., School of Psychology and Clinical Language Sciences, University of Reading, Whiteknights Road, Reading, Berkshire, RG6 6AL, United Kingdom","In this study, we explore whether the inclusion of monocular depth within a pseudo-3D picture gallery negatively affects visual search strategy and performance. Experimental design facilitated control of (i) the number of visible depth planes and (ii) the presence of semantic sorting. Our results show that increasing the number of visual depth planes facilitates efficiency in search, which in turn results in a decreased response time to target selection and a reduction in participant average pupil dilation - used for measuring cognitive load. Furthermore, results identified that search strategy is based on sorting, which implies that an appropriate management of semantic associations can increase search efficiency by decreasing the number of potential targets. © 2020 The Author(s) 2020. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.","empirical studies in HCI; eye tracking; laboratory experiments; usability testing","Efficiency; Semantics; Cognitive loads; Eye-tracking studies; Potential targets; Search efficiency; Search strategies; Semantic associations; Target selection; Visual search strategies; Eye tracking",Article,"Final","",Scopus,2-s2.0-85097497065
"Wen Q., Bradley D., Beeler T., Park S., Hilliges O., Yong J., Xu F.","57199711508;35955665700;36496499200;57195422868;14041644100;13907549600;57189663151;","Accurate Real-time 3D Gaze Tracking Using a Lightweight Eyeball Calibration",2020,"Computer Graphics Forum","39","2",,"475","485",,2,"10.1111/cgf.13945","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087768762&doi=10.1111%2fcgf.13945&partnerID=40&md5=bb807cf1b1c4de90c495b079a7b83beb","BNRist and School of Software, Tsinghua University, China; DisneyResearch|Studios, Canada; ETH Zurich, Switzerland","Wen, Q., BNRist and School of Software, Tsinghua University, China; Bradley, D., DisneyResearch|Studios, Canada; Beeler, T., DisneyResearch|Studios, Canada; Park, S., ETH Zurich, Switzerland; Hilliges, O., ETH Zurich, Switzerland; Yong, J., BNRist and School of Software, Tsinghua University, China; Xu, F., BNRist and School of Software, Tsinghua University, China","3D gaze tracking from a single RGB camera is very challenging due to the lack of information in determining the accurate gaze target from a monocular RGB sequence. The eyes tend to occupy only a small portion of the video, and even small errors in estimated eye orientations can lead to very large errors in the triangulated gaze target. We overcome these difficulties with a novel lightweight eyeball calibration scheme that determines the user-specific visual axis, eyeball size and position in the head. Unlike the previous calibration techniques, we do not need the ground truth positions of the gaze points. In the online stage, gaze is tracked by a new gaze fitting algorithm, and refined by a 3D gaze regression method to correct for bias errors. Our regression is pre-trained on several individuals and works well for novel users. After the lightweight one-time user calibration, our method operates in real time. Experiments show that our technique achieves state-of-the-art accuracy in gaze angle estimation, and we demonstrate applications of 3D gaze target tracking and gaze retargeting to an animated 3D character. © 2020 The Author(s) Computer Graphics Forum © 2020 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.","CCS Concepts; • Computing methodologies → Motion capture; • Human-centered computing → Interaction techniques","Calibration; Errors; Regression analysis; Target tracking; 3D characters; Calibration schemes; Calibration techniques; Fitting algorithms; Regression method; Size and position; State of the art; User calibration; Eye tracking",Article,"Final","",Scopus,2-s2.0-85087768762
"Manal S., Ov S., Kg S.","57217634014;57217633329;57217630791;","Catch Gesture: An Ultimate Action Recognition Technology Using IoT Device",2020,"Proceedings of the International Conference on Intelligent Computing and Control Systems, ICICCS 2020",,,"9121022","363","367",,,"10.1109/ICICCS48265.2020.9121022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087464015&doi=10.1109%2fICICCS48265.2020.9121022&partnerID=40&md5=005af7fc1fad06f06eff5f4ce302dd6b","Cochin College of Engineering and Technology, Deptartment of Computer Science and Engineering, Valanchey, Kerala, India","Manal, S., Cochin College of Engineering and Technology, Deptartment of Computer Science and Engineering, Valanchey, Kerala, India; Ov, S., Cochin College of Engineering and Technology, Deptartment of Computer Science and Engineering, Valanchey, Kerala, India; Kg, S., Cochin College of Engineering and Technology, Deptartment of Computer Science and Engineering, Valanchey, Kerala, India","Nowadays, the internet of things (IoT) which connects the objects with the network is one of the most important technology. Everyone needs to access the IoT devices from anywhere. A challenge in our world is how to provide an efficient and automatic approach to control the IoT devices. In this paper, a system is proposed which provide a smart IoT controlling system, for getting efficient communication between the IoT devices and the users. An IoT device controller named as catch gesture contains three modules 1) object recognition module, 2) eye tracking module, and 3) gesture recognition module. The IoT device is identified by using some deep learning-based methods for getting accurate output and performance. Subsequently, the IoT device control command is generated and transmitted to the IoT devices to control the IoT devices by using hand gesture. The performance evaluation gives the efficiency and usefulness of the proposed system in detecting the IoT devices. © 2020 IEEE.","Convolutional Neural Network; Pupil detection; python; User-machine interaction","Control systems; Deep learning; Eye tracking; Intelligent computing; Object recognition; Object tracking; Action recognition; Automatic approaches; Controlling system; Device control; Device controller; Efficient communications; Internet of thing (IOT); Learning-based methods; Internet of things",Conference Paper,"Final","",Scopus,2-s2.0-85087464015
"Kang J., Han X., Song J., Niu Z., Li X.","57192107293;57215872111;57208586652;57203040948;57192497946;","The identification of children with autism spectrum disorder by SVM approach on EEG and eye-tracking data",2020,"Computers in Biology and Medicine","120",,"103722","","",,19,"10.1016/j.compbiomed.2020.103722","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082169614&doi=10.1016%2fj.compbiomed.2020.103722&partnerID=40&md5=0ecb85ac45832e1569d4a73a85f7017a","College of Electronic & Information Engineering, Hebei University, Baoding, China; School of Information Science & Engineering, Yanshan University, Qinhuangdao, China; State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing, China","Kang, J., College of Electronic & Information Engineering, Hebei University, Baoding, China; Han, X., School of Information Science & Engineering, Yanshan University, Qinhuangdao, China; Song, J., College of Electronic & Information Engineering, Hebei University, Baoding, China; Niu, Z., State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing, China; Li, X., State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing, China","Objective: To identify autistic children, we used features extracted from two modalities (EEG and eye-tracking) as input to a machine learning approach (SVM). Methods: A total of 97 children aged from 3 to 6 were enrolled in the present study. After resting-state EEG data recording, the children performed eye-tracking tests individually on own-race and other-race stranger faces stimuli. Power spectrum analysis was used for EEG analysis and areas of interest (AOI) were selected for face gaze analysis of eye-tracking data. The minimum redundancy maximum relevance (MRMR) feature selection method combined with SVM classifiers were used for classification of autistic versus typically developing children. Results: Results showed that classification accuracy from combining two types of data reached a maximum of 85.44%, with AUC = 0.93, when 32 features were selected. Limitations: The sample consisted of children aged from 3 to 6, and no younger patients were included. Conclusions: Our machine learning approach, combining EEG and eye-tracking data, may be a useful tool for the identification of children with ASD, and may help for diagnostic processes. © 2020 Elsevier Ltd","Autism; EEG; Eye-tracking; Typically developing children","Diagnosis; Diseases; Electroencephalography; Learning systems; Spectrum analysis; Support vector machines; Autism; Children with autisms; Classification accuracy; Diagnostic process; Feature selection methods; Machine learning approaches; Minimum redundancy-maximum relevances; Typically developing children; Eye tracking; area under the curve; Article; autism; child; comparative study; controlled study; DSM-5; electroencephalogram; eye movement; eye tracking; facial recognition; feature selection; female; human; major clinical study; male; power spectrum; preschool child; priority journal; support vector machine",Article,"Final","",Scopus,2-s2.0-85082169614
"Kim J.-H., Jeong J.-W.","57205720618;57205722835;","A preliminary study on performance evaluation of multi-view multi-modal gaze estimation under challenging conditions",2020,"Conference on Human Factors in Computing Systems - Proceedings",,,"3382856","","",,1,"10.1145/3334480.3382856","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090036435&doi=10.1145%2f3334480.3382856&partnerID=40&md5=dd0b8416b7fc58fe17f11ab050c9f875","Kumoh National Institute of Technology, Gumi, South Korea","Kim, J.-H., Kumoh National Institute of Technology, Gumi, South Korea; Jeong, J.-W., Kumoh National Institute of Technology, Gumi, South Korea","In this paper, we address gaze estimation under practical and challenging conditions. Multi-view and multi-modal learning have been considered useful for various complex tasks; however, an in-depth analysis or a large-scale dataset on multi-view, multi-modal gaze estimation under a long-distance setup with a low illumination is still very limited. To address these limitations, first, we construct a dataset of images captured under challenging conditions. And we propose a simple deep learning architecture that can handle multi-view multi-modal data for gaze estimation. Finally, we conduct a performance evaluation of the proposed network with the constructed dataset to understand the effects of multiple views of a user and multi-modality (RGB, depth, and infrared). We report various findings from our preliminary experimental results and expect this would be helpful for gaze estimation studies to deal with challenging conditions. © 2020 Owner/Author.","Deep neural networks; Gaze estimation; Multi-modal interaction; Multi-view learning","Deep learning; Human engineering; Large dataset; Gaze estimation; In-depth analysis; Large-scale dataset; Learning architectures; Low illuminations; Multi-modal data; Multi-modal learning; Multiple views; Modal analysis",Conference Paper,"Final","",Scopus,2-s2.0-85090036435
"Sidenmark L., Clarke C., Zhang X., Phu J., Gellersen H.","57210111157;56559308000;57218825004;57219116481;6701531333;","Outline Pursuits: Gaze-assisted Selection of Occluded Objects in Virtual Reality",2020,"Conference on Human Factors in Computing Systems - Proceedings",,,"3376438","","",,10,"10.1145/3313831.3376438","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091288497&doi=10.1145%2f3313831.3376438&partnerID=40&md5=da31c18d4295d6aad41e1b6acc5e1c71","Lancaster University, Lancaster, United Kingdom; Ludwig Maximilian University of Munich, Munich, Germany; Aarhus University, Aarhus, Denmark","Sidenmark, L., Lancaster University, Lancaster, United Kingdom; Clarke, C., Lancaster University, Lancaster, United Kingdom; Zhang, X., Lancaster University, Lancaster, United Kingdom; Phu, J., Ludwig Maximilian University of Munich, Munich, Germany; Gellersen, H., Aarhus University, Aarhus, Denmark","In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded. © 2020 ACM.","eye tracking; occlusion; smooth pursuits; virtual reality","Human engineering; Virtual reality; 3-D environments; Candidate target; Hands-free; Line of Sight; Occluded objects; Raycasting; Smooth pursuit eye movement; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85091288497
"Bai H., Sasikumar P., Yang J., Billinghurst M.","55555685900;57212406932;57142855000;7006142663;","A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing",2020,"Conference on Human Factors in Computing Systems - Proceedings",,,"3376550","","",,24,"10.1145/3313831.3376550","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087127575&doi=10.1145%2f3313831.3376550&partnerID=40&md5=01365c87c7a25669f2c24b28f35e989c","University of Auckland, Auckland, New Zealand; Eth Zürich, Zürich, Switzerland","Bai, H., University of Auckland, Auckland, New Zealand; Sasikumar, P., University of Auckland, Auckland, New Zealand; Yang, J., Eth Zürich, Zürich, Switzerland; Billinghurst, M., University of Auckland, Auckland, New Zealand","Supporting natural communication cues is critical for people to work together remotely and face-to-face. In this paper we present a Mixed Reality (MR) remote collaboration system that enables a local worker to share a live 3D panorama of his/her surroundings with a remote expert. The remote expert can also share task instructions back to the local worker using visual cues in addition to verbal communication. We conducted a user study to investigate how sharing augmented gaze and gesture cues from the remote expert to the local worker could affect the overall collaboration performance and user experience. We found that by combing gaze and gesture cues, our remote collaboration system could provide a significantly stronger sense of co-presence for both the local and remote users than using the gaze cue alone. The combined cues were also rated significantly higher than the gaze in terms of ease of conveying spatial actions. © 2020 ACM.","3d panorama; augmented reality; eye gaze; hand gesture; mixed reality; remote collaboration; scene reconstruction; virtual reality","Human engineering; User experience; Face to face; Hand gesture; Natural communication; Remote collaboration; Remote experts; Remote users; Spatial action; Verbal communications; Mixed reality",Conference Paper,"Final","",Scopus,2-s2.0-85087127575
"Eraslan S., Yesilada Y., Yaneva V., Harper S.","55785840000;8454176800;57003253500;7102941677;","Autism detection based on eye movement sequences on the web: a scanpath trend analysis approach",2020,"Proceedings of the 17th International Web for All Conference, W4A 2020",,,,"","",,1,"10.1145/3371300.3383340","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086813828&doi=10.1145%2f3371300.3383340&partnerID=40&md5=18e7c75f9b167612f95b900f62166f90","Department of Computer Science, University of Manchester, Manchester, United Kingdom; Middle East Technical, University Northern Cyprus Campus, Kalkanli, Guzelyurt, Mersin 10, Turkey; Research Group in Computational Linguistics, University of Wolverhampton, Wolverhampton, United Kingdom","Eraslan, S., Department of Computer Science, University of Manchester, Manchester, United Kingdom; Yesilada, Y., Middle East Technical, University Northern Cyprus Campus, Kalkanli, Guzelyurt, Mersin 10, Turkey; Yaneva, V., Research Group in Computational Linguistics, University of Wolverhampton, Wolverhampton, United Kingdom; Harper, S., Department of Computer Science, University of Manchester, Manchester, United Kingdom","Autism diagnostic procedure is a subjective, challenging and expensive procedure and relies on behavioral, historical and parental report information. In our previous, we proposed a machine learning classifier to be used as a potential screening tool or used in conjunction with other diagnostic methods, thus aiding established diagnostic methods. The classifier uses eye movements of people on web pages but it only considers non-sequential data. It achieves the best accuracy by combining data from several web pages and it has varying levels of accuracy on different web pages. In this present paper, we investigate whether it is possible to detect autism based on eye-movement sequences and achieve stable accuracy across different web pages to be not dependent on specific web pages. We used Scanpath Trend Analysis (STA) which is designed for identifying a trending path of a group of users on a web page based on their eye movements. We first identify trending paths of people with autism and neurotypical people. To detect whether or not a person has autism, we calculate the similarity of his/her path to the trending paths of people with autism and neurotypical people. If the path is more similar to the trending path of neurotypical people, we classify the person as a neurotypical person. Otherwise, we classify her/him as a person with autism. We systematically evaluate our approach with an eye-tracking dataset of 15 verbal and highly-independent people with autism and 15 neurotypical people on six web pages. Our evaluation shows that the STA approach performs better on individual web pages and provides more stable accuracy across different pages. © 2020 ACM.","autism; classification; eye tracking; scanpath; STA; web pages","Clustering algorithms; Diseases; Eye tracking; Learning systems; Websites; Diagnostic methods; Diagnostic procedure; Scan path; Screening tool; Sequential data; Trend analysis; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85086813828
"Zheng L.J., Mountstephens J., Teo J.","57216398510;36915612500;57201882145;","Comparing eye-tracking versus EEG features for four-class emotion classification in VR predictive analytics",2020,"International Journal of Advanced Science and Technology","29","6 Special Issue",,"1492","1497",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083400937&partnerID=40&md5=b9559a214913b17133178763ecdfccb4","Faculty of Computing and Informatics, Universiti Malaysia Sabah, Jalan UMS, Kota Kinabalu, Sabah  88400, Malaysia","Zheng, L.J., Faculty of Computing and Informatics, Universiti Malaysia Sabah, Jalan UMS, Kota Kinabalu, Sabah  88400, Malaysia; Mountstephens, J., Faculty of Computing and Informatics, Universiti Malaysia Sabah, Jalan UMS, Kota Kinabalu, Sabah  88400, Malaysia; Teo, J., Faculty of Computing and Informatics, Universiti Malaysia Sabah, Jalan UMS, Kota Kinabalu, Sabah  88400, Malaysia","This paper presents a novel emotion recognition approach using electroencephalography (EEG) brainwave signals augmented with eye-tracking data in virtual reality (VR) to classify 4-quadrant circumplex model of emotions. 3600 videos are used as the stimuli to evoke user’s emotions (happy, angry, bored, calm) with a VR headset and a pair of earphones. EEG signals are recorded via a wearable EEG brain-computer interfacing (BCI) device and pupil diameter is collected also from a wearable portable eye-tracker. We extract 5 frequency bands which are Delta, Theta, Alpha, Beta, and Gamma from EEG data as well as obtaining pupil diameter from the eye-tracker as the chosen as the eye-related feature for this investigation. Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel is used as the classifier. The best accuracies based on EEG brainwave signals and pupil diameter are 98.44% and 58.30% respectively. © 2020 SERSC.","EEG; Emotion recognition; Eye-tracking; Machine learning; Pupil diameter; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85083400937
"Du Y., Yan Y., Chen S., Hua Y.","57205367242;56964626600;35213074100;57196115171;","Object-adaptive LSTM network for real-time visual tracking with adversarial data augmentation",2020,"Neurocomputing","384",,,"67","83",,6,"10.1016/j.neucom.2019.12.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076854903&doi=10.1016%2fj.neucom.2019.12.022&partnerID=40&md5=49b3d1cef6289842b721f591333711d0","School of Informatics, Xiamen University, Fujian, 361005, China; Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, 100084, China; School of Computer and Information Engineering, Xiamen University of Technology, Fujian, 361024, China; School of Electronics, Electrical Engineering and Computer Science, Queen's University Belfast, United Kingdom","Du, Y., School of Informatics, Xiamen University, Fujian, 361005, China, Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, 100084, China; Yan, Y., School of Informatics, Xiamen University, Fujian, 361005, China; Chen, S., School of Computer and Information Engineering, Xiamen University of Technology, Fujian, 361024, China; Hua, Y., School of Electronics, Electrical Engineering and Computer Science, Queen's University Belfast, United Kingdom","In recent years, deep learning based visual tracking methods have obtained great success owing to the powerful feature representation ability of Convolutional Neural Networks (CNNs). Among these methods, classification-based tracking methods exhibit excellent performance while their speeds are heavily limited by the expensive computation for massive proposal feature extraction. In contrast, matching-based tracking methods (such as Siamese networks) possess remarkable speed superiority. However, the absence of online updating renders these methods unadaptable to significant object appearance variations. In this paper, we propose a novel real-time visual tracking method, which adopts an object-adaptive LSTM network to effectively capture the video sequential dependencies and adaptively learn the object appearance variations. For high computational efficiency, we also present a fast proposal selection strategy, which utilizes the matching-based tracking method to pre-estimate dense proposals and selects high-quality ones to feed to the LSTM network for classification. This strategy efficiently filters out some irrelevant proposals and avoids the redundant computation for feature extraction, which enables our method to operate faster than conventional classification-based tracking methods. In addition, to handle the problems of sample inadequacy and class imbalance during online tracking, we adopt a data augmentation technique based on the Generative Adversarial Network (GAN) to facilitate the training of the LSTM network. Extensive experiments on four visual tracking benchmarks demonstrate the state-of-the-art performance of our method in terms of both tracking accuracy and speed, which exhibits great potentials of recurrent structures for visual tracking. © 2019 Elsevier B.V.","Data augmentation; Generative adversarial network; LSTM network; Visual tracking","Benchmarking; Computational efficiency; Deep learning; Extraction; Feature extraction; Adversarial networks; Convolutional neural network; Data augmentation; Feature representation; Redundant computation; Sequential dependencies; State-of-the-art performance; Visual Tracking; Long short-term memory; algorithm; Article; benchmarking; classification; classifier; conceptual framework; deep learning; eye tracking; information processing; long term memory; online analysis; priority journal; short term memory; training",Article,"Final","",Scopus,2-s2.0-85076854903
"Xu L., Chi J.","57218290275;8702376200;","3D eye model-based gaze tracking system with a consumer depth camera",2020,"Proceedings - 2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2020",,,"9131201","293","297",,,"10.1109/AEMCSE50948.2020.00070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088635925&doi=10.1109%2fAEMCSE50948.2020.00070&partnerID=40&md5=89ea83507a80dd0583b4bc727aec88fd","University of Science and Technology, China","Xu, L., University of Science and Technology, China; Chi, J., University of Science and Technology, China","Most existing gaze tracking systems are high-cost, intrusive and difficult to calibrate, and some rely on the infrared illuminant. However, such systems may not work outdoor and meet real-time requirements. This paper proposes a non-intrusive system based on the 3D eyeball model, which does not need the exact infrared illuminant and complicated calibration process and allows the natural movement of the head. In the proposed system, Kinect is used to track the iris center and face model of the person, and the 3D information is easy to obtain. At the same time, point cloud registration algorithm is applied based on feature points in the face model sequence to obtain accurate head pose estimation results. In this paper, a personal calibration process is also proposed to obtain the gaze model parameters for different users, such as the eyeball center and angle kappa. The proposed method has good adaptability to the change of illuminant and head movement. In the actual operating environment, the system speed reaches 30 fps, which can meet the requirements of real-time control. © 2020 IEEE.","Eye model-based; Gaze estimation; Personal calibration; Rgb-d camera","3D modeling; Calibration; Real time control; Software engineering; Calibration process; Gaze tracking system; Head Pose Estimation; Model parameters; Natural movements; Operating environment; Point cloud registration; Real time requirement; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85088635925
"Boutros F., Damer N., Raja K., Ramachandra R., Kirchbuchner F., Kuijper A.","57205379838;50861109400;57188866050;57190835798;57031859600;56131137100;","Periocular Biometrics in Head-Mounted Displays: A Sample Selection Approach for Better Recognition",2020,"2020 8th International Workshop on Biometrics and Forensics, IWBF 2020 - Proceedings",,,"9107939","","",,3,"10.1109/IWBF49977.2020.9107939","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087072799&doi=10.1109%2fIWBF49977.2020.9107939&partnerID=40&md5=44fcffcba30c7b9345307dad1dbe8552","Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Norwegian Colour and Visual Computing Laboratory, NTNU, Gjovik, Norway; Norwegian Biometrics Laboratory, NTNU, Gjovik, Norway","Boutros, F., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Damer, N., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Raja, K., Norwegian Colour and Visual Computing Laboratory, NTNU, Gjovik, Norway, Norwegian Biometrics Laboratory, NTNU, Gjovik, Norway; Ramachandra, R., Norwegian Biometrics Laboratory, NTNU, Gjovik, Norway; Kirchbuchner, F., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Kuijper, A., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany","Virtual and augmented reality technologies are increasingly used in a wide range of applications. Such technologies employ a Head Mounted Display (HMD) that typically includes an eye-facing camera and is used for eye tracking. As some of these applications require accessing or transmitting highly sensitive private information, a trusted verification of the operator's identity is needed. We investigate the use of HMD-setup to perform verification of operator using periocular region captured from inbuilt camera. However, the uncontrolled nature of the periocular capture within the HMD results in images with a high variation in relative eye location and eye-opening due to varied interactions. Therefore, we propose a new normalization scheme to align the ocular images and then, a new reference sample selection protocol to achieve higher verification accuracy. The applicability of our proposed scheme is exemplified using two handcrafted feature extraction methods and two deep-learning strategies. We conclude by stating the feasibility of such a verification approach despite the uncontrolled nature of the captured ocular images, especially when proper alignment and sample selection strategy is employed. © 2020 IEEE.","Biometrics; head-mounted display; periocular","Augmented reality; Biometrics; Cameras; Deep learning; Eye tracking; Learning systems; Street traffic control; Eye location; Feature extraction methods; Head mounted displays; Learning strategy; Ocular images; Private information; Sample selection; Virtual and augmented reality; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85087072799
"Guo Q., Tang H., Schmitz A., Zhang W., Lou Y., Fix A., Lovegrove S., Strasdat H.M.","57217176948;57217175506;57217177853;57217178166;56199038100;57196001099;36617719200;14016566200;","Raycast calibration for augmented reality HMDS with off-axis reflective combiners",2020,"IEEE International Conference on Computational Photography, ICCP 2020",,,"9105134","","",,1,"10.1109/ICCP48838.2020.9105134","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086627602&doi=10.1109%2fICCP48838.2020.9105134&partnerID=40&md5=7ea3b16b7682cd0aefe22635b43fd0cf","Harvard University, United States; Facebook Inc., United States","Guo, Q., Harvard University, United States; Tang, H., Facebook Inc., United States; Schmitz, A., Facebook Inc., United States; Zhang, W., Facebook Inc., United States; Lou, Y., Facebook Inc., United States; Fix, A., Facebook Inc., United States; Lovegrove, S., Facebook Inc., United States; Strasdat, H.M., Facebook Inc., United States","Augmented reality overlays virtual objects on the real world. To do so, the head mounted display (HMD) needs to be calibrated to establish a mapping between 3D points in the real world with 2D pixels on display panels. This distortion is a high-dimensional function that also depends on pupil position and varifocal settings. We present Raycast calibration, an efficient approach to geometrically calibrate AR displays with off-axis reflective combiners. Our approach requires a small amount of data to estimate a compact, physics-based, and ray-traceable model of the HMD optics. We apply this technique to automatically calibrate an AR prototype with display, SLAM and eye-tracker, without user in the loop. © 2020 IEEE.","Augmented reality; Off-axis reflectors; Raycast calibration","Augmented reality; Calibration; Color photography; Eye tracking; AR display; Display panels; Eye trackers; Head mounted displays; High-dimensional; Physics-based; Real-world; Virtual objects; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85086627602
"Li X., Younes R., Bairaktarova D., Guo Q.","57276954200;57147198000;47961736000;57216291792;","Predicting spatial visualization problems’ difficulty level from eye-tracking data",2020,"Sensors (Switzerland)","20","7","1949","","",,1,"10.3390/s20071949","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083041510&doi=10.3390%2fs20071949&partnerID=40&md5=2c5dfc833334c29661fc0d895a81e1e3","School of Electrical and Electronic Engineering, Huazhong University of Science and Technology, Wuhan, 430074, China; Department of Electrical and Computer Engineering, Duke University, Durham, NC  27708, United States; Department of Engineering Education, Virginia Tech, Blacksburg, VA  24061, United States; International School, Beijing University of Posts and Telecommunications, Beijing, 100876, China","Li, X., School of Electrical and Electronic Engineering, Huazhong University of Science and Technology, Wuhan, 430074, China; Younes, R., Department of Electrical and Computer Engineering, Duke University, Durham, NC  27708, United States; Bairaktarova, D., Department of Engineering Education, Virginia Tech, Blacksburg, VA  24061, United States; Guo, Q., International School, Beijing University of Posts and Telecommunications, Beijing, 100876, China","The difficulty level of learning tasks is a concern that often needs to be considered in the teaching process. Teachers usually dynamically adjust the difficulty of exercises according to the prior knowledge and abilities of students to achieve better teaching results. In e-learning, because there is no teacher involvement, it often happens that the difficulty of the tasks is beyond the ability of the students. In attempts to solve this problem, several researchers investigated the problem-solving process by using eye-tracking data. However, although most e-learning exercises use the form of filling in blanks and choosing questions, in previous works, research focused on building cognitive models from eye-tracking data collected from flexible problem forms, which may lead to impractical results. In this paper, we build models to predict the difficulty level of spatial visualization problems from eye-tracking data collected from multiple-choice questions. We use eye-tracking and machine learning to investigate (1) the difference of eye movement among questions from different difficulty levels and (2) the possibility of predicting the difficulty level of problems from eye-tracking data. Our models resulted in an average accuracy of 87.60% on eye-tracking data of questions that the classifier has seen before and an average of 72.87% on questions that the classifier has not yet seen. The results confirmed that eye movement, especially fixation duration, contains essential information on the difficulty of the questions and it is sufficient to build machine-learning-based models to predict difficulty level. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Engineering education; Eye-tracking; Machine learning; Proactive systems; Spatial visualization","Data visualization; E-learning; Eye movements; Forecasting; Machine learning; Problem solving; Visualization; Cognitive model; Fixation duration; Learning tasks; Multiple choice questions; Prior knowledge; Problem solving process; Spatial visualization; Teaching process; Eye tracking; eye movement; human; learning; problem solving; spatial orientation; teaching; Eye Movements; Eye-Tracking Technology; Humans; Learning; Problem Solving; Spatial Navigation; Teaching",Article,"Final","",Scopus,2-s2.0-85083041510
"Bace M., Staal S., Bulling A.","44461063200;57194012922;6505807414;","How Far Are We from Quantifying Visual Attention in Mobile HCI?",2020,"IEEE Pervasive Computing","19","2","9052000","46","55",,1,"10.1109/MPRV.2020.2967736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083007338&doi=10.1109%2fMPRV.2020.2967736&partnerID=40&md5=467028a65d442f4f0e07e587111d7a41","Department of Computer Science, ETH Zurich, Switzerland; Institute for Visualisation and Interactive SystemsUniversity of Stuttgart, Germany","Bace, M., Department of Computer Science, ETH Zurich, Switzerland; Staal, S., Department of Computer Science, ETH Zurich, Switzerland; Bulling, A., Institute for Visualisation and Interactive SystemsUniversity of Stuttgart, Germany","With an ever-increasing number of mobile devices competing for attention, quantifying when, how often, or for how long users look at their devices has emerged as a key challenge in mobile human-computer interaction. Encouraged by recent advances in automatic eye contact detection using machine learning and device-integrated cameras, we provide a fundamental investigation into the feasibility of quantifying overt visual attention during everyday mobile interactions. In this article, we discuss the main challenges and sources of error associated with sensing visual attention on mobile devices in the wild, including the impact of face and eye visibility, the importance of robust head poses estimation, and the need for accurate gaze estimation. Our analysis informs future research on this emerging topic and underlines the potential of eye contact detection for exciting new applications toward next-generation pervasive attentive user interfaces. © 2002-2012 IEEE.",,"Human computer interaction; User interfaces; Attentive user interfaces; Eye visibilities; Integrated cameras; Mobile human computer interaction; Mobile interaction; New applications; Overt visual attentions; Visual Attention; Behavioral research",Article,"Final","",Scopus,2-s2.0-85083007338
"Lee K.-F., Chen Y.-L., Yu C.-W., Chin K.-Y., Wu C.-H.","56413130800;35322122400;54379325600;25824572800;57203205774;","Gaze tracking and point estimation using low-cost head-mounted devices",2020,"Sensors (Switzerland)","20","7","1917","","",,5,"10.3390/s20071917","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082732579&doi=10.3390%2fs20071917&partnerID=40&md5=952798313432eb8c4b42adf49c4f90fa","Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, 10608, Taiwan; Department of Digital Humanities and Information Applications, Aletheia University, New Taipei City, Taiwan","Lee, K.-F., Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, 10608, Taiwan; Chen, Y.-L., Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, 10608, Taiwan; Yu, C.-W., Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, 10608, Taiwan; Chin, K.-Y., Department of Digital Humanities and Information Applications, Aletheia University, New Taipei City, Taiwan; Wu, C.-H., Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, 10608, Taiwan","In this study, a head-mounted device was developed to track the gaze of the eyes and estimate the gaze point on the user's visual plane. To provide a cost-effective vision tracking solution, this head-mounted device is combined with a sized endoscope camera, infrared light, and mobile phone; the devices are also implemented via 3D printing to reduce costs. Based on the proposed image pre-processing techniques, the system can efficiently extract and estimate the pupil ellipse from the camera module. A 3D eye model was also developed to effectively locate eye gaze points from extracted eye images. In the experimental results, average accuracy, precision, and recall rates of the proposed system can achieve an average of over 97%, which can demonstrate the efficiency of the proposed system. This study can be widely used in the Internet of Things, virtual reality, assistive devices, and human-computer interaction applications. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Eye tracking; Gaze estimation; Head-mounted; Mobile devices; Wearable devices","3D modeling; 3D printers; Cameras; Cost effectiveness; Costs; Human computer interaction; Image processing; Assistive devices; Camera modules; Cost effective; Gaze tracking; Image preprocessing; Infrared light; Point estimation; Vision tracking; Eye tracking; algorithm; eye fixation; eye movement; human; image processing; infrared radiation; mobile phone; photography; physiology; procedures; three dimensional printing; Algorithms; Cell Phone; Eye Movements; Eye-Tracking Technology; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Infrared Rays; Photography; Printing, Three-Dimensional",Article,"Final","",Scopus,2-s2.0-85082732579
"Shi Y., Du J., Ragan E.","57189999728;57219889677;26667185300;","Review visual attention and spatial memory in building inspection: Toward a cognition-driven information system",2020,"Advanced Engineering Informatics","44",,"101061","","",,3,"10.1016/j.aei.2020.101061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079853656&doi=10.1016%2fj.aei.2020.101061&partnerID=40&md5=e417f2425719114073f62ce4a35a3233","Engineering School of Sustainable Infrastructure and Environment (ESSIE), Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States; Department of Computer & Information Science & Engineering, Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States","Shi, Y., Engineering School of Sustainable Infrastructure and Environment (ESSIE), Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States; Du, J., Engineering School of Sustainable Infrastructure and Environment (ESSIE), Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States; Ragan, E., Department of Computer & Information Science & Engineering, Herbert Wertheim College of Engineering, University of Florida, Gainesville, FL  32611, United States","With the increasing complexity of modern buildings, it is becoming more challenging for the professionals in the Architecture, Engineering, and Construction (AEC) industry to effectively digest complex engineering and design information and develop an accurate spatial memory that is critical to their daily tasks. As emerging visualization technologies, such as Virtual Reality, are considered as a promising solution, there is a pressing need to understand the mechanism by which different information visualization methods affect AEC task performance. Cognition literature has discovered a strong relationship between attention and memory development, but little has been done to understand how the visual attention patterns during the design documents review affect the effectiveness of spatial memory in AEC tasks. To fill the knowledge gap, this paper presents a human-subject experiment (n = 63) to test how spatial knowledge is acquired in a building inspection task and how the different visual attention patterns affect the development of spatial memory. Participants were asked to review the design information of a real building on campus. To trigger different attention patterns, they were randomly assigned to one of the three groups based on the forms of information given in the review session, including 2D, 3D, and VR groups. After a brief review session, participants were asked to go to the real building to identify discrepancies (based on memory) that were intentionally inserted by the authors. The inspection performance was used to evaluate the spatial memory development. The results indicate that in general there is a positive relationship between test subjects’ visual attention (fixation time) and spatial memory, but the increasing rate varies across the three groups, suggesting that visual context plays a critical role in the development efficiency of spatial memory. The findings also indicate that the visual attention – spatial memory relationship may be mediated by the use of different spatial knowledge acquisition strategies. This study is expected to contribute to the construction information technology literature by setting the cornerstone of a cognition-driven information system that tailors into the spatial cognitive process of AEC professionals. © 2020 Elsevier Ltd","Attention; Eye-tracking; Spatial knowledge; Spatial memory; Virtual reality","Architectural design; Eye tracking; Information systems; Information use; Inspection; Virtual reality; Visualization; Architecture , engineering , and constructions; Attention; Human subject experiments; Information visualization; Spatial knowledge; Spatial knowledge acquisitions; Spatial memory; Visualization technologies; Behavioral research",Article,"Final","",Scopus,2-s2.0-85079853656
"Rudenko A., Kucner T.P., Swaminathan C.S., Chadalavada R.T., Arras K.O., Lilienthal A.J.","57192267970;56031229600;57207827238;57191629740;6701603183;15623407200;","THÖR: Human-Robot Navigation Data Collection and Accurate Motion Trajectories Dataset",2020,"IEEE Robotics and Automation Letters","5","2","8954833","676","682",,2,"10.1109/LRA.2020.2965416","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078707058&doi=10.1109%2fLRA.2020.2965416&partnerID=40&md5=74ba473c6752cd7d2c09010c8f3473e0","Robotics Research, Bosch Corporate Research, Stuttgart, 71272, Germany; Mobile Robotics and Olfaction Lab, Örebro University, Örebro, 70281, Sweden","Rudenko, A., Robotics Research, Bosch Corporate Research, Stuttgart, 71272, Germany, Mobile Robotics and Olfaction Lab, Örebro University, Örebro, 70281, Sweden; Kucner, T.P., Robotics Research, Bosch Corporate Research, Stuttgart, 71272, Germany; Swaminathan, C.S., Robotics Research, Bosch Corporate Research, Stuttgart, 71272, Germany; Chadalavada, R.T., Robotics Research, Bosch Corporate Research, Stuttgart, 71272, Germany; Arras, K.O., Robotics Research, Bosch Corporate Research, Stuttgart, 71272, Germany; Lilienthal, A.J., Robotics Research, Bosch Corporate Research, Stuttgart, 71272, Germany","Understanding human behavior is key for robots and intelligent systems that share a space with people. Accordingly, research that enables such systems to perceive, track, learn and predict human behavior as well as to plan and interact with humans has received increasing attention over the last years. The availability of large human motion datasets that contain relevant levels of difficulty is fundamental to this research. Existing datasets are often limited in terms of information content, annotation quality or variability of human behavior. In this article, we present THÖR, a new dataset with human motion trajectory and eye gaze data collected in an indoor environment with accurate ground truth for position, head orientation, gaze direction, social grouping, obstacles map and goal coordinates. THÖR also contains sensor data collected by a 3D lidar and involves a mobile robot navigating the space. We propose a set of metrics to quantitatively analyze motion trajectory datasets such as the average tracking duration, ground truth noise, curvature and speed variation of the trajectories. In comparison to prior art, our dataset has a larger variety in human motion behavior, is less noisy, and contains annotations at higher frequencies. © 2016 IEEE.","Human detection and tracking; motion and path planning; social human-robot interaction","Arts computing; Behavioral research; Human robot interaction; Intelligent robots; Intelligent systems; Large dataset; Motion planning; Trajectories; Higher frequencies; Human detection and tracking; Indoor environment; Information contents; Motion and path planning; Motion trajectories; Social human-robot interactions; Speed variations; Robot programming",Article,"Final","",Scopus,2-s2.0-85078707058
"Aydin A.S., Feiz S., Ashok V., Ramakrishnan I.V.","55749193700;57209393850;55633359800;7003899264;","SaIL",2020,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,"111","115",,2,"10.1145/3377325.3377540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082472741&doi=10.1145%2f3377325.3377540&partnerID=40&md5=a8f49613d8283d72cdbd62fc5fc66c8a","Stony Brook University, United States; Old Dominion University","Aydin, A.S., Stony Brook University, United States; Feiz, S., Stony Brook University, United States; Ashok, V., Old Dominion University; Ramakrishnan, I.V., Stony Brook University, United States","Navigating webpages with screen readers is a challenge even with recent improvements in screen reader technologies and the increased adoption of web standards for accessibility, namely ARIA. ARIA landmarks, an important aspect of ARIA, lets screen reader users access different sections of the webpage quickly, by enabling them to skip over blocks of irrelevant or redundant content. However, these landmarks are sporadically and inconsistently used by web developers, and in many cases, even absent in numerous web pages. Therefore, we propose SaIL, a scalable approach that automatically detects the important sections of a web page, and then injects ARIA landmarks into the corresponding HTML markup to facilitate quick access to these sections. The central concept underlying SaIL is visual saliency, which is determined using a state-of-the-art deep learning model that was trained on gaze-tracking data collected from sighted users in the context of web browsing. We present the findings of a pilot study that demonstrated the potential of SaIL in reducing both the time and effort spent in navigating webpages with screen readers. © ACM.","landmarks; screen reader; WAI-ARIA; web accessibility","Deep learning; Eye tracking; Websites; landmarks; Learning models; Redundant content; Scalable approach; Screen readers; State of the art; WAI-ARIA; Web accessibility; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85082472741
"Murthy L.R.D.","57205505055;","Multimodal interaction for real and virtual environments",2020,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,"29","30",,1,"10.1145/3379336.3381506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082168765&doi=10.1145%2f3379336.3381506&partnerID=40&md5=07cada36f153b60a0d0c9dbde92799a6","I3D Lab, Centre for Product Design and Manufacturing, Indian Institute of Science, Bangalore, India","Murthy, L.R.D., I3D Lab, Centre for Product Design and Manufacturing, Indian Institute of Science, Bangalore, India","Multimodal interfaces can leverage the information from multiple modalities to provide robust and error-free interaction. Early multimodal interfaces demonstrate the feasibility of building such systems but focused on specific applications. The challenge in building adaptive systems is lack of techniques for input data fusion. In this direction, we have developed a multimodal head and eye gaze interface and evaluated it in two scenarios. In aviation scenario, our interface has reduced the task time and perceived cognitive load significantly from the existing interface. We have also studied the effect of various output conditions on user's performance in a Virtual Reality (VR) task. Further, we are making our proposed interface to include additional modalities and building novel haptic and multimodal output systems for VR. © 2020 International Conference on Intelligent User Interfaces, Proceedings IUI. All rights reserved.","Machine Learning; Multimodal Interfaces; Virtual Reality","Data fusion; Interactive computer systems; Learning systems; Virtual reality; Cognitive loads; Eye-gaze interface; In-buildings; Multi-modal; Multi-Modal Interactions; Multi-modal interfaces; Multimodal output; Multiple modalities; Haptic interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85082168765
"Davari M., Hienert D., Kern D., Dietze S.","57215967301;49963420600;22834982600;19638604400;","The role of word-eye-fixations for query term prediction",2020,"CHIIR 2020 - Proceedings of the 2020 Conference on Human Information Interaction and Retrieval",,,,"422","426",,,"10.1145/3343413.3378010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082443691&doi=10.1145%2f3343413.3378010&partnerID=40&md5=5d59108a347ec2c76edd53e059e5dfb8","GESIS - Leibniz Institute for the Social Sciences, Cologne, Germany","Davari, M., GESIS - Leibniz Institute for the Social Sciences, Cologne, Germany; Hienert, D., GESIS - Leibniz Institute for the Social Sciences, Cologne, Germany; Kern, D., GESIS - Leibniz Institute for the Social Sciences, Cologne, Germany; Dietze, S., GESIS - Leibniz Institute for the Social Sciences, Cologne, Germany","Throughout the search process, the user's gaze on inspected SERPs and websites can reveal his or her search interests. Gaze behavior can be captured with eye tracking and described with word-eye-fixations. Word-eye-fixations contain the user's accumulated gaze fixation duration on each individual word of a web page. In this work, we analyze the role of word-eye-fixations for predicting query terms. We investigate the relationship between a range of in-session features, in particular, gaze data, with the query terms and train models for predicting query terms. We use a dataset of 50 search sessions obtained through a lab study in the social sciences domain. Using established machine learning models, we can predict query terms with comparably high accuracy, even with only little training data. Feature analysis shows that the categories Fixation, Query Relevance and Session Topic contain the most effective features for our task. © 2020 ACM.","Eye tracking; Gaze behavior; Prediction; Query terms","Forecasting; Websites; Feature analysis; Fixation duration; Gaze behavior; High-accuracy; Machine learning models; Query terms; Search process; Search sessions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082443691
"Vidyapu S., Vedula V.S., Bhattacharya S.","57192807280;23092262900;56282530700;","Weighted Voting-Based Effective Free-Viewing Attention Prediction on Web Image Elements",2020,"Interacting with Computers","32","2",,"170","184",,,"10.1093/iwcomp/iwaa013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095595539&doi=10.1093%2fiwcomp%2fiwaa013&partnerID=40&md5=2118c961cea9a5dd08fc927453fdd36f","Department of Computer Science & Engineering, Indian Institute of Technology, Guwahati, Assam, 781039, India","Vidyapu, S., Department of Computer Science & Engineering, Indian Institute of Technology, Guwahati, Assam, 781039, India; Vedula, V.S., Department of Computer Science & Engineering, Indian Institute of Technology, Guwahati, Assam, 781039, India; Bhattacharya, S., Department of Computer Science & Engineering, Indian Institute of Technology, Guwahati, Assam, 781039, India","Quantifying and predicting the user attention on web image elements finds applications in synthesis and rendering of elements on webpages. However, the majority of the existing approaches either overlook the visual characteristics of these elements or do not incorporate the users’ visual attention. Especially, obtaining a representative quantified attention (for images) from the attention allocation of multiple users is a challenging task. Toward overcoming the challenge for free-viewing attention, this paper introduces four weighted voting strategies to assign effective visual attention (fixation index (FI)) for web image elements. Subsequently, the prominent image visual features in explaining the assigned attention are identified. Further, the association between image visual features and the assigned attention is modeled as a multi-class prediction problem, which is solved through support vector machine-based classification. The analysis of the proposed approach on real-world webpages reveals the following: (i) image element’s position, size and mid-level color histograms are highly informative for the four weighting schemes; (ii) the presented computational approach outperforms the baseline for four weighted voting schemes with an average accuracy of 85% and micro F1-score of 60%; and (iii) uniform weighting (same weight for all FIs) is adequate for estimating the user’s initial attention while the proportional weighting (weight the FI in proportion to its likelihood of occurrence) extends to the latter attention prediction. © The Author(s) 2020. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved.","Eye-tracking; Image; Machine learning; Support vector machine; Visual attention; Webpage","Behavioral research; Support vector machines; Websites; Class prediction; Color histogram; Computational approach; Visual Attention; Visual feature; Weighted voting; Weighted voting strategies; Weighting scheme; Forecasting",Article,"Final","",Scopus,2-s2.0-85095595539
"Yang S., Bailey E., Yang Z., Ostrometzky J., Zussman G., Seskar I., Kostic Z.","57219269638;57219271586;57219270142;55014680800;14421965000;6602603899;57207510598;","COSMOS Smart Intersection: Edge Compute and Communications for Bird's Eye Object Tracking",2020,"2020 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2020",,,"9156225","","",,5,"10.1109/PerComWorkshops48775.2020.9156225","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086140954&doi=10.1109%2fPerComWorkshops48775.2020.9156225&partnerID=40&md5=0748dba39772d94a63c97f31f39b8a27","Columbia University, Dept. of Electrical Engineering, New York City, United States; Columbia University, Dept. of Computer Science, New York City, United States; Winlab, Rutgers University, New Jersey, United States","Yang, S., Columbia University, Dept. of Electrical Engineering, New York City, United States; Bailey, E., Columbia University, Dept. of Computer Science, New York City, United States; Yang, Z., Columbia University, Dept. of Electrical Engineering, New York City, United States; Ostrometzky, J., Columbia University, Dept. of Electrical Engineering, New York City, United States; Zussman, G., Columbia University, Dept. of Electrical Engineering, New York City, United States; Seskar, I., Winlab, Rutgers University, New Jersey, United States; Kostic, Z., Columbia University, Dept. of Electrical Engineering, New York City, United States","Smart-city intersections will play a crucial role in automated traffic management and improvement in pedestrian safety in cities of the future. They will (i) aggregate data from in-vehicle and infrastructure sensors; (ii) process the data by taking advantage of low-latency high-bandwidth communications, edge-cloud computing, and AI-based detection and tracking of objects; and (iii) provide intelligent feedback and input to control systems. The Cloud Enhanced Open Software Defined Mobile Wireless Testbed for City-Scale Deployment (COSMOS) enables research on technologies supporting smart cities. In this paper, we provide results of experiments using bird's eye cameras to detect and track vehicles and pedestrians from the COSMOS pilot site. We assess the capabilities for real-time computation, and detection and tracking accuracy-by evaluating and customizing a selection of video pre-processing and deep-learning algorithms. Distinct issues that are associated with the difference in scale for bird's eye view of pedestrians vs. cars are explored and addressed: the best multiple-object tracking accuracies (MOTA) for cars are around 73.2, and around 2.8 for pedestrians. The real-time goal of 30 frames-per-second-i.e., a total of 33.3 ms of latency for object detection for vehicles will be reachable once the processing time is improved roughly by a factor of three. © 2020 IEEE.","deep learning; detection; smart-city; smart-intersection; testbed; tracking; wireless","Birds; Deep learning; Edge computing; Eye tracking; Learning algorithms; Object detection; Object recognition; Pedestrian safety; Smart city; Ubiquitous computing; Vehicles; Aggregate datum; Detection and tracking; Frames per seconds; High bandwidth communication; Mobile wireless; Multiple object tracking; Real-time computations; Traffic management; Object tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086140954
"Wang Z., Zhao J., Lu C., Huang H., Yang F., Li L., Guo Y.","57195938983;57191896599;57217766799;57216948641;57216772514;57216955746;55642210700;","Learning to detect head movement in unconstrained remote gaze estimation in the wild",2020,"Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020",,,"9093476","3432","3441",,1,"10.1109/WACV45572.2020.9093476","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085499134&doi=10.1109%2fWACV45572.2020.9093476&partnerID=40&md5=d075fa34c89664023d7b09cdf430f9da","Columbia University, United States; XPENG Motors, China; Institute of North Electronic Equipment, Beijing, China","Wang, Z., Columbia University, United States, XPENG Motors, China; Zhao, J., Institute of North Electronic Equipment, Beijing, China; Lu, C., XPENG Motors, China; Huang, H., XPENG Motors, China; Yang, F., XPENG Motors, China; Li, L., XPENG Motors, China; Guo, Y., XPENG Motors, China","Unconstrained remote gaze estimation remains challenging mostly due to its vulnerability to the large variability in head-pose. Prior solutions struggle to maintain reliable accuracy in unconstrained remote gaze tracking. Among them, appearance-based solutions demonstrate tremendous potential in improving gaze accuracy. However, existing works still suffer from head movement and are not robust enough to handle real-world scenarios. Especially most of them study gaze estimation under controlled scenarios where the collected datasets often cover limited ranges of both head-pose and gaze which introduces further bias. In this paper, we propose novel end-to-end appearance-based gaze estimation methods that could more robustly incorporate different levels of head-pose representations into gaze estimation. Our method could generalize to real-world scenarios with low image quality, different lightings and scenarios where direct head-pose information is not available. To better demonstrate the advantage of our methods, we further propose a new benchmark dataset with the most rich distribution of head-gaze combination reflecting real-world scenarios. Extensive evaluations on several public datasets and our own dataset demonstrate that our method consistently outperforms the state-of-the-art by a significant margin. © 2020 IEEE.",,"Computer vision; Appearance based; Benchmark datasets; Gaze estimation; Gaze tracking; Head movements; Real-world scenario; Remote gaze estimation; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085499134
"Ichii T., Mitake H., Hasegawa S.","57204616710;14058759600;8410343100;","TEllipsoid: Ellipsoidal Display for Videoconference System Transmitting Accurate Gaze Direction",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2020",,,"9089609","775","781",,,"10.1109/VR46266.2020.1580098058128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085496290&doi=10.1109%2fVR46266.2020.1580098058128&partnerID=40&md5=01754d921e4e866d0a4dab1301c282e0","Tokyo Institute of Technology, Japan","Ichii, T., Tokyo Institute of Technology, Japan; Mitake, H., Tokyo Institute of Technology, Japan; Hasegawa, S., Tokyo Institute of Technology, Japan","We propose ""TEllipsoid"", an ellipsoidal display for video conference systems that can provide not only accurate eye-gaze transmission but also practicality in conferences, namely the convenience to use and the preservation of the identity of the displayed face.The display comprises an ellipsoidal screen, a small projector, and a convex mirror, where the bottom-installed projector projects the facial image of a remote participant onto the screen via the convex mirror. The facial image is made from photos shot from 360 degrees around the participant. Moreover, the image is modified to improve identity. The gaze representation is implemented by projecting the 3D model of eyeballs onto a virtual ellipsoidal screen.We evaluated the gaze transmissibility of the display in conference situations. As a result of experiments, we concluded that accurate gaze transmission is available in conferences when the angular distance of the adjacent participants is more than 38.5 degrees. © 2020 IEEE.","Communication hardware; Displays and imagers; Displays and imagers; Hardware; Human computer interaction (HCI); Human-centered computing; Interaction devices; Interfaces and storage","3D modeling; Image enhancement; Mirrors; Transmissions; User interfaces; Angular distance; Convex mirror; Eye-gaze; Facial images; Gaze direction; Gaze representation; Video conference systems; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85085496290
"Chen Z., Shi B.E.","56808413900;7402547071;","Offset calibration for appearance-based gaze estimation via gaze decomposition",2020,"Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020",,,"9093419","259","268",,3,"10.1109/WACV45572.2020.9093419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085491803&doi=10.1109%2fWACV45572.2020.9093419&partnerID=40&md5=3a8ca439f935368aecfed3a21e3063ce","Hong Kong University of Science and Technology, Hong Kong","Chen, Z., Hong Kong University of Science and Technology, Hong Kong; Shi, B.E., Hong Kong University of Science and Technology, Hong Kong","Appearance-based gaze estimation provides relatively unconstrained gaze tracking. However, subject-independent models achieve limited accuracy partly due to individual variations. To improve estimation, we propose a gaze decomposition method that enables low complexity calibration, i.e., using calibration data collected when subjects view only one or a few gaze targets and the number of images per gaze target is small. Lowering the complexity of calibration makes it more convenient and less timeconsuming for the user, and more widely applicable. Motivated by our finding that the inter-subject squared bias exceeds the intra-subject variance for a subject-independent estimator, we decompose the gaze estimate into the sum of a subject-independent term estimated from the input image by a deep convolutional network and a subject-dependent bias term. During training, both the weights of the deep network and the bias terms are estimated. During testing, if no calibration data is available, we can set the bias term to zero. Otherwise, the bias term can be estimated from images of the subject gazing at known gaze targets. Experimental results on three datasets show that without calibration, our method outperforms state-of-the-art by at least 6.3%. For low complexity calibration sets, our method outperforms other calibration methods. More complex calibration algorithms do not outperform our method until the size of the calibration set is excessively large. Even then, the gains obtained by alternatives are small, e.g., only 0.1° lower error for 64 gaze targets. Source code is available at https://github.com/czk32611/Gaze-Decomposition. © 2020 IEEE.",,"Calibration; Complex networks; Computer vision; Image enhancement; Appearance based; Calibration algorithm; Calibration method; Convolutional networks; Decomposition methods; Independent model; Offset calibration; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085491803
"Rahman Y., Asish S.M., Fisher N.P., Bruce E.C., Kulshreshth A.K., Borst C.W.","57215358246;57215357842;57216950668;57216956088;55315352400;9736479200;","Exploring Eye Gaze Visualization Techniques for Identifying Distracted Students in Educational VR",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2020",,,"9089578","868","877",,4,"10.1109/VR46266.2020.1581300202881","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085468135&doi=10.1109%2fVR46266.2020.1581300202881&partnerID=40&md5=025f7850e934a761cf646179939ba59c","University of Louisiana at Lafayette, United States","Rahman, Y., University of Louisiana at Lafayette, United States; Asish, S.M., University of Louisiana at Lafayette, United States; Fisher, N.P., University of Louisiana at Lafayette, United States; Bruce, E.C., University of Louisiana at Lafayette, United States; Kulshreshth, A.K., University of Louisiana at Lafayette, United States; Borst, C.W., University of Louisiana at Lafayette, United States","Virtual Reality (VR) headsets with embedded eye trackers are appearing as consumer devices (e.g. HTC Vive Eye, FOVE). These devices could be used in VR-based education (e.g., a virtual lab, a virtual field trip) in which a live teacher guides a group of students. The eye tracking could enable better insights into students' activities and behavior patterns. For real-time insight, a teacher's VR environment can display student eye gaze. These visualizations would help identify students who are confused/distracted, and the teacher could better guide them to focus on important objects. We present six gaze visualization techniques for a VR-embedded teacher's view, and we present a user study to compare these techniques. The results suggest that a short particle trail representing eye trajectory is promising. In contrast, 3D heatmaps (an adaptation of traditional 2D heatmaps) for visualizing gaze over a short time span are problematic. © 2020 IEEE.","Human-centered computing; Usability Testing; Visualization design and evaluation methods; Visualization techniques","Distance education; Eye tracking; Students; User interfaces; Visualization; Behavior patterns; Consumer devices; Eye trackers; Important object; User study; Virtual field trips; Virtual lab; Visualization technique; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85085468135
"Heo H., Lee M., Kim S., Hwang Y.","57204140010;57216933665;25652751100;7402311392;","Gaze+Gesture Interface: Considering Social Acceptability",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VRW 2020",,,"9090522","691","692",,,"10.1109/VRW50115.2020.00196","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085377665&doi=10.1109%2fVRW50115.2020.00196&partnerID=40&md5=87ec1e4194839d2367bd32a2e31b84e7","Korea Electronics Technology Institute, South Korea","Heo, H., Korea Electronics Technology Institute, South Korea; Lee, M., Korea Electronics Technology Institute, South Korea; Kim, S., Korea Electronics Technology Institute, South Korea; Hwang, Y., Korea Electronics Technology Institute, South Korea","In public places like cafes, the usage of smart glasses with interfaces including controller, touchpad, voice, or mid-air gesture not only receives a lot of interest from people around him or her but also cannot protect the privacy of individuals. If smart glasses become more advanced and more popular than regular glasses in the future, socially acceptable user interfaces can be required. In this paper, we propose a user interface on HoloLens by using gaze tracking instead of head tracking for navigation, and unobtrusive gesture based on deep learning instead of mid-air gestures for selection/manipulation, that is more socially acceptable than the existing user interfaces on smart glasses. A study was conducted to investigate social acceptability from the users' perspective, and the results showed the advantages of the proposed method to improve social acceptability. © 2020 IEEE.","Gestrual input; Human computer interaction (HCI); Human-centered computing; Interaction techniques","Air navigation; Deep learning; Eye tracking; Glass; Virtual reality; Gaze tracking; Gesture interfaces; Glasses In; Head tracking; Public places; Smart glass; Social acceptability; Touchpad; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85085377665
"Onuki Y., Kudo K., Kumazawa I.","57194029718;57203743861;55943452700;","Removal of the Infrared Light Reflection of Eyeglass Using Multi-Channel CycleGAN Applied for the Gaze Estimation Images",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VRW 2020",,,"9090544","591","592",,,"10.1109/VRW50115.2020.00146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085365484&doi=10.1109%2fVRW50115.2020.00146&partnerID=40&md5=4334679e9f8405f0b97e5b9e55809f56","Tokyo Institute of Technology, Japan","Onuki, Y., Tokyo Institute of Technology, Japan; Kudo, K., Tokyo Institute of Technology, Japan; Kumazawa, I., Tokyo Institute of Technology, Japan","In virtual reality (VR) environments, the importance of the eye gaze estimation is rapidly increasing. The geometrical model base method is commonly used by equipping infrared (IR) light sources and cameras inside of the head mount displays (HMDs). Some HMDs enable users to wear with spectacles on, and, in this case, IR light reflections of the eyeglass often causes serious obstruction to detect those of the corneal. In this study, we propose the multi-channel CycleGAN to generate the eye images with no eyeglass from those with eyeglass. Proposed method has 4 channels input, which consists of three normal eye images at different time points and an image in blinking, in order to distinguish stationary and moving reflections in images. Proposal achieved to selectively remove the eyeglass reflections and keep the corneal reflections alive in sequential gaze estimation images. © 2020 IEEE.","Computing methodologies; Human computer interaction; Human-centered computing; Interaction paradigms; Machine learning; Machine learning algorithm; Virtual reality","Light reflection; Light sources; User interfaces; Corneal reflection; Eye images; Gaze estimation; Geometrical modeling; Head-mount displays; Infrared light; Multi channel; Time points; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85085365484
"Pavan Kumar B.N., Balasubramanyam A., Patil A.K., Chethana B., Chai Y.H.","57202496237;57211045502;55967728300;57205201490;7102457214;","GazeGuide: An eye-gaze-guided active immersive UAV camera",2020,"Applied Sciences (Switzerland)","10","5","1668","","",,4,"10.3390/app10051668","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081891409&doi=10.3390%2fapp10051668&partnerID=40&md5=ea18010583794ce097fcedf651a61e04","Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 06974, South Korea","Pavan Kumar, B.N., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 06974, South Korea; Balasubramanyam, A., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 06974, South Korea; Patil, A.K., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 06974, South Korea; Chethana, B., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 06974, South Korea; Chai, Y.H., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 06974, South Korea","Over the years, gaze input modality has been an easy and demanding human-computer interaction (HCI) method for various applications. The research of gaze-based interactive applications has advanced considerably, as HCIs are no longer constrained to traditional input devices. In this paper, we propose a novel immersive eye-gaze-guided camera (called GazeGuide) that can seamlessly control the movements of a camera mounted on an unmanned aerial vehicle (UAV) from the eye-gaze of a remote user. The video stream captured by the camera is fed into a head-mounted display (HMD) with a binocular eye tracker. The user's eye-gaze is the sole input modality to maneuver the camera. A user study was conducted considering the static and moving targets of interest in a three-dimensional (3D) space to evaluate the proposed framework. GazeGuide was compared with a state-of-the-art input modality remote controller. The qualitative and quantitative results showed that the proposed GazeGuide performed significantly better than the remote controller. © 2020 by the authors.","Eye tracking; Eye-gaze; Gaze input; Gaze-based interaction; HMD; HRI; Robotics; Surveillance and monitoring; Virtual reality",,Article,"Final","",Scopus,2-s2.0-85081891409
"Temel D., Mathew M.J., Alregib G., Khalifa Y.M.","55652281700;57209026991;6506443965;26660972600;","Relative Afferent Pupillary Defect Screening through Transfer Learning",2020,"IEEE Journal of Biomedical and Health Informatics","24","3","8790783","788","795",,1,"10.1109/JBHI.2019.2933773","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081730252&doi=10.1109%2fJBHI.2019.2933773&partnerID=40&md5=c25846e49e77042fbaafb651bec55228","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA  30322, United States; Emory University, School of MedicineGA, United States","Temel, D., School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA  30322, United States; Mathew, M.J., School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA  30322, United States; Alregib, G., School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA  30322, United States; Khalifa, Y.M., Emory University, School of MedicineGA, United States","Abnormalities in pupillary light reflex can indicate optic nerve disorders that may lead to permanent visual loss if not diagnosed in an early stage. In this study, we focus on relative afferent pupillary defect (RAPD), which is based on the difference between the reactions of the eyes when they are exposed to light stimuli. Incumbent RAPD assessment methods are based on subjective practices that can lead to unreliable measurements. To eliminate subjectivity and obtain reliable measurements, we introduced an automated framework to detect RAPD. For validation, we conducted a clinical study with lab-on-a-headset, which can perform automated light reflex test. In addition to benchmarking handcrafted algorithms, we proposed a transfer learning-based approach that transformed a deep learning-based generic object recognition algorithm into a pupil detector. Based on the conducted experiments, proposed algorithm RAPDNet can achieve a sensitivity and a specificity of 90.6% over 64 test cases in a balanced set, which corresponds to an AUC of 0.929 in ROC analysis. According to our benchmark with three handcrafted algorithms and nine performance metrics, RAPDNet outperforms all other algorithms in every performance category. © 2013 IEEE.","artificial intelligence-based eye examination; deep learning; portable eye exam; pupil detection; Pupillary light reflex video dataset; relative afferent pupillary defect (RAPD); teleophthalmology; transfer learning","Benchmarking; Deep learning; Defects; Eye protection; Object recognition; portable eye exam; Pupil detection; relative afferent pupillary defect (RAPD); Teleophthalmology; Video dataset; Transfer learning; adult; afferent pupillary defect; algorithm; Article; binocular vision; controlled study; diagnostic accuracy; diagnostic test accuracy study; eye examination; eye tracking; female; human; information processing; learning; male; measurement accuracy; optic nerve; performance; predictive value; pupil reflex; receiver operating characteristic; recognition; relative; screening; sensitivity and specificity; validation study; visual field; computer assisted diagnosis; diagnostic imaging; machine learning; physiology; procedures; pupil; pupil disease; telemedicine; visual system examination; Diagnostic Techniques, Ophthalmological; Humans; Image Interpretation, Computer-Assisted; Machine Learning; Pupil; Pupil Disorders; Reflex, Pupillary; ROC Curve; Telemedicine",Article,"Final","",Scopus,2-s2.0-85081730252
"Wibirama S., Santosa P.I., Widyarani P., Brilianto N., Hafidh W.","26654457700;9636895500;57209498321;57209500344;57209502913;","Physical discomfort and eye movements during arbitrary and optical flow-like motions in stereo 3D contents",2020,"Virtual Reality","24","1",,"39","51",,1,"10.1007/s10055-019-00386-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067974009&doi=10.1007%2fs10055-019-00386-w&partnerID=40&md5=3a20495dca0df230201028f8b06a3c82","Department of Electrical Engineering and Information Technology, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia","Wibirama, S., Department of Electrical Engineering and Information Technology, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Santosa, P.I., Department of Electrical Engineering and Information Technology, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Widyarani, P., Department of Electrical Engineering and Information Technology, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Brilianto, N., Department of Electrical Engineering and Information Technology, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Hafidh, W., Department of Electrical Engineering and Information Technology, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia","Users of stereo 3D technology commonly report physical discomfort during or after exposure of stereo 3D contents. The discomfort has been associated with sensation of arbitrary and optical flow-like self-motion. However, there is no information on whether arbitrary motion induces stronger physical discomfort compared with optical flow-like motion. To address this research gap, we investigate physical discomfort among players and spectators of stereo 3D contents using eye tracking and Simulator Sickness Questionnaire. Thirty participants (N= 30) acted as players and spectators of a first-person shooter (FPS) and a car racing game. The FPS and the car racing game produce a sensation of arbitrary and optical flow-like self-motion, respectively. Experimental results show that the FPS game induces more severe physical discomfort than its racing counterpart (p< 0.0083 , with a Bonferroni correction to the p value). We also found that severeness of oculomotor symptoms can be predicted using two eye movements metrics: the amount of fixational eye movements and viewing duration at the center of the screen. Our study implies that one should pay particular attention to different types of self-motion in stereo 3D contents regardless of whether the user controls or solely watches the contents. Our study also suggests that physical discomfort can be reduced by decreasing the frequency of fixational eye movements while prolonging the duration of each fixation at the center of screen. © 2019, Springer-Verlag London Ltd., part of Springer Nature.","Cybersickness; Eye tracking; Human factors in virtual reality; Stereo 3D","Eye tracking; Optical flows; Racing automobiles; Virtual reality; 3D technology; Bonferroni correction; Cybersickness; First person shooter; Fixational eye movements; Self motion; Simulator sickness; User control; Eye movements",Article,"Final","",Scopus,2-s2.0-85067974009
"Garbin S.J., Shen Y., Schuetz I., Cavin R., Hughes G., Komogortsev O., Talathi S.S.","57191486204;57217096695;36894943600;57213731048;57217096836;6506328653;57224997923;","Dataset for eye tracking on a virtual reality platform",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,1,"10.1145/3379155.3391317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086183152&doi=10.1145%2f3379155.3391317&partnerID=40&md5=1accade7dbc012d130e63d98046b8559","University College London, United Kingdom; Facebook Reality Labs, United States; Google Via Adecco, United States; Facebook Reality Labs, Texas State University, United States","Garbin, S.J., University College London, United Kingdom; Shen, Y., Facebook Reality Labs, United States; Schuetz, I., Facebook Reality Labs, United States; Cavin, R., Facebook Reality Labs, United States; Hughes, G., Google Via Adecco, United States; Komogortsev, O., Facebook Reality Labs, Texas State University, United States; Talathi, S.S., Facebook Reality Labs, United States","We present a large scale data set of eye-images captured using a virtual-reality (VR) head mounted display mounted with two synchronized eye-facing cameras at a frame rate of 200 Hz under controlled illumination. This dataset is compiled from video capture of the eye-region collected from 152 individual participants and is divided into four subsets: (i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil and sclera (ii) 252,690 unlabeled eye-images, (iii) 91,200 frames from randomly selected video sequences of 1.5 seconds in duration, and (iv) 143 pairs of left and right point cloud data compiled from corneal topography of eye regions collected from a subset, 143 out of 152, participants in the study. A baseline experiment has been evaluated on the dataset for the task of semantic segmentation of pupil, iris, sclera and background, with the mean intersection-over-union (mIoU) of 98.3 %. We anticipate that this dataset will create opportunities to researchers in the eye tracking community and the broader machine learning and computer vision community to advance the state of eye-tracking for VR applications, which in its turn will have greater implications in Human-Computer Interaction. © 2020 ACM.","appearance-based eye tracking; eye tracking; iris; pupil; sclera; segmentation; virtual reality","Helmet mounted displays; Human computer interaction; Image segmentation; Semantics; Topography; Virtual reality; Corneal topography; Head mounted displays; Large scale data sets; Point cloud data; Semantic segmentation; Video sequences; Vision communities; VR applications; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086183152
"Keyvanara M., Allison R.","57195734927;7101890629;","Effect of a constant camera rotation on the visibility of transsaccadic camera shifts",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,,"10.1145/3379155.3391318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086181901&doi=10.1145%2f3379155.3391318&partnerID=40&md5=8a3d156dcd2cfa4e699fd543468560cf","Department of Electrical Engineering and Computer Science, York University, Canada","Keyvanara, M., Department of Electrical Engineering and Computer Science, York University, Canada; Allison, R., Department of Electrical Engineering and Computer Science, York University, Canada","Often in 3D games and virtual reality, changes in fixation occur during locomotion or other simulated head movements. We investigated whether a constant camera rotation in a virtual scene modulates saccadic suppression. The users viewed 3D scenes from the vantage point of a virtual camera which was either stationary or rotated at a constant rate about a vertical axis (camera pan) or horizontal axis (camera tilt). During this motion, observers fixated an object that was suddenly displaced horizontally/vertically in the scene, triggering them to produce a saccade. During the saccade an additional sudden movement was applied to the virtual camera. We estimated discrimination thresholds for these transsaccadic camera shifts using a Bayesian adaptive procedure. With an ongoing camera pan, we found higher thresholds (less noticeability) for additional sudden horizontal camera motion. Likewise, during simulated vertical head movements (i.e. a camera tilt), vertical transsaccadic image displacements were better hidden from the users for both horizontal and vertical saccades. Understanding the effect of continuous movement on the visibility of a sudden transsaccadic change can help optimize the visual performance of gaze-contingent displays and improve user experience. © 2020 ACM.","Eye Tracking; Foveated Rendering; Gaze-Contingent Displays; Redirected Walking; Saccadic Suppression; Virtual Reality","Eye movements; Eye tracking; User experience; Virtual reality; Visibility; Adaptive procedure; Camera rotations; Discrimination thresholds; Gaze-contingent displays; Horizontal axis; Saccadic suppression; Simulated heads; Visual performance; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-85086181901
"Barz M., Stauden S., Sonntag D.","57189847803;57204112887;12241487800;","Visual search target inference in natural interaction settings with machine learning",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,4,"10.1145/3379155.3391314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086181657&doi=10.1145%2f3379155.3391314&partnerID=40&md5=622a8dda11e08153ba8c12d3c3b46711","German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Saarland University, Saarbrücken, Germany; Saarbrücken Graduate School of Computer Science, Germany","Barz, M., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany, Saarbrücken Graduate School of Computer Science, Germany; Stauden, S., Saarland University, Saarbrücken, Germany; Sonntag, D., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany","Visual search is a perceptual task in which humans aim at identifying a search target object such as a traffic sign among other objects. Search target inference subsumes computational methods for predicting this target by tracking and analyzing overt behavioral cues of that person, e.g., the human gaze and fixated visual stimuli. We present a generic approach to inferring search targets in natural scenes by predicting the class of the surrounding image segment. Our method encodes visual search sequences as histograms of fixated segment classes determined by SegNet, a deep learning image segmentation model for natural scenes. We compare our sequence encoding and model training (SVM) to a recent baseline from the literature for predicting the target segment. Also, we use a new search target inference dataset. The results show that, first, our new segmentation-based sequence encoding outperforms the method from the literature, and second, that it enables target inference in natural settings. © 2020 ACM.","Machine Learning; Mobile Eyetracking; Search Target Inference; Visual Attention","Deep learning; Encoding (symbols); Eye tracking; Forecasting; Image segmentation; Signal encoding; Support vector machines; Behavioral cues; Generic approach; Image segmentation model; Image segments; Model training; Natural interactions; Sequence encoding; Visual stimulus; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85086181657
"Castner N., Küebler T.C., Scheiter K., Richter J., Eder T., Hüettig F., Keutel C., Kasneci E.","57193611337;55701951700;6507519353;57008653700;57202894457;57195118616;54393473100;56059892600;","Deep semantic gaze embedding and scanpath comparison for expertise classification during OPT viewing",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,6,"10.1145/3379155.3391320","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086181141&doi=10.1145%2f3379155.3391320&partnerID=40&md5=fbc09bcb40e162c481e41276af366f7f","Perception Engineering, University of Tübingen, Tübingen, Germany; Leibniz-Institut für Wissensmedien, Tübingen, Germany; University Hospital Tübingen, Tübingen, Germany; Department of Prosthodontics; Department of Radiology, Center of Dentistry, Oral Medicine and Maxillofacial Surgery","Castner, N., Perception Engineering, University of Tübingen, Tübingen, Germany; Küebler, T.C., Perception Engineering, University of Tübingen, Tübingen, Germany; Scheiter, K., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Richter, J., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Eder, T., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Hüettig, F., University Hospital Tübingen, Tübingen, Germany, Department of Prosthodontics; Keutel, C., University Hospital Tübingen, Tübingen, Germany, Department of Radiology, Center of Dentistry, Oral Medicine and Maxillofacial Surgery; Kasneci, E., Perception Engineering, University of Tübingen, Tübingen, Germany","Modeling eye movement indicative of expertise behavior is decisive in user evaluation. However, it is indisputable that task semantics affect gaze behavior. We present a novel approach to gaze scanpath comparison that incorporates convolutional neural networks (CNN) to process scene information at the fixation level. Image patches linked to respective fixations are used as input for a CNN and the resulting feature vectors provide the temporal and spatial gaze information necessary for scanpath similarity comparison. We evaluated our proposed approach on gaze data from expert and novice dentists interpreting dental radiographs using a local alignment similarity score. Our approach was capable of distinguishing experts from novices with 93% accuracy while incorporating the image semantics. Moreover, our scanpath comparison using image patch features has the potential to incorporate task semantics from a variety of tasks. © 2020 ACM.","Deep Learning; Eye Tracking; Learning; Medical image interpretation; Scanpath analysis","Convolutional neural networks; Eye movements; Semantics; Dental radiographs; Expert and novices; Feature vectors; Local alignment; Scanpath comparisons; Similarity scores; Temporal and spatial; User evaluations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086181141
"Eskildsen A.M., Hansen D.W.","57217096874;15063910800;","Label Likelihood Maximisation: Adapting iris segmentation models using domain adaptation",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,,"10.1145/3379155.3391327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086181110&doi=10.1145%2f3379155.3391327&partnerID=40&md5=353a17b46a91cd5eaa2195c22a4f2604","Department of Computer Science, IT University of Copenhagen, Copenhagen, Denmark","Eskildsen, A.M., Department of Computer Science, IT University of Copenhagen, Copenhagen, Denmark; Hansen, D.W., Department of Computer Science, IT University of Copenhagen, Copenhagen, Denmark","We propose to use unlabelled eye image data for domain adaptation of an iris segmentation network. Adaptation allows the model to be less reliant on its initial generality. This is beneficial due to the large variance exhibited by eye image data which makes training of robust models difficult. The method uses a label prior in conjunction with network predictions to produce pseudo-labels. These are used in place of ground-truth data to adapt a base model. A fully connected neural network performs the pixel-wise iris segmentation. The base model is trained on synthetic data and adapted to several existing datasets with real-world eye images. The adapted models improve the average pupil centre detection rates by 24% at a distance of 25 pixels. We argue that the proposed method, and domain adaptation in general, is an interesting direction for increasing robustness of eye feature detectors. © 2020 ACM.","deep learning; domain adaptation; iris segmentation","Feature extraction; Image segmentation; Pixels; Detection rates; Domain adaptation; Feature detector; Fully connected neural network; Ground truth data; Iris segmentation; Network prediction; Synthetic data; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086181110
"Venuprasad P., Xu L., Huang E., Gilman A., Leanne Chukoskie L., Cosman P.","57210106962;57217096675;57217096729;16318668400;6507740817;7003359562;","Analyzing gaze behavior using object detection and unsupervised clustering",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,2,"10.1145/3379155.3391316","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086180732&doi=10.1145%2f3379155.3391316&partnerID=40&md5=bd435c74df2eaaff3bdb37059656f50b","University of California San Diego, San Diego, CA, United States; Canyon Crest Academy, San Diego, CA, United States; Massey University, Auckland, New Zealand","Venuprasad, P., University of California San Diego, San Diego, CA, United States; Xu, L., University of California San Diego, San Diego, CA, United States; Huang, E., Canyon Crest Academy, San Diego, CA, United States; Gilman, A., Massey University, Auckland, New Zealand; Leanne Chukoskie, L., University of California San Diego, San Diego, CA, United States; Cosman, P., University of California San Diego, San Diego, CA, United States","Gaze behavior is important in early development, and atypical gaze behavior is among the first symptoms of autism. Here we describe a system that quantitatively assesses gaze behavior using eye-tracking glasses. Objects in the subject's field of view are detected using a deep learning model on the video captured by the glasses' world-view camera, and a stationary frame of reference is estimated using the positions of the detected objects. The gaze positions relative to the new frame of reference are subjected to unsupervised clustering to obtain the time sequence of looks. The clustering method increases the accuracy of look detection on test videos compared against a previous algorithm, and is considerably more robust on videos with poor calibration. © 2020 ACM.","computer vision; eye-tracking; gaze behavior; Kalman filtering; object detection; unsupervised clustering","Cameras; Deep learning; Glass; Object detection; Clustering methods; Field of views; Frame of reference; Gaze behavior; Learning models; Stationary frame; Time sequences; Unsupervised clustering; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086180732
"Bâce M., Becker V., Wang C., Bulling A.","44461063200;57195528513;57217097026;6505807414;","Combining gaze estimation and optical flow for pursuits interaction",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,1,"10.1145/3379155.3391315","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086180684&doi=10.1145%2f3379155.3391315&partnerID=40&md5=0fd144aa5451fb21505e263324c66602","Department of Computer Science, ETH Zürich, Switzerland; Department of Information Technology and Electrical Engineering, ETH Zürich, Switzerland; Institute for Visualisation and Interactive Systems, University of Stuttgart, Germany","Bâce, M., Department of Computer Science, ETH Zürich, Switzerland; Becker, V., Department of Computer Science, ETH Zürich, Switzerland; Wang, C., Department of Information Technology and Electrical Engineering, ETH Zürich, Switzerland; Bulling, A., Institute for Visualisation and Interactive Systems, University of Stuttgart, Germany","Pursuit eye movements have become widely popular because they enable spontaneous eye-based interaction. However, existing methods to detect smooth pursuits require special-purpose eye trackers. We propose the first method to detect pursuits using a single off-the-shelf RGB camera in unconstrained remote settings. The key novelty of our method is that it combines appearance-based gaze estimation with optical flow in the eye region to jointly analyse eye movement dynamics in a single pipeline. We evaluate the performance and robustness of our method for different numbers of targets and trajectories in a 13-participant user study. We show that our method not only outperforms the current state of the art but also achieves competitive performance to a consumer eye tracker for a small number of targets. As such, our work points towards a new family of methods for pursuit interaction directly applicable to an ever-increasing number of devices readily equipped with cameras. © 2020 ACM.","Gaze Estimation; Optical Flow; Pursuit Interaction; Smooth Pursuit","Cameras; Eye movements; Flow interactions; Optical flows; Appearance based; Competitive performance; Eye trackers; Gaze estimation; Remote settings; RGB cameras; Smooth pursuit; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086180684
"Szalma J., Weiss B.","57209807819;34168497800;","Data-Driven classification of dyslexia using eye-movement correlates of natural reading",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"45","","",,3,"10.1145/3379156.3391379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085738255&doi=10.1145%2f3379156.3391379&partnerID=40&md5=df3ea93078f4c41f0db2eaaf54c538c9","Brain Imaging Centre, Research Centre for Natural Sciences, Budapest, Hungary","Szalma, J., Brain Imaging Centre, Research Centre for Natural Sciences, Budapest, Hungary; Weiss, B., Brain Imaging Centre, Research Centre for Natural Sciences, Budapest, Hungary","Developmental dyslexia is a reading disability estimated to affect between 5 to 10 percent of the population. Current screening methods are limited as they tell very little about the oculomotor processes underlying natural reading. Investigation of eye-movement correlates of reading using machine learning could enhance detection of dyslexia. Here we used eye-tracking data collected during natural reading of 48 young adults (24 dyslexic, 24 control). We established a set of 67 features containing saccade-, glissade-, fixation-related measures and the reading speed. To detect participants with dyslexic reading patterns, we used a linear support vector machine with 10-fold stratified cross-validation repeated 10 times. For feature selection we used a recursive feature elimination method, and we also considered hyperparameter optimization, both with nested and regular cross-validation. The overall best model achieved a 90.1% classification accuracy, while the best nested model achieved a 75.75% accuracy. © 2020 ACM.","Classification; Dyslexia; Eye tracking; Feature selection; Machine learning; Reading","Eye tracking; Support vector machines; Classification accuracy; Cross validation; Current screening; Developmental dyslexia; Hyper-parameter optimizations; Linear Support Vector Machines; Reading patterns; Recursive feature elimination; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85085738255
"Katrychuk D., Griffith H., Komogortsev O.","57210105027;57189386560;6506328653;","A calibration framework for photosensor-based eye-tracking system",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"36","","",,,"10.1145/3379156.3391370","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085735944&doi=10.1145%2f3379156.3391370&partnerID=40&md5=d5d9c88eed44615c7dd9499aef134445","Department of Computer Science, Texas State University, San Marcos, TX, United States","Katrychuk, D., Department of Computer Science, Texas State University, San Marcos, TX, United States; Griffith, H., Department of Computer Science, Texas State University, San Marcos, TX, United States; Komogortsev, O., Department of Computer Science, Texas State University, San Marcos, TX, United States","The majority of eye-tracking systems require user-specific calibration to achieve suitable accuracy. Traditional calibration is performed by presenting targets at fixed locations that form a certain coverage of the device screen. If simple regression methods are used to learn a gaze map from the recorded data, the risk of overfitting is minimal. This is not the case if a gaze map is formed using neural networks, as is often employed in photosensor oculography (PSOG), which raises the question of careful design of calibration procedure. This paper evaluates different calibration data parsing approaches and the collection time-performance trade-off effect of grid density to build a calibration framework for PSOG with the use of video-based simulation framework. © 2020 ACM.","Calibration procedure; Eye-tracking; Machine learning; ML; Photo-sensor oculography; PSOG","Calibration; Economic and social effects; Optical sensors; Photosensitivity; Regression analysis; Calibration data; Calibration procedure; Collection time; Eye tracking systems; Performance trade-off; Regression method; Simulation framework; Traditional calibration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085735944
"Schäfer A., Isomura T., Reis G., Watanabe K., Stricker D.","57211147360;56028836100;16553196900;7406697665;6701489212;","MutualEyeContact: A conversation analysis tool with focus on eye contact",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"6","","",,,"10.1145/3379156.3391340","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085735508&doi=10.1145%2f3379156.3391340&partnerID=40&md5=afc99ddf6b228b2d2d6c9562f8c29b6f","German Research Center for Artificial Intelligence, TU Kaiserslautern, Germany; Waseda University, Japan Society for the Promotion of Science, Japan; Waseda University, University of New South Wales, Australia","Schäfer, A., German Research Center for Artificial Intelligence, TU Kaiserslautern, Germany; Isomura, T., Waseda University, Japan Society for the Promotion of Science, Japan; Reis, G., German Research Center for Artificial Intelligence, TU Kaiserslautern, Germany; Watanabe, K., Waseda University, University of New South Wales, Australia; Stricker, D., German Research Center for Artificial Intelligence, TU Kaiserslautern, Germany","Eye contact between individuals is particularly important for understanding human behaviour. To further investigate the importance of eye contact in social interactions, portable eye tracking technology seems to be a natural choice. However, the analysis of available data can become quite complex. Scientists need data that is calculated quickly and accurately. Additionally, the relevant data must be automatically separated to save time. In this work, we propose a tool called MutualEyeContact which excels in those tasks and can help scientists to understand the importance of (mutual) eye contact in social interactions. We combine state-of-the-art eye tracking with face recognition based on machine learning and provide a tool for analysis and visualization of social interaction sessions. This work is a joint collaboration of computer scientists and cognitive scientists. It combines the fields of social and behavioural science with computer vision and deep learning. © 2020 ACM.","Conversation analysis; Eye tracking; Mutual eye contact; Visualization tool","Behavioral research; Deep learning; Face recognition; Behavioural science; Computer scientists; Conversation analysis; Human behaviours; On-machines; Portable eye-tracking; Social interactions; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085735508
"Garde G., Larumbe-Bergera A., Bossavit B., Cabeza R., Porta S., Villanueva A.","57215963483;57210106737;36730794700;36763933900;7005292345;7101612861;","Gaze estimation problem tackled through synthetic images",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"34","","",,1,"10.1145/3379156.3391368","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734782&doi=10.1145%2f3379156.3391368&partnerID=40&md5=a0cdb6210711bebdc5ac60358ca9b701","Public University of Navarre, Pamplona, Spain; Trinity College Dublin, Dublin, Ireland","Garde, G., Public University of Navarre, Pamplona, Spain; Larumbe-Bergera, A., Public University of Navarre, Pamplona, Spain; Bossavit, B., Trinity College Dublin, Dublin, Ireland; Cabeza, R., Public University of Navarre, Pamplona, Spain; Porta, S., Public University of Navarre, Pamplona, Spain; Villanueva, A., Public University of Navarre, Pamplona, Spain","In this paper, we evaluate a synthetic framework to be used in the field of gaze estimation employing deep learning techniques. The lack of sufficient annotated data could be overcome by the utilization of a synthetic evaluation framework as far as it resembles the behavior of a real scenario. In this work, we use U2Eyes synthetic environment employing I2Head datataset as real benchmark for comparison based on alternative training and testing strategies. The results obtained show comparable average behavior between both frameworks although significantly more robust and stable performance is retrieved by the synthetic images. Additionally, the potential of synthetically pretrained models in order to be applied in user's specific calibration strategies is shown with outstanding performances. © 2020 ACM.","Datasets gaze estimation; Neural networks","Deep learning; Average behavior; Gaze estimation; Learning techniques; Stable performance; Synthetic environments; Synthetic evaluation; Synthetic images; Training and testing; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085734782
"Tavakoli H.R., Borji A., Kannala J., Rahtu E.","57201603431;23395793600;8928446400;6505786260;","Deep audio-visual saliency: Baseline model and data",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3","","",,2,"10.1145/3379156.3391337","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734752&doi=10.1145%2f3379156.3391337&partnerID=40&md5=c9d2323927a2f1a904c4593bfa2dcc92","Nokia Technologies, Finland; Aalto University, Finland; Tampere University, Finland","Tavakoli, H.R., Nokia Technologies, Finland; Borji, A.; Kannala, J., Aalto University, Finland; Rahtu, E., Tampere University, Finland","This paper introduces a conceptually simple and effective Deep Audio-Visual Embedding for dynamic saliency prediction dubbed ""DAVE"" in conjunction with our efforts towards building an Audio-Visual Eye-tracking corpus named ""AVE"". Despite existing a strong relation between auditory and visual cues for guiding gaze during perception, video saliency models only consider visual cues and neglect the auditory information that is ubiquitous in dynamic scenes. Here, we propose a baseline deep audio-visual saliency model for multi-modal saliency prediction in the wild. Thus the proposed model is intentionally designed to be simple. A video baseline model is also developed on the same architecture to assess effectiveness of the audio-visual models on a fair basis. We demonstrate that audio-visual saliency model outperforms the video saliency models. The data and code are available at https://hrtavakoli.github.io/AVE/and https://github.com/hrtavakoli/DAVE. © 2020 ACM.","Audio-Visual Saliency; Deep Learning; Dynamic Visual Attention","HTTP; Object recognition; Visualization; Baseline models; Dynamic scenes; Multi-modal; Video saliencies; Visual cues; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085734752
"Palmero Cantarino C., Komogortsev O.V., Talathi S.S.","57188829002;6506328653;57224997923;","Benefits of temporal information for appearance-based gaze estimation",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"42","","",,3,"10.1145/3379156.3391376","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734633&doi=10.1145%2f3379156.3391376&partnerID=40&md5=b0cfeaf0e12cbfba8fb2f1a968f6bbc0","Universitat de Barcelona, Barcelona, Spain; Computer Vision Center, Barcelona, Spain; Texas State University, Texas, United States; Facebook Reality Labs, Redmond, United States","Palmero Cantarino, C., Universitat de Barcelona, Barcelona, Spain, Computer Vision Center, Barcelona, Spain; Komogortsev, O.V., Texas State University, Texas, United States, Facebook Reality Labs, Redmond, United States; Talathi, S.S., Facebook Reality Labs, Redmond, United States","State-of-the-art appearance-based gaze estimation methods, usually based on deep learning techniques, mainly rely on static features. However, temporal trace of eye gaze contains useful information for estimating a given gaze point. For example, approaches leveraging sequential eye gaze information when applied to remote or low-resolution image scenarios with off-the-shelf cameras are showing promising results. The magnitude of contribution from temporal gaze trace is yet unclear for higher resolution/frame rate imaging systems, in which more detailed information about an eye is captured. In this paper, we investigate whether temporal sequences of eye images, captured using a high-resolution, high-frame rate head-mounted virtual reality system, can be leveraged to enhance the accuracy of an end-to-end appearance-based deep-learning model for gaze estimation. Performance is compared against a static-only version of the model. Results demonstrate statistically-significant benefits of temporal information, particularly for the vertical component of gaze. © 2020 ACM.","Convolutional neural networks; Eye movements; Fixation; Gaze dynamics; Gaze estimation; Recurrent neural networks; Saccade","Deep learning; Image enhancement; Image resolution; Learning systems; Virtual reality; Appearance based; Head mounted virtual reality; Higher resolution; Learning techniques; Low resolution images; Temporal information; Temporal sequences; Vertical component; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085734633
"Menges R., Kramer S., Hill S., Nisslmueller M., Kumar C., Staab S.","57192103839;57217013349;57217014415;57217015593;57192105487;7004053291;","A visualization tool for eye tracking data analysis in the web",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"48","","",,4,"10.1145/3379156.3391831","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734401&doi=10.1145%2f3379156.3391831&partnerID=40&md5=f9d3e0ffa5e67a3eebdb99ec0abdba8f","University of Koblenz, Germany; Universitat Stuttgart, Germany; University of Southampton, United Kingdom","Menges, R., University of Koblenz, Germany; Kramer, S., University of Koblenz, Germany; Hill, S., University of Koblenz, Germany; Nisslmueller, M., University of Koblenz, Germany; Kumar, C., University of Koblenz, Germany; Staab, S., Universitat Stuttgart, Germany, University of Southampton, United Kingdom","Usability analysis plays a significant role in optimizing Web interaction by understanding the behavior of end users. To support such analysis, we present a tool to visualize gaze and mouse data of Web site interactions. The proposed tool provides not only the traditional visualizations with fixations, scanpath, and heatmap, but allows for more detailed analysis with data clustering, demographic correlation, and advanced visualization like attention flow and 3D-scanpath. To demonstrate the usefulness of the proposed tool, we conducted a remote qualitative study with six analysts, using a dataset of 20 users browsing eleven real-world Web sites. © 2020 ACM.","Gaze visualization; Heatmap; Scanpath; Web interaction","Clustering algorithms; Data visualization; Three dimensional computer graphics; Visualization; Websites; Advanced visualizations; Data clustering; Qualitative study; Real-world web sites; Site interaction; Usability analysis; Visualization tools; Web interactions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085734401
"Pathmanathan N., Becher M., Rodrigues N., Reina G., Ertl T., Weiskopf D., Sedlmair M.","57217013171;56347887400;57195548048;23475376900;7004765655;6603960393;24802477200;","Eye vs. Head: Comparing gaze methods for interaction in augmented reality",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"46","","",,3,"10.1145/3379156.3391829","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085732220&doi=10.1145%2f3379156.3391829&partnerID=40&md5=a8bc4bec3c9cf8531deab65dbf3ccc78","University of Stuttgart, Germany","Pathmanathan, N., University of Stuttgart, Germany; Becher, M., University of Stuttgart, Germany; Rodrigues, N., University of Stuttgart, Germany; Reina, G., University of Stuttgart, Germany; Ertl, T., University of Stuttgart, Germany; Weiskopf, D., University of Stuttgart, Germany; Sedlmair, M., University of Stuttgart, Germany","Visualization in virtual 3D environments can provide a natural way for users to explore data. Often, arm and short head movements are required for interaction in augmented reality, which can be tiring and strenuous though. In an effort toward more user-friendly interaction, we developed a prototype that allows users to manipulate virtual objects using a combination of eye gaze and an external clicker device. Using this prototype, we performed a user study comparing four different input methods of which head gaze plus clicker was preferred by most participants. © 2020 ACM.","Augmented reality; Eye tracking; Immersive analytics; Interaction; Visualization","Augmented reality; Data visualization; Three dimensional computer graphics; Eye-gaze; Head movements; Input methods; User friendly; User study; Virtual 3d environments; Virtual objects; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085732220
"Mokatren M., Kuflik T., Shimshoni I.","57190072235;6602259371;7003816361;","EyeLinks: Methods to compute reliable stereo mappings used for eye gaze tracking",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"20","","",,,"10.1145/3379156.3391354","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085731939&doi=10.1145%2f3379156.3391354&partnerID=40&md5=d5142544329c1ee9c190a95d4024e581","University of Haifa, Mount Carmel, Haifa, 31905, Israel","Mokatren, M., University of Haifa, Mount Carmel, Haifa, 31905, Israel; Kuflik, T., University of Haifa, Mount Carmel, Haifa, 31905, Israel; Shimshoni, I., University of Haifa, Mount Carmel, Haifa, 31905, Israel","We present methods for extracting corneal images and estimating pupil centers continuously and reliably using head worn glasses that consists of two eye cameras. An existing CNN was modified for detecting pupils in IR and RGB images, and stereo vision together with 2D and 3D models are used. We confirm the feasibility of the proposed methods through user study results, which show that the methods can be used in future real gaze estimation systems. © 2020 ACM.","Corneal Imaging; Gaze Estimation; Mapping Transformation","Stereo image processing; Stereo vision; Corneal images; Eye camera; Eye gaze tracking; Gaze estimation; Pupil centers; RGB images; Stereo mapping; User study; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085731939
"Kumar A., Howlader P., Garcia R., Weiskopf D., Mueller K.","57202315472;57193579313;57202278157;6603960393;55366200700;","Challenges in interpretability of neural networks for eye movement data",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"27","","",,3,"10.1145/3379156.3391361","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085729949&doi=10.1145%2f3379156.3391361&partnerID=40&md5=d5d797bb9fd1f656d46e194b8c410e1f","Stony Brook University, United States; University of Stuttgart, Germany","Kumar, A., Stony Brook University, United States; Howlader, P., Stony Brook University, United States; Garcia, R., University of Stuttgart, Germany; Weiskopf, D., University of Stuttgart, Germany; Mueller, K., Stony Brook University, United States","Many applications in eye tracking have been increasingly employing neural networks to solve machine learning tasks. In general, neural networks have achieved impressive results in many problems over the past few years, but they still suffer from the lack of interpretability due to their black-box behavior. While previous research on explainable AI has been able to provide high levels of interpretability for models in image classification and natural language processing tasks, little effort has been put into interpreting and understanding networks trained with eye movement datasets. This paper discusses the importance of developing interpretability methods specifically for these models. We characterize the main problems for interpreting neural networks with this type of data, how they differ from the problems faced in other domains, and why existing techniques are not sufficient to address all of these issues. We present preliminary experiments showing the limitations that current techniques have and how we can improve upon them. Finally, based on the evaluation of our experiments, we suggest future research directions that might lead to more interpretable and explainable neural networks for eye tracking. © 2020 ACM.","Deep learning; Explainable AI; Eye tracking; Visualization","Classification (of information); Eye tracking; Learning algorithms; Natural language processing systems; Black boxes; Eye movement datum; Future research directions; Interpretability; NAtural language processing; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85085729949
"Öney S., Rodrigues N., Becher M., Ertl T., Reina G.","57217015408;57195548048;56347887400;7004765655;23475376900;","Evaluation of gaze depth estimation from eye tracking in augmented reality",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"51","","",,1,"10.1145/3379156.3391835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085729610&doi=10.1145%2f3379156.3391835&partnerID=40&md5=395744e8ef6f3b9d45affd3001994aa0","University of Stuttgart, Germany","Öney, S., University of Stuttgart, Germany; Rodrigues, N., University of Stuttgart, Germany; Becher, M., University of Stuttgart, Germany; Ertl, T., University of Stuttgart, Germany; Reina, G., University of Stuttgart, Germany","Gaze tracking in 3D has the potential to improve interaction with objects and visualizations in augmented reality. However, previous research showed that subjective perception of distance varies between real and virtual surroundings. We wanted to determine whether objectively measured 3D gaze depth through eye tracking also exhibits differences between entirely real and augmented environments. To this end, we conducted an experiment (N = 25) in which we used Microsoft HoloLens with a binocular eye tracking add-on from Pupil Labs. Participants performed a task that required them to look at stationary real and virtual objects while wearing a HoloLens device. We were not able to find significant differences in the gaze depth measured by eye tracking. Finally, we discuss our findings and their implications for gaze interaction in immersive analytics, and the quality of the collected gaze data. © 2020 ACM.","Augmented reality; Depth perception; Eye tracking; Immersive analytics; User study; Visualization","Augmented reality; Object tracking; Three dimensional computer graphics; Augmented environments; Depth Estimation; Gaze interaction; Gaze tracking; Immersive; MicroSoft; Subjective perceptions; Virtual objects; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085729610
"Fuhl W., Gao H., Kasneci E.","56770084800;57217014766;56059892600;","Neural networks for optical vector and eye ball parameter estimation",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"12","","",,8,"10.1145/3379156.3391346","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085729348&doi=10.1145%2f3379156.3391346&partnerID=40&md5=7629d12798b0b93d4210e1c06142a560","Human-Computer Interaction, University of Tübingen, Germany","Fuhl, W., Human-Computer Interaction, University of Tübingen, Germany; Gao, H., Human-Computer Interaction, University of Tübingen, Germany; Kasneci, E., Human-Computer Interaction, University of Tübingen, Germany","In this work we evaluate neural networks, support vector machines and decision trees for the regression of the center of the eyeball and the optical vector based on the pupil ellipse. In the evaluation we analyze single ellipses as well as window-based approaches as input. Comparisons are made regarding accuracy and runtime. The evaluation gives an overview of the general expected accuracy with different models and amounts of input ellipses. A simulator was implemented for the generation of the training and evaluation data. For a visual evaluation and to push the state of the art in optical vector estimation, the best model was applied to real data. This real data came from public data sets in which the ellipse is already annotated by an algorithm. The optical vectors on real data and the generator are made publicly available. Link to the generator and models. © 2020 ACM.","Data set; Eye tracking; Gaze vector estimation; Machine learning; Pupil ellipse generator; Runtime comparison","Decision trees; Eye tracking; Geometry; Petroleum reservoir evaluation; Support vector machines; Vectors; Best model; Eye balls; Public data; Runtimes; State of the art; Vector estimation; Visual evaluation; Window-based; Support vector regression",Conference Paper,"Final","",Scopus,2-s2.0-85085729348
"Hausamann P., Sinnott C., MacNeilage P.R.","57210106409;57212193834;10240387800;","Positional head-eye tracking outside the lab: An open-source solution",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"31","","",,6,"10.1145/3379156.3391365","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085728310&doi=10.1145%2f3379156.3391365&partnerID=40&md5=065cb127b5c7eaa8b042ba61659786cd","Technical University of Munich, Munich, Germany; University of Nevada-Reno, Reno, Nevada, United States","Hausamann, P., Technical University of Munich, Munich, Germany; Sinnott, C., University of Nevada-Reno, Reno, Nevada, United States; MacNeilage, P.R., University of Nevada-Reno, Reno, Nevada, United States","Simultaneous head and eye tracking has traditionally been confined to a laboratory setting and real-world motion tracking limited to measuring linear acceleration and angular velocity. Recently available mobile devices such as the Pupil Core eye tracker and the Intel RealSense T265 motion tracker promise to deliver accurate measurements outside the lab. Here, the researchers propose a hard-and software framework that combines both devices into a robust, usable, low-cost head and eye tracking system. The developed software is open source and the required hardware modifications can be 3D printed. The researchers demonstrate the system's ability to measure head and eye movements in two tasks: an eyes-fixed head rotation task eliciting the vestibulo-ocular reflex inside the laboratory, and a natural locomotion task where a subject walks around a building outside of the laboratory. The resultant head and eye movements are discussed, as well as future implementations of this system. © 2020 ACM.","Eye tracking; Gaze estimation; Head tracking; Locomotion; Mobile; Open source; Simultaneous localization and mapping","3D printers; Computer programming; Eye movements; Laboratories; Motion tracking; Open source software; Open systems; Accurate measurement; Eye tracking systems; Hardware modifications; Linear accelerations; Natural locomotions; Open-source solutions; Software frameworks; Vestibulo-ocular reflex; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085728310
"Streichert A., Angerbauer K., Schwarzl M., Sedlmair M.","57217014908;57056390800;57211934172;24802477200;","Comparing input modalities for shape drawing tasks",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"47","","",,,"10.1145/3379156.3391830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085727625&doi=10.1145%2f3379156.3391830&partnerID=40&md5=290bb3102878b09a373418d005f681a3","VISUS, Universitat Stuttgart, Stuttgart, Germany","Streichert, A., VISUS, Universitat Stuttgart, Stuttgart, Germany; Angerbauer, K., VISUS, Universitat Stuttgart, Stuttgart, Germany; Schwarzl, M., VISUS, Universitat Stuttgart, Stuttgart, Germany; Sedlmair, M., VISUS, Universitat Stuttgart, Stuttgart, Germany","With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N=20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods. © 2020 ACM.","Immersive analytics; Input modalities; Interaction","Motion tracking; User experience; Data glove; Hand motion tracking; Head tracking; Immersive; Input methods; Input modalities; Touch inputs; User study; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085727625
"Bozkir E., Ünal A.B., Akgün M., Kasneci E., Pfeifer N.","57210111357;57212002288;26021030000;56059892600;23670837500;","Privacy preserving gaze estimation using synthetic images via a randomized encoding based framework",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"30","","",,3,"10.1145/3379156.3391364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085727385&doi=10.1145%2f3379156.3391364&partnerID=40&md5=a7d766e9c0c8050dffac4c811e69ef98","Human-Computer Interaction, University of Tübingen, Germany; Methods in Medical Informatics, Translational Bioinformatics, University of Tübingen, Germany; Statistical Learning in Computational Biology, Max Planck Institute for Informatics, Saarbrücken, Germany","Bozkir, E., Human-Computer Interaction, University of Tübingen, Germany; Ünal, A.B., Methods in Medical Informatics, Translational Bioinformatics, University of Tübingen, Germany; Akgün, M., Methods in Medical Informatics, Translational Bioinformatics, University of Tübingen, Germany; Kasneci, E., Human-Computer Interaction, University of Tübingen, Germany; Pfeifer, N., Methods in Medical Informatics, Translational Bioinformatics, University of Tübingen, Germany, Statistical Learning in Computational Biology, Max Planck Institute for Informatics, Saarbrücken, Germany","Eye tracking is handled as one of the key technologies for applications that assess and evaluate human attention, behavior, and biometrics, especially using gaze, pupillary, and blink behaviors. One of the challenges with regard to the social acceptance of eye tracking technology is however the preserving of sensitive and personal information. To tackle this challenge, we employ a privacy-preserving framework based on randomized encoding to train a Support Vector Regression model using synthetic eye images privately to estimate the human gaze. During the computation, none of the parties learn about the data or the result that any other party has. Furthermore, the party that trains the model cannot reconstruct pupil, blinks or visual scanpath. The experimental results show that our privacy-preserving framework is capable of working in real-time, with the same accuracy as compared to non-private version and could be extended to other eye tracking related problems. © 2020 ACM.","Eye tracking; Gaze estimation; Human computer interaction; Privacy preserving machine learning; Randomized encoding","Behavioral research; Encoding (symbols); Image coding; Signal encoding; Support vector regression; Eye tracking technologies; Gaze estimation; Key technologies; Personal information; Privacy preserving; Social acceptance; Support vector regression models; Synthetic images; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085727385
"Ahn S., Kelton C., Balasubramanian A., Zelinsky G.","57210117778;57195739976;23048825000;6701733486;","Towards predicting reading comprehension from gaze behavior",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"1","","",,1,"10.1145/3379156.3391335","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085726293&doi=10.1145%2f3379156.3391335&partnerID=40&md5=1dc80daae3d693082eaeac0ab6f128c7","Stony Brook University, Stony Brook, NY, United States","Ahn, S., Stony Brook University, Stony Brook, NY, United States; Kelton, C., Stony Brook University, Stony Brook, NY, United States; Balasubramanian, A., Stony Brook University, Stony Brook, NY, United States; Zelinsky, G., Stony Brook University, Stony Brook, NY, United States","As readers of a language, we all agree to move our eyes in roughly the same way. Yet might there be hidden within this self-similar behavior subtle clues as to how a reader is understanding the material being read? Here we attempt to decode a reader's eye movements to predict their level of text comprehension and related states. Eye movements were recorded from 95 people reading 4 published SAT passages, each followed by corresponding SAT questions and self-evaluation questionnaires. A sequence of 21 fixation-location (x,y), fixation-duration, and pupil-size features were extracted from the reading behavior and input to two deep networks (CNN/RNN), which were used to predict the reader's comprehension level and other comprehension-related variables. The best overall comprehension prediction accuracy was 65% (cf. null accuracy = 54%) obtained by CNN. This prediction generalized well to fixations on new passages (64%) from the same readers, but did not generalize to fixations from new readers (41%), implying substantial individual differences in reading behavior. Our work is the first attempt to predict comprehension from fixations using deep networks, where we hope that our large reading dataset and our protocol for evaluation will benefit the development of new methods for predicting reading comprehension by decoding gaze behavior. © 2020 ACM.","Eye tracking; Machine learning; Reading dataset; Text comprehension prediction","Decoding; Eye movements; Forecasting; Large dataset; Surveys; Fixation duration; Gaze behavior; Individual Differences; Prediction accuracy; Reading comprehension; Related variables; Self evaluation; Text comprehensions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085726293
"Taha Ahmed Z.A., Jadhav M.E.","57221609216;57201156883;","A Review of Early Detection of Autism Based on Eye-Tracking and Sensing Technology",2020,"Proceedings of the 5th International Conference on Inventive Computation Technologies, ICICT 2020",,,"9112493","160","166",,1,"10.1109/ICICT48043.2020.9112493","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084503207&doi=10.1109%2fICICT48043.2020.9112493&partnerID=40&md5=09b271cb8a2e5f46b60ea9f4e61d46da","Dr. Babasaheb Ambedkar Marathwada University, Department of Computer Science, Aurangabad, India","Taha Ahmed, Z.A., Dr. Babasaheb Ambedkar Marathwada University, Department of Computer Science, Aurangabad, India; Jadhav, M.E., Dr. Babasaheb Ambedkar Marathwada University, Department of Computer Science, Aurangabad, India","The current paper is a review of eye-tracking and sensing technologies that detect and monitor Autism Spectrum Disorder (ASD). Nowadays, the biggest challenge is the detection of autism before the age of 36 months. The diagnosis of autism in the early stage of life can help autistic children improve their social communication and quality of life. Therefore, the technology can support psychologists to get the right diagnoses of autism and accordingly the autistic children can get appropriate treatment for their condition. In this review, the focus is on eyetracking and sensing technologies. The autistic children have different attentional biases in social interactions that can be measured by eye-tracking technology. Moreover, the autistic children have some signs that can be easily detected by using the sensing technology such as hand flapping, body rocking and motion trackers. © 2020 IEEE.","Autism Spectrum Disorder (ASD); Eye Tracking; Hand Flapping; Machine Learning; Motion Trackers; Sensing Technology; Typical Developing (TD)","Diseases; Autism spectrum disorders; Autistic children; Eye tracking technologies; Motion tracker; Quality of life; Sensing technology; Social communications; Social interactions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85084503207
"Saddler J.A., Peterson C.S., Sama S., Nagaraj S., Baysal O., Guerrouj L., Sharif B.","57192652492;57209304815;57216459425;57211411603;21742200600;36703012700;22235542400;","Studying Developer Reading Behavior on Stack Overflow during API Summarization Tasks",2020,"SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",,,"9054848","195","205",,1,"10.1109/SANER48275.2020.9054848","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083567063&doi=10.1109%2fSANER48275.2020.9054848&partnerID=40&md5=8df7f829252cc8a597bac60af7e3cb81","University of Nebraska - Lincoln, Lincoln, NE  68588, United States; Youngstown State University, Youngstown, OH  44555, United States; Carleton University, Ottawa, ON, Canada; École de Technologie Supérieure (ETS), Montréal, QC, Canada","Saddler, J.A., University of Nebraska - Lincoln, Lincoln, NE  68588, United States; Peterson, C.S., University of Nebraska - Lincoln, Lincoln, NE  68588, United States; Sama, S., Youngstown State University, Youngstown, OH  44555, United States; Nagaraj, S., Carleton University, Ottawa, ON, Canada; Baysal, O., Carleton University, Ottawa, ON, Canada; Guerrouj, L., École de Technologie Supérieure (ETS), Montréal, QC, Canada; Sharif, B., University of Nebraska - Lincoln, Lincoln, NE  68588, United States","Stack Overflow is commonly used by software developers to help solve problems they face while working on software tasks such as fixing bugs or building new features. Recent research has explored how the content of Stack Overflow posts affects attraction and how the reputation of users attracts more visitors. However, there is very little evidence on the effect that visual attractors and content quantity have on directing gaze toward parts of a post, and which parts hold the attention of a user longer. Moreover, little is known about how these attractors help developers (students and professionals) answer comprehension questions. This paper presents an eye tracking study on thirty developers constrained to reading only Stack Overflow posts while summarizing four open source methods or classes. Results indicate that on average paragraphs and code snippets were fixated upon most often and longest. When ranking pages by number of appearance of code blocks and paragraphs, we found that while the presence of more code blocks did not affect number of fixations, the presence of increasing numbers of plain text paragraphs significantly drove down the fixations on comments. SO posts that were looked at only by students had longer fixation times on code elements within the first ten fixations. We found that 16 developer summaries contained 5 or more meaningful terms from SO posts they viewed. We discuss how our observations of reading behavior could benefit how users structure their posts. © 2020 IEEE.","API summarization; controlled experiment; eye tracking; reading behavior; Stack Overflow","Dynamical systems; Eye tracking; Program debugging; Reengineering; Code blocks; Eye-tracking studies; Number of fixations; Open sources; Recent researches; Software developer; Software tasks; Stack overflow; Open source software",Conference Paper,"Final","",Scopus,2-s2.0-85083567063
"Sulikowski P., Zdziebko T.","55878976900;56565572400;","Deep learning-enhanced framework for performance evaluation of a recommending interface with varied recommendation position and intensity based on eye-tracking equipment data processing",2020,"Electronics (Switzerland)","9","2","266","","",,24,"10.3390/electronics9020266","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079200995&doi=10.3390%2felectronics9020266&partnerID=40&md5=143c8f845cc779023b6f717c5babb8ec","Faculty of Information Technology and Computer Science, West Pomeranian University of Technology, ul. Zolnierska 49, Szczecin, 71-210, Poland; Faculty of Economics, Finance and Management, University of Szczecin, ul. Mickiewicza 64, Szczecin, 71-101, Poland","Sulikowski, P., Faculty of Information Technology and Computer Science, West Pomeranian University of Technology, ul. Zolnierska 49, Szczecin, 71-210, Poland; Zdziebko, T., Faculty of Economics, Finance and Management, University of Szczecin, ul. Mickiewicza 64, Szczecin, 71-101, Poland","The increasing amount of marketing content in e-commerce websites results in the limited attention of users. For recommender systems, the way recommended items are presented becomes as important as the underlying algorithms for product selection. In order to improve the effectiveness of content presentation, marketing experts experiment with the layout and other visual aspects of website elements to find the most suitable solution. This study investigates those aspects for a recommending interface. We propose a framework for performance evaluation of a recommending interface, which takes into consideration individual user characteristics and goals. At the heart of the proposed solution is a deep neutral network trained to predict the efficiency a particular recommendation presented in a selected position and with a chosen degree of intensity. The proposed Performance Evaluation of a Recommending Interface (PERI) framework can be used to automate an optimal recommending interface adjustment according to the characteristics of the user and their goals. The experimental results from the study are based on research-grade measurement electronics equipment Gazepoint GP3 eye-tracker data, together with synthetic data that were used to perform pre-assessment training of the neural network. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Eye-tracking device; Human computer interaction; Recommender system",,Article,"Final","",Scopus,2-s2.0-85079200995
"Li L., Xu M., Liu H., Li Y., Wang X., Jiang L., Wang Z., Fan X., Wang N.","57210991974;55703599800;57202104848;56075073900;36769701900;57188642646;24170127500;36463269800;7404340277;","A Large-Scale Database and a CNN Model for Attention-Based Glaucoma Detection",2020,"IEEE Transactions on Medical Imaging","39","2","8756196","413","424",,37,"10.1109/TMI.2019.2927226","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078845889&doi=10.1109%2fTMI.2019.2927226&partnerID=40&md5=7bad9d182853c399fdcd6edc222633a3","School of Electronic and Information Engineering, Beihang University, Beijing, China; Beijing Institute of Ophthalmology, Beijing, China; School of Automation Sciences and Electrical Engineering, Beihang University, Beijing, China; Department of Ophthalmology, Peking University Third Hospital, Beijing, China","Li, L., School of Electronic and Information Engineering, Beihang University, Beijing, China; Xu, M., School of Electronic and Information Engineering, Beihang University, Beijing, China; Liu, H., Beijing Institute of Ophthalmology, Beijing, China; Li, Y., Beijing Institute of Ophthalmology, Beijing, China; Wang, X., School of Automation Sciences and Electrical Engineering, Beihang University, Beijing, China; Jiang, L., School of Electronic and Information Engineering, Beihang University, Beijing, China; Wang, Z., School of Electronic and Information Engineering, Beihang University, Beijing, China; Fan, X., Department of Ophthalmology, Peking University Third Hospital, Beijing, China; Wang, N., Beijing Institute of Ophthalmology, Beijing, China","Glaucoma is one of the leading causes of irreversible vision loss. Many approaches have recently been proposed for automatic glaucoma detection based on fundus images. However, none of the existing approaches can efficiently remove high redundancy in fundus images for glaucoma detection, which may reduce the reliability and accuracy of glaucoma detection. To avoid this disadvantage, this paper proposes an attention-based convolutional neural network (CNN) for glaucoma detection, called AG-CNN. Specifically, we first establish a large-scale attention-based glaucoma (LAG) database, which includes 11 760 fundus images labeled as either positive glaucoma (4878) or negative glaucoma (6882). Among the 11 760 fundus images, the attention maps of 5824 images are further obtained from ophthalmologists through a simulated eye-tracking experiment. Then, a new structure of AG-CNN is designed, including an attention prediction subnet, a pathological area localization subnet, and a glaucoma classification subnet. The attention maps are predicted in the attention prediction subnet to highlight the salient regions for glaucoma detection, under a weakly supervised training manner. In contrast to other attention-based CNN methods, the features are also visualized as the localized pathological area, which are further added in our AG-CNN structure to enhance the glaucoma detection performance. Finally, the experiment results from testing over our LAG database and another public glaucoma database show that the proposed AG-CNN approach significantly advances the state-of-the-art in glaucoma detection. © 1982-2012 IEEE.","attention mechanism; Glaucoma detection; pathological area detection; weakly supervised","Convolutional neural networks; Database systems; Eye tracking; Redundancy; Area detection; Attention mechanisms; Glaucoma detection; High redundancy; Large-scale database; State of the art; weakly supervised; Weakly supervised trainings; Ophthalmology; adult; Article; controlled study; convolutional neural network; data analysis; data base; deep learning; deep neural network; disease classification; eye fundus; eye tracking; female; glaucoma; human; large scale production; major clinical study; male; middle aged; ophthalmologist; receiver operating characteristic; retinal nerve fiber layer thickness; sensitivity and specificity; support vector machine; aged; computer assisted diagnosis; diagnostic imaging; factual database; glaucoma; procedures; supervised machine learning; Adult; Aged; Databases, Factual; Female; Glaucoma; Humans; Image Interpretation, Computer-Assisted; Male; Middle Aged; Neural Networks, Computer; ROC Curve; Supervised Machine Learning",Article,"Final","",Scopus,2-s2.0-85078845889
"Dunnhofer M., Antico M., Sasazawa F., Takeda Y., Camps S., Martinel N., Micheloni C., Carneiro G., Fontanarosa D.","57200015315;57207455135;55936011600;57213065402;57188671461;52463990900;6507976201;23003641100;17433992600;","Siam-U-Net: encoder-decoder siamese network for knee cartilage tracking in ultrasound images",2020,"Medical Image Analysis","60",,"101631","","",,16,"10.1016/j.media.2019.101631","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077514053&doi=10.1016%2fj.media.2019.101631&partnerID=40&md5=f8be56c95b3386729cf6213e71ec1674","Machine Learning and Perception Lab, Department of Mathematics, Computer Science and Physics, University of Udine, via delle Scienze 206, Udine, 33100, Italy; School of Chemistry, Physics and Mechanical Engineering, Queensland University of Technology, Brisbane, Queensland, Australia; Institute of Health Biomedical Innovation, Queensland University of Technology, Brisbane, Queensland, Australia; School of Clinical Sciences, Queensland University of Technology, Brisbane, Queensland, Australia; Department of Orthopaedic Surgery, Faculty of Medicine and Graduate School of Medicine, Hokkaido University, Sapporo, Japan; Department of Orthopaedic Surgery, Hyogo College of Medicine, Nishinomiya, Hyogo, Japan; Faculty of Electrical Engineering, Eindhoven University of Technology, Eindhoven, Netherlands; Oncology Solutions Department, Philips Research, Eindhoven, Netherlands; Australian Institute for Machine Learning, School of Computer Science, The University of Adelaide, Adelaide, Australia","Dunnhofer, M., Machine Learning and Perception Lab, Department of Mathematics, Computer Science and Physics, University of Udine, via delle Scienze 206, Udine, 33100, Italy; Antico, M., School of Chemistry, Physics and Mechanical Engineering, Queensland University of Technology, Brisbane, Queensland, Australia, Institute of Health Biomedical Innovation, Queensland University of Technology, Brisbane, Queensland, Australia; Sasazawa, F., School of Chemistry, Physics and Mechanical Engineering, Queensland University of Technology, Brisbane, Queensland, Australia, Institute of Health Biomedical Innovation, Queensland University of Technology, Brisbane, Queensland, Australia, Department of Orthopaedic Surgery, Faculty of Medicine and Graduate School of Medicine, Hokkaido University, Sapporo, Japan; Takeda, Y., Department of Orthopaedic Surgery, Hyogo College of Medicine, Nishinomiya, Hyogo, Japan; Camps, S., Faculty of Electrical Engineering, Eindhoven University of Technology, Eindhoven, Netherlands, Oncology Solutions Department, Philips Research, Eindhoven, Netherlands; Martinel, N., Machine Learning and Perception Lab, Department of Mathematics, Computer Science and Physics, University of Udine, via delle Scienze 206, Udine, 33100, Italy; Micheloni, C., Machine Learning and Perception Lab, Department of Mathematics, Computer Science and Physics, University of Udine, via delle Scienze 206, Udine, 33100, Italy; Carneiro, G., Australian Institute for Machine Learning, School of Computer Science, The University of Adelaide, Adelaide, Australia; Fontanarosa, D., Institute of Health Biomedical Innovation, Queensland University of Technology, Brisbane, Queensland, Australia, School of Clinical Sciences, Queensland University of Technology, Brisbane, Queensland, Australia","The tracking of the knee femoral condyle cartilage during ultrasound-guided minimally invasive procedures is important to avoid damaging this structure during such interventions. In this study, we propose a new deep learning method to track, accurately and efficiently, the femoral condyle cartilage in ultrasound sequences, which were acquired under several clinical conditions, mimicking realistic surgical setups. Our solution, that we name Siam-U-Net, requires minimal user initialization and combines a deep learning segmentation method with a siamese framework for tracking the cartilage in temporal and spatio-temporal sequences of 2D ultrasound images. Through extensive performance validation given by the Dice Similarity Coefficient, we demonstrate that our algorithm is able to track the femoral condyle cartilage with an accuracy which is comparable to experienced surgeons. It is additionally shown that the proposed method outperforms state-of-the-art segmentation models and trackers in the localization of the cartilage. We claim that the proposed solution has the potential for ultrasound guidance in minimally invasive knee procedures. © 2019","Deep learning; Fully convolutional siamese networks; Knee arthroscopy; Knee cartilage; Ultrasound; Ultrasound guidance; Visual tracking","Cartilage; Image segmentation; Network coding; Ultrasonic equipment; Ultrasonics; Clinical conditions; Knee arthroscopy; Performance validation; Segmentation methods; Segmentation models; Similarity coefficients; Ultrasound guidance; Visual Tracking; Deep learning; algorithm; article; deep learning; echography; eye tracking; femoral condyle; human; knee arthroscopy; knee meniscus; surgeon; arthroscopy; articular cartilage; diagnostic imaging; female; image processing; interventional ultrasonography; knee; male; normal human; procedures; three-dimensional imaging; Arthroscopy; Cartilage, Articular; Deep Learning; Female; Healthy Volunteers; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Knee Joint; Male; Neural Networks, Computer; Ultrasonography, Interventional",Article,"Final","",Scopus,2-s2.0-85077514053
"Li G., Patel N.A., Hagemeister J., Yan J., Wu D., Sharma K., Cleary K., Iordachita I.","55755433900;57037850200;57211576141;57209185853;57209025732;57195323506;26643081000;11142173400;","Body-mounted robotic assistant for MRI-guided low back pain injection",2020,"International Journal of Computer Assisted Radiology and Surgery","15","2",,"321","331",,5,"10.1007/s11548-019-02080-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074534586&doi=10.1007%2fs11548-019-02080-3&partnerID=40&md5=2a709e18ed01f0c29029cb27d960860a","Laboratory for Computational Sensing and Robotics (LCSR), Johns Hopkins University, Baltimore, MD, United States; Sheikh Zayed Institute for Pediatric Surgical Innovation, Children’s National Health System, Washington, DC, United States","Li, G., Laboratory for Computational Sensing and Robotics (LCSR), Johns Hopkins University, Baltimore, MD, United States; Patel, N.A., Laboratory for Computational Sensing and Robotics (LCSR), Johns Hopkins University, Baltimore, MD, United States; Hagemeister, J., Laboratory for Computational Sensing and Robotics (LCSR), Johns Hopkins University, Baltimore, MD, United States; Yan, J., Laboratory for Computational Sensing and Robotics (LCSR), Johns Hopkins University, Baltimore, MD, United States; Wu, D., Laboratory for Computational Sensing and Robotics (LCSR), Johns Hopkins University, Baltimore, MD, United States; Sharma, K., Sheikh Zayed Institute for Pediatric Surgical Innovation, Children’s National Health System, Washington, DC, United States; Cleary, K., Sheikh Zayed Institute for Pediatric Surgical Innovation, Children’s National Health System, Washington, DC, United States; Iordachita, I., Laboratory for Computational Sensing and Robotics (LCSR), Johns Hopkins University, Baltimore, MD, United States","Purpose: This paper presents the development of a body-mounted robotic assistant for magnetic resonance imaging (MRI)-guided low back pain injection. Our goal was to eliminate the radiation exposure of traditional X-ray guided procedures while enabling the exquisite image quality available under MRI. The robot is designed with a compact and lightweight profile that can be mounted directly on the patient’s lower back via straps, thus minimizing the effect of patient motion by moving along with the patient. The robot was built with MR-conditional materials and actuated with piezoelectric motors so it can operate inside the MRI scanner bore during imaging and therefore streamline the clinical workflow by utilizing intraoperative MR images. Methods: The robot is designed with a four degrees of freedom parallel mechanism, stacking two identical Cartesian stages, to align the needle under intraoperative MRI-guidance. The system targeting accuracy was first evaluated in free space with an optical tracking system, and further assessed with a phantom study under live MRI-guidance. Qualitative imaging quality evaluation was performed on a human volunteer to assess the image quality degradation caused by the robotic assistant. Results: Free space positioning accuracy study demonstrated that the mean error of the tip position to be 0.51 ± 0.27 mm and needle angle to be 0. 70 ∘± 0. 38 ∘. MRI-guided phantom study indicated the mean errors of the target to be 1.70 ± 0.21 mm, entry point to be 1.53 ± 0.19 mm, and needle angle to be 0. 66 ∘± 0. 43 ∘. Qualitative imaging quality evaluation validated that the image degradation caused by the robotic assistant in the lumbar spine anatomy is negligible. Conclusions: The study demonstrates that the proposed body-mounted robotic system is able to perform MRI-guided low back injection in a phantom study with sufficient accuracy and with minimal visible image degradation that should not affect the procedure. © 2019, CARS.","Body-mounted robot; Image-guided therapy; MRI-guided robot; Pain injection","plastic; polyvinylchloride; Article; body mounted robot assisted surgery; body mounted robotics; communication protocol; degradation; eye tracking; human; image quality; image registration; injection pain; interventional magnetic resonance imaging; intraspinal drug administration; kinematics; low back pain; nuclear magnetic resonance imaging guided injection; parallel design; priority journal; qualitative analysis; robotics; signal noise ratio; diagnostic imaging; imaging phantom; low back pain; nuclear magnetic resonance imaging; procedures; robot assisted surgery; workflow; Humans; Low Back Pain; Magnetic Resonance Imaging; Phantoms, Imaging; Robotic Surgical Procedures; Workflow",Article,"Final","",Scopus,2-s2.0-85074534586
"Ewaisha M., El Shawarby M., Abbas H., Sobh I.","57219710517;57219717150;7102564939;57209617737;","End-to-end multitask learning for driver gaze and head pose estimation",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","16","110","","",,,"10.2352/ISSN.2470-1173.2020.16.AVM-110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094927422&doi=10.2352%2fISSN.2470-1173.2020.16.AVM-110&partnerID=40&md5=fc4633ae0dae18af8faad4d2e1891e2e","Valeo Group, Cairo, Egypt; Ain Shams University, Cairo, Egypt","Ewaisha, M., Valeo Group, Cairo, Egypt; El Shawarby, M.; Abbas, H., Ain Shams University, Cairo, Egypt; Sobh, I., Valeo Group, Cairo, Egypt","Modern automobiles accidents occur mostly due to inattentive behavior of drivers, which is why driver's gaze estimation is becoming a critical component in automotive industry. Gaze estimation has introduced many challenges due to the nature of the surrounding environment like changes in illumination, or driver's head motion, partial face occlusion, or wearing eye decorations. Previous work conducted in this field includes explicit extraction of hand-crafted features such as eye corners and pupil center to be used to estimate gaze, or appearance-based methods like Convolutional Neural Networks which implicitly extracts features from an image and directly map it to the corresponding gaze angle. In this work, a multitask Convolutional Neural Network architecture is proposed to predict subject's gaze yaw and pitch angles, along with the head pose as an auxiliary task, making the model robust to head pose variations, without needing any complex preprocessing or hand-crafted feature extraction.Then the network's output is clustered into nine gaze classes relevant in the driving scenario. The model achieves 95.8% accuracy on the test set and 78.2% accuracy in cross-subject testing, proving the model's generalization capability and robustness to head pose variation. © 2020, Society for Imaging Science and Technology.",,"Accidents; Convolution; Multi-task learning; Network architecture; Appearance-based methods; Critical component; Gaze estimation; Generalization capability; Head Pose Estimation; Partial faces; Pupil centers; Surrounding environment; Convolutional neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85094927422
"Blakey W.A., Katsigiannis S., Hajimirza N., Ramzan N.","57219709782;36632225900;56928331400;16069861100;","Defining gaze tracking metrics by observing a growing divide between 2D and 3D tracking",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","11","129","","",,,"10.2352/ISSN.2470-1173.2020.11.HVEI-129","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094897163&doi=10.2352%2fISSN.2470-1173.2020.11.HVEI-129&partnerID=40&md5=b451ceb88e762d36f40a3731b69f043d","School of Computing, Engineering and Physical Sciences, University of the West of Scotland, Paisley, United Kingdom; Lumen Research Ltd, London, United Kingdom","Blakey, W.A., School of Computing, Engineering and Physical Sciences, University of the West of Scotland, Paisley, United Kingdom, Lumen Research Ltd, London, United Kingdom; Katsigiannis, S., School of Computing, Engineering and Physical Sciences, University of the West of Scotland, Paisley, United Kingdom; Hajimirza, N., Lumen Research Ltd, London, United Kingdom; Ramzan, N., School of Computing, Engineering and Physical Sciences, University of the West of Scotland, Paisley, United Kingdom","This work examines the different terminology used for defining gaze tracking technology and explores the different methodologies used for describing their respective accuracy. Through a comparative study of different gaze tracking technologies, such as infrared and webcam-based, and utilising a variety of accuracy metrics, this work shows how the reported accuracy can be misleading. The lack of intersection points between the gaze vectors of different eyes (also known as convergence points) in definitions has a huge impact on accuracy measures and directly impacts the robustness of any accuracy measuring methodology. Different accuracy metrics and tracking definitions have been collected and tabulated to more formally demonstrate the divide in definitions. © 2020, Society for Imaging Science and Technology",,"3D tracking; Accuracy measures; Comparative studies; Convergence points; Gaze tracking; Intersection points; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85094897163
"Tanaka M., Lanaro M.P., Horiuchi T., Rizzi A.","55502310300;57195470572;55420359700;56962787100;","Random spray retinex extensions considering region of interest and eyemovements",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","14","060403","","",,,"10.2352/J.ImagingSci.Technol.2019.63.6.060403","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094870022&doi=10.2352%2fJ.ImagingSci.Technol.2019.63.6.060403&partnerID=40&md5=174cc49ece3443ddac4ce8091d7eaa7f","College of Liberal Arts and Sciences, Chiba University, Chiba, Japan; Department of Computer Science, University of Milano, Milano, Italy; Graduate School of Engineering, Chiba University, Chiba, Japan","Tanaka, M., College of Liberal Arts and Sciences, Chiba University, Chiba, Japan; Lanaro, M.P., Department of Computer Science, University of Milano, Milano, Italy; Horiuchi, T., Graduate School of Engineering, Chiba University, Chiba, Japan; Rizzi, A., Department of Computer Science, University of Milano, Milano, Italy","The Random spray Retinex (RSR) algorithm was developed by taking into consideration the mathematical description of Milano-Retinex. The RSR substituted random paths with random sprays. Mimicking some characteristics of the human visual system (HVS), this article proposes two variants of RSR adding a mechanism of region of interest (ROI). In the first proposed model, a cone distribution based on anatomical data is considered as ROI. In the second model, the visual resolution depending on the visual field based on the knowledge of visual information processing is considered as ROI. We have measured actual eye movements using an eye-tracking system. By using the eye-tracking data, we have simulated the HVS using test images. Results show an interesting qualitative computation of the appearance of the processed area around real gaze points. © 2019 Society for Imaging Science and Technology.",,"Eye movements; Image segmentation; Spectral resolution; Eye tracking systems; Human visual systems; Mathematical descriptions; Random paths; Region of interest; Visual fields; Visual information processing; Visual resolutions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85094870022
"Jogeshwar A.K., Diaz G.J., Farnand S.P., Pelz J.B.","57193727303;55436582600;6506684949;7007018556;","The cone model: Recognizing gaze uncertainty in virtual environments",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","9","288","","",,1,"10.2352/ISSN.2470-1173.2020.9.IQSP-288","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086303720&doi=10.2352%2fISSN.2470-1173.2020.9.IQSP-288&partnerID=40&md5=332fa176cecc51d879bb0e18d17bbc05","Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY  14623, United States; Program of Color Science, Rochester Institute of Technology, Rochester, NY  14623, United States","Jogeshwar, A.K., Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY  14623, United States; Diaz, G.J., Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY  14623, United States; Farnand, S.P., Program of Color Science, Rochester Institute of Technology, Rochester, NY  14623, United States; Pelz, J.B., Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY  14623, United States","Eye tracking is used by psychologists, neurologists, vision researchers, and many others to understand the nuances of the human visual system, and to provide insight into a person's allocation of attention across the visual environment. When tracking the gaze behavior of an observer immersed in a virtual environment displayed on a head-mounted display, estimated gaze direction is encoded as a three-dimensional vector extending from the estimated location of the eyes into the 3D virtual environment. Additional computation is required to detect the target object at which gaze was directed. These methods must be robust to calibration error or eye tracker noise, which may cause the gaze vector to miss the target object and hit an incorrect object at a different distance. Thus, the straightforward solution involving a single vector-to-object collision could be inaccurate in indicating object gaze. More involved metrics that rely upon an estimation of the angular distance from the ray to the center of the object must account for an object's angular size based on distance, or irregularly shaped edges - information that is not made readily available by popular game engines (e.g. Unity © 2020 Society for Imaging Science and Technology.",,"Helmet mounted displays; Object detection; Object tracking; 3-D virtual environment; Allocation of attentions; Calibration error; Head mounted displays; Human Visual System; Object collision; Three-dimensional vectors; Visual environments; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086303720
"Fahim Shahriar A.B.M., Moon M.Z., Mahmud H., Hasan K.","57216082098;57216081044;35767973600;57203437844;","Online product recommendation system by using eye gaze data",2020,"ACM International Conference Proceeding Series",,,,"","",,1,"10.1145/3377049.3377108","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082560100&doi=10.1145%2f3377049.3377108&partnerID=40&md5=7501241f6ce62280e7fa34cfc7dae484","Islamic University of Technology, Dhaka, Bangladesh","Fahim Shahriar, A.B.M., Islamic University of Technology, Dhaka, Bangladesh; Moon, M.Z., Islamic University of Technology, Dhaka, Bangladesh; Mahmud, H., Islamic University of Technology, Dhaka, Bangladesh; Hasan, K., Islamic University of Technology, Dhaka, Bangladesh","Recommendation system takes information related to the users’ habits or interest or profile to suggest users with more convenient or similar materials that the users might be interested in. In general, these systems mostly rely on explicit feedback techniques (rating, search history etc.) to recommend products. In this case, users need to interact directly with the system. We seek implicit methods (indirect interaction) to relate users’ preferences and recommend desired products automatically on the interface in order to minimize the meddling interaction and workload. In this paper, we present a recommendation system that will use users’ eye gaze data to apprehend their interest to recommend products as an implicit feedback technique. Eye gaze data can provide information about the products that the users are interested in, without any direct interaction with the system. Eye gaze features is very effective as an implicit interaction technique. So, integrating eye gaze features with recommendation system will generate more user-oriented results. This system will collect users’ eye gaze data during an e-commerce website navigation through a web-cam based eye tracker. Other features of a product (ratings, number of orders) were also included in generating the results in order to get more convenient results. Finally, a clustering-based machine learning algorithm was used to group the similar product based on the input data and recommend similar products to the users, implicitly expressed greater interest. In this study, we concluded that users can find their desired products with less physical assertion and more satisfaction. © 2020 Association for Computing Machinery.","Clustering; Eye gaze features; Eye tracker; K-means","K-means clustering; Learning algorithms; Machine learning; Recommender systems; Clustering; Direct interactions; Eye trackers; Eye-gaze; Implicit interaction; Indirect interactions; K-means; Online product recommendations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082560100
"Franken G.","56272923700;","Packaging design and testing by eye tracking",2020,"International Symposium on Graphic Engineering and Design",,,,"347","354",,,"10.24867/GRID-2020-p38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113524973&doi=10.24867%2fGRID-2020-p38&partnerID=40&md5=f5ef4e1f17a938413ae662e8e3bcd3ee","University of Ljubljana, Faculty of Natural Sciences and Engineering, Department of Textiles, Graphic Arts and Design Ljubljana, Chair of Information and Graphic Arts Technology, Ljubljana, Slovenia","Franken, G., University of Ljubljana, Faculty of Natural Sciences and Engineering, Department of Textiles, Graphic Arts and Design Ljubljana, Chair of Information and Graphic Arts Technology, Ljubljana, Slovenia","The importance of packaging design has been increasing in today’s competitive world. Approximately 70% of purchasing decisions are made in a store. Over 60% of purchasing decisions are based on packaging; the actual shopping is thus the final chance for the packaging to attract the buyer. Packaging has between 2 and 3 seconds to convince the buyer. In addition to the appearance of the individual packages, the appearance of brand packaging is important. We compared different designs of packages. Finally, we placed individual packages on the shelves in a store and carried out measurement of in-store noting. The measurements were carried out using eye tracking equipment (Tobii X120). For each participant, the observing time and the number of fixations in individual areas of interest were measured; both were then compared with heat maps. In this way, we compared the suitability of the form of individual packages and the salience of the packages on the shelves for potential buyers. © 2020 Authors.","Design; Fixations; Heat maps; Observing time; Online observing; Packaging",,Conference Paper,"Final","",Scopus,2-s2.0-85113524973
"Han S.Y., Cho N.I.","57193417343;7201718669;","User-independent gaze estimation by extracting pupil parameter and its mapping to the gaze angle",2020,"Proceedings - International Conference on Pattern Recognition",,,"9412709","1993","2000",,,"10.1109/ICPR48806.2021.9412709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110536822&doi=10.1109%2fICPR48806.2021.9412709&partnerID=40&md5=04756865f7bd34c715e647e738b697de","Dep. of ECE, INMC Seoul National Univ, South Korea","Han, S.Y., Dep. of ECE, INMC Seoul National Univ, South Korea; Cho, N.I., Dep. of ECE, INMC Seoul National Univ, South Korea","Since gaze estimation plays a crucial role in recognizing human intentions, it has been researched for a long time, and its accuracy is ever increasing. However, due to the wide variation in eye shapes and focusing abilities between the individuals, accuracies of most algorithms vary depending on each person in the test group, especially when the initial calibration is not well performed. To alleviate the user-dependency, we attempt to derive features that are general for most people and use them as the input to a deep network instead of using the images as the input. Specifically, we use the pupil shape as the core feature because it is directly related to the 3D eyeball rotation, and thus the gaze direction. While existing deep learning methods learn the gaze point by extracting various features from the image, we focus on the mapping function from the eyeball rotation to the gaze point by using the pupil shape as the input. It is shown that the accuracy of gaze point estimation also becomes robust for the uncalibrated points by following the characteristics of the mapping function. Also, our gaze network learns the gaze difference to facilitate the re-calibration process to fix the calibration-drift problem that typically occurs with glass-type or head-mount devices. © 2020 IEEE",,"Calibration; Deep learning; Learning systems; Mapping; Calibration drift; Gaze direction; Gaze estimation; Gaze point estimations; Human intentions; Learning methods; Mapping functions; User independents; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85110536822
"Chugh S., Brousseau B., Rose J., Eizenman M.","57226118880;55601672200;35586991000;6701402159;","Detection and correspondence matching of corneal reflections for eye tracking using deep learning",2020,"Proceedings - International Conference on Pattern Recognition",,,"9412066","2210","2217",,,"10.1109/ICPR48806.2021.9412066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110506988&doi=10.1109%2fICPR48806.2021.9412066&partnerID=40&md5=857c88950038a4f82d4b84ec4c891c63","Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Department of Electrical and Computer Engineering, Ophthalmology and Vision Sciences, University of Toronto, Toronto, Canada","Chugh, S., Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Brousseau, B., Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Rose, J., Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Eizenman, M., Department of Electrical and Computer Engineering, Ophthalmology and Vision Sciences, University of Toronto, Toronto, Canada","Eye tracking systems that estimate the point-of-gaze are essential in extended reality (XR) systems as they enable new interaction paradigms and technological improvements. It is important for these systems to maintain accuracy when the headset moves relative to the head (known as device slippage) due to head movements or user adjustment. One of the most accurate eye tracking techniques, which is also insensitive to shifts of the system relative to the head, uses two or more infrared (IR) light emitting diodes to illuminate the eye and an IR camera to capture images of the eye. An essential step in estimating the point-of-gaze in these systems is the precise determination of the location of two or more corneal reflections (virtual images of the IR-LEDs that illuminate the eye) in images of the eye. Eye trackers tend to have multiple light sources to ensure at least one pair of reflections for each gaze position. The use of multiple light sources introduces a difficult problem: the need to match the corneal reflections with the corresponding light source over the range of expected eye movements. Corneal reflection detection and matching often fail in XR systems due to the proximity of camera and steep illumination angles of light sources with respect to the eye. The failures are caused by corneal reflections having varying shape and intensity levels or disappearance due to rotation of the eye, or the presence of spurious reflections. We have developed a fully convolutional neural network, based on the UNET architecture, that solves the detection and matching problem in the presence of spurious and missing reflections. Eye images of 25 people were collected in a virtual reality headset using a binocular eye tracking module consisting of five infrared light sources per eye. A set of 4,000 eye images were manually labelled for each of the corneal reflections, and data augmentation was used to generate a dataset of 40,000 images. The network is able to correctly identify and match 91% of corneal reflections present in the test set. This is comparable to a state-of-the-art deep learning system, but our approach requires 33 times less memory and executes 10 times faster. The proposed algorithm, when used in an eye tracker in a VR system, achieved an average mean absolute gaze error of 1°. This is a significant improvement over the state-of-the-art learning-based XR eye tracking systems that have reported gaze errors of 2-3°. © 2020 IEEE","Corneal reflections; Eye tracking; Pattern recognition; Semantic segmentation; Virtual reality","Cameras; Convolutional neural networks; Deep learning; Eye movements; Learning systems; Light sources; Pattern recognition; Virtual reality; Infrared light emitting diodes; Infrared light sources; Interaction paradigm; Multiple light source; Precise determinations; Spurious reflections; Technological improvements; Virtual-reality headsets; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85110506988
"Ralekar C., Gandhi T.K., Chaudhury S.","57189240043;24343318600;7005182122;","Collaborative human machine attention module for character recognition",2020,"Proceedings - International Conference on Pattern Recognition",,,"9413229","9874","9880",,,"10.1109/ICPR48806.2021.9413229","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110467280&doi=10.1109%2fICPR48806.2021.9413229&partnerID=40&md5=44eac1f392eea60345277e5274af06c5","Department of Electrical Engineering, IIT Delhi, New Delhi, 110016, India; Department of Computer Science and Engineering, IIT Jodhpur, Jodhpur, Rajasthan, 342037, India","Ralekar, C., Department of Electrical Engineering, IIT Delhi, New Delhi, 110016, India; Gandhi, T.K., Department of Electrical Engineering, IIT Delhi, New Delhi, 110016, India; Chaudhury, S., Department of Electrical Engineering, IIT Delhi, New Delhi, 110016, India, Department of Computer Science and Engineering, IIT Jodhpur, Jodhpur, Rajasthan, 342037, India","The deep learning models, which include attention mechanisms, are shown to enhance the performance and efficiency of the various computer vision tasks such as pattern recognition, object detection, face recognition, etc. Although the visual attention mechanism is the source of inspiration for these models, recent attention models consider 'attention' as a pure machine vision optimization problem, and visual attention remains the most neglected aspect. Therefore, this paper presents a collaborative human and machine attention module which considers both visual and network's attention. The proposed module is inspired by the dorsal ('where') pathways of visual processing and can be integrated with any convolutional neural network (CNN) model. First, the module computes the spatial attention map from the input feature maps, which is then combined with the visual attention maps. The visual attention maps are created using eye-fixations obtained by performing an eye-tracking experiment with human participants. The visual attention map covers the highly salient and discriminating image regions as humans tend to focus on such regions, whereas the other relevant image regions are processed by spatial attention map. The combination of these two maps results in the finer refinement in feature maps, resulting in improved performance. The comparative analysis reveals that our model not only shows significant improvement over the baseline model but also outperforms the other models. We hope that our findings using a collaborative human-machine attention module will be helpful in other computer vision tasks as well. © 2020 IEEE","Attention mechanism; Deep neural networks; Eye-tracking; Human-Machine collaboration","Behavioral research; Character recognition; Computer vision; Convolutional neural networks; Deep learning; Eye tracking; Maps; Object detection; Object recognition; Attention mechanisms; Baseline models; Comparative analysis; Optimization problems; Spatial attention; Visual Attention; Visual attention mechanisms; Visual-processing; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85110467280
"Sean Liu H.Y., Chung J., Eizenman M.","57226116006;57188836982;6701402159;","A general end-to-end method for characterizing neuropsychiatric disorders using free-viewing visual scanning tasks",2020,"Proceedings - International Conference on Pattern Recognition",,,"9412857","8570","8577",,,"10.1109/ICPR48806.2021.9412857","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110432107&doi=10.1109%2fICPR48806.2021.9412857&partnerID=40&md5=6e48e9211538082ee3e801d4fa79991d","Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, Canada","Sean Liu, H.Y., Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Chung, J., Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Eizenman, M., Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada, Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, Canada","The growing availability of eye-gaze tracking technology has allowed for its employment in a wide variety of applications, one of which is the objective diagnosis and monitoring of neuropsychiatric disorders from features of attentional bias extracted from visual scanning patterns. Current techniques in this field are largely comprised of non-generalizable methodologies that rely on domain expertise and study-specific assumptions. In this paper, we present a general, data-driven, end-to-end framework that extracts relevant features of attentional bias from visual scanning behaviour and uses these features to classify between subject groups with standard machine learning techniques. The general framework uses visual scanning data from free-viewing tasks. In these tasks, subjects look at sets of slides with several thematic images while their visual scanning patterns (sets of ordered fixations) are monitored by an eye-tracking system. Subjects' fixations are encoded into relative visual attention maps (RVAMs), and two data-driven methods are proposed to segment regions of interests (ROIs) from RVAMs: 1) using group average RVAMs, and 2) using differences of group average RVAMs. Relative fixation times within the segmented ROIs are then used as input features for a vanilla multilayered perceptron to classify between patient groups. The methods were evaluated on data from two studies: an anorexia nervosa (AN)/healthy controls study (AN study) with 37 subjects, and a bipolar disorder (BD)/major depressive disorder (MDD) study (BD-MDD study) with 73 subjects. Using leave-one-subject-out cross validation, the general methods achieved an area under the receiver operating curve (AUROC) score of 0.935 for the AN study and 0.888 for the BD-MDD study, the latter of which exceeds the performance of the state-of-the-art analysis model designed specifically for the BD-MDD study, which had an AUROC of 0.879. The results validate the proposed framework's efficacy as a generalizable, standard baseline for analyzing visual scanning data. © 2020 IEEE",,"Behavioral research; Eye tracking; Multilayer neural networks; Pattern recognition; Scanning; Data-driven methods; Eye gaze tracking; Eye tracking systems; Multi-layered Perceptron; Neuropsychiatric disorder; Objective diagnosis; Receiver operating curves; Regions of interest; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85110432107
"Li M., Liu B., Hu Y., Wang Y.","57218087423;55544736500;35766130600;55545836600;","Exposing deepfake videos by tracking eye movements",2020,"Proceedings - International Conference on Pattern Recognition",,,"9413139","5184","5189",,,"10.1109/ICPR48806.2021.9413139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109873464&doi=10.1109%2fICPR48806.2021.9413139&partnerID=40&md5=b5450e8051c8825fd7ee4670e88e88e5","School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; Sino-Singapore International Joint Research Institute, Guangzhou, China","Li, M., School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; Liu, B., School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China; Hu, Y., School of Electronic and Information Engineering, South China University of Technology, Guangzhou, China, Sino-Singapore International Joint Research Institute, Guangzhou, China; Wang, Y., Sino-Singapore International Joint Research Institute, Guangzhou, China","It has recently become a major threat to the public media that fake videos are rapidly spreading over the Internet. The advent of Deepfake, a deep-learning based toolkit, has facilitated a massive abuse of improper synthesized videos, which may influence the media credibility and human rights. A worldwide alert has been set off that finding ways to detect such fake videos is not only crucial but also urgent. This paper reports a novel approach to expose deepfake videos. We found that most fake videos are markedly different from the real ones in the way the eyes move. We are thus motivated to define four features that could well capture such differences. The features are then fed to SVM for classification. It is shown to be a promising approach that without high dimensional features and complicated neural networks, we are able to achieve competitive results on several public datasets. Moreover, the proposed features could well participate with other existing methods in the confrontation with deepfakes. © 2020 IEEE",,"Deep learning; Eye tracking; Pattern recognition; Support vector machines; High dimensional feature; Human rights; Set offs; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85109873464
"Zheng C., Zhou J., Sun J., Zhao L.","57218370643;56203158200;57223816983;57224881018;","Adaptive Person-Specific Appearance-Based Gaze Estimation",2020,"Communications in Computer and Information Science","1181",,,"126","139",,,"10.1007/978-981-15-3341-9_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108459958&doi=10.1007%2f978-981-15-3341-9_11&partnerID=40&md5=896c2b4530b746eaddf16b9e2d255728","Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China; Shanghai Key Lab of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, 200240, China; Children’s Hospital of Shanghai, Shanghai, 200062, China","Zheng, C., Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China, Shanghai Key Lab of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, 200240, China; Zhou, J., Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China, Shanghai Key Lab of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, 200240, China; Sun, J., Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China, Shanghai Key Lab of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, 200240, China; Zhao, L., Children’s Hospital of Shanghai, Shanghai, 200062, China","Non-invasive gaze estimation from only eye images captured by camera is a challenging problem due to various eye shapes, eye structures and image qualities. Recently, CNN network has been applied to directly regress eye image to gaze direction and obtains good performance. However, generic approaches are susceptible to bias and variance highly relating to different individuals. In this paper, we study the person-specific bias when applying generic methods on new person. And we introduce a novel appearance-based deep neural network integrating meta-learning to reduce the person-specific bias. Given only a few person-specific calibration images collected in normal calibration process, our model adapts quickly to test person and predicts more accurate gaze directions. Experiments on public MPIIGaze dataset and Eyediap dataset show our approach has achieved competitive accuracy to current state-of-the-art methods and are able to alleviate person-specific bias problem. © 2020, Springer Nature Singapore Pte Ltd.","Deep learning; Gaze estimation; Meta-learning; Person-specific bias","Calibration; Digital television; Multimedia systems; Appearance based; Bias and variance; Calibration process; Gaze direction; Gaze estimation; Generic approach; Generic method; State-of-the-art methods; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85108459958
"Zheng Y., Park S., Zhang X., de Mello S., Hilliges O.","57221387496;57195422868;57142162900;57201314496;14041644100;","Self-learning transformations for improving gaze and head redirection",2020,"Advances in Neural Information Processing Systems","2020-December",,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108406905&partnerID=40&md5=0f3148520704d8957043182e9933be0f","Department of Computer Science, ETH Zurich, Switzerland; NVIDIA","Zheng, Y., Department of Computer Science, ETH Zurich, Switzerland; Park, S., Department of Computer Science, ETH Zurich, Switzerland; Zhang, X., Department of Computer Science, ETH Zurich, Switzerland; de Mello, S., NVIDIA; Hilliges, O., Department of Computer Science, ETH Zurich, Switzerland","Many computer vision tasks rely on labeled data. Rapid progress in generative modeling has led to the ability to synthesize photorealistic images. However, controlling specific aspects of the generation process such that the data can be used for supervision of downstream tasks remains challenging. In this paper we propose a novel generative model for images of faces, that is capable of producing high-quality images under fine-grained control over eye gaze and head orientation angles. This requires the disentangling of many appearance related factors including gaze and head orientation but also lighting, hue etc. We propose a novel architecture which learns to discover, disentangle and encode these extraneous variations in a self-learned manner. We further show that explicitly disentangling task-irrelevant factors results in more accurate modelling of gaze and head orientation. A novel evaluation scheme shows that our method improves upon the state-of-the-art in redirection accuracy and disentanglement between gaze direction and head orientation changes. Furthermore, we show that in the presence of limited amounts of real-world training data, our method allows for improvements in the downstream task of semi-supervised cross-dataset gaze estimation. Please check our project page at: https://ait.ethz.ch/projects/2020/STED-gaze/ © 2020 Neural information processing systems foundation. All rights reserved.",,"Evaluation scheme; Fine-grained control; Generation process; High quality images; Novel architecture; Orientation angles; Orientation changes; Photorealistic images; Quality control",Conference Paper,"Final","",Scopus,2-s2.0-85108406905
"Elbattah M., Guérin J.-L., Carette R., Cilia F., Dequen G.","57163770900;57200857001;57200860085;57200855464;23396657900;","Generative modeling of synthetic eye-tracking data: NLP-based approach with recurrent neural networks",2020,"IJCCI 2020 - Proceedings of the 12th International Joint Conference on Computational Intelligence",,,,"479","484",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107266211&partnerID=40&md5=808f37f12dce754edca8aefce37a0c40","Laboratoire MIS, Université de Picardie Jules Verne, Amiens, France; Evolucare Technologies, Villers Bretonneux, France; Laboratoire CRP-CPO, Université de Picardie Jules Verne, Amiens, France","Elbattah, M., Laboratoire MIS, Université de Picardie Jules Verne, Amiens, France; Guérin, J.-L., Laboratoire MIS, Université de Picardie Jules Verne, Amiens, France; Carette, R., Laboratoire MIS, Université de Picardie Jules Verne, Amiens, France, Evolucare Technologies, Villers Bretonneux, France; Cilia, F., Laboratoire CRP-CPO, Université de Picardie Jules Verne, Amiens, France; Dequen, G., Laboratoire MIS, Université de Picardie Jules Verne, Amiens, France","This study explores a Machine Learning-based approach for generating synthetic eye-tracking data. In this respect, a novel application of Recurrent Neural Networks is experimented. Our approach is based on learning the sequence patterns of eye-tracking data. The key idea is to represent eye-tracking records as textual strings, which describe the sequences of fixations and saccades. The study therefore could borrow methods from the Natural Language Processing (NLP) domain for transforming the raw eye-tracking data. The NLP-based transformation is utilised to convert the high-dimensional eye-tracking data into an amenable representation for learning. Furthermore, the generative modeling could be implemented as a task of text generation. Our empirical experiments support further exploration and development of such NLP-driven approaches for the purpose of producing synthetic eye-tracking datasets for a variety of potential applications. Copyright © 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved","Eye-tracking; Machine learning; NLP; Recurrent neural networks","Intelligent computing; Metadata; Natural language processing systems; Recurrent neural networks; Turing machines; Empirical experiments; Exploration and development; Generative model; High-dimensional; NAtural language processing; Novel applications; Sequence patterns; Text generations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107266211
"Lueckenhoff A., Wessels C., Kyrarini M., Makedon F.","57220054298;57219945746;56786567700;57207521629;","Towards game-based assessment of executive functions in children",2020,"CEUR Workshop Proceedings","2844",,,"15","18",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104585970&partnerID=40&md5=c620641e68417bd9465fe73662b3abdc","Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, United States","Lueckenhoff, A., Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, United States; Wessels, C., Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, United States; Kyrarini, M., Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, United States; Makedon, F., Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, United States","Executive Functions are very important mental skills that help us to coordinate, plan, pay attention, organize, and multitask, among others. Weak executive functions may affect school or work performance. Therefore, there is a need of identifying executive function deficits early during childhood and enable interventions that could improve executive functioning skills. In this work, we present a game-based assessment system of executive functions in children that could be performed at home. The proposed system utilizes machine learning techniques to detect and track head and eye movements from image frames and fuses this data with game performance. A novel variation of the Flanker task has been developed as a game to measure engagement, attention, working memory, and processing speed. In the future, the proposed system will be evaluated in a real-world study on children between 6 and 14 years old. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Executive function; Eye gaze; Flanker task; Game-based assessment","Eye movements; Learning systems; Assessment system; Executive function; Image frames; Machine learning techniques; Processing speed; Real-world; Work performance; Working memory; Artificial intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85104585970
"Bao Y., Cheng Y., Liu Y., Lu F.","57222730666;57220572010;57215289395;54956194300;","Adaptive feature fusion network for gaze tracking in mobile tablets",2020,"Proceedings - International Conference on Pattern Recognition",,,"9412205","9936","9943",,1,"10.1109/ICPR48806.2021.9412205","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103871941&doi=10.1109%2fICPR48806.2021.9412205&partnerID=40&md5=8a669c1894913fde791624b07644fe97","State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University, Beijing, China; Peng Cheng Laboratory, Shenzhen, China","Bao, Y., State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University, Beijing, China; Cheng, Y., State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University, Beijing, China; Liu, Y., State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University, Beijing, China; Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University, Beijing, China, Peng Cheng Laboratory, Shenzhen, China","Recently, many multi-stream gaze estimation methods have been proposed. They estimate gaze from eye and face appearances and achieve reasonable accuracy. However, most of the methods simply concatenate the features extracted from eye and face appearance. The feature fusion process has been ignored. In this paper, we propose a novel Adaptive Feature Fusion Network (AFF-Net), which performs gaze tracking task in mobile tablets. We stack two-eye feature maps and utilize Squeeze-and-Excitation layers to adaptively fuse two-eye features according to their similarity on appearance. Meanwhile, we also propose Adaptive Group Normalization to recalibrate eye features with the guidance of facial feature. Extensive experiments on both GazeCapture and MPIIFaceGaze datasets demonstrate consistently superior performance of the proposed method. © 2020 IEEE",,"Pattern recognition; Adaptive features; Facial feature; Feature fusion; Feature map; Gaze estimation; Gaze tracking; Multi-stream; Reasonable accuracy; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85103871941
"Wan Z., Xiong C., Li Q., Chen W., Wong K.K.L., Wu S.","57195490237;57211738191;57204585324;55715974300;26434942800;55913991500;","Accurate regression-based 3d gaze estimation using multiple mapping surfaces",2020,"IEEE Access","8",,,"166460","166471",,,"10.1109/ACCESS.2020.3023448","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102869728&doi=10.1109%2fACCESS.2020.3023448&partnerID=40&md5=f5d9066962878a17b78a41a3828ebe0d","State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, 430074, China; School of Information Science and Engineering, Institute of Robotics and Intelligent Systems, Wuhan University of Science and Technology, Wuhan, 430081, China","Wan, Z., State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, 430074, China; Xiong, C., State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, 430074, China; Li, Q., State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, 430074, China; Chen, W., State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, 430074, China; Wong, K.K.L., School of Information Science and Engineering, Institute of Robotics and Intelligent Systems, Wuhan University of Science and Technology, Wuhan, 430081, China; Wu, S., School of Information Science and Engineering, Institute of Robotics and Intelligent Systems, Wuhan University of Science and Technology, Wuhan, 430081, China","Accurate 3D gaze estimation using a simple setup remains a challenging issue for head-mounted eye tracking. Current regression-based gaze direction estimation methods implicitly assume that all gaze directions intersect at one point called the eyeball pseudo-center. The effect of this implicit assumption on gaze estimation is unknown. In this paper, we find that this assumption is approximate based on a simulation of all intersections of gaze directions, and it is conditional based on a sensitivity analysis of the assumption in gaze estimation. Hence, we propose a gaze direction estimation method with one mapping surface that satisfies conditions of the assumption by configuring one mapping surface and achieving a high-quality calibration of the eyeball pseudo-center. This method only adds two additional calibration points outside the mapping surface. Furthermore, replacing the eyeball pseudo-center with an additional calibrated surface, we propose a gaze direction estimation method with two mapping surfaces that further improves the accuracy of gaze estimation. This method improves accuracy on the state-of-the-art method by 20 percent (from a mean error of 1.84 degrees to 1.48 degrees) on a public dataset with a usage range of 1 meter and by 17 percent (from a mean error of 2.22 degrees to 1.85 degrees) on a public dataset with a usage range of 2 meters. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","3D gaze estimation; Eyeball center; Gaze direction estimation; Head-mounted eye tracking; Mapping surface","Calibration; Mapping; Sensitivity analysis; Calibration points; Gaze direction; Gaze estimation; Head-mounted eye tracking; High quality; Pseudo centers; Public dataset; State-of-the-art methods; Eye tracking",Article,"Final","",Scopus,2-s2.0-85102869728
"Valenzuela A., Arellano C., Tapia J.E.","57216819899;57207768339;7005419930;","Towards an efficient segmentation algorithm for near-infrared eyes images",2020,"IEEE Access","8",,,"171598","171607",,2,"10.1109/ACCESS.2020.3025195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102826657&doi=10.1109%2fACCESS.2020.3025195&partnerID=40&md5=b1c8a7c18899d23d9e50b6587f933b78","R+D CORFO Center SR-226, TOC Biometric, Santiago, 7520000, Chile; R+D Visiometrica, Santiago, 7500000, Chile; Escuela de Negocios, Universidad Adolfo Ibañez, Santiago, 7911547, Chile; Departamento de Ingenieria Informatica, Universidad de Santiago, Santiago, 9170197, Chile","Valenzuela, A., R+D CORFO Center SR-226, TOC Biometric, Santiago, 7520000, Chile; Arellano, C., R+D Visiometrica, Santiago, 7500000, Chile, Escuela de Negocios, Universidad Adolfo Ibañez, Santiago, 7911547, Chile; Tapia, J.E., R+D CORFO Center SR-226, TOC Biometric, Santiago, 7520000, Chile, Departamento de Ingenieria Informatica, Universidad de Santiago, Santiago, 9170197, Chile","Semantic segmentation has been widely used for several applications, including the detection of eye structures. This is used in tasks such as eye-tracking and gaze estimation, which are useful techniques for human-computer interfaces, salience detection, and Virtual reality (VR), amongst others. Most of the state of the art techniques achieve high accuracy but with a considerable number of parameters. This article explores alternatives to improve the efficiency of the state of the art method, namely DenseNet Tiramisu, when applied to NIR image segmentation. This task is not trivial; the reduction of block and layers also affects the number of feature maps. The growth rate (k) of the feature maps regulates how much new information each layer contributes to the global state, therefore the trade-off amongst grown rate (k), IOU, and the number of layers needs to be carefully studied. The main goal is to achieve a light-weight and efficient network with fewer parameters than traditional architectures in order to be used for mobile device applications. As a result, a DenseNet with only three blocks and ten layers is proposed (DenseNet10). Experiments show that this network achieved higher IOU rates when comparing with Encoder-Decoder, DensetNet56-67-103, MaskRCNN, and DeeplabV3+ models in the Facebook database. Furthermore, this method reached 8th place in The Facebook semantic segmentation challenge with 0.94293 mean IOU and 202.084 parameters with a final score of 0.97147. This score is only 0,001 lower than the first place in the competition. The sclera was identified as the more challenging structure to be segmented. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Biometrics; Deep learning; Semantic segmentation","Economic and social effects; Eye tracking; Growth rate; Image enhancement; Infrared devices; Semantics; Social networking (online); Human computer interfaces; Mobile device applications; Number of layers; Segmentation algorithms; Semantic segmentation; State-of-the-art methods; State-of-the-art techniques; Traditional architecture; Image segmentation",Article,"Final","",Scopus,2-s2.0-85102826657
"Chi J., Wang D., Lu N., Wang Z.","8702376200;57222478849;57217305267;55880036500;","Cornea radius calibration for remote 3D gaze tracking systems",2020,"IEEE Access","8",,,"187634","187647",,,"10.1109/ACCESS.2020.3029300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102809780&doi=10.1109%2fACCESS.2020.3029300&partnerID=40&md5=df0ec9c4a14ac64e7d4a6324da4254c0","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, 100083, China","Chi, J., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Wang, D., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Lu, N., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Wang, Z., School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, 100083, China","Cornea radius estimation is a key technique for 3D gaze estimation in the single-camera 3D gaze tracking system. Traditional methods with one-camera-one-light-source systems or one-camera-two- light-source systems cannot achieve 3D gaze estimation. The 3D line-of-sight can be estimated only when the cornea radius is pre-calibrated by the user. A cornea radius calibration method based on the iris radius is proposed in this paper for 3D gaze estimation in remote one-camera-two-light-source systems. We first calibrate the iris radius based on the binocular strategy, estimate the spatial iris center using the calibrated iris radius, and then calibrate the cornea radius by a set of non-linear equations under the constraint of equivalent distances from the cornea center to the iris edge points. The calibrated cornea radius is verified by binocular optimization constraints. Simulations and physical experiments validate the effectiveness of the proposed method. The iris-based cornea radius calibration approach is novel; it can be used to obtain the cornea radius and 3D gaze using remote one-camera-one-light-source or one-camera-multi-light-source systems. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","3D line-of-sight estimation; Cornea radius; Corneal refiections; Gaze tracking; Optimization constraints; User calibration","Binoculars; Calibration; Cameras; Light sources; Calibration method; Gaze estimation; Gaze tracking system; Light-source systems; Multi-light sources; Physical experiments; Radius estimation; Single cameras; Eye tracking",Article,"Final","",Scopus,2-s2.0-85102809780
"Günther U., Harrington K.I.S., Dachselt R., Sbalzarini I.F.","56661947000;19638432100;6507253418;8650204800;","Bionic Tracking: Using Eye Tracking to Track Biological Cells in Virtual Reality",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12535 LNCS",,,"280","297",,,"10.1007/978-3-030-66415-2_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101327963&doi=10.1007%2f978-3-030-66415-2_18&partnerID=40&md5=97377cdcf3d5c4f0728a32ebe62b176e","Center for Advanced Systems Understanding, Görlitz, Germany; Center for Systems Biology Dresden, Dresden, Germany; Max Planck Institute of Molecular Cell Biology and Genetics, Dresden, Germany; Faculty of Computer Science, Technische Universität Dresden, Dresden, Germany; Cluster of Excellence Physics of Life, Technische Universität Dresden, Dresden, Germany; Virtual Technology and Design, University of Idaho, Moscow, ID, United States; HHMI Janelia Research Campus, Ashburn, VA, United States","Günther, U., Center for Advanced Systems Understanding, Görlitz, Germany, Center for Systems Biology Dresden, Dresden, Germany, Max Planck Institute of Molecular Cell Biology and Genetics, Dresden, Germany; Harrington, K.I.S., Virtual Technology and Design, University of Idaho, Moscow, ID, United States, HHMI Janelia Research Campus, Ashburn, VA, United States; Dachselt, R., Faculty of Computer Science, Technische Universität Dresden, Dresden, Germany, Cluster of Excellence Physics of Life, Technische Universität Dresden, Dresden, Germany; Sbalzarini, I.F., Center for Systems Biology Dresden, Dresden, Germany, Max Planck Institute of Molecular Cell Biology and Genetics, Dresden, Germany, Faculty of Computer Science, Technische Universität Dresden, Dresden, Germany, Cluster of Excellence Physics of Life, Technische Universität Dresden, Dresden, Germany","We present Bionic Tracking, a novel method for solving biological cell tracking problems with eye tracking in virtual reality using commodity hardware. Using gaze data, and especially smooth pursuit eye movements, we are able to track cells in time series of 3D volumetric datasets. The problem of tracking cells is ubiquitous in developmental biology, where large volumetric microscopy datasets are acquired on a daily basis, often comprising hundreds or thousands of time points that span hours or days. The image data, however, is only a means to an end, and scientists are often interested in the reconstruction of cell trajectories and cell lineage trees. Reliably tracking cells in crowded three-dimensional space over many time points remains an open problem, and many current approaches rely on tedious manual annotation or curation. In the Bionic Tracking approach, we substitute the usual 2D point-and-click interface for annotation or curation with eye tracking in a virtual reality headset, where users follow cells with their eyes in 3D space in order to track them. We detail the interaction design of our approach and explain the graph-based algorithm used to connect different time points, also taking occlusion and user distraction into account. We demonstrate Bionic Tracking using examples from two different biological datasets. Finally, we report on a user study with seven cell tracking experts, highlighting the benefits and limitations of Bionic Tracking compared to point-and-click interfaces. © 2020, Springer Nature Switzerland AG.",,"Bionics; Cells; Computer vision; Cytology; Eye movements; Graph algorithms; Graphic methods; Large dataset; Trees (mathematics); Virtual reality; Developmental biology; Graph-based algorithms; Point-and-click interface; Smooth pursuit eye movement; Three dimensional space; Tracking approaches; Virtual-reality headsets; Volumetric data sets; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85101327963
"Gudi A., Li X., van Gemert J.","56685323300;57221401175;15119779400;","Efficiency in Real-Time Webcam Tracking",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12535 LNCS",,,"529","543",,1,"10.1007/978-3-030-66415-2_34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101327825&doi=10.1007%2f978-3-030-66415-2_34&partnerID=40&md5=3e04bccf8ac3d83a1799cbfea0b1e541","Vicarious Perception Technologies [VicarVision], Amsterdam, Netherlands; Delft University of Technology [TU Delft], Delft, Netherlands","Gudi, A., Vicarious Perception Technologies [VicarVision], Amsterdam, Netherlands, Delft University of Technology [TU Delft], Delft, Netherlands; Li, X., Vicarious Perception Technologies [VicarVision], Amsterdam, Netherlands, Delft University of Technology [TU Delft], Delft, Netherlands; van Gemert, J., Delft University of Technology [TU Delft], Delft, Netherlands","Efficiency and ease of use are essential for practical applications of camera based eye/gaze-tracking. Gaze tracking involves estimating where a person is looking on a screen based on face images from a computer-facing camera. In this paper we investigate two complementary forms of efficiency in gaze tracking: 1. The computational efficiency of the system which is dominated by the inference speed of a CNN predicting gaze-vectors; 2. The usability efficiency which is determined by the tediousness of the mandatory calibration of the gaze-vector to a computer screen. To do so, we evaluate the computational speed/accuracy trade-off for the CNN and the calibration effort/accuracy trade-off for screen calibration. For the CNN, we evaluate the full face, two-eyes, and single eye input. For screen calibration, we measure the number of calibration points needed and evaluate three types of calibration: 1. pure geometry, 2. pure machine learning, and 3. hybrid geometric regression. Results suggest that a single eye input and geometric regression calibration achieve the best trade-off. © 2020, Springer Nature Switzerland AG.",,"Calibration; Cameras; Computational efficiency; Computer vision; Economic and social effects; Efficiency; Geometry; Calibration points; Camera-based; Computational speed; Computer screens; Ease-of-use; Face images; Gaze tracking; Screen calibrations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85101327825
"Lee S., Park J., Nam D.","35778392800;50961546500;43761314400;","Crosstalk minimization method for eye-tracking-based 3D display",2020,"Journal of Imaging Science and Technology","64","6","060407","","",,,"10.2352/J.IMAGINGSCI.TECHNOL.2020.64.6.060407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101168167&doi=10.2352%2fJ.IMAGINGSCI.TECHNOL.2020.64.6.060407&partnerID=40&md5=933a406393223ed407143dc15043cac8","Computer Vision Lab, Samsung Advanced Institute of Technology, Yeongtong-gu, Suwon-si, Gyeonggi-do, South Korea","Lee, S., Computer Vision Lab, Samsung Advanced Institute of Technology, Yeongtong-gu, Suwon-si, Gyeonggi-do, South Korea; Park, J., Computer Vision Lab, Samsung Advanced Institute of Technology, Yeongtong-gu, Suwon-si, Gyeonggi-do, South Korea; Nam, D., Computer Vision Lab, Samsung Advanced Institute of Technology, Yeongtong-gu, Suwon-si, Gyeonggi-do, South Korea","In this article, the authors present an image processing method to reduce three-dimensional (3D) crosstalk for eye-tracking-based 3D display. Specifically, they considered 3D pixel crosstalk and offset crosstalk and applied different approaches based on its characteristics. For 3D pixel crosstalk which depends on the viewer’s relative location, they proposed output pixel value weighting scheme based on viewer’s eye position, and for offset crosstalk they subtracted luminance of crosstalk components according to the measured display crosstalk level in advance. By simulations and experiments using the 3D display prototypes, the authors evaluated the effectiveness of proposed method. c 2020 Society for Imaging Science and Technology. c Society for Imaging Science and Technology 2020",,"Crosstalk; Eye tracking; Pixels; Cross-talk levels; Crosstalk minimization; Image processing - methods; Pixel values; Relative location; Science and Technology; Threedimensional (3-d); Weighting scheme; Three dimensional displays",Article,"Final","",Scopus,2-s2.0-85101168167
"Kim H., Martin S., Tawari A., Misu T., Gabbard J.L.","57211829917;55510668500;36470963300;14018320600;6603313947;","Toward Real-Time Estimation of Driver Situation Awareness: An Eye-tracking Approach based on Moving Objects of Interest",2020,"IEEE Intelligent Vehicles Symposium, Proceedings",,,"9304770","1035","1041",,1,"10.1109/IV47402.2020.9304770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099876491&doi=10.1109%2fIV47402.2020.9304770&partnerID=40&md5=82035a5a5e1c7a69fa7e25407cf16c16","Virginia Tech Transportation Institute, 3500 Transportation Research Plaza, Blacksburg, VA, United States; Honda Research Institute Usa, 70 Rio Robles, San Jose, CA, United States; Acubed, 601 W California Ave, Sunnyvale, CA, United States; Virginia Tech, 250 Durham Hall (0118), 1145 Perry Street, Blacksburg, VA  24061, United States","Kim, H., Virginia Tech Transportation Institute, 3500 Transportation Research Plaza, Blacksburg, VA, United States; Martin, S., Honda Research Institute Usa, 70 Rio Robles, San Jose, CA, United States; Tawari, A., Acubed, 601 W California Ave, Sunnyvale, CA, United States; Misu, T., Honda Research Institute Usa, 70 Rio Robles, San Jose, CA, United States; Gabbard, J.L., Virginia Tech, 250 Durham Hall (0118), 1145 Perry Street, Blacksburg, VA  24061, United States","Eye-tracking techniques have the potential for estimating driver awareness of road hazards. However, traditional eye-movement measures based on static areas of interest may not capture the unique characteristics of driver eyeglance behavior and challenge the real-time application of the technology on the road. This article proposes a novel method to operationalize driver eye-movement data analysis based on moving objects of interest. A human-subject experiment conducted in a driving simulator demonstrated the potential of the proposed method. Correlation and regression analyses between indirect (i.e., eye-tracking) and direct measures of driver awareness identified some promising variables that feature both spatial and temporal aspects of driver eye-glance behavior relative to objects of interest. Results also suggest that eye-glance behavior might be a promising but insufficient predictor of driver awareness. This work is a preliminary step toward real-time, on-road estimation of driver awareness of road hazards. The proposed method could be further combined with computer-vision techniques such as object recognition to fully automate eye-movement data processing as well as machine learning approaches to improve the accuracy of driver awareness estimation. © 2020 IEEE.",,"Data handling; Eye movements; Hazards; Object recognition; Object tracking; Regression analysis; Roads and streets; Computer vision techniques; Correlation and regression analysis; Eye-movement measures; Human subject experiments; Machine learning approaches; Real-time application; Real-time estimation; Situation awareness; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85099876491
"Youssef H., Laurent P., Magalie O., Thierry C.","57216271632;57216270160;57216269786;57203462297;","Identifying causal relationships between behavior and local brain activity during natural conversation",2020,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH","2020-October",,,"101","105",,,"10.21437/Interspeech.2020-2074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098206896&doi=10.21437%2fInterspeech.2020-2074&partnerID=40&md5=b4900b945d8fe2aafd064a371bfa286a","Aix Marseille Université, Université de Toulon, CNRS, LIS, UMR7020, Marseille, France; Aix Marseille Université, CNRS, LPL, France; Institut Universitaire de France, Paris, France; Aix Marseille Université, CNRS, INT, UMR7289, Marseille, France","Youssef, H., Aix Marseille Université, Université de Toulon, CNRS, LIS, UMR7020, Marseille, France, Aix Marseille Université, CNRS, LPL, France; Laurent, P., Aix Marseille Université, CNRS, LPL, France, Institut Universitaire de France, Paris, France; Magalie, O., Aix Marseille Université, Université de Toulon, CNRS, LIS, UMR7020, Marseille, France; Thierry, C., Aix Marseille Université, CNRS, INT, UMR7289, Marseille, France","Characterizing precisely neurophysiological activity involved in natural conversations remains a major challenge. We explore in this paper the relationship between multimodal conversational behavior and brain activity during natural conversations. This is challenging due to Functional Magnetic Resonance Imaging (fMRI) time resolution and to the diversity of the recorded multimodal signals. We use a unique corpus including localized brain activity and behavior recorded during a fMRI experiment when several participants had natural conversations alternatively with a human and a conversational robot. The corpus includes fMRI responses as well as conversational signals that consist of synchronized raw audio and their transcripts, video and eye-tracking recordings. The proposed approach includes a first step to extract discrete neurophysiological time-series from functionally well defined brain areas, as well as behavioral time-series describing specific behaviors. Then, machine learning models are applied to predict neurophysiological time-series based on the extracted behavioral features. The results show promising prediction scores, and specific causal relationships are found between behaviors and the activity in functional brain areas for both conditions, i.e., human-human and human-robot conversations. Copyright © 2020 ISCA","Functional MRI; Human-human and human-machine interactions; Machine learning; Multimodal signals processing; Natural conversation","Eye tracking; Magnetic resonance imaging; Neurophysiology; Robots; Speech communication; Time series; Behavioral features; Brain activity; Brain areas; Causal relationships; Functional magnetic resonance imaging; Human robots; Machine learning models; Time resolution; Brain",Conference Paper,"Final","",Scopus,2-s2.0-85098206896
"Lee K.I., Jeon J.H., Song B.C.","57217629614;57211625894;36062099900;","Deep Learning-Based Pupil Center Detection for Fast and Accurate Eye Tracking System",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12364 LNCS",,,"36","52",,2,"10.1007/978-3-030-58529-7_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097309459&doi=10.1007%2f978-3-030-58529-7_3&partnerID=40&md5=c4609c2c6646ff73561941bbe1280b1f","Department of Electrical Engineering, Inha University, Incheon, South Korea","Lee, K.I., Department of Electrical Engineering, Inha University, Incheon, South Korea; Jeon, J.H., Department of Electrical Engineering, Inha University, Incheon, South Korea; Song, B.C., Department of Electrical Engineering, Inha University, Incheon, South Korea","In augmented reality (AR) or virtual reality (VR) systems, eye tracking is a key technology and requires significant accuracy as well as real-time operation. Many techniques for detecting pupil centers with error range of iris radius have been developed, but few techniques have precise performance with error range of pupil radius. In addition, the conventional methods rarely guarantee real-time pupil center detection in a general-purpose computer environment due to high complexity. Thus, we propose more accurate pupil center detection by improving the representation quality of the network in charge of pupil center detection. This is realized by representation learning based on mutual information. Also, the latency of the entire system is greatly reduced by using non-local block and self-attention block with large receptive field, which makes it accomplish real-time operation. The proposed system not only shows real-time performance of 52 FPS in a general-purpose computer environment but also provides state-of-the-art accuracy in terms of fine level index of 96.71%, 99.84% and 96.38% for BioID, GI4E and Talking Face Video datasets, respectively. © 2020, Springer Nature Switzerland AG.","Mobile applications; Remote eye tracking","Augmented reality; Computer vision; Eye tracking; General purpose computers; Real time systems; Conventional methods; Eye tracking systems; Key technologies; Mutual informations; Real time performance; Real-time operation; Receptive fields; State of the art; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85097309459
"de Lope J., Graña M.","55888684400;7005388617;","Comparison of Labeling Methods for Behavioral Activity Classification Based on Gaze Ethograms",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12344 LNAI",,,"132","144",,,"10.1007/978-3-030-61705-9_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097107413&doi=10.1007%2f978-3-030-61705-9_12&partnerID=40&md5=346ca2eb2066a55615e68dd86e5f1bd0","Computational Cognitive Robotics Group, Department of Artificial Intelligence, Universidad Politécnica de Madrid (UPM), Madrid, Spain; Computational Intelligence Group, University of the Basque Country (UPV/EHU), San Sebastian, Spain","de Lope, J., Computational Cognitive Robotics Group, Department of Artificial Intelligence, Universidad Politécnica de Madrid (UPM), Madrid, Spain; Graña, M., Computational Intelligence Group, University of the Basque Country (UPV/EHU), San Sebastian, Spain","The paper describes and compares several novel alternatives for labeling gaze ethograms data to estimate the activity that users carry out in front of computers with the use of the onboard camera. Gaze ethograms are basically discrete functions of time, therefore, the problem can be formulated by applying statistical and machine learning inspired methods to reduce the amount of information on a specific activity. To compare the proposed methods we carry out several experiments with experimental subjects in an office-like environment with no special lighting conditions. The result is a set of recommendations that allow to classify the activities with high precision. © 2020, Springer Nature Switzerland AG.","Activity recognition; Behavioral activity classification; Eye tracking; Gaze ethograms; Neuroethology; Non-invasive techniques; Screen-based eye tracker","Artificial intelligence; Computer science; Computers; Activity classifications; Amount of information; Discrete functions; Experimental subjects; High-precision; Labeling methods; Lighting conditions; Specific activity; Intelligent systems",Conference Paper,"Final","",Scopus,2-s2.0-85097107413
"Nguyen D.-L., Putro M.D., Jo K.-H.","57214528479;57189328179;56978116100;","Human Eye Detector with Light-Weight and Efficient Convolutional Neural Network",2020,"Communications in Computer and Information Science","1287",,,"186","196",,,"10.1007/978-3-030-63119-2_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097080431&doi=10.1007%2f978-3-030-63119-2_16&partnerID=40&md5=b4dd31ad63511bd75a5246c6816ef165","School of Electrical Engineering, University of Ulsan, Ulsan, South Korea","Nguyen, D.-L., School of Electrical Engineering, University of Ulsan, Ulsan, South Korea; Putro, M.D., School of Electrical Engineering, University of Ulsan, Ulsan, South Korea; Jo, K.-H., School of Electrical Engineering, University of Ulsan, Ulsan, South Korea","The human eye detection plays an important role in computer vision. Along with face detection, it is widely applied in practical security, surveillance, and warning systems such as eye tracking system, eye disease detection, gaze detection, eye blink, and drowsiness detection system. There have been many studies to detect eyes from applying traditional methods to using modern methods based on machine learning and deep learning. This network is deployed with two main blocks, namely the feature extraction block and the detection block. The feature extraction block starts with the use of the convolution layers, C.ReLU (Concatenated Rectified Linear Unit) module, and max pooling layers alternately, followed by the last six inception modules and four convolution layers. The detection block is constructed by two sibling convolution layers using for classification and regression. The experiment was trained and tested on CEW (Closed Eyes In The Wild), BioID Face and GI4E (Gaze Interaction for Everybody) dataset with the results achieved 96.48%, 99.58%, and 75.52% of AP (Average Precision), respectively. The speed was tested in real-time by 37.65 fps (frames per second) on Intel Core I7-4770 CPU @ 3.40 GHz. © 2020, Springer Nature Switzerland AG.","Convolutional Neural Network (CNN); Deep learning; Drowsiness detection system; Eye detection","Convolution; Convolutional neural networks; Deep learning; Extraction; Eye protection; Eye tracking; Face recognition; Detection blocks; Drowsiness detection; Eye tracking systems; Frames per seconds; Gaze detection; Gaze interaction; Intel core i7; Linear units; Feature extraction",Conference Paper,"Final","",Scopus,2-s2.0-85097080431
"Cenzi E.D., Rudek M.","57220157704;16043686600;","A Method to Gaze Following Detection by Computer Vision Applied to Production Environments",2020,"IFIP Advances in Information and Communication Technology","594",,,"36","49",,,"10.1007/978-3-030-62807-9_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097060290&doi=10.1007%2f978-3-030-62807-9_4&partnerID=40&md5=21f21fbf0112e591705f86ede9a45315","Pontifícia Universidade Católica do Paraná PUCPR, Industrial and Systems Engineering Graduate Program – PPGEPS, Curitiba, PR  80215-901, Brazil","Cenzi, E.D., Pontifícia Universidade Católica do Paraná PUCPR, Industrial and Systems Engineering Graduate Program – PPGEPS, Curitiba, PR  80215-901, Brazil; Rudek, M., Pontifícia Universidade Católica do Paraná PUCPR, Industrial and Systems Engineering Graduate Program – PPGEPS, Curitiba, PR  80215-901, Brazil","The humans have the natural ability of following objects with the head and eyes and identify the relationship between those objects. This daily activity represents a challenge for computer vision systems. The procedure to identify the relationship between human eye gaze and the trackable objects is complex and demands several details. In this current paper we proposed a review of the main gazing following methods, identified the respective performance of them and also proposed an AI based method to estimate the gaze from 2D images based on head pose estimation. The main important details to be recovered from images are scene depth, head position and alignment and ocular rotation. In this approach we perform a track estimation of the gaze direction without the use of the eye position, and also, the face partial occlusion is considered in the analysis. The proposed approach allows low cost in processing with considerable accuracy at low complexity sceneries, because we don’t need to extract the facial features. Gaze tracking is important to evaluate employees’ attention to specific tasks in order to prevent accidents and improve work quality. The presented method improves the current knowing workflow by applying the head pose estimation instead face detection for training and inference. The promisors results are presented and open points are also discussed. © 2020, IFIP International Federation for Information Processing.","Artificial intelligence; Artificial neural network; Computer vision; Gaze direction; Gaze following","Computer vision; Eye tracking; Face recognition; Quality control; Computer vision system; Daily activity; Gaze direction; Head Pose Estimation; Partial occlusions; Production environments; Specific tasks; Track estimation; Life cycle",Conference Paper,"Final","",Scopus,2-s2.0-85097060290
"Cheng Y., Huang S., Wang F., Qian C., Lu F.","57220572010;57204286590;57197712252;57204290076;54956194300;","A Coarse-to-fine adaptive network for appearance-based gaze estimation",2020,"AAAI 2020 - 34th AAAI Conference on Artificial Intelligence",,,,"10623","10630",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095506412&partnerID=40&md5=8e7d800d7c8df133c5b8cc3c1822232b","State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University, China; SenseTime Co., Ltd.; Peng Cheng Laboratory, Shenzhen, China; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, China","Cheng, Y., State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University, China; Huang, S., SenseTime Co., Ltd.; Wang, F., SenseTime Co., Ltd.; Qian, C., SenseTime Co., Ltd.; Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University, China, Peng Cheng Laboratory, Shenzhen, China, Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, China","Human gaze is essential for various appealing applications. Aiming at more accurate gaze estimation, a series of recent works propose to utilize face and eye images simultaneously. Nevertheless, face and eye images only serve as independent or parallel feature sources in those works, the intrinsic correlation between their features is overlooked. In this paper we make the following contributions: 1) We propose a coarseto- fine strategy which estimates a basic gaze direction from face image and refines it with corresponding residual predicted from eye images. 2) Guided by the proposed strategy, we design a framework which introduces a bi-gram model to bridge gaze residual and basic gaze direction, and an attention component to adaptively acquire suitable fine-grained feature. 3) Integrating the above innovations, we construct a coarse-to-fine adaptive network named CA-Net and achieve state-of-the-art performances on MPIIGaze and EyeDiap. © AAAI 2020 - 34th AAAI Conference on Artificial Intelligence. All Rights Reserved.",,"Adaptive networks; Appearance based; Coarse to fine; Coarse-to-fine strategy; Fine grained; Gaze direction; Gaze estimation; State-of-the-art performance; Artificial intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85095506412
"Xi Z., Newton O., McGowin G., Sukthankar G., Fiore S., Oden K.","57219597778;57190797973;57219601540;6507708407;35490410200;57188972018;","Predicting Student Flight Performance with Multimodal Features",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12268 LNCS",,,"277","287",,,"10.1007/978-3-030-61255-9_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094164069&doi=10.1007%2f978-3-030-61255-9_27&partnerID=40&md5=a18f57f11c55101da2386f5f67310955","University of Central Florida, Orlando, FL, United States; Lockheed Martin, Orlando, FL, United States","Xi, Z., University of Central Florida, Orlando, FL, United States; Newton, O., University of Central Florida, Orlando, FL, United States; McGowin, G., University of Central Florida, Orlando, FL, United States; Sukthankar, G., University of Central Florida, Orlando, FL, United States; Fiore, S., University of Central Florida, Orlando, FL, United States; Oden, K., Lockheed Martin, Orlando, FL, United States","This paper investigates the problem of predicting student flight performance in a training simulation from multimodal features, including flight controls, visual attention, and knowledge acquisition tests. This is a key component of developing next generation training simulations that can optimize the usage of student training time. Two types of supervised machine learning classifiers (random forest and support vector machines) were trained to predict the performance of twenty-three students performing simple flight tasks in virtual reality. Our experiments reveal the following: 1) features derived from gaze tracking and knowledge acquisition tests can serve as an adequate substitute for flight control features; 2) data from the initial portion of the flight task is sufficient to predict the final outcome; 3) our classifiers perform slightly better at predicting student failure than success. These results indicate the feasibility of using machine learning for early prediction of student failures during flight training. © 2020, Springer Nature Switzerland AG.","Gaze tracking; Intelligent training simulations; Multimodal features; Pilot training; Supervised learning","Behavioral research; Decision trees; Eye tracking; Forecasting; Knowledge acquisition; Learning systems; Random forests; Students; Support vector machines; Early prediction; Flight control; Flight training; Multimodal features; Student training; Supervised machine learning; Training simulation; Visual Attention; Flight simulators",Conference Paper,"Final","",Scopus,2-s2.0-85094164069
"Rakhmatulin I., Duchowski A.T.","57214824551;6701824388;","Deep neural networks for low-cost eye tracking",2020,"Procedia Computer Science","176",,,"685","694",,1,"10.1016/j.procs.2020.09.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093361943&doi=10.1016%2fj.procs.2020.09.041&partnerID=40&md5=8e47baac9dc6d55b60addbcc9e082c65","South Ural State University, Department of Power Plants Networks and Systems, Chelyabinsk, 454080, Russian Federation; Clemson University, 100 McAdams Hall, Clemson, SC  29634, United States","Rakhmatulin, I., South Ural State University, Department of Power Plants Networks and Systems, Chelyabinsk, 454080, Russian Federation; Duchowski, A.T., Clemson University, 100 McAdams Hall, Clemson, SC  29634, United States","The paper presents a detailed analysis of modern techniques that can be used to track gaze with a webcam. We present a practical implementation of the most popular methods for tracking gaze. Various models of deep neural networks that can be involved in the process of online gaze monitoring are reviewed. We introduce a new eye-tracking approach where the effectiveness of using a deep learning method is significantly increased. Implementation is in Python where its application is demonstrated by controlling interaction with the computer. Specifically, a dual coordinate system is given for controlling the computer with the help of a gaze. The first set of coordinates-the position of the face relative to the computer, is implemented by detecting color from the infrared LED via the OpenCV library. The second set of coordinates-giving gaze position-is obtained via the YOLO (v3) package. A method of labeling the eyes is given, in which 3 objects are used to track gaze (to the left, to the right, and in the center). © 2020 The Authors. Published by Elsevier B.V.","Deep learning in eye tracking; Deep learning in gaze tracking; Eye tracking; Yolov3 in eye tracking","Costs; Deep learning; Deep neural networks; Knowledge based systems; Learning systems; Neural networks; Co-ordinate system; Infrared leds; ITS applications; Learning methods; Low cost eye tracking; Modern techniques; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85093361943
"Prasse P., Jäger L.A., Makowski S., Feuerpfeil M., Scheffer T.","55376814000;56503248300;57205689294;57219503299;55122621900;","On the relationship between eye tracking resolution and performance of oculomotoric biometric identification",2020,"Procedia Computer Science","176",,,"2088","2097",,,"10.1016/j.procs.2020.09.245","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093360330&doi=10.1016%2fj.procs.2020.09.245&partnerID=40&md5=68f9780eca802b2b3f7d6b767a458402","University of Potsdam, Department for Computer Science, August-Bebel-Str. 89, Potsdam, 14482, Germany","Prasse, P., University of Potsdam, Department for Computer Science, August-Bebel-Str. 89, Potsdam, 14482, Germany; Jäger, L.A., University of Potsdam, Department for Computer Science, August-Bebel-Str. 89, Potsdam, 14482, Germany; Makowski, S., University of Potsdam, Department for Computer Science, August-Bebel-Str. 89, Potsdam, 14482, Germany; Feuerpfeil, M., University of Potsdam, Department for Computer Science, August-Bebel-Str. 89, Potsdam, 14482, Germany; Scheffer, T., University of Potsdam, Department for Computer Science, August-Bebel-Str. 89, Potsdam, 14482, Germany","Distributional properties of fixations and saccades are known to constitute biometric characteristics. Additionally, high-frequency micro-movements of the eyes have recently been found to constitute biometric characteristics that allow for faster and more robust biometric identification than just macro-movements. Micro-movements of the eyes occur on scales that are very close to the precision of currently available eye trackers. This study therefore characterizes the relationship between the temporal and spatial resolution of eye tracking recordings on one hand and the performance of a biometric identification method that processes micro- and macro-movements via a deep convolutional network. We find that that the deteriorating effects of decreasing both, the temporal and spatial resolution are not cumulative. We observe that on low-resolution data, the network reaches performance levels above chance and outperforms statistical approaches. © 2020 The Authors. Published by Elsevier B.V.","Biometrics; Data quality; Deep learning; Eye tracking","Anthropometry; Biometrics; Convolutional neural networks; Eye movements; Image resolution; Knowledge based systems; Biometric identification methods; Biometric identifications; Convolutional networks; Distributional property; High frequency HF; Performance level; Statistical approach; Temporal and spatial; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85093360330
"Park S., Aksan E., Zhang X., Hilliges O.","57195422868;57073715000;57142162900;14041644100;","Towards End-to-End Video-Based Eye-Tracking",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12357 LNCS",,,"747","763",,3,"10.1007/978-3-030-58610-2_44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093118867&doi=10.1007%2f978-3-030-58610-2_44&partnerID=40&md5=f16ab6bab356ba6b8a6cb197236b0ed9","Department of Computer Science, ETH Zurich, Zürich, Switzerland","Park, S., Department of Computer Science, ETH Zurich, Zürich, Switzerland; Aksan, E., Department of Computer Science, ETH Zurich, Zürich, Switzerland; Zhang, X., Department of Computer Science, ETH Zurich, Zürich, Switzerland; Hilliges, O., Department of Computer Science, ETH Zurich, Zürich, Switzerland","Estimating eye-gaze from images alone is a challenging task, in large parts due to un-observable person-specific factors. Achieving high accuracy typically requires labeled data from test users which may not be attainable in real applications. We observe that there exists a strong relationship between what users are looking at and the appearance of the user’s eyes. In response to this understanding, we propose a novel dataset and accompanying method which aims to explicitly learn these semantic and temporal relationships. Our video dataset consists of time-synchronized screen recordings, user-facing camera views, and eye gaze data, which allows for new benchmarks in temporal gaze tracking as well as label-free refinement of gaze. Importantly, we demonstrate that the fusion of information from visual stimuli as well as eye images can lead towards achieving performance similar to literature-reported figures acquired through supervised personalization. Our final method yields significant performance improvements on our proposed EVE dataset, with up to 28 % improvement in Point-of-Gaze estimates (resulting in 2. 49∘ in angular error), paving the path towards high-accuracy screen-based eye tracking purely from webcam sensors. The dataset and reference source code are available at https://ait.ethz.ch/projects/2020/EVE. © 2020, Springer Nature Switzerland AG.","Computer vision dataset; Eye tracking; Gaze estimation","Computer vision; Semantics; Angular errors; Personalizations; Real applications; Reference source; Temporal relationships; Video dataset; Video-based eye-tracking; Visual stimulus; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85093118867
"Yu Y., Odobez J.-M.","57188644020;57203103085;","Unsupervised representation learning for gaze estimation",2020,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",,,"9157235","7312","7322",,9,"10.1109/CVPR42600.2020.00734","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093107084&doi=10.1109%2fCVPR42600.2020.00734&partnerID=40&md5=76efd0300079e57e6b1e24c46b839875","Idiap Research Institute, Martigny, CH-1920, Switzerland; EPFL, Lausanne, CH-1015, Switzerland","Yu, Y., Idiap Research Institute, Martigny, CH-1920, Switzerland, EPFL, Lausanne, CH-1015, Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, CH-1920, Switzerland, EPFL, Lausanne, CH-1015, Switzerland","Although automatic gaze estimation is very important to a large variety of application areas, it is difficult to train accurate and robust gaze models, in great part due to the difficulty in collecting large and diverse data (annotating 3D gaze is expensive and existing datasets use different setups). To address this issue, our main contribution in this paper is to propose an effective approach to learn a low dimensional gaze representation without gaze annotations, which to the best of our best knowledge, is the first work to do so. The main idea is to rely on a gaze redirection network and use the gaze representation difference of the input and target images (of the redirection network) as the redirection variable. A redirection loss in image domain allows the joint training of both the redirection network and the gaze representation network. In addition, we propose a warping field regularization which not only provides an explicit physical meaning to the gaze representations but also avoids redirection distortions. Promising results on few-shot gaze estimation (competitive results can be achieved with as few as ≤ 100 calibration samples), cross-dataset gaze estimation, gaze network pretraining, and another task (head pose estimation) demonstrate the validity of our framework. © 2020 IEEE",,"Pattern recognition; Three dimensional computer graphics; Application area; Calibration samples; Effective approaches; Gaze estimation; Gaze representation; Head Pose Estimation; Low dimensional; Physical meanings; Large dataset",Conference Paper,"Final","",Scopus,2-s2.0-85093107084
"You F., Khakhar R., Picht T., Dobbelstein D.","57207816563;57219439698;12239885300;55655102700;","VR Simulation of Novel Hands-Free Interaction Concepts for Surgical Robotic Visualization Systems",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12263 LNCS",,,"440","450",,,"10.1007/978-3-030-59716-0_42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092741566&doi=10.1007%2f978-3-030-59716-0_42&partnerID=40&md5=62e5493ae02be6e0ee67ebde5b4b9a0d","Carl Zeiss AG, Carl-Zeiss-Strasse 22, Oberkochen, 73447, Germany; Charité Universitätsmedizin Berlin, Charitéplatz 1, Berlin, 10117, Germany; Institute for Anthropomatics and Robotics - Intelligent Process Automation and Robotics Lab (IAR-IPR), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany","You, F., Carl Zeiss AG, Carl-Zeiss-Strasse 22, Oberkochen, 73447, Germany, Institute for Anthropomatics and Robotics - Intelligent Process Automation and Robotics Lab (IAR-IPR), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Khakhar, R., Charité Universitätsmedizin Berlin, Charitéplatz 1, Berlin, 10117, Germany; Picht, T., Charité Universitätsmedizin Berlin, Charitéplatz 1, Berlin, 10117, Germany; Dobbelstein, D., Carl Zeiss AG, Carl-Zeiss-Strasse 22, Oberkochen, 73447, Germany","In microsurgery, visualization systems such as the traditional surgical microscope are essential, as surgeons rely on the highly magnified stereoscopic view for performing their operative tasks. For well-aligned visual perspectives onto the operating field during surgery, precise adjustments of the positioning of the system are frequently required. This, however, implies that the surgeon has to reach for the device and each time remove their hand(s) from the operating field, i.e. a disruptive event to the operative task at hand. To address this, we propose two novel hands-free interaction concepts based on head-, and gaze-tracking, that should allow surgeons to efficiently control the 6D positioning of a robotic visualization system with little interruptions to the main operative task. The new concepts were purely simulated in a virtual reality (VR) environment using a HTC Vive for a robotic visualization system. The two interaction concepts were evaluated within the virtual reality simulation in a quantitative user study with 11 neurosurgeons at the Charité hospital and compared to conventional interaction with a surgical microscope. After a brief introduction to the interaction concepts in the virtual reality simulation, neurosurgeons were 29% faster in reaching a set of virtual targets (position and orientation) in simulation as compared to reaching equivalent physical targets on a 3D-printed reference object. © 2020, Springer Nature Switzerland AG.","Hands-free interaction; Human-robot-interaction; Intraoperative visualization and guidance; Microsurgery; Virtual reality","3D printers; Eye tracking; Medical computing; Medical imaging; Robotics; Stereo image processing; Surgery; Three dimensional computer graphics; Uncertainty analysis; Virtual reality; Visualization; Hands-free interactions; Interaction concepts; Position and orientations; Reference objects; Stereoscopic view; Surgical microscopes; Virtual reality simulations; Visualization system; Robotic surgery",Conference Paper,"Final","",Scopus,2-s2.0-85092741566
"Leong-Hoi A., Murat S., Acker T., Sattler Q., Radoux J.-P.","56204372500;57219439541;57219435354;57219437868;56352046600;","Touchless interface interaction by hand tracking with a depth camera and a Convolutional Neural Network",2020,"Proceedings of SPIE - The International Society for Optical Engineering","11353",,"113531Z","","",,,"10.1117/12.2555676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092732052&doi=10.1117%2f12.2555676&partnerID=40&md5=f37d18f7ff99bb952cf3f2e024f0ea4e","Altran Research Department, 950 Boulevard Sébastien Brandt, Bât Gauss CS 20143, Illkirch, 67404, France","Leong-Hoi, A., Altran Research Department, 950 Boulevard Sébastien Brandt, Bât Gauss CS 20143, Illkirch, 67404, France; Murat, S., Altran Research Department, 950 Boulevard Sébastien Brandt, Bât Gauss CS 20143, Illkirch, 67404, France; Acker, T., Altran Research Department, 950 Boulevard Sébastien Brandt, Bât Gauss CS 20143, Illkirch, 67404, France; Sattler, Q., Altran Research Department, 950 Boulevard Sébastien Brandt, Bât Gauss CS 20143, Illkirch, 67404, France; Radoux, J.-P., Altran Research Department, 950 Boulevard Sébastien Brandt, Bât Gauss CS 20143, Illkirch, 67404, France","In operating rooms (OR), physicians have to work in strict compliance to asepsis rules to not endanger the health of patients. In the case of laparoscopic mini-invasive surgery, where the field of view of the surgeon is restricted, the use of computers is necessary to provide the missing information. Physicians must therefore interact with computers either by directly manipulating mouse and keyboard, by removing their gloves or by using a protection for the devices, or by voice-commanding an assistant to do so. However, in addition to be time-consuming, it may cause hygiene issues in the first case and a lack of precision in the second. The need to have better way of interactions with computers had led to important researches in that area during the last ten years, especially in Touchless Human Machine Interaction (THMI). Indeed, THMI, including gesture recognition, voice recognition and eye-tracking, has a promising future in the medical field, allowing surgeons to interact by themselves with devices, thereby avoiding error-prone process while complying to asepsis rules. In this context, the “Intelligent Touchless Glassless Human-Machine Interface” (ITG-HMI) project aims to provide a new tool for viewing and manipulating 3D objects. In this article, we present how this interface was implemented, through the detection and recognition of hand gestures using Deep Learning, the establishment of a graphical interface to display 3D models and the adaptation of gestures recognized in actions to achieve. © 2020 SPIE","3D interface; Convolutional Neural Network (CNN); Deep learning; Hand detection; YOLOv2","Convolutional neural networks; Deep learning; Eye tracking; Mammals; Man machine systems; Palmprint recognition; Error-prone process; Field of views; Graphical interface; Human machine interaction; Human Machine Interface; Interface interaction; Medical fields; Missing information; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-85092732052
"Jiao J., Cai Y., Alsharid M., Drukker L., Papageorghiou A.T., Noble J.A.","56414059500;57202376461;57189700785;36241434600;57194082999;56185660000;","Self-Supervised Contrastive Video-Speech Representation Learning for Ultrasound",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12263 LNCS",,,"534","543",,,"10.1007/978-3-030-59716-0_51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092701936&doi=10.1007%2f978-3-030-59716-0_51&partnerID=40&md5=15e2c88453c0f29c0b1f6480ba0a610b","Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Nuffield Department of Women’s & Reproductive Health, University of Oxford, Oxford, United Kingdom","Jiao, J., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Cai, Y., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Alsharid, M., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Drukker, L., Nuffield Department of Women’s & Reproductive Health, University of Oxford, Oxford, United Kingdom; Papageorghiou, A.T., Nuffield Department of Women’s & Reproductive Health, University of Oxford, Oxford, United Kingdom; Noble, J.A., Department of Engineering Science, University of Oxford, Oxford, United Kingdom","In medical imaging, manual annotations can be expensive to acquire and sometimes infeasible to access, making conventional deep learning-based models difficult to scale. As a result, it would be beneficial if useful representations could be derived from raw data without the need for manual annotations. In this paper, we propose to address the problem of self-supervised representation learning with multi-modal ultrasound video-speech raw data. For this case, we assume that there is a high correlation between the ultrasound video and the corresponding narrative speech audio of the sonographer. In order to learn meaningful representations, the model needs to identify such correlation and at the same time understand the underlying anatomical features. We designed a framework to model the correspondence between video and audio without any kind of human annotations. Within this framework, we introduce cross-modal contrastive learning and an affinity-aware self-paced learning scheme to enhance correlation modelling. Experimental evaluations on multi-modal fetal ultrasound video and audio show that the proposed approach is able to learn strong representations and transfers well to downstream tasks of standard plane detection and eye-gaze prediction. © 2020, Springer Nature Switzerland AG.","Representation learning; Self-supervised; Video-audio","Deep learning; Medical computing; Medical imaging; Ultrasonic applications; Anatomical features; Correlation modelling; Experimental evaluation; Eye-gaze predictions; Learning Based Models; Manual annotation; Self-paced learning; Ultrasound videos; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85092701936
"Freitas L.F.S., Ancioto A.S.R., de Fátima Rodrigues Guimarães R., Martins V.F., Dias D.R.C., de Paiva Guimarães M.","57210972461;57210968745;57219435558;55557223800;36782058600;6507640953;","A Virtual Reality Simulator to Assist in Memory Management Lectures",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12255 LNCS",,,"810","825",,,"10.1007/978-3-030-58820-5_58","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092701468&doi=10.1007%2f978-3-030-58820-5_58&partnerID=40&md5=47a4a610f810bda4d8ff8898b72e9cb1","Computer Science Master Program, Unifaccamp, Campo Limpo Paulista, Brazil; Applied Linguistics (Institute of Language Studies/Unicamp), Campinas, Brazil; Faculty of Computing and Informatics, Mackenzie Presbyterian University, São Paulo, Brazil; Computer Science Department (UFSJ), São João del Rei, Brazil; Núcleo de Ensino a Distância (UAB/Reitoria), UNIFESP, São Paulo, Brazil","Freitas, L.F.S., Computer Science Master Program, Unifaccamp, Campo Limpo Paulista, Brazil; Ancioto, A.S.R., Computer Science Master Program, Unifaccamp, Campo Limpo Paulista, Brazil; de Fátima Rodrigues Guimarães, R., Computer Science Master Program, Unifaccamp, Campo Limpo Paulista, Brazil, Applied Linguistics (Institute of Language Studies/Unicamp), Campinas, Brazil; Martins, V.F., Faculty of Computing and Informatics, Mackenzie Presbyterian University, São Paulo, Brazil; Dias, D.R.C., Computer Science Department (UFSJ), São João del Rei, Brazil; de Paiva Guimarães, M., Computer Science Master Program, Unifaccamp, Campo Limpo Paulista, Brazil, Núcleo de Ensino a Distância (UAB/Reitoria), UNIFESP, São Paulo, Brazil","Virtual reality technology can assist the teaching-learning process via concrete concepts that follow a sequence of steps to accomplish a task in a three-dimensional space, as proven in the literature (e.g., operating a vehicle or disaster relief simulation). However, it is not clear whether virtual reality can also enhance situations involving concepts that are not inherently related to a three-dimensional space, such as the execution of a computer algorithm. This paper presents an immersive and interactive virtual reality simulator that can aid in the teaching of the memory management functionality of operating systems, including single, fixed and dynamic contiguous techniques, and the non-contiguous technique of paging. Learners are immersed inside a computer motherboard to learn memory management functionality of operating systems. They use a head-mounted display such as 3D virtual reality headsets and can interact with the environment using eye-gaze tracking and a joystick. We also present a case study in which 80 students were divided into two groups to evaluate the simulator. Our results indicate that using our simulator is a more effective approach for teaching memory management concepts than expositive classes. The functionality and usability tests results highlighted the positive aspects of the simulator and improvements that could be made. © 2020, Springer Nature Switzerland AG.","Computer science; Learning environment; Memory management; Operating system; Virtual reality","Computer operating systems; Disaster prevention; Eye tracking; Helmet mounted displays; Learning systems; Simulators; Computer motherboard; Effective approaches; Head mounted displays; Interactive virtual reality; Teaching-learning process; Three dimensional space; Virtual reality simulator; Virtual reality technology; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85092701468
"Jin Y., Chen M., Bell T.G., Wan Z., Bovik A.","57225874348;57212195714;57219424445;57194421176;56984291600;","Study of 2D foveated video quality in virtual reality",2020,"Proceedings of SPIE - The International Society for Optical Engineering","11510",,"1151007","","",,2,"10.1117/12.2568883","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092628513&doi=10.1117%2f12.2568883&partnerID=40&md5=02dfe173c6cf7cc6002ae5407c2c0358","University of Texas at Austin, 2501 Speedway, Austin, United States; Formerly at Facebook Reality Labs, 9845 Willows Rd, Redmond, United States; Harbin Institute of Technology, No. 92 Xidazhi Street, Heilongjiang, China","Jin, Y., University of Texas at Austin, 2501 Speedway, Austin, United States; Chen, M., University of Texas at Austin, 2501 Speedway, Austin, United States; Bell, T.G., Formerly at Facebook Reality Labs, 9845 Willows Rd, Redmond, United States; Wan, Z., Harbin Institute of Technology, No. 92 Xidazhi Street, Heilongjiang, China; Bovik, A., University of Texas at Austin, 2501 Speedway, Austin, United States","In Virtual Reality (VR), the necessity of immersive videos leads to greater challenges in compression and communication owing to the much higher spatial resolution, rapid, and the often real-time changes in viewing direction. Foveation in displays exploits the space-variant density of the retinal photoreceptors, which decreases exponentially with increasing eccentricity, to reduce the amount of data from the visual periphery. Foveated compression is gaining relevance and popularity for Virtual Reality. Likewise, being able to predict the quality of displayed foveated and compressed content has become more important. Towards advancing the development of objective quality assessment algorithms for foveated and compressed measurements of VR video contents, we built a new VR database of foveated/compressed videos, and conducted a human study of perceptual quality on it. A foveated video player having low motion-to-photon latency (50ms) was designed to meet the requirements of smooth playback, while an eye tracker was deployed to provide gaze direction in real time. We generated 180 distorted videos from 10 pristine 8K videos (30fps) having varying levels and combinations of foveation and compression distortions. These contents were viewed and quality-rated by 36 subjects in a controlled VR setting. Both the subject ratings and the eye tracking data are being made available along with the rest of the database. © 2020 SPIE","Foveated video quality assessment; Full reference; Human perception; Immersive video database; Virtual reality","Aldehydes; Eye tracking; Motion tracking; Measurements of; Objective quality assessment; Perceptual quality; Real-time changes; Space variants; Spatial resolution; Video contents; Viewing directions; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85092628513
"Kanda D., Kawai S., Nobuhara H.","57219345132;54393290500;7004339328;","Visualization method corresponding to regression problems and its application to deep learning-based gaze estimation model",2020,"Journal of Advanced Computational Intelligence and Intelligent Informatics","24","5",,"676","684",,1,"10.20965/JACIII.2020.P0676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092266729&doi=10.20965%2fJACIII.2020.P0676&partnerID=40&md5=932b018997ef71b36c984251ed376b36","Department of Intelligent Interaction Technologies, Graduate School of Systems and Information Engineering, University of Tsukuba, 1-1-1 Tennoudai, Tsukuba, Ibaraki, 305-8573, Japan","Kanda, D., Department of Intelligent Interaction Technologies, Graduate School of Systems and Information Engineering, University of Tsukuba, 1-1-1 Tennoudai, Tsukuba, Ibaraki, 305-8573, Japan; Kawai, S., Department of Intelligent Interaction Technologies, Graduate School of Systems and Information Engineering, University of Tsukuba, 1-1-1 Tennoudai, Tsukuba, Ibaraki, 305-8573, Japan; Nobuhara, H., Department of Intelligent Interaction Technologies, Graduate School of Systems and Information Engineering, University of Tsukuba, 1-1-1 Tennoudai, Tsukuba, Ibaraki, 305-8573, Japan","The human gaze contains substantial personal information and can be extensively employed in several applications if its relevant factors can be accurately measured. Further, several fields could be substantially innovated if the gaze could be analyzed using popular and familiar smart devices. Deep learning-based methods are robust, making them crucial for gaze estimation on smart devices. However, because internal functions in deep learning are black boxes, deep learning systems often make estimations for unclear reasons. In this paper, we propose a visualization method corresponding to a regression problem to solve the black box problem of the deep learning-based gaze estimation model. The proposed visualization method can clarify which region of an image contributes to deep learning-based gaze estimation. We visualized the gaze estimation model proposed by a research group at the Massachusetts Institute of Technology. The accuracy of the estimation was low, even when the facial features important for gaze estimation were recognized correctly. The effectiveness of the proposed method was further determined through quantitative evaluation using the area over the MoRF perturbation curve (AOPC). © 2020 Fuji Technology Press. All rights reserved.","CNN; Eye tracking; Grad-CAM; Regression problem","Learning systems; Visualization; Internal function; ITS applications; Learning-based methods; Massachusetts Institute of Technology; Personal information; Quantitative evaluation; Regression problem; Visualization method; Deep learning",Article,"Final","",Scopus,2-s2.0-85092266729
"Fu B., Steichen B., McBride A.","57220885355;25928750600;57219164840;","Tumbling to Succeed: A Predictive Analysis of User Success in Interactive Ontology Visualization",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare","Part F162565",,,"78","87",,,"10.1145/3405962.3405966","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091524093&doi=10.1145%2f3405962.3405966&partnerID=40&md5=02be11a07cb9a73e08aec6492b2bb4ca","Computer Engineering and Computer Science, California State University Long, Beach, United States; Computer Science, California State Polytechnic, University Pomona, United States","Fu, B., Computer Engineering and Computer Science, California State University Long, Beach, United States; Steichen, B., Computer Science, California State Polytechnic, University Pomona, United States; McBride, A., Computer Engineering and Computer Science, California State University Long, Beach, United States","Ontology visualization is an important component in the support of human-ontology interaction, as it amplifies cognition and offloads cognitive efforts to the human perceptual system. While a significant amount of research efforts has focused on designing and developing various visual layouts and improve performance of large-scale visualizations, the differences in user preferences and cognitive abilities have been largely overlooked. This provides an opportunity to investigate ways to potentially provide more personalized visual support in human-ontology interaction. To this end, this paper demonstrates successful predictions on an individual user's likelihood to succeed in a given task, based on this person's gaze data collected during interaction. Specifically, we show several statistically significant predictions against a baseline classifier when inferring users' success before a given task is actually completed. Moreover, we present results showing that accurate predictions of user success can be achieved early on during user interaction, such as after just a few minutes in some cases. These findings suggest there are ample opportunities throughout various stages of human-ontology interaction where the underlying visual system may adapt in real time to the user's visual needs to provide the most appropriate visualization with the overall goal of possibly increasing user success in a given task. © 2020 ACM.","Adaptive Ontology Visualization; Applied Machine Learning; Eye Tracking; Predictive User Analysis; Tumbling Window","Forecasting; Intelligent systems; Predictive analytics; Real time systems; Semantics; Visualization; Accurate prediction; Cognitive ability; Cognitive efforts; Human perceptual system; Improve performance; Large-scale visualization; Ontology visualizations; User interaction; Ontology",Conference Paper,"Final","",Scopus,2-s2.0-85091524093
"George C., Buschek D., Ngao A., Khamis M.","57196008992;55850134500;57219049705;35243028400;","Gazeroomlock: Using gaze and head-pose to improve the usability and observation resistance of 3d passwords in virtual reality",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12242 LNCS",,,"61","81",,3,"10.1007/978-3-030-58465-8_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091135492&doi=10.1007%2f978-3-030-58465-8_5&partnerID=40&md5=8fdee126a2ed26d3f43556d14b19e87c","Chair for Media Informatics, LMU Munich, Munich, Germany; Research Group HCI + AI, Department of Computer Science, University of Bayreuth, Bayreuth, Germany; School of Computing Science, University of Glasgow, Glasgow, United Kingdom","George, C., Chair for Media Informatics, LMU Munich, Munich, Germany; Buschek, D., Research Group HCI + AI, Department of Computer Science, University of Bayreuth, Bayreuth, Germany; Ngao, A., Chair for Media Informatics, LMU Munich, Munich, Germany; Khamis, M., School of Computing Science, University of Glasgow, Glasgow, United Kingdom","Authentication has become an important component of Immersive Virtual Reality (IVR) applications, such as virtual shopping stores, social networks, and games. Recent work showed that compared to traditional graphical and alphanumeric passwords, a more promising form of passwords for IVR is 3D passwords. This work evaluates four multimodal techniques for entering 3D passwords in IVR that consist of multiple virtual objects selected in succession. Namely, we compare eye gaze and head pose for pointing, and dwell time and tactile input for selection. A comparison of a) usability in terms of entry time, error rate, and memorability, and b) resistance to real world and offline observations, reveals that: multimodal authentication in IVR by pointing at targets using gaze, and selecting them using a handheld controller significantly improves usability and security compared to the other methods and to prior work. We discuss how the choice of pointing and selection methods impacts the usability and security of 3D passwords in IVR. © Springer Nature Switzerland AG 2020.",,"Augmented reality; Authentication; Usability engineering; Alphanumeric passwords; Handheld controllers; Immersive virtual reality; Multi-modal techniques; Multimodal authentication; Pointing and selections; Usability and security; Virtual shopping; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85091135492
"Mao Y., He Y., Liu L., Chen X.","17346073100;57218616343;57218621304;57218618670;","Disease Classification Based on Synthesis of Multiple Long Short-Term Memory Classifiers Corresponding to Eye Movement Features",2020,"IEEE Access","8",,"9170601","151624","151633",,,"10.1109/ACCESS.2020.3017680","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090786416&doi=10.1109%2fACCESS.2020.3017680&partnerID=40&md5=03c02eeec3dbb68bf3038100e8e80b9d","State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing, China","Mao, Y., State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing, China; He, Y., State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing, China; Liu, L., State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing, China; Chen, X., State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing, China","Medical research confirms that eye movement abnormalities are related to a variety of psychological activities, mental disorders and physical diseases. However, as the specific manifestations of various diseases in terms of eye movement disorders remain unclear, the accurate diagnosis of diseases according to eye movement is difficult. In this paper, a deep neural network (DNN) method is employed to establish a disease discrimination model according to eye movement. First, multiple eye-tracking experiments are designed to obtain eye images. Second, pupil characteristics, including position and size, are extracted, and the feature vectors of eye movement are obtained from the normalized pupil information. Based on a long short-term memory (LSTM) network, a classifier that corresponds to each feature, which is referred to as a weak classifier, is built. The experimental samples are preclassified, and the classification ability of each weak classifier for different diseases is also calculated. Last, a strong classifier is achieved for disease discrimination by synthesizing all the weak classifiers and their classification abilities. By classification testing for three categories of healthy controls, brain injury patients and vertigo patients, the experimental results demonstrated the efficiency of this method. With the deep learning method, more medical information can be excavated from eye movement to improve the values in disease diagnosis. © 2013 IEEE.","classifier; deep learning; disease discrimination; Eye movement; long short-term memory network","Brain; Classification (of information); Deep learning; Deep neural networks; Diagnosis; Eye tracking; Learning systems; Long short-term memory; Classification ability; Discrimination model; Disease classification; Disease diagnosis; Medical information; Mental disorders; Movement disorders; Strong classifiers; Eye movements",Article,"Final","",Scopus,2-s2.0-85090786416
"Küchemann S., Klein P., Becker S., Kumari N., Kuhn J.","36240700600;56374702400;57191904903;57217561140;55984409000;","Classification of students' conceptual understanding in STEM education using their visual attention distributions: A comparison of three machine-learning approaches",2020,"CSEDU 2020 - Proceedings of the 12th International Conference on Computer Supported Education","1",,,"36","46",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090773287&partnerID=40&md5=2514d6f6e3a56e727ef5c06fbeda748f","Physics Department, Physics Education Research Group, TU Kaiserslautern, Erwin-Schrödinger-Strasse 46, Kaiserslautern, 67663, Germany","Küchemann, S., Physics Department, Physics Education Research Group, TU Kaiserslautern, Erwin-Schrödinger-Strasse 46, Kaiserslautern, 67663, Germany; Klein, P., Physics Department, Physics Education Research Group, TU Kaiserslautern, Erwin-Schrödinger-Strasse 46, Kaiserslautern, 67663, Germany; Becker, S., Physics Department, Physics Education Research Group, TU Kaiserslautern, Erwin-Schrödinger-Strasse 46, Kaiserslautern, 67663, Germany; Kumari, N., Physics Department, Physics Education Research Group, TU Kaiserslautern, Erwin-Schrödinger-Strasse 46, Kaiserslautern, 67663, Germany; Kuhn, J., Physics Department, Physics Education Research Group, TU Kaiserslautern, Erwin-Schrödinger-Strasse 46, Kaiserslautern, 67663, Germany","Line-Graphs play a central role in STEM education, for instance, for the instruction of mathematical concepts or for analyzing measurement data. Consequently, they have been studied intensively in the past years. However, despite this wide and frequent use, little is known about students' visual strategy when solving line-graph problems. In this work, we study two example line-graph problems addressing the slope and the area concept, and apply three supervised machine-learning approaches to classify the students performance using visual attention distributions measured via remote eye tracking. The results show the dominance of a large-margin classifier at small training data sets above random decision forests and a feed-forward artificial neural network. However, we observe a sensitivity of the large-margin classifier towards the discriminatory power of used features which provides a guide for a selection of machine learning algorithms for the optimization of adaptive learning environments. Copyright © 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved","Adaptive Learning Systems; Deep Learning; Eye-tracking; Line-graphs; Machine Learning; Performance Prediction; Problem-solving; Total Visit Duration","Behavioral research; Classification (of information); Computer aided instruction; Decision trees; E-learning; Eye tracking; Feedforward neural networks; Graph theory; Learning systems; STEM (science, technology, engineering and mathematics); Students; Supervised learning; Adaptive learning environment; Conceptual understanding; Discriminatory power; Feed-forward artificial neural networks; Large margin classifiers; Machine learning approaches; Mathematical concepts; Supervised machine learning; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85090773287
"Lediaeva I., LaViola J.J., Jr.","57218918763;6602792780;","Evaluation of Body-referenced graphical menus in virtual environments",2020,"Proceedings - Graphics Interface","2020-May",,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090759242&partnerID=40&md5=0bdc50ec66deb5117c10c7e83331edb8","University of Central Florida, United States","Lediaeva, I., University of Central Florida, United States; LaViola, J.J., Jr., University of Central Florida, United States","Graphical menus have been extensively used in desktop applications and widely adopted and integrated into virtual environments (VEs). However, while desktop menus are well evaluated and established, adopted 2D menus in VEs are still lacking a thorough evaluation. In this paper, we present the results of a comprehensive study on body-referenced graphical menus in a virtual environment. We compare menu placements (spatial, arm, hand, and waist) in conjunction with various shapes (linear and radial) and selection techniques (ray-casting with a controller device, head, and eye gaze). We examine task completion time, error rates, number of target re-entries, and user preference for each condition and provide design recommendations for spatial, arm, hand, and waist graphical menus. Our results indicate that the spatial, hand, and waist menus are significantly faster than the arm menus, and the eye gaze selection technique is more prone to errors and has a significantly higher number of target re-entries than the other selection techniques. Additionally, we found that a significantly higher number of participants ranked the spatial graphical menus as their favorite menu placement and the arm menu as their least favorite one. © 2020 Canadian Information Processing Society. All rights reserved.","3D Menus; Menu Placements; Menu Selection Techniques; Menu Shapes; Virtual Reality","Rendering (computer graphics); Design recommendations; Desktop applications; Error rate; Eye-gaze; On-body; Ray casting; Selection techniques; Task completion time; Petroleum reservoir evaluation",Conference Paper,"Final","",Scopus,2-s2.0-85090759242
"Garro V., Sundstedt V.","57296525000;10240036700;","Pose and visual attention: Exploring the effects of 3D shape near-isometric deformations on gaze",2020,"Journal of WSCG","2020","2020",,"153","160",,1,"10.24132/CSRN.2020.3001.18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090529801&doi=10.24132%2fCSRN.2020.3001.18&partnerID=40&md5=dd6f5b66ccf281004523b2c2ca18e543","Blekinge Institute of Technology, Karlskrona, 371 79, Sweden","Garro, V., Blekinge Institute of Technology, Karlskrona, 371 79, Sweden; Sundstedt, V., Blekinge Institute of Technology, Karlskrona, 371 79, Sweden","Recent research in 3D shape analysis focuses on the study of visual attention on rendered 3D shapes investigating the impact of different factors such as material, illumination, and camera movements. In this paper, we analyze how the pose of a deformable shape affects visual attention. We describe an eye-tracking experiment that studied the influence of different poses of non-rigid 3D shapes on visual attention. The subjects free-viewed a set of 3D shapes rendered in different poses and from different camera views. The fixation maps obtained by the aggregated gaze data were projected onto the 3D shapes and compared at vertex level. The results indicate an impact of the pose for some of the tested shapes and also that view variation influences visual attention. The qualitative analysis of the 3D fixation maps shows high visual focus on the facial regions regardless of the pose, coherent with previous works. The visual attention variation between poses appears to correspond to geometric salient features and semantically salient parts linked to the action represented by the pose. © 2020, Vaclav Skala Union Agency. All rights reserved.","Gaze Analysis; Mesh Models; Perception; Shape Analysis; Visual Attention",,Article,"Final","",Scopus,2-s2.0-85090529801
"Akinyelu A.A., Blignaut P.","56132371000;6602384906;","Convolutional Neural Network-Based Methods for Eye Gaze Estimation: A Survey",2020,"IEEE Access","8",,"9153754","142581","142605",,4,"10.1109/ACCESS.2020.3013540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089957381&doi=10.1109%2fACCESS.2020.3013540&partnerID=40&md5=b6f5c2236a501b8613fa247523d30e8f","Department of Computer Science and Informatics, University of the Free State, Bloemfontein, 9301, South Africa","Akinyelu, A.A., Department of Computer Science and Informatics, University of the Free State, Bloemfontein, 9301, South Africa; Blignaut, P., Department of Computer Science and Informatics, University of the Free State, Bloemfontein, 9301, South Africa","Eye tracking is becoming a very important tool across many domains, including human-computer-interaction, psychology, computer vision, and medical diagnosis. Different methods have been used to tackle eye tracking, however, some of them are inaccurate under real-world conditions, while some require explicit user calibration which can be burdensome. Some of these methods suffer from poor image quality and variable light conditions. The recent success and prevalence of deep learning have greatly improved the performance of eye-tracking. The availability of large-scale datasets has further improved the performance of deep learning-based methods. This article presents a survey of the current state-of-the-art on deep learning-based gaze estimation techniques, with a focus on Convolutional Neural Networks (CNN). This article also provides a survey on other machine learning-based gaze estimation techniques. This study aims to empower the research community with valuable and useful insights that can enhance the design and development of improved and efficient deep learning-based eye-tracking models. This study also provides information on various pre-trained models, network architectures, and open-source datasets that are useful for training deep learning models. © 2013 IEEE.","computer vision; Convolutional neural network; deep learning; eye movements; eye tracking; gaze estimation; region of interest","Convolution; Convolutional neural networks; Deep learning; Diagnosis; Human computer interaction; Large dataset; Learning systems; Network architecture; Surveys; Design and Development; Large-scale datasets; Learning models; Learning-based methods; Light conditions; Research communities; State of the art; User calibration; Eye tracking",Article,"Final","",Scopus,2-s2.0-85089957381
"Ben Slama A., Sahli H., Mouelhi A., Marrakchi J., Sayadi M., Trabelsi H.","57190947496;56640967900;23975192300;55749430500;6603581615;57215412807;","DBN-DNN: discrimination and classification of VNG sequence using deep neural network framework in the EMD domain",2020,"Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization","8","6",,"681","690",,,"10.1080/21681163.2020.1799867","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089254448&doi=10.1080%2f21681163.2020.1799867&partnerID=40&md5=2f2fb0f71dd1625d8e406f2f0016cab9","University of Tunis El Manar, ISTMT, Laboratory of Biophysics and Medical Technologies (BTM), LR13ES07, Tunis, Tunisia; University of Tunis, ENSIT, Laboratory of Signal Image and Energy mastery (SIME), LR13ES03, Tunis, Tunisia; Department of Oto-Rhino-laryngology, La Rabta Hospital, Tunis, Tunisia","Ben Slama, A., University of Tunis El Manar, ISTMT, Laboratory of Biophysics and Medical Technologies (BTM), LR13ES07, Tunis, Tunisia; Sahli, H., University of Tunis, ENSIT, Laboratory of Signal Image and Energy mastery (SIME), LR13ES03, Tunis, Tunisia; Mouelhi, A., University of Tunis, ENSIT, Laboratory of Signal Image and Energy mastery (SIME), LR13ES03, Tunis, Tunisia; Marrakchi, J., Department of Oto-Rhino-laryngology, La Rabta Hospital, Tunis, Tunisia; Sayadi, M., University of Tunis, ENSIT, Laboratory of Signal Image and Energy mastery (SIME), LR13ES03, Tunis, Tunisia; Trabelsi, H., University of Tunis El Manar, ISTMT, Laboratory of Biophysics and Medical Technologies (BTM), LR13ES07, Tunis, Tunisia","The Vestibulo-ocular response VOR is characterized by a smooth pursuit eye movements in one direction, called slow phase of ocular nystagmus, interrupted by resetting saccades fast phase of nystagmus in the other direction. Recording of ocular nystagmus during vestibular tests does not quantify the true response of the vestibulo-ocular reflex (VOR). In order to extract the real VOR, our study is focused on nystagmus analysis using videonystagmography (VNG) technique based on measuring amplitude vibration of eyeball movement. The effectiveness of this attendance is severely topic to the attention and the experience of ENT doctors. In this case, automatic methods of image analysis offer the possibility of obtaining a homogeneous, objective and above all fast diagnosis of vestibular disorder.  In this paper, a fully automatic system based on nystagmus parameter analysis using a pupil detection algorithm is proposed. After a segmentation stage, a deep neural Network based classification method is applied on 90 eye movement recordings from videonystagmography (VNG) containing two types of peripheral vestibular disorders and normal patients. Experimental results obtained after several simulation, show the efficiency of the proposed methodology when compared with other classification methods. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","(VNG) system; deep neural network; empirical mode decomposition; Eye movement; vestibular disorder","adult; aged; Article; benign paroxysmal positional vertigo; Boltzmann machine; clinical article; deep belief network; deep neural network; empirical mode decomposition; entropy; eye movement; eye tracking; feature extraction algorithm; female; human; learning algorithm; machine learning; male; mathematical model; Meniere disease; nystagmography; nystagmus; priority journal; support vector machine; training; validation process; vestibular disorder; vestibular neuronitis; vestibuloocular reflex",Article,"Final","",Scopus,2-s2.0-85089254448
"Khatri J., Moghaddasi M., Llanes-Jurado J., Spinella L., Marín-Morales J., Guixeres J., Alcañiz M.","57218446760;57211442960;57218430050;57218454532;57191977544;26423690300;7003335420;","Optimizing virtual reality eye tracking fixation algorithm thresholds based on shopper behavior and age",2020,"Communications in Computer and Information Science","1225 CCIS",,,"64","69",,1,"10.1007/978-3-030-50729-9_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089220983&doi=10.1007%2f978-3-030-50729-9_9&partnerID=40&md5=47638c5e2a7ece59bb38ca9cd655138a","Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain","Khatri, J., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Moghaddasi, M., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Llanes-Jurado, J., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Spinella, L., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Marín-Morales, J., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Guixeres, J., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Alcañiz, M., Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain","Eye tracking (ET) is becoming a popular tool to study the consumer behavior. One of the significant problems that arise with ET integrated into 3D virtual reality is defining fixations and saccades, which are essential part of feature extraction in ET analysis and have a critical impact on higher level analysis. In this study, the ET data from 60 subjects, were recorded. To define the fixations, Dispersion Threshold Identification algorithm was used which requires to define several thresholds. Since there are multiple thresholds and extracted features, a Multi-Objective Reinforcement Learning (MORL) algorithm was implemented to solve this problem. The objective of the study was to optimize these thresholds in order to improve accuracy of classification of the age based on different visual patterns undertaken by the subject during shopping in a virtual store. Regarding the nature of the classification, the objective for this optimization problem was to maximize the differences between the averages of each feature in different classes and minimize the variances of the same feature within each class. For the current study, thresholds optimization has shown an improvement in results for the accuracies of classification between age groups after applying the MORL algorithm. In conclusion, the results suggest that the optimization of thresholds is an important factor to improve feature extraction methods and in turn improve the overall results of an ET study involving consumer behavior inside virtual reality. This method can be used to optimize thresholds in similar studies to provide improved accuracy of classification results. © Springer Nature Switzerland AG 2020.","Dispersion; Eye tracking; Multi-objective optimization; Optimization; Reinforcement learning; Threshold; Virtual reality","Consumer behavior; Extraction; Feature extraction; Human computer interaction; Reinforcement learning; Virtual reality; 3D virtual reality; Accuracy of classifications; Algorithm threshold; Different class; Feature extraction methods; Identification algorithms; Multiple threshold; Optimization problems; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85089220983
"van der Graaf J., Molenaar I., Lim L., Fan Y., Engelmann K., Gašević D., Bannert M.","56496687300;24577798700;57218403632;57193777414;56736021100;8549413500;6603100059;","Facilitating self-regulated learning with personalized scaffolds on student’s own regulation activities",2020,"CEUR Workshop Proceedings","2610",,,"46","48",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089124258&partnerID=40&md5=7c3f6b158bde03f915a24002630a592c","Behavioural Science Institute, Radboud University, Netherlands; Technical University of Munich, Germany; University of Edinburgh, Edinburgh, United Kingdom; Monash University, Melbourne, Australia","van der Graaf, J., Behavioural Science Institute, Radboud University, Netherlands; Molenaar, I., Behavioural Science Institute, Radboud University, Netherlands; Lim, L., Technical University of Munich, Germany; Fan, Y., University of Edinburgh, Edinburgh, United Kingdom; Engelmann, K., Technical University of Munich, Germany; Gašević, D., University of Edinburgh, Edinburgh, United Kingdom, Monash University, Melbourne, Australia; Bannert, M., Technical University of Munich, Germany","The focus of education is increasingly set on students’ ability to regulate their own learning within technology-enhanced learning environments. Scaffolds have been used to foster self-regulated learning, but scaffolds often are standardized and do not do not adapt to the individual learning process. Learning analytics and machine learning offer an approach to better understand SRL-processes during learning. Yet, current approaches lack validity or require extensive analysis after the learning process. The FLORA project aims to investigate how to advance support given to students by i) improving unobtrusive data collection and machine learning techniques to gain better measurement and understanding of SRL-processes and ii) using these new insights to facilitate student’s SRL by providing personalized scaffolds. We will reach this goal by investigating and improving trace data in exploratory studies (exploratory study1 and study 2) and using the insight gained from these studies to develop and test personalized scaffolds based on individual learning processes in laboratory (experimental study 3 and study 4) and a subsequent field study (field study 5). At the moment study 2 is ongoing. The setup consists of a learning environment presented on a computer with a screen-based eye-tracker. Other data sources are log files and audio of students’ think aloud. The analysis will focus on detecting sequences that are indicative of micro-level self-regulated learning processes and aligning them between the different data sources. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International 1 (CC BY 4.0).","Adaptive systems; Instructional scaffolds; Learning analytics; Machine learning; Personalized learning; Self-regulated-learning","Eye tracking; Machine learning; Scaffolds; Students; Data collection; Exploratory studies; Individual learning process; Learning environments; Learning process; Machine learning techniques; Self-regulated learning; Technology enhanced learning; Computer aided instruction",Conference Paper,"Final","",Scopus,2-s2.0-85089124258
"Hasegawa S., Hirako A., Zheng X., Karimah S.N., Ota K., Unoki T.","7401474763;57218363667;57218365161;57190859275;14021969000;57203460019;","Learner’s mental state estimation with pc built-in camera",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12206 LNCS",,,"165","175",,1,"10.1007/978-3-030-50506-6_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088916166&doi=10.1007%2f978-3-030-50506-6_12&partnerID=40&md5=8e23b164582ce69b5e34a0e4704d7032","Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; IMAGICA GROUP/Photron, 1-105 Jinboucho, Kanda, Chiyoda, Tokyo, 101-0051, Japan","Hasegawa, S., Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; Hirako, A., Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; Zheng, X., Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; Karimah, S.N., Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; Ota, K., Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; Unoki, T., IMAGICA GROUP/Photron, 1-105 Jinboucho, Kanda, Chiyoda, Tokyo, 101-0051, Japan","The purpose of this research is to estimate learners’ mental states such as difficulty, interest, fatigue, and concentration that change with the time series between learners and their learning tasks. Nowadays, we have many opportunities to learn specific topics in the individual learning process, such as active learning and self-directed learning. In such situations, it is challenging to grasp learners’ progress and engagement in their learning process. Several studies have estimated learners’ engagement from facial images/videos in the learning process. However, there is no extensive benchmark dataset except for the video watching process. Therefore, we gathered learners’ videos with facial expression and retrospective self-report from 19 participants through the CAB test process using a PC built-in camera. In this research, we applied an existing face image recognition library Face++ to extract the data such as estimated emotion, eye gaze, face orientation, face position (percentage on the screen) by each frame of the videos. Then, we built a couple of machine learning models, including deep learning methods, to estimate their mental states from the facial expressions and compared them with the average accuracy of prediction. The results demonstrated the potential of the proposed method to the estimation and provided the improvement plan from the accuracy point of view. © Springer Nature Switzerland AG 2020.","Machine learning; Mental state estimation; PC built-in camera","Built-in self test; Cameras; Deep learning; Face recognition; Human computer interaction; Image recognition; Benchmark datasets; Face image recognition; Facial Expressions; Improvement plans; Individual learning process; Learning methods; Machine learning models; Self-directed learning; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85088916166
"Monaro M., Capuozzo P., Ragucci F., Maffei A., Curci A., Scarpazza C., Angrilli A., Sartori G.","57191077038;57195261865;57218310949;57103014500;6602457440;37059831700;6603845293;7007044463;","Using blink rate to detect deception: A study to validate an automatic blink detector and a new dataset of videos from liars and truth-tellers",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12183 LNCS",,,"494","509",,,"10.1007/978-3-030-49065-2_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088751838&doi=10.1007%2f978-3-030-49065-2_35&partnerID=40&md5=7aa5efed903b8d4b8998a27df372bf92","University of Padova, Via Venezia 8, Padua, 35131, Italy; Università degli Studi di Bari Aldo Moro, Via Scipione Crisanzio, 42, Bari, 70122, Italy","Monaro, M., University of Padova, Via Venezia 8, Padua, 35131, Italy; Capuozzo, P., University of Padova, Via Venezia 8, Padua, 35131, Italy; Ragucci, F., University of Padova, Via Venezia 8, Padua, 35131, Italy; Maffei, A., University of Padova, Via Venezia 8, Padua, 35131, Italy; Curci, A., Università degli Studi di Bari Aldo Moro, Via Scipione Crisanzio, 42, Bari, 70122, Italy; Scarpazza, C., University of Padova, Via Venezia 8, Padua, 35131, Italy; Angrilli, A., University of Padova, Via Venezia 8, Padua, 35131, Italy; Sartori, G., University of Padova, Via Venezia 8, Padua, 35131, Italy","Eye-blink is a sensitive index of cognitive load and some studies have reported that it can be a useful cue for detecting deception. However, it is difficult to apply in the real forensic scenario as very complex techniques to record eye blinking are usually needed (e.g., electrooculography, eye tracker technology). In this paper, we propose a new approach to automatically detect eye blinking based on a computer vision algorithm, which does not require any expensive technology to record data. Results demonstrated that the automatic blink detector reached an accuracy similar to the electrooculogram in detecting the blink rate. Moreover, the automatic blink detector was applied to 68 videos of people who were lying or telling the truth about a past holiday, testing the difference between the two groups in terms of blink rate and response timing. Training machine learning classification models on these features, an accuracy up to 70% in identifying liars and truth-tellers was obtained. © Springer Nature Switzerland AG 2020.","Automatic blink detector; Cognitive load; Deception; Eye-blink; Lie detection","Eye tracking; Cognitive loads; Computer vision algorithms; Electro-oculogram; Eye-blinking; New approaches; Response timings; Sensitive index; Training machines; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85088751838
"Larradet F., Barresi G., Mattos L.S.","57196196474;55877110100;7005199692;","Affective communication enhancement system for locked-in syndrome patients",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12188 LNCS",,,"143","156",,,"10.1007/978-3-030-49282-3_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088751237&doi=10.1007%2f978-3-030-49282-3_10&partnerID=40&md5=ddfba704c0fbfd59dff57d9c5abff591","Istituto Italiano di Tecnologia, Genova, Italy","Larradet, F., Istituto Italiano di Tecnologia, Genova, Italy; Barresi, G., Istituto Italiano di Tecnologia, Genova, Italy; Mattos, L.S., Istituto Italiano di Tecnologia, Genova, Italy","Patients with Locked-In Syndrome such as people with Amyotrophic Lateral Sclerosis (ALS) rely on technology for basic communication. However, available Augmentative and Alternative Communication (AAC) tools such as gaze-controlled keyboards have limited abilities. In particular, they do not allow for expression of emotions in addition to words. In this paper we propose a novel gaze-based speaking tool that enable locked-in syndrome patients to express emotions as well as sentences. It also features patient-controlled emotionally modulated speech synthesis. Additionally, an emotional 3D avatar can be controlled by the patient to represent emotional facial-expressions. The systems were tested with 36 people without disabilities separated into an affective group - full control of emotional voice, avatar facial expressions and laugh - and a control group - no emotional tools. The study proved the system’s capacity to enhance communication for both the patient and the interlocutor. The emotions embedded in the synthesized voices were found recognizable at 80% on the first trial and 90% on the second trial. The conversation was perceived as more natural when using the affective tool. The subjects felt it was easier to express and identify emotions using this system. The emotional voice and the emotional avatar were found to help the conversation. This highlights the needs for more affective-driven communicative solutions for locked-in patients. © Springer Nature Switzerland AG 2020.","Communication; Emotions; Eye-tracking; Gaze","Human rehabilitation engineering; Speech synthesis; Three dimensional computer graphics; Affective communication; Amyotrophic lateral sclerosis; Augmentative-and-alternative communication; Control groups; Emotional voices; Express emotions; Facial Expressions; Locked-in syndrome; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85088751237
"Babicsné-Horváth M., Hercegfi K.","57210143965;6503884074;","The difficulties in usability testing of 3-dimensional software applying eye-tracking methodology – Presented via two case studies of evaluation of digital human modelling software",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12199 LNCS",,,"311","321",,,"10.1007/978-3-030-49907-5_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088747456&doi=10.1007%2f978-3-030-49907-5_22&partnerID=40&md5=07570ff030d788015c99cd6e4a736ec0","Department of Ergonomics and Psychology, Budapest University of Technology and Economics, Magyar Tudosok Krt. 2, Budapest, 1117, Hungary","Babicsné-Horváth, M., Department of Ergonomics and Psychology, Budapest University of Technology and Economics, Magyar Tudosok Krt. 2, Budapest, 1117, Hungary; Hercegfi, K., Department of Ergonomics and Psychology, Budapest University of Technology and Economics, Magyar Tudosok Krt. 2, Budapest, 1117, Hungary","Eye-tracking based usability testing methods today are very accepted by researchers. These methods are ones of the most commons in human-computer interaction. There are various types of applications of these methods in software or web usability area, however, there is a difficulty during usability tests with the 3D environment. The problem is occurred when the participant wants to rotate, zoom or move the 3D space. In these cases, the gaze plots, the heatmaps, or the statistics of Area of Interests (AOI) cannot be used regarding the 3D workspace. The data on the menu bar is interpretable, however, on the 3D environment hardly or not at all. In our research, we tested ViveLab and Jack Digital Human Modelling (DHM) software knowing the mentioned problem. Our goal was dual. Firstly, with this usability tests, we wanted to detect the issues in the software. Secondly, we tested the utility of a new methodology which was included the tests. At one point of the usability test, the participants was asked not to move the 3D space, while they had to perform the given tasks. Several methods were used to locate the usability problems of the software. During the tests, we applied eye-tracking method, and after that, each participant was interviewed. Based on the experiences of this research, we can advise future researchers testing similar products. This methodology is useful, and applicable in other related usability tests, and its visualisation techniques for one or more participants are interpretable. © Springer Nature Switzerland AG 2020.","3D environment; Eye-tracking; Human factors and ergonomics (HFE); Jack; Usability testing; ViveLab","Application programs; Ergonomics; Eye tracking; Health risks; Human computer interaction; Risk management; Safety engineering; Testing; Usability engineering; 3-D environments; Area of interest; Digital human modelling; Eye tracking methods; Usability problems; Usability testing; Usability testing methods; Usability tests; Software testing",Conference Paper,"Final","",Scopus,2-s2.0-85088747456
"Wang C., Biancardi B., Mancini M., Cafaro A., Pelachaud C., Pun T., Chanel G.","57202029873;56897257000;13008942700;35104557300;6602822871;7005509099;14049885900;","Impression Detection and Management Using an Embodied Conversational Agent",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12182 LNCS",,,"260","278",,1,"10.1007/978-3-030-49062-1_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088746063&doi=10.1007%2f978-3-030-49062-1_18&partnerID=40&md5=7f659b6cad2ddc498cefca138f66279c","University of Geneva, Geneva, Switzerland; Telecom Paris, Paris, France; University College Cork, Cork, Ireland; CNRS-ISIR, Sorbonne Université, Paris, France","Wang, C., University of Geneva, Geneva, Switzerland; Biancardi, B., Telecom Paris, Paris, France; Mancini, M., University College Cork, Cork, Ireland; Cafaro, A., CNRS-ISIR, Sorbonne Université, Paris, France; Pelachaud, C., CNRS-ISIR, Sorbonne Université, Paris, France; Pun, T., University of Geneva, Geneva, Switzerland; Chanel, G., University of Geneva, Geneva, Switzerland","Embodied Conversational Agents (ECAs) are a promising medium for human-computer interaction, since they are capable of engaging users in real-time face-to-face interaction [1, 2]. Users’ formed impressions of an ECA (e.g. favour or dislike) could be reflected behaviourally [3, 4]. These impressions may affect the interaction and could even remain afterwards [5, 7]. Thus, when we build an ECA to impress users, it is important to detect how users feel about the ECA. The impression the ECA leaves can then be adjusted by controlling its non-verbal behaviour [7]. Motivated by the role of ECAs in interpersonal interaction and the state-of-the-art on affect recognition, we investigated three research questions: 1) which modality (facial expressions, eye movements, and physiological signals) reveals most of the formed impressions; 2) whether an ECA could leave a better impression by maximizing the impression it produces; 3) whether there are differences in impression formation during human-human vs. human-agent interaction. Our results firstly showed the interest to use different modalities to detect impressions. An ANOVA test indicated that facial expressions performance outperforms the physiological modality performance (M = 1.27, p = 0.02). Secondly, our results presented the possibility of creating an adaptive ECA. Compared with the randomly selected ECA behaviour, participants’ ratings tended to be higher in the conditions where the ECA adapted its behaviour based on the detected impressions. Thirdly, we found similar behaviour during human-human vs. human-agent interaction. People treated an ECA similarly to a human by spending more time observing the face area when forming an impression. © 2020, Springer Nature Switzerland AG.","Affective computing; Eye gaze; Impression detection; Impression management; Machine learning; Reinforcement learning; Virtual agent","Behavioral research; Eye movements; Affect recognition; Embodied conversational agent; Face-to-face interaction; Facial Expressions; Human agent interactions; Non-verbal behaviours; Physiological signals; Research questions; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85088746063
"Suk H.J., Kim S.J.","56157231500;57007273000;","The effect of ocular dominance on decision making in a virtual environment",2020,"Advances in Intelligent Systems and Computing","1217 AISC",,,"671","676",,,"10.1007/978-3-030-51828-8_88","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088745752&doi=10.1007%2f978-3-030-51828-8_88&partnerID=40&md5=70a97cfa4a689476ebd6cce2f534aa56","Digital Media, Ajou University, 206 Worldcup-ro, Woncheon-dong, Yeongtong-gu, Suwon-si, Gyeonggi-do, South Korea; College of Engineering, UNLV, 4515 S. Maryland Parkway, Las Vegas, NV, United States","Suk, H.J., Digital Media, Ajou University, 206 Worldcup-ro, Woncheon-dong, Yeongtong-gu, Suwon-si, Gyeonggi-do, South Korea; Kim, S.J., College of Engineering, UNLV, 4515 S. Maryland Parkway, Las Vegas, NV, United States","Human laterality is the preference of an individual to use one side of their body rather than the other. Ocular dominance, also known as eye dominance, is one type of laterality in that eye dominance affects human visual perception. This study explores how ocular human laterality is associated with decision making in determining one choice from the left or right and top or bottom. Two simple tasks are developed running in a virtual environment in which a user wears a head-mounted display and observes two 3D ball shape objects that one moves to the left and the other moves to the right side of their eyes. The user performs the same task as the direction of the ball movement is changed to up and down. An eye-tracking device is used to measure the users’ decision on the two tasks by tracking down their eye movements. A user study conducted with a total of 20 college students, ages ranged from 18 to 30 years old, revealed that there was a significant relationship between eye dominance and their decision making. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2020.","Eye dominance; Eye-tracking; Head-mounted display; Human laterality; Ocular dominance; Virtual environments; Visual perception","Decision making; Eye movements; Eye tracking; Helmet mounted displays; Human engineering; Students; Wearable technology; College students; Eye dominance; Eye tracking devices; Head mounted displays; Human laterality; Human visual perception; Ocular dominance; Running-in; User experience",Conference Paper,"Final","",Scopus,2-s2.0-85088745752
"Almenara C.A., Aimé A., Maïano C.","54384963700;6508344276;6506008043;","Effect of Online Weight Loss Advertising in Young Women with Body Dissatisfaction: An Experimental Protocol Using Eye-Tracking and Facial Electromyography",2020,"Communications in Computer and Information Science","1226 CCIS",,,"139","148",,,"10.1007/978-3-030-50732-9_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088740676&doi=10.1007%2f978-3-030-50732-9_19&partnerID=40&md5=17c556a1414eef124f8c72d398eeee02","Universidad Peruana de Ciencias Aplicadas, Av. Alameda San Marcos, S/N, Lima, 15067, Peru; Université du Québec en Outaouais, 5, Rue Saint-Joseph, Saint-Jérôme, QC  J7Z 0B7, Canada","Almenara, C.A., Universidad Peruana de Ciencias Aplicadas, Av. Alameda San Marcos, S/N, Lima, 15067, Peru; Aimé, A., Université du Québec en Outaouais, 5, Rue Saint-Joseph, Saint-Jérôme, QC  J7Z 0B7, Canada; Maïano, C., Université du Québec en Outaouais, 5, Rue Saint-Joseph, Saint-Jérôme, QC  J7Z 0B7, Canada","The weight loss industry is projected to reach USD$ 278.95 billion worldwide by 2023. Weight loss companies devote a large part of their budget for advertising their products. Unfortunately, as revealed by the Federal Trade Commission (FTC), there are many deceptive ads. The effect of weight loss advertising on consumer’s diet and eating behavior is so large that it has been proposed a causal relationship between advertising and diet. Adolescents, women with appearance concerns, and obese people, are the most vulnerable consumers for this kind of advertising. Within the Internet, most weight loss products are advertised under algorithmic rules. This algorithmic regulation refers to the online advertising being established by a series of rules (i.e., algorithms). These algorithms collect information about our online identity and behavior (e.g., sociodemographic characteristics, online searches we do, online content we download, “liked” content, etc.), to personalize the content displayed while we browse the Internet. Because of it, this algorithmic regulation has been described as a “filter bubble”, because most content we see on the Internet is reflecting our idiosyncratic interests, desires, and needs. Following this paradigm, this study presents a research protocol to experimentally examine the effect of online weight loss advertising in the attention (using eye-tracking) and physiological response (using facial electromyography) of women with different levels of body dissatisfaction. The protocol describes the methodology for: participants’ recruitment; collecting weight loss ads; and the experimental study, which includes the stimuli (ads) and the responses (eye fixations and facial muscles activity). © 2020, Springer Nature Switzerland AG.","Algorithmic regulation; Body dissatisfaction; Eye-tracking; Facial electromyography; Internet; Online advertising; Weight loss","Budget control; Consumer behavior; Human computer interaction; Physiological models; Causal relationships; Experimental protocols; Facial electromyographies; Federal Trade commissions; Online advertising; Physiological response; Socio-demographic characteristics; Weight loss products; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85088740676
"Bruno A., Gugliuzza F., Pirrone R., Ardizzone E.","7102246682;57196279194;6603614874;7003271888;","A Multi-Scale Colour and Keypoint Density-Based Approach for Visual Saliency Detection",2020,"IEEE Access","8",,"9131794","121330","121343",,4,"10.1109/ACCESS.2020.3006700","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088295485&doi=10.1109%2fACCESS.2020.3006700&partnerID=40&md5=b851451e44255249cb8f552f78f85074","National Centre for Computer Animation (NCCA), Bournemouth University, Poole, United Kingdom; Dipartimento di Fisica e Chimica 'Emilio Segrè' (DIFC), Università degli Studi di Palermo, Palermo, Italy; Dipartimento di Ingegneria, Università degli Studi di Palermo, Palermo, Italy","Bruno, A., National Centre for Computer Animation (NCCA), Bournemouth University, Poole, United Kingdom; Gugliuzza, F., Dipartimento di Fisica e Chimica 'Emilio Segrè' (DIFC), Università degli Studi di Palermo, Palermo, Italy; Pirrone, R., Dipartimento di Ingegneria, Università degli Studi di Palermo, Palermo, Italy; Ardizzone, E., Dipartimento di Ingegneria, Università degli Studi di Palermo, Palermo, Italy","In the first seconds of observation of an image, several visual attention processes are involved in the identification of the visual targets that pop-out from the scene to our eyes. Saliency is the quality that makes certain regions of an image stand out from the visual field and grab our attention. Saliency detection models, inspired by visual cortex mechanisms, employ both colour and luminance features. Furthermore, both locations of pixels and presence of objects influence the Visual Attention processes. In this paper, we propose a new saliency method based on the combination of the distribution of interest points in the image with multiscale analysis, a centre bias module and a machine learning approach. We use perceptually uniform colour spaces to study how colour impacts on the extraction of saliency. To investigate eye-movements and assess the performances of saliency methods over object-based images, we conduct experimental sessions on our dataset ETTO (Eye Tracking Through Objects). Experiments show our approach to be accurate in the detection of saliency concerning state-of-the-art methods and accessible eye-movement datasets. The performances over object-based images are excellent and consistent on generic pictures. Besides, our work reveals interesting findings on some relationships between saliency and perceptually uniform colour spaces. © 2013 IEEE.","Eye-movements; interest points; saliency map; visual attention","Behavioral research; Color; Eye movements; Eye tracking; Object tracking; Density-based approaches; Machine learning approaches; Multi scale analysis; Saliency detection; State-of-the-art methods; Visual Attention; Visual cortexes; Visual saliency detections; Image processing",Article,"Final","",Scopus,2-s2.0-85088295485
"Christofi M., Katsaros M., Kotsopoulos S.D.","57218144132;57204360483;26429692600;","Form Follows Brain Function: A Computational Mapping Approach",2020,"Procedia Manufacturing","44",,,"108","115",,1,"10.1016/j.promfg.2020.02.211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088018619&doi=10.1016%2fj.promfg.2020.02.211&partnerID=40&md5=317b2fbec661320a0b8d19334a90c8e7","National Technical University of Athens, School of Architecture, 42 Patision str, Athens, 10682, Greece","Christofi, M., National Technical University of Athens, School of Architecture, 42 Patision str, Athens, 10682, Greece; Katsaros, M., National Technical University of Athens, School of Architecture, 42 Patision str, Athens, 10682, Greece; Kotsopoulos, S.D., National Technical University of Athens, School of Architecture, 42 Patision str, Athens, 10682, Greece","The strong association between computation and the built environment gives birth to new research approaches to urban design. Although these approaches enable the implementation of urban systems based on various collections of datasets, they usually neglect human experience. To address this problem, our research identifies and quantifies spatial information and patterns of urban organization based on experience, by gathering two parallel types of user data: sensory and recollection. The presented experiment, executed in Copenhagen, Denmark with 23 participants, involves a walking task, and a recollection task. The walking task is 1 km walk from the Copenhagen Court House building to the Royal Danish Theatre through Strøget Street, while wearing eye-tracking spectacles. The recollection task includes the underlining - on a 3D photorealistic digital model - of the elements that the participants can recall from their walk. We monitor the eye movement of the participants to locally characterize the urban path by computing four spatial attributes: a) The fixations on elements (counted through iMotions Software), b) The distance from the viewer (taken by GPS points through FUMapp software), c) The position of the stimuli related to the eye-level (calculated in FUMapp), and d) The degree of spatial transformations on street intersections (through spatial variables in FUMapp). Then we compare the eye-tracking and the recollection data and we evaluate their heat maps. The originality of this research is twofold: First, for the first time a substantial amount of quantifiable optical data related to urban walking is recorded; Second, for the first time this experiment is performed on the public street. The proposed method could offer a quantifiable basis for predicting eye fixation locations, determining human centric guidelines for urban design. Additional modes of body sensors could yield even more all-encompassing body-metrics related to the urban walking experience. © 2020 The Authors.","Architectural Design; Behavioral Patterns; Eye-Tracking; Fixation; Human Experience; Saccades",,Conference Paper,"Final","",Scopus,2-s2.0-85088018619
"Hsieh A.-Y., Lo S.-K., Chiu Y.-P., Lie T.","55638265000;23091322600;55575887100;23482124200;","Do not allow pop-up ads to appear too early internet users’ browsing behaviour to pop-up ads",2020,"Behaviour and Information Technology",,,,"","",,1,"10.1080/0144929X.2020.1784282","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087341933&doi=10.1080%2f0144929X.2020.1784282&partnerID=40&md5=4eef051d8119641f0859d9855dffa3bb","Master Program of Marketing, College of Business, Chinese Culture University, Taipei, Taiwan; Department of International Trade, Chinese Culture University, Taipei, Taiwan; Department of Advertising, Chinese Culture University, Taipei, Taiwan; Department of Information Management, Yuan Ze University, Taoyuan, Taiwan","Hsieh, A.-Y., Master Program of Marketing, College of Business, Chinese Culture University, Taipei, Taiwan; Lo, S.-K., Department of International Trade, Chinese Culture University, Taipei, Taiwan; Chiu, Y.-P., Department of Advertising, Chinese Culture University, Taipei, Taiwan; Lie, T., Department of Information Management, Yuan Ze University, Taoyuan, Taiwan","This study examines the timing of pop-up advertising appearance and its effect on perceived intrusiveness, advertising irritation and advertising avoidance. Experiment was designed to build a virtual Internet environment (including the main content on the webpage and a pop-up ad) and to manipulate the timing of the pop-up advertising appearance. Participants were invited to participate in two experiments, and then assigned to a specific target browsing task; their advertising browsing activities during the task were measured. In order to measure their cognitive advertising avoidance, an eye-tracking device was utilised to gain objective and accurate psychological information. Results showed that earlier pop-up advertising appearances are associated with a lower consumer fixation count and fixation length; in contrast, pop-up advertising that appears later is associated with a higher fixation count and fixation length. This study attempts to gain more objective and accurate psychological data by using an eye-tracking device to collect information about eye movements associated with the appearance of pop-up advertising to better analyse consumer behaviours towards them. These results offer insights to Internet advertisers and Internet platform companies on how to provide more efficient Internet advertising. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.","advertising avoidance; advertising intrusiveness; eye tracking; Pop-up advertising; pop-up timing","Eye movements; Eye tracking; Browsing behaviour; Eye tracking devices; Internet advertising; Internet environment; Internet users; Pop-up ads; Marketing",Article,"Article in Press","",Scopus,2-s2.0-85087341933
"Liu M., Li Y., Liu H.","57217199438;8589964900;56450301400;","3D Gaze Estimation for Head-Mounted Eye Tracking System with Auto-Calibration Method",2020,"IEEE Access","8",,"9107144","104207","104215",,7,"10.1109/ACCESS.2020.2999633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086730972&doi=10.1109%2fACCESS.2020.2999633&partnerID=40&md5=19d72f09a87bc0df58f3bf1758bbd30c","Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, 430079, China","Liu, M., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Li, Y., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Liu, H., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, 430079, China","The general challenges of 3D gaze estimation for head-mounted eye tracking systems are inflexible marker-based calibration procedure and significant errors of depth estimation. In this paper, we propose a 3D gaze estimation with an auto-calibration method. To acquire the accurate 3D structure of the environment, an RGBD camera is applied as the scene camera of our system. By adopting the saliency detection method, saliency maps can be acquired through scene images, and 3D salient pixels in the scene are considered potential 3D calibration targets. The 3D eye model is built on the basis of eye images to determine gaze vectors. By combining 3D salient pixels and gaze vectors, the auto-calibration can be achieved with our calibration method. Finally, the 3D gaze point is obtained through the calibrated gaze vectors, and the point cloud is generated from the RGBD camera. The experimental result shows that the proposed system can achieve an average accuracy of 3.7° in the range of 1 m to 4 m indoors and 4.0° outdoors. The proposed system also presents a great improvement in depth measurement, which is sufficient for tracking users' visual attention in real scenes. © 2013 IEEE.","3D gaze estimation; Auto-calibration; Head-mounted gaze tracking system; Saliency maps","3D modeling; Behavioral research; Calibration; Cameras; Pixels; Auto calibration; Auto-calibration method; Calibration method; Calibration procedure; Depth Estimation; Head-mounted eye tracking; Saliency detection; Visual Attention; Eye tracking",Article,"Final","",Scopus,2-s2.0-85086730972
"Oue S., Yamada R., Akamatsu S.","57217177488;57202581917;57206206447;","Human performance of face recognition inferred from characteristics of observing eye movement patterns learned by hidden Markov model",2020,"Proceedings of SPIE - The International Society for Optical Engineering","11515",,"1151502","","",,,"10.1117/12.2566952","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086629305&doi=10.1117%2f12.2566952&partnerID=40&md5=807c4ac41787c8e2d89844912e86ae0c","Hosei University, 3−7−2 Kajino-cho, Koganei-shi, Tokyo, 184−8584, Japan","Oue, S., Hosei University, 3−7−2 Kajino-cho, Koganei-shi, Tokyo, 184−8584, Japan; Yamada, R., Hosei University, 3−7−2 Kajino-cho, Koganei-shi, Tokyo, 184−8584, Japan; Akamatsu, S., Hosei University, 3−7−2 Kajino-cho, Koganei-shi, Tokyo, 184−8584, Japan","We investigated the relationship between the face recognition performance of individuals and their eye movement characteristics that were measured while each subject observed the faces that were displayed on a screen. We formulated the statistical nature of their eye movements from a machine-learning perspective by applying a hidden Markov model (HMM). We used a set of computer-generated faces that included both the images of actual faces and synthetic images obtained by slightly transforming the impressions of the original faces. With these visual stimuli, we conducted a simple face recognition experiment, and subjects judged whether they had seen the faces before. We obtained a quantitative hit rate score for each stimulus and subject. We also tracked their eye movements and recorded as temporal chains their eye fixation points using an eye-tracking system. For each class of face stimulus and subject, we estimated the HMM parameters from the training samples of the eye movement. For the given eye movement data as test samples, we conducted a classification test among the pre-defined classes based on the differences of the log-likelihood values obtained from each HMM. Better discrimination of the subjects by the HMM-based classification of the eye movement data corresponded to lower face recognition scores by the subjects, suggesting that individually consistent eye movement patterns may lower the face recognition performance by humans. © 2020 SPIE.","Eye movement; Face recognition performance; Hidden Markov model","Eye tracking; Face recognition; Hidden Markov models; Imaging techniques; Classification tests; Computer generated; Eye movement datum; Eye movement patterns; Eye tracking systems; Face recognition experiment; Face recognition performance; Movement characteristics; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85086629305
"Pfeiffer J., Pfeiffer T., Meißner M., Weiß E.","19638908300;14027435500;36621114600;57219467915;","Eye-tracking-based classification of information search behavior using machine learning: Evidence from experiments in physical shops and virtual reality shopping environments",2020,"Information Systems Research","31","3",,"675","691",,7,"10.1287/ISRE.2019.0907","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086485830&doi=10.1287%2fISRE.2019.0907&partnerID=40&md5=06163e2e90f2536e5af373ee2564610b","Justus Liebig University Giessen, Gießen, 35394, Germany; University of Applied Sciences Emden/Leer, Emden, 26723, Germany; Zeppelin University, Friedrichshafen, 88045, Germany; Karlsruhe Institute of Technology, Karlsruhe, 76133, Germany","Pfeiffer, J., Justus Liebig University Giessen, Gießen, 35394, Germany; Pfeiffer, T., University of Applied Sciences Emden/Leer, Emden, 26723, Germany; Meißner, M., Zeppelin University, Friedrichshafen, 88045, Germany; Weiß, E., Karlsruhe Institute of Technology, Karlsruhe, 76133, Germany","Classifying information search behavior helps tailor recommender systems to individual customers' shopping motives. But how can we identify these motives without requiring users to exert too much effort? Our research goal is to demonstrate that eye tracking can be used at the point of sale to do so. We focus on two frequently investigated shopping motives: Goal-directed and exploratory search. To train and test a prediction model, we conducted two eye-tracking experiments in front of supermarket shelves. The first experiment was carried out in immersive virtual reality; the second, in physical reality-in other words, as a field study in a real supermarket. We conducted a virtual reality study, because recently launched virtual shopping environments suggest that there is great interest in using this technology as a retail channel. Our empirical results show that support vector machines allow the correct classification of search motives with 80% accuracy in virtual reality and 85% accuracy in physical reality. Our findings also imply that eye movements allow shopping motives to be identified relatively early in the search process: Our models achieve 70% prediction accuracy after only 15 seconds in virtual reality and 75% in physical reality. Applying an ensemble method increases the prediction accuracy substantially, to about 90%. Consequently, the approach that we propose could be used for the satisfiable classification of consumers in practice. Furthermore, both environments' best predictor variables overlap substantially. This finding provides evidence that in virtual reality, information search behavior might be similar to the one used in physical reality. Finally, we also discuss managerial implications for retailers and companies that are planning to use our technology to personalize a consumer assistance system. © 2020 The Author(s).","Decision support systems; Electronic commerce; Exploratory search; Field experiments; Goal-directed search; Laboratory experiments; Mobile eye tracking; Recommender system; Virtual reality","Classification (of information); E-learning; Eye movements; Forecasting; Information retrieval; Machine shops; Predictive analytics; Retail stores; Sales; Search engines; Support vector machines; Virtual reality; Assistance system; Exploratory search; Immersive virtual reality; Individual customers; Information search behavior; Managerial implications; Prediction accuracy; Predictor variables; Eye tracking",Article,"Final","",Scopus,2-s2.0-85086485830
"Kaczorowska M., Wawrzyk M., Plechawska-Wójcik M.","57194211584;57194205056;41262115200;","Binary Classification of Cognitive Workload Levels with Oculography Features",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12133 LNCS",,,"243","254",,,"10.1007/978-3-030-47679-3_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086137733&doi=10.1007%2f978-3-030-47679-3_21&partnerID=40&md5=69af90da14fb9b3ff80ee3daf897b2c5","Lublin University of Technology, Nadbystrzycka 36B, Lublin, 20-618, Poland","Kaczorowska, M., Lublin University of Technology, Nadbystrzycka 36B, Lublin, 20-618, Poland; Wawrzyk, M., Lublin University of Technology, Nadbystrzycka 36B, Lublin, 20-618, Poland; Plechawska-Wójcik, M., Lublin University of Technology, Nadbystrzycka 36B, Lublin, 20-618, Poland","Assessment of cognitive workload level is important to understand human mental fatigue, especially in the case of performing intellectual tasks. The paper presents a case study on binary classification of cognitive workload levels. The dataset was received from two versions of the digit symbol substitution test (DSST), conducted on 26 healthy volunteers. A screen-based eye tracker was applied during an examination gathering oculographic data. DSST test results such as total number of matches and error ratio were also applied. Classification was performed with several different machine learning models. The best accuracy (97%) was achieved with linear SVM classifier. The final dataset for classification was based on nine features selected with the Fisher score feature selection method. © 2020, Springer Nature Switzerland AG.","Binary classification; Cognitive workload; Eye-tracking signal; SVM","Eye tracking; Industrial management; Information management; Information systems; Information use; Statistical tests; Support vector machines; Binary classification; Cognitive workloads; Eye trackers; Feature selection methods; Fisher score; Healthy volunteers; Machine learning models; Mental fatigue; Classification (of information)",Conference Paper,"Final","",Scopus,2-s2.0-85086137733
"Saravanakumar D., Ramasubba Reddy M.","57205753234;57213457220;","A Brain Computer Interface Based Visual Keyboard System Using SSVEP and Electrooculogram",2020,"Communications in Computer and Information Science","1203 CCIS",,,"64","74",,,"10.1007/978-981-15-4301-2_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085726330&doi=10.1007%2f978-981-15-4301-2_6&partnerID=40&md5=f0ec6f4d6e0d5edb9ace40417ab1814e","Department of Applied Mechanics, Biomedical Group, Indian Institute of Technology Madras, Chennai, India","Saravanakumar, D., Department of Applied Mechanics, Biomedical Group, Indian Institute of Technology Madras, Chennai, India; Ramasubba Reddy, M., Department of Applied Mechanics, Biomedical Group, Indian Institute of Technology Madras, Chennai, India","This study aims to design a steady-state visual evoked potential (SSVEP) based, on-screen keyboard/speller system along with the integration of electrooculogram (EOG). The characters/targets were designed using the pattern reversal square checkerboard flickering visual stimuli. In this study, twenty-three characters were randomly selected and their corresponding visual stimuli were designed using five frequencies (6, 6.667, 7.5, 8.57 and 10 Hz). The keyboard layout was divided into nine regions and each region was identified by using the subject’s eye gaze information with the help of EOG data. The information from the EOG was used to locate the area on the visual keyboard/display, where the subject is looking. The region identification helps to use the same frequency valued visual stimuli more than once on the keyboard layout. In this proposed study, more targets were designed using less number of visual stimulus frequencies by integrating EOG with the SSVEP keyboard system. The multi-threshold algorithm and extended multivariate synchronization index (EMSI) method were used for eye gaze detection and SSVEP frequency recognition respectively. Ten healthy subjects were recruited for validating the proposed visual keyboard system. © Springer Nature Singapore Pte Ltd. 2020.","Brain computer interface (BCI); Electro-oculogram (EOG); Extended multivariate synchronization index (EMSI); Steady-state visual evoked potential (SSVEP); Visual keyboard","Brain computer interface; Computer keyboards; Heuristic algorithms; Image segmentation; Learning algorithms; Electro-oculogram; Eye gaze detection; Healthy subjects; Keyboard layout; On-screen keyboard; Region identification; Steady state visual evoked potentials; Synchronization index; Machine learning",Conference Paper,"Final","",Scopus,2-s2.0-85085726330
"Zhou X., Jiang J., Liu Q., Fang J., Chen S., Cai H.","55743240400;57211817476;57202720113;37050903500;24491760700;56763253600;","Learning a 3D Gaze Estimator with Adaptive Weighted Strategy",2020,"IEEE Access","8",,"9079544","82142","82152",,,"10.1109/ACCESS.2020.2990685","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084929459&doi=10.1109%2fACCESS.2020.2990685&partnerID=40&md5=2bb466304ae455f7393019a01b630a61","College of Electrical and Information Engineering, Quzhou University, Quzhou, 324000, China; College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, 310023, China; School of Computer Communication and Engineering, Tianjin University of Technology, Tianjin, 300384, China; Department of Computer Science, Loughborough University, Loughborough, LE11 3TU, United Kingdom","Zhou, X., College of Electrical and Information Engineering, Quzhou University, Quzhou, 324000, China, College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, 310023, China; Jiang, J., College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, 310023, China; Liu, Q., College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, 310023, China; Fang, J., College of Electrical and Information Engineering, Quzhou University, Quzhou, 324000, China; Chen, S., College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, 310023, China, School of Computer Communication and Engineering, Tianjin University of Technology, Tianjin, 300384, China; Cai, H., Department of Computer Science, Loughborough University, Loughborough, LE11 3TU, United Kingdom","As a method of predicting the target's attention distribution, gaze estimation plays an important role in human-computer interaction. In this paper, we learn a 3D gaze estimator with adaptive weighted strategy to get the mapping from the complete images to the gaze vector. We select the both eyes, the complete face and their fusion features as the input of the regression model of gaze estimator. Considering that the different areas of the face have different contributions on the results of gaze estimation under free head movement, we design a new learning strategy for the regression net. To improve the efficiency of the regression model to a great extent, we propose a weighted network that can adjust the learning strategy of the regression net adaptively. Experimental results conducted on the MPIIGaze and EyeDiap datasets demonstrate that our method can achieve superior performance compared with other state-of-the-art 3D gaze estimation methods. © 2013 IEEE.","adaptive strategy; Gaze estimation; regression model; weighted network","Human computer interaction; Regression analysis; Free-head; Fusion features; Gaze estimation; Learning strategy; Regression model; State of the art; Weighted networks; Learning systems",Article,"Final","",Scopus,2-s2.0-85084929459
"Pappas I.O., Sharma K., Mikalef P., Giannakos M.N.","55387371600;55903734200;42761793700;36462343600;","How Quickly Can We Predict Users’ Ratings on Aesthetic Evaluations of Websites? Employing Machine Learning on Eye-Tracking Data",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12067 LNCS",,,"429","440",,1,"10.1007/978-3-030-45002-1_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084924819&doi=10.1007%2f978-3-030-45002-1_37&partnerID=40&md5=66074420f7e4da3560188fe5dcb83122","University of Agder, Kristiansand, 4639, Norway; Norwegian University of Science and Technology, Trondheim, 7491, Norway","Pappas, I.O., University of Agder, Kristiansand, 4639, Norway, Norwegian University of Science and Technology, Trondheim, 7491, Norway; Sharma, K., Norwegian University of Science and Technology, Trondheim, 7491, Norway; Mikalef, P., Norwegian University of Science and Technology, Trondheim, 7491, Norway; Giannakos, M.N., Norwegian University of Science and Technology, Trondheim, 7491, Norway","This study examines how quickly we can predict users’ ratings on visual aesthetics in terms of simplicity, diversity, colorfulness, craftsmanship. To predict users’ ratings, first we capture gaze behavior while looking at high, neutral, and low visually appealing websites, followed by a survey regarding user perceptions on visual aesthetics towards the same websites. We conduct an experiment with 23 experienced users in online shopping, capture gaze behavior and through employing machine learning we examine how fast we can accurately predict their ratings. The findings show that after 25 s we can predict ratings with an error rate ranging from 9% to 11% depending on which facet of visual aesthetic is examined. Furthermore, within the first 15 s we can have a good and sufficient prediction for simplicity and colorfulness, with error rates 11% and 12% respectively. For diversity and craftsmanship, 20 s are needed to get a good and sufficient prediction similar to the one from 25 s. The findings indicate that we need more than 10 s of viewing time to be able to accurately capture perceptions on visual aesthetics. The study contributes by offering new ways for designing systems that will take into account users’ gaze behavior in an unobtrusive manner and will be able inform researchers and designers about their perceptions of visual aesthetics. © 2020, IFIP International Federation for Information Processing.","Aesthetics; Artificial Intelligence; Design; E-commerce; Eye-tracking; Machine learning","Electronic commerce; Forecasting; Information use; Machine learning; Websites; Designing systems; Error rate; Gaze behavior; Online shopping; User perceptions; Visual Aesthetics; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85084924819
"Xu B., Li X., Wang Y.","56780818100;57213267610;56032563800;","2D-3D Autostereoscopic Switchable Display Based on Multi-distance Dynamic Directional Backlight",2020,"Smart Innovation, Systems and Technologies","179",,,"475","484",,1,"10.1007/978-981-15-3863-6_52","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084828556&doi=10.1007%2f978-981-15-3863-6_52&partnerID=40&md5=bd12a43ba73c8724c87515c83b1f83cc","School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China","Xu, B., School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China; Li, X., School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China; Wang, Y., School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China","A 2D-3D autostereoscopic switchable display based on multi-distance dynamic directional backlight is introduced in this paper. The working principle and design process of the system are described in detail as well. Our prototype consists of a multi-distance dynamic directional backlight, LCD panel, and eye tracking system. The multi-distance dynamic directional backlight includes an LED array and corresponding driving circuit. The LEDs in the array are controlled in synchronization with the vertical synchronization signal of the LCD panel. The prototype can measure stereoacuity of the viewer’s eyes in different viewing distance with full resolution, low crosstalk, and 2D-3D compatibility. Combined with mechanical structure and eye tracking technique, the system can measure stereoacuity at 0.4, 1.0, and 5.0 m. Also, a multi-distance autostereoscopic display prototype is fabricated and demonstrated experimentally. It shows that the viewer can perceive high-quality 3D images in different distance. Without the auxiliary glasses, the crosstalk is about 5%. Our prototype is also compatible with active shutter glasses. In conjunction with active shutter glasses, the crosstalk is about 1%, just next to that of commercial display (View Sonic VX2268WM). © Springer Nature Singapore Pte Ltd. 2020.","Autostereoscopic display; Crosstalk; Dynamic directional backlight; Multi-Distance","Crosstalk; Eye tracking; Glass; Imaging techniques; Light emitting diodes; Optical shutters; Stereo image processing; Three dimensional computer graphics; Auto stereoscopic; Auto-stereoscopic display; Driving circuits; Eye tracking systems; Full resolutions; Mechanical structures; Synchronization signals; Viewing distance; Display devices",Conference Paper,"Final","",Scopus,2-s2.0-85084828556
"Li X.-L., Xu B., Wu Q.-Q., Wang Y.-Q.","57213267610;56780818100;56953855200;56032563800;","Autostereoscopic 3D Display System Based on Lenticular Lens and Quantum-Dot Film",2020,"Smart Innovation, Systems and Technologies","179",,,"425","433",,,"10.1007/978-981-15-3863-6_47","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084810338&doi=10.1007%2f978-981-15-3863-6_47&partnerID=40&md5=9aa1239630f7908c1eae8de9b75c54f8","School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China","Li, X.-L., School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China; Xu, B., School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China; Wu, Q.-Q., School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China; Wang, Y.-Q., School of Electronic Science and Engineering, Nanjing University, No. 163 Xianlin Avenue, Qixia District, Nanjing, 210023, China","An autostereoscopic 3D display system based on the lenticular lens and Quantum-Dot (QD) film is presented in this paper. Wide color gamut, multi-view, and high brightness are achieved in this system. In addition, it can be easily realized on flat-panel display, which is the mainstream of display nowadays. Multiple viewing positions are available and with the assistance of eye tracking device, one pair of parallax images can be projected to the viewer’s left and right eyes, respectively. In order to value the performance of the backlight system, a prototype (10.1 inches) is fabricated and demonstrated experimentally. The structure of the system and working principle are explained in detail. The error of the exit pupils is analyzed. Compared with the traditional display platform, the color gamut of this system benefited from the use of QD material can be extended to 78%. The crosstalk of our prototype is about 7.7% and motion resolution is 30 mm. © Springer Nature Singapore Pte Ltd. 2020.","Autostereoscopic display; Lenticular lens; Multi-view; Quantum-Dot film","Eye tracking; Flat panel displays; Geometrical optics; Imaging techniques; Nanocrystals; Semiconductor quantum dots; Stereo image processing; Three dimensional computer graphics; Autostereoscopic 3D displays; Backlight systems; Display platform; Eye tracking devices; High brightness; Motion resolution; Quantum dot films; Wide color gamut; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85084810338
"Jäger L.A., Makowski S., Prasse P., Liehr S., Seidler M., Scheffer T.","56503248300;57205689294;55376814000;24278384100;57216829527;55122621900;","Deep Eyedentification: Biometric Identification Using Micro-movements of the Eye",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11907 LNAI",,,"299","314",,1,"10.1007/978-3-030-46147-8_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084802103&doi=10.1007%2f978-3-030-46147-8_18&partnerID=40&md5=daaf3488e007c592636d14f4ae1aad69","Department of Computer Science, University of Potsdam, Potsdam, Germany; Berlin, Germany","Jäger, L.A., Department of Computer Science, University of Potsdam, Potsdam, Germany; Makowski, S., Department of Computer Science, University of Potsdam, Potsdam, Germany; Prasse, P., Department of Computer Science, University of Potsdam, Potsdam, Germany; Liehr, S., Berlin, Germany; Seidler, M., Department of Computer Science, University of Potsdam, Potsdam, Germany; Scheffer, T., Department of Computer Science, University of Potsdam, Potsdam, Germany","We study involuntary micro-movements of the eye for biometric identification. While prior studies extract lower-frequency macro-movements from the output of video-based eye-tracking systems and engineer explicit features of these macro-movements, we develop a deep convolutional architecture that processes the raw eye-tracking signal. Compared to prior work, the network attains a lower error rate by one order of magnitude and is faster by two orders of magnitude: it identifies users accurately within seconds. © Springer Nature Switzerland AG 2020.","Biometrics; Deep learning; Eye movements; Eye-tracking; Machine learning; Ocular micro-movements","Anthropometry; Biometrics; Eye movements; Machine learning; Biometric identifications; Error rate; Lower frequencies; Micro-movements; Orders of magnitude; Video-based eye-tracking; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85084802103
"Abdelwahab A., Landwehr N.","57205686686;10244371900;","Quantile Layers: Statistical Aggregation in Deep Neural Networks for Eye Movement Biometrics",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11907 LNAI",,,"332","348",,,"10.1007/978-3-030-46147-8_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084793074&doi=10.1007%2f978-3-030-46147-8_20&partnerID=40&md5=691d60ff05070a630c87e589a415816a","Leibniz Institute of Agricultural Engineering and Bioeconomy e.V. (ATB), Potsdam, Germany","Abdelwahab, A., Leibniz Institute of Agricultural Engineering and Bioeconomy e.V. (ATB), Potsdam, Germany; Landwehr, N., Leibniz Institute of Agricultural Engineering and Bioeconomy e.V. (ATB), Potsdam, Germany","Human eye gaze patterns are highly individually characteristic. Gaze patterns observed during the routine access of a user to a device or document can therefore be used to identify subjects unobtrusively, that is, without the need to perform an explicit verification such as entering a password. Existing approaches to biometric identification from gaze patterns segment raw gaze data into short, local patterns called saccades and fixations. Subjects are then identified by characterizing the distribution of these patterns or deriving hand-crafted features for them. In this paper, we follow a different approach by training deep neural networks directly on the raw gaze data. As the distribution of short, local patterns has been shown to be particularly informative for distinguishing subjects, we introduce a parameterized and end-to-end learnable statistical aggregation layer called the quantile layer that enables the network to explicitly fit the distribution of filter activations in preceding layers. We empirically show that deep neural networks with quantile layers outperform existing probabilistic and feature-based methods for identifying subjects based on eye movements by a large margin. © Springer Nature Switzerland AG 2020.","Biometry; Deep learning; Eye movements","Biometrics; Deep learning; Eye movements; Learning systems; Multilayer neural networks; Network layers; Biometric identifications; End to end; Feature-based method; Human eye; Large margins; Local patterns; Parameterized; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85084793074
"Ali A., Kim Y.-G.","57216564371;24081003200;","Deep Fusion for 3D Gaze Estimation from Natural Face Images Using Multi-Stream CNNs",2020,"IEEE Access","8",,"9062592","69212","69221",,2,"10.1109/ACCESS.2020.2986815","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083892421&doi=10.1109%2fACCESS.2020.2986815&partnerID=40&md5=9af37973f948c91545a4a08a372135b8","Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea","Ali, A., Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea; Kim, Y.-G., Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea","Over the last few decades, eye gaze estimation techniques have been thoroughly investigated by many researchers. However, predicting a 3D gaze from a 2D natural image remains challenging because it has to deal with several issues such as diverse head positions, face shape transformation, illumination variations, and subject individuality. Many previous studies employ convolutional neural networks (CNNs) for this task, and yet the accuracy needs improvement for its practical use. In this paper, we propose a 3D gaze estimation framework based on the data science perspective: First, a novel neural network architecture is designed to exploit every possible visual attribute such as the states of both eyes and the head position, including several augmentations; secondly, the data fusion method is utilized by incorporating multiple gaze datasets. Extensive experiments were carried out using two standard eye gaze datasets, including comparative analysis. The experimental results suggest that our method outperforms state-of-the-art with 2.8 degrees for MPIIGaze and 3.05 degrees for EYEDIAP dataset, respectively, indicating that it has a potential for real applications. © 2013 IEEE.","convolutional neural networks; data fusion; EYEDIAP; Gaze estimation; MPIIGaze","Data fusion; Network architecture; Comparative analysis; Data fusion methods; Gaze estimation; Illumination variation; Novel neural network; Real applications; State of the art; Visual attributes; Convolutional neural networks",Article,"Final","",Scopus,2-s2.0-85083892421
"Gjoreski M., Gams M.Ž., Luštrek M., Genc P., Garbas J.-U., Hassan T.","56470741800;35617835400;12796966600;57209510645;8367980700;57193091146;","Machine Learning and End-to-End Deep Learning for Monitoring Driver Distractions from Physiological and Visual Signals",2020,"IEEE Access","8",,"9062481","70590","70603",,13,"10.1109/ACCESS.2020.2986810","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083889984&doi=10.1109%2fACCESS.2020.2986810&partnerID=40&md5=53469c45f932b96cb1ddd88970e9d6af","Jožef Stefan Institute, Ljubljana, 1000, Slovenia; Jožef Stefan Postgraduate School, Ljubljana, 1000, Slovenia; Intelligent Systems Group, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, 91058, Germany","Gjoreski, M., Jožef Stefan Institute, Ljubljana, 1000, Slovenia, Jožef Stefan Postgraduate School, Ljubljana, 1000, Slovenia; Gams, M.Ž., Jožef Stefan Institute, Ljubljana, 1000, Slovenia, Jožef Stefan Postgraduate School, Ljubljana, 1000, Slovenia; Luštrek, M., Jožef Stefan Institute, Ljubljana, 1000, Slovenia, Jožef Stefan Postgraduate School, Ljubljana, 1000, Slovenia; Genc, P., Intelligent Systems Group, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, 91058, Germany; Garbas, J.-U., Intelligent Systems Group, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, 91058, Germany; Hassan, T., Intelligent Systems Group, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, 91058, Germany","It is only a matter of time until autonomous vehicles become ubiquitous; however, human driving supervision will remain a necessity for decades. To assess the driver's ability to take control over the vehicle in critical scenarios, driver distractions can be monitored using wearable sensors or sensors that are embedded in the vehicle, such as video cameras. The types of driving distractions that can be sensed with various sensors is an open research question that this study attempts to answer. This study compared data from physiological sensors (palm electrodermal activity (pEDA), heart rate and breathing rate) and visual sensors (eye tracking, pupil diameter, nasal EDA (nEDA), emotional activation and facial action units (AUs)) for the detection of four types of distractions. The dataset was collected in a previous driving simulation study. The statistical tests showed that the most informative feature/modality for detecting driver distraction depends on the type of distraction, with emotional activation and AUs being the most promising. The experimental comparison of seven classical machine learning (ML) and seven end-to-end deep learning (DL) methods, which were evaluated on a separate test set of 10 subjects, showed that when classifying windows into distracted or not distracted, the highest F1-score of 79% was realized by the extreme gradient boosting (XGB) classifier using 60-second windows of AUs as input. When classifying complete driving sessions, XGB's F1-score was 94%. The best-performing DL model was a spectro-temporal ResNet, which realized an F1-score of 75% when classifying segments and an F1-score of 87% when classifying complete driving sessions. Finally, this study identified and discussed problems, such as label jitter, scenario overfitting and unsatisfactory generalization performance, that may adversely affect related ML approaches. © 2013 IEEE.","deep learning; driver distraction; facial expressions; Machine learning; sensors","Adaptive boosting; Chemical activation; Classification (of information); Eye tracking; Learning systems; Physiology; Vehicles; Video cameras; Wearable sensors; Driver distractions; Driving distractions; Driving simulation; Electrodermal activity; Experimental comparison; Generalization performance; Physiological sensors; Research questions; Deep learning",Article,"Final","",Scopus,2-s2.0-85083889984
"Han S.Y., Kwon H.J., Kim Y., Cho N.I.","57193417343;57202583011;57194873756;7201718669;","Noise-Robust Pupil Center Detection through CNN-Based Segmentation with Shape-Prior Loss",2020,"IEEE Access","8",,"9055424","64739","64749",,8,"10.1109/ACCESS.2020.2985095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083699962&doi=10.1109%2fACCESS.2020.2985095&partnerID=40&md5=6a3166d863e409e4a0b9a04c99484c56","Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, 08826, South Korea","Han, S.Y., Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, 08826, South Korea; Kwon, H.J., Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, 08826, South Korea; Kim, Y., Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, 08826, South Korea; Cho, N.I., Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, 08826, South Korea","Detecting the pupil center plays a key role in human-computer interaction, especially for gaze tracking. The conventional deep learning-based method for this problem is to train a convolutional neural network (CNN), which takes the eye image as the input and gives the pupil center as a regression result. In this paper, we propose an indirect use of the CNN for the task, which first segments the pupil region by a CNN as a classification problem, and then finds the center of the segmented region. This is based on the observation that CNN works more robustly for the pupil segmentation than for the pupil center-point regression when the inputs are noisy IR images. Specifically, we use the UNet model for the segmentation of pupil regions in IR images and then find the pupil center as the center of mass of the segment. In designing the loss function for the segmentation, we propose a new loss term that encodes the convex shape-prior for enhancing the robustness to noise. Precisely, we penalize not only the deviation of each predicted pixel from the ground truth label but also the non-convex shape of pupils caused by the noise and reflection. For the training, we make a new dataset of 111,581 images with hand-labeled pupil regions from 29 IR eye video sequences. We also label commonly used datasets (ExCuSe and ElSe dataset) that are considered real-world noisy ones to validate our method. Experiments show that the proposed method performs better than the conventional methods that directly find the pupil center as a regression result. © 2013 IEEE.","Convex shape prior; deep learning; pupil segmentation; U-Net","Convolutional neural networks; Deep learning; Eye tracking; Human computer interaction; Infrared imaging; Regression analysis; Center of mass; Conventional methods; Learning-based methods; Non-convex shapes; Pupil segmentation; Robustness to noise; Segmented regions; Video sequences; Image segmentation",Article,"Final","",Scopus,2-s2.0-85083699962
"Selim M., Firintepe A., Pagani A., Stricker D.","56785358000;57216462432;23398181100;6701489212;","Autopose: Large-scale automotive driver head pose and gaze dataset with deep head orientation baseline",2020,"VISIGRAPP 2020 - Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","4",,,"599","606",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083558622&partnerID=40&md5=781b51a8be76b3b98a8ac3c4592e3066","German Research Center for Artificial Intelligence (DFKI), Trippstadter Str. 122, Kaiserslautern, Germany; BMW Group, Munich, Germany","Selim, M., German Research Center for Artificial Intelligence (DFKI), Trippstadter Str. 122, Kaiserslautern, Germany; Firintepe, A., BMW Group, Munich, Germany; Pagani, A., German Research Center for Artificial Intelligence (DFKI), Trippstadter Str. 122, Kaiserslautern, Germany; Stricker, D., German Research Center for Artificial Intelligence (DFKI), Trippstadter Str. 122, Kaiserslautern, Germany","In computer vision research, public datasets are crucial to objectively assess new algorithms. By the wide use of deep learning methods to solve computer vision problems, large-scale datasets are indispensable for proper network training. Various driver-centered analysis depend on accurate head pose and gaze estimation. In this paper, we present a new large-scale dataset, AutoPOSE. The dataset provides ∼ 1.1M IR images taken from the dashboard view, and ∼ 315K from Kinect v2 (RGB, IR, Depth) taken from center mirror view. AutoPOSE’s ground truth -head orientation and position- was acquired with a sub-millimeter accurate motion capturing system. Moreover, we present a head orientation estimation baseline with a state-of-the-art method on our AutoPOSE dataset. We provide the dataset as a downloadable package from a public website. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Deep Learning; Driving; Eye Gaze; Head Pose Estimation; Infrared Camera; Kinect V2","Computer graphics; Computer vision; Deep learning; Infrared imaging; Learning systems; Accurate motion; Computer vision problems; Head orientation estimations; Large-scale dataset; Large-scale datasets; Learning methods; Network training; State-of-the-art methods; Large dataset",Conference Paper,"Final","",Scopus,2-s2.0-85083558622
"Dhingra N., Hirt C., Angst M., Kunz A.","57209979094;55614615100;57225469533;7005939819;","Eye gaze tracking for detecting non-verbal communication in meeting environments",2020,"VISIGRAPP 2020 - Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","2",,,"239","246",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083521690&partnerID=40&md5=e797c8f95818f9b633968284285dceab","Innovation Center Virtual Reality, ETH Zurich, Zurich, Switzerland","Dhingra, N., Innovation Center Virtual Reality, ETH Zurich, Zurich, Switzerland; Hirt, C., Innovation Center Virtual Reality, ETH Zurich, Zurich, Switzerland; Angst, M., Innovation Center Virtual Reality, ETH Zurich, Zurich, Switzerland; Kunz, A., Innovation Center Virtual Reality, ETH Zurich, Zurich, Switzerland","Non-verbal communication in a team meeting is important to understand the essence of the conversation. Among other gestures, eye gaze shows the focus of interest on a common workspace and can also be used for an interpersonal synchronisation. If this non-verbal information is missing and or cannot be perceived by blind and visually impaired people (BVIP), they would lack important information to get fully immersed in the meeting and may feel alienated in the course of the discussion. Thus, this paper proposes an automatic system to track where a sighted person is gazing at. We use the open source software 'OpenFace' and develop it as an eye tracker by using a support vector regressor to make it work similarly to commercially available expensive eye trackers. We calibrate OpenFace using a desktop screen with a 2×3 box matrix and conduct a user study with 28 users on a big screen (161.7 cm × 99.8 cm × 11.5 cm) with a 1×5 box matrix. In this user study, we compare the results of our developed algorithm for OpenFace to an SMI RED 250 eye tracker. The results showed that our work achieved an overall relative accuracy of 58.54%. Copyright © 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","Data processing; Eye gaze; Eye tracker; Machine learning; OpenFace; Regression; Support vector machine","Computer graphics; Computer vision; Open source software; Open systems; Automatic systems; Blind and visually impaired; Eye gaze tracking; Non-verbal communications; Non-verbal information; Relative accuracy; Support vector regressor; Team meetings; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85083521690
"Cheng Y., Zhang X., Lu F., Sato Y.","57220572010;57142162900;54956194300;35230954300;","Gaze Estimation by Exploring Two-Eye Asymmetry",2020,"IEEE Transactions on Image Processing","29",,"9050633","5259","5272",,8,"10.1109/TIP.2020.2982828","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082921437&doi=10.1109%2fTIP.2020.2982828&partnerID=40&md5=96fcc7405e8bbd76cd9c02f775a9590c","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science, ETH Zurich, Zurich, Switzerland; Institute of Industrial Science, University of Tokyo, Tokyo, Japan","Cheng, Y., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Zhang, X., Department of Computer Science, ETH Zurich, Zurich, Switzerland; Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Sato, Y., Institute of Industrial Science, University of Tokyo, Tokyo, Japan","Eye gaze estimation is increasingly demanded by recent intelligent systems to facilitate a range of interactive applications. Unfortunately, learning the highly complicated regression from a single eye image to the gaze direction is not trivial. Thus, the problem is yet to be solved efficiently. Inspired by the two-eye asymmetry as two eyes of the same person may appear uneven, we propose the face-based asymmetric regression-evaluation network (FARE-Net) to optimize the gaze estimation results by considering the difference between left and right eyes. The proposed method includes one face-based asymmetric regression network (FAR-Net) and one evaluation network (E-Net). The FAR-Net predicts 3D gaze directions for both eyes and is trained with the asymmetric mechanism, which asymmetrically weights and sums the loss generated by two-eye gaze directions. With the asymmetric mechanism, the FAR-Net utilizes the eyes that can achieve high performance to optimize network. The E-Net learns the reliabilities of two eyes to balance the learning of the asymmetric mechanism and symmetric mechanism. Our FARE-Net achieves leading performances on MPIIGaze, EyeDiap and RT-Gene datasets. Additionally, we investigate the effectiveness of FARE-Net by analyzing the distribution of errors and ablation study. © 1992-2012 IEEE.","asymmetric regression; evaluation network; eye appearance; Gaze estimation","Intelligent systems; Asymmetric mechanisms; asymmetric regression; eye appearance; Eye images; Eye-gaze; Gaze direction; Gaze estimation; Interactive applications; Regression analysis",Article,"Final","",Scopus,2-s2.0-85082921437
"Lee B., Yoo D., Jeong J., Bang K., Moon S.","57211723930;57191505147;56374410600;57193717839;57111280200;","Ultra-high-definition holography for near-eye display",2020,"Proceedings of SPIE - The International Society for Optical Engineering","11305",,"113050L","","",,,"10.1117/12.2552997","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082748084&doi=10.1117%2f12.2552997&partnerID=40&md5=14b4bd9a305a4f6dd9ecc8815e90bf13","School of Electrical and Computer Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 08826, South Korea","Lee, B., School of Electrical and Computer Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 08826, South Korea; Yoo, D., School of Electrical and Computer Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 08826, South Korea; Jeong, J., School of Electrical and Computer Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 08826, South Korea; Bang, K., School of Electrical and Computer Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 08826, South Korea; Moon, S., School of Electrical and Computer Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 08826, South Korea","Holographic near-eye displays (NEDs) have large potential for augmented reality (AR) devices as they modulate the wavefront of light. They can provide observers with comfortable three-dimensional (3D) views with focus-cues, and little optical aberrations since the unwanted phase delay added by optical systems can be compensated by wavefront modulation. With the advent of ultra-high-definition (UHD) spatial light modulator (SLM), a degree of freedom in designing holographic NEDs has been further expanded. Here, we introduce several holographic NEDs using UHD SLM. The holographic NED using an HOE is introduced for the optical see-Through display. Besides, the holographic NED with enlarged eye-box using point-source array and eye-Tracking method will be presented. Finally, the holographic NED of which optical aberration is compensated by Zernike's polynomial adaptation will be introduced. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Holography; Near-eye displays; Ultra-high-definition","Augmented reality; Degrees of freedom (mechanics); Display devices; Eye tracking; Holographic displays; Holography; Imaging systems; Light modulation; Light modulators; Optical systems; Wavefronts; Comfortable three-dimensional (3D); Degree of freedom; Eye tracking methods; Optical see-through display; Spatial light modulators; Ultra high definition (UHD); Ultra high definitions; Wavefront modulations; Aberrations",Conference Paper,"Final","",Scopus,2-s2.0-85082748084
"Jia Z., Chung H., Daiker J., Sedighi S., Morley N., Dominguez D., Grata J., Welch H.","57216149444;57216149062;57216154502;57216160563;57216156840;57216163168;16312336300;7103106250;","Eyeball camera based calibration and performance verification for spatial computing systems",2020,"Proceedings of SPIE - The International Society for Optical Engineering","11310",,"113101H","","",,,"10.1117/12.2553259","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082699166&doi=10.1117%2f12.2553259&partnerID=40&md5=b2b77be22889ab1e02a2cf860b0552d3","Magic Leap, 7500 W Sunrise Blvd, Plantation, FL  33322, United States","Jia, Z., Magic Leap, 7500 W Sunrise Blvd, Plantation, FL  33322, United States; Chung, H., Magic Leap, 7500 W Sunrise Blvd, Plantation, FL  33322, United States; Daiker, J., Magic Leap, 7500 W Sunrise Blvd, Plantation, FL  33322, United States; Sedighi, S., Magic Leap, 7500 W Sunrise Blvd, Plantation, FL  33322, United States; Morley, N., Magic Leap, 7500 W Sunrise Blvd, Plantation, FL  33322, United States; Dominguez, D., Magic Leap, 7500 W Sunrise Blvd, Plantation, FL  33322, United States; Grata, J., Magic Leap, 7500 W Sunrise Blvd, Plantation, FL  33322, United States; Welch, H., Magic Leap, 7500 W Sunrise Blvd, Plantation, FL  33322, United States","Spatial computing enables overlay of the digital world over the real world in a spatially interactive manner by merging digital light-fields, perception systems, and computing. The digital content presented by the spatial computing needs to work tandemly with real-world surroundings, and more importantly the human eye-brain system, which is the ultimate judge for system success. As a result, to develop a spatial computing system, it would be essential to have a proxy for the human eye-brain to calibrate and verify the performance of the spatial computing system. This paper proposes a novel camera design for such purpose which mimics human ocular anatomy and physiology in the following aspects: geometry, optical performance and ocular motor control. Specifically, the proposed camera not only adopts the same corneal and pupil geometry from human eye, also the iris and pupil can be configured with multiple texture, color and diameter options. Furthermore, the resolution of eyeball camera is designed to match the acuity of typical 20/20 human vision, and focus can be dynamically adjusted from 0 to 3 diopters. Lastly, a pair of eyeball cameras are mounted independently on two hexapods to simulate the eye gaze and vergence. With the help of the eyeball cameras, both perceived virtual and real world can be calibrated and evaluated in a deterministic and quantifiable eye conditions like pupil location and gaze. Principally, the proposed eyeball camera serves as a bridge which combines all the data from spatial computing like eye tracking, 3D geometry of the digital world, display color accuracy/uniformity, and display optical quality (sharpness, contrast, etc) for a holistic view, which helps to effectively blend the virtual and real worlds together seamlessly. © 2020 SPIE.","Calibration; Eyeball camera; Spatial computing; System verification","Calibration; Cameras; Geometry; Mixed reality; Textures; Multiple texture; Ocular motor controls; Optical performance; Optical qualities; Perception systems; Performance verification; Spatial computing; System verifications; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082699166
"Alão N.","57216151927;","Qualitative and quantitative visual information detected by portable eye-tracking technology",2020,"Proceedings of SPIE - The International Society for Optical Engineering","11310",,"1131008","","",,,"10.1117/12.2548336","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082696535&doi=10.1117%2f12.2548336&partnerID=40&md5=71f181a7f759ff9cea853e60410f889b","Lisbon School Of Architecture, Department Of Drawing, Geometry And Computation, Portugal","Alão, N., Lisbon School Of Architecture, Department Of Drawing, Geometry And Computation, Portugal","This paper presents some results of a larger study about vision geometry, carried out between 2014 and 2016 at the Lisbon School of Architecture, considering eye-tracking technology as an effective way of studying human vision. The methodology relied on portable eye-tracking equipment to analyze 3D immersive visual perception. 30 observers conducted 120 analyses of four different three-dimensional architectural spaces. The 120 samples allowed the analysis of 60,000 video frames to understand different kinds of elements that can be used to describe and study visual information. Quantitative and qualitative results are presented in the form of graphics to better understand eye movements in the perception of three-dimensional visual spaces, with a particular focus on macro-saccadic movements. The purpose of this work is to examine how the technology works, the possibilities it offers, and its limitations. The paper presents results in a way that seems trustworthy to work with at present, in order to obtain insights of scientific value that can point towards future possible resolutions of current methodological and technological issues. © 2020 SPIE.",,"Architecture; Eye movements; Mixed reality; Architectural space; Eye tracking technologies; Portable eye-tracking; Scientific values; Vision geometries; Visual information; Visual perception; Visual space; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082696535
"Nakamura H., Kitada T., Hamagishi G., Yoshimoto K., Kusafuka K., Takahashi H.","57208888581;57216163689;6603319161;56492328000;57203998567;7405468853;","Control method of active parallax barrier and binocular image for glasses-free stereoscopic display according to viewing position",2020,"Proceedings of SPIE - The International Society for Optical Engineering","11304",,"1130414","","",,,"10.1117/12.2545394","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082671172&doi=10.1117%2f12.2545394&partnerID=40&md5=13b61343c9dd1440069d363713ad06d3","Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Kyocera Corp., 800 Ichimiyake, Yasu, 520-2362, Japan","Nakamura, H., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Kitada, T., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Hamagishi, G., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Yoshimoto, K., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Kusafuka, K., Kyocera Corp., 800 Ichimiyake, Yasu, 520-2362, Japan; Takahashi, H., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan","In 3D displays based on parallax barriers, active parallax barriers that can change the barrier pattern according to the viewing position have been proposed to expand the viewing area. However, the production cost increases because the active barrier requires a special LC panel. Therefore, to lower the cost, we propose a glasses-free stereoscopic display using an active parallax barrier of an LC panel with the same specifications as the image LC panel. The ideal image pattern cannot be formed because the minimum control unit of the LC panel is equal to one subpixel. However, by using an image cycle pitch method (ICPM) that periodically increases the horizontal pitch of one pair of binocular images by one subpixel, we can realize the ideal relationship between the average pitch of the binocular image and the barrier pitch. In our previous research, we could make the average pitch match to the ideal pitch on the discrete optimum viewing distances (OVDs), but we could not follow the viewing position between the discrete OVDs. In this paper, we propose the ICPM that can keep the stereoscopic vision even at any viewing position. In order to verify the effectiveness of the proposed method, we made the prototype displays and evaluated them. As a result, we showed the crosstalk ratio could be suppressed low over a very wide range by using the proposed method. Also, we confirmed we could obtain the stereoscopic vision at the viewing distance from 300 mm to 729 mm by a subjective evaluation. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Active parallax barrier; Eye tracking; Glasses-free stereoscopic display; Step barrier; Wide viewing area","Binoculars; Eye tracking; Geometrical optics; Glass; Pixels; Stereo image processing; Barrier patterns; Parallax barriers; Prototype displays; Step barrier; Stereoscopic display; Stereoscopic vision; Subjective evaluations; Wide viewing; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85082671172
"Panetta K., Wan Q., Rajeev S., Kaszowska A., Gardony A.L., Naranjo K., Taylor H.A., Agaian S.","6507727793;57188747389;57191541248;57203481497;36570266900;57216039069;7403057334;35321805400;","ISeeColor: Method for Advanced Visual Analytics of Eye Tracking Data",2020,"IEEE Access","8",,"9036879","52278","52287",,2,"10.1109/ACCESS.2020.2980901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082536811&doi=10.1109%2fACCESS.2020.2980901&partnerID=40&md5=b7ac1a242f50f86a339f324bc14e8e16","Department of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Department of Psychology, Tufts University, Medford, MA  02155, United States; U.S. Army Combat Capabilities Development Command Soldier Center, Natick, MA  01760, United States; Department of Computer Science, City University of New York, New York, NY  10019, United States","Panetta, K., Department of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Wan, Q., Department of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Rajeev, S., Department of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Kaszowska, A., Department of Psychology, Tufts University, Medford, MA  02155, United States; Gardony, A.L., U.S. Army Combat Capabilities Development Command Soldier Center, Natick, MA  01760, United States; Naranjo, K., Department of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Taylor, H.A., Department of Psychology, Tufts University, Medford, MA  02155, United States; Agaian, S., Department of Computer Science, City University of New York, New York, NY  10019, United States","Recent advances in head-mounted eye-tracking technology have allowed researchers to monitor eye movements during locomotion in real-world environments, increasing the ecological validity of research on human gaze behavior. While collecting eye-tracking data is becoming more accessible, visual analytics of eye-tracking data remains difficult and time-consuming. As such, there is a significant need for developing efficient visualization and analysis tools for large-scale eye-tracking data. This work develops a first-of-its-kind eye-tracking data visualization and analysis system that allows for automatic recognition of independent objects within field-of-vision, using deep-learning-based semantic segmentation. This system recolors the fixated objects-of-interest by integrating gaze fixation information with semantic maps. The system effectively allows researchers to automatically infer what objects users view and for how long in dynamic contexts. The contributions are 1) a data visualization and analysis system that uses deep-learning technology along with eye-tracking data to automatically recognize objects-of-interest from head-mounted eye-tracking video recordings, and 2) a graphical user interface that presents objects-of-interest annotation along with eye-tracking data information. The architecture is tested with an outdoor case study of users walking around the Tufts University campus as part of a navigation study, which was administered by a team of research psychologists. © 2013 IEEE.","cognitive science; data analysis; data visualization; deep-learning; Eye-trackers","Advanced Analytics; Behavioral research; Data reduction; Data visualization; Deep learning; Eye movements; Graphical user interfaces; Learning systems; Object tracking; Semantics; Video recording; Visualization; Automatic recognition; Cognitive science; Eye trackers; Head-mounted eye tracking; Independent objects; Real world environments; Semantic segmentation; Visualization and analysis; Eye tracking",Article,"Final","",Scopus,2-s2.0-85082536811
"Emoto J., Hirata Y.","57215926029;35797191900;","Lightweight convolutional neural network for image processing method for gaze estimation and eye movement event detection",2020,"IPSJ Transactions on Bioinformatics","13",,,"7","15",,2,"10.2197/ipsjtbio.13.7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082392820&doi=10.2197%2fipsjtbio.13.7&partnerID=40&md5=467f5e6f5adbb86e0870d1ef995f7570","Chubu University, Kasugai, Aichi, 487-8501, Japan","Emoto, J., Chubu University, Kasugai, Aichi, 487-8501, Japan; Hirata, Y., Chubu University, Kasugai, Aichi, 487-8501, Japan","Advancements in technology have recently made it possible to obtain various types of biometric information from humans, enabling studies on estimation of human conditions in medicine, automobile safety, marketing, and other areas. These studies have particularly pointed to eye movement as an effective indicator of human conditions, and research on its applications is actively being pursued. The devices now widely used for measuring eye movements are based on the video-oculography (VOG) method, wherein the direction of gaze is estimated by processing eye images obtained through a camera. Applying convolutional neural networks (ConvNet) to the processing of eye images has been shown to enable accurate and robust gaze estimation. Conventional image processing, however, is premised on execution using a personal computer, making it difficult to carry out real-time gaze estimation using ConvNet, which involves the use of a large number of parameters, in a small arithmetic unit. Also, detecting eye movement events, such as blinking and saccadic movements, from the inferred gaze direction sequence for particular purposes requires the use of a separate algorithm. We therefore propose a new eye image processing method that batch-processes gaze estimation and event detection from end to end using an independently designed lightweight ConvNet. This paper discusses the structure of the proposed lightweight ConvNet, the methods for learning and evaluation used, and the proposed method's ability to simultaneously detect gaze direction and event occurrence using a smaller memory and at lower computational complexity than conventional methods. © 2020 Information Processing Society of Japan. All rights reserved.","Blink; Deep learning; Multi-task; Pupil; Saccade","Batch data processing; Convolution; Convolutional neural networks; Deep learning; Image processing; Multi-task learning; Personal computers; Processing; Biometric informations; Blink; Conventional methods; Human conditions; Image processing - methods; Pupil; Separate algorithm; Video oculography; Eye movements",Article,"Final","",Scopus,2-s2.0-85082392820
"Śledzianowski A., Szymanski A., Drabik A., Szlufik S., Koziorowski D.M., Przybyszewski A.W.","57201155975;56304189900;6506466125;55334567200;7801382272;6603763540;","Combining Results of Different Oculometric Tests Improved Prediction of Parkinson’s Disease Development",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12034 LNAI",,,"517","526",,1,"10.1007/978-3-030-42058-1_43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082387644&doi=10.1007%2f978-3-030-42058-1_43&partnerID=40&md5=06fa5a294c8035e0596c3ecc169d73a1","Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Neurology, Faculty of Health Science, Medical University of Warsaw, Warsaw, 03-242, Poland","Śledzianowski, A., Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Szymanski, A., Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Drabik, A., Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Szlufik, S., Neurology, Faculty of Health Science, Medical University of Warsaw, Warsaw, 03-242, Poland; Koziorowski, D.M., Neurology, Faculty of Health Science, Medical University of Warsaw, Warsaw, 03-242, Poland; Przybyszewski, A.W., Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland","In this text we compare the measurement results of reflexive saccades and antisaccades of patients with Parkinson’s Disease (PD), trying to determine the best settings to predict the Unified Parkinson’s Disease Rating Scale (UPDRS) results. After Alzheimer’s disease, PD statistically is the second one and until today, no effective therapy has been found. Luckily, PD develops very slowly and early detection can be very important in slowing its progression. In this experiment we examined the reflective saccades (RS) and antisaccades (AS) of 11 PD patients who performed eye-tracking tests in controlled conditions. We correlated neurological measurements of patient’s abilities described by the Unified Parkinson’s Disease Rating Scale (UPDRS) scale with parameters of RS and AS. We used tools implemented in the Scikit-Learn for data preprocessing and predictions of the UPDRS scoring groups [1]. By experimenting with different datasets we achieved best results by combining means of RS and AS parameters into computed attributes. We also showed, that the accuracy of the prediction increases with the number of such derived attributes. We achieved 89% accuracy of predictions and showed that computed attributes have 50% higher results in the feature importance scoring than source parameters. The eye-tracking tests described in this text are relatively easy to carry out and could support the PD diagnosis. © 2020, Springer Nature Switzerland AG.","Antisaccades; Data mining; Eye tracking; Machine learning; Parkinson’s Disease; Reflexive saccades; UPDRS","Data mining; Database systems; Disease control; Eye movements; Eye tracking; Forecasting; Learning systems; Antisaccades; Controlled conditions; Data preprocessing; Disease development; Effective therapy; Measurements of; Source parameters; UPDRS; Diagnosis",Conference Paper,"Final","",Scopus,2-s2.0-85082387644
"Chudzik A., Szymański A., Nowacki J.P., Przybyszewski A.W.","57208342920;56304189900;57024785700;6603763540;","Eye-Tracking and Machine Learning Significance in Parkinson’s Disease Symptoms Prediction",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12034 LNAI",,,"537","547",,,"10.1007/978-3-030-42058-1_45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082384684&doi=10.1007%2f978-3-030-42058-1_45&partnerID=40&md5=4b25d383c0e4c4a24bc8dbeb6084cb8a","Polish-Japanese Academy of Information Technology, Koszykowa 86 Street, Warsaw, 02-008, Poland; Department of Neurology, University of Massachusetts Medical School, 65 Lake Avenue, Worcester, MA  01655, United States","Chudzik, A., Polish-Japanese Academy of Information Technology, Koszykowa 86 Street, Warsaw, 02-008, Poland; Szymański, A., Polish-Japanese Academy of Information Technology, Koszykowa 86 Street, Warsaw, 02-008, Poland; Nowacki, J.P., Polish-Japanese Academy of Information Technology, Koszykowa 86 Street, Warsaw, 02-008, Poland; Przybyszewski, A.W., Polish-Japanese Academy of Information Technology, Koszykowa 86 Street, Warsaw, 02-008, Poland, Department of Neurology, University of Massachusetts Medical School, 65 Lake Avenue, Worcester, MA  01655, United States","Parkinson’s disease (PD) is a progressive, neurodegenerative disorder characterized by resting tremor, rigidity, bradykinesia, and postural instability. The standard measure of the PD progression is Unified Parkinson’s Disease Rating (UPDRS). Our goal was to predict patients’ UPDRS development based on the various groups of patients in the different stages of the disease. We used standard neurological and neuropsychological tests, aligned with eye movements on a dedicated computer system. For predictions, we have applied various machine learning models with different parameters embedded in our dedicated data science framework written in Python and based on the Scikit Learn and Pandas libraries. Models proposed by us reached 75% and 70% of accuracy while predicting subclasses of UPDRS for patients in advanced stages of the disease who respond to treatment, with a global 57% accuracy score for all classes. We have demonstrated that it is possible to use eye movements as a biomarker for the assessment of symptom progression in PD. © 2020, Springer Nature Switzerland AG.","Eye-tracking; Machine learning; Parkinson’s Disease; Saccades","Database systems; Eye movements; Forecasting; Learning systems; Machine learning; Neurodegenerative diseases; Patient treatment; Dedicated computers; Different stages; Disease ratings; Disease symptoms; Machine learning models; Neurodegenerative disorders; Neuropsychological tests; Postural instabilities; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082384684
"Wang W., Chen X., Zheng S., Li H.","57215831361;55739042600;57215828936;57215834831;","Fast Head Pose Estimation via Rotation-Adaptive Facial Landmark Detection for Video Edge Computation",2020,"IEEE Access","8",,"9020163","45023","45032",,3,"10.1109/ACCESS.2020.2977729","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082003182&doi=10.1109%2fACCESS.2020.2977729&partnerID=40&md5=81cf30ed53752f8c1011b7d5a2b9ea89","College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, 300222, China; IriStar Technology Ltd., Beijing, 100191, China","Wang, W., College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, 300222, China; Chen, X., College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, 300222, China; Zheng, S., College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, 300222, China; Li, H., IriStar Technology Ltd., Beijing, 100191, China","The human head pose estimation is an important and challenging problem, which provides the estimation of the head posture in 3D space from 2D image. It is a crucial technique for face recognition, gaze estimation, facial attribute recognition, etc. However, fast head pose estimation executing on the terminal for video edge computation has many challenges due to the computational complexity of the existing algorithms. In this paper, we propose a fast head pose estimation method based on a novel Rotation-Adaptive facial landmark detection powered by Local Binary Feature (RALBF). The landmark detection method is structured through fusing the prior of the rotation information provided by the Progressive Calibration Networks (PCN) face detector to a Local Binary Feature (LBF) based landmark detection method, which improves the robustness against head pose variations and simultaneously keep the computing efficiency. RALBF is trained and tested on 300W dataset and AFLW2000 dataset, it is verified by the accuracy evaluation that RALBF performs better than LBF. To improve the speed of head pose estimation, the 68, 51 and 10 landmarks distribution schemes are explored and compared on speed and accuracy. In the 10 landmarks scheme, the head pose estimation running once only takes 8.3ms on Intel i7-6700HQ CPU and takes 21.8ms on HiSilicon SoC Hi3519AV100, and the average error of Euler angle is 5.9973° when the face yaw angle is between ±35° on AFLW2000 3D dataset. Experiments demonstrate our approach performing well on real scenes. © 2013 IEEE.","facial landmark detection; Head pose estimation; local binary features; PnP problem","Edge computing; Feature extraction; System-on-chip; Attribute recognition; Binary features; Calibration network; Computing efficiency; Distribution scheme; Facial landmark detection; Head Pose Estimation; PnP problems; Face recognition",Article,"Final","",Scopus,2-s2.0-85082003182
"Albalawi T., Ghazinour K., Melton A.","57189593089;24766252900;7004152586;","Quantifying the effect of cognitive bias on security decision-making for authentication methods",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11691 LNAI",,,"139","154",,,"10.1007/978-3-030-39431-8_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080896951&doi=10.1007%2f978-3-030-39431-8_14&partnerID=40&md5=ef99c76ff19a1ed840e02ba9d0593d65","Department of Computer Science, Kent State University, Kent, United States","Albalawi, T., Department of Computer Science, Kent State University, Kent, United States; Ghazinour, K., Department of Computer Science, Kent State University, Kent, United States; Melton, A., Department of Computer Science, Kent State University, Kent, United States","The main challenge that can impact the effectiveness of authentication mechanisms is human error (unintentional threats). Irrational judgment associated with human error is often linked to a unique attribute called cognitive bias (CB). CB is a tendency to think irrationally in certain situations and make irrational judgment. The appearance of CB in human decisions is considered one of the implications of system usability. In the security filed, usability is recognized as one of the main issues that affect an individual’s security decisions. Clearly, security decision-making is a result of three overlapping factors: security, usability and CB. In this paper, we quantify security decision making by providing a holistic view on how these factors affect the security decision. For this purpose, an experiment was conducted involving 54 participants who performed multiple security tasks related to authentication. An eye-tracking machine was used to record cognitive measurements that were used for decision analysis. Multi Criteria Decision Analysis (MCDA) approach was then used to evaluate the participants’ decisions. The result showed that participants security decisions are varied depends on the authentication method. For instance, picture type was the authentication method least influenced by CB. Low system usability is one of the major causes of CB in decisions. This was not the case for the picture password method. The different levels of usability associated with the picture method resulted in low impact of CB on participants’ security decision. This finding point to investigating how picture-based authentication methods are capable of handling the issue of the CB. © Springer Nature Switzerland AG 2020.","Authentication; Cognitive bias; Decision; Human error; MCDM; Security; Usability","Authentication; Brain; Decision making; Errors; Eye tracking; Usability engineering; Cognitive bias; Decision; Human errors; MCDM; Security; Usability; Cognitive systems",Conference Paper,"Final","",Scopus,2-s2.0-85080896951
"Yan X., Wang Z., Sun M.","57204951521;24175350400;8927434200;","Eye fixation assisted detection of video salient objects",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11691 LNAI",,,"211","223",,,"10.1007/978-3-030-39431-8_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080892715&doi=10.1007%2f978-3-030-39431-8_20&partnerID=40&md5=e3aeb0736ae2464ce63b2dd486be2402","College of Intelligence and Computing, Tianjin University, Tianjin, China","Yan, X., College of Intelligence and Computing, Tianjin University, Tianjin, China; Wang, Z., College of Intelligence and Computing, Tianjin University, Tianjin, China; Sun, M., College of Intelligence and Computing, Tianjin University, Tianjin, China","With the increasing maturity of image saliency detection, more and more people are focusing their research on video saliency detection. Currently, video saliency detection can be divided into two forms, eye fixation detection and salient objects detection. In this article, we focus on exploring the relationship between them. Firstly, we propose a network called fixation assisted video salient object detection network (FAVSODNet), which uses the eye gaze information in videos to assist in detecting video salient objects. A fixation assisted module (FAM) is designed to connect FP task and SOD task deeply. Under the guidance of the eye fixation information, multiple salient objects in complex scene can be detected more correctly. Moreover, when the scene suddenly changes or a new person appears, it can better to detect the correct salient objects with the aid of fixation maps. In addition, we adopt an extended multi-scale feature extraction module (EMFEM) to extract rich object features. Thus, the neural network can aware the objects with variable scales in videos more comprehensively. Finally, the experimental results show that our method advances the state-of-art in video salient object detection. © Springer Nature Switzerland AG 2020.","Deep learning; Eye fixation prediction; Video salient object detection","Brain; Cognitive systems; Deep learning; Object recognition; Complex scenes; Eye fixations; Fixation map; Image saliencies; Multi-scale features; Salient object detection; Salient objects; Video saliencies; Object detection",Conference Paper,"Final","",Scopus,2-s2.0-85080892715
"Tamuly S., Jyotsna C., Amudha J.","57214072113;57193578958;35766448700;","Deep learning model for image classification",2020,"Advances in Intelligent Systems and Computing","1108 AISC",,,"312","320",,1,"10.1007/978-3-030-37218-7_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078537686&doi=10.1007%2f978-3-030-37218-7_36&partnerID=40&md5=2ad68b6c39b292b38f7d633b20093dfe","Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bengaluru, India","Tamuly, S., Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bengaluru, India; Jyotsna, C., Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bengaluru, India; Amudha, J., Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bengaluru, India","Starting from images captured on mobile phones, advertisements popping up on our internet browser to e-retail sites displaying flashy dresses for sale, every day we are dealing with a large quantity of visual information and sometimes, finding a particular image or visual content might become a very tedious task to deal with. By classifying the images into different logical categories, our quest to find an appropriate image becomes much easier. Image classification is generally done with the help of computer vision, eye tracking and ways as such. What we intend to implement in classifying images is the use of deep learning for classifying images into pleasant and unpleasant categories. We proposed the use of deep learning in image classification because deep learning can give us a deeper understanding as to how a subject reacts to a certain visual stimuli when exposed to it. © 2020, Springer Nature Switzerland AG.","Computer vision; Deep learning; Eye tracking; Heat map; Image classification","Biomimetics; Computer vision; Eye tracking; Image classification; E retails; Exposed to; Heat maps; Internet browsers; Learning models; Visual content; Visual information; Visual stimulus; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85078537686
"Anusree K., Amudha J.","57224239053;35766448700;","Eye movement event detection with deep neural networks",2020,"Advances in Intelligent Systems and Computing","1108 AISC",,,"921","930",,,"10.1007/978-3-030-37218-7_98","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078523747&doi=10.1007%2f978-3-030-37218-7_98&partnerID=40&md5=2eab5e440f7c7fd0913a82f00261f786","Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bengaluru, India","Anusree, K., Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bengaluru, India; Amudha, J., Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bengaluru, India","This paper presents a comparison of event detection task in eye movement with the exact events recorded from eye tracking device. The primary goal of this research work is to build a general approach for eye-movement based event detection, which will work with all eye tracking data collected using different eye tracking devices. It utilizes an end to end method based on deep learning, which can efficiently utilize eye tracking raw particulars that is further grouped into Saccades, post-saccadic oscillations and Fixations. The drawback of deep learning method is that it requires a lot of preprocessing data. At first, we have to build up a strategy to enlarge handcoded information, with the goal that we can unequivocally augment the informational index utilized for preparing, limiting the run through time on coding by a human. Utilizing this all-encompassing hand-coded information, we instruct neural networks model to process eye-development fixation grouping from eye-movement information in the absence of any previously defined extraction or post-preparing steps. © 2020, Springer Nature Switzerland AG.","Deep learning; Event detection; Eye tracking; Fixations; Saccades","Biomimetics; Deep learning; Deep neural networks; Eye tracking; Neural networks; End to end; Event detection; Eye development; Eye tracking devices; Fixations; Learning methods; Movement-based; Neural networks model; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85078523747
"Vo M.T., Nguyen T., Le T.","57203062383;57194430442;55513981400;","Robust Head Pose Estimation Using Extreme Gradient Boosting Machine on Stacked Autoencoders Neural Network",2020,"IEEE Access","8",,"8945218","3687","3694",,6,"10.1109/ACCESS.2019.2962974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078479614&doi=10.1109%2fACCESS.2019.2962974&partnerID=40&md5=c76146b0261f98792c31515c2b2ebd1f","Institute of Research and Development, Duy Tan University, Da Nang, 550000, Viet Nam; Faculty of Information Technology, Ho Chi Minh City Open University, Ho Chi Minh City, 700000, Viet Nam; Faculty of Information Technology, Ho Chi Minh City University of Technology (HUTECH), Ho Chi Minh, 700000, Viet Nam","Vo, M.T., Institute of Research and Development, Duy Tan University, Da Nang, 550000, Viet Nam; Nguyen, T., Faculty of Information Technology, Ho Chi Minh City Open University, Ho Chi Minh City, 700000, Viet Nam; Le, T., Faculty of Information Technology, Ho Chi Minh City University of Technology (HUTECH), Ho Chi Minh, 700000, Viet Nam","Head pose estimation is an important sign in helping robots and other intelligence machines understand human. It plays a vital role in designing human computer interaction systems because many applications rely on precise results of head pose angles such as human behavior analysis, gaze estimation, 3D head reconstruction etc. This study presents a robust approach for estimating the head pose angles in a single image. More specifically, the proposed system first encodes the global features extracted from Histogram of Oriented Gradients in a multi stacked autoencoders neural network. Based on the hidden nodes in deep layers, Autoencoder has been proposed for feature reduction while maintaining the key information of data. A scalable gradient boosting machine is then employed to train and classify the embedded features. Experiences have evaluated on the Pointing 04 dataset and show that the proposed approach outperforms the state-of-the-art methods with the low head pose angle errors in pitch and yaw as 6.16° and 7.17°, respectively. © 2013 IEEE.","autoencoder; feature reduction; global features; gradient boosting; Head pose estimation","Behavioral research; Intelligent robots; Learning systems; Auto encoders; Feature reduction; Global feature; Gradient boosting; Head Pose Estimation; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85078479614
"Laubrock J., Dunst A.","6504602165;55225794100;","Computational Approaches to Comics Analysis",2020,"Topics in Cognitive Science","12","1",,"274","310",,4,"10.1111/tops.12476","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075140816&doi=10.1111%2ftops.12476&partnerID=40&md5=6d1c7ab43beace5a739503dcf2343216","Department of Psychology, University of Potsdam, Germany; Department of English and American Studies, University of Paderborn, Germany","Laubrock, J., Department of Psychology, University of Potsdam, Germany; Dunst, A., Department of English and American Studies, University of Paderborn, Germany","Comics are complex documents whose reception engages cognitive processes such as scene perception, language processing, and narrative understanding. Possibly because of their complexity, they have rarely been studied in cognitive science. Modeling the stimulus ideally requires a formal description, which can be provided by feature descriptors from computer vision and computational linguistics. With a focus on document analysis, here we review work on the computational modeling of comics. We argue that the development of modern feature descriptors based on deep learning techniques has made sufficient progress to allow the investigation of complex material such as comics for reception studies, including experimentation and computational modeling of cognitive processes. © 2019 Cognitive Science Society, Inc","Comics; Computer vision; Deep learning; Document analysis; Eye tracking; Scene perception; Visual attention","art; attention; automated pattern recognition; human; language; physiology; verbal communication; vision; Attention; Cartoons as Topic; Deep Learning; Eye-Tracking Technology; Humans; Language; Narration; Pattern Recognition, Automated; Visual Perception",Article,"Final","",Scopus,2-s2.0-85075140816
"Van Oosterom M.N., Van Der Poel H.G., Van Leeuwen F.W.B., Meershoek P., Welling M.M., Pinto F., Matthies P., Simon H., Wendler T., Navab N., Van De Velde C.J.H.","56291986200;7004230990;57203178308;57191952678;7004935135;57014391800;55560804200;26538132900;8546266900;7003458998;57202638098;","Extending the Hybrid Surgical Guidance Concept with Freehand Fluorescence Tomography",2020,"IEEE Transactions on Medical Imaging","39","1","8743423","226","235",,11,"10.1109/TMI.2019.2924254","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073910133&doi=10.1109%2fTMI.2019.2924254&partnerID=40&md5=0a7d38bf5646ab5b4bfe3054b47f9c95","Department of Radiology, Interventional Molecular Imaging Laboratory, Leiden University Medical Center, Leiden, 2333, Netherlands; Department of Urology, Netherlands Cancer Institute-Antoni Van Leeuwenhoek Hospital, Amsterdam, 1066, Netherlands; Health Data Pioneers GmbH, Munich, 81671, Germany; Eurorad SA, Eckbolsheim, 67201, France; Computer Aided Medical Procedures, Technical University Munich, Munich, 85748, Germany; Department of Surgery, Leiden University Medical Center, Leiden, 2333, Netherlands; SurgicEye GmbH, Munich, 81671, Germany; Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD  21218, United States","Van Oosterom, M.N., Department of Radiology, Interventional Molecular Imaging Laboratory, Leiden University Medical Center, Leiden, 2333, Netherlands, Department of Surgery, Leiden University Medical Center, Leiden, 2333, Netherlands; Van Der Poel, H.G., Department of Urology, Netherlands Cancer Institute-Antoni Van Leeuwenhoek Hospital, Amsterdam, 1066, Netherlands; Van Leeuwen, F.W.B., Department of Radiology, Interventional Molecular Imaging Laboratory, Leiden University Medical Center, Leiden, 2333, Netherlands, Department of Urology, Netherlands Cancer Institute-Antoni Van Leeuwenhoek Hospital, Amsterdam, 1066, Netherlands; Meershoek, P., Department of Radiology, Interventional Molecular Imaging Laboratory, Leiden University Medical Center, Leiden, 2333, Netherlands, Department of Urology, Netherlands Cancer Institute-Antoni Van Leeuwenhoek Hospital, Amsterdam, 1066, Netherlands; Welling, M.M., Department of Radiology, Interventional Molecular Imaging Laboratory, Leiden University Medical Center, Leiden, 2333, Netherlands; Pinto, F., Health Data Pioneers GmbH, Munich, 81671, Germany; Matthies, P., Health Data Pioneers GmbH, Munich, 81671, Germany; Simon, H., Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD  21218, United States; Wendler, T., Computer Aided Medical Procedures, Technical University Munich, Munich, 85748, Germany, SurgicEye GmbH, Munich, 81671, Germany; Navab, N., Computer Aided Medical Procedures, Technical University Munich, Munich, 85748, Germany, Computer Aided Medical Procedures, Johns Hopkins University, Baltimore, MD  21218, United States; Van De Velde, C.J.H., Eurorad SA, Eckbolsheim, 67201, France, Department of Surgery, Leiden University Medical Center, Leiden, 2333, Netherlands","Within image-guided surgery, 'hybrid' guidance technologies have been used to integrate the complementary features of radioactive guidance and fluorescence guidance. Here, we explore how the generation of a novel freehand fluorescence (fhFluo) imaging approach complements freehand SPECT (fhSPECT) in a hybrid setup. Near-infrared optical tracking was used to register the position and the orientation of a hybrid opto-nuclear detection probe while recording its readings. Dedicated look-up table models were used for 3D reconstruction. In phantom and excised tissue settings (i.e., flat-surface human skin explants), fhSPECT and fhFluo were investigated for image resolution and in-tissue signal penetration. Finally, the combined potential of these freehand technologies was evaluated on prostate and lymph node specimens of prostate cancer patients receiving prostatectomy and sentinel lymph node dissection (tracers: indocyanine green (ICG) +99m Tc-nanocolloid or ICG-99mTc-nanocolloid). After hardware and software integration, the hybrid setup created 3D nuclear and fluorescence tomography scans. The imaging resolution of fhFluo (1 mm) was superior to that of fhSPECT (6 mm). Fluorescence modalities were confined to a maximum depth of 0.5 cm, while nuclear modalities were usable at all evaluated depths (&lt;2 cm). Both fhSPECT and fhFluo enabled augmented-and virtual-reality navigation toward segmented image hotspots, including relative hotspot quantification with an accuracy of 3.9% and 4.1%. Imaging in surgical specimens confirmed these trends (fhSPECT: in-depth detectability, low resolution, and fhFluo: superior resolution, superficial detectability). Overall, when radioactive and fluorescent tracer signatures are used, fhFluo has complementary value to fhSPECT. Combined the freehand technologies render a unique hybrid imaging and navigation modality. © 1982-2012 IEEE.","augmented reality; fluorescence tomography; freehand SPECT; hybrid imaging; Image-guided surgery; surgical navigation","Augmented reality; Diseases; Image resolution; Infrared devices; Navigation; Optical tomography; Patient rehabilitation; Radioactivity; Single photon emission computed tomography; Table lookup; Three dimensional computer graphics; Tissue; Transplantation (surgical); Urology; Virtual reality; Fluorescence tomography; freehand SPECT; Hybrid imaging; Image guided surgery; Surgical navigation; Fluorescence; indocyanine green; nanocoll tc 99m; Article; cancer patient; cancer surgery; clinical article; explant; eye tracking; fluorescence imaging; freehand fluorescence imaging; freehand single photon emission computed tomography; human; human tissue; image reconstruction; lymph node dissection; male; prostate cancer; prostatectomy; sentinel lymph node; single photon emission computed tomography; computer assisted surgery; devices; equipment design; imaging phantom; optical tomography; procedures; prostate tumor; signal processing; single photon emission computed tomography; three-dimensional imaging; Equipment Design; Humans; Imaging, Three-Dimensional; Male; Phantoms, Imaging; Prostatic Neoplasms; Sentinel Lymph Node; Signal Processing, Computer-Assisted; Surgery, Computer-Assisted; Tomography, Emission-Computed, Single-Photon; Tomography, Optical",Article,"Final","",Scopus,2-s2.0-85073910133
"Xia Y., Lou J., Dong J., Qi L., Li G., Yu H.","57192671097;57192670261;22634069200;36606745500;13408307700;56115992300;","Hybrid regression and isophote curvature for accurate eye center localization",2020,"Multimedia Tools and Applications","79","1-2",,"805","824",,4,"10.1007/s11042-019-08160-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073823365&doi=10.1007%2fs11042-019-08160-5&partnerID=40&md5=5d73a8024dbcdf7a94e93763eb4922fc","School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Key Laboratory of Metallurgical Equipment and Control Technology, Wuhan University of Science and Technology, Wuhan, China","Xia, Y., School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Lou, J., School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Dong, J., Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Qi, L., Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Li, G., Key Laboratory of Metallurgical Equipment and Control Technology, Wuhan University of Science and Technology, Wuhan, China; Yu, H., School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom","The eye center localization is a crucial requirement for various human-computer interaction applications such as eye gaze estimation and eye tracking. However, although significant progress has been made in the field of eye center localization in recent years, it is still very challenging for tasks under the significant variability situations caused by different illumination, shape, color and viewing angles. In this paper, we propose a hybrid regression and isophote curvature for accurate eye center localization under low resolution. The proposed method first applies the regression method, which is called Supervised Descent Method (SDM), to obtain the rough location of eye region and eye centers. SDM is robust against the appearance variations in the eye region. To make the center points more accurate, isophote curvature method is employed on the obtained eye region to obtain several candidate points of eye center. Finally, the proposed method selects several estimated eye center locations from the isophote curvature method and SDM as our candidates and a SDM-based means of gradient method further refine the candidate points. Therefore, we combine regression and isophote curvature method to achieve robustness and accuracy. In the experiment, we have extensively evaluated the proposed method on the two public databases which are very challenging and realistic for eye center localization and compared our method with existing state-of-the-art methods. The results of the experiment confirm that the proposed method outperforms the state-of-the-art methods with a significant improvement in accuracy and robustness and has less computational complexity. © 2019, The Author(s).","Eye center localization; Eye gaze estimation; Eye tracking; Human-computer interaction","Gradient methods; Human computer interaction; Regression analysis; Curvature method; Experiment confirm; Eye center localization; Eye center locations; Eye-gaze; Public database; Regression method; State-of-the-art methods; Eye tracking",Article,"Final","",Scopus,2-s2.0-85073823365
"Piotrowski P., Nowosielski A.","57211167743;8652849400;","Gaze-Based Interaction for VR Environments",2020,"Advances in Intelligent Systems and Computing","1062",,,"41","48",,2,"10.1007/978-3-030-31254-1_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072866926&doi=10.1007%2f978-3-030-31254-1_6&partnerID=40&md5=907fb9b507c0388cd4ca1ecc132dad08","Faculty of Computer Science and Information Technology, West Pomeranian University of Technology, Szczecin, Żołnierska 52, Szczecin, 71-210, Poland","Piotrowski, P., Faculty of Computer Science and Information Technology, West Pomeranian University of Technology, Szczecin, Żołnierska 52, Szczecin, 71-210, Poland; Nowosielski, A., Faculty of Computer Science and Information Technology, West Pomeranian University of Technology, Szczecin, Żołnierska 52, Szczecin, 71-210, Poland","In this paper we propose a steering mechanism for VR headset utilizing eye tracking. Based on the fovea region traced by the eyetracker assembled into VR headset the visible 3D ray is generated towards the focal point of sight. The user can freely look around the virtual scene and is able to interact with objects indicated by the eyes. The paper gives an overview of the proposed interaction system and addresses the effectiveness and precision issues of such interaction modality. © 2020, Springer Nature Switzerland AG.","Eye tracking; Gaze-based interaction; Gaze-operated games; Virtual reality","Image processing; Virtual reality; Eye trackers; Focal points; Fovea region; Gaze-based interaction; Gaze-operated games; Interaction systems; Steering mechanisms; Virtual scenes; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85072866926
"Li Q., Hou W.","57211155729;57211157140;","Using experts’ perceptual skill for dermatological image segmentation",2020,"Advances in Intelligent Systems and Computing","1038",,,"1199","1208",,,"10.1007/978-3-030-29513-4_86","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072832856&doi=10.1007%2f978-3-030-29513-4_86&partnerID=40&md5=b4d0e35182d151f76c4e363169102323","School of Foreign Languages, Tianjin University of Technology, Tianjin, 300384, China; Tianjin Gong An Hospital, Tianjin, 300042, China","Li, Q., School of Foreign Languages, Tianjin University of Technology, Tianjin, 300384, China; Hou, W., Tianjin Gong An Hospital, Tianjin, 300042, China","There is a growing reliance on imaging equipment in medical domain, hence medical experts’ specialized visual perceptual capability becomes the key of their superior performance. In this paper, we propose a principled generative model to detect and segment out dermatological lesions by exploiting the experts’ perceptual expertise represented by their patterned eye movement behaviors during examining and diagnosing dermatological images. The image superpixels’ diagnostic significance levels are inferred based on the correlations between their appearances and the spatial structures of the experts’ signature eye movement patterns. In this process, the global relationships between the superpixels are also manifested by the spans of the signature eye movement patterns. Our model takes into account these dependencies between experts’ perceptual skill and image properties to generate a holistic understanding of cluttered dermatological images. A Gibbs sampler is derived to use the generative model’s structure to estimate the diagnostic significance and lesion spatial distributions from superpixel-based representation of dermatological images and experts’ signature eye movement patterns. We demonstrate the effectiveness of our approach on a set of dermatological images on which dermatologists’ eye movements are recorded. It suggests that the integration of experts’ perceptual skill and dermatological images is able to greatly improve medical image understanding and retrieval. © Springer Nature Switzerland AG 2020.","Dermatological image understanding; Eye tracking; Gibbs sampling; Perceptual expertise; Probabilistic modeling; Superpixels","Dermatology; Diagnosis; Eye tracking; Image enhancement; Image segmentation; Image understanding; Intelligent systems; Medical imaging; Superpixels; Dermatological images; Eye movement patterns; Gibbs sampling; Imaging equipment; Perceptual expertise; Perceptual skills; Probabilistic modeling; Significance levels; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85072832856
"Yuan Y., Wang Q.","57210426837;55609471600;","Feature Extraction for Eye Movement Video Data",2020,"Advances in Intelligent Systems and Computing","1017",,,"115","121",,,"10.1007/978-3-030-25128-4_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070724258&doi=10.1007%2f978-3-030-25128-4_15&partnerID=40&md5=ced124cffabdb9a7743833c874cd7c26","School of Computer Science and Technology, Qilu University of Technology, Shandong Academy of Sciences, Jinan, China","Yuan, Y., School of Computer Science and Technology, Qilu University of Technology, Shandong Academy of Sciences, Jinan, China; Wang, Q., School of Computer Science and Technology, Qilu University of Technology, Shandong Academy of Sciences, Jinan, China","Random objects in videos are common stimuli in eye tracker based studies and their locations and time of appearance need to be detected in related research such as depression detection. In this paper, we propose a new method to extract features in eye movement video data captured by the SMI eye tracker. Firstly, we provide a feature extraction method by using the circle Hough transform and the Douglas–Peucker algorithm to extract the feature for each frame of the eye movement video data, and verify its validity in eye movement video data processing. Secondly, because the storage time of the eye tracker is more accurate than the on-screen time of the exported video, we choose to extract the storage time of the eye tracker to improve the quality of feature extraction. Finally, we add batch processing function to improve the efficiency of the experiment. Experimental results show that the method can extract the eye movement features in the eye movement video accurately and effectively. © 2020, Springer Nature Switzerland AG.","Circle Hough transform; Douglas–Peucker algorithm; Eye movement video; Feature extraction","Batch data processing; Data handling; Data mining; Digital storage; Extraction; Eye tracking; Feature extraction; Hough transforms; Video recording; Video signal processing; Circle Hough transforms; Douglas; Eye trackers; Feature extraction methods; Screen time; Storage time; Video data; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85070724258
"Guilei S.","57209601640;","Research on Location of Emergency Sign Based on Virtual Reality and Eye Tracking Technology",2020,"Advances in Intelligent Systems and Computing","973",,,"401","408",,,"10.1007/978-3-030-20476-1_40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068228505&doi=10.1007%2f978-3-030-20476-1_40&partnerID=40&md5=2cf418d8b50f5cf6579e334f65f9a033","Department of Safety Engineering, China University of Labor Relations, Beijing, 100048, China","Guilei, S., Department of Safety Engineering, China University of Labor Relations, Beijing, 100048, China","In order to analyze the attention to exit sign during the emergency state, virtual reality technology (VR) was used to simulate escape scene and 3D Max was used to design the scene. To obtain the number of gaze points and the gaze duration of subjects, eye tracker was utilized to get data and spss 21.0 was taken advantage of to analyze the data. Results show that the position of emergency exit sign has significant influence on the recognition. And the exit sign of the height of 1.0 m on the front of the observer&#x2019;s line of sight is the most beneficial to discovery and identification. Moreover, the height and the position of exit sign have no significant influence on reading time. &#x00A9; 2020, Springer Nature Switzerland AG.","Exit sign; Eye tracking; Fixation time; Height; Identification","Human engineering; Identification (control systems); Virtual reality; Wearable technology; Emergency exit; Emergency state; Exit sign; Eye tracking technologies; Fixation time; Height; Line of Sight; Virtual reality technology; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85068228505
"Brandenburger J., Constapel M., Hellbrück H., Janneck M.","57205557234;57203367941;15019443300;15769712600;","Analysis of Types, Positioning and Appearance of Visualizations in Online Teaching Environments to Improve Learning Experiences",2020,"Advances in Intelligent Systems and Computing","963",,,"355","366",,1,"10.1007/978-3-030-20135-7_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067121205&doi=10.1007%2f978-3-030-20135-7_35&partnerID=40&md5=965f1ffb2c966d588d3889de67f2fccf","The Center of Excellence CoSA, Department of Electrical Engineering, Luebeck University of Applied Sciences, Lübeck, Germany","Brandenburger, J., The Center of Excellence CoSA, Department of Electrical Engineering, Luebeck University of Applied Sciences, Lübeck, Germany; Constapel, M., The Center of Excellence CoSA, Department of Electrical Engineering, Luebeck University of Applied Sciences, Lübeck, Germany; Hellbrück, H., The Center of Excellence CoSA, Department of Electrical Engineering, Luebeck University of Applied Sciences, Lübeck, Germany; Janneck, M., The Center of Excellence CoSA, Department of Electrical Engineering, Luebeck University of Applied Sciences, Lübeck, Germany","In this paper we investigate different visualizations of learners’ data related to collaborative online learning in terms of suitability and attractiveness to students. Furthermore, we analyze whether positioning and color appearance of these data visualizations might have an effect on learners’ behavior. To that end, we conducted an online study (n = 120) as well as an eye tracking study (n = 20) to compare different types of visualizations. Results show that students prefer classical data visualizations like bar charts. Visualizations placed in the sidebar of a two column web interface get less attention than visualizations in the header of the main content area. Color schemes do not seem to influence the perception of visualizations. We discuss possible explanations and implications for designing data visualizations in learning environments. © 2020, Springer Nature Switzerland AG.","Eye tracking; Human computer interaction; Learning analytics; Online teaching environments; Visualization methods; Visualizations","Computer aided instruction; Eye tracking; Human computer interaction; Human engineering; Students; Visualization; Color appearance; Eye-tracking studies; Learning analytics; Learning environments; Learning experiences; Online learning; Online teaching; Visualization method; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85067121205
"Gomolka Z., Twarog B., Zeslawska E., Kordos D.","26039058200;55202221500;6603134013;55372750600;","Registration and Analysis of a Pilot’s Attention Using a Mobile Eyetracking System",2020,"Advances in Intelligent Systems and Computing","987",,,"215","224",,1,"10.1007/978-3-030-19501-4_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065892520&doi=10.1007%2f978-3-030-19501-4_21&partnerID=40&md5=56af1a01e1cdf16e722f33d2d17ab596","Faculty of Mathematics and Natural Sciences, University of Rzeszow, ul. Pigonia 1, Rzeszow, 35-959, Poland; Faculty of Applied Informatics, Department of Applied Information, University of Information Technology and Management in Rzeszow, ul. Sucharskiego 2, Rzeszow, 35-225, Poland; Rzeszow University of Technology, al. Powstańców Warszawy 8, Rzeszów, 35-959, Poland","Gomolka, Z., Faculty of Mathematics and Natural Sciences, University of Rzeszow, ul. Pigonia 1, Rzeszow, 35-959, Poland; Twarog, B., Faculty of Mathematics and Natural Sciences, University of Rzeszow, ul. Pigonia 1, Rzeszow, 35-959, Poland; Zeslawska, E., Faculty of Applied Informatics, Department of Applied Information, University of Information Technology and Management in Rzeszow, ul. Sucharskiego 2, Rzeszow, 35-225, Poland; Kordos, D., Rzeszow University of Technology, al. Powstańców Warszawy 8, Rzeszów, 35-959, Poland","This paper presents the next stage of research into the registration and analysis of a pilot’s attention during the take-off and landing procedures. This is a continuation of the research, which the authors conducted using the static SensoMotoric Instruments (SMI) Red500 eyetracker on a group of pilots applying for a pilot’s license. Because static eyetracking introduces a series of restrictions on the properties of the observed scene, a system using Tobii Glasses Pro mobile eye tracker was proposed. Using the mobile system and a training simulator, a measurement stand was prepared for the flight of the aircraft configured as follows: Seneca II airplane with a three-point hidden chassis, location: EPRZ airport - Rzeszów Jasionka. An aviation task was prepared, which was carried out by participants in the experiment. A Tobii project was developed to define several Area of Interests (AOI) for key instruments in the cockpit of the used aircraft. It was observed that for the implementation of an exemplary task, there is no possibility of smoothly modifying coordinates of areas of interest of AOI in subsequent frames of the recorded video stream. Authors designed in the Matlab environment, the Smart Trainer application for a smooth analysis of attention using the characteristic points tracking mechanism. A fuzzy AOI contour is proposed using a modified form of the Butterworth 2D filter for individual instruments, which allows more effective registration of fixations. During the measurement it is possible to observe fixation histograms for a defined set of instruments. © 2020, Springer Nature Switzerland AG.","Eye tracking; Pilot attention; Tobii Glasses Pro","Aircraft instruments; Butterworth filters; Cockpits (aircraft); Fuzzy filters; Glass; Training aircraft; Characteristic point; MATLAB environment; Measurement stand; Mobile eye-tracking; Pilot attention; Tobii Glasses Pro; Tracking mechanism; Training simulator; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85065892520
"Bakry A., Al-Khatib R., Negm R., Sabra E., Maher M., Mohamed Z., Shawky D., Badawi A.","57208175630;57208180173;57208175536;57208178594;57208178316;57204583324;8213342300;57004506800;","Using Eye Movement to Assess Auditory Attention",2020,"Advances in Intelligent Systems and Computing","921",,,"200","208",,1,"10.1007/978-3-030-14118-9_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064059747&doi=10.1007%2f978-3-030-14118-9_20&partnerID=40&md5=624392a2adaa68ae93e1d80b33bbae05","Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Engineering Mathematics Department, Faculty of Engineering, Cairo University, Giza, Egypt","Bakry, A., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Al-Khatib, R., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Negm, R., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Sabra, E., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Maher, M., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Mohamed, Z., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt; Shawky, D., Engineering Mathematics Department, Faculty of Engineering, Cairo University, Giza, Egypt; Badawi, A., Center for Learning Technologies, University of Science and Technology, Zewail City, Giza, Egypt","Eye movement has been found to be one of the factors that highly affect attention. In this paper, a study for detecting the influence of eye movement on attention is presented. Forty-three participants attended four sessions introducing different auditory stimuli while wearing a 14-channel wireless headset that collects their EEG signals. The participants were asked to fix their eyes in two of the sessions and they were allowed to move them freely in the other two. Their attention during the sessions was estimated using questionnaires that assess the information they were able to gain after the sessions. Different classifiers were trained to predict the attention scores when subjects were freely moving their eyes or fixing them. Among the trained classifiers, K nearest-neighbors classifiers yielded the best classification accuracy, which varied with the addition of the eye-movement features from about 72% to 87%. Thus, the obtained results show that there is an effect of eye movement on the gained attention. Hence, it is possible to detect attention of subjects using their eye movement patterns. © 2020, Springer Nature Switzerland AG.","Attention measurement; EEG; Eye gaze; Eye movement","Electroencephalography; Machine learning; Nearest neighbor search; Surveys; Auditory attention; Auditory stimuli; Classification accuracy; Eye movement patterns; Eye-gaze; Freely moving; K-nearest neighbors classifiers; Wireless headsets; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85064059747
"Vielhaben J., Camalan H., Samek W., Wenzel M.","57215313715;57215335017;40762215900;56414278600;","Viewport forecasting in 360° virtual reality videos with machine learning",2019,"Proceedings - 2019 IEEE International Conference on Artificial Intelligence and Virtual Reality, AIVR 2019",,,"8942328","74","81",,,"10.1109/AIVR46125.2019.00020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078038862&doi=10.1109%2fAIVR46125.2019.00020&partnerID=40&md5=984464b5bafbe8dd995de066b3ae9488","Machine Learning Group, Fraunhofer Heinrich-Hertz-Institute, Berlin, Germany","Vielhaben, J., Machine Learning Group, Fraunhofer Heinrich-Hertz-Institute, Berlin, Germany; Camalan, H., Machine Learning Group, Fraunhofer Heinrich-Hertz-Institute, Berlin, Germany; Samek, W., Machine Learning Group, Fraunhofer Heinrich-Hertz-Institute, Berlin, Germany; Wenzel, M., Machine Learning Group, Fraunhofer Heinrich-Hertz-Institute, Berlin, Germany","Objective. Virtual reality (VR) cloud gaming and 360° video streaming are on the rise. With a VR headset, viewers can individually choose the perspective they see on the head-mounted display by turning their head, which creates the illusion of being in a virtual room. In this experimental study, we applied machine learning methods to anticipate future head rotations (a) from preceding head and eye motions, and (b) from the statistics of other spherical video viewers. Approach. Ten study participants watched each 3 1/3 hours of spherical video clips, while head and eye gaze motions were tracked, using a VR headset with a built-in eye tracker. Machine learning models were trained on the recorded head and gaze trajectories to predict (a) changes of head orientation and (b) the viewport from population statistics. Results. We assembled a dataset of head and gaze trajectories of spherical video viewers with great stimulus variability. We extracted statistical features from these time series and showed that a Support Vector Machine can classify the range of future head movements with a time horizon of up to one second with good accuracy. Even population statistics among only ten subjects show prediction success above chance level. %Both approaches resulted in a considerable amount of prediction success using head movements, but using gaze movement did not contribute to prediction performance in a meaningful way. Even basic machine learning models can successfully predict head movement and aspects thereof, while being naive to visual content. Significance. Viewport forecasting opens up various avenues to optimize VR rendering and transmission. While the viewer can see only a section of the surrounding 360° sphere, the entire panorama has typically to be rendered and/or broadcast. The reason is rooted in the transmission delay, which has to be taken into account in order to avoid simulator sickness due to motion-To-photon latencies. Knowing in advance, where the viewer is going to look at may help to make cloud rendering and video streaming of VR content more efficient and, ultimately, the VR experience more appealing. © 2019 IEEE.","360° Video; Body Motion Prediction; Cloud Gaming; Eye Tracking; Head Mounted Display; Machine Learning; Virtual Reality","E-learning; Eye tracking; Forecasting; Helmet mounted displays; Learning systems; Machine learning; Population statistics; Rendering (computer graphics); Spheres; Support vector machines; Video streaming; Virtual reality; Applied machine learning; Body motions; Cloud gamings; Head mounted displays; Machine learning models; Prediction performance; Statistical features; Transmission delays; Motion estimation",Conference Paper,"Final","",Scopus,2-s2.0-85078038862
"Fu B., Steichen B., Zhang W.","57220885355;25928750600;55975167900;","Towards Adaptive Ontology Visualization - Predicting User Success from Behavioral Data",2019,"International Journal of Semantic Computing","13","4",,"431","452",,1,"10.1142/S1793351X1940018X","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077181829&doi=10.1142%2fS1793351X1940018X&partnerID=40&md5=ac6b285a5ac83faafabc4e8ba9a7ea55","Computer Engineering and Computer Science, California State University Long Beach, Long Beach, CA  90840, United States; Computer Science, California State Polytechnic University Pomona, Pomona, CA  91768, United States","Fu, B., Computer Engineering and Computer Science, California State University Long Beach, Long Beach, CA  90840, United States; Steichen, B., Computer Science, California State Polytechnic University Pomona, Pomona, CA  91768, United States; Zhang, W., Computer Engineering and Computer Science, California State University Long Beach, Long Beach, CA  90840, United States","Ontology visualization plays an important role in human data interaction by offering clarity and insight for complex structured datasets. Recent usability studies of ontology visualization techniques have added to our understanding of desired features when assisting users in the interactive process. However, user behavioral data such as eye gaze and event logs have largely been used as indirect evidence to explain why a user may have carried out certain tasks in a controlled environment, as opposed to direct input that informs the underlying visualization system. Although findings from usability studies have contributed to the refinement of ontology visualizations as a whole, the visualization techniques themselves remain a one-size-fits-all approach, where all users are presented with the same visualizations and interactive features. By contrast, this paper investigates the feasibility of using behavioral data, such as user gaze and event logs, as real-time indicators of how appropriate or effective a given visualization may be for a specific user at a moment in time, which in turn may be used to inform the adaptation of the visualization to the user on the fly. To this end, we apply established predictive modeling techniques in Machine Learning to predict user success using gaze data and event logs. We present a detailed analysis from a controlled experiment and demonstrate such predictions are not only feasible, but can also be significantly better than a baseline classifier during visualization usage. These predictions can then be used to drive the adaptations of visual systems in providing ad hoc visualizations on a per user basis, which in turn may increase individual user success and performance. Furthermore, we demonstrate the prediction performance using several different feature sets, and report on the results generated from several notable classifiers, where a decision tree-based learning model using a boosting algorithm produced the best overall results. © 2019 World Scientific Publishing Company.","adaptive visualization; Eye tracking; machine learning; ontology visualization; user prediction","Data visualization; Decision trees; Digital storage; Eye tracking; Forecasting; Learning systems; Machine learning; Ontology; Trees (mathematics); Visualization; Adaptive visualization; Controlled environment; Controlled experiment; Interactive features; Ontology visualizations; Prediction performance; Visualization system; Visualization technique; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85077181829
"Ullah A., Wang J., Shahid Anwar M., Ahmad U., Saeed U., Fei Z.","57204010903;55904263200;57208044272;57204023672;57206662682;7005339161;","Facial expression recognition of nonlinear facial variations using deep locality de-expression residue learning in the wild",2019,"Electronics (Switzerland)","8","12","1487","","",,3,"10.3390/electronics8121487","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076624713&doi=10.3390%2felectronics8121487&partnerID=40&md5=255a1ab53f795553921684db6b31fe7d","School of Information and Electronics, Beijing Institute of Technology, Haidian District, Beijing, 100081, China; Riphah International University, Faisalabad Campus, Faisalabad, 38000, Pakistan; School of Computer Science and Technology, Beijing Institute of Technology, Haidian District, Beijing, 100081, China","Ullah, A., School of Information and Electronics, Beijing Institute of Technology, Haidian District, Beijing, 100081, China, Riphah International University, Faisalabad Campus, Faisalabad, 38000, Pakistan; Wang, J., School of Information and Electronics, Beijing Institute of Technology, Haidian District, Beijing, 100081, China; Shahid Anwar, M., School of Information and Electronics, Beijing Institute of Technology, Haidian District, Beijing, 100081, China; Ahmad, U., School of Computer Science and Technology, Beijing Institute of Technology, Haidian District, Beijing, 100081, China; Saeed, U., School of Computer Science and Technology, Beijing Institute of Technology, Haidian District, Beijing, 100081, China; Fei, Z., School of Information and Electronics, Beijing Institute of Technology, Haidian District, Beijing, 100081, China","Automatic facial expression recognition is an emerging field. Moreover, the interest has been increased with the transition from laboratory-controlled conditions to in the wild scenarios. Most of the research has been done over nonoccluded faces under the constrained environment, while automatic facial expression is less understood/implemented for partial occlusion in the real world conditions. Apart from that, our research aims to tackle the issues of overfitting (caused by the shortage of adequate training data) and to alleviate the expression-unrelated/intraclass/nonlinear facial variations, such as head pose estimation, eye gaze estimation, intensity and microexpressions. In our research, we control the magnitude of each Action Unit (AU) and combine several of the Action Unit combinations to leverage learning from the generative and discriminative representations for automatic FER. We have also addressed the problem of diversification of expressions from lab controlled to real-world scenarios from our cross-database study and proposed a model for enhancement of the discriminative power of deep features while increasing the interclass scatters, by preserving the locality closeness. Furthermore, facial expression consists of an expressive component as well as neutral component, so we proposed a generative model which is capable of generating neutral expression from an input image using cGAN. The expressive component is filtered and passed to the intermediate layers and the process is called De-expression Residue Learning. The residue in the intermediate/middle layers is very important for learning through expressive components. Finally, we validate the effectiveness of our method (DLP-DeRL) through qualitative and quantitative experimental results using four databases. Our method is more accurate and robust, and outperforms all the existing methods (hand crafted features and deep learning) while dealing the images in the wild. © 2019, MDPI AG. All rights reserved.","Conditional graphical adversarial network; De-expression residue; Deep locality preserving; Facial variations",,Article,"Final","",Scopus,2-s2.0-85076624713
"Ferreira D.S., Ramalho G.L.B., Torres D., Tobias A.H.G., Rezende M.T., Medeiros F.N.S., Bianchi A.G.C., Carneiro C.M., Ushizima D.M.","57190837735;36900177100;57210960037;57093336500;57192720442;7005769170;57192394965;54790578700;14827670900;","Saliency-driven system models for cell analysis with deep learning",2019,"Computer Methods and Programs in Biomedicine","182",,"105053","","",,1,"10.1016/j.cmpb.2019.105053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072048959&doi=10.1016%2fj.cmpb.2019.105053&partnerID=40&md5=ca0b0ac3f84efd7d23a9e3e336832a46","Berkeley Institute of Data Science, University of California, Berkeley, CA, United States; Lawrence Berkeley National Laboratory, Berkeley, CA, United States; Departamento de Engenharia de Teleinformática, Universidade Federal do Ceará, Fortaleza, CE, Brazil; Instituto Federal de Educação, Ciência e Tecnologia do Ceará, Maracanaú, CE, Brazil; Departamento de Física, Universidade Federal do Ceará, Fortaleza, CE, Brazil; Universidade Federal de Ouro Preto, Ouro Preto, MG, Brazil","Ferreira, D.S., Berkeley Institute of Data Science, University of California, Berkeley, CA, United States, Lawrence Berkeley National Laboratory, Berkeley, CA, United States, Departamento de Engenharia de Teleinformática, Universidade Federal do Ceará, Fortaleza, CE, Brazil, Instituto Federal de Educação, Ciência e Tecnologia do Ceará, Maracanaú, CE, Brazil; Ramalho, G.L.B., Instituto Federal de Educação, Ciência e Tecnologia do Ceará, Maracanaú, CE, Brazil; Torres, D., Departamento de Física, Universidade Federal do Ceará, Fortaleza, CE, Brazil; Tobias, A.H.G., Universidade Federal de Ouro Preto, Ouro Preto, MG, Brazil; Rezende, M.T., Universidade Federal de Ouro Preto, Ouro Preto, MG, Brazil; Medeiros, F.N.S., Departamento de Engenharia de Teleinformática, Universidade Federal do Ceará, Fortaleza, CE, Brazil; Bianchi, A.G.C., Universidade Federal de Ouro Preto, Ouro Preto, MG, Brazil; Carneiro, C.M., Universidade Federal de Ouro Preto, Ouro Preto, MG, Brazil; Ushizima, D.M., Berkeley Institute of Data Science, University of California, Berkeley, CA, United States, Lawrence Berkeley National Laboratory, Berkeley, CA, United States, Departamento de Engenharia de Teleinformática, Universidade Federal do Ceará, Fortaleza, CE, Brazil","Background and objectives: Saliency refers to the visual perception quality that makes objects in a scene to stand out from others and attract attention. While computational saliency models can simulate the expert's visual attention, there is little evidence about how these models perform when used to predict the cytopathologist's eye fixations. Saliency models may be the key to instrumenting fast object detection on large Pap smear slides under real noisy conditions, artifacts, and cell occlusions. This paper describes how our computational schemes retrieve regions of interest (ROI) of clinical relevance using visual attention models. We also compare the performance of different computed saliency models as part of cell screening tasks, aiming to design a computer-aided diagnosis systems that supports cytopathologists. Method: We record eye fixation maps from cytopathologists at work, and compare with 13 different saliency prediction algorithms, including deep learning. We develop cell-specific convolutional neural networks (CNN) to investigate the impact of bottom-up and top-down factors on saliency prediction from real routine exams. By combining the eye tracking data from pathologists with computed saliency models, we assess algorithms reliability in identifying clinically relevant cells. Results:The proposed cell-specific CNN model outperforms all other saliency prediction methods, particularly regarding the number of false positives. Our algorithm also detects the most clinically relevant cells, which are among the three top salient regions, with accuracy above 98% for all diseases, except carcinoma (87%). Bottom-up methods performed satisfactorily, with saliency maps that enabled ROI detection above 75% for carcinoma and 86% for other pathologies. Conclusions:ROIs extraction using our saliency prediction methods enabled ranking the most relevant clinical areas within the image, a viable data reduction strategy to guide automatic analyses of Pap smear slides. Top-down factors for saliency prediction on cell images increases the accuracy of the estimated maps while bottom-up algorithms proved to be useful for predicting the cytopathologist's eye fixations depending on parameters, such as the number of false positive and negative. Our contributions are: comparison among 13 state-of-the-art saliency models to cytopathologists’ visual attention and deliver a method that the associate the most conspicuous regions to clinically relevant cells. © 2019","Cell analysis; Convolutional neural network; Eye tracking experiment; Saliency prediction","Behavioral research; Cells; Computer aided diagnosis; Convolution; Cytology; Eye tracking; Forecasting; Neural networks; Object detection; Bottom-up and top-down; Cell analysis; Computational schemes; Computer aided diagnosis systems; Convolutional neural network; Prediction algorithms; Regions of interest; Visual attention model; Deep learning; Article; convolutional neural network; cytopathologist; deep learning; eye fixation; eye tracking; human; salience network; female; Papanicolaou test; pathology; uterine cervix; Cervix Uteri; Deep Learning; Female; Humans; Neural Networks, Computer; Papanicolaou Test",Article,"Final","",Scopus,2-s2.0-85072048959
"Lin C.J., Prasetyo Y.T., Widyaningrum R.","35345469600;57204827000;57222252302;","Eye movement measures for predicting eye gaze accuracy and symptoms in 2D and 3D displays",2019,"Displays","60",,,"1","8",,23,"10.1016/j.displa.2019.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071083446&doi=10.1016%2fj.displa.2019.08.002&partnerID=40&md5=dd3c2f62d790d4ba82cc0ec671f574a1","Department of Industrial Management, National Taiwan University of Science and Technology, No. 43, Sec. 4, Keelung Rd., Da‘an Dist., Taipei City, 10607, Taiwan; School of Industrial Engineering and Engineering Management, Mapua University, 658 Muralla St., Intramuros, Manila  1002, Philippines; Department of Industrial Engineering, Sepuluh Nopember Institute of Technology, Kampus ITS Sukolilo, Surabaya, 60111, Indonesia","Lin, C.J., Department of Industrial Management, National Taiwan University of Science and Technology, No. 43, Sec. 4, Keelung Rd., Da‘an Dist., Taipei City, 10607, Taiwan; Prasetyo, Y.T., School of Industrial Engineering and Engineering Management, Mapua University, 658 Muralla St., Intramuros, Manila  1002, Philippines; Widyaningrum, R., Department of Industrial Engineering, Sepuluh Nopember Institute of Technology, Kampus ITS Sukolilo, Surabaya, 60111, Indonesia","The current study applied Structural Equation Modeling (SEM) to analyze the interrelationship among index of difficulty (ID), environment, saccade duration (SD), revisited fixation duration (RFD), number of fixation (NF), pupil size (PS), eye gaze accuracy (AC), and symptoms simultaneously. SD, RFD, NF, PS, and AC were measured by utilizing the Tobii eye tracker system. Twelve participants were recruited to perform multi-directional tapping task using within-subject design with two different environments (2D screen display and 3D stereoscopic display) and six different levels of ID. SEM showed that ID had significant direct effects on SD and RFD while environment was found had significant direct effects on SD, RFD, PS, AC, and symptoms. Among selected eye movement measures, NF was found to be the best predictor of AC and PS was found to be the best predictor of symptoms. In addition, RFD was also found to be a good predictor of symptoms. Our results found that higher AC was achieved by projecting the image in the 2D screen display with higher ID and it resulted in higher SD and higher NF. Regarding the symptoms, our results found that lower symptoms were achieved by projecting the image in the 2D screen display with lower ID and it resulted in lower PS, and lower RFD. Practitioner Summary: The SEM could provide valuable theoretical foundations to identify the interrelationship among eye movement measures, AC, and symptoms particularly for VR researchers and interface developers. © 2019 Elsevier B.V.","Eye movement; Stereoscopic; Structural equation modeling","Eye movements; Eye tracking; Stereo image processing; Eye-movement measures; Fixation duration; Number of fixations; Screen displays; Stereoscopic; Stereoscopic display; Structural equation modeling; Theoretical foundations; Three dimensional displays",Article,"Final","",Scopus,2-s2.0-85071083446
"Vasilyev A.","57204322635;","Optimal Control of Eye Movements during Visual Search",2019,"IEEE Transactions on Cognitive and Developmental Systems","11","4","8501565","548","559",,1,"10.1109/TCDS.2018.2877128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055188077&doi=10.1109%2fTCDS.2018.2877128&partnerID=40&md5=bd6b190e6de1b571af70cb9b7e40be4e","School of Electrical Engineering and Computer Science, Queen Mary University of London, London, E1 4NS, United Kingdom","Vasilyev, A., School of Electrical Engineering and Computer Science, Queen Mary University of London, London, E1 4NS, United Kingdom","In this paper, we study the problem of an optimal oculomotor control during the execution of visual search tasks. We introduce a computational model of human eye movements, which takes into account various constraints of the human visual and oculomotor systems. In the model, the choice of the subsequent fixation location is posed as a problem of a stochastic optimal control, which relies on reinforcement learning methods. We show that if biological constraints are taken into account, the trajectories simulated under a learned policy share both basic statistical properties and a scaling behavior with human eye movements. We validated our model simulations with human psychophysical eye-tracking experiments. © 2016 IEEE.","Multifractal analysis; reinforcement learning; scaling in biology; visual search","Behavioral research; Biological systems; Computation theory; Data mining; Data visualization; Eye tracking; Flow visualization; Information management; Job analysis; Machine learning; Reinforcement learning; Search engines; Stochastic control systems; Stochastic models; Stochastic systems; Biological system modeling; Computational model; Multifractal analysis; Observers; Resource management; Task analysis; Visual search; Eye movements",Article,"Final","",Scopus,2-s2.0-85055188077
"Zhou Y., Feng T., Shuai S., Li X., Sun L., Duh H.B.L.","57202023332;57204395722;57215140927;57202025304;55492960600;6603986391;","An eye-tracking dataset for visual attention modelling in a virtual museum context",2019,"Proceedings - VRCAI 2019: 17th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry",,,"a39","","",,1,"10.1145/3359997.3365738","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077112217&doi=10.1145%2f3359997.3365738&partnerID=40&md5=c01c954695434b4c301499aa70661c33","La Trobe University, Australia; Zhejiang University, China","Zhou, Y., La Trobe University, Australia, Zhejiang University, China; Feng, T., La Trobe University, Australia; Shuai, S., Zhejiang University, China; Li, X., Zhejiang University, China; Sun, L., Zhejiang University, China; Duh, H.B.L., La Trobe University, Australia","Predicting the user's visual attention enables a virtual reality (VR) environment to provide a context-aware and interactive user experience. Researchers have attempted to understand visual attention using eye-tracking data in a 2D plane. In this poster, we propose the first 3D eye-tracking dataset for visual attention modelling in the context of a virtual museum. It comprises about 7 million records and may facilitate visual attention modelling in a 3D VR space. © 2019 Association for Computing Machinery.","Eye-tracking datasets; Gaze detection; Neural networks; Visual attention","3D modeling; Behavioral research; Interactive computer graphics; Museums; Neural networks; Virtual reality; Context-Aware; Gaze detection; User experience; Virtual museum; Visual Attention; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85077112217
"Matos A.C., Azevedo Terroso T., Corte-Real L., Carvalho P.","57214532199;57190289573;9273876500;55951850400;","Stereo vision system for human motion analysis in a rehabilitation context",2019,"Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization","7","5-6",,"707","723",,,"10.1080/21681163.2018.1542346","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057331484&doi=10.1080%2f21681163.2018.1542346&partnerID=40&md5=93f02870fd494314bbef74149fefb5b3","FEUP - Faculdade de Engenharia da Universidade do Porto, Porto, Portugal; Tecnologia e Ciência, INESC TEC - Instituto de Engenharia de Sistemas e Computadores, Porto, Portugal; Instituto Politécnico do Porto, ESMAD, IPP - Escola Superior de Media Artes e Design, Vila do Conde, Portugal","Matos, A.C., FEUP - Faculdade de Engenharia da Universidade do Porto, Porto, Portugal; Azevedo Terroso, T., Tecnologia e Ciência, INESC TEC - Instituto de Engenharia de Sistemas e Computadores, Porto, Portugal, Instituto Politécnico do Porto, ESMAD, IPP - Escola Superior de Media Artes e Design, Vila do Conde, Portugal; Corte-Real, L., FEUP - Faculdade de Engenharia da Universidade do Porto, Porto, Portugal, Tecnologia e Ciência, INESC TEC - Instituto de Engenharia de Sistemas e Computadores, Porto, Portugal; Carvalho, P., Tecnologia e Ciência, INESC TEC - Instituto de Engenharia de Sistemas e Computadores, Porto, Portugal","The present demographic trends point to an increase in aged population and chronic diseases which symptoms can be alleviated through rehabilitation. The applicability of passive 3D reconstruction for motion tracking in a rehabilitation context was explored using a stereo camera. The camera was used to acquire depth and color information from which the 3D position of predefined joints was recovered based on: kinematic relationships, anthropometrically feasible lengths and temporal consistency. Finally, a set of quantitative measures were extracted to evaluate the performed rehabilitation exercises. Validation study using data provided by a marker based as ground-truth revealed that our proposal achieved errors within the range of state-of-the-art active markerless systems and visual evaluations done by physical therapists. The obtained results are promising and demonstrate that the developed methodology allows the analysis of human motion for a rehabilitation purpose. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.","biomedical engineering; human motion tracking; Rehabilitation; stereo vision","abduction; analytic method; Article; chronic disease; exercise; eye tracking; hand surgery; human; image analysis; image segmentation; imaging algorithm; information processing; joint function; kinanthropometry; kinematics; knee surgery; mathematical analysis; motion analysis; motion correction algorithm; physiotherapist; priority journal; quantitative study; range of motion; recognition; rehabilitation research; three dimensional imaging; validation process; validation study",Article,"Final","",Scopus,2-s2.0-85057331484
"Xu D., Tan J.","56801200900;57217004368;","Design of Close Scleral Vascular Imaging System Used for Gazing Tracking",2019,"Proceedings - 2019 2nd International Conference on Safety Produce Informatization, IICSPI 2019",,,"9096019","486","489",,,"10.1109/IICSPI48186.2019.9096019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085705904&doi=10.1109%2fIICSPI48186.2019.9096019&partnerID=40&md5=e55f1b7be3998a341e51e75eb44f5216","Beihang University, School of Instrumentation Science and Opto-electronics Engineering, Beijing, China","Xu, D., Beihang University, School of Instrumentation Science and Opto-electronics Engineering, Beijing, China; Tan, J., Beihang University, School of Instrumentation Science and Opto-electronics Engineering, Beijing, China","Scleral blood vessels are very stable biological features. Lightweight head-mounted scleral vascular imaging system can be widely used in personal identification, gaze tracking and many other fields. However, it is difficult to acquire clear images of scleral blood vessels at a small distance with traditional optical imaging systems. We proposed a new scleral vascular imaging system based on micro-lens array, which can capture the light field of scleral blood vessels near eyes with sub-image array. The system has a simple and integrative structure. And it can easily reconstruct the 3D position and structure of scleral blood vessels from multiple sub-aperture images. © 2019 IEEE.","light field; micro-lens array; optical imaging system; scleral vascular","Blood; Eye tracking; Imaging systems; Microlenses; Biological features; Gaze tracking; Integrative structure; Micro-lens arrays; Optical imaging system; Personal identification; Sub-apertures; Vascular imaging; Blood vessels",Conference Paper,"Final","",Scopus,2-s2.0-85085705904
"Valenzuela A., Arellano C., Tapia J.","57216819899;57207768339;7005419930;","An efficient dense network for semantic segmentation of eyes images captured with virtual reality lens",2019,"Proceedings - 15th International Conference on Signal Image Technology and Internet Based Systems, SISITS 2019",,,"9067945","28","34",,1,"10.1109/SITIS.2019.00017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084850623&doi=10.1109%2fSITIS.2019.00017&partnerID=40&md5=1015867287d9c9869cfa456c4142196b","Universidad Andres Bello, Department of Engineering Sciences, Chile; Universidad Tecnologica de Chile, INACAP, Santiago, Chile","Valenzuela, A., Universidad Andres Bello, Department of Engineering Sciences, Chile; Arellano, C.; Tapia, J., Universidad Andres Bello, Department of Engineering Sciences, Chile, Universidad Tecnologica de Chile, INACAP, Santiago, Chile","Eye-tracking and Gaze estimation are difficult tasks that may be used for several applications including human-computer interfaces, salience detection and Virtual reality amongst others. This paper presents a segmentation algorithm based on deep learning that efficiently discriminates pupils, iris, and sclera from the background in images captured using a Virtual Reality lens. A light network called DensetNet10 trained from scratch is proposed. It contains fewer parameters than traditional architectures based on DenseNet which allows it to be used in mobile device applications. Experiments show that this network achieved higher IOU rates when comparing with DensetNet56-67-103 and DeeplabV3+ models in the Facebook database. Furthermore, this method reached 8th place in The Facebook semantic segmentation challenge with 0.94293 mean IOU and 202.084 parameters with a final score of 0.97147. © 2019 IEEE.","Biometrics; Facebook Challengue; Semantic Segmentation","Deep learning; Eye tracking; Semantic Web; Semantics; Social networking (online); Virtual reality; Dense network; Facebook; Gaze estimation; Human computer interfaces; Mobile device applications; Segmentation algorithms; Semantic segmentation; Traditional architecture; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85084850623
"Chopade P., Edwards D., Khan S.M., Andrade A., Pu S.","37460936700;57204202470;57202790555;35185951800;57216152180;","CPSX: Using AI-Machine Learning for Mapping Human-Human Interaction and Measurement of CPS Teamwork Skills",2019,"2019 IEEE International Symposium on Technologies for Homeland Security, HST 2019",,,"9032906","","",,3,"10.1109/HST47167.2019.9032906","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082705770&doi=10.1109%2fHST47167.2019.9032906&partnerID=40&md5=db8e1ba98824a4c66eeded6080688d1b","Artificial Intelligence and Machine Learning, ACTNext, ACT Inc., Iowa City, IA, United States","Chopade, P., Artificial Intelligence and Machine Learning, ACTNext, ACT Inc., Iowa City, IA, United States; Edwards, D., Artificial Intelligence and Machine Learning, ACTNext, ACT Inc., Iowa City, IA, United States; Khan, S.M., Artificial Intelligence and Machine Learning, ACTNext, ACT Inc., Iowa City, IA, United States; Andrade, A., Artificial Intelligence and Machine Learning, ACTNext, ACT Inc., Iowa City, IA, United States; Pu, S., Artificial Intelligence and Machine Learning, ACTNext, ACT Inc., Iowa City, IA, United States","The objective of this work is to present a machine learning (ML)-based framework to identify evidence about collaborative problem solving (CPS) cognitive (teamwork) and social-emotional learning (SEL) skills from the dyadic (human-human-HH) interactions. This work extends our previous work (Chopade et al. IEEE HST 2018, LAK2019) [1], [2]. Explicitly, we are interested in how teamwork skills and team dynamics are demonstrated as verbal and nonverbal behaviors, and how these behaviors can be captured and analyzed via passive data collection. For this work we use a two-player cooperative CPS game, Crisis in Space (CIS) from LRNG (Previously GlassLab Inc). During the summer of 2018, we implemented this CIS game for interns as a group study. A total of 34 participants played the game and provided study and survey data. During the study, we collected participants' game play data, such as audio, video and eye tracking data streams. This research involves analyzing CIS multimodal game data, and developing skill models, and machine learning techniques for CPS skills measurement. In this paper, we present our ML framework for the analysis of audio data along with preliminary results from a pilot study. The analysis of audio data uses natural language processing (NLP) techniques, such as bag-of-words and sentence embedding. Our preliminary results show that various NLP features can be used to describe successful and unsuccessful CPS performances. The ML based framework supports the development of evidence centered design for teamwork skills-mapping and aims to help teams operate effectively in a complex situation. Potential applications of this work include support for the Department of Homeland Security (DHS), and the US Army for the development of learner and team centric training, cohort, and team behavioral skill-mapping. © 2019 IEEE.","Artificial Intelligence; Behaviors; Collaborative Problem Solving (CPS); Congitive Skills; Human-Human Interactions; Machine Learning; Natural Language Processing (NLP); Performance Measurement; Social-Emotional Learning (SEL) Skills; Teamwork Skills","Artificial intelligence; Data streams; Eye tracking; Learning algorithms; Learning systems; Machine learning; Mapping; National security; Problem solving; Security systems; Surveys; Behaviors; Collaborative problem solving; Congitive Skills; Emotional learning; Human-human interactions; NAtural language processing; Performance measurements; Teamwork skills; Natural language processing systems",Conference Paper,"Final","",Scopus,2-s2.0-85082705770
"Su D., Li Y.F., Chen H.","57193014913;8589964900;57191584510;","Region-wise Polynomial Regression for 3D Mobile Gaze Estimation",2019,"IEEE International Conference on Intelligent Robots and Systems",,,"8968536","907","913",,,"10.1109/IROS40897.2019.8968536","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081168064&doi=10.1109%2fIROS40897.2019.8968536&partnerID=40&md5=93eb16055cb1084fddd1515e4a43e745","City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong","Su, D., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Li, Y.F., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Chen, H., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong","In the context of mobile gaze tracking techniques, a 3D gaze point can be calculated as the middle point between two 3D visual axes. To infer gaze directions and eyeball positions, a nonlinear optimization problem is typically formulated to minimize the angular disparities between the training gaze directions and prediction ones. Nonetheless, the experimental results reported by some previous works show that this kind of approaches are very likely to yield large prediction errors hence considered less useful for human-machine interactions. In this study, we aim to address this widespread issue in three aspects. At first, instead of using a global regression model, a simple local polynomial model is proposed to back-project a pupil center onto its corresponding visual axis. Based on the Leave-One-Out cross-validation criterion, the partition structure is automatically learned in the process of resolving a homography-like relationship. Secondly, a good starting point for nonlinear-optimization is obtained by the image eyeball center, which can be estimated by systematic parallax errors. Meanwhile, it is necessary to add the suitable constraints for 3D eye positions. Otherwise, the optimization may end up with trivial solutions, i.e., faraway eye positions. Thirdly, we explore a strategy for designing the spatial distribution of calibration points in a principled manner. The experiment results demonstrate that an encouraging gaze estimation accuracy can be achieved by our proposed framework for both the normal vision and eyewear users. © 2019 IEEE.",,"Geometrical optics; Intelligent robots; Nonlinear programming; Polynomial regression; Systematic errors; Calibration points; Human machine interaction; Leave-one-out cross validations; Local polynomials; Non-linear optimization; Non-linear optimization problems; Partition structures; Trivial solutions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85081168064
"Parikh D., Lu Y., Xin Y., Wu D., Pelz J., Lu G.","57215126501;57212483262;57215128937;57215128978;7007018556;55872294200;","Where am i looking: Localizing gaze in reconstructed 3D space",2019,"GlobalSIP 2019 - 7th IEEE Global Conference on Signal and Information Processing, Proceedings",,,"8969158","","",,1,"10.1109/GlobalSIP45357.2019.8969158","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079278730&doi=10.1109%2fGlobalSIP45357.2019.8969158&partnerID=40&md5=c13fcaa706b29dbaeafec704ef39749f","Rochester Institute of Technology, Chester Carlson Center for Imaging Science, United States; Tencent Deep Sea Lab, China","Parikh, D., Rochester Institute of Technology, Chester Carlson Center for Imaging Science, United States; Lu, Y., Rochester Institute of Technology, Chester Carlson Center for Imaging Science, United States; Xin, Y., Tencent Deep Sea Lab, China; Wu, D., Tencent Deep Sea Lab, China; Pelz, J., Rochester Institute of Technology, Chester Carlson Center for Imaging Science, United States; Lu, G., Rochester Institute of Technology, Chester Carlson Center for Imaging Science, United States","We propose a method to estimate the 3D gaze of the observer onto the scene using a portable eye tracker with a monocular camera. We reconstruct the 3D scene using Structure from Motion (SfM) and use camera pose and 3D reconstruction information of the scene to localize the position and pose of the observer in the 3D reconstructed space. Along with the position, we can obtain the 2D gaze of the observer using an eye-tracker. Each person may have a different perspective of the same 3D object in the scene, observing it from different positions. We use this information to fuse these multiple perspectives in 3D space to get a better understanding of how differently each observer perceives the same scene, compared to others. In the entire system, we developed a convo-lutional neural network to detect and track eye movement and a camera re-localization method to localize the camera in 3D environment. Based on our novel eye tracking and camera re-localization methods, we can accurately localize the gaze in the 3D reconstructed environment. © 2019 IEEE.",,"Cameras; Eye movements; Eye tracking; 3-D environments; 3D reconstruction; 3D scenes; Entire system; Eye trackers; Monocular cameras; Re-localization; Structure from motion; Image reconstruction",Conference Paper,"Final","",Scopus,2-s2.0-85079278730
"Ishibashi K.","36147517800;","Application of deep learning to pre-processing of cousumer's eye tracking data in supermarket",2019,"IEEE International Conference on Data Mining Workshops, ICDMW","2019-November",,"8955495","341","348",,,"10.1109/ICDMW.2019.00057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078795961&doi=10.1109%2fICDMW.2019.00057&partnerID=40&md5=a0c0e0771f5b0143fd56e26459366b82","Research Institute for Socionetwork Strategies, Kansai University, Osaka, Japan","Ishibashi, K., Research Institute for Socionetwork Strategies, Kansai University, Osaka, Japan","The purpose of this study is to automate pre-processing of eye tracking data. In an investigation with eye tracking in a field such as supermarket, pre-processing of data has enormous cost. This is because it is very difficult to map consumer's gaze points to certain snapshot due to her/his movement. This study uses deep learning for automating pre-processing of eye tracking data. General object recognition using deep learning can classify an image into various classes. The proposed method attempts to classify fixated object of consumer into three classes, product, promotion and etc., based on the result of general object recognition. This paper discusses the applicability of proposed method through cross validation using eye tracking data preprocessed manually. © 2019 IEEE.","Consumer behavior; Depp learning; Eye tracking; General object recognition; Pre-processing","Consumer behavior; Data handling; Data mining; Deep learning; Object recognition; Retail stores; Cross validation; Depp learning; Gaze point; Pre-processing; Pre-processing of data; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85078795961
"Lwin K.N., Myint M., Mukada N., Yamada D., Matsuno T., Saitou K., Godou W., Sakamoto T., Minami M.","57190192601;57188841197;57192592961;57202502530;7101883099;55243677400;8602121400;15920049900;7402907499;","Sea Docking by Dual-eye Pose Estimation with Optimized Genetic Algorithm Parameters",2019,"Journal of Intelligent and Robotic Systems: Theory and Applications","96","2",,"245","266",,1,"10.1007/s10846-018-0970-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059663545&doi=10.1007%2fs10846-018-0970-x&partnerID=40&md5=9c9f416fc26bd659e92a958236f20c6e","Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan","Lwin, K.N., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Myint, M., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Mukada, N., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Yamada, D., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Matsuno, T., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Saitou, K., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Godou, W., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Sakamoto, T., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan; Minami, M., Graduate School of Natural Science and Technology, Okayama University, Okayama, Japan","Three-dimensional (3D) estimation of position and orientation (pose) using dynamic (successive) images input at video rates needs to be performed rapidly when the estimated pose is used for real-time feedback control. Single-camera 3D pose estimation has been studied thoroughly, but the estimated position accuracy in the camera depth of field has proven insufficient. Thus, docking systems for underwater vehicles with single-eye cameras have not reached practical application. The authors have proposed a new 3D pose estimation method with dual cameras that exploits the parallactic nature of stereoscopic vision to enable reliable 3D pose estimation in real time. We call this method the “real-time multi-step genetic algorithm (RM-GA).” However, optimization of the pose tracking performance has been left unchallenged despite the fact that improved tracking performance in the time domain would help improve performance and stability of the closed-loop feedback system, such as visual servoing of an underwater vehicle. This study focused on improving the dynamic performance of dual-eye real-time pose tracking by tuning RM-GA parameters and confirming optimization of the dynamical performance to estimate a target marker’s pose in real time. Then, the effectiveness and practicality of the real-time 3D pose estimation system was confirmed by conducting a sea docking experiment using the optimum RM-GA parameters in an actual marine environment with turbidity. © 2019, Springer Nature B.V.","Dual-eye tracking; Pose estimation; Real-time multi-step GA; Underwater docking; Visual servoing","Cameras; Eye tracking; Feedback control; Genetic algorithms; Gesture recognition; Stereo image processing; Time domain analysis; Visual servoing; Closed-loop feedback system; Dual eye tracking; Dynamical performance; Multi-step; Pose estimation; Position and orientations; Threedimensional (3-d); Underwater docking; Parameter estimation",Article,"Final","",Scopus,2-s2.0-85059663545
"Li C., Lin L., Zuo W., Tang J., Yang M.-H.","56699429900;15061363400;56888903800;24286986300;7404927015;","Visual Tracking via Dynamic Graph Learning",2019,"IEEE Transactions on Pattern Analysis and Machine Intelligence","41","11","8434114","2770","2782",,29,"10.1109/TPAMI.2018.2864965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052586220&doi=10.1109%2fTPAMI.2018.2864965&partnerID=40&md5=c8f6167f7c8f1e9c551a7080f1a707ac","School of Computer Science and Technology, Anhui University, Hefei, 230601, China; Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, 100190, China; Institute of Physical Science and Information Technology, Anhui University, Hefei, 230601, China; School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, 510006, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; School of Engineering, University of California, Merced, CA  95344, United States","Li, C., School of Computer Science and Technology, Anhui University, Hefei, 230601, China, Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), Beijing, 100190, China, Institute of Physical Science and Information Technology, Anhui University, Hefei, 230601, China; Lin, L., School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, 510006, China; Zuo, W., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; Tang, J., School of Computer Science and Technology, Anhui University, Hefei, 230601, China; Yang, M.-H., School of Engineering, University of California, Merced, CA  95344, United States","Existing visual tracking methods usually localize a target object with a bounding box, in which the performance of the foreground object trackers or detectors is often affected by the inclusion of background clutter. To handle this problem, we learn a patch-based graph representation for visual tracking. The tracked object is modeled by with a graph by taking a set of non-overlapping image patches as nodes, in which the weight of each node indicates how likely it belongs to the foreground and edges are weighted for indicating the appearance compatibility of two neighboring nodes. This graph is dynamically learned and applied in object tracking and model updating. During the tracking process, the proposed algorithm performs three main steps in each frame. First, the graph is initialized by assigning binary weights of some image patches to indicate the object and background patches according to the predicted bounding box. Second, the graph is optimized to refine the patch weights by using a novel alternating direction method of multipliers. Third, the object feature representation is updated by imposing the weights of patches on the extracted image features. The object location is predicted by maximizing the classification score in the structured support vector machine. Extensive experiments show that the proposed tracking algorithm performs well against the state-of-the-art methods on large-scale benchmark datasets. © 1979-2012 IEEE.","ADMM optimization; graph learning; object segmentation; Visual tracking","Clutter (information theory); Data structures; Data visualization; Edge detection; Feature extraction; Flow visualization; Object detection; Robustness (control systems); Support vector machines; Background clutter; Feature representation; Graph representation; Image edge detection; Overlapping images; State-of-the-art methods; Structured supports; Tracking algorithm; Target tracking; algorithm; article; eye tracking; learning; support vector machine",Article,"Final","",Scopus,2-s2.0-85052586220
"Gebhardt C., Hecox B., Van Opheusden B., Wigdor D., Hillis J., Hilliges O., Benko H.","56160304800;57211682052;55603068900;6507569914;57225392470;14041644100;9737287100;","Learning cooperative personalized policies from gaze data",2019,"UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology",,,,"197","208",,7,"10.1145/3332165.3347933","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074859364&doi=10.1145%2f3332165.3347933&partnerID=40&md5=33d6c48a513503dd8f3e7769fd884ea5","Facebook Reality Labs; ETH Zürich, Switzerland; Princeton University, United States; University of Toronto, Canada","Gebhardt, C., Facebook Reality Labs, ETH Zürich, Switzerland; Hecox, B., Facebook Reality Labs; Van Opheusden, B., Facebook Reality Labs, Princeton University, United States; Wigdor, D., University of Toronto, Canada; Hillis, J., Facebook Reality Labs; Hilliges, O., ETH Zürich, Switzerland; Benko, H., Facebook Reality Labs","An ideal Mixed Reality (MR) system would only present virtual information (e.g., a label) when it is useful to the person. However, deciding when a label is useful is challenging: it depends on a variety of factors, including the current task, previous knowledge, context, etc. In this paper, we propose a Reinforcement Learning (RL) method to learn when to show or hide an object's label given eye movement data. We demonstrate the capabilities of this approach by showing that an intelligent agent can learn cooperative policies that better support users in a visual search task than manually designed heuristics. Furthermore, we show the applicability of our approach to more realistic environments and use cases (e.g., grocery shopping). By posing MR object labeling as a model-free RL problem, we can learn policies implicitly by observing users' behavior without requiring a visual search model or data annotation. Copyright © 2019 Association of Computing Machinery.","Eye tracking; Mixed reality; Reinforcement learning","Eye movements; Eye tracking; Machine learning; Mixed reality; Reinforcement learning; Cooperative policy; Data annotation; Eye movement datum; Grocery shopping; Object labeling; Realistic environments; Reinforcement learning method; Virtual information; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85074859364
"Sidenmark L., Gellersen H.","57210111157;6701531333;","Eye & Head: Synergetic eye and head movement for gaze pointing and selection",2019,"UIST 2019 - Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology",,,,"1161","1174",,26,"10.1145/3332165.3347921","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074847825&doi=10.1145%2f3332165.3347921&partnerID=40&md5=6931e28f4fd5577090b42a0782dd0ec5","Lancaster University, Lancaster, United Kingdom","Sidenmark, L., Lancaster University, Lancaster, United Kingdom; Gellersen, H., Lancaster University, Lancaster, United Kingdom","Eye gaze involves the coordination of eye and head movement to acquire gaze targets, but existing approaches to gaze pointing are based on eye-tracking in abstraction from head motion. We propose to leverage the synergetic movement of eye and head, and identify design principles for Eye & Head gaze interaction. We introduce three novel techniques that build on the distinction of head-supported versus eyes-only gaze, to enable dynamic coupling of gaze and pointer, hover interaction, visual exploration around pre-selections, and iterative and fast con-frmation of targets. We demonstrate Eye & Head interaction on applications in virtual reality, and evaluate our techniques against baselines in pointing and confrmation studies. Our results show that Eye & Head techniques enable novel gaze behaviours that provide users with more control and fexibility in fast gaze pointing and selection. Copyright © 2019 Association of Computing Machinery.","3D interaction; Eye tracking; Eye-head coordination; Gaze interaction; Target selection; Virtual reality","Eye movements; Iterative methods; Target tracking; User interfaces; Virtual reality; 3D interactions; Design Principles; Dynamic couplings; Eye-head coordination; Gaze interaction; Novel techniques; Target selection; Visual exploration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85074847825
"Zhu Y., Sun W., Yuan T.T., Li J.","57211862124;57211851905;57211853615;57211852866;","Gaze detection and prediction using data from infrared cameras",2019,"MAHCI 2019 - Proceedings of the 2nd Workshop on Multimedia for Accessible Human Computer Interfaces, co-located with MM 2019",,,,"41","46",,,"10.1145/3347319.3356838","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075148730&doi=10.1145%2f3347319.3356838&partnerID=40&md5=a287a1f656e50b9971f53781c70ffd15","Futurewei Technologies, Framingham, MA, United States; Huawei Technologies, Xi'an, China; Huawei Technologies, Shenzhen, China","Zhu, Y., Futurewei Technologies, Framingham, MA, United States; Sun, W., Huawei Technologies, Xi'an, China; Yuan, T.T., Huawei Technologies, Shenzhen, China; Li, J., Futurewei Technologies, Framingham, MA, United States","Knowing the point of gaze on a screen can benefit a variety of applications and improve user experiences. Some electronic devices with infrared cameras can generate 3D point cloud for user identification. We propose a paradigm to use 3D point cloud and eye images for gaze detection and prediction. Our method fuses 3D point cloud with eye images by image registration methods. We develop a cost function to detect saggital plane from point cloud data, and reconstruct a symmetric face by saggital plane. Symmetric face data increase the accuracy of gaze detection. We use long-short term memory models to track head and eye movement, and predict next point of gaze. Our method utilizes the existing hardware setup and provides options to improve user experiences. © 2019 Association for Computing Machinery.","Gaze contingent interface; Gaze detection; Gaze tracking; Human computer interaction","Cost functions; Eye movements; Forecasting; Human computer interaction; Infrared devices; Temperature indicating cameras; Electronic device; Gaze detection; Gaze tracking; Gaze-contingent; Infra-red cameras; Registration methods; Short term memory; User identification; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85075148730
"Chaabouni S., Precioso F.","57190285991;6508047326;","Impact of saliency and gaze features on visual control: Gaze-saliency interest estimator",2019,"MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia",,,,"1367","1374",,1,"10.1145/3343031.3350964","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074870668&doi=10.1145%2f3343031.3350964&partnerID=40&md5=7645d96768a53c0f99af5cc095011ab6","Universite Cote d'Azur, CNRS, I3S, Nice, France","Chaabouni, S., Universite Cote d'Azur, CNRS, I3S, Nice, France; Precioso, F., Universite Cote d'Azur, CNRS, I3S, Nice, France","Predicting user intent from gaze presents a challenging question for developing real-time interactive systems like interactive search engine, implicit annotations of large datasets or intelligent robot behavior. Indeed, solutions to annotate easily large sets of images while reducing the burden of annotators is a key aspect for current machine learning techniques. We propose in this paper to design an estimator of the user interest for a given visual content based on eye-tracker feature analysis. We revise existing gaze-based interest estimator, and analyze the impact of the intrinsic saliency of the content displayed for interest estimation. We first explore low-level saliency prediction and propose a new gaze and saliency interest estimator. Experimental results show the advantage of our method for the annotation task in a weakly supervised context. In particular, we extend previous evaluation criteria on new experimental protocol displaying four images by frame as a first step towards ""Google Image search-like"" interfaces. Our Gaze and Saliency Interest Estimator (GSIE) reaches an overall accuracy of 83% in average of user interest prediction. If we consider the accuracy reached in a limited time, the GSIE is 70% in average within about 500ms and 80% in average within 1000ms. This result confirms our GSIE as an efficient real-time visual control solution. © 2019 Association for Computing Machinery.","Datasets; Gaze detection; Neural networks; Text tagging","Forecasting; Intelligent robots; Large dataset; Learning systems; Neural networks; Real time systems; Search engines; Visual servoing; Datasets; Evaluation criteria; Experimental protocols; Gaze detection; Interactive search; Machine learning techniques; Text tagging; User interest predictions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85074870668
"Mall S., Brennan P.C., Mello-Thoms C.","56694126500;7402306108;6701661177;","Can a Machine Learn from Radiologists’ Visual Search Behaviour and Their Interpretation of Mammograms—a Deep-Learning Study",2019,"Journal of Digital Imaging","32","5",,"746","760",,2,"10.1007/s10278-018-00174-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068646450&doi=10.1007%2fs10278-018-00174-z&partnerID=40&md5=d65057bc410436a1334e94c73fc6fb4b","Medical Image Optimisation and Perception Research Group (MIOPeG), Faculty of Medicine and Health, University of Sydney, 75 East Street, Lidcombe, NSW  2141, Australia; Department of Radiology, University of Iowa, 200 Hawkins Drive, Iowa City, IA  52242, United States","Mall, S., Medical Image Optimisation and Perception Research Group (MIOPeG), Faculty of Medicine and Health, University of Sydney, 75 East Street, Lidcombe, NSW  2141, Australia; Brennan, P.C., Medical Image Optimisation and Perception Research Group (MIOPeG), Faculty of Medicine and Health, University of Sydney, 75 East Street, Lidcombe, NSW  2141, Australia; Mello-Thoms, C., Medical Image Optimisation and Perception Research Group (MIOPeG), Faculty of Medicine and Health, University of Sydney, 75 East Street, Lidcombe, NSW  2141, Australia, Department of Radiology, University of Iowa, 200 Hawkins Drive, Iowa City, IA  52242, United States","Visual search behaviour and the interpretation of mammograms have been studied for errors in breast cancer detection. We aim to ascertain whether machine-learning models can learn about radiologists’ attentional level and the interpretation of mammograms. We seek to determine whether these models are practical and feasible for use in training and teaching programmes. Eight radiologists of varying experience levels in reading mammograms reviewed 120 two-view digital mammography cases (59 cancers). Their search behaviour and decisions were captured using a head-mounted eye-tracking device and software allowing them to record their decisions. This information from radiologists was used to build an ensembled machine-learning model using top-down hierarchical deep convolution neural network. Separately, a model to determine type of missed cancer (search, perception or decision-making) was also built. Analysis and comparison of variants of these models using different convolution networks with and without transfer learning were also performed. Our ensembled deep-learning network architecture can be trained to learn about radiologists’ attentional level and decisions. High accuracy (95%, p value ≅ 0 [better than dumb/random model]) and high agreement between true and predicted values (kappa = 0.83) in such modelling can be achieved. Transfer learning techniques improve by ' 10% with the performance of this model. We also show that spatial convolution neural networks are insufficient in determining the type of missed cancers. Ensembled hierarchical deep convolution machine-learning models are plausible in modelling radiologists’ attentional level and their interpretation of mammograms. However, deep convolution networks fail to characterise the type of false-negative decisions. © 2019, Society for Imaging Informatics in Medicine.","Breast cancer; Deep learning; Machine learning; Mammography; Visual search","Convolution; Decision making; Diseases; Eye tracking; Learning systems; Machine learning; Mammography; Network architecture; X ray screens; Breast Cancer; Breast cancer detection; Convolution neural network; Digital mammography; Head-mounted eye tracking; Machine learning models; Spatial convolution; Visual search; Deep learning; breast; breast tumor; computer assisted diagnosis; diagnostic imaging; female; human; machine learning; mammography; pattern recognition; procedures; radiologist; Breast; Breast Neoplasms; Deep Learning; Female; Humans; Machine Learning; Mammography; Neural Networks, Computer; Pattern Recognition, Visual; Radiographic Image Interpretation, Computer-Assisted; Radiologists",Article,"Final","",Scopus,2-s2.0-85068646450
"Lucas A., Wang K., Santillan C., Hsiao A., Sirlin C.B., Murphy P.M.","57196096645;57213025117;35389750200;53263994800;7003768876;57199318626;","Image Annotation by Eye Tracking: Accuracy and Precision of Centerlines of Obstructed Small-Bowel Segments Placed Using Eye Trackers",2019,"Journal of Digital Imaging","32","5",,"855","864",,1,"10.1007/s10278-018-0169-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066796855&doi=10.1007%2fs10278-018-0169-5&partnerID=40&md5=1d684693be6151709b9a5a0f7dcb4fb2","Department of Bioengineering, University of California, San Diego, CA, United States; Department of Radiology, University of California, San Diego, CA, United States","Lucas, A., Department of Bioengineering, University of California, San Diego, CA, United States; Wang, K., Department of Radiology, University of California, San Diego, CA, United States; Santillan, C., Department of Radiology, University of California, San Diego, CA, United States; Hsiao, A., Department of Radiology, University of California, San Diego, CA, United States; Sirlin, C.B., Department of Radiology, University of California, San Diego, CA, United States; Murphy, P.M., Department of Radiology, University of California, San Diego, CA, United States","Small-bowel obstruction (SBO) is a common and important disease, for which machine learning tools have yet to be developed. Image annotation is a critical first step for development of such tools. This study assesses whether image annotation by eye tracking is sufficiently accurate and precise to serve as a first step in the development of machine learning tools for detection of SBO on CT. Seven subjects diagnosed with SBO by CT were included in the study. For each subject, an obstructed segment of bowel was chosen. Three observers annotated the centerline of the segment by manual fiducial placement and by visual fiducial placement using a Tobii 4c eye tracker. Each annotation was repeated three times. The distance between centerlines was calculated after alignment using dynamic time warping (DTW) and statistically compared to clinical thresholds for diagnosis of SBO. Intra-observer DTW distance between manual and visual centerlines was calculated as a measure of accuracy. These distances were 1.1 ± 0.2, 1.3 ± 0.4, and 1.8 ± 0.2 cm for the three observers and were less than 1.5 cm for two of three observers (P < 0.01). Intra- and inter-observer DTW distances between centerlines placed with each method were calculated as measures of precision. These distances were 0.6 ± 0.1 and 0.8 ± 0.2 cm for manual centerlines, 1.1 ± 0.4 and 1.9 ± 0.6 cm for visual centerlines, and were less than 3.0 cm in all cases (P < 0.01). Results suggest that eye tracking–based annotation is sufficiently accurate and precise for small-bowel centerline annotation for use in machine learning–based applications. © 2019, Society for Imaging Informatics in Medicine.","Centerline; Eye tracking; Machine learning; Small bowel","Computerized tomography; Diagnosis; Image analysis; Image annotation; Learning systems; Machine learning; Accuracy and precision; Centerlines; Dynamic time warping; Eye trackers; Small bowel; Eye tracking; adult; aged; automated pattern recognition; computer assisted diagnosis; diagnostic imaging; eye fixation; female; human; intestine obstruction; male; middle aged; procedures; reproducibility; retrospective study; small intestine; x-ray computed tomography; Adult; Aged; Female; Fixation, Ocular; Humans; Intestinal Obstruction; Intestine, Small; Male; Middle Aged; Pattern Recognition, Automated; Radiographic Image Interpretation, Computer-Assisted; Reproducibility of Results; Retrospective Studies; Tomography, X-Ray Computed",Article,"Final","",Scopus,2-s2.0-85066796855
"Vortmann L.-M.","57211492447;","Attention-driven interaction systems for augmented reality",2019,"ICMI 2019 - Proceedings of the 2019 International Conference on Multimodal Interaction",,,,"482","486",,4,"10.1145/3340555.3356088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074931843&doi=10.1145%2f3340555.3356088&partnerID=40&md5=6922153ab4f913d939d52da10232ed29","Cognitive Systems Lab, Department of Mathematics and Computer Science, University of Bremen, Bremen, Germany","Vortmann, L.-M., Cognitive Systems Lab, Department of Mathematics and Computer Science, University of Bremen, Bremen, Germany","Augmented reality (AR) glasses enable the embedding of visual content in a real-world surroundings. In this PhD project, I will implement user interfaces which adapt to the cognitive state of the user, for example by avoiding distractions or re-directing the user's attention towards missed information. For this purpose, sensory data from the user is captured (Brain activity via EEG of fNIRS, eye tracking, physiological measurements) and modeled with machine learning techniques. The focus of the cognitive state estimation is centered around attention related aspects. The main task is to build models for an estimation of a person's attentional state from the combination and classification of multimodal data streams and context information, as well as their evaluation. Furthermore, the goal is to develop prototypical user interfaces for AR glasses and to test their usability in different scenarios. © 2019 Copyright held by the owner/author(s).","Adaptive systems; Attention; Augmented reality; Classification; Interaction; Machine learning; User interface","Adaptive systems; Augmented reality; Brain; Classification (of information); Eye tracking; Glass; Interactive computer systems; Interface states; Learning systems; Machine learning; Petroleum reservoir evaluation; Attention; Cognitive state; Context information; Interaction; Interaction systems; Machine learning techniques; Multimodal data streams; Physiological measurement; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85074931843
"Appel T., Sevcenko N., Wortha F., Tsarava K., Moeller K., Ninaus M., Gerjets P., Kasneci E.","57191500368;57211756979;57201334648;56916124000;23019055400;55797096600;6507437265;56059892600;","Predicting cognitive load in an emergency simulation based on behavioral and physiological measures",2019,"ICMI 2019 - Proceedings of the 2019 International Conference on Multimodal Interaction",,,,"154","163",,13,"10.1145/3340555.3353735","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074926077&doi=10.1145%2f3340555.3353735&partnerID=40&md5=c495dd76bd85fa97056fa7cebaefc78e","LEAD Graduate School and Research Network, Tübingen, Germany; Leibniz-Institut für Wissensmedien, Tübingen, Germany; University of Tübingen, Tübingen, Germany","Appel, T., LEAD Graduate School and Research Network, Tübingen, Germany; Sevcenko, N., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Wortha, F., LEAD Graduate School and Research Network, Tübingen, Germany; Tsarava, K., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Moeller, K., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Ninaus, M., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Gerjets, P., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Kasneci, E., University of Tübingen, Tübingen, Germany","The reliable estimation of cognitive load is an integral step towards real-time adaptivity of learning or gaming environments. We introduce a novel and robust machine learning method for cognitive load assessment based on behavioral and physiological measures in a combined within- and crossparticipant approach. 47 participants completed different scenarios of a commercially available emergency personnel simulation game realizing several levels of difficulty based on cognitive load. Using interaction metrics, pupil dilation, eye-fixation behavior, and heart rate data, we trained individual, participant-specific forests of extremely randomized trees differentiating between low and high cognitive load. We achieved an average classification accuracy of 72%. We then apply these participant-specific classifiers in a novel way, using similarity between participants, normalization, and relative importance of individual features to successfully achieve the same level of classification accuracy in cross-participant classification. These results indicate that a combination of behavioral and physiological indicators allows for reliable prediction of cognitive load in an emergency simulation game, opening up new avenues for adaptivity and interaction. © 2019 Copyright held by the owner/author(s).","Classification; Cognitive Load; Eye Tracking; Heart Rate; Multimodal","Classification (of information); Eye tracking; Heart; Interactive computer systems; Learning systems; Classification accuracy; Cognitive loads; Heart rates; Individual features; Machine learning methods; Multi-modal; Physiological indicators; Physiological measures; Psychophysiology",Conference Paper,"Final","",Scopus,2-s2.0-85074926077
"Krishna V., Ding Y., Xu A., Höllerer T.","57215827635;57211971230;57211501288;8358959700;","Multimodal Biometric Authentication for VR/AR using EEG and Eye Tracking",2019,"Adjunct of the 2019 International Conference on Multimodal Interaction, ICMI 2019",,,"3360655","","",,4,"10.1145/3351529.3360655","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074216271&doi=10.1145%2f3351529.3360655&partnerID=40&md5=6f04e4e38b80684067b3eeba1dcceebd","Research Mentorship Program - UCSB National Public School - Indiranagar, Bangalore, Karnataka, India; University of California - Santa Barbara, Santa Barbara, CA, United States","Krishna, V., Research Mentorship Program - UCSB National Public School - Indiranagar, Bangalore, Karnataka, India; Ding, Y., University of California - Santa Barbara, Santa Barbara, CA, United States; Xu, A., University of California - Santa Barbara, Santa Barbara, CA, United States; Höllerer, T., University of California - Santa Barbara, Santa Barbara, CA, United States","Electroencephalogram (EEG) signals can enable an additional non-intrusive input modality especially when paired with a wearable headset (i.e. AR/VR). A great challenge in using EEG data for Brain-Computer Interface (BCI) algorithms is its poor generalization performance across users. Taking advantage of these inter-user differences, we investigate the potential in using this technology for user authentication – similar to facial recognition in smartphones. Additionally, we evaluate this in combination with eye tracking data which is also readily available in such headsets. We develop a biometric authentication systems for each of these systems and for their fusion. We formulate a novel evaluation paradigm using publicly available EEG motor imagery and eye tracking data and demonstrate strong feasibility towards using EEG and eye tracking for authentication. © 2019 Copyright held by the owner/author(s).","EEG; Eye Tracking; Machine Learning; Multimodal biometrics","Authentication; Biometrics; Brain computer interface; Electroencephalography; Face recognition; Interactive computer systems; Learning systems; Biometric authentication system; Electroencephalogram signals; Facial recognition; Generalization performance; Input modalities; Multi-modal biometrics; Multimodal biometric authentications; User authentication; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85074216271
"Babicsne-Horvath M., Hercegfi K.","57210143965;6503884074;","Early Results of a Usability Evaluation of Two Digital Human Model-based Ergonomic Software Applying Eye-Tracking Methodology Comparison of the usability of ViveLab and Jack software",2019,"10th IEEE International Conference on Cognitive Infocommunications, CogInfoCom 2019 - Proceedings",,,"9089993","205","210",,1,"10.1109/CogInfoCom47531.2019.9089993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085571389&doi=10.1109%2fCogInfoCom47531.2019.9089993&partnerID=40&md5=975af7e3e93bde54f6b4226f1e34a886","Budapest University of Technology and Economics, Department of Ergonomics and Psychology, Budapest, Hungary","Babicsne-Horvath, M., Budapest University of Technology and Economics, Department of Ergonomics and Psychology, Budapest, Hungary; Hercegfi, K., Budapest University of Technology and Economics, Department of Ergonomics and Psychology, Budapest, Hungary","Analysis software for ergonomics are more and more wide spread among researchers. There are various software for ergonomics available on the market, and it would be important to know which one to choose in various research tasks. Although data on the capabilities of the software can be found relatively easily, a comparison regarding their ease of use and the quality of their user interface cannot be found in the literature. In this article, the usability of two software were compared. A cloud based Hungarian software, ViveLab and the well-known Jack software was chosen. In addition to the traditional software usability testing technique based on screen and event recording and user camera, eye-Tracking methodology was also applied. The goal of this research is to find out which software is more usable in different situations. The results were divisive and in some cases astonishing. We are not able to give a definite answer which software is easier to use. There were several cases when ViveLab was easier and almost as much cases when Jack. There are areas for improvement in both software. Our results can help researchers to choose software for their specific tasks. Furthermore, the results can give additional information on which user interface elements and concepts are worth to improve by developers of the ergonomic software, and, in some cases, may inspire developers of other 3D-based software in general. © 2019 IEEE.","Computer Aided Anthropometric Design (CAAD); Digital Human Model; Eye-Tracking; Human Factors and Ergonomics (HFE); Jack; Software for ergonomic risk assessment; Usability testing; ViveLab","Ergonomics; Eye tracking; Testing; Usability engineering; User interfaces; Analysis softwares; Digital human modeling; Interface elements; Jack softwares; Software usabilities; Specific tasks; Usability evaluation; Wide spreads; Software testing",Conference Paper,"Final","",Scopus,2-s2.0-85085571389
"Ujbanyi T., Stankov G., Nagy B.","56208620400;56465901200;57207757571;","Eye tracking based usability evaluation of the MaxWhere virtual space in a search task",2019,"10th IEEE International Conference on Cognitive Infocommunications, CogInfoCom 2019 - Proceedings",,,"9089922","469","474",,3,"10.1109/CogInfoCom47531.2019.9089922","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085564483&doi=10.1109%2fCogInfoCom47531.2019.9089922&partnerID=40&md5=3e53a39ba6c8682bddd3ae711f6093ca","University of Dunaújváros, CogInfoCom Based Learn AbilityResearch Lab, Hungary, Hungary; Subotica Tech-College of Applied Sciences, Serbia; Institute of Informatics, Department of Mathematics and University of Dunaújváros, Hungary, Hungary","Ujbanyi, T., University of Dunaújváros, CogInfoCom Based Learn AbilityResearch Lab, Hungary, Hungary; Stankov, G., Subotica Tech-College of Applied Sciences, Serbia; Nagy, B., Institute of Informatics, Department of Mathematics and University of Dunaújváros, Hungary, Hungary","The generations of the past decades are addicted to the information and the internet. The ICT devices determine their everyday lives. Several studies suggest that such tools are needed to teach these generations effectively, to make the learning process easier and quicker. The corresponding facts must be chosen, filtered and utilized from a constantly growing set of information by the students. They had to think and then Figure out the answer from these information. The ability of algorithmic thinking, the development of analytical thinking, and the promotion of thinking in the system play a key role. The involvement of 3D virtual spaces in education is investigated by more and more studies nowadays. In this article, through an example of a search task, an eye-Tracking system is used to find out how much a 3D virtual space is more effective than a conventional LMS system. © 2019 IEEE.","Eye-Tracking; MaxWhere; VR Learning","Information filtering; Virtual reality; 3D virtual spaces; Algorithmic thinking; Analytical thinking; Eye tracking systems; Learning process; Search tasks; Usability evaluation; Virtual spaces; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085564483
"Liu Z., Chen Z., Bai J., Li S., Lian S.","57211257370;57215289589;55889538000;57215969499;7005702391;","Facial pose estimation by deep learning from label distributions",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022536","1232","1240",,7,"10.1109/ICCVW.2019.00156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082506421&doi=10.1109%2fICCVW.2019.00156&partnerID=40&md5=87f2ffe6013cc1f661fa5d2d9f104583","Cloudminds, China; Beihang University, China","Liu, Z., Cloudminds, China; Chen, Z., Cloudminds, China; Bai, J., Beihang University, China; Li, S., Cloudminds, China; Lian, S., Cloudminds, China","Facial pose estimation has gained a lot of attentions in many practical applications, such as human-robot interaction, gaze estimation and driver monitoring. Meanwhile, end-to-end deep learning-based facial pose estimation is becoming more and more popular. However, facial pose estimation suffers from a key challenge: the lack of sufficient training data for many poses, especially for large poses. Inspired by the observation that the faces under close poses look similar, we reformulate the facial pose estimation as a label distribution learning problem, considering each face image as an example associated with a Gaussian label distribution rather than a single label, and construct a convolutional neural network which is trained with a multi-loss function on AFLW dataset and 300W-LP dataset to predict the facial poses directly from color image. Extensive experiments are conducted on several popular benchmarks, including AFLW2000, BIWI, AFLW and AFW, where our approach shows a significant advantage over other state-of-the-art methods. © 2019 IEEE.","Deep learning; Facial pose estimation; Label distribution","Computer vision; Convolutional neural networks; Human robot interaction; Driver monitoring; Facial pose estimation; Gaze estimation; Label distribution; Learning problem; Loss functions; State-of-the-art methods; Training data; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85082506421
"He J., Pham K., Valliappan N., Xu P., Roberts C., Lagun D., Navalpakkam V.","55714925900;57208582090;35333214700;55267609700;57215969114;36727735000;6507746235;","On-device few-shot personalization for real-time gaze estimation",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9021975","1149","1158",,11,"10.1109/ICCVW.2019.00146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082505641&doi=10.1109%2fICCVW.2019.00146&partnerID=40&md5=7ac0f3f79edebf52e461962f1d0b4efb","Google Inc., United States; University of Maryland, College Park, United States","He, J., Google Inc., United States; Pham, K., University of Maryland, College Park, United States; Valliappan, N., Google Inc., United States; Xu, P., Google Inc., United States; Roberts, C., Google Inc., United States; Lagun, D., Google Inc., United States; Navalpakkam, V., Google Inc., United States","Building fast and accurate gaze estimation models without additional specialized hardware is a hard problem. In this paper, we present on-device few-shot personalization methods for 2D gaze estimation. The proposed supervised method achieves better accuracy using as few as 2-5 calibration points per user compared to prior methods that require more than 13 calibration points. In addition, we propose an unsupervised personalization method which uses only unlabeled facial images to improve gaze estimation accuracy. Our best personalized model achieves 24-26% better accuracy (measured by mean error) on phones compared to the state-of-the-art using <=5 calibration points per user. It is also computationally efficient, requiring 20x fewer FLOPS when compared to prior methods. This unlocks a variety of important real world applications such as using gaze for accessibility, gaming and human-computer interaction while running entirely on-device in real-time. © 2019 IEEE.","Few shot learning; Gaze; Haze tracking; On device learning; Personalization","Calibration; Computer vision; Image enhancement; Computationally efficient; Few shot learning; Gaze; On device learning; Personalizations; Personalized model; Specialized hardware; Supervised methods; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85082505641
"Griffith H., Katrychuk D., Komogortsev O.","57189386560;57210105027;6506328653;","Assessment of shift-invariant CNN gaze mappings for PS-OG eye movement sensors",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022315","3651","3659",,3,"10.1109/ICCVW.2019.00450","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082500122&doi=10.1109%2fICCVW.2019.00450&partnerID=40&md5=708d874ace8ea35f69b6ca63f38e52aa","Texas State University, San Marcos, United States","Griffith, H., Texas State University, San Marcos, United States; Katrychuk, D., Texas State University, San Marcos, United States; Komogortsev, O., Texas State University, San Marcos, United States","Photosensor oculography (PS-OG) eye movement sensors offer desirable performance characteristics for integration within wireless head mounted devices (HMDs), including low power consumption and high sampling rates. To address the known performance degradation of these sensors due to HMD shifts, various machine learning techniques have been proposed for mapping sensor outputs to gaze location. This paper advances the understanding of a recently introduced convolutional neural network designed to provide shift invariant gaze mapping within a specified range of sensor translations. Performance is assessed for shift training examples which better reflect the distribution of values that would be generated through manual repositioning of the HMD during a dedicated collection of training data. The network is shown to exhibit comparable accuracy for this realistic shift distribution versus a previously considered rectangular grid, thereby enhancing the feasibility of in-field set-up. In addition, this work further demonstrates the practical viability of the proposed initialization process by demonstrating robust mapping performance versus training data scale. The ability to maintain reasonable accuracy for shifts extending beyond those introduced during training is also demonstrated. © 2019 IEEE.","Eye tracking; Machine learning; Photo sensor oculography; Virtual reality","Computer vision; Convolutional neural networks; Eye tracking; Helmet mounted displays; Learning systems; Machine learning; Mapping; Optical sensors; Virtual reality; High sampling rates; Low-power consumption; Machine learning techniques; Performance characteristics; Performance degradation; Photo-sensors; Reasonable accuracy; Rectangular grids; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85082500122
"Cortacero K., Fischer T., Demiris Y.","57215970582;57190126084;6506125343;","RT-BENE: A dataset and baselines for real-time blink estimation in natural environments",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022030","1159","1168",,3,"10.1109/ICCVW.2019.00147","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082494646&doi=10.1109%2fICCVW.2019.00147&partnerID=40&md5=03983e4d1a87d85393a51146d3d3dc02","Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom","Cortacero, K., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom; Fischer, T., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom; Demiris, Y., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom","In recent years gaze estimation methods have made substantial progress, driven by the numerous application areas including human-robot interaction, visual attention estimation and foveated rendering for virtual reality headsets. However, many gaze estimation methods typically assume that the subject's eyes are open; for closed eyes, these methods provide irregular gaze estimates. Here, we address this assumption by first introducing a new open-sourced dataset with annotations of the eye-openness of more than 200,000 eye images, including more than 10,000 images where the eyes are closed. We further present baseline methods that allow for blink detection using convolutional neural networks. In extensive experiments, we show that the proposed baselines perform favourably in terms of precision and recall. We further incorporate our proposed RT-BENE baselines in the recently presented RT-GENE gaze estimation framework where it provides a real-time inference of the openness of the eyes. We argue that our work will benefit both gaze estimation and blink estimation methods, and we take steps towards unifying these methods. © 2019 IEEE.","Blink detection; Convolutional neural network; Deep learning; Haze estimation","Behavioral research; Computer vision; Convolution; Deep learning; Deep neural networks; Human robot interaction; Virtual reality; Application area; Blink detections; Estimation methods; Natural environments; Precision and recall; Real-time inference; Virtual-reality headsets; Visual Attention Estimation; Convolutional neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85082494646
"Boutros F., Damer N., Kirchbuchner F., Kuijper A.","57205379838;50861109400;57031859600;56131137100;","Eye-MMS: Miniature multi-scale segmentation network of key eye-regions in embedded applications",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022048","3665","3670",,14,"10.1109/ICCVW.2019.00452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082493607&doi=10.1109%2fICCVW.2019.00452&partnerID=40&md5=f767e071ce9f8f2e80f951c63200ac45","Fraunhofer Institute for Computer Graphics Research IGD, Germany; Technische Universität Darmstadt, Germany","Boutros, F., Fraunhofer Institute for Computer Graphics Research IGD, Germany, Technische Universität Darmstadt, Germany; Damer, N., Fraunhofer Institute for Computer Graphics Research IGD, Germany, Technische Universität Darmstadt, Germany; Kirchbuchner, F., Fraunhofer Institute for Computer Graphics Research IGD, Germany, Technische Universität Darmstadt, Germany; Kuijper, A., Fraunhofer Institute for Computer Graphics Research IGD, Germany, Technische Universität Darmstadt, Germany","Segmentation of the iris or sclera is an essential processing block in ocular biometric systems. However, human-computer interaction, as in VR/AR applications, requires multiple region segmentation to enable smoother interaction and eye-tracking. Such application does not only demand highly accurate and generalizable segmentation, it requires such segmentation model to be appropriate for the limited computational power of embedded systems. This puts strict limits on the size of the deployed deep learning models. This work presents a miniature multi-scale segmentation network consisting of inter-connected convolutional modules. We present a baseline multi-scale segmentation network and modify it to reduce its parameters by more than 80 times, while reducing its accuracy by less than 3%, resulting in our Eye-MMS model containing only 80k parameters. This work is developed on the OpenEDS database and is conducted in preparation for the OpenEDS Semantic Segmentation Challenge. © 2019 IEEE.","Biometrics; Embedded biometrics; Eye segmentation; Semantic segmentation","Biometrics; Computer vision; Deep learning; Embedded systems; Human computer interaction; Semantics; Biometric systems; Computational power; Embedded application; Embedded biometrics; Multiple regions; Multiscale segmentation; Segmentation models; Semantic segmentation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082493607
"Perry J., Fernandez A.","57215964232;57212193065;","MinENet: A dilated CNN for semantic segmentation of eye features",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022100","3671","3676",,9,"10.1109/ICCVW.2019.00453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082492287&doi=10.1109%2fICCVW.2019.00453&partnerID=40&md5=ac343ce44784ad0fd109b2117dd49775","Department of Computer Science, University of Texas at San Antonio, One UTSA Circle, San Antonio, Texas  78249, United States","Perry, J., Department of Computer Science, University of Texas at San Antonio, One UTSA Circle, San Antonio, Texas  78249, United States; Fernandez, A., Department of Computer Science, University of Texas at San Antonio, One UTSA Circle, San Antonio, Texas  78249, United States","Fast and accurate eye tracking is a critical task for a range of research in virtual and augmented reality, attention tracking, mobile applications, and medical analysis. While deep neural network models excel at image analysis tasks, existing approaches to segmentation often consider only one class, emphasize classification over segmentation, or come with prohibitively high resource costs. In this work, we propose MinENet, a minimized efficient neural network architecture designed for fast multi-class semantic segmentation. We demonstrate performance of MinENet on the OpenEDS Semantic Segmentation Challenge dataset, against a baseline model as well as standard state-of-the-art neural network architectures - a convolutional neural network (CNN) and a dilated CNN. Our encoder-decoder architecture improves accuracy of multi-class segmentation of eye features in this large-scale high-resolution dataset, while also providing a design that is demonstrably lightweight and efficient. © 2019 IEEE.","Cnn; Deep learning; Eye tracking; Image segmentation","Augmented reality; Computer vision; Convolutional neural networks; Deep learning; Deep neural networks; Eye tracking; Large dataset; Network architecture; Semantics; Encoder-decoder architecture; Mobile applications; Multi-class segmentations; Neural network model; Over segmentation; Semantic segmentation; State of the art; Virtual and augmented reality; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85082492287
"Porta S., Bossavit B., Cabeza R., Larumbe-Bergera A., Garde G., Villanueva A.","7005292345;36730794700;36763933900;57210106737;57215963483;7101612861;","U2Eyes: A binocular dataset for eye tracking and gaze estimation",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022577","3660","3664",,8,"10.1109/ICCVW.2019.00451","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082489044&doi=10.1109%2fICCVW.2019.00451&partnerID=40&md5=89905765d2816f5d7a324e6b4d06579f","Public University of Navarre, Pamplona, Spain; Trinity College Dublin, Dublin, Ireland","Porta, S., Public University of Navarre, Pamplona, Spain; Bossavit, B., Trinity College Dublin, Dublin, Ireland; Cabeza, R., Public University of Navarre, Pamplona, Spain; Larumbe-Bergera, A., Public University of Navarre, Pamplona, Spain; Garde, G., Public University of Navarre, Pamplona, Spain; Villanueva, A., Public University of Navarre, Pamplona, Spain","Theory shows that huge amount of labelled data are needed in order to achieve reliable classification/regression methods when using deep/machine learning techniques. However, in the eye tracking field, manual annotation is not a feasible option due to the wide variability to be covered. Hence, techniques devoted to synthesizing images show up as an opportunity to provide vast amounts of annotated data. Considering that the well-known UnityEyes tool provides a framework to generate single eye images and taking into account that both eyes information can contribute to improve gaze estimation accuracy we present U2Eyes dataset, that is publicly available. It comprehends about 6 million of synthetic images containing binocular data. Furthermore, the physiology of the eye model employed is improved, simplified dynamics of binocular vision are incorporated and more detailed 2D and 3D labelled data are provided. Additionally, an example of application of the dataset is shown as work in progress. Employing U2Eyes as training framework Supervised Descent Method (SDM) is used for eyelids segmentation. The model obtained as result of the training process is then applied on real images from GI4E dataset showing promising results. © 2019 IEEE.","Annotation; Binocular dataset; Eye tracking; Low resolution; Unity","Binocular vision; Binoculars; Computer vision; Deep learning; Image enhancement; Learning systems; Stereo image processing; Annotation; Learning techniques; Low resolution; Manual annotation; Synthetic images; Training framework; Unity; Work in progress; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082489044
"Wu Z., Rajendran S., Van As T., Badrinarayanan V., Rabinovich A.","57141032200;57215971788;57215965908;24821754400;8764903400;","EyeNet: A multi-task deep network for off-axis eye gaze estimation",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022317","3683","3687",,2,"10.1109/ICCVW.2019.00455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082483189&doi=10.1109%2fICCVW.2019.00455&partnerID=40&md5=3f1c5dce3b5417a88d0ac97d0ecb83c2","Magic Leap, Inc., United States","Wu, Z., Magic Leap, Inc., United States; Rajendran, S., Magic Leap, Inc., United States; Van As, T., Magic Leap, Inc., United States; Badrinarayanan, V., Magic Leap, Inc., United States; Rabinovich, A., Magic Leap, Inc., United States","Eye gaze estimation is a crucial component in Virtual and Mixed Reality. In head-mounted VR/MR devices the eyes are imaged off-axis to avoid blocking the user's gaze, this view-point makes drawing eye related inferences very challenging. In this work, we present EyeNet, the first single deep neural network which solves multiple heterogeneous tasks related to eye gaze estimation for an off-axis camera setting. The tasks include eye segmentation, IR LED glints detection, pupil and cornea center estimation. We benchmark all tasks on MagicEyes, a large and new dataset of 587 subjects with varying morphology, gender, skin-color, make-up and imaging conditions. © 2019 IEEE.","Computer vision; Deep learning; Eye tracking; Gaze estimation; Mixed reality; Multi task learning","Computer vision; Deep learning; Deep neural networks; Large dataset; Mixed reality; Multi-task learning; Center estimations; Eye-gaze; Gaze estimation; Imaging conditions; Off-axis; Off-axis cameras; Skin color; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082483189
"Maekawa Y., Akai N., Hirayama T., Morales L.Y., Deguchi D., Kawanishi Y., Ide I., Murase H.","57215965276;55671618700;55531799600;57205545886;6602666462;36188305700;13406373500;7101900108;","An analysis of how driver experience affects eye-gaze behavior for robotic wheelchair operation",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022595","4443","4451",,2,"10.1109/ICCVW.2019.00545","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082481878&doi=10.1109%2fICCVW.2019.00545&partnerID=40&md5=d750c203706cf5b9128a6b21bac4e940","Nagoya University, Japan","Maekawa, Y., Nagoya University, Japan; Akai, N., Nagoya University, Japan; Hirayama, T., Nagoya University, Japan; Morales, L.Y., Nagoya University, Japan; Deguchi, D., Nagoya University, Japan; Kawanishi, Y., Nagoya University, Japan; Ide, I., Nagoya University, Japan; Murase, H., Nagoya University, Japan","Drivers obtain information on surrounding environment using their eyesights. Experienced eye-gaze behavior is needed when driving at places where multiple risks exist to prepare for and avoid them. In this work, we analyze the change in eye-gaze behavior in such situations while a driver gains experience on the operation of a robotic wheelchair. Accurate distance information in the traffic environment is important to analyze the eye-gaze behavior. However, almost all previous works analyze eye-gaze behavior in a 2D environment, so they could not obtain accurate distance information. For this reason, we analyze eye-gaze behavior in 3D space. Concretely, we developed a novel eye-gaze behavior analysis platform based on a robotic wheelchair and estimated the driver's attention in 3D space. We try to analyze the eye-gaze behavior considering a useful field-of-view in 3D space based on the distance information instead of only the fixation point to investigate the objects that a driver implicitly pays attention to and from where s/he focuses on them. Results show that novice drivers pay attention to a single risk at a time. In contrast, they pay more attention to multiple risks simultaneously as they gain experience. Additionally, we discuss what features are effective to model the eye-gaze behavior based on the results. © 2019 IEEE.","Experience; Eye gaze; Field of view; Three dimensional; Wheelchair","Robotics; Three dimensional computer graphics; Wheelchairs; Distance information; Experience; Eye-gaze; Field of views; Robotic wheelchairs; Surrounding environment; Traffic environment; Useful field of view; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85082481878
"Linden E., Sjostrand J., Proutiere A.","57215962232;57217753974;6603905943;","Learning to personalize in appearance-based gaze tracking",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022231","1140","1148",,7,"10.1109/ICCVW.2019.00145","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082478391&doi=10.1109%2fICCVW.2019.00145&partnerID=40&md5=6b80ab60ad317303056e3080520e4107","Tobii, Sweden; KTH Royal Insitute of Technology, Sweden","Linden, E., Tobii, Sweden; Sjostrand, J., Tobii, Sweden; Proutiere, A., KTH Royal Insitute of Technology, Sweden","Personal variations severely limit the performance of appearance-based gaze tracking. Adapting to these variations using standard neural network model-adaption methods is difficult. The problems range from overfitting, due to small amounts of training data, to underfitting, due to restrictive model architectures. We tackle these problems by introducing SPatial Adaptive GaZe Estimator (SPAZE ). By modeling personal variations as a low-dimensional latent parameter space, SPAZE provides just enough adaptability to capture the range of personal variations without being prone to overfitting. Calibrating SPAZE for a new person reduces to solving a small and simple optimization problem. SPAZE achieves an error of 2.70 degrees on the MPIIGaze dataset, improving on the state-of-the-art by 14 %. We contribute to gaze tracking research by empirically showing that personal variations are well-modeled as a 3-dimensional latent parameter space for each eye. We show that this low-dimensionality is expected by examining model-based approaches to gaze tracking. © 2019 IEEE.","Appearance based gaze estimation; Convolutional neural network; Deep learning; Haze tracking","Computer vision; Convolutional neural networks; Deep learning; Deep neural networks; Gaze estimation; Low dimensionality; Model architecture; Model based approach; Optimization problems; Parameter spaces; Standard neural network models; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082478391
"Chang Z., DI Martino J.M., Qiu Q., Espinosa S., Sapiro G.","57190190445;36664485600;54956074400;14017589600;7005450011;","Salgaze: Personalizing gaze estimation using visual saliency",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022412","1169","1178",,2,"10.1109/ICCVW.2019.00148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082468271&doi=10.1109%2fICCVW.2019.00148&partnerID=40&md5=5c447c56aabf57874f7c502c1863e8f5","Duke University, Durham, NC  27708, United States","Chang, Z., Duke University, Durham, NC  27708, United States; DI Martino, J.M., Duke University, Durham, NC  27708, United States; Qiu, Q., Duke University, Durham, NC  27708, United States; Espinosa, S., Duke University, Durham, NC  27708, United States; Sapiro, G., Duke University, Durham, NC  27708, United States","Traditional gaze estimation methods typically require explicit user calibration to achieve high accuracy. This process is cumbersome and recalibration is often required when there are changes in factors such as illumination and pose. To address this challenge, we introduce SalGaze, a framework that utilizes saliency information in the visual content to transparently adapt the gaze estimation algorithm to the user without explicit user calibration. We design an algorithm to transform a saliency map into a differentiable loss map that can be used for the optimization of CNN-based models. SalGaze is also able to greatly augment standard point calibration data with implicit video saliency calibration data using a unified framework. We show accuracy improvements over 24% using our technique on existing methods. © 2019 IEEE.","Calibration; Convolutional neural network; Deep learning; Gaze estimation; Saliency","Computer vision; Convolutional neural networks; Deep learning; Deep neural networks; Object recognition; Accuracy Improvement; Calibration data; Gaze estimation; Saliency; Unified framework; User calibration; Video saliencies; Visual saliency; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-85082468271
"Guo T., Liu Y., Zhang H., Liu X., Kwak Y., Yoo B.I., Han J.-J., Choi C.","55624159300;57215967428;57216240708;57215965302;57201425904;36096230000;55646340200;7402961607;","A generalized and robust method towards practical gaze estimation on smart phone",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022325","1131","1139",,7,"10.1109/ICCVW.2019.00144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082466578&doi=10.1109%2fICCVW.2019.00144&partnerID=40&md5=8c95e9c953c23d0b895bbc0752bcc035","Samsung Research China - Beijing, China; Samsung Advanced Institute of Technology, China","Guo, T., Samsung Research China - Beijing, China; Liu, Y., Samsung Research China - Beijing, China; Zhang, H., Samsung Research China - Beijing, China; Liu, X., Samsung Research China - Beijing, China; Kwak, Y., Samsung Research China - Beijing, China; Yoo, B.I., Samsung Advanced Institute of Technology, China; Han, J.-J., Samsung Advanced Institute of Technology, China; Choi, C., Samsung Research China - Beijing, China","Gaze estimation for ordinary smart phone, e.g. estimating where the user is looking at on the phone screen, can be applied in various applications. However, the widely used appearance-based CNN methods still have two issues for practical adoption. First, due to the limited dataset, gaze estimation is very likely to suffer from over-fitting, leading to poor accuracy at run time. Second, the current methods are usually not robust, i.e. their prediction results having notable jitters even when the user is performing gaze fixation, which degrades user experience greatly. For the first issue, we propose a new tolerant and talented (TAT) training scheme, which is an iterative random knowledge distillation framework enhanced with cosine similarity pruning and aligned orthogonal initialization. The knowledge distillation is a tolerant teaching process providing diverse and informative supervision. The enhanced pruning and initialization is a talented learning process prompting the network to escape from the local minima and re-born from a better start. For the second issue, we define a new metric to measure the robustness of gaze estimator, and propose an adversarial training based Disturbance with Ordinal loss (DwO) method to improve it. The experimental results show that our TAT method achieves state-of-the-art performance on GazeCapture dataset, and that our DwO method improves the robustness while keeping comparable accuracy. © 2019 IEEE.","Gaze estimation; Over fitting; Robustness; Smart phone","Computer vision; Distillation; Iterative methods; Robustness (control systems); User experience; Appearance based; Cosine similarity; Gaze estimation; Learning process; Overfitting; State-of-the-art performance; Teaching process; Training schemes; Smartphones",Conference Paper,"Final","",Scopus,2-s2.0-85082466578
"Chen Z., Deng D., Pi J., Shi B.E.","56808413900;57215966283;57195220788;7402547071;","Unsupervised outlier detection in appearance-based gaze estimation",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022626","1088","1097",,1,"10.1109/ICCVW.2019.00139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082444548&doi=10.1109%2fICCVW.2019.00139&partnerID=40&md5=9b946867748c37909753d83a7b4fec26","Hong Kong University of Science and Technology, Hong Kong","Chen, Z., Hong Kong University of Science and Technology, Hong Kong; Deng, D., Hong Kong University of Science and Technology, Hong Kong; Pi, J., Hong Kong University of Science and Technology, Hong Kong; Shi, B.E., Hong Kong University of Science and Technology, Hong Kong","Appearance-based gaze estimation maps RGB images to estimates of gaze directions. One problem in gaze estimation is that there always exist low-quality samples (outliers) in which the eyes are barely visible. These low-quality samples are mainly caused by blinks, occlusions (e.g. by eye glasses), blur (e.g. due to motion) and failures of the eye landmark detection. Training on these outliers degrades the performance of gaze estimators, since they have no or limited information about gaze directions. It is also risky to give estimates based on these images in real-world applications, as these estimates may be unreliable. To solve this problem, we propose an algorithm that detects outliers without supervision. Based on the input images with only gaze labels, the proposed algorithm learns to predict a gaze estimates and an additional confidence score, which alleviates the impact of outliers during learning. We evaluated this algorithm on the MPIIGaze dataset and on an internal dataset. In cross-subject evaluation, our experimental results show that the proposed algorithm results in a better gaze estimator (8% improvement). The proposed algorithm is also able to reliably detect outliers during testing, with a precision of 0.71 when the recall is 0.63. © 2019 IEEE.","Appearance based gaze estimation; Outlier detection","Anomaly detection; Computer vision; Data handling; Appearance based; Confidence score; Gaze direction; Gaze estimation; Input image; Landmark detection; Limited information; Low qualities; Statistics",Conference Paper,"Final","",Scopus,2-s2.0-85082444548
"Kellnhofer P., Recasens A., Stent S., Matusik W., Torralba A.","55250016000;57189096003;56229805900;56230515000;57216041654;","Gaze360: Physically unconstrained gaze estimation in the wild",2019,"Proceedings of the IEEE International Conference on Computer Vision","2019-October",,"9010825","6911","6920",,31,"10.1109/ICCV.2019.00701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081928506&doi=10.1109%2fICCV.2019.00701&partnerID=40&md5=b2c73c34b069d1f3d2ccfabe75cf183b","Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Toyota Research Institute, Cambridge, MA  02139, United States","Kellnhofer, P., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Recasens, A., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Stent, S., Toyota Research Institute, Cambridge, MA  02139, United States; Matusik, W., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Torralba, A., Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale remote gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models will be made available at http://gaze360.csail.mit.edu. © 2019 IEEE.",,"3D modeling; Benchmarking; Computer vision; Large dataset; Uncertainty analysis; Benchmark datasets; Collection methods; Cross-dataset evaluation; Domain adaptation; Gaze estimation; Generalization performance; Outdoor environment; Temporal information; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85081928506
"He Z., Spurr A., Zhang X., Hilliges O.","57215780380;57200213697;57142162900;14041644100;","Photo-realistic monocular gaze redirection using generative adversarial networks",2019,"Proceedings of the IEEE International Conference on Computer Vision","2019-October",,"9008804","6931","6940",,10,"10.1109/ICCV.2019.00703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081906519&doi=10.1109%2fICCV.2019.00703&partnerID=40&md5=3f70e9fd8179b8a914c62833e8d94d9d","AIT Lab, ETH Zürich, Switzerland; Institute of Neuroinformatics, ETH Zürich, University of Zürich, Switzerland","He, Z., AIT Lab, ETH Zürich, Switzerland, Institute of Neuroinformatics, ETH Zürich, University of Zürich, Switzerland; Spurr, A., AIT Lab, ETH Zürich, Switzerland; Zhang, X., AIT Lab, ETH Zürich, Switzerland; Hilliges, O., AIT Lab, ETH Zürich, Switzerland","Gaze redirection is the task of changing the gaze to a desired direction for a given monocular eye patch image. Many applications such as videoconferencing, films, games, and generation of training data for gaze estimation require redirecting the gaze, without distorting the appearance of the area surrounding the eye and while producing photo-realistic images. Existing methods lack the ability to generate perceptually plausible images. In this work, we present a novel method to alleviate this problem by leveraging generative adversarial training to synthesize an eye image conditioned on a target gaze direction. Our method ensures perceptual similarity and consistency of synthesized images to the real images. Furthermore, a gaze estimation loss is used to control the gaze direction accurately. To attain high-quality images, we incorporate perceptual and cycle consistency losses into our architecture. In extensive evaluations we show that the proposed method outperforms state-of-the-art approaches in terms of both image quality and redirection precision. Finally, we show that generated images can bring significant improvement for the gaze estimation task if used to augment real training data. © 2019 IEEE.",,"Computer vision; Quality control; Video conferencing; Adversarial networks; Gaze estimation; High quality images; Perceptual similarity; Photo-realistic; Photorealistic images; State-of-the-art approach; Synthesized images; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85081906519
"Berga D., Vidal X.R.F., Otazu X., Pardo X.M.","57204681680;6701725046;6602773644;6603442652;","SID4VAM: A benchmark dataset with synthetic images for visual attention modeling",2019,"Proceedings of the IEEE International Conference on Computer Vision","2019-October",,"9008799","8788","8797",,3,"10.1109/ICCV.2019.00888","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080952558&doi=10.1109%2fICCV.2019.00888&partnerID=40&md5=026b317b378275639c612c7f0b8735fb","Computer Vision Center, Universitat Autònoma de Barcelona, Spain; CiTIUS, Universidade de Santiago de Compostela, Spain","Berga, D., Computer Vision Center, Universitat Autònoma de Barcelona, Spain; Vidal, X.R.F., CiTIUS, Universidade de Santiago de Compostela, Spain; Otazu, X., Computer Vision Center, Universitat Autònoma de Barcelona, Spain; Pardo, X.M., CiTIUS, Universidade de Santiago de Compostela, Spain","A benchmark of saliency models performance with a synthetic image dataset is provided. Model performance is evaluated through saliency metrics as well as the influence of model inspiration and consistency with human psychophysics. SID4VAM is composed of 230 synthetic images, with known salient regions. Images were generated with 15 distinct types of low-level features (e.g. orientation, brightness, color, size...) with a target-distractor pop-out type of synthetic patterns. We have used Free-Viewing and Visual Search task instructions and 7 feature contrasts for each feature category. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation. This study proposes a new way to evaluate saliency models in the forthcoming literature, accounting for synthetic images with uniquely low-level feature contexts, distinct from previous eye tracking image datasets. © 2019 IEEE.",,"Behavioral research; Computer vision; Deep learning; Eye tracking; Benchmark datasets; Feature contrasts; Human psychophysics; Low-level features; Model performance; Synthetic image dataset; Synthetic patterns; Visual attention model; Benchmarking",Conference Paper,"Final","",Scopus,2-s2.0-85080952558
"Honda T., Matsunaga N., Okajima H.","57214777434;57191567702;57191568231;","Hybrid Steering Model depending on Driver's Gazing Point to detect inattentive driving using Machine Learning",2019,"International Conference on Control, Automation and Systems","2019-October",,"8971669","1344","1349",,,"10.23919/ICCAS47443.2019.8971669","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079087265&doi=10.23919%2fICCAS47443.2019.8971669&partnerID=40&md5=0ee3bcceaa9e3b07a0656dd247c475b3","Graduate School of Science and Technology, Kumamoto University, Kumamoto, Japan; Faculty of Advanced Science and Technology, Kumamoto University, Kumamoto, Japan","Honda, T., Graduate School of Science and Technology, Kumamoto University, Kumamoto, Japan; Matsunaga, N., Faculty of Advanced Science and Technology, Kumamoto University, Kumamoto, Japan; Okajima, H., Faculty of Advanced Science and Technology, Kumamoto University, Kumamoto, Japan","The modeling of driving behaviors is important to analyze and design comfortable functions for the driving systems. The estimation method of the non-linear steering model using the eye tracking information was studied using the heuristic search algorithm. However, the model was limited to gaze and the inattentive driving was not modeled. It is considered that the steering model is consists of a hybrid steering model that switches the controllers according to the eye tracking information is classified in the effective/peripheral viewing field. In this paper, an estimation method of the hybrid system focusing on the effective visual field during driving is proposed. The hybrid model is constructed by steering model depending on the gazing distance and simple on-off controller. This estimation algorithm consists of 2-steps; clustering which classifies data by k-means method and the estimation of the parameters by Particle Swarm Optimization. The experiment with the long driving course consisting of five-curves and straight lines is demonstrated by HONDA driving simulator. © 2019 Institute of Control, Robotics and Systems - ICROS.","Hybrid system; k-means; Particle Swam Optimization; Steering model; Visual field","Controllers; Eye tracking; Heuristic algorithms; Heuristic methods; Hybrid systems; Information use; K-means clustering; Machine learning; Particle swarm optimization (PSO); Vision; Driving simulator; Estimation algorithm; Estimation methods; Heuristic search algorithms; K-means; Particle Swam Optimization; Steering models; Visual fields; Automobile steering equipment",Conference Paper,"Final","",Scopus,2-s2.0-85079087265
"Carrillo C.-A., Bolivar H., Chaves M.L.","57214751031;56244696500;35228528000;","Methodology of Neuromarketing in Websites Analysis Approach",2019,"2019 Congreso Internacional de Innovacion y Tendencias en Ingenieria, CONIITI 2019 - Conference Proceedings",,,"8960864","","",,1,"10.1109/CONIITI48476.2019.8960864","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079058927&doi=10.1109%2fCONIITI48476.2019.8960864&partnerID=40&md5=88330fb2eea82e2c9dab807a6e0bd506","Università degli Studi di Salerno, Dipartimento di Ingegneria Ind., Salerno, Italy; Universidad Catolica de Colombia, Systems and Computers Engineering, Bogotá, Colombia","Carrillo, C.-A., Università degli Studi di Salerno, Dipartimento di Ingegneria Ind., Salerno, Italy; Bolivar, H., Universidad Catolica de Colombia, Systems and Computers Engineering, Bogotá, Colombia; Chaves, M.L., Universidad Catolica de Colombia, Systems and Computers Engineering, Bogotá, Colombia","According to Salesforce of 6,421 customers who bought Web pages during 2016 at the end of 2017, only 2,917 remained, corresponding to a 54% decrease. When tracking 850 clients to know the causes of dissatisfaction with the product of Web pages, it was found that 35.92% of the clients surveyed indicated that the design of their website was not appropriate for their business, this fact allowed to identify a set of possible elements or factors that cause the failure of the web pages, as: not identify potential clients, not identify the final objective, web pages without Consistency, think that appearance is the only thing that matters and optimize before getting traffic. Based on the techniques of Neuromarketing and Eye Tracking a methodology was developed to obtain an indicator that allows to measure the level of empathy that a user presents with a website, studying the moments in which the neuronal stimuli when interacting with the different elements of the site The web and the visual route present a wave behavior similar to those of the emotions, in order to solve the problems that exist in the world of web design on what factors are determinant in the presentation, usability and design. © 2019 IEEE.","Empathy; Eye Tracking; Neuromarketing; Usability; Website","Eye tracking; Product design; Web Design; Analysis approach; Empathy; Final objective; Neuromarketing; Salesforce; Usability; Wave behavior; Websites",Conference Paper,"Final","",Scopus,2-s2.0-85079058927
"Asvestopoulou T., Manousaki V., Psistakis A., Nikolli E., Andreadakis V., Aslanides I.M., Pantazis Y., Smyrnakis I., Papadopouli M.","57215290507;57202311904;57212487003;57215295376;57195327839;6602200402;8837185000;55927202900;6602494263;","Towards a robust and accurate screening tool for dyslexia with data augmentation using GANs",2019,"Proceedings - 2019 IEEE 19th International Conference on Bioinformatics and Bioengineering, BIBE 2019",,,"8941765","775","782",,,"10.1109/BIBE.2019.00145","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078578034&doi=10.1109%2fBIBE.2019.00145&partnerID=40&md5=6df1ec2286ef179ca8f374b5d8c15fed","Department of Computer Science, University of Crete, Heraklion, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas, Heraklion, Greece; Optotech Ltd., Heraklion, Greece; Emmetropia Eye Institute, Heraklion, Greece; Institute of Applied and Computational Mathematics, Foundation for Research and Technology-Hellas, Heraklion, Greece","Asvestopoulou, T., Department of Computer Science, University of Crete, Heraklion, Greece, Institute of Computer Science, Foundation for Research and Technology-Hellas, Heraklion, Greece; Manousaki, V., Department of Computer Science, University of Crete, Heraklion, Greece, Institute of Computer Science, Foundation for Research and Technology-Hellas, Heraklion, Greece; Psistakis, A., Department of Computer Science, University of Crete, Heraklion, Greece, Institute of Computer Science, Foundation for Research and Technology-Hellas, Heraklion, Greece; Nikolli, E., Department of Computer Science, University of Crete, Heraklion, Greece, Institute of Computer Science, Foundation for Research and Technology-Hellas, Heraklion, Greece; Andreadakis, V., Optotech Ltd., Heraklion, Greece; Aslanides, I.M., Emmetropia Eye Institute, Heraklion, Greece; Pantazis, Y., Institute of Applied and Computational Mathematics, Foundation for Research and Technology-Hellas, Heraklion, Greece; Smyrnakis, I., Institute of Computer Science, Foundation for Research and Technology-Hellas, Heraklion, Greece, Optotech Ltd., Heraklion, Greece; Papadopouli, M., Department of Computer Science, University of Crete, Heraklion, Greece, Institute of Computer Science, Foundation for Research and Technology-Hellas, Heraklion, Greece","Eye movements during text reading can provide insights about reading disorders. We developed the DysLexML, a screening tool for developmental dyslexia, based on various ML algorithms that analyze gaze points recorded via eye-tracking during silent reading of children. We comparatively evaluated its performance using measurements collected from two systematic field studies with 221 participants in total. This work presents DysLexML and its performance. It identifies the features with prominent predictive power and performs dimensionality reduction. Specifically, it achieves its best performance using linear SVM, with an accuracy of 97% and 84% respectively, using a small feature set. We show that DysLexML is also robust in the presence of noise. These encouraging results set the basis for developing screening tools in less controlled, larger-scale environments, with inexpensive eye-trackers, potentially reaching a larger population for early intervention. Unlike other related studies, DysLexML achieves the aforementioned performance by employing only a small number of selected features, that have been identified with prominent predictive power. Finally, we developed a new data augmentation/substitution technique based on GANs for generating synthetic data similar to the original distributions. © 2019 IEEE.","Children; Data augmentation; Dyslexia; Eye tracking; GANs; Machine learning; Screening","Bioinformatics; Eye movements; Learning systems; Screening; Children; Data augmentation; Developmental dyslexia; Dimensionality reduction; Dyslexia; Early intervention; GANs; Reading disorders; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85078578034
"Dimas G., Iakovidis D., Koulaouzidis A.","57195485922;6603967427;14627591700;","MedGaze: Gaze Estimation on WCE Images Based on a CNN Autoencoder",2019,"Proceedings - 2019 IEEE 19th International Conference on Bioinformatics and Bioengineering, BIBE 2019",,,"8941676","363","367",,1,"10.1109/BIBE.2019.00071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078573803&doi=10.1109%2fBIBE.2019.00071&partnerID=40&md5=dc6225afbc5ec3e651c891aeb88da295","Dept. of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Endoscopy Unit, Royal Infirmary of Edinburgh, Edinburgh, United Kingdom","Dimas, G., Dept. of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Iakovidis, D., Dept. of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Koulaouzidis, A., Endoscopy Unit, Royal Infirmary of Edinburgh, Edinburgh, United Kingdom","The interpretation of medical images depends on physicians' experience. Over time, physicians develop their ability to examine the images, and this is usually reflected on gaze patterns they follow to observe visual cues, which lead them to diagnostic decisions. In the context of gaze prediction, graph and machine learning methods have been proposed for the visual saliency estimation on generic images. In this work we preset a novel and robust gaze estimation methodology based on physicians' eye fixations, using convolutional neural networks combined with regularization methods, on medical images taken during Wireless Capsule Endoscopy (WCE). Furthermore, we present a novel dataset of physicians' eye fixation patterns which was used for the training of the neural network model. The model was able to achieve 68.5% Judd's Area Under the receiver operating Characteristic (AUC-J). © 2019 IEEE.","Convolutional neural networks; Eye-tracking; Gaze estimation; Machine learning; Saliency","Bioinformatics; Convolution; Endoscopy; Gallium compounds; Learning algorithms; Learning systems; Machine learning; Medical imaging; Neural networks; Convolutional neural network; Gaze estimation; Machine learning methods; Neural network model; Receiver operating characteristics; Regularization methods; Saliency; Wireless capsule endoscopy; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85078573803
"Merenda C., Suga C., Gabbard J.L., Misu T.","57191362444;54795923300;6603313947;14018320600;","Effects of 'real-world' visual fidelity on ar interface assessment: A case study using ar head-up display graphics in driving",2019,"Proceedings - 2019 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2019",,,"8943689","145","156",,2,"10.1109/ISMAR.2019.00-10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078259326&doi=10.1109%2fISMAR.2019.00-10&partnerID=40&md5=9614bc39919ba151c08657ba98c61cb9","Grado Department of Industrial and Systems Engineering, Virginia Tech, Blacksburg, VA  24061, United States; Volvo Cars R&D, Sunnyvale, CA  94085, United States; Honda Research Instute USA, Inc., San Jose, CA  95134, United States","Merenda, C., Grado Department of Industrial and Systems Engineering, Virginia Tech, Blacksburg, VA  24061, United States; Suga, C., Volvo Cars R&D, Sunnyvale, CA  94085, United States; Gabbard, J.L., Grado Department of Industrial and Systems Engineering, Virginia Tech, Blacksburg, VA  24061, United States; Misu, T., Honda Research Instute USA, Inc., San Jose, CA  95134, United States","Recent AR research efforts have explored the use of virtual envi-ronments to test augmented reality (AR) user interfaces. However, it is yet to be seen what effects the visual fidelity of such virtual environments may have on AR interface assessment, and specifical-ly to what degree assessment results observed in a virtual world would apply to the real world. Automotive AR head-up (HUD) interfaces provide a meaningful application area to examine this problem, especially given that immersive, 3D-graphics-based driving simulators are established tools to examine in-vehicle interfaces safely before testing in real vehicles. In this work, we present an argument that adequately assessing AR interfaces requires a suite of different measures, and that such measures should be considered when debating the appropriateness of virtual environments for AR interface assessment. We present a case study that examines how an AR interface presented via HUD effects driver performance and behavior in different virtual and real environments. Twelve partici-pants completed the study measuring driver task performance, eye gaze behavior and situational awareness during AR guided navigation in low-and high-fidelity virtual simulation, and an on-road environment. Our results suggest that the visual fidelity of the envi-ronmental in which an AR interface is assessed, could impact some measures of effectiveness. Discussion is guided by a proposed initial assessment classification for AR user interfaces that may serve to guide future discussions on AR interface evaluation, as well as the suitability of virtual environments for AR assessment. © 2019 IEEE.","Augmented Reality; Head-up Displays; User Interface Assessment; Virtual Reality","Augmented reality; Behavioral research; Petroleum reservoir evaluation; Three dimensional computer graphics; User interfaces; Virtual reality; Ar user interfaces; Driver performance; Initial assessment; Interface evaluation; Measures of effectiveness; Real environments; Situational awareness; Virtual simulations; Head-up displays",Conference Paper,"Final","",Scopus,2-s2.0-85078259326
"Souchet A., Philippe S., Ober F., Leveque A., Leroy L.","57205639297;57205640104;57205643957;57205652990;19640647700;","Investigating cyclical stereoscopy effects over visual discomfort and fatigue in virtual reality while learning",2019,"Proceedings - 2019 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2019",,,"8943767","328","338",,3,"10.1109/ISMAR.2019.00031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078239706&doi=10.1109%2fISMAR.2019.00031&partnerID=40&md5=73a4c6f9f4aa3bd98cfe37c1f1d6e9e8","Paragraphe Laboratory, Paris 8 University, Saint-Denis and Manzalab, Paris, France; R&D Department Manzalab, Paris, France; Manzavision Aix-en-Provence, France; Paris 8 University, Saint-Denis and Armed Forces Biomedical Research Institute (IRBA), Neuroscience Department, Brétigny-sur-Orge, France","Souchet, A., Paragraphe Laboratory, Paris 8 University, Saint-Denis and Manzalab, Paris, France; Philippe, S., R&D Department Manzalab, Paris, France; Ober, F., Manzavision Aix-en-Provence, France; Leveque, A., Manzavision Aix-en-Provence, France; Leroy, L., Paris 8 University, Saint-Denis and Armed Forces Biomedical Research Institute (IRBA), Neuroscience Department, Brétigny-sur-Orge, France","Purpose: It is hypothesized that cyclical stereoscopy (displaying stereoscopy or 2D cyclically) has effect over visual fatigue, learning curves and quality of experience, and that those effects are different from regular stereoscopy. Materials and Methods: 59 participants played a serious game simulating a job interview with a Samsung Gear VR Head Mounted Display (HMD). Participants were randomly assigned to 3 groups: HMD with regular stereoscopy (S3D) and HMD with cyclical stereoscopy (cycles of 1 or 3 minutes). Participants played the game thrice (third try on a PC one month later). Visual discomfort, Flow, Presence, were measured with questionnaires. Visual Fatigue was assessed pre-and post-exposure with optometric measures. Learning traces were obtained in-game. Results: Visual discomfort and flow are lower with cyclical-S3D than S3D but not Presence. Cyclical stereoscopy every 1 minute is more tiring than stereoscopy. Cyclical stereoscopy every 3 minutes tends to be more tiring than stereoscopy. Cyclical stereoscopy groups improved during Short-Term Learning. None of the statistical tests showed a difference between groups in either Short-Term Learning or Long-Term Learning curves. Conclusion: cyclical stereoscopy displayed cyclically had a positive impact on Visual Comfort and Flow, but not Presence. It affects oculomotor functions in a HMD while learning with a serious game with low disparities and easy visual tasks. Other visual tasks should be tested, and eye-tracking should be considered to assess visual fatigue during exposure. Results in ecological conditions seem to support models suggesting that activating cyclically stereopsis in a HMD is more tiring than maintaining it. © 2019 IEEE.","Cyclical stereoscopy; Head mounted display; Serious game; Virtual reality; Visual fatigue","Augmented reality; E-learning; Eye tracking; Flow simulation; Quality of service; Serious games; Stereo image processing; Surveys; Virtual reality; Vision; Ecological conditions; Head mounted displays; Learning curves; Long-term learning; Quality of experience (QoE); Short term learning; Visual discomfort; Visual fatigue; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85078239706
"Liu Y., Li F., Tang L.H., Lan Z., Cui J., Sourina O., Chen C.-H.","36809950300;57196404325;57212572665;56480052000;56479735300;57204345367;25921980900;","Detection of humanoid robot design preferences using EEG and eye tracker",2019,"Proceedings - 2019 International Conference on Cyberworlds, CW 2019",,,"8919017","219","224",,4,"10.1109/CW.2019.00044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077129196&doi=10.1109%2fCW.2019.00044&partnerID=40&md5=649334c807cd0f49921ccee13f1f2c41","Fraunhofer Singapore, Singapore, Singapore; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, Singapore; Fraunhofer Singapore, Nanyang Technological University, Singapore, Singapore","Liu, Y., Fraunhofer Singapore, Singapore, Singapore; Li, F., School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, Singapore; Tang, L.H., School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, Singapore; Lan, Z., Fraunhofer Singapore, Nanyang Technological University, Singapore, Singapore; Cui, J., Fraunhofer Singapore, Nanyang Technological University, Singapore, Singapore; Sourina, O., Fraunhofer Singapore, Nanyang Technological University, Singapore, Singapore; Chen, C.-H., School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, Singapore","Currently, many modern humanoid robots have little appeal due to their simple designs and bland appearances. To provide recommendations for designers and improve the designs of humanoid robots, a study of human's perception on humanoid robot designs is conducted using Electroencephalogram (EEG), eye tracking information and questionnaires. We proposed and carried out an experiment with 20 subjects to collect the EEG and eye tracking data to study their reaction to different robot designs and the corresponding preference towards these designs. This study can possibly give us some insights on how people react to the aesthetic designs of different humanoid robot models and the important traits in a humanoid robot design, such as the perceived smartness and friendliness of the robots. Another point of interest is to investigate the most prominent feature of the robot, such as the head, facial features and the chest. The result shows that the head and facial features are the focus. It is also discovered that more attention is paid to the robots that appear to be more appealing. Lastly, it is affirmed that the first impressions of the robots generally do not change over time, which may imply that a good humanoid robot design impress the observers at first sight. © 2019 IEEE.","Design preference; EEG; Emotion recognition; Eye tracking; Workload recognition","Anthropomorphic robots; Electroencephalography; Eye tracking; Surveys; Aesthetic design; Design preferences; Electro-encephalogram (EEG); Emotion recognition; First impressions; Point of interest; Prominent features; Workload recognition; Machine design",Conference Paper,"Final","",Scopus,2-s2.0-85077129196
"Klein Salvalaio B., De Oliveira Ramos G.","57215354145;55617224100;","Self-adaptive appearance-based eye-tracking with online transfer learning",2019,"Proceedings - 2019 Brazilian Conference on Intelligent Systems, BRACIS 2019",,,"8924035","383","388",,,"10.1109/BRACIS.2019.00074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077026019&doi=10.1109%2fBRACIS.2019.00074&partnerID=40&md5=7b0988d351a586db34bfaea222724813","SAP Labs Latin America, São Leopoldo, RS, Brazil; Universidade Do Vale Do Rio Dos Sinos, São Leopoldo, RS, Brazil","Klein Salvalaio, B., SAP Labs Latin America, São Leopoldo, RS, Brazil; De Oliveira Ramos, G., Universidade Do Vale Do Rio Dos Sinos, São Leopoldo, RS, Brazil","Eye-tracking plays a role in human-computer interactions and has proven useful in a wide variety of domains. We consider appearance-based eye-tracking, where one tracks eye movements based solely on conventional images (rather than on sophisticated additional hardware). Recent advances made in Deep Learning and, in particular, convolutional neural networks have allowed appearance-based eye-tracking to achieve better results than ever. However, current literature still lacks methods that generalize to different combinations of user, environment and device. In this work, we introduce Online Deep Appearance-Based Eye-Tracking (ODABE), which overcomes such a limitation by considering online transfer learning, thus enabling eye-tracking models to self-adapt to different context very rapidly. Our results show that ODABE improves upon previous research when context changes, decreasing the prediction error by 50.95% on average, on tested cases. © 2019 IEEE.","Deep Learning; Eye-Tracking; Gaze-Tracking; Transfer Learning","Deep learning; E-learning; Eye movements; Human computer interaction; Intelligent systems; Neural networks; Appearance based; Convolutional neural network; Gaze tracking; Prediction errors; Self-adapt; Transfer learning; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85077026019
"Heng S.G., Samad R., Mustafa M., Abdullah N.R.H., Pebrianti D.","57212303267;8546771300;36069366700;35791718100;55268466200;","Analysis of performance between kinect V1 and kinect V2 for various facial part movements",2019,"2019 IEEE 9th International Conference on System Engineering and Technology, ICSET 2019 - Proceeding",,,"8906419","17","22",,4,"10.1109/ICSEngT.2019.8906419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076441380&doi=10.1109%2fICSEngT.2019.8906419&partnerID=40&md5=7fe83b81b60933509be0cc8c073a29bf","Universiti Malaysia Pahang, Faculty of Electrical and Electronics Engineering, Pekan, Pahang, 26600, Malaysia","Heng, S.G., Universiti Malaysia Pahang, Faculty of Electrical and Electronics Engineering, Pekan, Pahang, 26600, Malaysia; Samad, R., Universiti Malaysia Pahang, Faculty of Electrical and Electronics Engineering, Pekan, Pahang, 26600, Malaysia; Mustafa, M., Universiti Malaysia Pahang, Faculty of Electrical and Electronics Engineering, Pekan, Pahang, 26600, Malaysia; Abdullah, N.R.H., Universiti Malaysia Pahang, Faculty of Electrical and Electronics Engineering, Pekan, Pahang, 26600, Malaysia; Pebrianti, D., Universiti Malaysia Pahang, Faculty of Electrical and Electronics Engineering, Pekan, Pahang, 26600, Malaysia","The aim of this study is to determine the suitable version of Kinect motion sensor for developing facial therapy or exercise throughout the analysis of face tracking performance. A face tracking system is developed in both version of Kinect cameras by referring to the respective version of Kinect SDK. The created face tracking algorithms are then modified to display the detected facial points which are 121 points and 1347 points in total for Kinect v1 and Kinect v2 respectively. A total number of 18 desired facial feature point at similar landmarks will be extracted in the format of 3D coordinates for both Kinect cameras. To investigate the changes in the movement of facial feature points, the points will be paired up for distance ratio calculation between different frames of face image. The action unit of facial points for both Kinect cameras are different and there are some improvements in Kinect v2: asymmetrical facial points, high definition face detection and eye tracking as added action units. However, it shows poor detection in outer eyebrow part compared to Kinect v1. In overall, the Kinect v2 has better performance than Kinect v1 as it provides faster response speed and more detailed facial points movement detection in real-time operation for rehabilitation purposes. © 2019 IEEE.","Animation unit; Asymmetry; Face tracking; Facial part movement; Kinect v1; Kinect v2","Cameras; Eye tracking; Systems engineering; Asymmetry; Face Tracking; Facial parts; Kinect v1; Kinect v2; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85076441380
"Opromolla R., Inchingolo G., Fasano G.","56368337300;57211231076;16174945500;","Airborne visual detection and tracking of cooperative UAVs exploiting deep learning",2019,"Sensors (Switzerland)","19","19","4332","","",,17,"10.3390/s19194332","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073067938&doi=10.3390%2fs19194332&partnerID=40&md5=a97b80228f123561bcf2962e15358ed7","Department of Industrial Engineering, University of Naples Federico II, Piazzale Tecchio 80, Naples, 80125, Italy","Opromolla, R., Department of Industrial Engineering, University of Naples Federico II, Piazzale Tecchio 80, Naples, 80125, Italy; Inchingolo, G., Department of Industrial Engineering, University of Naples Federico II, Piazzale Tecchio 80, Naples, 80125, Italy; Fasano, G., Department of Industrial Engineering, University of Naples Federico II, Piazzale Tecchio 80, Naples, 80125, Italy","The performance achievable by using Unmanned Aerial Vehicles (UAVs) for a large variety of civil and military applications, as well as the extent of applicable mission scenarios, can significantly benefit from the exploitation of formations of vehicles able to fly in a coordinated manner (swarms). In this respect, visual cameras represent a key instrument to enable coordination by giving each UAV the capability to visually monitor the other members of the formation. Hence, a related technological challenge is the development of robust solutions to detect and track cooperative targets through a sequence of frames. In this framework, this paper proposes an innovative approach to carry out this task based on deep learning. Specifically, the You Only Look Once (YOLO) object detection system is integrated within an original processing architecture in which the machine-vision algorithms are aided by navigation hints available thanks to the cooperative nature of the formation. An experimental flight test campaign, involving formations of two multirotor UAVs, is conducted to collect a database of images suitable to assess the performance of the proposed approach. Results demonstrate high-level accuracy, and robustness against challenging conditions in terms of illumination, background and target-range variability. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Machine vision; UAV swarms; Unmanned aerial vehicles; Visual detection; Visual tracking; YOLO","Aircraft detection; Antennas; Computer architecture; Computer vision; Military applications; Military vehicles; Object detection; Unmanned aerial vehicles (UAV); Innovative approaches; Machine vision algorithm; Object detection systems; Processing architectures; Technological challenges; Visual detection; Visual Tracking; YOLO; Deep learning; article; deep learning; eye tracking; illumination; vision",Article,"Final","",Scopus,2-s2.0-85073067938
"Du X., Allan M., Bodenstedt S., Maier-Hein L., Speidel S., Dore A., Stoyanov D.","57188747603;55626720600;38560976600;22634618600;22433983800;22233346900;57203105770;","Patch-based adaptive weighting with segmentation and scale (PAWSS) for visual tracking in surgical video",2019,"Medical Image Analysis","57",,,"120","135",,3,"10.1016/j.media.2019.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068536374&doi=10.1016%2fj.media.2019.07.002&partnerID=40&md5=a882fe99fdf91bde0917d0895e9a6845","Wellcome / EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, United Kingdom; Intuitive Surgical Inc., United States; Karlsruhe Institute of Technology, Karlsruhe, Germany; Division of Computer-Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Heidelberg, Germany; Deliveroo, London, United Kingdom","Du, X., Wellcome / EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, United Kingdom; Allan, M., Intuitive Surgical Inc., United States; Bodenstedt, S., Karlsruhe Institute of Technology, Karlsruhe, Germany; Maier-Hein, L., Division of Computer-Assisted Medical Interventions (CAMI), German Cancer Research Center (DKFZ), Heidelberg, Germany; Speidel, S., Karlsruhe Institute of Technology, Karlsruhe, Germany; Dore, A., Deliveroo, London, United Kingdom; Stoyanov, D., Wellcome / EPSRC Centre for Interventional and Surgical Sciences (WEISS), University College London, United Kingdom","Vision-based tracking in an important component for building computer assisted interventions in minimally invasive surgery as it facilitates estimation of motion for instruments and anatomical targets. Tracking-by-detection algorithms are widely used for visual tracking, where the problem is treated as a classification task and a tracking target appearance model is updated over time using online learning. In challenging conditions, like surgical scenes, where tracking targets deform and vary in scale, the update step is prone to include background information in model appearance or to lack the ability to estimate change of scale, which degrades the performance of classifier. In this paper, we propose a Patch-based Adaptive Weighting with Segmentation and Scale (PAWSS) tracking framework that tackles both scale and background problems. A simple but effective colour-based segmentation model is used to suppress background information and multi-scale samples are extracted to enrich the training pool, which allows the tracker to handle both incremental and abrupt scale variations between frames. Experimentally, we evaluate our approach on Online Tracking Benchmark (OTB) dataset and Visual Object Tracking (VOT) challenge datasets, showing that our approach outperforms recent state-of-the-art trackers, and it especially improves successful rate score on OTB dataset, while on VOT datasets, PAWSS ranks among the top trackers while operating at real-time frame rates. Focusing on the application of PAWSS to surgical scenes, we evaluate on MICCAI 2015 challenge instrument tracking challenge and in vivo datasets, showing that our approach performs the best among all submitted methods and also has promising performance on in vivo surgical instrument tracking. © 2019 The Authors","Computer assisted interventions; Surgical instrument tracking; Tracking-by-detection; Visual object tracking","Classification (of information); Image segmentation; Scales (weighing instruments); Surgery; Surgical equipment; Background information; Colour-based segmentation; Computer assisted; Minimally invasive surgery; Performance of classifier; Surgical instrument; Tracking by detections; Visual object tracking; Target tracking; Article; benchmarking; computer assisted surgery; conceptual framework; ex vivo study; eye tracking; image analysis; image segmentation; machine learning; minimally invasive surgery; patch based adaptive weighting with segmentation and scale; priority journal; rating scale; support vector machine; algorithm; computer assisted surgery; computer interface; devices; human; image processing; procedures; robot assisted surgery; surgical equipment; videorecording; Algorithms; Humans; Image Processing, Computer-Assisted; Minimally Invasive Surgical Procedures; Robotic Surgical Procedures; Surgery, Computer-Assisted; Surgical Instruments; User-Computer Interface; Video Recording",Article,"Final","",Scopus,2-s2.0-85068536374
"Li F., Lee C.-H., Chen C.-H., Khoo L.P.","57196404325;56939382300;25921980900;7005381643;","Hybrid data-driven vigilance model in traffic control center using eye-tracking data and context data",2019,"Advanced Engineering Informatics","42",,"100940","","",,17,"10.1016/j.aei.2019.100940","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067337045&doi=10.1016%2fj.aei.2019.100940&partnerID=40&md5=9de00439369d600f1ba0dae0766531ad","School of Public Policy and Administration, Xi'an Jiaotong University, China; School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; Maritime Institute, Nanyang Technological University, Singapore","Li, F., School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, Maritime Institute, Nanyang Technological University, Singapore; Lee, C.-H., School of Public Policy and Administration, Xi'an Jiaotong University, China; Chen, C.-H., School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore; Khoo, L.P., School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore","Vigilance decrement of traffic controllers would greatly threaten public safety. Hence, extensive studies have been conducted to establish the physiological data-based vigilance model for objectively monitoring or detecting vigilance decrement. Nevertheless, most of them using intrusive devices to collect physiological data and failed to consider context information. Consequently, these models can be used in a laboratory environment while cannot adapt to dynamic working conditions of traffic controllers. The goal of this research is to develop an adaptive vigilance model for monitoring vigilance objectively and non-intrusively. In recent years, with advanced information and communication technology, a massive amount of data can be collected from connected daily use items. Hence, we proposed a hybrid data-driven approach based on connected objects for establishing vigilance model in the traffic control center and provide an elaborated case study to illustrate the method. Specifically, eye movements are selected as the primary inputs of the proposed vigilance model; Bagged trees technique is adapted to generate the vigilance model. The results of case study indicated that (1) eye metrics would be correlated with the vigilance performance subjected to the mental fatigue levels, (2) the bagged trees with the fusion features as inputs achieved a relatively stable performance under the condition of data loss, (3) the proposed method could achieve better performance than the other classic machine learning methods. © 2019 Elsevier Ltd","Data-driven; Eye movements; Internet of things; Traffic control center; Vigilance detection","Controllers; Eye movements; Forestry; Internet of things; Learning systems; Physiological models; Trees (mathematics); Advanced informations; Context information; Data driven; Laboratory environment; Machine learning methods; Traffic controllers; Vigilance decrement; Vigilance performance; Eye tracking",Article,"Final","",Scopus,2-s2.0-85067337045
"Giannakos M.N., Sharma K., Pappas I.O., Kostakos V., Velloso E.","36462343600;55903734200;55387371600;6508135447;53364337000;","Multimodal data as a means to understand the learning experience",2019,"International Journal of Information Management","48",,,"108","119",,44,"10.1016/j.ijinfomgt.2019.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062217663&doi=10.1016%2fj.ijinfomgt.2019.02.003&partnerID=40&md5=384be824196aab4492144f263bb599e1","Department of Computer and Information Science, Norwegian University of Science and, Technology (NTNU), Sem Sælands vei 7-9, Trondheim, 7491, Norway; School of Computing & Information Systems, University of Melbourne, Room 9.03, Doug McDonell Building 168, Parkville, VIC-3010, Australia; University of Agder (UiA), Kristiansand, Norway","Giannakos, M.N., Department of Computer and Information Science, Norwegian University of Science and, Technology (NTNU), Sem Sælands vei 7-9, Trondheim, 7491, Norway; Sharma, K., Department of Computer and Information Science, Norwegian University of Science and, Technology (NTNU), Sem Sælands vei 7-9, Trondheim, 7491, Norway; Pappas, I.O., Department of Computer and Information Science, Norwegian University of Science and, Technology (NTNU), Sem Sælands vei 7-9, Trondheim, 7491, Norway, University of Agder (UiA), Kristiansand, Norway; Kostakos, V., School of Computing & Information Systems, University of Melbourne, Room 9.03, Doug McDonell Building 168, Parkville, VIC-3010, Australia; Velloso, E., School of Computing & Information Systems, University of Melbourne, Room 9.03, Doug McDonell Building 168, Parkville, VIC-3010, Australia","Most work in the design of learning technology uses click-streams as their primary data source for modelling & predicting learning behaviour. In this paper we set out to quantify what, if any, advantages do physiological sensing techniques provide for the design of learning technologies. We conducted a lab study with 251 game sessions and 17 users focusing on skill development (i.e., user's ability to master complex tasks). We collected click-stream data, as well as eye-tracking, electroencephalography (EEG), video, and wristband data during the experiment. Our analysis shows that traditional click-stream models achieve 39% error rate in predicting learning performance (and 18% when we perform feature selection), while for fused multimodal the error drops up to 6%. Our work highlights the limitations of standalone click-stream models, and quantifies the expected benefits of using a variety of multimodal data coming from physiological sensing. Our findings help shape the future of learning technology research by pointing out the substantial benefits of physiological sensing. © 2019 Elsevier Ltd","Human learning; Machine learning; Multimodal data; Multimodal learning analytics; Skill acquisition; User-generated data","Electroencephalography; Electrophysiology; Eye tracking; Learning systems; Human learning; Multi-modal data; Multi-modal learning; Skill acquisition; User-generated; Physiological models",Article,"Final","",Scopus,2-s2.0-85062217663
"Widyantara P.B., Puspasari M.A.","57212465956;55628930600;","Breakpoint of attention media evaluation as countermeasure for computer vision syndrome",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"188","192",,1,"10.1145/3364335.3364360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076790686&doi=10.1145%2f3364335.3364360&partnerID=40&md5=6b07ad12b8066586f9f3171ffea8fff1","Department of Industrial Engineering, Universitas Indonesia, Depok, 16424, Indonesia","Widyantara, P.B., Department of Industrial Engineering, Universitas Indonesia, Depok, 16424, Indonesia; Puspasari, M.A., Department of Industrial Engineering, Universitas Indonesia, Depok, 16424, Indonesia","Computer Vision Syndrome (CVS) is a complex problem in the eyes and vision that is related to computer use. Approximately 60% reduction in the frequency of blinking during computer use increases the risk of dry eyes and other symptoms related to CVS. There is a need from ergonomics interventions as preventive strategy to CVS risk; one of them is breakpoint of attention media that can trigger the appearance of regular blinks. This study aimed to evaluate the breakpoint of attention media in visual form (which is represented by blink-blink application) and audio (which is represented by metronome application) as CVS countermeasures. Data retrieval was conducted using an eye-tracker for 30 participants. The measurement variables used were the overall blink rate, the frequency of blink duration > 500 ms (indicated as microsleep propensity), and ocular symptoms questionnaire. The results of this study indicate that the blink-blink application is statistically better than metronome in increasing the overall increasing blink frequency, reducing microsleep propensity and from subjective score of ocular symptoms. © 2019 Association for Computing Machinery.","Blink frequency; Breakpoint of attention; Computer Vision Syndrome; Countermeasure; Ergonomics","Ergonomics; Eye tracking; Blink frequencies; Breakpoint; Complex problems; Computer vision syndromes; Countermeasure; Ergonomics intervention; Ocular symptoms; Preventive strategies; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85076790686
"Hildebrandt M., Langstrand J.-P., Nguyen H.T.","8209590400;57209102920;57209108684;","Synopticon: A real-time data fusion platform for behavioral research",2019,"Adjunct Proceedings - 11th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI 2019",,,,"494","497",,1,"10.1145/3349263.3349597","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073596403&doi=10.1145%2f3349263.3349597&partnerID=40&md5=1e575be76cd23f0c536ebc5a24e97688","Biometrics Lab., Institute for Energy Technology, Halden, 1777, Norway","Hildebrandt, M., Biometrics Lab., Institute for Energy Technology, Halden, 1777, Norway; Langstrand, J.-P., Biometrics Lab., Institute for Energy Technology, Halden, 1777, Norway; Nguyen, H.T., Biometrics Lab., Institute for Energy Technology, Halden, 1777, Norway","Synopticon is a collection of tools for managing complex, multi-sensory data streams when conducting behavioral research in simulators or on the road. Synopticon’s functionality includes automatic gaze object detection, multi-camera synchronization, camera-sensor synchronization (e.g. physiological sensors), camera-simulator synchronization, and support for computer vision and machine learning. © 2019 Copyright is held by the owner/author(s).","Computer vision; Data fusion; Eye tracking; Machine learning; Simulator studies; Training; Virtual reality","Cameras; Computer vision; Data fusion; Eye tracking; Learning systems; Machine learning; Object detection; Personnel training; Simulators; Synchronization; User interfaces; Virtual reality; Camera sensor; Camera simulators; Multi-cameras; Multi-Sensory; Physiological sensors; Real-time data fusion; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85073596403
"Keyvanara M., Allison R.","57195734927;7101890629;","Transsaccadic awareness of scene transformations in a 3D virtual environment",2019,"Proceedings - SAP 2019: ACM Conference on Applied Perception",,,"a19","","",,2,"10.1145/3343036.3343121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073331310&doi=10.1145%2f3343036.3343121&partnerID=40&md5=e4d18604ddc27e2ebaed871f5416ee24","Department of Electrical Engineering and Computer Science, York University, Toronto, Canada","Keyvanara, M., Department of Electrical Engineering and Computer Science, York University, Toronto, Canada; Allison, R., Department of Electrical Engineering and Computer Science, York University, Toronto, Canada","In gaze-contingent displays, the viewer's eye movement data are processed in real-time to adjust the graphical content. To provide a high-quality user experience, these graphical updates must occur with minimum delay. Such updates can be used to introduce imperceptible changes in virtual camera pose in applications such as networked gaming, collaborative virtual reality and redirected walking. For such applications, perceptual saccadic suppression can help to hide the graphical artifacts. We investigated whether the visibility of these updates depends on the type of image transformation. Users viewed 3D scenes in which the displacement of a target object triggered them to generate a vertical or horizontal saccade, during which a translation or rotation was applied to the virtual camera used to render the scene. After each trial, users indicated the direction of the scene change in a forced-choice task. Results show that type and size of the image transformation affected change detectability. During horizontal or vertical saccades, rotations along the roll axis were the most detectable, while horizontal and vertical translations were least noticed. We confirm that large 3D adjustments to the scene viewpoint can be introduced unobtrusively and with low latency during saccades, but the allowable extent of the correction varies with the transformation applied. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Eye Tracking; Gaze-Contingent Displays; Image Transformations; Saccadic Suppression; Virtual Environments","Cameras; Eye tracking; Virtual reality; 3-D virtual environment; Collaborative virtual reality; Eye movement datum; Gaze-contingent displays; Image transformations; Redirected walkings; Saccadic suppression; User experience; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85073331310
"Volonte M., Anaraky R.G., Knijnenburg B.P., Duchowski A.T., Babu S.V.","57203974310;57211297282;35225065100;6701824388;9039004700;","Empirical evaluation of the interplay of emotion and visual attention in human-virtual human interaction",2019,"Proceedings - SAP 2019: ACM Conference on Applied Perception",,,"a1","","",,3,"10.1145/3343036.3343118","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073326692&doi=10.1145%2f3343036.3343118&partnerID=40&md5=5a0366504da2417c0650412811b236cc","Clemson University, United States","Volonte, M., Clemson University, United States; Anaraky, R.G., Clemson University, United States; Knijnenburg, B.P., Clemson University, United States; Duchowski, A.T., Clemson University, United States; Babu, S.V., Clemson University, United States","We examined the effect of rendering style and the interplay between attention and emotion in users during interaction with a virtual patient in a medical training simulator. The virtual simulation was rendered representing a sample from the photo-realistic to the non-photorealistic continuum, namely Near-Realistic, Cartoon or Pencil-Shader. In a mixed design study, we collected 45 participants' emotional responses and gaze behavior using surveys and an eye tracker while interacting with a virtual patient who was medically deteriorating over time. We used a cross-lagged panel analysis of attention and emotion to understand their reciprocal relationship over time. We also performed a mediation analysis to compare the extent to which the virtual agent's appearance and his affective behavior impacted users' emotional and attentional responses. Results showed the interplay between participants' visual attention and emotion over time and also showed that attention was a stronger variable than emotion during the interaction with the virtual human. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Eye tracking; Human-Computer Interaction; Virtual Human","Behavioral research; Eye tracking; Human computer interaction; Surveys; Affective behaviors; Emotional response; Empirical evaluations; Mediation analysis; Medical training simulator; Virtual humans; Virtual simulations; Visual Attention; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85073326692
"Landsmann M., Augereau O., Kise K.","57211179842;25421349800;16178222100;","Classification of reading and not reading behavior based on eye movement analysis",2019,"UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers",,,,"109","112",,1,"10.1145/3341162.3343811","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072888464&doi=10.1145%2f3341162.3343811&partnerID=40&md5=479ca30d8f663c1ababc044192cfc213","Department of Computer Science and Intelligent Systems, Osaka Prefecture University, Japan","Landsmann, M., Department of Computer Science and Intelligent Systems, Osaka Prefecture University, Japan; Augereau, O., Department of Computer Science and Intelligent Systems, Osaka Prefecture University, Japan; Kise, K., Department of Computer Science and Intelligent Systems, Osaka Prefecture University, Japan","Nowadays, many researchers analyze reading behavior with eye trackers. Various traits of reading like engagement, or text difficulty have been observed in laboratory settings. But, their automatic application for daily life is usually prevented by one question: when is somebody reading? We have developed a tool to classify short sequences of fixations from eye gaze data into reading and not reading. Our specific use case is the Vocabulometer, a website for learning English by reading texts. We used supervised learning on data from nonnative English speakers to train decision trees for the classification. With features based on vertical eye movement, we achieved 93.1% of correct classifications. © 2019 Copyright held by the owner/author(s).","Eye Movement; Eye Tracking; Reading; Supervised Learning","Behavioral research; Classification (of information); Decision trees; Eye tracking; Machine learning; Supervised learning; Trees (mathematics); Ubiquitous computing; Wearable computers; Automatic application; Behavior-based; Daily lives; Eye movement analysis; Eye trackers; Learning English; Reading; Short sequences; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85072888464
"Khan A.A.","57216684394;","Gaze assisted voice note taking system",2019,"UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers",,,,"367","371",,1,"10.1145/3341162.3349308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072883712&doi=10.1145%2f3341162.3349308&partnerID=40&md5=6c3445e47f6a9fc64175caf15f0163f7","University of Melbourne, Melbourne, VIC, Australia","Khan, A.A., University of Melbourne, Melbourne, VIC, Australia",[无可用摘要],"Eye tracking; Implicit tagging; Machine learning; Note-taking",,Conference Paper,"Final","",Scopus,2-s2.0-85072883712
"Breitenfellner M., Jungwirth F., Ferscha A.","57211180100;57144436700;6701318941;","Towards 3D smooth pursuit interaction",2019,"UbiComp/ISWC 2019- - Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers",,,,"619","623",,2,"10.1145/3341162.3348385","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072883017&doi=10.1145%2f3341162.3348385&partnerID=40&md5=0a4ff50deca5c2c38ba9a950bf24c8d7","Institute of Pervasive Computing, Johannes Kepler University, Linz, Austria","Breitenfellner, M., Institute of Pervasive Computing, Johannes Kepler University, Linz, Austria; Jungwirth, F., Institute of Pervasive Computing, Johannes Kepler University, Linz, Austria; Ferscha, A., Institute of Pervasive Computing, Johannes Kepler University, Linz, Austria","In this position paper, we encourage the use of novel 3D gaze tracking possibilities in the field of gaze-based interaction. Smooth pursuit offers great benefits over other gaze interaction approaches, like the ability to work with uncalibrated eye trackers, but also has disadvantages like the produced visual clutter in more complex user interfaces. We examine the basic concept of smooth pursuits, its hardware and algorithmic requirements and how this can be applied to real world problems. Then we evaluate how the recent change in availability of 3D eye tracking hardware can be used to approach the challenges of 2D smooth pursuit interaction. We take a look at different research opportunities, show concrete ideas and discuss why they are relevant for future research. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Eye Tracker Calibration; Gaze Interaction; Smooth Pursuit","Computer hardware; Ubiquitous computing; User interfaces; Wearable computers; Basic concepts; Eye trackers; Gaze interaction; Gaze-based interaction; Position papers; Real-world problem; Research opportunities; Smooth pursuit; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85072883017
"Trokielewicz M., Czajka A., Maciejewicz P.","56520110000;6602827299;15122266600;","Perception of Image Features in Post-Mortem Iris Recognition: Humans vs Machines",2019,"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems, BTAS 2019",,,"9185980","","",,4,"10.1109/BTAS46853.2019.9185980","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087302662&doi=10.1109%2fBTAS46853.2019.9185980&partnerID=40&md5=cd1c02439b87685d5d7b195b0f95c90d","Research and Academic Computer Network, Biometrics and Machine Intelligence Lab, Kolska 12, Warsaw, 01045, Poland; University of Notre Dame, Department of Computer Science and Engineering, Notre Dame, IN  46556, United States; Medical University of Warsaw, Department of Ophthalmology, Lindleya 4, Warsaw, 02005, Poland","Trokielewicz, M., Research and Academic Computer Network, Biometrics and Machine Intelligence Lab, Kolska 12, Warsaw, 01045, Poland; Czajka, A., University of Notre Dame, Department of Computer Science and Engineering, Notre Dame, IN  46556, United States; Maciejewicz, P., Medical University of Warsaw, Department of Ophthalmology, Lindleya 4, Warsaw, 02005, Poland","Post-mortem iris recognition can offer an additional forensic method of personal identification. However, in contrary to already well-established human examination of fingerprints, making iris recognition human-interpretable is harder, and therefore it has never been applied in forensic proceedings. There is no strong consensus among biometric experts which iris features, especially those in iris images acquired post-mortem, are the most important for human experts solving an iris recognition task. This paper explores two ways of broadening this knowledge: (a) with an eye tracker, the salient features used by humans comparing iris images on a screen are extracted, and (b) class-activation maps produced by the convolutional neural network solving the iris recognition task are analyzed. Both humans and deep learning-based solutions were examined with the same set of iris image pairs. This made it possible to compare the attention maps and conclude that (a) deep learning-based method can offer human-interpretable decisions backed by visual explanations pointing a human examiner to salient regions, and (b) in many cases humans and a machine used different features, what means that a deep learning-based method can offer a complementary support to human experts. This paper offers the first known to us human-interpretable comparison of machine-based and human-based post-mortem iris recognition, and the trained models annotating salient iris image regions. © 2019 IEEE.",,"Convolutional neural networks; Deep learning; Eye tracking; Forensic science; Turing machines; Activation maps; Image features; Iris features; Iris recognition; Learning-based methods; Personal identification; Salient features; Salient regions; Biometrics",Conference Paper,"Final","",Scopus,2-s2.0-85087302662
"Changwani A., Sarode T.","57216489584;24829549800;","Low-Cost Eye Tracking for Foveated Rendering Using Machine Learning",2019,"Proceedings - 2019 8th IEEE International Conference on Cloud Computing in Emerging Markets, CCEM 2019",,,"9051953","32","39",,1,"10.1109/CCEM48484.2019.000-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083662700&doi=10.1109%2fCCEM48484.2019.000-1&partnerID=40&md5=7e294d6037ff9b1df748030ae48aa0e2","Computer Science, Thadomal Shahani Engineering College, Mumbai University, India","Changwani, A., Computer Science, Thadomal Shahani Engineering College, Mumbai University, India; Sarode, T., Computer Science, Thadomal Shahani Engineering College, Mumbai University, India","This paper outlines a $50 head-mounted real-time eye tracker to track the user's eye, and uses foveated rendering reduce bandwidth costs and improve immersion. This is accomplished by using two cameras for detecting the iris and locating the beacons around the screen which tracks head movement in three dimensions. A neural net is trained on this data which then predicts where the user is looking based on the inputs from the two cameras. Foveated rendering is performed by only rendering the area of the screen currently being focused and blurring the area that falls under peripheral vision. Maturing this technology could be integrated into virtual reality headsets and for other immersive media experiences while dramatically decreasing bandwidth costs and increasing the overall functionality and capabilities of the devices. This would allow for a more robust as well as wireless virtual reality experience, which would mitigate the largest drawback to mass adoption: clunky wires and lack of a truly realistic experience. © 2019 IEEE.","eye tracking; foveated rendering; iris detection; machine learning; neural network","Bandwidth; Cameras; Cloud computing; Commerce; Costs; Machine learning; Virtual reality; Head movements; Immersive media; Low cost eye tracking; Mass adoption; Peripheral vision; Three dimensions; Virtual reality experiences; Virtual-reality headsets; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85083662700
"Dipaola S., Yalcin O.N.","14035416700;57203034300;","A multi-layer artificial intelligence and sensing based affective conversational embodied agent",2019,"2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos, ACIIW 2019",,,"8925291","91","92",,1,"10.1109/ACIIW.2019.8925291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077818348&doi=10.1109%2fACIIW.2019.8925291&partnerID=40&md5=49ce11dd00a59cacaee1067695a622c4","School of Interactive Art and Tech, Simon Fraser University, Vancouver, Canada","Dipaola, S., School of Interactive Art and Tech, Simon Fraser University, Vancouver, Canada; Yalcin, O.N., School of Interactive Art and Tech, Simon Fraser University, Vancouver, Canada","Building natural and conversational virtual humans is a task of formidable complexity. We believe that, especially when building agents that affectively interact with biological humans in real-time, a cognitive science-based, multilayered sensing and artificial intelligence (AI) systems approach is needed. For this demo, we show a working version (through human interaction with it) our modular system of natural, conversation 3D virtual human using AI or sensing layers. These including sensing the human user via facial emotion recognition, voice stress, semantic meaning of the words, eye gaze, heart rate, and galvanic skin response. These inputs are combined with AI sensing and recognition of the environment using deep learning natural language captioning or dense captioning. These are all processed by our AI avatar system allowing for an affective and empathetic conversation using an NLP topic-based dialogue capable of using facial expressions, gestures, breath, eye gaze and voice language-based two-way back and forth conversations with a sensed human. Our lab has been building these systems in stages over the years. © 2019 IEEE.","affective computing; artificial intelligence; biosensing; conversational agent; deep learning; embodied agent; embodied character agents; sensing systems","Artificial intelligence; Cognitive systems; Deep learning; Electrophysiology; FORTH (programming language); Intelligent computing; Real time systems; Semantics; Speech recognition; Affective Computing; Biosensing; Conversational agents; Embodied agent; Embodied characters; Sensing systems; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85077818348
"Alexiou E., Xu P., Ebrahimi T.","57195290308;57212481131;35560920500;","Towards Modelling of Visual Saliency in Point Clouds for Immersive Applications",2019,"Proceedings - International Conference on Image Processing, ICIP","2019-September",,"8803479","4325","4329",,1,"10.1109/ICIP.2019.8803479","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076823536&doi=10.1109%2fICIP.2019.8803479&partnerID=40&md5=ea056021aadfc5622dc7264e4b913186","École Polytechnique Fédéral de Lausanne (EPFL), Multimedia Signal Processing Group (MMSPG), Switzerland","Alexiou, E., École Polytechnique Fédéral de Lausanne (EPFL), Multimedia Signal Processing Group (MMSPG), Switzerland; Xu, P., École Polytechnique Fédéral de Lausanne (EPFL), Multimedia Signal Processing Group (MMSPG), Switzerland; Ebrahimi, T., École Polytechnique Fédéral de Lausanne (EPFL), Multimedia Signal Processing Group (MMSPG), Switzerland","Modelling human visual attention is of great importance in the field of computer vision and has been widely explored for 3D imaging. Yet, in the absence of ground truth data, it is unclear whether such predictions are in alignment with the actual human viewing behavior in virtual reality environments. In this study, we work towards solving this problem by conducting an eye-tracking experiment in an immersive 3D scene that offers 6 degrees of freedom. A wide range of static point cloud models is inspected by human subjects, while their gaze is captured in real-time. The visual attention information is used to extract fixation density maps, that can be further exploited for saliency modelling. To obtain high quality fixation points, we devise a scheme that utilizes every recorded gaze measurement from the two eye-cameras of our set-up. The obtained fixation density maps together with the recorded gaze and head trajectories are made publicly available, to enrich visual saliency datasets for 3D models. © 2019 IEEE.","eye-tracking; immersive environments; point clouds; virtual reality; visual saliency",,Conference Paper,"Final","",Scopus,2-s2.0-85076823536
"Alkabbany I., Ali A., Farag A., Bennett I., Ghanoum M., Farag A.","55165355300;15041795600;32367543600;56748760400;57202375563;7102591299;","Measuring Student Engagement Level Using Facial Information",2019,"Proceedings - International Conference on Image Processing, ICIP","2019-September",,"8803590","3337","3341",,3,"10.1109/ICIP.2019.8803590","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076809824&doi=10.1109%2fICIP.2019.8803590&partnerID=40&md5=ef32c083334e2f16412cc25f76402dab","CVIP Lab University of Louisville, United States; TSN, Inc., Palo Alto, CA, United States","Alkabbany, I., CVIP Lab University of Louisville, United States; Ali, A., CVIP Lab University of Louisville, United States; Farag, A., TSN, Inc., Palo Alto, CA, United States; Bennett, I., TSN, Inc., Palo Alto, CA, United States; Ghanoum, M., CVIP Lab University of Louisville, United States; Farag, A., CVIP Lab University of Louisville, United States","In this paper, we propose a novel framework that measures the engagement level of students either in a class environment or in an e-learning environment. The proposed framework captures the user's video and tracks their faces' through the video's frames. Different features are extracted from the user's face e.g., facial fiducial points, head pose, eye gaze, learned features, etc. These features are then used to detect the Facial Action Coding System (FACS), which decomposes facial expressions in terms of the fundamental actions of individual muscles or groups of muscles (i.e., action units). The decoded action units (AU's) are then used to measures the student's willingness to participate in the learning process (i.e., behavioral engagement) and his/her emotional attitude towards learning (i.e., emotional engagement). This framework will allow the lecturer to receive a real-time feedback from facial features, gaze, and other body kinesics. The framework is robust and can be utilized in numerous applications including but not limited to the monitoring the progress of students with various degrees of learning disabilities, and the analysis of nerve palsy and its effects on facial expression and social interactions. © 2019 IEEE.","Engagement level measurement; Learning Disabilities; Machine Learning",,Conference Paper,"Final","",Scopus,2-s2.0-85076809824
"Cristina S., Camilleri K.P.","49963155000;8301303700;","Gaze tracking by joint head and eye pose estimation under free head movement",2019,"European Signal Processing Conference","2019-September",,,"","",,,"10.23919/EUSIPCO.2019.8902786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075621911&doi=10.23919%2fEUSIPCO.2019.8902786&partnerID=40&md5=ba8888192d6c60ba861c0efe632f1595","Department of Systems and Control Engineering, University of Malta, Msida, Malta","Cristina, S., Department of Systems and Control Engineering, University of Malta, Msida, Malta; Camilleri, K.P., Department of Systems and Control Engineering, University of Malta, Msida, Malta","Recent trends in the field of eye-gaze tracking have been shifting towards the estimation of gaze direction in everyday life settings, hence calling for methods that alleviate the constraints typically associated with existing methods, which limit their applicability in less controlled conditions. In this paper, we propose a method for eye-gaze estimation as a function of both eye and head pose components, without requiring prolonged user-cooperation prior to gaze estimation. Our method exploits the trajectories of salient feature trackers spread randomly over the face region for the estimation of the head rotation angles, which are subsequently used to drive a spherical eye-in-head rotation model that compensates for the changes in eye region appearance under head rotation. We investigate the validity of the proposed method on a publicly available data set. © 2019 IEEE","Eye-gaze tracking; Passive; Pervasive","Digital storage; Eye movements; Image segmentation; Controlled conditions; Eye gaze tracking; Gaze estimation; Passive; Pervasive; Pose estimation; Salient features; User cooperation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85075621911
"Porta M., Barboni A.","35100711800;57211503033;","Strengthening Security in Industrial Settings: A Study on Gaze-Based Biometrics through Free Observation of Static Images",2019,"IEEE International Conference on Emerging Technologies and Factory Automation, ETFA","2019-September",,"8868961","1273","1277",,1,"10.1109/ETFA.2019.8868961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074203607&doi=10.1109%2fETFA.2019.8868961&partnerID=40&md5=53976039113f1941dfedef2560cdbaf5","Dipartimento di Ingegneria Industriale e dell'Informazione, Università di Pavia, Via A. Ferrata 5, Pavia, 27100, Italy","Porta, M., Dipartimento di Ingegneria Industriale e dell'Informazione, Università di Pavia, Via A. Ferrata 5, Pavia, 27100, Italy; Barboni, A., Dipartimento di Ingegneria Industriale e dell'Informazione, Università di Pavia, Via A. Ferrata 5, Pavia, 27100, Italy","As security becomes crucial in an increasing number of industrial contexts, the need arises for new ways to check or authenticate the identity of people. In this paper, we present a method that exploits gaze data to implement a soft biometric technique. Specifically, the user's gaze behavior is inspected during the unconstrained observation of different kinds of static images. The obtained results, achieved using a machine learning approach, are generally satisfying, although more experiments will be necessary to fully confirm the viability of the proposed method. © 2019 IEEE.","eye tracking; gaze behavior; security; soft biometrics","Behavioral research; Biometrics; Factory automation; Gaze behavior; Industrial context; Industrial settings; Machine learning approaches; security; Soft biometrics; Static images; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85074203607
"Xia Y., Yu H., Wang F.-Y.","57192671097;56115992300;57211758869;","Accurate and robust eye center localization via fully convolutional networks",2019,"IEEE/CAA Journal of Automatica Sinica","6","5","8823575","1127","1138",,24,"10.1109/JAS.2019.1911684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072205184&doi=10.1109%2fJAS.2019.1911684&partnerID=40&md5=784d75d4187458d061ca3bd484cda82d","School of Creative Technologies, University of Portsmouth, Portsmouth, PO1 2DJ, United Kingdom; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China","Xia, Y., School of Creative Technologies, University of Portsmouth, Portsmouth, PO1 2DJ, United Kingdom; Yu, H., School of Creative Technologies, University of Portsmouth, Portsmouth, PO1 2DJ, United Kingdom; Wang, F.-Y., State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China","Eye center localization is one of the most crucial and basic requirements for some human-computer interaction applications such as eye gaze estimation and eye tracking. There is a large body of works on this topic in recent years, but the accuracy still needs to be improved due to challenges in appearance such as the high variability of shapes, lighting conditions, viewing angles and possible occlusions. To address these problems and limitations, we propose a novel approach in this paper for the eye center localization with a fully convolutional network (FCN), which is an end-to-end and pixels-to-pixels network and can locate the eye center accurately. The key idea is to apply the FCN from the object semantic segmentation task to the eye center localization task since the problem of eye center localization can be regarded as a special semantic segmentation problem. We adapt contemporary FCN into a shallow structure with a large kernel convolutional block and transfer their performance from semantic segmentation to the eye center localization task by fine-tuning. Extensive experiments show that the proposed method outperforms the state-of-the-art methods in both accuracy and reliability of eye center localization. The proposed method has achieved a large performance improvement on the most challenging database and it thus provides a promising solution to some challenging applications. © 2014 Chinese Association of Automation.","Deep learning; eye center localization; eye gaze estimation; eye tracking; fully convolutional network (FCN); humancomputer interaction","Convolution; Convolutional neural networks; Deep learning; Human computer interaction; Pixels; Semantics; Convolutional networks; eye center localization; Eye-gaze; Lighting conditions; Object semantic; Semantic segmentation; Shallow structure; State-of-the-art methods; Eye tracking",Article,"Final","",Scopus,2-s2.0-85072205184
"Corcoran P., Lemley J., Costache C., Varkarakis V.","57190839462;23005117400;56956976400;57204764493;","Deep Learning for Consumer Devices and Services 2-AI Gets Embedded at the Edge",2019,"IEEE Consumer Electronics Magazine","8","5","8822518","10","19",,2,"10.1109/MCE.2019.2923042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072088033&doi=10.1109%2fMCE.2019.2923042&partnerID=40&md5=668c048809a00ac216491b89ce10bab0","National University of Ireland Galway (NUIG), Galway, Ireland","Corcoran, P., National University of Ireland Galway (NUIG), Galway, Ireland; Lemley, J., National University of Ireland Galway (NUIG), Galway, Ireland; Costache, C., National University of Ireland Galway (NUIG), Galway, Ireland; Varkarakis, V., National University of Ireland Galway (NUIG), Galway, Ireland","The recent explosive growth of deep learning is enabling a new generation of intelligent consumer devices. Specialized deep learning inference now provides data analysis capabilities that once required an active cloud connection, while reducing latency and enhancing data privacy. This paper addresses current progress in Edge artificial intelligence (AI) technology in several consumer contexts including privacy, biometrics, eye gaze, driver monitoring systems, and more. New developments and challenges in edge hardware and emerging opportunities are identified. Our previous article, Deep learning for consumer devices and services, introduced many of the basics of deep learning and AI. In this paper, we explore the current paradigm shift of AI from the data center into CE devices-Edge-AI. © 2012 IEEE.",,"Data privacy; Analysis capabilities; Artificial intelligence technologies; Cloud connections; Consumer devices; Data centers; Driver monitoring system; Explosive growth; Paradigm shifts; Deep learning",Article,"Final","",Scopus,2-s2.0-85072088033
"Gite S., Agrawal H., Kotecha K.","56656365900;56537986300;6506676097;","Early anticipation of driver’s maneuver in semiautonomous vehicles using deep learning",2019,"Progress in Artificial Intelligence","8","3",,"293","305",,4,"10.1007/s13748-019-00177-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070707497&doi=10.1007%2fs13748-019-00177-z&partnerID=40&md5=21d5bd018e84abededa05dec3d5bc05d","Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, India","Gite, S., Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, India; Agrawal, H., Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, India; Kotecha, K., Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, India","Making machines to anticipate human action is a complex research problem. Some of the recent research studies on computer vision and assistive driving have reported that the anticipation of driver’s action few seconds in advance is a challenging problem. These studies are based on the driver’s head movement tracking, eye gaze tracking, and spatiotemporal interest points. The study is aimed to address an important question of how to anticipate a driver’s action while driving and improve the anticipation time. The goal of this study is to review the existing deep learning framework for assistive driving. This paper differs from the existing solutions in two ways. First, it proposes a simplified framework using the driver’s inside video data and develops a driver’s movement tracking (DMT) algorithm. Majority of the existing state of the art is based on inside and outside features of the vehicles. Second, the proposed work tends to improve the image pattern recognition by introducing a fusion of spatiotemporal data points (STIPs) for movement tracking along with eye cuboids and then action anticipation by using deep learning. The proposed DMT algorithm tracks the driver’s movement using STIPs from the input video. Also, a fast eye gaze algorithm tracks eye movements. The features extracted from STIP and eye gaze are fused and analyzed by a deep recurrent neural network to improve the prediction time, thereby giving a few extra seconds to anticipate the driver’s correct action. The performance of the DMT algorithm is compared with the previous algorithms and found that DMT offers 30% improvement with regards to anticipating driver’s action over two recently proposed deep learning algorithms. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Cuboids; Driver safety; Driver’s action tracking; Eye gaze features; Recurrent neural network; Spatiotemporal interest points","Eye movements; Eye tracking; Image enhancement; Learning algorithms; Motion analysis; Pattern recognition; Recurrent neural networks; Action tracking; Cuboids; Driver safety; Eye-gaze; Spatio-temporal interest points; Deep learning",Article,"Final","",Scopus,2-s2.0-85070707497
"Alkhatib M., Hafiane A., Vieyres P., Delbos A.","57201492293;23396705600;6602685261;6602923490;","Deep visual nerve tracking in ultrasound images",2019,"Computerized Medical Imaging and Graphics","76",,"101639","","",,5,"10.1016/j.compmedimag.2019.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069687388&doi=10.1016%2fj.compmedimag.2019.05.007&partnerID=40&md5=97a1c818ef65abf7413861bb494bb0a0","INSA Centre Val de Loire, Laboratoire PRISME EA 4229, Bourges, F-18000, France; Université d'Orléans, Laboratoire PRISME EA 4229, Bourges, F-18000, France; Clinique Médipôle Garonne, Toulouse, F-31036, France","Alkhatib, M., INSA Centre Val de Loire, Laboratoire PRISME EA 4229, Bourges, F-18000, France, Université d'Orléans, Laboratoire PRISME EA 4229, Bourges, F-18000, France; Hafiane, A., INSA Centre Val de Loire, Laboratoire PRISME EA 4229, Bourges, F-18000, France; Vieyres, P., Université d'Orléans, Laboratoire PRISME EA 4229, Bourges, F-18000, France; Delbos, A., Clinique Médipôle Garonne, Toulouse, F-31036, France","Ultrasound-guided regional anesthesia (UGRA) becomes a standard procedure in surgical operations and pain management, offers the advantages of nerve localization, and provides region of interest anatomical structure visualization. Nerve tracking presents a crucial step for practicing UGRA and it is useful and important to develop a tool to facilitate this step. However, nerve tracking is a very challenging task that anesthetists can encounter due to the noise, artifacts, and nerve structure variability. Deep-learning has shown outstanding performances in computer vision task including tracking. Many deep-learning trackers have been proposed, where their performance depends on the application. While no deep-learning study exists for tracking the nerves in ultrasound images, this paper explores thirteen most recent deep-learning trackers for nerve tracking and presents a comparative study for the best deep-learning trackers on different types of nerves in ultrasound images. We evaluate the performance of the trackers in terms of accuracy, consistency, time complexity, and handling different nerve situations, such as disappearance and losing shape information. Through the experimentation, certain conclusions were noted on deep learning trackers performance. Overall, deep-learning trackers provide good performance and show a comparative performance for tracking different kinds of nerves in ultrasound images. © 2019 Elsevier Ltd","Deep-learning; Nerve tracking; Regional anesthesia; Ultrasound images; Visual tracking","Anesthesiology; Image segmentation; Surgery; Ultrasonics; Anatomical structures; Comparative performance; Comparative studies; Regional anesthesia; Standard procedures; Surgical operation; Ultrasound images; Visual Tracking; Deep learning; article; comparative study; controlled study; deep learning; eye tracking; human; nerve; regional anesthesia; ultrasound; vision; diagnostic imaging; interventional ultrasonography; peripheral nerve; procedures; Anesthesia, Conduction; Deep Learning; Humans; Peripheral Nerves; Ultrasonography, Interventional",Article,"Final","",Scopus,2-s2.0-85069687388
"Bocheńska A., Markiewicz J., Łapiński S.","57202440705;56539603600;57190759314;","THE COMBINATION of the IMAGE and RANGE-BASED 3D ACQUISITION in ARCHAEOLOGICAL and ARCHITECTURAL RESEARCH in the ROYAL CASTLE in WARSAW",2019,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","2/W15",,"177","184",,5,"10.5194/isprs-archives-XLII-2-W15-177-2019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072208076&doi=10.5194%2fisprs-archives-XLII-2-W15-177-2019&partnerID=40&md5=fc0a2d956c79aefbcc266ff5ec162883","Royal Castle in Warsaw, Archaeology Department, Poland; Faculty of Geodesy and Cartography, Division of Photogrammetry, Remote Sensing and Spatial Information Systems, Warsaw University of Technology, Warsaw, Poland; Faculty of Geodesy and Cartography, Division of Engineering Geodesy and Control Surveying System, Warsaw University of Technology, Warsaw, Poland","Bocheńska, A., Royal Castle in Warsaw, Archaeology Department, Poland; Markiewicz, J., Faculty of Geodesy and Cartography, Division of Photogrammetry, Remote Sensing and Spatial Information Systems, Warsaw University of Technology, Warsaw, Poland; Łapiński, S., Faculty of Geodesy and Cartography, Division of Engineering Geodesy and Control Surveying System, Warsaw University of Technology, Warsaw, Poland","The paper presents archaeological and architectural research in the Royal Castle in Warsaw where a combination of image- and range-based 3D acquisition was applied. The area examined included excavations situated inside the Tower and near its outer western wall. The work was carried out at various periods and in different weather conditions. As part of the measurements, laser scanning was performed (with a Z+F 5006h scanner) and a series of close-range images were taken. It was important to integrate the data acquired to create a comprehensive documentation of archaeological excavations. When data was acquired from TLS together with photogrammetric data (in different measurement periods), the points' displacements were controlled and analysed. The process of orienting and processing the terrestrial images included photographs taken during the inventory of the tower (Canon 5D Mark II) and photographs provided by the Castle's employees (Canon PowerShot G5 X). Agisoft PhotoScan software was used to orient and process the terrestrial images, and LupoScan for the TLS data. In order to integrate the TLS data and the clouds of points from the photographs from the various stages, they were processed into a raster form; our own software (based on the OpenCV library and the Structure-from-Motion method) and LupoScan software were used to interconnect the multi-temporal and multi-sensor data sets. As a result of processing photographs and TLS data, point clouds in an external reference system were obtained. This data was then used to study the thickness of the walls of the Justice Court Tower, to analyse the course of the retaining wall, and to generate the orthoimages necessary for chronological analysis. © 2019 International Society for Photogrammetry and Remote Sensing. All rights reserved.","Codesign; Cultural heritage; Data Driven Design; EEG; Eye-tracking; Landscape; Perception; Survey","Electroencephalography; Excavation; Eye tracking; Photography; Seebeck effect; Sensory perception; Surveying; Archaeological excavations; Architectural research; Chronological analysis; Co-designs; Comprehensive documentation; Cultural heritages; Data-driven design; Landscape; Surveying instruments",Conference Paper,"Final","",Scopus,2-s2.0-85072208076
"Stember J.N., Celik H., Krupinski E., Chang P.D., Mutasa S., Wood B.J., Lignelli A., Moonis G., Schwartz L.H., Jambawalikar S., Bagci U.","18538253000;57212691442;26643320200;57192687394;56060604800;7401873523;6506606486;57209728962;7402459609;6507408536;24176491700;","Eye Tracking for Deep Learning Segmentation Using Convolutional Neural Networks",2019,"Journal of Digital Imaging","32","4",,"597","604",,12,"10.1007/s10278-019-00220-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065245397&doi=10.1007%2fs10278-019-00220-4&partnerID=40&md5=6be09cea0783801dfe1c5ba0c0c5f9b4","Department of Radiology, Columbia University Medical Center - NYPH, New York, NY  10032, United States; The National Institutes of Health, Clinical Center, Bethesda, MD  20892, United States; Department of Radiology & Imaging Sciences, Emory University, Atlanta, GA  30322, United States; Department of Radiology, University of California, Irvine, CA  92697, United States; Center for Research in Computer Vision, University of Central Florida, 4328 Scorpius St. HEC 221, Orlando, FL  32816, United States","Stember, J.N., Department of Radiology, Columbia University Medical Center - NYPH, New York, NY  10032, United States; Celik, H., The National Institutes of Health, Clinical Center, Bethesda, MD  20892, United States; Krupinski, E., Department of Radiology & Imaging Sciences, Emory University, Atlanta, GA  30322, United States; Chang, P.D., Department of Radiology, University of California, Irvine, CA  92697, United States; Mutasa, S., Department of Radiology, Columbia University Medical Center - NYPH, New York, NY  10032, United States; Wood, B.J., The National Institutes of Health, Clinical Center, Bethesda, MD  20892, United States; Lignelli, A., Department of Radiology, Columbia University Medical Center - NYPH, New York, NY  10032, United States; Moonis, G., Department of Radiology, Columbia University Medical Center - NYPH, New York, NY  10032, United States; Schwartz, L.H., Department of Radiology, Columbia University Medical Center - NYPH, New York, NY  10032, United States; Jambawalikar, S., Department of Radiology, Columbia University Medical Center - NYPH, New York, NY  10032, United States; Bagci, U., Center for Research in Computer Vision, University of Central Florida, 4328 Scorpius St. HEC 221, Orlando, FL  32816, United States","Deep learning with convolutional neural networks (CNNs) has experienced tremendous growth in multiple healthcare applications and has been shown to have high accuracy in semantic segmentation of medical (e.g., radiology and pathology) images. However, a key barrier in the required training of CNNs is obtaining large-scale and precisely annotated imaging data. We sought to address the lack of annotated data with eye tracking technology. As a proof of principle, our hypothesis was that segmentation masks generated with the help of eye tracking (ET) would be very similar to those rendered by hand annotation (HA). Additionally, our goal was to show that a CNN trained on ET masks would be equivalent to one trained on HA masks, the latter being the current standard approach. Step 1: Screen captures of 19 publicly available radiologic images of assorted structures within various modalities were analyzed. ET and HA masks for all regions of interest (ROIs) were generated from these image datasets. Step 2: Utilizing a similar approach, ET and HA masks for 356 publicly available T1-weighted postcontrast meningioma images were generated. Three hundred six of these image + mask pairs were used to train a CNN with U-net-based architecture. The remaining 50 images were used as the independent test set. Step 1: ET and HA masks for the nonneurological images had an average Dice similarity coefficient (DSC) of 0.86 between each other. Step 2: Meningioma ET and HA masks had an average DSC of 0.85 between each other. After separate training using both approaches, the ET approach performed virtually identically to HA on the test set of 50 images. The former had an area under the curve (AUC) of 0.88, while the latter had AUC of 0.87. ET and HA predictions had trimmed mean DSCs compared to the original HA maps of 0.73 and 0.74, respectively. These trimmed DSCs between ET and HA were found to be statistically equivalent with a p value of 0.015. We have demonstrated that ET can create segmentation masks suitable for deep learning semantic segmentation. Future work will integrate ET to produce masks in a faster, more natural manner that distracts less from typical radiology clinical workflow. © 2019, The Author(s).","Artificial intelligence; Deep learning; Eye tracking; Meningioma; Segmentation","Artificial intelligence; Convolution; Eye tracking; Image segmentation; Medical imaging; Neural networks; Radiation; Radiology; Semantics; Area under the curves; Convolutional neural network; Eye tracking technologies; Health care application; Meningiomas; Regions of interest; Semantic segmentation; Similarity coefficients; Deep learning; computer assisted diagnosis; diagnostic imaging; eye movement; human; meningioma; meninx; nuclear magnetic resonance imaging; physiology; procedures; Deep Learning; Eye Movements; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Meningeal Neoplasms; Meninges; Meningioma; Neural Networks, Computer",Article,"Final","",Scopus,2-s2.0-85065245397
"Fasanmade A., Aliyu S., He Y., Al-Bayatti A.H., Sharif M.S., Alfakeeh A.S.","57215319726;56743380400;57197817050;27867501600;26428534500;57209777937;","Context-aware driver distraction severity classification using LSTM network",2019,"Proceedings - 2019 International Conference on Computing, Electronics and Communications Engineering, iCCECE 2019",,,"8941966","147","152",,1,"10.1109/iCCECE46942.2019.8941966","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078319267&doi=10.1109%2fiCCECE46942.2019.8941966&partnerID=40&md5=3458e6db0d3751e7a3faf9f0ce0c042a","Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom; School of Architecture Computing and Engineering, College of Arts, Technology and Innovation, UEL University Way, Dockland Campus, London, E16 2RD, United Kingdom","Fasanmade, A., Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom; Aliyu, S., Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom; He, Y., Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom; Al-Bayatti, A.H., Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom; Sharif, M.S., School of Architecture Computing and Engineering, College of Arts, Technology and Innovation, UEL University Way, Dockland Campus, London, E16 2RD, United Kingdom; Alfakeeh, A.S., Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom","Advanced Driving Assistance Systems (ADAS) has been a critical component in vehicles and vital to the safety of vehicle drivers and public road transportation systems. In this paper, we present a deep learning technique that classifies drivers' distraction behaviour using three contextual awareness parameters: speed, manoeuver and event type. Using a video coding taxonomy, we study drivers' distractions based on events information from Regions of Interest (RoI) such as hand gestures, facial orientation and eye gaze estimation. Furthermore, a novel probabilistic (Bayesian) model based on the Long short-term memory (LSTM) network is developed for classifying driver's distraction severity. This paper also proposes the use of frame-based contextual data from the multi-view TeleFOT naturalistic driving study (NDS) data monitoring to classify the severity of driver distractions. Our proposed methodology entails recurrent deep neural network layers trained to predict driver distraction severity from time series data. © 2019 IEEE.","Context awareness; Driver Distraction; Dynamic Bayesian networks (DBN); LSTM networks; Severity prediction; Time series","Advanced public transportation systems; Bayesian networks; Deep neural networks; Long short-term memory; Multilayer neural networks; Network layers; Time series; Video signal processing; Context- awareness; Driver distractions; Driving assistance systems; Dynamic Bayesian networks; Facial orientations; Learning techniques; Naturalistic driving studies; Regions of interest; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85078319267
"MacHado E., Carrillo I., Collado M., Chen L.","57204632417;57191370301;57209098497;35519432500;","Visual attention-based object detection in cluttered environments",2019,"Proceedings - 2019 IEEE SmartWorld, Ubiquitous Intelligence and Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Internet of People and Smart City Innovation, SmartWorld/UIC/ATC/SCALCOM/IOP/SCI 2019",,,"9060082","133","139",,2,"10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075151756&doi=10.1109%2fSmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00064&partnerID=40&md5=17d2071fd02fe6a1be3a77bfeeeda816","Department of Innovation and Technology, Ingenieria y Soluciones Informatica, Sevilla, Spain; School of Computer Science and Informatics, De Montfort University, Leicester, United Kingdom","MacHado, E., Department of Innovation and Technology, Ingenieria y Soluciones Informatica, Sevilla, Spain; Carrillo, I., Department of Innovation and Technology, Ingenieria y Soluciones Informatica, Sevilla, Spain, School of Computer Science and Informatics, De Montfort University, Leicester, United Kingdom; Collado, M., Department of Innovation and Technology, Ingenieria y Soluciones Informatica, Sevilla, Spain; Chen, L., School of Computer Science and Informatics, De Montfort University, Leicester, United Kingdom","The study of human visual attention is considered a hot topic in the field of activity recognition, experimental psychology research and human computer interaction. The importance of detecting user objects of interest in real time is critical to provide accurate cues about the user intentions.However, current methods for visual attention extraction and object detection suffer from low performance when moving to ongoing condition. Inherent complexity of cluttered environmentsis considered the major barrier to achieve good performances. To address this challenge, we present a novel method that includes head-worn eye tracker and egocentric video. Our method exploits sliding window-based time series approach in conjunction with aHeuristic probabilistic function to analyse user fixations around potential object of interest in an egocentric video. We evaluate the proposed method using a new dataset annotated with user gaze data and object within a frame image. Our experimental results show that our approach can outperforms several state-of-the-art commonality visual attention-based object detection methods. © 2019 IEEE.","Cluttered Environments; Deep Learning; Fixations; Object Detection; Visual Attention","Behavioral research; Eye tracking; Human computer interaction; Object recognition; Smart city; Trusted computing; Ubiquitous computing; Activity recognition; Cluttered environments; Human visual attention; Inherent complexity; Object detection method; Probabilistic functions; Sliding window-based; State of the art; Object detection",Conference Paper,"Final","",Scopus,2-s2.0-85075151756
"Alarifi J., Fry J., Dancey D., Yap M.H.","57194786999;36179593500;17343675200;15129176400;","Understanding face age estimation: Humans and machine",2019,"CITS 2019 - Proceeding of the 2019 International Conference on Computer, Information and Telecommunication Systems",,,"8862107","","",,3,"10.1109/CITS.2019.8862107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074167227&doi=10.1109%2fCITS.2019.8862107&partnerID=40&md5=c389c5d24d00624cf41006ebb6b56019","Faculty of Science and Engineering, Manchester Metropolitan University, Chester Street, M1 5GD, United Kingdom","Alarifi, J., Faculty of Science and Engineering, Manchester Metropolitan University, Chester Street, M1 5GD, United Kingdom; Fry, J., Faculty of Science and Engineering, Manchester Metropolitan University, Chester Street, M1 5GD, United Kingdom; Dancey, D., Faculty of Science and Engineering, Manchester Metropolitan University, Chester Street, M1 5GD, United Kingdom; Yap, M.H., Faculty of Science and Engineering, Manchester Metropolitan University, Chester Street, M1 5GD, United Kingdom","Face age estimation is an important part of many disciplines, including dermatology, cosmetology, and computer vision. Traditional age estimation studies focus on certain parts of the face to analyse its surface topology. With the advances of deep learning, many Convolutional Neural Networks (CNNs) now outperform traditional methods in the age estimation task. However, it is still not clear what type of features these networks learn when estimating age. This study aims to investigate which facial features are important, for humans and for CNNs, on the age estimation of women in five age groups. We then compare the heat-maps from the human eye gaze with those of the CNN. We consider two main research questions: (1) Which facial regions do humans look at when performing age estimation? (2) Do humans and machine focus on the same facial regions when estimating the age? We answered these questions by conducting two experiments. In the first experiment, we used an eye-tracking software to detect where on the face the gaze of human participants focused the most when they were asked to assign the person in the image to an age class. In the second experiment, we used transfer learning on the network pre-trained on ImageNet, then fine-tuned the network on a benchmark face age dataset to classify the same images shown to the participants. The heat-maps of the VGG16 network were then visualised using Gradient-based Class Activation Map (Grad-CAM). The results showed how our model was almost as accurate as humans in predicting the age of a person from a single image of their face (CNN: 60%; humans: 61%). The results also showed that people mainly look at the eyes and nose when predicting a person's age, while the features learned by the CNN included the eyes, the mouth, and the skin surface. © 2019 IEEE.",,"Classification (of information); Deep learning; Neural networks; Age estimation; Convolutional neural network; Facial feature; Facial regions; Gradient based; Research questions; Surface topology; Transfer learning; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85074167227
"Illavarason P., Arokia Renjit J., Mohan Kumar P.","57202709209;56578567800;57196053984;","Medical Diagnosis of Cerebral Palsy Rehabilitation Using Eye Images in Machine Learning Techniques",2019,"Journal of Medical Systems","43","8","278","","",,13,"10.1007/s10916-019-1410-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068776512&doi=10.1007%2fs10916-019-1410-6&partnerID=40&md5=3935c6c4db1204f71525bf50349ed3de","Faculty of Information and Communication Engineering, CEG, Anna University, Chennai, India; Department of CSE, Jeppiaar Engineering College, Chennai, India; Department of IT, Jeppiaar Engineering College, Chennai, India","Illavarason, P., Faculty of Information and Communication Engineering, CEG, Anna University, Chennai, India; Arokia Renjit, J., Department of CSE, Jeppiaar Engineering College, Chennai, India; Mohan Kumar, P., Department of IT, Jeppiaar Engineering College, Chennai, India","Cerebral Palsy (CP) is a non progressive neurological disorders commonly associated with a spectrum of developmental disabilities such as strabismus (misalignment of eye). The Eye image are captured through camera, this make the quick diagnosis and examination the periodical assessment for CP kids. By capturing the Eye Movement of 40 children with CP (aged 3–11 years) with relatively mild motor-impairment and also we have analyzed the performance of CP children periodically. Nowadays, Bio-Medical image processing and Machine learning Classification algorithm used for detection and diagnosis the certain diseases and plays the important tool to decrease the risk of any diseases. This work presents a computational methodology to automatically diagnose the Improvement of CP children and performance can be evaluated. The alternate medical evaluation techniques have shown their potential for the treatment and diagnosis of disease like strabismus and nystagmus for CP kids. The proposed method is used to measure and quantify the performance improvement by classify the abnormal eye condition of CP kids and these results attained by machine learning method. The results show the best classification accuracy of 94.17% calculated from Neural Network Classifier. Specificity Rate were absorbed as 0.9800 and Sensitivity Rate were absorbed as 0.9165 respectively. The proposed method for non-invasive and automatic detection of abnormalities in CP kids and evaluates the performance improvement more accurately. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Classification; CP kids; Image processing techniques; Improvement analyzed; Machine learning techniques","Article; artificial neural network; cerebral palsy; child; clinical article; convergent strabismus; diagnostic accuracy; diagnostic imaging; divergent strabismus; eye examination; eye movement; eye movement disorder; eye tracking; human; image processing; machine learning; medical assessment; motor dysfunction; motor performance; nystagmus; rehabilitation center; sensitivity and specificity; strabismus; vertical strabismus; algorithm; cerebral palsy; eye malformation; female; male; preschool child; Algorithms; Cerebral Palsy; Child; Child, Preschool; Eye Abnormalities; Female; Humans; Image Processing, Computer-Assisted; Machine Learning; Male; Neural Networks, Computer",Article,"Final","",Scopus,2-s2.0-85068776512
"Gao J., Zhang T., Xu C.","57190345334;55729040600;56153258200;","SMART: Joint Sampling and Regression for Visual Tracking",2019,"IEEE Transactions on Image Processing","28","8","8666049","3923","3935",,7,"10.1109/TIP.2019.2904434","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067825428&doi=10.1109%2fTIP.2019.2904434&partnerID=40&md5=e4ce5ff2d291e4cc63b98939e38db175","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciencesxs, Beijing, 100190, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 518055, China; Peng Cheng Laboratory, Shenzhen, 518055, China","Gao, J., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciencesxs, Beijing, 100190, China, School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 518055, China, Peng Cheng Laboratory, Shenzhen, 518055, China; Zhang, T., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciencesxs, Beijing, 100190, China, School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 518055, China, Peng Cheng Laboratory, Shenzhen, 518055, China; Xu, C., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciencesxs, Beijing, 100190, China, School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 518055, China, Peng Cheng Laboratory, Shenzhen, 518055, China","Most existing trackers are either sampling-based or regression-based methods. Sampling-based methods estimate the target state by sampling many target candidates. Although these methods achieve significant performance, they often suffer from a high computational burden. Regression-based methods often learn a computationally efficient regression function to directly predict the geometric distortion between frames. However, most of these methods require large-scale external training videos and are still not very impressive in terms of accuracy. To make both types of methods enhance and complement each other, in this paper, we propose a joint sampling and regression scheme for visual tracking, which leverages the region proposal network by a novel design. Specifically, our method can jointly exploit discriminative target proposal generation and structural target regression to predict target location in a simple feedforward propagation. We evaluate the proposed method on five challenging benchmarks, and extensive experimental results demonstrate that our method performs favorably compared with state-of-the-art trackers with respect to both accuracy and speed. © 1992-2012 IEEE.","deep learning; sampling and regression; Visual tracking","Deep learning; Computational burden; Computationally efficient; Geometric distortion; Regression function; Sampling-based method; State of the art; Structural target; Visual Tracking; Regression analysis; article; eye tracking; joint; sampling; velocity; videorecording",Article,"Final","",Scopus,2-s2.0-85067825428
"Chaplin V., Phipps M.A., Jonathan S.V., Grissom W.A., Yang P.F., Chen L.M., Caskey C.F.","35221173300;57202249550;36628486000;14422072600;56258845300;57206671672;10044625700;","On the accuracy of optically tracked transducers for image-guided transcranial ultrasound",2019,"International Journal of Computer Assisted Radiology and Surgery","14","8",,"1317","1327",,8,"10.1007/s11548-019-01988-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065527668&doi=10.1007%2fs11548-019-01988-0&partnerID=40&md5=afcd685a126dc4315b877086ceec974d","Department of Radiology and Radiological Sciences, Institute of Imaging Science, Vanderbilt University Medical Center, AA 1105 MCN, 1161 21st Ave. S, Nashville, TN  TN 37232, United States; Biomedical Engineering, Vanderbilt University, Nashville, TN, United States","Chaplin, V., Department of Radiology and Radiological Sciences, Institute of Imaging Science, Vanderbilt University Medical Center, AA 1105 MCN, 1161 21st Ave. S, Nashville, TN  TN 37232, United States; Phipps, M.A., Department of Radiology and Radiological Sciences, Institute of Imaging Science, Vanderbilt University Medical Center, AA 1105 MCN, 1161 21st Ave. S, Nashville, TN  TN 37232, United States; Jonathan, S.V., Biomedical Engineering, Vanderbilt University, Nashville, TN, United States; Grissom, W.A., Department of Radiology and Radiological Sciences, Institute of Imaging Science, Vanderbilt University Medical Center, AA 1105 MCN, 1161 21st Ave. S, Nashville, TN  TN 37232, United States, Biomedical Engineering, Vanderbilt University, Nashville, TN, United States; Yang, P.F., Department of Radiology and Radiological Sciences, Institute of Imaging Science, Vanderbilt University Medical Center, AA 1105 MCN, 1161 21st Ave. S, Nashville, TN  TN 37232, United States; Chen, L.M., Department of Radiology and Radiological Sciences, Institute of Imaging Science, Vanderbilt University Medical Center, AA 1105 MCN, 1161 21st Ave. S, Nashville, TN  TN 37232, United States; Caskey, C.F., Department of Radiology and Radiological Sciences, Institute of Imaging Science, Vanderbilt University Medical Center, AA 1105 MCN, 1161 21st Ave. S, Nashville, TN  TN 37232, United States, Biomedical Engineering, Vanderbilt University, Nashville, TN, United States","Purpose: Transcranial focused ultrasound (FUS) is increasingly being explored to modulate neuronal activity. To target neuromodulation, researchers often localize the FUS beam onto the brain region(s) of interest using spatially tracked tools overlaid on pre-acquired images. Here, we quantify the accuracy of optically tracked image-guided FUS with magnetic resonance imaging (MRI) thermometry, evaluate sources of error and demonstrate feasibility of these procedures to target the macaque somatosensory region. Methods: We developed an optically tracked FUS system capable of projecting the transducer focus onto a pre-acquired MRI volume. To measure the target registration error (TRE), we aimed the transducer focus at a desired target in a phantom under image guidance, heated the target while imaging with MR thermometry and then calculated the TRE as the difference between the targeted and heated locations. Multiple targets were measured using either an unbiased or bias-corrected calibration. We then targeted the macaque S1 brain region, where displacement induced by the acoustic radiation force was measured using MR acoustic radiation force imaging (MR-ARFI). Results: All calibration methods enabled registration with TRE on the order of 3 mm. Unbiased calibration resulted in an average TRE of 3.26 mm (min–max: 2.80–4.53 mm), which was not significantly changed by prospective bias correction (TRE of 3.05 mm; 2.06–3.81 mm, p = 0.55). Restricting motion between the transducer and target and increasing the distance between tracked markers reduced the TRE to 2.43 mm (min–max: 0.79–3.88 mm). MR-ARFI images showed qualitatively similar shape and extent as projected beam profiles in a living non-human primate. Conclusions: Our study describes methods for image guidance of FUS neuromodulation and quantifies errors associated with this method in a large animal. The workflow is efficient enough for in vivo use, and we demonstrate transcranial MR-ARFI in vivo in macaques for the first time. © 2019, CARS.","Focused ultrasound; Image-guided therapy; Neuromodulation; Optical tracking; Ultrasound neuromodulation","adult; animal experiment; animal tissue; Article; brain region; calibration; controlled study; experimental design; eye tracking; Macaca fascicularis; magnetic field; male; measurement accuracy; measurement error; nonhuman; nuclear magnetic resonance imaging; priority journal; somatosensory cortex; thermometry; transcranial Doppler ultrasonography; transcranial focused ultrasound; animal; brain; diagnostic imaging; echography; equipment design; imaging phantom; Macaca; motion; neuroimaging; nuclear magnetic resonance imaging; optics; physiology; prospective study; reproducibility; transducer; Animals; Brain; Calibration; Equipment Design; Macaca; Magnetic Resonance Imaging; Male; Motion; Neuroimaging; Optics and Photonics; Phantoms, Imaging; Prospective Studies; Reproducibility of Results; Thermometry; Transducers; Ultrasonography",Article,"Final","",Scopus,2-s2.0-85065527668
"Zhou J., Wang L., Yin H., Bovik A.C.","56203158200;56941061800;7403114004;56984291600;","Eye movements and visual discomfort when viewing stereoscopic 3D content",2019,"Digital Signal Processing: A Review Journal","91",,,"41","53",,3,"10.1016/j.dsp.2018.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059198711&doi=10.1016%2fj.dsp.2018.12.008&partnerID=40&md5=8016e2522bbacbc383a63c7a0eb2f8d6","Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai, China; Shanghai Key Laboratory of Digital Media Processing and Transmission, China; Shanghai Media and Entertainment Technology (Group) Co. Ltd., Shanghai, China; Hangzhou Dianzi University, Hangzhou, China; The University of Texas at Austin, Austin, United States","Zhou, J., Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai, China, Shanghai Key Laboratory of Digital Media Processing and Transmission, China; Wang, L., Shanghai Media and Entertainment Technology (Group) Co. Ltd., Shanghai, China; Yin, H., Hangzhou Dianzi University, Hangzhou, China; Bovik, A.C., The University of Texas at Austin, Austin, United States","The visual brain fuses the left and right images projected onto the two eyes from a stereoscopic 3D (S3D) display, perceives parallax, and rebuilds a sense of depth. In this process, the eyes adjust vergence and accommodation to adapt to the depths and parallax of the points they gazed at. Conflicts between accommodation and vergence when viewing S3D content potentially lead to visual discomfort. A variety of approaches have been taken towards understanding the perceptual bases of discomfort felt when viewing S3D, including extreme disparities or disparity gradients, negative disparities, dichoptic presentations, and so on. However less effort has been applied towards understanding the role of eye movements as they relate to visual discomfort when viewing S3D. To study eye movements in the context of S3D viewing discomfort, a Shifted-S3D-Image-Database (SSID) is constructed using 11 original nature scene S3D images and their 6 shifted versions. We conducted eye-tracking experiments on humans viewing S3D images in SSID while simultaneously collecting their judgments of experienced visual discomfort. From the collected eye-tracking data, regions of interest (ROIs) were extracted by kernel density estimation using the fixation data, and an empirical formula fitted between the disparities of salient objects marked by the ROIs and the mean opinion scores (MOS). Finally, eye-tracking data was used to analyze the eye movement characteristics related to S3D image quality. Fifteen eye movement features were extracted, and a visual discomfort predication model learned using a support vector regressor (SVR). By analyzing the correlations between features and MOS, we conclude that angular disparity features have a strong correlation with human judgments of discomfort. © 2018 Elsevier Inc.","Eye movement features; Eye-tracking; Stereoscopic 3D; Visual discomfort","Eye tracking; Geometrical optics; Object tracking; Stereo image processing; Empirical formulas; Kernel Density Estimation; Mean opinion scores; Movement characteristics; Regions of interest; Strong correlation; Support vector regressor; Visual discomfort; Eye movements",Article,"Final","",Scopus,2-s2.0-85059198711
"Susan S., Agarwal A., Gulati C., Singh S., Chauhan V.","26423246100;57213411800;57212409209;57212408746;57212409648;","Human attention span modeling using 2D visualization plots for gaze progression and gaze sustenance",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"42","46",,1,"10.1145/3348488.3348494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076592210&doi=10.1145%2f3348488.3348494&partnerID=40&md5=b8117ea64a8c5ce7c81dee9067f6dfb8","Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India","Susan, S., Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India; Agarwal, A., Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India; Gulati, C., Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India; Singh, S., Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India; Chauhan, V., Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India","This paper presents a novel perspective on human attention span modeling based on gaze estimation from head pose data extracted from videos. This is achieved by devising specialized 2D visualization plots that capture gaze progression and gaze sustenance over time. In doing so, a low-resolution analysis is assumed, as is the case with most crowd surveillance videos wherein the retinal analysis and iris pattern extraction of individual subjects is made impossible. The information is useful for studies involving the random gaze behavior pattern of humans in a crowded place, or in a controlled environment in seminars or office meetings. The extraction of useful information regarding the attention span of the individual from the spatial and temporal analysis of gaze points is the subject of study in this paper. Different solutions ranging from plotting temporal gaze plots to sustained attention span graphs are investigated, and the results are compared with the existing techniques of attention span modeling and visualization. © 2019 Association for Computing Machinery.","Attention span modeling; Gaze analysis; Head pose estimation","Extraction; Security systems; Virtual reality; Visualization; 2-D visualizations; Controlled environment; Crowd surveillance; Gaze analysis; Gaze estimation; Head Pose Estimation; Spatial and temporal analysis; Sustained attention; Artificial intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85076592210
"Xin M., Zheng J., Li B., Niu G., Zhang M.","24402992600;50662210800;56092633500;57191197891;24402985200;","Real-time object tracking via self-adaptive appearance modeling",2019,"Neurocomputing","349",,,"21","30",,2,"10.1016/j.neucom.2019.04.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064437616&doi=10.1016%2fj.neucom.2019.04.024&partnerID=40&md5=298f03f9a7611795a30d164fd17c4ecd","Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; School of Computer and Information Engineering, Henan University, Kaifeng, 475001, China","Xin, M., Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China, School of Computer and Information Engineering, Henan University, Kaifeng, 475001, China; Zheng, J., Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; Li, B., Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; Niu, G., Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; Zhang, M., School of Computer and Information Engineering, Henan University, Kaifeng, 475001, China","One of the main factors that limit the accuracy and robustness of visual tracking algorithms is the lack of suitable appearance models. The robustness and effectiveness of object appearance models is severely affected by the changing object appearances during the tracking process and the interference of other similar objects around the truth object. In this paper, a self-adaptive appearance model pool based on multi-sample is constructed to improve the robustness of the object appearance models. In order to deal with variable object states, the initial sample given in the first frame and the samples generated in the subsequent tracking process are combined into a sample set to represent various appearances of the object. In addition, a dynamic selection strategy is explored to update and maintain the sample components that are derived from varieties of sources. In order to distinguish the tracking object from other similar candidate objects, multi-feature response fusion strategy is proposed, which can effectively improve the expression ability of the appearance model. Extensive experiments on the popular benchmark datasets demonstrate that the proposed tracking approach performs favorably against several other state-of-the-art tracking algorithms. © 2019","Adaptive features fusion; Dynamic samples selection; Self-adaptive appearance modeling; Visual tracking","Computer applications; Neural networks; Adaptive appearance models; Adaptive features; Dynamic selection strategies; Real-time object tracking; Samples selection; Tracking approaches; Visual Tracking; Visual tracking algorithm; Tracking (position); article; eye tracking",Article,"Final","",Scopus,2-s2.0-85064437616
"Larrazabal A.J., Cena C.E.G., Martinez C.E.","57208260242;36633947900;8118083200;","Eye corners tracking for head movement estimation",2019,"IWOBI 2019 - IEEE International Work Conference on Bioinspired Intelligence, Proceedings",,,"9114393","53","58",,,"10.1109/IWOBI47054.2019.9114393","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087281347&doi=10.1109%2fIWOBI47054.2019.9114393&partnerID=40&md5=432da96c352c1d81e0feffec635159f5","Research Institute for Signals Systems and Computational Intelligence Sinc(i) FICH-UNL/CONICET, Santa Fe, 3000, Argentina; Centre for Robotics and Automation UPM-CSIC, Madrid, 28006, Spain","Larrazabal, A.J., Research Institute for Signals Systems and Computational Intelligence Sinc(i) FICH-UNL/CONICET, Santa Fe, 3000, Argentina; Cena, C.E.G., Centre for Robotics and Automation UPM-CSIC, Madrid, 28006, Spain; Martinez, C.E., Research Institute for Signals Systems and Computational Intelligence Sinc(i) FICH-UNL/CONICET, Santa Fe, 3000, Argentina","Recently, video-oculographic gaze tracking has begun to be used in the diagnosis of a wide variety of neurological diseases, such as Parkinson and Alzheimer. For this application, the so-called feature-based methods are used, more precisely, 2D regression-based methods. They use geometrically derived eye features from high-resolution eye images captured by zooming into the user's eyes. The main weakness of these methods is that the head of the user must remain motionless to avoid estimation errors. In some patients, some involuntary movements cannot be avoided and it is necessary to measure them. In this paper, we tackle the measurement of head position as a way to improve the gaze tracking on these precision demanding medical applications. As a first stage, we propose to obtain the eye corners coordinates as a reference point, since they are the most stable points in front of the eyeball and eyelids movements. The problem was handled as a regression problem using a coarse-to-fine cascaded convolutional neural network in order to accurately regress the coordinates of the eye corner. Particularly, with the aim of achieving high precision we cascade two levels of convolutional networks. Finally, we added temporal information to increase accuracy and decrease computation time. The accuracy of the estimation was calculated from the mean square error between the predictions and the ground truth. Subjective performance was also evaluated through video inspection. In both cases, satisfactory results were obtained. © 2019 IEEE.","Convolutional Neural Networks; Head Movements; Landmark Tracking","Convolution; Convolutional neural networks; Diagnosis; Eye movements; Mean square error; Medical applications; Motion estimation; Convolutional networks; Estimation errors; Feature-based method; Involuntary movements; Neurological disease; Regression problem; Subjective performance; Temporal information; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85087281347
"Wang C.-C., Wang S.-C., Chu C.-P.","7501632228;57212476915;57215423018;","Combining Virtual Reality Advertising and Eye Tracking to Understand Visual Attention: A Pilot Study",2019,"Proceedings - 2019 8th International Congress on Advanced Applied Informatics, IIAI-AAI 2019",,,"8992734","160","165",,,"10.1109/IIAI-AAI.2019.00041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080964224&doi=10.1109%2fIIAI-AAI.2019.00041&partnerID=40&md5=4a73728a72a7793ad9291c44dfe4a49e","School of Information and Design Chang, Jung Christian University, Tainan, Taiwan; Department of Computer Science and Information Engineering, Chang Jung Christian University, Tainan, Taiwan; School of Information and Design, Chang Jung Christian University, Tainan, Taiwan","Wang, C.-C., School of Information and Design Chang, Jung Christian University, Tainan, Taiwan; Wang, S.-C., Department of Computer Science and Information Engineering, Chang Jung Christian University, Tainan, Taiwan; Chu, C.-P., School of Information and Design, Chang Jung Christian University, Tainan, Taiwan","This paper aims to design a virtual reality eye tracker for the analysis of dynamic behavior of viewers when watching a virtual reality (VR) commercial advertising. Seven experimental participants were recruited for the pilot study. Experiment contents included a one-minute VR commercial advertising designed with Unity 3D development tool and through integration into eye tracking device of a head-mounted display (HMD), eye movements of participants were recorded. The finding results of the paper showed that: 1) when watching the VR commercial advertising, participants did not put the first sequence of gaze in each dynamic regions of interest (ROIs) in the commercial object; 2) in terms for concentration span sequence of gaze in each dynamic ROIs, if participant gazed the commercial object, more concerns were put and there was longer time for gazing; on the contrary, there was no eye movement information about gazing at the commercial object; 3) when watching the VR commercial advertising, the commercial object did not receive longer total gazing time and numbers of relative concentration gazing of participants spending on all dynamic ROIs. According to experimental results, we concluded that the VR 360-degree VR film affects the expected outcome in the operation of eye tracking technology. © 2019 IEEE.","dynamic region of interest; eye tracking; video advertising; visual attention","Behavioral research; Eye movements; Helmet mounted displays; Image segmentation; Marketing; Virtual reality; Development tools; Dynamic region; Eye tracking devices; Eye tracking technologies; Head mounted displays; Relative concentration; Video advertisings; Visual Attention; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85080964224
"Pichitwong W., Chamnongthai K.","57191333091;57202765861;","Obscured 3D point-of-gaze estimation by multipoint cloud data",2019,"Proceedings of the 16th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2019",,,"8955244","947","950",,,"10.1109/ECTI-CON47248.2019.8955244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078850350&doi=10.1109%2fECTI-CON47248.2019.8955244&partnerID=40&md5=c968b90b3d53e2893877c20a16c4bf82","King Mongkut's University of Technology Thonburi, Dept. Electronic and Telecommunication Engineering, Bangkok, Thailand","Pichitwong, W., King Mongkut's University of Technology Thonburi, Dept. Electronic and Telecommunication Engineering, Bangkok, Thailand; Chamnongthai, K., King Mongkut's University of Technology Thonburi, Dept. Electronic and Telecommunication Engineering, Bangkok, Thailand","Point cloud sensor is currently used to sense point cloud data information which is 3D position on the surface of target object. Point cloud data information can be input to improve for 3D point of gaze estimation (3D POG). Presently, there are limitation on creating point cloud data information on target object since point cloud data cannot be found if any obstacle in front of the sensor, there are shadow projection. This paper proposes method of multipoint cloud data to create point cloud data on the surface of target object and obscured point cloud data in the shadow projection. Eye tracker sensor provides 3D eyes position data and 2D POG on screen data which each origin represents center of eye tracker and center of screen respectively. These mentioned data are integrated by model fitting to draw a straight line, originating from the center point between left pupil and right pupil, which passes through the2D POG on virtual screen and ends when the line meets the closest point on the target object. In performance evaluation of proposed method, firstly the obscured point cloud data are successfully defined. Secondly, experiment by 4 participants by watching 9 units of testing objects at 2 seconds in free move provide the result of 3D POG estimation at average distance errors by 1.09 cm. © 2019 IEEE.","3-D gaze estimation; Multipoint cloud data","Computer science; Computers; Electrical engineering; Mathematical techniques; Average Distance; Center points; Cloud data; Gaze estimation; Model fitting; Point cloud data; Point of gaze; Target object; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85078850350
"Tamuly S., Jyotsna C., Amudha J.","57214072113;57193578958;35766448700;","Tracking Eye Movements to Predict the Valence of A Scene",2019,"2019 10th International Conference on Computing, Communication and Networking Technologies, ICCCNT 2019",,,"8944564","","",,1,"10.1109/ICCCNT45670.2019.8944564","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078131619&doi=10.1109%2fICCCNT45670.2019.8944564&partnerID=40&md5=f55c2dcf4a321e97456a2aa99744f823","Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Dept. of Computer Science and Engineering, Bengaluru, India","Tamuly, S., Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Dept. of Computer Science and Engineering, Bengaluru, India; Jyotsna, C., Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Dept. of Computer Science and Engineering, Bengaluru, India; Amudha, J., Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Dept. of Computer Science and Engineering, Bengaluru, India","Studying human bio signals such as eye movements and tracking them can help in identifying and classifying the emotional essence of a scene. The existing methods employed to evaluate the reaction of the eyes based on exposure to a scene or image often use a classifier to extract features from eye movements. These extracted features are then evaluated to determine the valence of a scene. On the contrary, as much as eye movement has proved to be a reliable source in scene or image detection, factors such as how each feature affects the outcome of the prediction have not been explored. For the determination of the emotional category of images using eye movements, images are categorized into pleasant, neutral and unpleasant images and then these images are shown to the test subjects to record their response. Features of eye movement like fixation count, fixation frequency, saccade count, and saccade frequency among others, along with a machine learning approach was used for scene classification. © 2019 IEEE.","Eye tracking; Fixation frequency; Image classification; Saccade frequency; Scene valence","Eye tracking; Image classification; Biosignals; Image detection; Machine learning approaches; Scene classification; Scene valence; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85078131619
"Elbattah M., Carette R., Dequen G., Guerin J.-L., Cilia F.","57163770900;57200860085;23396657900;57200857001;57200855464;","Learning Clusters in Autism Spectrum Disorder: Image-Based Clustering of Eye-Tracking Scanpaths with Deep Autoencoder",2019,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,,"8856904","1417","1420",,5,"10.1109/EMBC.2019.8856904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077873529&doi=10.1109%2fEMBC.2019.8856904&partnerID=40&md5=019b4e524347886403186ca0b1956b87","Laboratoire MIS, Université de Picardie Jules Verne, France","Elbattah, M., Laboratoire MIS, Université de Picardie Jules Verne, France; Carette, R., Laboratoire MIS, Université de Picardie Jules Verne, France; Dequen, G., Laboratoire MIS, Université de Picardie Jules Verne, France; Guerin, J.-L., Laboratoire MIS, Université de Picardie Jules Verne, France; Cilia, F., Laboratoire MIS, Université de Picardie Jules Verne, France","Autism spectrum disorder (ASD) is a lifelong condition characterized by social and communication impairments. This study attempts to apply unsupervised Machine Learning to discover clusters in ASD. The key idea is to learn clusters based on the visual representation of eye-tracking scanpaths. The clustering model was trained using compressed representations learned by a deep autoencoder. Our experimental results demonstrate a promising tendency of clustering structure. Further, the clusters are explored to provide interesting insights into the characteristics of the gaze behavior involved in autism. © 2019 IEEE.","Autism Spectrum Disorder; autoencoder; Clustering; Eye-Tracking; Machine Learning; Scanpath","Deep learning; Diseases; Learning systems; Autism spectrum disorders; Auto encoders; Clustering; Clustering model; Gaze behavior; Scan path; Unsupervised machine learning; Visual representations; Eye tracking; autism; cluster analysis; human; unsupervised machine learning; Autism Spectrum Disorder; Cluster Analysis; Humans; Unsupervised Machine Learning",Conference Paper,"Final","",Scopus,2-s2.0-85077873529
"Jiang M., Francis S.M., Srishyla D., Conelea C., Zhao Q., Jacob S.","56027704500;55979295000;57213596359;13402971200;55743334300;7202574761;","Classifying Individuals with ASD Through Facial Emotion Recognition and Eye-Tracking",2019,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,,"8857005","6063","6068",,11,"10.1109/EMBC.2019.8857005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077838673&doi=10.1109%2fEMBC.2019.8857005&partnerID=40&md5=a339f3911dc652c353ad1b28844a793b","Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, United States; Department of Psychiatry and Behavioral Sciences, University of Minnesota, Minneapolis, MN, United States","Jiang, M., Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, United States; Francis, S.M., Department of Psychiatry and Behavioral Sciences, University of Minnesota, Minneapolis, MN, United States; Srishyla, D., Department of Psychiatry and Behavioral Sciences, University of Minnesota, Minneapolis, MN, United States; Conelea, C., Department of Psychiatry and Behavioral Sciences, University of Minnesota, Minneapolis, MN, United States; Zhao, Q., Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, United States; Jacob, S., Department of Psychiatry and Behavioral Sciences, University of Minnesota, Minneapolis, MN, United States","Individuals with Autism Spectrum Disorder (ASD) have been shown to have atypical scanning patterns during face and emotion perception. While previous studies characterized ASD using eye-tracking data, this study examined whether the use of eye movements combined with task performance in facial emotion recognition could be helpful to identify individuals with ASD. We tested 23 subjects with ASD and 35 controls using a Dynamic Affect Recognition Evaluation (DARE) task that requires an individual to recognize one of six emotions (i.e., anger, disgust, fear, happiness, sadness, and surprise) while observing a slowly transitioning face video. We observed differences in response time and eye movements, but not in the recognition accuracy. Based on these observations, we proposed a machine learning method to distinguish between individuals with ASD and typically developing (TD) controls. The proposed method classifies eye fixations based on a comprehensive set of features that integrate task performance, gaze information, and face features extracted using a deep neural network. It achieved an 86% classification accuracy that is comparable with the standardized diagnostic scales, with advantages of efficiency and objectiveness. Feature visualization and interpretations were further carried out to reveal distinguishing features between the two subject groups and to understand the social and attentional deficits in ASD. © 2019 IEEE.",,"Behavioral research; Deep neural networks; Diagnosis; Eye movements; Eye tracking; Learning systems; Speech recognition; Turing machines; Affect recognition; Autism spectrum disorders; Classification accuracy; Eye fixations; Facial emotions; Machine learning methods; Recognition accuracy; Task performance; Face recognition; autism; emotion; face; facial expression; facial recognition; human; Autism Spectrum Disorder; Emotions; Face; Facial Expression; Facial Recognition; Humans",Conference Paper,"Final","",Scopus,2-s2.0-85077838673
"Su H., Hou Z., Huan J., Yan K., Ding H.","57215334520;14029905400;36559243500;57215335044;57215336179;","A multi-modal gaze tracking algorithm",2019,"Proceedings - 2019 IEEE International Congress on Cybermatics: 12th IEEE International Conference on Internet of Things, 15th IEEE International Conference on Green Computing and Communications, 12th IEEE International Conference on Cyber, Physical and Social Computing and 5th IEEE International Conference on Smart Data, iThings/GreenCom/CPSCom/SmartData 2019",,,"8875311","655","660",,,"10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074827466&doi=10.1109%2fiThings%2fGreenCom%2fCPSCom%2fSmartData.2019.00126&partnerID=40&md5=b0337a00a36cee66a51e124fd0c6285e","Changzhou University, Jiangsu Province, China; Jiangsu Province Networking and Mobile Internet Technology Engineering Key Laboratory, Jiangsu Province, China","Su, H., Changzhou University, Jiangsu Province, China; Hou, Z., Changzhou University, Jiangsu Province, China, Jiangsu Province Networking and Mobile Internet Technology Engineering Key Laboratory, Jiangsu Province, China; Huan, J., Changzhou University, Jiangsu Province, China; Yan, K., Changzhou University, Jiangsu Province, China; Ding, H., Changzhou University, Jiangsu Province, China","Gaze tracking is an assistant system of human-computer interaction. Aiming at the problem of high misjudgment rate and long time-consuming of traditional iris location methods, this paper proposes a gaze tracking method based on human eye geometric characteristics to improve the tracking accuracy in 2D environment. Firstly, the human face is located by face location algorithm and the position of human eye is estimated roughly. Then the iris template is built by iris image, and the iris center location algorithm is used to locate the iris center position. Finally, the eyes corners and iris center points are extracted to locate the eye area accurately and obtain the binocular image. The binocular images are input into the feature extraction network as multi-modal information in parallel, and the convoluted feature channels are reconstructed using the weight redistribution module in the network. Then the reconstructed features are fused in the full connection layer. Finally, the output layer is used to classify the reconstructed features. Experiments were carried out on a self-built screen block data set. For 12 classified data, the lowest recognition error rate is 5.34%. © 2019 IEEE.","Gaze tracking; Geometric Characteristics; Multi-modal","Binoculars; Green computing; Human computer interaction; Image processing; Internet of things; Location; Center locations; Face location; Gaze tracking; Geometric characteristics; Multi-modal; Multi-modal information; Recognition error; Tracking accuracy; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85074827466
"Dubey N., Ghosh S., Dhall A.","57211276941;57202710986;35229206900;","Unsupervised Learning of Eye Gaze Representation from the Web",2019,"Proceedings of the International Joint Conference on Neural Networks","2019-July",,"8851961","","",,1,"10.1109/IJCNN.2019.8851961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073227265&doi=10.1109%2fIJCNN.2019.8851961&partnerID=40&md5=ac0f8409519137c45cfc460373049467","Learning Affect and Semantic Image AnalysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India","Dubey, N., Learning Affect and Semantic Image AnalysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India; Ghosh, S., Learning Affect and Semantic Image AnalysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India; Dhall, A., Learning Affect and Semantic Image AnalysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India","Automatic eye gaze estimation has interested researchers for a while now. In this paper, we propose an unsupervised learning based method for estimating the eye gaze region. To train the proposed network ""Ize-Net"" in self-supervised manner, we collect a large 'in the wild' dataset containing 1,54,251 images from the web. For the images in the database, we divide the gaze into three regions based on an automatic technique based on pupil-centers localization and then use a feature-based technique to determine the gaze region. The performance is evaluated on the Tablet Gaze and CAVE datasets by fine-tuning results of Ize-Net for the task of eye gaze estimation. The feature representation learned is also used to train traditional machine learning algorithms for eye gaze estimation. The results demonstrate that the proposed method learns a rich data representation, which can be efficiently finetuned for any eye gaze estimation dataset. © 2019 IEEE.",,"Large dataset; Learning algorithms; Unsupervised learning; Automatic technique; Data representations; Eye-gaze; Feature representation; Feature-based techniques; Fine tuning; Pupil centers; Machine learning",Conference Paper,"Final","",Scopus,2-s2.0-85073227265
"Wang J., Fu E.Y., Ngai G., Leong H.V.","57202161640;57189356098;8915594400;7005127948;","Investigating differences in gaze and typing behavior across age groups and writing genres",2019,"Proceedings - International Computer Software and Applications Conference","1",,"8754370","622","629",,1,"10.1109/COMPSAC.2019.00095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072710379&doi=10.1109%2fCOMPSAC.2019.00095&partnerID=40&md5=402ad122aa0fee4a414418dc0ae33c23","Department of Computing, Hong Kong Polytechnic University Hong Kong, Hong Kong","Wang, J., Department of Computing, Hong Kong Polytechnic University Hong Kong, Hong Kong; Fu, E.Y., Department of Computing, Hong Kong Polytechnic University Hong Kong, Hong Kong; Ngai, G., Department of Computing, Hong Kong Polytechnic University Hong Kong, Hong Kong; Leong, H.V., Department of Computing, Hong Kong Polytechnic University Hong Kong, Hong Kong","Typing is one of the most common activities that are undertaken on a computer. It would therefore be interesting to investigate whether it is possible to deduce characteristics of the user, such as their age or the type of the document that they are writing, just simply from typing dynamics. In this paper, we study the coordination between eye gaze and typing dynamics, or the gaze-typing behavior, of subjects who are producing original text. We focus upon the differences between different age groups (children vs elderly seniors) and different genres of writing (reminiscent, logical and creative). Using machine-learning, we achieve an accuracy of 93.5% for age detection and 61.1% for the article-category detection, using a leave-one-subject-out cross-validation evaluation, which is 44% and 28% higher than baselines. © 2019 IEEE.","Eye-gaze behavior; Eye-hand coordination; Human-computer interaction; Typing behavior","Application programs; Age groups; Cross validation; Eye-gaze; Eye-hand coordination; Typing behaviors; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85072710379
"Ali A., Alsufyani N., Hoque S., Deravi F.","57198724269;57198352588;8360937200;6603854917;","Gaze-based presentation attack detection for users wearing tinted glasses",2019,"2019 8th International Conference on Emerging Security Technologies, EST 2019",,,"8806201","","",,2,"10.1109/EST.2019.8806201","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072036037&doi=10.1109%2fEST.2019.8806201&partnerID=40&md5=0ef5b7ffbe8ea7888b7816ce6f3ce666","School of Engineering and Digital Arts, University of Kent, Canterbury, United Kingdom","Ali, A., School of Engineering and Digital Arts, University of Kent, Canterbury, United Kingdom; Alsufyani, N., School of Engineering and Digital Arts, University of Kent, Canterbury, United Kingdom; Hoque, S., School of Engineering and Digital Arts, University of Kent, Canterbury, United Kingdom; Deravi, F., School of Engineering and Digital Arts, University of Kent, Canterbury, United Kingdom","Biometric authentication is vulnerable to presentation (spoofing) attacks. It is important to address the security vulnerability of spoofing attacks where an attacker uses an artefact presented at the sensor to subvert the system. Gaze-tracking has been proposed for such attack detection. In this paper, we explore the sensitivity of a gaze-based approach to spoofing detection in the presence of eye-glasses that may impact detection performance. In particular, we investigate the use of partially tinted glasses such as may be used in hazardous environments or outdoors in mobile application scenarios The attack scenarios considered in this work include the use of projected photos, 2D and 3D masks. A gaze-based spoofing detection system has been extensively evaluated using data captured from volunteers performing genuine attempts (with and without wearing such tinted glasses) as well as spoofing attempts using various artefacts. The results of the evaluations indicate that the presence of tinted glasses has a small impact on the accuracy of attack detection, thereby making the use of such gaze-based features possible for a wider range of applications. © 2019 IEEE.","biometrics; challenge-response technique; gaze tracking; liveness; mobile security; spoofing","Biometrics; Glass; Mobile security; Wear of materials; Biometric authentication; Challenge response techniques; Gaze tracking; Hazardous environment; Liveness; Mobile applications; Security vulnerabilities; spoofing; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85072036037
"Xu W., Cheung S.-C.","36167812500;34869344500;","Fully automatic photorealistic facial expression and eye gaze transfer with a single image",2019,"Proceedings - 2019 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2019",,,"8794910","519","524",,,"10.1109/ICMEW.2019.00095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071493581&doi=10.1109%2fICMEW.2019.00095&partnerID=40&md5=25ab3009d41d22d68674dd9268be257a","University of Kentucky, Lexington, KY, United States","Xu, W., University of Kentucky, Lexington, KY, United States; Cheung, S.-C., University of Kentucky, Lexington, KY, United States","Facial expression transfer has many applications in animation, special effects, and social media. Due to the variability in pose and environmental condition, generating a photo-realistic facial expression with a single image under uncon-trolled environment is a challenging problem. In this paper, we present a novel approach to simultaneously transfer both the facial expression and the eye gaze of an actor from one image to another. Based on the estimated facial geometry, the key idea of our work is to use mesh deformation to manipulate the expression of the actor in the target image. We start the process by reconstructing 3D facial geometry from un-constrained 2D images using a two-stage optimization strategy. A non-rigid registration is then used to find correspon-dences and alignment between reconstructed models, while eye gaze transfer is accomplished by piecewise affine warping. Finally we re-render the manipulated image with the use of Poisson image blending. We demonstrate the effectiveness of our method on images with various lighting conditions, head poses, and facial expressions. © 2019 IEEE.","3D reconstruc tion; Face expression transfer; Poisson image editing","3D reconstruc tion; Environmental conditions; Face expressions; Facial Expressions; Lighting conditions; Nonrigid registration; Poisson image editing; Two stage optimizations; Constrained optimization",Conference Paper,"Final","",Scopus,2-s2.0-85071493581
"Zhou X., Lin J., Jiang J., Chen S.","55743240400;57210574380;57211817476;24491760700;","Learning a 3D gaze estimator with improved itracker combined with bidirectional LSTM",2019,"Proceedings - IEEE International Conference on Multimedia and Expo","2019-July",,"8784770","850","855",,5,"10.1109/ICME.2019.00151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071031540&doi=10.1109%2fICME.2019.00151&partnerID=40&md5=c89a7ca6c2c8a609f81d446acad41cf7","College of Computer Science and Technology, Zhejiang University of Technology, China","Zhou, X., College of Computer Science and Technology, Zhejiang University of Technology, China; Lin, J., College of Computer Science and Technology, Zhejiang University of Technology, China; Jiang, J., College of Computer Science and Technology, Zhejiang University of Technology, China; Chen, S., College of Computer Science and Technology, Zhejiang University of Technology, China","Free-head 3D gaze estimation which outputs gaze vector in 3D space has wide application in human-computer interaction. In this paper, we propose a novel 3D gaze estimator by improving the Itracker and employing a many-to-one bidirectional LSTM (bi-LSTM). First, we improve the conventional Itracker by removing the face-grid and reducing one network branch via concatenating the two-eye region images to predict the subject's gaze of a single frame. Then, we employ the bi-LSTM to fit the temporal information between frames to estimate gaze vector for video sequence. Experimental results show that our improved Itracker obtains 11.6% significant improvement over the state-of-the-art methods on MPIIGaze dataset (single image frame) and has robust estimation accuracy for different image resolutions. Moreover, experimental results on EyeDiap dataset (video sequence) further bring 3% accuracy improvement by employing the bi-LSTM. © 2019 IEEE.","Gaze estimation; Itracker; LSTM; RNN","Human computer interaction; Image enhancement; Image resolution; Vector spaces; Video recording; Accuracy Improvement; Gaze estimation; Itracker; LSTM; Robust estimation; State-of-the-art methods; Temporal information; Video sequences; Long short-term memory",Conference Paper,"Final","",Scopus,2-s2.0-85071031540
"Li Z., Zhang X.","57201258724;56909566000;","Collaborative deep reinforcement learning for image cropping",2019,"Proceedings - IEEE International Conference on Multimedia and Expo","2019-July",,"8784906","254","259",,6,"10.1109/ICME.2019.00052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070996075&doi=10.1109%2fICME.2019.00052&partnerID=40&md5=11d49d8fa1eed8824a49e66c10640edb","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, 518000, China","Li, Z., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, 518000, China; Zhang, X., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, 518000, China","An automatic photo composition method based on collaborative deep reinforcement learning(called CDRL-RC) is proposed in this paper. Our method models photo composition as a markov decision-making process by reinforcement learning and generates cropping result through a series of moving and zooming actions. Emotional attention information is added to the composition task, which was trained by eye-tracking datasets to consider the relationship and importance between objects. In order to sufficiently use the emotional attention map and original image for image cropping, they are processed as inputs to two collaborative agents. For the collaborative composition of two agents, we design an information interaction module, which allows inter-agents to exchange information and give advice to each other, and finally predict the action together. In addition, we add attention weight to the traditional IoU to efficiently evaluate the cropping quantity in the reward function. Experiment results show that our CDRL-RC model achieved the state-of-the-art photo composition performance on a variety of datasets. © 2019 IEEE.","Emotion attention; Photo composition; Reinforcement learning","Decision making; Eye tracking; Image processing; Machine learning; Reinforcement learning; Collaborative agents; Emotion attention; Emotional attention; Information interaction; Markov decision; Original images; Reward function; State of the art; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85070996075
"Volonte M., Duchowski A.T., Babu S.V.","57203974310;6701824388;9039004700;","Effects of a virtual human appearance fidelity continuum on visual attention in virtual reality",2019,"IVA 2019 - Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents",,,,"141","147",,1,"10.1145/3308532.3329461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069744335&doi=10.1145%2f3308532.3329461&partnerID=40&md5=ce8311a9b013cb4261689c908d190158","Clemson University, United States","Volonte, M., Clemson University, United States; Duchowski, A.T., Clemson University, United States; Babu, S.V., Clemson University, United States","In this contribution we studied how different rendering styles of a virtual human impacted users' visual attention in an interactive medical training simulator. In a mixed design experiment, 78 participants interacted with a virtual human representing a sample from the non-photorealistic (NPR) to the photorealistic (PR) rendering continuity. We presented five rendering style samples scenarios, namely low fidelity all Pencil Shaded (APS), Low to Mid Fidelity Pencil Shader on virtual patient (VP) only (PS), Mid Fidelity All Cartoon Shaded (ACT), Mid to High Fidelity Cartoon Shader on VP only (CT), and relatively High Fidelity Human Like (HL) appearance, and compared how visual attention differed between groups of users. For this study, we employed an eye tracking system for collecting and analyzing users' gaze during interaction with the virtual human in a failure to rescue medical training simulation. Results suggests that users may spend more time in the simulations on the non-realistic fidelity continuum that necessarily do not involve interaction with the virtual human. However, users preferred visually attending to virtual humans in the middle and high fidelity visual appearance conditions when engaging virtual humans in simulated social face-To-face dialogue as compared to the other conditions. © 2019 Copyright held by the owner/author(s).","Eye tracking; Human Computer Interaction; Rendering Style; Virtual Humans; Visual Attention","Behavioral research; Eye tracking; Human computer interaction; Design experiments; Eye tracking systems; Face-to-face dialogues; Medical training simulator; Rendering Style; Virtual humans; Visual appearance; Visual Attention; Intelligent virtual agents",Conference Paper,"Final","",Scopus,2-s2.0-85069744335
"Xia C., Han J., Qi F., Shi G.","56102195500;24450644400;48161721000;55536676300;","Predicting Human Saccadic Scanpaths Based on Iterative Representation Learning",2019,"IEEE Transactions on Image Processing","28","7","8637020","3502","3515",,10,"10.1109/TIP.2019.2897966","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061549642&doi=10.1109%2fTIP.2019.2897966&partnerID=40&md5=461b6fa20948ade9cc74a5c437b843f1","School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; School of Artificial Intelligence, Xidian University, Xi'an, 710071, China","Xia, C., School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Han, J., School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Qi, F., School of Artificial Intelligence, Xidian University, Xi'an, 710071, China; Shi, G., School of Artificial Intelligence, Xidian University, Xi'an, 710071, China","Visual attention is a dynamic process of scene exploration and information acquisition. However, existing research on attention modeling has concentrated on estimating static salient locations. In contrast, dynamic attributes presented by saccade have not been well explored in previous attention models. In this paper, we address the problem of saccadic scanpath prediction by introducing an iterative representation learning framework. Within the framework, saccade can be interpreted as an iterative process of predicting one fixation according to the current representation and updating the representation based on the gaze shift. In the predicting phase, we propose a Bayesian definition of saccade to combine the influence of perceptual residual and spatial location on the selection of fixations. In implementation, we compute the representation error of an autoencoder-based network to measure perceptual residuals of each area. Simultaneously, we integrate saccade amplitude and center-weighted mechanism to model the influence of spatial location. Based on estimating the influence of two parts, the final fixation is defined as the point with the largest posterior probability of gaze shift. In the updating phase, we update the representation pattern for the subsequent calculation by retraining the network with samples extracted around the current fixation. In the experiments, the proposed model can replicate the fundamental properties of psychophysics in visual search. In addition, it can achieve superior performance on several benchmark eye-tracking data sets. © 1992-2012 IEEE.","deep learning; representation learning; saccade; scanpath; Visual attention","Behavioral research; Benchmarking; Brain models; Data structures; Data visualization; Estimation; Eye movements; Eye tracking; Feature extraction; Flow visualization; Forecasting; Iterative methods; Location; Computational model; Predictive models; representation learning; Scan path; Visual Attention; Deep learning; adult; algorithm; attention; biological model; factual database; female; human; image processing; male; physiology; procedures; saccadic eye movement; young adult; Adult; Algorithms; Attention; Databases, Factual; Deep Learning; Female; Humans; Image Processing, Computer-Assisted; Male; Models, Neurological; Saccades; Young Adult",Article,"Final","",Scopus,2-s2.0-85061549642
"Goltz J., Grossberg M., Etemadpour R.","57210122607;57190595232;36165000200;","Exploring simple neural network architectures for eye movement classification",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a4","","",,,"10.1145/3314111.3319813","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069533165&doi=10.1145%2f3314111.3319813&partnerID=40&md5=932138a42630f9ff1181f7bb3bb53181","University of Applied Science Munich, Munich, Germany; City University of New York, New York, NY, United States","Goltz, J., University of Applied Science Munich, Munich, Germany; Grossberg, M., City University of New York, New York, NY, United States; Etemadpour, R., City University of New York, New York, NY, United States","Analysis of eye-gaze is a critical tool for studying human-computer interaction and visualization. Yet eye tracking systems only report eye-gaze on the scene by producing large volumes of coordinate time series data. To be able to use this data, we must first extract salient events such as eye fixations, saccades, and post-saccadic oscillations (PSO). Manually extracting these events is time-consuming, labor-intensive and subject to variability. In this paper, we present and evaluate simple and fast automatic solutions for eye-gaze analysis based on supervised learning. Similar to some recent studies, we developed different simple neural networks demonstrating that feature learning produces superior results in identifying events from sequences of gaze coordinates. We do not apply any ad-hoc post-processing, thus creating a fully automated end-to-end algorithms that perform as good as current state-of-the-art architectures. Once trained they are fast enough to be run in a near real time setting. © 2019 Association for Computing Machinery.","Deep learning; Event detection; Eye movement; Machine learning","Deep learning; Eye movements; Human computer interaction; Learning systems; Machine learning; Network architecture; Neural networks; Coordinate time series; Event detection; Eye movement classifications; Eye tracking systems; Feature learning; Fully automated; Post processing; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069533165
"Eivazi S., Santini T., Keshavarzi A., Kübler T., Mazzei A.","37019970200;54881866000;57210108025;55701951700;55843510100;","Improving real-time CNN-based pupil detection through domain-specific data augmentation",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a40","","",,8,"10.1145/3314111.3319914","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069532560&doi=10.1145%2f3314111.3319914&partnerID=40&md5=86588e24b9db3a6bf55241d159902a24","University of Tübingen, Perception Engineering, Germany; Cortical Arts GmbH, Switzerland","Eivazi, S., University of Tübingen, Perception Engineering, Germany; Santini, T., University of Tübingen, Perception Engineering, Germany; Keshavarzi, A., University of Tübingen, Perception Engineering, Germany; Kübler, T., University of Tübingen, Perception Engineering, Germany; Mazzei, A., Cortical Arts GmbH, Switzerland","Deep learning is a promising technique for real-world pupil detection. However, the small amount of available accurately-annotated data poses a challenge when training such networks. Here, we utilize non-challenging eye videos where algorithmic approaches perform virtually without errors to automatically generate a foundational data set containing subpixel pupil annotations. Then, we propose multiple domain-specific data augmentation methods to create unique training sets containing controlled distributions of pupil-detection challenges. The feasibility, convenience, and advantage of this approach is demonstrated by training a CNN with these datasets. The resulting network outperformed current methods in multiple publicly-available, realistic, and challenging datasets, despite being trained solely with the augmented eye images. This network also exhibited better generalization w.r.t. the latest state-of-the-art CNN: Whereas on datasets similar to training data, the nets displayed similar performance, on datasets unseen to both networks, ours outperformed the state-of-the-art by ≈27% in terms of detection rate. © 2019 Association for Computing Machinery.","Data augmentation; Deep learning; Pupil detection","Deep learning; Algorithmic approach; Data augmentation; Detection rates; Domain specific; Multiple domains; Pupil detection; State of the art; Training sets; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069532560
"Venuprasad P., Dobhal T., Paul A., Nguyen T.N.M., Gilman A., Cosman P., Chukoskie L.","57210106962;57210105601;57210123331;57210107998;16318668400;7003359562;6507740817;","Characterizing joint attention behavior during real world interactions using automated object and gaze detection",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a21","","",,4,"10.1145/3314111.3319843","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069528016&doi=10.1145%2f3314111.3319843&partnerID=40&md5=b774889d47583f5b94177941b50cbec1","University of California San Diego, San Diego, CA, United States; Minerva Schools at the Keck Graduate Institute, United States; Massey University, Auckland, New Zealand","Venuprasad, P., University of California San Diego, San Diego, CA, United States; Dobhal, T., University of California San Diego, San Diego, CA, United States; Paul, A., University of California San Diego, San Diego, CA, United States; Nguyen, T.N.M., Minerva Schools at the Keck Graduate Institute, United States; Gilman, A., Massey University, Auckland, New Zealand; Cosman, P., University of California San Diego, San Diego, CA, United States; Chukoskie, L., University of California San Diego, San Diego, CA, United States","Joint attention is an essential part of the development process of children, and impairments in joint attention are considered as one of the first symptoms of autism. In this paper, we develop a novel technique to characterize joint attention in real time, by studying the interaction of two human subjects with each other and with multiple objects present in the room. This is done by capturing the subjects’ gaze through eye-tracking glasses and detecting their looks on predefined indicator objects. A deep learning network is trained and deployed to detect the objects in the field of vision of the subject by processing the video feed of the world view camera mounted on the eye-tracking glasses. The looking patterns of the subjects are determined and a real-time audio response is provided when a joint attention is detected, i.e., when their looks coincide. Our findings suggest a trade-off between the accuracy measure (Look Positive Predictive Value) and the latency of joint look detection for various system parameters. For more accurate joint look detection, the system has higher latency, and for faster detection, the detection accuracy goes down. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Autism; Computer Vision; Deep Learning; Eye-Tracking; Gaze Behavior; Joint Attention; Object Detection","Cameras; Computer vision; Deep learning; Diseases; Economic and social effects; Glass; Object detection; Accuracy measures; Autism; Detection accuracy; Development process; Gaze behavior; Joint attention; Novel techniques; Positive predictive values; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069528016
"Dierkes K., Kassner M., Bulling A.","57202889988;56406193700;6505807414;","A fast approach to refraction-aware eye-model fitting and gaze prediction",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a23","","",,4,"10.1145/3314111.3319819","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069521443&doi=10.1145%2f3314111.3319819&partnerID=40&md5=2463fbba3947b6cf5acba4c35be2b371","Pupil Labs Research, Berlin, Germany","Dierkes, K., Pupil Labs Research, Berlin, Germany; Kassner, M., Pupil Labs Research, Berlin, Germany; Bulling, A., Pupil Labs Research, Berlin, Germany","By temporally integrating information about pupil contours extracted from eye images, model-based methods for glint-free gaze estimation can mitigate pupil detection noise. However, current approaches require time-consuming iterative solving of a nonlinear minimization problem to estimate key parameters, such as eyeball position. Based on the method presented by [Swirski and Dodgson 2013], we propose a novel approach to glint-free 3D eye-model fitting and gaze prediction using a single near-eye camera. By recasting model optimization as a least-squares intersection of lines, we make it amenable to a fast non-iterative solution. We further present a method for estimating deterministic refraction-correction functions from synthetic eye images and validate them on both synthetic and real eye images. We demonstrate the robustness of our method in the presence of pupil detection noise and show the benefit of temporal integration of pupil contour information on eyeball position and gaze estimation accuracy. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D eye model; Contour-based; Eye tracking; Glint-free; Pupil detection; Refraction","3D modeling; Iterative methods; Refraction; 3D eye models; Contour-based; Glint-free; Integrating information; Non-iterative solutions; Nonlinear minimization; Pupil detection; Temporal integration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069521443
"Utsu T., Takemura K.","57210122289;8575290600;","Remote corneal imaging by integrating a 3D face model and an eyeball model",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a44","","",,,"10.1145/3314111.3319817","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069519730&doi=10.1145%2f3314111.3319817&partnerID=40&md5=e620a164f5b14d2241a610ffa862e88c","Tokai University, Hiratsuka, Kanagawa, Japan","Utsu, T., Tokai University, Hiratsuka, Kanagawa, Japan; Takemura, K., Tokai University, Hiratsuka, Kanagawa, Japan","In corneal imaging methods, it is essential to use a 3D eyeball model for generating an undistorted image. Thus, the relationship between the eye and eye camera is fixed by using a head-mounted device. Remote corneal imaging has several potential applications such as surveillance systems and driver monitoring. Therefore, we integrated a 3D eyeball model with a 3D face model to facilitate remote corneal imaging. We conducted evaluation experiments and confirmed the feasibility of remote corneal imaging. We showed that the center of the eyeball can be estimated based on face tracking, and thus, corneal imaging can function as continuous remote eye tracking. © 2019 Association for Computing Machinery.","3D eye model; Corneal imaging; Face Tracking","3D modeling; Face recognition; 3-D face modeling; 3D eye models; Driver monitoring; Evaluation experiments; Face Tracking; Imaging method; Surveillance systems; Undistorted images; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069519730
"Wang H., Shi B.E.","56809110800;7402547071;","Gaze awareness improves collaboration efficiency in a collaborative assembly task",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a88","","",,1,"10.1145/3317959.3321492","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069518064&doi=10.1145%2f3317959.3321492&partnerID=40&md5=612ee57489e7bff637e1f61a9753fe64","Hong Kong University of Science and Technology, Hong Kong, Hong Kong","Wang, H., Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Shi, B.E., Hong Kong University of Science and Technology, Hong Kong, Hong Kong","In building human robot interaction systems, it would be helpful to understand how humans collaborate, and in particular, how humans use others’ gaze behavior to estimate their intent. Here we studied the use of gaze in a collaborative assembly task, where a human user assembled an object with the assistance of a human helper. We found that the being aware of the partner’s gaze significantly improved collaboration efficiency. Task completion times were much shorter when gaze communication was available, than when it was blocked. In addition, we found that the user’s gaze was more likely to lie on the object of interest in the gaze-aware case than the gaze-blocked case. In the context of human-robot collaboration systems, our results suggest that gaze data in the period surrounding verbal requests will be more informative and can be used to predict the target object. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D gaze estimation; Gaze awareness; Gaze tracking; Human robot collaboration","Distributed computer systems; Efficiency; Human robot interaction; Collaborative assembly; Completion time; Gaze awareness; Gaze behavior; Gaze communications; Gaze estimation; Gaze tracking; Human-robot collaboration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069518064
"Aronson R.M., Admoni H.","57201505550;35180933000;","Semantic gaze labeling for human-robot shared manipulation",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a2","","",,3,"10.1145/3314111.3319840","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069502880&doi=10.1145%2f3314111.3319840&partnerID=40&md5=09b1ab74a505df4669f97f7e07d96404","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States","Aronson, R.M., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Admoni, H., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States","Human-robot collaboration systems benefit from recognizing people’s intentions. This capability is especially useful for collaborative manipulation applications, in which users operate robot arms to manipulate objects. For collaborative manipulation, systems can determine users’ intentions by tracking eye gaze and identifying gaze fixations on particular objects in the scene (i.e., semantic gaze labeling). Translating 2D fixation locations (from eye trackers) into 3D fixation locations (in the real world) is a technical challenge. One approach is to assign each fixation to the object closest to it. However, calibration drift, head motion, and the extra dimension required for real-world interactions make this position matching approach inaccurate. In this work, we introduce velocity features that compare the relative motion between subsequent gaze fixations and a finite set of known points and assign fixation position to one of those known points. We validate our approach on synthetic data to demonstrate that classifying using velocity features is more robust than a position matching approach. In addition, we show that a classifier using velocity features improves semantic labeling on a real-world dataset of human-robot assistive manipulation interactions. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Assistive robotics; Eye tracking; Human-robot interaction; Intention recognition; Semantic gaze labeling","Classification (of information); Human robot interaction; Semantics; Assistive robotics; Calibration drift; Extra dimensions; Human-robot collaboration; Intention recognition; Relative motion; Semantic labeling; Technical challenges; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069502880
"Mardanbegi D., Clarke C., Gellersen H.","42761947400;56559308000;6701531333;","Monocular gaze depth estimation using the vestibulo-ocular reflex",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a20","","",,3,"10.1145/3314111.3319822","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069500554&doi=10.1145%2f3314111.3319822&partnerID=40&md5=6805937d9fb8e3555f59b748b254276e","Lancaster University, United Kingdom","Mardanbegi, D., Lancaster University, United Kingdom; Clarke, C., Lancaster University, United Kingdom; Gellersen, H., Lancaster University, United Kingdom","Gaze depth estimation presents a challenge for eye tracking in 3D. This work investigates a novel approach to the problem based on eye movement mediated by the vestibulo-ocular reflex (VOR). VOR stabilises gaze on a target during head movement, with eye movement in the opposite direction, and the VOR gain increases the closer the fixated target is to the viewer. We present a theoretical analysis of the relationship between VOR gain and depth which we investigate with empirical data collected in a user study (N=10). We show that VOR gain can be captured using pupil centres, and propose and evaluate a practical method for gaze depth estimation based on a generic function of VOR gain and two-point depth calibration. The results show that VOR gain is comparable with vergence in capturing depth while only requiring one eye, and provide insight into open challenges in harnessing VOR gain as a robust measure. © 2019 Association for Computing Machinery.","3D gaze estimation; Eye movement; Eye tracking; Fixation depth; Gaze depth estimation; VOR","Eye movements; Depth calibration; Depth Estimation; Gaze estimation; Generic functions; Head movements; Practical method; Robust measures; Vestibulo-ocular reflex; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069500554
"Yang Z., Bailey R.","57221061924;16641965200;","Towards a Data-driven Framework for Realistic Self-Organized Virtual Humans: Coordinated Head and Eye movements",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a55","","",,,"10.1145/3314111.3322874","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069500464&doi=10.1145%2f3314111.3322874&partnerID=40&md5=c7166b7c517a50238cc7507a7bc7f293","Rochester Institute of Technology, Rochester, NY, United States","Yang, Z., Rochester Institute of Technology, Rochester, NY, United States; Bailey, R., Rochester Institute of Technology, Rochester, NY, United States","Driven by significant investments from the gaming, film, advertising, and customer service industries among others, efforts across many different fields are converging to create realistic representations of humans that look like (computer graphics), sound like (natural language generation), move like (motion capture), and reason like (artificial intelligence) real humans. The ultimate goal of this work is to push the boundaries even further by exploring the development of realistic self-organized virtual humans that are capable of demonstrating coordinated behaviors across different modalities. Eye movements, for example, may be accompanied by changes in facial expression, head orientation, posture, gait properties, or speech. Traditionally however, these modalities are captured and modeled separately and this disconnect contributes to the well-known uncanny valley phenomenon. We focus initially on facial modalities, in particular, coordinated eye and head movements (and eventually facial expressions), but our proposed data-driven framework will be able to accommodate other modalities as well. © 2019 Copyright held by the owner/author(s).","Data driven animation; Eye-head coordination; Machine learning","Artificial intelligence; Eye tracking; Learning systems; Natural language processing systems; Virtual reality; Coordinated behavior; Customer services; Data-driven animation; Eye-head coordination; Facial Expressions; Head movements; Motion capture; Natural language generation; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85069500464
"Katrychuk D., Griffith H.K., Komogortsev O.V.","57210105027;57189386560;6506328653;","Power-efficient and shift-robust eye-tracking sensor for portable VR headsets",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a19","","",,8,"10.1145/3314111.3319821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069483095&doi=10.1145%2f3314111.3319821&partnerID=40&md5=457b404170995b342473d55c75de6d33","Department of Computer Science, Texas State University, San Marcos, TX, United States","Katrychuk, D., Department of Computer Science, Texas State University, San Marcos, TX, United States; Griffith, H.K., Department of Computer Science, Texas State University, San Marcos, TX, United States; Komogortsev, O.V., Department of Computer Science, Texas State University, San Marcos, TX, United States","Photosensor oculography (PSOG) is a promising solution for reducing the computational requirements of eye tracking sensors in wireless virtual and augmented reality platforms. This paper proposes a novel machine learning-based solution for addressing the known performance degradation of PSOG devices in the presence of sensor shifts. Namely, we introduce a convolutional neural network model capable of providing shift-robust end-to-end gaze estimates from the PSOG array output. Moreover, we propose a transfer-learning strategy for reducing model training time. Using a simulated workflow with improved realism, we show that the proposed convolutional model offers improved accuracy over a previously considered multilayer perceptron approach. In addition, we demonstrate that the transfer of initialization weights from pre-trained models can substantially reduce training time for new users. In the end, we provide the discussion regarding the design trade-offs between accuracy, training time, and power consumption among the considered models. © 2019 Association for Computing Machinery.","Eye-tracking; Machine learning; ML; Photo-sensor oculography; PSOG; Virtual reality; VR","Augmented reality; Convolution; Economic and social effects; Learning systems; Machine learning; Neural networks; Optical sensors; Virtual reality; Computational requirements; Convolutional model; Convolutional neural network; Eye-tracking sensors; Performance degradation; Photo-sensors; PSOG; Virtual and augmented reality; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069483095
"Hassoumi A., Peysakhovich V., Hurter C.","57207818964;56644829800;24740874700;","Eyeflow: Pursuit interactions using an unmodified camera",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a3","","",,3,"10.1145/3314111.3319820","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069477141&doi=10.1145%2f3314111.3319820&partnerID=40&md5=4e96327bf9d723a6395f45eef7a315d1","École Nationale de l’Aviation Civile, Toulouse, France; ISAE-SUPAERO, Université de Toulouse, France","Hassoumi, A., École Nationale de l’Aviation Civile, Toulouse, France; Peysakhovich, V., ISAE-SUPAERO, Université de Toulouse, France; Hurter, C., École Nationale de l’Aviation Civile, Toulouse, France","We investigate the smooth pursuit eye movement based interaction using an unmodiied of-the-shelf RGB camera. In each pair of sequential video frames, we compute the indicative direction of the eye movement by analyzing low vectors obtained using the Lucas-Kanade optical low algorithm. We discuss how carefully selected low vectors could replace the traditional pupil centers detection in smooth pursuit interaction. We examine implications of unused features in the eye camera imaging frame as potential elements for detecting gaze gestures. This simple approach is easy to implement and abstains from many of the complexities of pupil based approaches. In particular, EyeFlow does not call for either a 3D pupil model or 2D pupil detection to track the pupil center location. We compare this method to state-of-the-art approaches and ind that this can enable pursuit interactions with standard cameras. Results from the evaluation with 12 users data yield an accuracy that compares to previous studies. In addition, the beneit of this work is that the approach does not necessitate highly matured computer vision algorithms and expensive IR-pass cameras. © 2019 Association for Computing Machinery.","Eye tracking; Interaction; Smooth pursuit; Touch-free interaction","3D modeling; Cameras; Eye movements; Computer vision algorithms; Interaction; Pupil detection; Smooth pursuit; Smooth pursuit eye movement; Standard cameras; State-of-the-art approach; Touch-free interaction; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069477141
"Siegfried R., Yu Y., Odobez J.-M.","57195685304;57188644020;57203103085;","A deep learning approach for robust head pose independent eye movements recognition from videos",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a31","","",,1,"10.1145/3314111.3319844","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069475315&doi=10.1145%2f3314111.3319844&partnerID=40&md5=9518db12f9618cf737194d30b3d78a0b","Idiap Research Institute Martigny, Switzerland EPFL, Lausanne, Switzerland","Siegfried, R., Idiap Research Institute Martigny, Switzerland EPFL, Lausanne, Switzerland; Yu, Y., Idiap Research Institute Martigny, Switzerland EPFL, Lausanne, Switzerland; Odobez, J.-M., Idiap Research Institute Martigny, Switzerland EPFL, Lausanne, Switzerland","Recognizing eye movements is important for gaze behavior understanding like in human communication analysis (human-human or robot interactions) or for diagnosis (medical, reading impairments). In this paper, we address this task using remote RGB-D sensors to analyze people behaving in natural conditions. This is very challenging given that such sensors have a normal sampling rate of 30 Hz and provide low-resolution eye images (typically 36x60 pixels), and natural scenarios introduce many variabilities in illumination, shadows, head pose, and dynamics. Hence gaze signals one can extract in these conditions have lower precision compared to dedicated IR eye trackers, rendering previous methods less appropriate for the task. To tackle these challenges, we propose a deep learning method that directly processes the eye image video streams to classify them into fixation, saccade, and blink classes, and allows to distinguish irrelevant noise (illumination, low-resolution artifact, inaccurate eye alignment, difficult eye shapes) from true eye motion signals. Experiments on natural 4-party interactions demonstrate the benefit of our approach compared to previous methods, including deep learning models applied to gaze outputs. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Blink; Convolutional neural network; Eye movements; Remote sensors; Saccade; Video processing","Deep learning; Diagnosis; Eye tracking; Human robot interaction; Motion estimation; Neural networks; Remote sensing; Video signal processing; Blink; Convolutional neural network; Human communications; Learning approach; Natural conditions; Remote sensors; Robot interactions; Video processing; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85069475315
"Agtzidis I., Dorr M.","56241483100;10244404800;","Getting (more) real: Bringing eye movement classification to HMD experiments with equirectangular stimuli",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a18","","",,1,"10.1145/3314111.3319829","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069473167&doi=10.1145%2f3314111.3319829&partnerID=40&md5=7775da32ba4330234ed8e20bd325c3c3","Technical University of Munich, Munich, Germany","Agtzidis, I., Technical University of Munich, Munich, Germany; Dorr, M., Technical University of Munich, Munich, Germany","The classification of eye movements is a very important part of eye tracking research and has been studied since its early days. Over recent years, we have experienced an increasing shift towards more immersive experimental scenarios with the use of eye-tracking enabled glasses and head-mounted displays. In these new scenarios, however, most of the existing eye movement classification algorithms cannot be applied robustly anymore because they were developed with monitor-based experiments using regular 2D images and videos in mind. In this paper, we describe two approaches that reduce artifacts of eye movement classification for 360° videos shown in head-mounted displays. For the first approach, we discuss how decision criteria have to change in the space of 360° videos, and use these criteria to modify five popular algorithms from the literature. The modified algorithms are publicly available at https://web.gin.g-node.org/ioannis.agtzidis/360_em_algorithms. For cases where an existing algorithm cannot be modified, e.g. because it is closed-source, we present a second approach that maps the data instead of the algorithm to the 360° space. An empirical evaluation of both approaches shows that they significantly reduce the artifacts of the initial algorithm, especially in the areas further from the horizontal midline. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","360° content; Event detection; Eye movement classification","Eye tracking; Helmet mounted displays; Motion analysis; Closed source; Decision criterions; Empirical evaluations; Event detection; Eye movement classifications; Head mounted displays; Immersive; Modified algorithms; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85069473167
"Rostaminia S., Mayberry A., Ganesan D., Marlin B., Gummeson J.","57210122473;55819678200;10739214500;6506955008;26221694900;","ILID: Eyewear solution for low-power fatigue and drowsiness monitoring",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a62","","",,,"10.1145/3314111.3322503","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069470708&doi=10.1145%2f3314111.3322503&partnerID=40&md5=89c855a8f5232ca26eed38ebee834fb6","University of Massachusetts Amherst, United States","Rostaminia, S., University of Massachusetts Amherst, United States; Mayberry, A., University of Massachusetts Amherst, United States; Ganesan, D., University of Massachusetts Amherst, United States; Marlin, B., University of Massachusetts Amherst, United States; Gummeson, J., University of Massachusetts Amherst, United States","The ability to monitor eye closures and blink patterns has long been known to enable accurate assessment of fatigue and drowsiness in individuals. Many measures of the eye are known to be correlated with fatigue including coarse-grained measures like the rate of blinks as well as fine-grained measures like the duration of blinks and the extent of eye closures. Despite a plethora of research validating these measures, we lack wearable devices that can continually and reliably monitor them in the natural environment. In this work, we present a low-power system, iLid, that can continually sense fine-grained measures such as blink duration and Percentage of Eye Closures (PERCLOS) at high frame rates of 100fps. We present a complete solution including design of the sensing, signal processing, and machine learning pipeline and implementation on a prototype computational eyeglass platform. © 2019 Copyright held by the owner/author(s).",,"Signal processing; Blink duration; Coarse-grained; Complete solutions; High frame rate; Low-power systems; Natural environments; PERcentage of eye CLOSure (PERCLOS); Wearable devices; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069470708
"Fuhl W., Bozkir E., Hosp B., Castner N., Geisler D., Santini T.C., Kasneci E.","56770084800;57210111357;57202890167;57193611337;57189847283;54881866000;56059892600;","Encodji: Encoding gaze data into emoji space for an amusing scanpath classification approach",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a64","","",,8,"10.1145/3314111.3323074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069462159&doi=10.1145%2f3314111.3323074&partnerID=40&md5=3c968a927fc6eaa4051d60b68101d083","Perception Engineering, University Tübingen, Germany","Fuhl, W., Perception Engineering, University Tübingen, Germany; Bozkir, E., Perception Engineering, University Tübingen, Germany; Hosp, B., Perception Engineering, University Tübingen, Germany; Castner, N., Perception Engineering, University Tübingen, Germany; Geisler, D., Perception Engineering, University Tübingen, Germany; Santini, T.C., Perception Engineering, University Tübingen, Germany; Kasneci, E., Perception Engineering, University Tübingen, Germany","To this day, a variety of information has been obtained from human eye movements, which holds an imense potential to understand and classify cognitive processes and states – e.g., through scanpath classification. In this work, we explore the task of scanpath classification through a combination of unsupervised feature learning and convolutional neural networks. As an amusement factor, we use an Emoji space representation as feature space. This representation is achieved by training generative adversarial networks (GANs) for unpaired scanpath-to-Emoji translation with a cyclic loss. The resulting Emojis are then used to train a convolutional neural network for stimulus prediciton, showing an accuracy improvement of more than five percentual points compared to the same network trained using solely the scanpath data. As a side effect, we also obtain novel unique Emojis representing each unique scanpath. Our goal is to demonstrate the applicability and potential of unsupervised feature learning to scanpath classification in a humorous and entertaining way. © 2019 Association for Computing Machinery.","Emoji; Eye Tracking; Generative Adversarial Networks; Image Generation; Scanpath","Convolution; Eye movements; Eye tracking; Machine learning; Neural networks; Signal encoding; Accuracy Improvement; Adversarial networks; Classification approach; Convolutional neural network; Emoji; Image generations; Scan path; Unsupervised feature learning; Classification (of information)",Conference Paper,"Final","",Scopus,2-s2.0-85069462159
"Le Louedec J., Guntz T., Crowley J.L., Vaufreydaz D.","57210107020;57205029905;7202580103;14720435700;","Deep learning investigation for chess player attention prediction using eye-tracking and game data",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a1","","",,2,"10.1145/3314111.3319827","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069450379&doi=10.1145%2f3314111.3319827&partnerID=40&md5=46d64109a439a7cb05c91eae03e76b8d","Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, Grenoble, 38000, France","Le Louedec, J., Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, Grenoble, 38000, France; Guntz, T., Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, Grenoble, 38000, France; Crowley, J.L., Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, Grenoble, 38000, France; Vaufreydaz, D., Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, Grenoble, 38000, France","This article reports on an investigation of the use of convolutional neural networks to predict the visual attention of chess players. The visual attention model described in this article has been created to generate saliency maps that capture hierarchical and spatial features of chessboard, in order to predict the probability fixation for individual pixels Using a skip-layer architecture of an autoencoder, with a unified decoder, we are able to use multiscale features to predict saliency of part of the board at different scales, showing multiple relations between pieces. We have used scan path and fixation data from players engaged in solving chess problems, to compute 6600 saliency maps associated to the corresponding chess piece configurations. This corpus is completed with synthetically generated data from actual games gathered from an online chess platform. Experiments realized using both scan-paths from chess players and the CAT2000 saliency dataset of natural images, highlights several results. Deep features, pretrained on natural images, were found to be helpful in training visual attention prediction for chess. The proposed neural network architecture is able to generate meaningful saliency maps on unseen chess configurations with good scores on standard metrics. This work provides a baseline for future work on visual attention prediction in similar contexts. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Chess; Computer vision; Deep neural network; Visual attention","Behavioral research; Computer vision; Deep neural networks; Forecasting; Network architecture; Neural networks; Chess; Convolutional neural network; Layer architectures; Multi-scale features; Spatial features; Standard metrics; Visual Attention; Visual attention model; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069450379
"Larumbe-Bergera A., Porta S., Cabeza R., Villanueva A.","57210106737;7005292345;36763933900;7101612861;","SetA: Semiautomatic tool for annotation of eye tracking images",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a45","","",,3,"10.1145/3314111.3319830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069446912&doi=10.1145%2f3314111.3319830&partnerID=40&md5=da7de7222df68e548ac651f7ea792a28","Public University of Navarre, Pamplona, Spain","Larumbe-Bergera, A., Public University of Navarre, Pamplona, Spain; Porta, S., Public University of Navarre, Pamplona, Spain; Cabeza, R., Public University of Navarre, Pamplona, Spain; Villanueva, A., Public University of Navarre, Pamplona, Spain","Availability of large scale tagged datasets is a must in the field of deep learning applied to the eye tracking challenge. In this paper, the potential of Supervised-Descent-Method (SDM) as a semiautomatic labelling tool for eye tracking images is shown. The objective of the paper is to evidence how the human effort needed for manually labelling large eye tracking datasets can be radically reduced by the use of cascaded regressors. Different applications are provided in the fields of high and low resolution systems. An iris/pupil center labelling is shown as example for low resolution images while a pupil contour points detection is demonstrated in high resolution. In both cases manual annotation requirements are drastically reduced. © 2019 Copyright held by the owner/author(s).","Eye tracking; Image annotation; Supervised-Descent-Method","Deep learning; Image annotation; Large dataset; Contour points; Descent method; High resolution; Low resolution; Low resolution images; Manual annotation; Semi-automatic tools; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069446912
"Soret R., Charras P., Hurter C., Peysakhovich V.","57210112083;24483147000;24740874700;56644829800;","Attentional orienting in virtual reality using endogenous and exogenous cues in auditory and visual modalities",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a86","","",,4,"10.1145/3317959.3321490","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069443612&doi=10.1145%2f3317959.3321490&partnerID=40&md5=1eff19f808b351606dd9f14ea9b73954","ISAE-SUPAERO, Université de Toulouse, France; Université Paul-Valéry Montpellier 3, France; ENAC, Toulouse, France","Soret, R., ISAE-SUPAERO, Université de Toulouse, France; Charras, P., Université Paul-Valéry Montpellier 3, France; Hurter, C., ENAC, Toulouse, France; Peysakhovich, V., ISAE-SUPAERO, Université de Toulouse, France","The virtual reality (VR) has nowadays numerous applications in training, education, and rehabilitation. To efficiently present the immersive 3D stimuli, we need to understand how spatial attention is oriented in VR. The efficiency of different cues can be compared using the Posner paradigm. In this study, we designed an ecological environment where participants were presented with a modified version of the Posner cueing paradigm. Twenty subjects equipped with an eye-tracking system and VR HMD performed a sandwich preparation task. They were asked to assemble the ingredients which could be either endogenously and exogenously cued in both auditory and visual modalities. The results showed that all valid cues made participants react faster. While directional arrow (visual endogenous) and 3D sound (auditory exogenous) oriented attention globally to the entire cued hemifield, the vocal instruction (auditory endogenous) and object highlighting (visual exogenous) allowed more local orientation, in a specific region of space. No differences in gaze shift initiation nor time to fixate the target were found suggesting the covert orienting. © 2019 Association for Computing Machinery.","Attentional orienting; Endogenous; Exogenous; Virtual reality","Virtual reality; Attentional orienting; Ecological environments; Endogenous; Exogenous; Eye tracking systems; Local orientations; Spatial attention; Visual modalities; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069443612
"Gunawardena N., Matscheko M., Anzengruber B., Ferscha A., Schobesberger M., Shamiyeh A., Klugsberger B., Solleder P.","57210115715;24605414700;55510533000;6701318941;57210124254;6701562343;56183817800;57210123292;","Assessing surgeons’ skill level in laparoscopic cholecystectomy using eye metrics",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a30","","",,2,"10.1145/3314111.3319832","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069443333&doi=10.1145%2f3314111.3319832&partnerID=40&md5=8615ee26573a2d28565b5475305d19a1","Johannes Kepler University, Linz, Austria; Kepler University Clinic, Linz, Austria; KARL STORZ SE and Co. KG, Tuttlingen, Germany","Gunawardena, N., Johannes Kepler University, Linz, Austria; Matscheko, M., Johannes Kepler University, Linz, Austria; Anzengruber, B., Johannes Kepler University, Linz, Austria; Ferscha, A., Johannes Kepler University, Linz, Austria; Schobesberger, M., Johannes Kepler University, Linz, Austria; Shamiyeh, A., Kepler University Clinic, Linz, Austria; Klugsberger, B., Kepler University Clinic, Linz, Austria; Solleder, P., KARL STORZ SE and Co. KG, Tuttlingen, Germany","Laparoscopic surgery has revolutionised state of the art in surgical health care. However, its complexity puts a significant burden on the surgeon’s cognitive resources resulting in major biliary injuries. With the increasing number of laparoscopic surgeries, it is crucial to identify surgeons’ cognitive loads (CL) and levels of focus in real time to give them unobtrusive feedback when detecting the suboptimal level of attention. Assuming that the experts appear to be more focused on attention, we investigate how the skill level of surgeons during live surgery is reflected through eye metrics. Forty-two laparoscopic surgeries have been conducted with four surgeons who have different expertise levels. Concerning eye metrics, we have used six metrics which belong to fixation and pupillary based metrics. With the use of mean, standard deviation and ANOVA test we have proven three reliable metrics which we can use to differentiate the skill level during live surgeries. In future studies, these three metrics will be used to classify the surgeons’ cognitive load and level of focus during the live surgery using machine learning techniques. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Cognitive Computing; Eye metrics; Gaze based interaction; Laparoscopic cholecystectomy; Skill level detection","Laparoscopy; Learning systems; Transplantation (surgical); Cognitive Computing; Eye metrics; Gaze-based interaction; Laparoscopic cholecystectomy; Skill levels; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069443333
"Paletta L., Dini A., Murko C., Yahyanejad S., Augsdörfer U.","6602696802;57193756837;57193761146;36474258100;6507964057;","Estimation of situation awareness score and performance using eye and head gaze for human-robot collaboration",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a61","","",,1,"10.1145/3314111.3322504","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069429707&doi=10.1145%2f3314111.3322504&partnerID=40&md5=e3c787baddc6441d0d326e0abfffa505","Institute DIGITAL, JOANNEUM, RESEARCH FgesmbH, Graz, Austria; Inst. Comp. Graphics and Knowledge Visualis, TU Graz, Graz, Austria; Institute ROBOTICS, JOANNEUM, RESEARCH FgesmbH, Klagenfurt, Austria","Paletta, L., Institute DIGITAL, JOANNEUM, RESEARCH FgesmbH, Graz, Austria; Dini, A., Inst. Comp. Graphics and Knowledge Visualis, TU Graz, Graz, Austria; Murko, C., Institute DIGITAL, JOANNEUM, RESEARCH FgesmbH, Graz, Austria; Yahyanejad, S., Institute ROBOTICS, JOANNEUM, RESEARCH FgesmbH, Klagenfurt, Austria; Augsdörfer, U., Inst. Comp. Graphics and Knowledge Visualis, TU Graz, Graz, Austria","Human attention processes play a major role in the optimization of human-robot collaboration (HRC) [Huang et al. 2015]. We describe a novel methodology to measure and predict situation awareness from eye and head gaze features in real-time. The awareness about scene objects of interest was described by 3D gaze analysis using data from eye tracking glasses and a precise optical tracking system. A probabilistic framework of uncertainty considers coping with measurement errors in eye and position estimation. Comprehensive experiments on HRC were conducted with typical tasks including handover in a lab based prototypical manufacturing environment. The gaze features highly correlate with scores of standardized questionnaires of situation awareness (SART [Taylor 1990], SAGAT [Endsley 2000]) and predict performance in the HRC task. This will open new opportunities for human factors based optimization in HRC applications. © 2019 Copyright is held by the owner/author(s).","Dual task; Human-robot collaboration; Situation awareness","Robots; Surveys; Uncertainty analysis; Dual-tasks; Human-robot collaboration; Manufacturing environments; Novel methodology; Optical tracking systems; Position estimation; Probabilistic framework; Situation awareness; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069429707
"Hayek U.W., Müller K., Göbel F., Kiefer P., Spielhofer R., Grêt-Regamey A.","36894532200;57214989230;55303168800;7005116386;57214988664;16024264700;","3D point clouds and eye tracking for investigating the perception and acceptance of power lines in different landscapes",2019,"Multimodal Technologies and Interaction","3","2","40","","",,2,"10.3390/mti3020040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079040211&doi=10.3390%2fmti3020040&partnerID=40&md5=d02bb7a3b5997a1b2ee2d78c239c23dd","Planning of Landscape and Urban Systems (PLUS), IRL, ETH Zürich, Zürich, 8093, Switzerland; Geoinformation Engineering, IKG, ETH Zürich, Zürich, 8093, Switzerland","Hayek, U.W., Planning of Landscape and Urban Systems (PLUS), IRL, ETH Zürich, Zürich, 8093, Switzerland; Müller, K., Planning of Landscape and Urban Systems (PLUS), IRL, ETH Zürich, Zürich, 8093, Switzerland; Göbel, F., Geoinformation Engineering, IKG, ETH Zürich, Zürich, 8093, Switzerland; Kiefer, P., Geoinformation Engineering, IKG, ETH Zürich, Zürich, 8093, Switzerland; Spielhofer, R., Planning of Landscape and Urban Systems (PLUS), IRL, ETH Zürich, Zürich, 8093, Switzerland; Grêt-Regamey, A., Planning of Landscape and Urban Systems (PLUS), IRL, ETH Zürich, Zürich, 8093, Switzerland","The perception of the visual landscape impact is a significant factor explaining the public’s acceptance of energy infrastructure developments. Yet, there is lack of knowledge how people perceive and accept power lines in certain landscape types and in combination with wind turbines, a required setting to achieve goals of the energy turnaround. The goal of this work was to demonstrate how 3D point cloud visualizations could be used for an eye tracking study to systematically investigate the perception of landscape scenarios with power lines. 3D visualizations of near-natural and urban landscapes were prepared based on data from airborne and terrestrial laser scanning. These scenes were altered with varying amounts of the respective infrastructure, and they provided the stimuli in a laboratory experiment with 49 participants. Eye tracking and questionnaires served for measuring the participants’ responses. The results show that the point cloud-based simulations offered suitable stimuli for the eye tracking study. Particularly for the analysis of guided perceptions, the approach fostered an understanding of disturbing landscape elements. A comparative in situ eye tracking study is recommended to further evaluate the quality of the point cloud simulations, whether they produce similar responses as in the real world. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","3D landscape visualizations; Eye tracking; Landscape planning; LiDAR data; Perception studies",,Article,"Final","",Scopus,2-s2.0-85079040211
"Yu Y., Liu G., Odobez J.-M.","57188644020;56420692700;57203103085;","Improving few-shot user-specific gaze adaptation via gaze redirection synthesis",2019,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June",,"8953732","11929","11938",,26,"10.1109/CVPR.2019.01221","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078808349&doi=10.1109%2fCVPR.2019.01221&partnerID=40&md5=c63dd3b3386115bad7b4a933e880309a","Idiap Research Institute, Martigny, CH-1920, Switzerland; EPFL, Lausanne, CH-1015, Switzerland","Yu, Y., Idiap Research Institute, Martigny, CH-1920, Switzerland, EPFL, Lausanne, CH-1015, Switzerland; Liu, G., Idiap Research Institute, Martigny, CH-1920, Switzerland, EPFL, Lausanne, CH-1015, Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, CH-1920, Switzerland, EPFL, Lausanne, CH-1015, Switzerland","As an indicator of human attention gaze is a subtle behavioral cue which can be exploited in many applications. However, inferring 3D gaze direction is challenging even for deep neural networks given the lack of large amount of data (groundtruthing gaze is expensive and existing datasets use different setups) and the inherent presence of gaze biases due to person-specific difference. In this work, we address the problem of person-specific gaze model adaptation from only a few reference training samples. The main and novel idea is to improve gaze adaptation by generating additional training samples through the synthesis of gaze-redirected eye images from existing reference samples. In doing so, our contributions are threefold:(i) we design our gaze redirection framework from synthetic data, allowing us to benefit from aligned training sample pairs to predict accurate inverse mapping fields; (ii) we proposed a self-supervised approach for domain adaptation; (iii) we exploit the gaze redirection to improve the performance of person-specific gaze estimation. Extensive experiments on two public datasets demonstrate the validity of our gaze retargeting and gaze estimation framework. © 2019 IEEE.","And Body Pose; Face; Gesture; Image and Video Synthesis; Vision Applications and Systems","Computer vision; Deep neural networks; Large dataset; Sampling; Body pose; Face; Gesture; Video synthesis; Vision applications; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85078808349
"Wang K., Su H., Ji Q.","56637259500;57202802558;18935108400;","Neuro-inspired eye tracking with eye movement dynamics",2019,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June",,"8953735","9823","9832",,9,"10.1109/CVPR.2019.01006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078783804&doi=10.1109%2fCVPR.2019.01006&partnerID=40&md5=47c421f9813c76ad2fddae6a4d6bfbaa","RPI, United Kingdom; IBM, United States","Wang, K., RPI, United Kingdom; Su, H., RPI, United Kingdom, IBM, United States; Ji, Q., RPI, United Kingdom","Generalizing eye tracking to new subjects/environments remains challenging for existing appearance-based methods. To address this issue, we propose to leverage on eye movement dynamics inspired by neurological studies. Studies show that there exist several common eye movement types, independent of viewing contents and subjects, such as fixation, saccade, and smooth pursuits. Incorporating generic eye movement dynamics can therefore improve the generalization capabilities. In particular, we propose a novel Dynamic Gaze Transition Network (DGTN) to capture the underlying eye movement dynamics and serve as the topdown gaze prior. Combined with the bottom-up gaze measurements from the deep convolutional neural network, our method achieves better performance for both within-dataset and cross-dataset evaluations compared to state-of-the-art. In addition, a new DynamicGaze dataset is also constructed to study eye movement dynamics and eye gaze estimation. © 2019 IEEE.","And Body Pose; Face; Gesture","Computer vision; Convolutional neural networks; Deep neural networks; Dynamics; Eye tracking; Appearance-based methods; Body pose; Cross-dataset evaluation; Face; Generalization capability; Gesture; Smooth pursuit; State of the art; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85078783804
"Wang K., Zhao R., Su H., Ji Q.","56637259500;56461916600;57202802558;18935108400;","Generalizing eye tracking with bayesian adversarial learning",2019,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June",,"8953976","11899","11908",,12,"10.1109/CVPR.2019.01218","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078714537&doi=10.1109%2fCVPR.2019.01218&partnerID=40&md5=16970fbcb634c6dc04b201329b0aa55d","RPI; IBM","Wang, K., RPI; Zhao, R., RPI; Su, H., RPI, IBM; Ji, Q., RPI","Existing appearance-based gaze estimation approaches with CNN have poor generalization performance. By systematically studying this issue, we identify three major factors: 1) appearance variations; 2) head pose variations and 3) over-fitting issue with point estimation. To improve the generalization performance, we propose to incorporate adversarial learning and Bayesian inference into a unified framework. In particular, we first add an adversarial component into traditional CNN-based gaze estimator so that we can learn features that are gaze-responsive but can generalize to appearance and pose variations. Next, we extend the point-estimation based deterministic model to a Bayesian framework so that gaze estimation can be performed using all parameters instead of only one set of parameters. Besides improved performance on several benchmark datasets, the proposed method also enables online adaptation of the model to new subjects/environments, demonstrating the potential usage for practical real-time eye tracking applications. © 2019 IEEE.","And Body Pose; Face; Gesture","Bayesian networks; Benchmarking; Computer vision; Inference engines; Adversarial learning; Bayesian frameworks; Body pose; Deterministic modeling; Face; Generalization performance; Gesture; Real-time eye tracking; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85078714537
"Al-Btoush A.I., Abbadi M.A., Hassanat A.B., Tarawneh A.S., Hasanat A., Prasath V.B.S.","57215358929;57194447491;24343672100;57190020156;57215345783;24829700400;","New Features for Eye-Tracking Systems: Preliminary Results",2019,"2019 10th International Conference on Information and Communication Systems, ICICS 2019",,,"8809129","179","184",,,"10.1109/IACS.2019.8809129","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072130881&doi=10.1109%2fIACS.2019.8809129&partnerID=40&md5=c292daad5ee1ba36a9b43d37f25870fb","Faculty of Information Technology, Mutah University, Karak, Jordan; Department of Algorithm and Their Applications, Eötvös Loránd University, Budapest, Hungary; Queen Rania Hospital, Ministry of Health, Petra, Jordan; Division of Biomedical Informatics, Cincinnati Childrens Hospital Medical Center, Cincinnati, OH  45229, United States","Al-Btoush, A.I., Faculty of Information Technology, Mutah University, Karak, Jordan; Abbadi, M.A., Faculty of Information Technology, Mutah University, Karak, Jordan; Hassanat, A.B., Faculty of Information Technology, Mutah University, Karak, Jordan; Tarawneh, A.S., Department of Algorithm and Their Applications, Eötvös Loránd University, Budapest, Hungary; Hasanat, A., Queen Rania Hospital, Ministry of Health, Petra, Jordan; Prasath, V.B.S., Division of Biomedical Informatics, Cincinnati Childrens Hospital Medical Center, Cincinnati, OH  45229, United States","Due to their large number of applications, eye-tracking systems have gain attention recently. In this work, we propose 4 new features to support the most used feature by these systems, which is the location (x, y). These features are based on the white areas in the four corners of the sclera; the ratio of the whites area (after segmentation) to the corners area is used as a feature coming from each corner. In order to evaluate the new features, we designed a simple eye-tracking system using a simple webcam, where the users faces and eyes are detected, which allows for extracting the traditional and the new features. The system was evaluated using 10 subjects, who looked at 5 objects on the screen. The experimental results using some machine learning algorithms show that the new features are user dependent, and therefore, they cannot be used (in their current format) for a multiuser eye-tracking system. However, the new features might be used to support the traditional features for a better single-user eye-tracking system, where the accuracy results were in the range of 0.90 to 0.98. © 2019 IEEE.","Eye detection; Eye tracking; Eye-gaze; Face detection; Hough transform; Machine learning.","Data communication systems; Eye protection; Face recognition; Hough transforms; Learning algorithms; Learning systems; Machine learning; Eye detection; Eye tracking systems; Eye-gaze; Multi-user; Single users; User-dependent; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85072130881
"Rupanagudi S.R., Bhat V.G., Gurikar S.K., Pranava Koundinya S., Sumedh Kumar M.S., Shreyas R., Shilpa S., Suman N.M., Bademi R.R., Koppisetti M., Satyananda V.","55557953200;56267586000;57203065845;57210378399;57210368821;57193140831;57213638459;57210372650;57210379325;57203207426;57203140766;","A Video Processing Based Eye Gaze Recognition Algorithm for Wheelchair Control",2019,"Conference Proceedings of 2019 10th International Conference on Dependable Systems, Services and Technologies, DESSERT 2019",,,"8770025","241","247",,5,"10.1109/DESSERT.2019.8770025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070521750&doi=10.1109%2fDESSERT.2019.8770025&partnerID=40&md5=e528130c2d4e951480a4a2e606ca9403","WorldServe Education, Bangalore, India; Sai Vidya Institute of Technology, Bangalore, India; Gopalan College of Engineering Management, Bangalore, India; Atria Institute of Technology, Bangalore, India","Rupanagudi, S.R., WorldServe Education, Bangalore, India; Bhat, V.G., WorldServe Education, Bangalore, India; Gurikar, S.K., WorldServe Education, Bangalore, India; Pranava Koundinya, S., WorldServe Education, Bangalore, India; Sumedh Kumar, M.S., WorldServe Education, Bangalore, India; Shreyas, R., WorldServe Education, Bangalore, India; Shilpa, S., Sai Vidya Institute of Technology, Bangalore, India; Suman, N.M., Sai Vidya Institute of Technology, Bangalore, India; Bademi, R.R., Sai Vidya Institute of Technology, Bangalore, India; Koppisetti, M., Gopalan College of Engineering Management, Bangalore, India; Satyananda, V., Atria Institute of Technology, Bangalore, India","Over the past few years, a lot of research has been carried out in eye gaze recognition and its applications. From controlling wheelchairs to selecting options on a screen, utilizing the gaze of an individual has become a long-sought way for performing these tasks and in turn making the life of several differently abled people easy. In this paper a novel methodology to perform iris segmentation and gaze recognition has been introduced and described. The method elaborated utilizes a segmentation algorithm which can successfully extract the iris under varying lighting conditions with the help of machine learning. All experiments were conducted using the MATLAB R2013a software and a speed improvement of almost 3.433 times was achieved as opposed to other popular methods of iris extraction. In terms of accuracy, the algorithm proved to be 86% accurate and was also adopted to control an actual wheelchair. © 2019 IEEE.","Blink recognition; Eye gaze; Image processing; Iris detection; LDR; Machine learning; Regression; Segmentation; Video processing; Wheelchair movement","Eye movements; Food products; Image processing; Image segmentation; Learning systems; Machine learning; MATLAB; Wheelchairs; Blink recognition; Eye-gaze; Iris detection; Regression; Video processing; Video signal processing",Conference Paper,"Final","",Scopus,2-s2.0-85070521750
"Wu H., Lin Q., Yang R., Zhou Y., Zheng L., Huang Y., Wang Z., Lao Y., Huang J.","57208493587;56985002800;7403924515;57208479697;57220820440;57190310475;57189472105;55111946200;34975055200;","An Accurate Recognition of Infrared Retro-Reflective Markers in Surgical Navigation",2019,"Journal of Medical Systems","43","6","153","","",,4,"10.1007/s10916-019-1257-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064933811&doi=10.1007%2fs10916-019-1257-x&partnerID=40&md5=8eba1e2404c3608cf197ad0ec1f6b107","Department of Biomedical Engineering, South China University of Technology, Guangzhou, Guangdong, China; School of Medicine, Yale University, New Haven, CT  06520, United States; Guangdong Engineering Technology Research Center for Translational Medicine of Mental Disorders, Guangzhou, Guangdong, China; Guangzhou Aimooe Technology Co., Ltd., Guangzhou, Guangdong, China; Sun Yat-Sen University Cancer Center, Guangzhou, Guangdong, China","Wu, H., Department of Biomedical Engineering, South China University of Technology, Guangzhou, Guangdong, China; Lin, Q., Department of Biomedical Engineering, South China University of Technology, Guangzhou, Guangdong, China; Yang, R., Department of Biomedical Engineering, South China University of Technology, Guangzhou, Guangdong, China, School of Medicine, Yale University, New Haven, CT  06520, United States, Guangdong Engineering Technology Research Center for Translational Medicine of Mental Disorders, Guangzhou, Guangdong, China; Zhou, Y., Department of Biomedical Engineering, South China University of Technology, Guangzhou, Guangdong, China; Zheng, L., Department of Biomedical Engineering, South China University of Technology, Guangzhou, Guangdong, China; Huang, Y., Department of Biomedical Engineering, South China University of Technology, Guangzhou, Guangdong, China; Wang, Z., Guangzhou Aimooe Technology Co., Ltd., Guangzhou, Guangdong, China; Lao, Y., Department of Biomedical Engineering, South China University of Technology, Guangzhou, Guangdong, China; Huang, J., Sun Yat-Sen University Cancer Center, Guangzhou, Guangdong, China","Marker-based optical tracking systems (OTS) are widely used in clinical image-guided therapy. However, the emergence of ghost markers, which is caused by the mistaken recognition of markers and the incorrect correspondences between marker projections, may lead to tracking failures for these systems. Therefore, this paper proposes a strategy to prevent the emergence of ghost markers by identifying markers based on the features of their projections, finding the correspondences between marker projections based on the geometric information provided by markers, and fast-tracking markers in a 2D image between frames based on the sizes of their projections. Apart from validating its high robustness, the experimental results show that the proposed strategy can accurately recognize markers, correctly identify their correspondences, and meet the requirements of real-time tracking. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Accurate stereo-matching; Fast-tracking; Ghost-markers elimination; Marker recognition; Optical tracking system","article; eye tracking; human; infrared radiation; writing; algorithm; computer assisted surgery; optical instrumentation; procedures; Algorithms; Humans; Optical Devices; Surgery, Computer-Assisted",Article,"Final","",Scopus,2-s2.0-85064933811
"Sun X., Zhang L., Wang Z., Chang J., Yao Y., Li P., Zimmermann R.","7405624387;35231925400;57195973746;57202859501;57005815900;57210250079;55423994500;","Scene Categorization Using Deeply Learned Gaze Shifting Kernel",2019,"IEEE Transactions on Cybernetics","49","6","8358008","2156","2166",,12,"10.1109/TCYB.2018.2820731","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046814295&doi=10.1109%2fTCYB.2018.2820731&partnerID=40&md5=708f4698b8394c62d13a40c407732ff4","Department of CSIE, Hefei University of Technology, Hefei, 230009, China; School of Medical Information, Wannan Medical College Research Center of Health Big Data Mining and Applications, Wannan Medical College, Wuhu, 241002, China; State Grid Zhejiang Electric Power Company, Ltd., Information and Telecommunication Company, Quzhou, 132100, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, 310000, China; School of Computing, National University of Singapore, Singapore, Singapore","Sun, X., Department of CSIE, Hefei University of Technology, Hefei, 230009, China; Zhang, L., Department of CSIE, Hefei University of Technology, Hefei, 230009, China; Wang, Z., Department of CSIE, Hefei University of Technology, Hefei, 230009, China; Chang, J., School of Medical Information, Wannan Medical College Research Center of Health Big Data Mining and Applications, Wannan Medical College, Wuhu, 241002, China; Yao, Y., State Grid Zhejiang Electric Power Company, Ltd., Information and Telecommunication Company, Quzhou, 132100, China; Li, P., School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, 310000, China; Zimmermann, R., School of Computing, National University of Singapore, Singapore, Singapore","Accurately recognizing sophisticated sceneries from a rich variety of semantic categories is an indispensable component in many intelligent systems, e.g., scene parsing, video surveillance, and autonomous driving. Recently, there have emerged a large quantity of deep architectures for scene categorization, wherein promising performance has been achieved. However, these models cannot explicitly encode human visual perception toward different sceneries, i.e., the sequence of humans sequentially allocates their gazes. To solve this problem, we propose deep gaze shifting kernel to distinguish sceneries from different categories. Specifically, we first project regions from each scenery into the so-called perceptual space, which is established by combining color, texture, and semantic features. Then, a novel non-negative matrix factorization algorithm is developed which decomposes the regions' feature matrix into the product of the basis matrix and the sparse codes. The sparse codes indicate the saliency level of different regions. In this way, the gaze shifting path from each scenery is derived and an aggregation-based convolutional neural network is designed accordingly to learn its deep representation. Finally, the deep representations of gaze shifting paths from all the scene images are incorporated into an image kernel, which is further fed into a kernel SVM for scene categorization. Comprehensive experiments on six scenery data sets have demonstrated the superiority of our method over a series of shallow/deep recognition models. Besides, eye tracking experiments have shown that our predicted gaze shifting paths are 94.6% consistent with the real human gaze allocations. © 2013 IEEE.","Gaze shifting; image kernel; machine learning; non-negative matrix factorization (NMF); scene categorization","Eye tracking; Factorization; Feature extraction; Flow visualization; Image segmentation; Intelligent systems; Learning systems; Neural networks; Security systems; Semantics; Gaze shifting; Image color analysis; image kernel; Kernel; Nonnegative matrix factorization; Scene categorization; Sparse matrices; Matrix algebra",Article,"Final","",Scopus,2-s2.0-85046814295
"Wang Y., Huang R., Guo L.","15761455700;57205761321;56510659300;","Eye gaze pattern analysis for fatigue detection based on GP-BCNN with ESM",2019,"Pattern Recognition Letters","123",,,"61","74",,6,"10.1016/j.patrec.2019.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063356339&doi=10.1016%2fj.patrec.2019.03.013&partnerID=40&md5=19d8524951db0d455486b3ebd2624bdf","School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Big-Data Based Precision Medicine, Beihang University, Beijing, 100191, China","Wang, Y., School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China, Beijing Advanced Innovation Center for Big-Data Based Precision Medicine, Beihang University, Beijing, 100191, China; Huang, R., School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China; Guo, L., School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China","This paper presents a robust fatigue detection system based on binocular consistency, which integrates artificial modulation into deep learning to guide the learning process and removes the extreme cases of dynamic objects through screening mechanism. Specifically, we first build a dual-stream bidirectional convolutional neural network (BCNN) for eye gaze pattern detection, which uses binocular consistency for information interaction. Then we incorporate vectorized local integral projection features which named projection vectors and Gabor filters into BCNN to construct GP-BCNN that not only enhances the resistance of deep learned features to the orientation and scale changes, but strengthens the learning of texture information. Finally, an eye screening mechanism (ESM) based on pupil distance is proposed to eliminate the detected errors caused by the occluded eyes when the lateral face is detected. Demonstrated by introducing binocular consistency and artificial modulation to convolutional neural network (CNN), GP-BCNN improves the widely used CNNs architectures and yields a 2.9% promotion in the average accuracy rate compared with the results obtained by CNN alone. Our approach obtains the state-of-the-art results in fatigue detection and has the generalization potential in general image recognition tasks. © 2019 Elsevier B.V.","Artificial modulation; Convolutional neural network; Eye gaze pattern; Fatigue detection; Information interaction","Binoculars; Convolution; Deep learning; Gabor filters; Image recognition; Modulation; Neural networks; Textures; Artificial modulation; Convolutional neural network; Eye-gaze; Fatigue detection; Information interaction; Fatigue of materials",Article,"Final","",Scopus,2-s2.0-85063356339
"Lee S., Kang B., Nam D.","35778392800;36620258400;43761314400;","Position prediction for eye-tracking based 3D display",2019,"Proceedings Digital Holography and Three-Dimensional Imaging 2019",,,"DH-2019-W2A.6","","",,,"10.1364/DH.2019.W2A.6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085651593&doi=10.1364%2fDH.2019.W2A.6&partnerID=40&md5=8001765478afcc87c17a7897ce7cacb6","Multimedia Processing Lab, Suwon-si, South Korea","Lee, S., Multimedia Processing Lab, Suwon-si, South Korea; Kang, B., Multimedia Processing Lab, Suwon-si, South Korea; Nam, D., Multimedia Processing Lab, Suwon-si, South Korea","The accurate eye position prediction method is presented for eye-tracking based glasses free 3D display to reduced system latency effect on the 3D image quality. Proposed method is experimentally validated using 3D display prototype. © OSA 2019 © 2019 The Author(s)",,"Eye tracking; Holography; Imaging systems; 3-D displays; Eye position; Glasses-free 3-D displays; Position predictions; Reduced systems; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85085651593
"Kim J., Stengel M., Majercik A., De Mello S., Dunn D., Laine S., McGuire M., Luebke D.","57201935260;42162165500;57209395817;57201314496;57193954919;13008308700;16175848100;57204337568;","NVGaze: An anatomically-informed dataset for low-latency, near-eye gaze estimation",2019,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,34,"10.1145/3290605.3300780","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067624610&doi=10.1145%2f3290605.3300780&partnerID=40&md5=c60a8b332f6abd093916ae8726530399","NVIDIA, United States","Kim, J., NVIDIA, United States; Stengel, M., NVIDIA, United States; Majercik, A., NVIDIA, United States; De Mello, S., NVIDIA, United States; Dunn, D., NVIDIA, United States; Laine, S., NVIDIA, United States; McGuire, M., NVIDIA, United States; Luebke, D., NVIDIA, United States","Quality, diversity, and size of training data are critical factors for learning-based gaze estimators. We create two datasets satisfying these criteria for near-eye gaze estimation under infrared illumination: a synthetic dataset using anatomically-informed eye and face models with variations in face shape, gaze direction, pupil and iris, skin tone, and external conditions (2M images at 1280x960), and a real-world dataset collected with 35 subjects (2.5M images at 640x480). Using these datasets we train neural networks performing with sub-millisecond latency. Our gaze estimation network achieves 2.06(±0.44)◦ of accuracy across a wide 30◦ × 40◦ field of view on real subjects excluded from training and 0.5◦ best-case accuracy (across the same FOV) when explicitly trained for one real subject. We also train a pupil localization network which achieves higher robustness than previous methods. © 2019 Association for Computing Machinery.","Dataset; Eye tracking; Machine learning; Virtual reality","Eye tracking; Human computer interaction; Human engineering; Learning systems; Virtual reality; Critical factors; Dataset; External conditions; Field of views; Gaze direction; Gaze estimation; Infrared illumination; Pupil localization; Vanadium compounds",Conference Paper,"Final","",Scopus,2-s2.0-85067624610
"Hirzle T., Gugenheimer J., Geiselhart F., Bulling A., Rukzio E.","57203517985;55876769400;56940701200;6505807414;18233783900;","A design space for gaze interaction on head-mounted displays",2019,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,24,"10.1145/3290605.3300855","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067621120&doi=10.1145%2f3290605.3300855&partnerID=40&md5=9311602cae5ace0f46b9e50a113c9269","Institute of Media Informatics, Ulm University, Germany; Institute for Visualisation and Interactive Systems, University of Stuttgart, Germany","Hirzle, T., Institute of Media Informatics, Ulm University, Germany; Gugenheimer, J., Institute of Media Informatics, Ulm University, Germany; Geiselhart, F., Institute of Media Informatics, Ulm University, Germany; Bulling, A., Institute for Visualisation and Interactive Systems, University of Stuttgart, Germany; Rukzio, E., Institute of Media Informatics, Ulm University, Germany","Augmented and virtual reality (AR/VR) has entered the mass market and, with it, will soon eye tracking as a core technology for next generation head-mounted displays (HMDs). In contrast to existing gaze interfaces, the 3D nature of AR and VR requires estimating a user’s gaze in 3D. While first applications, such as foveated rendering, hint at the compelling potential of combining HMDs and gaze, a systematic analysis is missing. To fill this gap, we present the first design space for gaze interaction on HMDs. Our design space covers human depth perception and technical requirements in two dimensions aiming to identify challenges and opportunities for interaction design. As such, our design space provides a comprehensive overview and serves as an important guideline for researchers and practitioners working on gaze interaction on HMDs. We further demonstrate how our design space is used in practice by presenting two interactive applications: EyeHealth and XRay-Vision. © 2019 Copyright held by the owner/author(s).","3D gaze; Augmented reality; Design space; Gaze interaction; Head-mounted displays; Interaction design; Virtual reality","Augmented reality; Depth perception; Helmet mounted displays; Human computer interaction; Human engineering; Virtual reality; 3D gaze; Design spaces; Gaze interaction; Head mounted displays; Interaction design; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85067621120
"Mardanbegi D., Langlotz T., Gellersen H.","42761947400;8250843500;6701531333;","Resolving target ambiguity in 3D gaze interaction through VOR depth estimation",2019,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,11,"10.1145/3290605.3300842","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067610387&doi=10.1145%2f3290605.3300842&partnerID=40&md5=55bc08d1d362d914ff884c6f16316948","Lancaster University, Lancaster, United Kingdom; University of Otago, Dunedin, New Zealand","Mardanbegi, D., Lancaster University, Lancaster, United Kingdom; Langlotz, T., University of Otago, Dunedin, New Zealand; Gellersen, H., Lancaster University, Lancaster, United Kingdom","Target disambiguation is a common problem in gaze interfaces, as eye tracking has accuracy and precision limitations. In 3D environments this is compounded by objects overlapping in the field of view, as a result of their positioning at different depth with partial occlusion. We introduce VOR depth estimation, a method based on the vestibulo-ocular reflex of the eyes in compensation of head movement, and explore its application to resolve target ambiguity. The method estimates gaze depth by comparing the rotations of the eye and the head when the users look at a target and deliberately rotate their head. We show that VOR eye movement presents an alternative to vergence for gaze depth estimation, that is feasible also with monocular tracking. In an evaluation of its use for target disambiguation, our method outperforms vergence for targets presented at greater depth. © 2019 Association for Computing Machinery.","Depth estimation; Disambiguation; Eye tracking; Vergence; Vestibulo-ocular reflex","Eye movements; Human computer interaction; Human engineering; 3-D environments; Accuracy and precision; Depth Estimation; Disambiguation; Gaze interaction; Partial occlusions; Vergences; Vestibulo-ocular reflex; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85067610387
"Zhang X., Sugano Y., Bulling A.","57142162900;7005470045;6505807414;","Evaluation of appearance-based methods and implications for gaze-based applications",2019,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,25,"10.1145/3290605.3300646","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067609997&doi=10.1145%2f3290605.3300646&partnerID=40&md5=ec97f2ec63a6c234437b57ae9e1d8362","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Osaka University, Graduate School of Information Science and Technology, Japan; University of Stuttgart, Institute for Visualisation and Interactive Systems, Germany","Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Sugano, Y., Osaka University, Graduate School of Information Science and Technology, Japan; Bulling, A., University of Stuttgart, Institute for Visualisation and Interactive Systems, Germany","Appearance-based gaze estimation methods that only require an off-the-shelf camera have significantly improved but they are still not yet widely used in the human-computer interaction (HCI) community. This is partly because it remains unclear how they perform compared to model-based approaches as well as dominant, special-purpose eye tracking equipment. To address this limitation, we evaluate the performance of state-of-the-art appearance-based gaze estimation for interaction scenarios with and without personal calibration, indoors and outdoors, for different sensing distances, as well as for users with and without glasses. We discuss the obtained findings and their implications for the most important gaze-based applications, namely explicit eye input, attentive user interfaces, gaze-based user modelling, and passive eye monitoring. To democratise the use of appearance-based gaze estimation and interaction in HCI, we finally present OpenGaze (www.opengaze.org), the first software toolkit for appearance-based gaze estimation and interaction. © 2019 Copyright held by the owner/author(s).","Appearance-based gaze estimation; Model-based gaze estimation; OpenGaze; Software toolkit; Tobii eyex","Computer software; Gallium compounds; Human computer interaction; Human engineering; User interfaces; Appearance-based methods; Attentive user interfaces; Gaze estimation; Human computer interaction (HCI); Model based approach; OpenGaze; Software toolkits; Tobii eyex; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85067609997
"Wang X., Ley A., Koch S., Lindlbauer D., Hays J., Holmqvist K., Alexa M.","57190735109;55117014800;57198210324;55841358000;7102191400;8357720500;7003588954;","The mental image revealed by gaze tracking",2019,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,5,"10.1145/3290605.3300839","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067595316&doi=10.1145%2f3290605.3300839&partnerID=40&md5=3c1181cdc450f209073217f49b6f3081","TU Berlin, Germany; ETH Zurich, Switzerland; Georgia Institute of Technology, United States; Universität Regensburg, Germany","Wang, X., TU Berlin, Germany; Ley, A., TU Berlin, Germany; Koch, S., TU Berlin, Germany; Lindlbauer, D., TU Berlin, Germany, ETH Zurich, Switzerland; Hays, J., Georgia Institute of Technology, United States; Holmqvist, K., Universität Regensburg, Germany; Alexa, M., TU Berlin, Germany","Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked, but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely recall an image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that these results generalize to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario. © 2019 Association for Computing Machinery.","Eye tracking; Gaze pattern; Mental imagery","Eye movements; Gallium compounds; Human computer interaction; Human engineering; Image retrieval; Learning systems; Controlled experiment; Gaze pattern; Gaze tracking; Matching techniques; Mental imagery; Mental images; Physical activity; Realistic scenario; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85067595316
"Hiroe M., Nagamatsu T., Mitsunaga S.","57202892342;23398000100;57209308234;","Implicit user calibration for model-based gaze-tracking system using face detection around optical axis of eye",2019,"Conference on Human Factors in Computing Systems - Proceedings",,,"3312942","","",,,"10.1145/3290607.3312942","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067280600&doi=10.1145%2f3290607.3312942&partnerID=40&md5=59569de07d45d8a470fe6dba88bfedf0","Kobe University, Kobe, Japan; Kobe Univesity Kobe, Japan","Hiroe, M., Kobe University, Kobe, Japan; Nagamatsu, T., Kobe University, Kobe, Japan; Mitsunaga, S., Kobe Univesity Kobe, Japan","In recent studies of gaze tracking system using 3D model-based methods, the optical axis of the eye is estimated without user calibration. The remaining problem for achieving implicit user calibration is to estimate the difference between the optical axis and visual axis of the eye (angle Κ). In this paper, we propose an implicit user calibration method using face detection around the optical axis of the eye. We assume that the peak of the average of face region images indicates the visual axis of the eye in the eye coordinate system. The angle Κ is estimated as the difference between the optical axis of the eye and the peak of the average of face region images. We developed a prototype system with two cameras and two IR-LEDs. The experimental results showed that the proposed method can estimate the angle Κ more accurately than the method that uses Itti's saliency map instead of face detection. © 2019 Copyright held by the owner/author(s).","Calibration; Eye tracking; Face detection","3D modeling; Calibration; Face recognition; Human computer interaction; Human engineering; Vision; Eye coordinates; Face regions; Gaze tracking system; Model-based OPC; Optical axis; Prototype system; Saliency map; User calibration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85067280600
"Lee K.-F., Chen Y.-L., Yu C.-W., Wu C.-H., Hsiao C.-Y.","56413130800;35322122400;54379325600;57203205774;57215218409;","Low-cost Wearable Eye Gaze Detection and Tracking System",2019,"2019 IEEE International Conference on Consumer Electronics - Taiwan, ICCE-TW 2019",,,"8991784","","",,1,"10.1109/ICCE-TW46550.2019.8991784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080126840&doi=10.1109%2fICCE-TW46550.2019.8991784&partnerID=40&md5=96e37b504f11e61caa3657a0eedbd4de","National Taipei University of Technology, Dept. Computer Science and Information Engineering, Taipei, Taiwan","Lee, K.-F., National Taipei University of Technology, Dept. Computer Science and Information Engineering, Taipei, Taiwan; Chen, Y.-L., National Taipei University of Technology, Dept. Computer Science and Information Engineering, Taipei, Taiwan; Yu, C.-W., National Taipei University of Technology, Dept. Computer Science and Information Engineering, Taipei, Taiwan; Wu, C.-H., National Taipei University of Technology, Dept. Computer Science and Information Engineering, Taipei, Taiwan; Hsiao, C.-Y., National Taipei University of Technology, Dept. Computer Science and Information Engineering, Taipei, Taiwan","This study use headset is adaptable by integration of the elastic mechanism design. This proposed system can effectively extract and estimate pupil ellipse from few camera-captured samples of an eye, and compute the corresponding 3D eye model. Then match the later pupil ellipse to give the possible visual angle. We use multiple points calibration method to solve the related polynomial formula for future angle-to-gaze mapping. The proposed eye tracking algorithms can provide a low-complexity solution and provide high accuracy precision and speed. © 2019 IEEE.",,"3D modeling; Costs; Machine design; Wearable technology; 3D eye models; Calibration method; Elastic mechanism; Eye gaze detection; High-accuracy; Low costs; Multiple points; Visual angle; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85080126840
"Shafti A., Orlov P., Faisal A.A.","56183259000;56319626100;6602900233;","Gaze-based, context-aware robotic system for assisted reaching and grasping",2019,"Proceedings - IEEE International Conference on Robotics and Automation","2019-May",,"8793804","863","869",,11,"10.1109/ICRA.2019.8793804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071479396&doi=10.1109%2fICRA.2019.8793804&partnerID=40&md5=3385e43ba2ecee70465a4f054ab14739","Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, United Kingdom","Shafti, A., Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, United Kingdom; Orlov, P., Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, United Kingdom; Faisal, A.A., Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, United Kingdom","Assistive robotic systems endeavour to support those with movement disabilities, enabling them to move again and regain functionality. Main issue with these systems is the complexity of their low-level control, and how to translate this to simpler, higher level commands that are easy and intuitive for a human user to interact with. We have created a multi-modal system, consisting of different sensing, decision making and actuating modalities, leading to intuitive, human-in-the-loop assistive robotics. The system takes its cue from the user's gaze, to decode their intentions and implement low-level motion actions to achieve high-level tasks. This results in the user simply having to look at the objects of interest, for the robotic system to assist them in reaching for those objects, grasping them, and using them to interact with other objects. We present our method for 3D gaze estimation, and grammars-based implementation of sequences of action with the robotic system. The 3D gaze estimation is evaluated with 8 subjects, showing an overall accuracy of 4.68pm 0.14cm. The full system is tested with 5 subjects, showing successful implementation of 100% of reach to gaze point actions and full implementation of pick and place tasks in 96%, and pick and pour tasks in 76% of cases. Finally we present a discussion on our results and what future work is needed to improve the system. © 2019 IEEE.",,,Conference Paper,"Final","",Scopus,2-s2.0-85071479396
"Li B., Zhang Y., Zheng X., Huang X., Zhang S., He J.","57102055600;57016903800;57187383000;55500177600;57208438931;57210368932;","A Smart Eye Tracking System for Virtual Reality",2019,"IEEE MTT-S 2019 International Microwave Biomedical Conference, IMBioC 2019 - Proceedings",,,"8777841","","",,1,"10.1109/IMBIOC.2019.8777841","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070500169&doi=10.1109%2fIMBIOC.2019.8777841&partnerID=40&md5=12999c31a8f84ef9f11cc74ec6f4559d","Northwest University, School of Information Science and Technology, Xi'an, Shaanxi, China; Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China; Sichuan University, College of Electrical Engineering and Information Technology, Chengdu, Sichuan, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China","Li, B., Northwest University, School of Information Science and Technology, Xi'an, Shaanxi, China; Zhang, Y., Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China; Zheng, X., Sichuan University, College of Electrical Engineering and Information Technology, Chengdu, Sichuan, China; Huang, X., School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China; Zhang, S., Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China; He, J., Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China","Virtual reality (VR) technology provides specific three-dimensional (3D) scenes for users, which could be benefit for the applications in the field of medical diagnosis, psychological analysis, cognitive researches and entertainments. The current VR device provides the virtual 3D images, but cannot synchronously detect the user's eye movements which could be significant for deriving the users' gaze points and interest regions in varied applications. In this paper, we proposed a smart eye tracking system for VR device. Firstly, a smart eye movement detection kit is mounted inside the VR device to capture the human eye movement images with the rate of 30 Hz. Based on the proposed eye detection kit, a gaze detection algorithm is then established for VR devices. Next, a gaze estimation model is proposed for human gaze estimation. The proposed smart eye tracking system can detect the user's eye movements in real time under VR stimulation. Moreover, it also provide a new human-VR interaction mode. © 2019 IEEE.",,"Diagnosis; Eye movements; Eye protection; Virtual reality; Eye tracking systems; Gaze detection; Gaze estimation; Interaction modes; Interest regions; Movement detection; Psychological analysis; Threedimensional (3-d); Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85070500169
"Jha S., Busso C.","57193014012;35742852700;","Estimation of Gaze Region Using Two Dimensional Probabilistic Maps Constructed Using Convolutional Neural Networks",2019,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May",,"8683794","3792","3796",,,"10.1109/ICASSP.2019.8683794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069529490&doi=10.1109%2fICASSP.2019.8683794&partnerID=40&md5=a22c0c4b917196cf7ee4fb9919a2be76","Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, Department of Electrical Computer Engineering, Richardson, TX  75080, United States","Jha, S., Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, Department of Electrical Computer Engineering, Richardson, TX  75080, United States; Busso, C., Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, Department of Electrical Computer Engineering, Richardson, TX  75080, United States","Predicting the gaze of a user can have important applications in human computer interactions (HCI). They find applications in areas such as social interaction, driver distraction, human robot interaction and education. Appearance based models for gaze estimation have significantly improved due to recent advances in convolutional neural network (CNN). This paper proposes a method to predict the gaze of a user with deep models purely based on CNNs. A key novelty of the proposed model is that it produces a probabilistic map describing the gaze distribution (as opposed to predicting a single gaze direction). This approach is achieved by converting the regression problem into a classification problem, predicting the probability at the output instead of a single direction. The framework relies in a sequence of downsampling followed by upsampling to obtain the probabilistic gaze map. We observe that our proposed approach works better than a regression model in terms of prediction accuracy. The average mean squared error between the predicted gaze and the true gaze is observed to be 6.89 in a model trained and tested on the MSP-Gaze database, without any calibration or adaptation to the target user. © 2019 IEEE.","Convolutional neural networks; gaze estimation; regression by classification","Audio signal processing; Convolution; Driver training; Forecasting; Human computer interaction; Mean square error; Neural networks; Probability distributions; Regression analysis; Signal sampling; Speech communication; Appearance-based models; Convolutional neural network; Driver distractions; Gaze estimation; Human computer interaction (HCI); Prediction accuracy; Probabilistic maps; Social interactions; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85069529490
"Causse M., Lancelot F., Maillant J., Behrend J., Cousy M., Schneider N.","35084804500;55339696900;57205214703;57205965142;57190617327;56946329300;","Encoding decisions and expertise in the operator's eyes: Using eye-tracking as input for system adaptation",2019,"International Journal of Human Computer Studies","125",,,"55","65",,10,"10.1016/j.ijhcs.2018.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059175807&doi=10.1016%2fj.ijhcs.2018.12.010&partnerID=40&md5=4a4464a294ef063ee482813643746cf2","ISAE-SUPAERO, Université de Toulouse, France; Airbus Group Innovations, Toulouse, France; Laboratoire de neurosciences cognitives, Département d’études cognitives, École normale supérieure, INSERM, PSL Research University, Paris, 75005, France; ENAC, University of Toulouse, France","Causse, M., ISAE-SUPAERO, Université de Toulouse, France; Lancelot, F., ISAE-SUPAERO, Université de Toulouse, France, Airbus Group Innovations, Toulouse, France; Maillant, J., ISAE-SUPAERO, Université de Toulouse, France; Behrend, J., Laboratoire de neurosciences cognitives, Département d’études cognitives, École normale supérieure, INSERM, PSL Research University, Paris, 75005, France; Cousy, M., ENAC, University of Toulouse, France; Schneider, N., Airbus Group Innovations, Toulouse, France","We investigated the possibility of developing a decision support system (DSS) that integrates eye-fixation measurements to better adapt its suggestions. Indeed, eye fixation give insight into human decision-making: Individuals tend to pay more attention to key information in line with their upcoming selection. Thus, eye-fixation measures can help the DSS to better capture the context that determines user decisions. Twenty-two participants performed a simplified Air Traffic Control (ATC) simulation in which they had to decide to accept or to modify route suggestions according to specific parameter values displayed on the screen. Decisions and fixation times on each parameter were recorded. The user fixation times were used by an algorithm to estimate the utility of each parameter for its decision. Immediately after this training phase, the algorithm generated new route suggestions under two conditions: 1) Taking into account the participant's decisions, 2) Taking into account the participant's decisions plus their visual behavior using the measurements of dwell times on displayed parameters. Results showed that system suggestions were more accurate than the base system when taking into account the participant's decisions, and even more accurate using their dwell times. Capturing the crucial information for the decision using the eye tracker accelerated the DSS learning phase, and thus helped to further enhance the accuracy of consecutive suggestions. Moreover, exploratory eye-tracking analysis reflected two different stages of the decision-making process, with longer dwell times on relevant parameters (i.e. involved in a rule) during the entire decision time course, and frequency of fixations on these relevant parameters that increased, especially during the last fixations prior to the decision. Consequently, future DSS integrating eye-tracking data should pay specific care to the final fixations prior to the decision. In general, our results emphasize the potential interest of eye-tracking to enhance and accelerate system adaptation to user preference, knowledge, and expertise. © 2018","Adaptive system; Air Traffic Control; Case-Based Reasoning; Decision-making; Eye-tracking; Human factors; Machine learning","Adaptive control systems; Adaptive systems; Air navigation; Air traffic control; Behavioral research; Case based reasoning; Control towers; Decision making; Decision support systems; Human engineering; Learning systems; Parameter estimation; Signal encoding; Air traffic control (ATC); Decision making process; Decision support system (dss); Different stages; Eye-tracking analysis; Human decision making; Specific parameter values; System adaptation; Eye tracking",Article,"Final","",Scopus,2-s2.0-85059175807
"Jiang B., Zhang Y., Tang J., Luo B., Li C.","56890202300;56496788300;24286986300;56245740100;56699429900;","Robust visual tracking via Laplacian Regularized Random Walk Ranking",2019,"Neurocomputing","339",,,"139","148",,8,"10.1016/j.neucom.2019.01.102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061837201&doi=10.1016%2fj.neucom.2019.01.102&partnerID=40&md5=8ff93306ab0fc4f30c782a7feab29688","School of Computer Science and Technology, Anhui University, Hefei, China","Jiang, B., School of Computer Science and Technology, Anhui University, Hefei, China; Zhang, Y., School of Computer Science and Technology, Anhui University, Hefei, China; Tang, J., School of Computer Science and Technology, Anhui University, Hefei, China; Luo, B., School of Computer Science and Technology, Anhui University, Hefei, China; Li, C., School of Computer Science and Technology, Anhui University, Hefei, China","Visual tracking is a fundamental and important problem in computer vision and pattern recognition. Existing visual tracking methods usually localize the visual object with a bounding box. Recently, learning the patch-based weighted features has been demonstrated to be an effective way to mitigate the background effects in the target bounding box descriptions, and can thus improve tracking performance significantly. In this paper, we propose a simple yet effective approach, called Laplacian Regularized Random Walk Ranking (LRWR), to learn more robust patch-based weighted features of the target object for visual tracking. The main advantages of our LRWR model over existing methods are: (1) it integrates both local spatial and global appearance cues simultaneously, and thus leads to a more robust solution for patch weight computation; (2) it has a simple closed-form solution, which makes our tracker efficiently. The learned features are incorporated into the structured SVM to perform object tracking. Experiments show that our approach performs favorably against the state-of-the-art trackers on two standard benchmark datasets. © 2019 Elsevier B.V.","Laplacian regularization; Random walk; Structured SVM; Visual tracking","Image processing; Laplace transforms; Pattern recognition; Random processes; Closed form solutions; Effective approaches; Global appearances; Laplacian regularizations; Random Walk; Structured SVM; Tracking performance; Visual Tracking; Target tracking; article; eye tracking",Article,"Final","",Scopus,2-s2.0-85061837201
"O'Reilly J., Khan A.S., Li Z., Cai J., Hu X., Chen M., Tong Y.","57195295644;57189308442;57202807301;57195222772;57208721397;57193831567;8644533500;","A Novel Remote Eye Gaze Tracking System Using Line Illumination Sources",2019,"Proceedings - 2nd International Conference on Multimedia Information Processing and Retrieval, MIPR 2019",,,"8695323","449","454",,2,"10.1109/MIPR.2019.00090","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065616181&doi=10.1109%2fMIPR.2019.00090&partnerID=40&md5=689c8476cdd12a4ac3da3a2cb122be7b","Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; University of Washington, Bothell, United States","O'Reilly, J., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; Khan, A.S., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; Li, Z., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; Cai, J., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; Hu, X., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; Chen, M., University of Washington, Bothell, United States; Tong, Y., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States","This paper proposes a novel system to estimate the 3D point of gaze using observations of the pupil center and corneal reflections (glints). A mathematical model is developed with two solutions to estimate the corneal center efficiently using lines of LED lights. Differing from existing 3D approaches requiring associating light sources with glints, the model automatically associates glints and LED lines and can handle missing glints well. The new model also enables a user-friendly calibration process, allowing natural head movement. Experiments demonstrate that the proposed system can achieve accurate gaze estimation with natural head movement. The performance is impressive when using the natural calibration, requiring less user cooperation. © 2019 IEEE.","Eye Tracking; Gaze Estimation; Robust; User Friendly Calibration","Calibration; Estimation; Light emitting diodes; Calibration process; Corneal reflection; Eye gaze tracking; Gaze estimation; Illumination sources; Robust; User cooperation; User friendly; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85065616181
"Veliyath N., De P., Allen A.A., Hodges C.B., Mitra A.","57208835879;7101660853;24821826100;27267668300;7402542590;","Modeling students’ attention in the classroom using eyetrackers",2019,"ACMSE 2019 - Proceedings of the 2019 ACM Southeast Conference",,,,"2","9",,7,"10.1145/3299815.3314424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065911819&doi=10.1145%2f3299815.3314424&partnerID=40&md5=285ee5f4f600601cd82775c8e2ae4488","Computer Science, Georgia Southern University, United States; College of Education, Georgia Southern University, United States; Mechanical Engineering, Georgia Southern University, United States","Veliyath, N., Computer Science, Georgia Southern University, United States; De, P., Computer Science, Georgia Southern University, United States; Allen, A.A., Computer Science, Georgia Southern University, United States; Hodges, C.B., College of Education, Georgia Southern University, United States; Mitra, A., Mechanical Engineering, Georgia Southern University, United States","The process of learning is not merely determined by what the instructor teaches, but also by how the student receives that information. An attentive student will naturally be more open to obtaining knowledge than a bored or frustrated student. In recent years, tools such as skin temperature measurements and body posture calculations have been developed for the purpose of determining a student’s affect, or emotional state of mind. However, measuring eye-gaze data is particularly noteworthy in that it can collect measurements non-intrusively, while also being relatively simple to set up and use. This paper details how data obtained from an eye-tracker can indeed be used to predict a student’s attention as a measure of affect over the course of a class. From this research, an accuracy of 77% was achieved using the Extreme Gradient Boosting technique of machine learning. The outcome indicates that eye-gaze can be indeed used as a basis for constructing a predictive model. © 2019 Association for Computing Machinery.","Affective computing; Attention; Eyetracking; Machine learning","Adaptive boosting; Eye tracking; Learning systems; Machine learning; Temperature measurement; Affective Computing; Attention; Body postures; Emotional state; Eye trackers; Gradient boosting; Predictive modeling; Process of learning; Students",Conference Paper,"Final","",Scopus,2-s2.0-85065911819
"Song S., Yamada S.","57191886833;35418721700;","Designing LED lights for a robot to communicate gaze",2019,"Advanced Robotics","33","7-8",,"360","368",,3,"10.1080/01691864.2019.1600426","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064165246&doi=10.1080%2f01691864.2019.1600426&partnerID=40&md5=6030e0efc45adc8c2833b17b708a3c64","The Graduate University for Advanced Studies (SOKENDAI), Tokyo, Japan; National Institute of Informatics and SOKENDAI, Tokyo, Japan","Song, S., The Graduate University for Advanced Studies (SOKENDAI), Tokyo, Japan; Yamada, S., National Institute of Informatics and SOKENDAI, Tokyo, Japan","Eye gaze is considered to be a particularly important non-verbal communication cue. Gaze research is also becoming a hot topic in human–robot interaction (HRI). However, research on social eye gaze for HRI focuses mainly on human-like robots. There remains a lack of methods for functional robots, which are constrained in appearance, to show gaze-like behavior. In this work, we investigate how we can implement gaze behavior in functional robots to assist humans in reading their intent. We explore design implications based on LED lights as we consider LEDs to be easily installed in most robots while not introducing features that are too human-like (to prevent users from having high expectations towards the robots). In this paper, we first developed a design interface that allows designers to freely test different parameter settings for an LED-based gaze display for a Roomba robot. We summarized design principles for well simulating LED-based gazes. Our suggested design is further evaluated by a large group of participants with regard to their perception and interpretation of the robot's behaviors. On the basis of the findings, we finally offer a set of design implications that can be beneficial to HRI and HCI researchers. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group and The Robotics Society of Japan.","appearance-constrained robot; expressive lights; Gaze; human–robot interaction (HRI)","Light emitting diodes; Machine design; Constrained robots; Design implications; Design Principles; Gaze; Human like robots; Non-verbal communications; Parameter setting; Robot interactions; Human robot interaction",Article,"Final","",Scopus,2-s2.0-85064165246
"Liu Y., Lee B.-S., Rajan D., Sluzek A., McKeown M.J.","57192561421;7405441352;7005909381;6701500691;7005375626;","CamType: assistive text entry using gaze with an off-the-shelf webcam",2019,"Machine Vision and Applications","30","3",,"407","421",,4,"10.1007/s00138-018-00997-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061457966&doi=10.1007%2fs00138-018-00997-4&partnerID=40&md5=569480a52c8439f75a2ee8da245a3cb8","Nanyang Institute of Technology in Health and Medicine, Interdisciplinary Graduate School, Nanyang Technological University, 50 Nanyang Avenue, Singapore, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Department of Electrical and Computer Engineering, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Medicine, The University of British Columbia, Vancouver, Canada","Liu, Y., Nanyang Institute of Technology in Health and Medicine, Interdisciplinary Graduate School, Nanyang Technological University, 50 Nanyang Avenue, Singapore, Singapore; Lee, B.-S., School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Rajan, D., School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Sluzek, A., Department of Electrical and Computer Engineering, Khalifa University, Abu Dhabi, United Arab Emirates; McKeown, M.J., Department of Medicine, The University of British Columbia, Vancouver, Canada","As modern assistive technology advances, eye-based text entry systems have been developed to help a subset of physically challenged people to improve their communication ability. However, speed of text entry in early eye-typing system tends to be relatively slow due to dwell time. Recently, dwell-free methods have been proposed which outperform the dwell-based systems in terms of speed and resilience, but the extra eye-tracking device is still an indispensable equipment. In this article, we propose a prototype of eye-typing system using an off-the-shelf webcam without the extra eye tracker, in which the appearance-based method is proposed to estimate people’s gaze coordinates on the screen based on the frontal face images captured by the webcam. We also investigate some critical issues of the appearance-based method, which helps to improve the estimation accuracy and reduce computing complexity in practice. The performance evaluation shows that eye typing with webcam using the proposed method is comparable to the eye tracker under a small degree of head movement. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Appearance-based method; Assistive technology; Dwell-free methods; Eye-typing system","Eye movements; Appearance-based methods; Assistive technology; Computing complexity; Critical issues; Dwell-free methods; Eye tracking devices; Text entry systems; Typing systems; Eye tracking",Article,"Final","",Scopus,2-s2.0-85061457966
"Liao H., Dong W., Huang H., Gartner G., Liu H.","56059959800;22233370800;16417466100;23049699400;37018507600;","Inferring user tasks in pedestrian navigation from eye movement data in real-world environments",2019,"International Journal of Geographical Information Science","33","4",,"739","763",,26,"10.1080/13658816.2018.1482554","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049076929&doi=10.1080%2f13658816.2018.1482554&partnerID=40&md5=70a545bcc53549f6d1746bda58e847b5","State Key Laboratory of Remote Sensing Science, Beijing Key Laboratory for Remote Sensing of Environment and Digital Cities, and Faculty of Geographical Science, Beijing Normal University, Beijing, China; Department of Geodesy and Geoinformation, Vienna University of Technology, Vienna, Austria; GIScience Center, Department of Geography, University of Zurich, Zurich, Switzerland","Liao, H., State Key Laboratory of Remote Sensing Science, Beijing Key Laboratory for Remote Sensing of Environment and Digital Cities, and Faculty of Geographical Science, Beijing Normal University, Beijing, China, Department of Geodesy and Geoinformation, Vienna University of Technology, Vienna, Austria; Dong, W., State Key Laboratory of Remote Sensing Science, Beijing Key Laboratory for Remote Sensing of Environment and Digital Cities, and Faculty of Geographical Science, Beijing Normal University, Beijing, China; Huang, H., GIScience Center, Department of Geography, University of Zurich, Zurich, Switzerland; Gartner, G., Department of Geodesy and Geoinformation, Vienna University of Technology, Vienna, Austria; Liu, H., State Key Laboratory of Remote Sensing Science, Beijing Key Laboratory for Remote Sensing of Environment and Digital Cities, and Faculty of Geographical Science, Beijing Normal University, Beijing, China","Eye movement data convey a wealth of information that can be used to probe human behaviour and cognitive processes. To date, eye tracking studies have mainly focused on laboratory-based evaluations of cartographic interfaces; in contrast, little attention has been paid to eye movement data mining for real-world applications. In this study, we propose using machine-learning methods to infer user tasks from eye movement data in real-world pedestrian navigation scenarios. We conducted a real-world pedestrian navigation experiment in which we recorded eye movement data from 38 participants. We trained and cross-validated a random forest classifier for classifying five common navigation tasks using five types of eye movement features. The results show that the classifier can achieve an overall accuracy of 67%. We found that statistical eye movement features and saccade encoding features are more useful than the other investigated types of features for distinguishing user tasks. We also identified that the choice of classifier, the time window size and the eye movement features considered are all important factors that influence task inference performance. Results of the research open doors to some potential real-world innovative applications, such as navigation systems that can provide task-related information depending on the task a user is performing. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.","eye tracking; machine learning; random forests; task inference; Wayfinding","cartography; data mining; forest; machine learning; navigation; pedestrian",Article,"Final","",Scopus,2-s2.0-85049076929
"Modak M., Ghotane K., Siddhanth V., Kelkar N., Iyer A., Prachi G.","57189037840;57210798336;57210804462;57210802266;57210806114;57210803690;","Detection of dyslexia using eye tracking measures",2019,"International Journal of Innovative Technology and Exploring Engineering","8","6 Special Issue 4",,"1011","1014",,,"10.35940/ijitee.F1208.0486S419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071430737&doi=10.35940%2fijitee.F1208.0486S419&partnerID=40&md5=be1372b186f1a856ac5b9372452a435e","Department of Computer Engineering, SIES Graduate School of Technology, Nerul., Mumbai, Maharashtra, India; Department of Computer Engineering, SIES Graduate School of Technology, Nerul., India; Department of Computer Engineering, Sardar Patel Institute of Technology, Andheri, Mumbai, Maharashtra, India","Modak, M., Department of Computer Engineering, SIES Graduate School of Technology, Nerul., Mumbai, Maharashtra, India; Ghotane, K., Department of Computer Engineering, SIES Graduate School of Technology, Nerul., Mumbai, Maharashtra, India; Siddhanth, V., Department of Computer Engineering, SIES Graduate School of Technology, Nerul., Mumbai, Maharashtra, India; Kelkar, N., Department of Computer Engineering, SIES Graduate School of Technology, Nerul., Mumbai, Maharashtra, India; Iyer, A., Department of Computer Engineering, SIES Graduate School of Technology, Nerul., India; Prachi, G., Department of Computer Engineering, Sardar Patel Institute of Technology, Andheri, Mumbai, Maharashtra, India","Dyslexia is one of the most common and hidden learning disabilities found in people, especially in the young age. It particularly affects reading, where the impaired reader takes a longer time to read and grasp the concept than the non-impaired reader. This further leads to academic failures. So studies to detect such issues have been conducted considering various factors like the reading times, fixation times, number of saccades(sudden movements in the eye), of both the impaired and non-impaired subjects, and give the best possible results. Thus, we plan to use the same eye tracking technique supported with machine learning models to detect and classify the individuals with and without dyslexia. The factors considered during the study are font-size, typeface, frequency of words(fixation times of non-impaired readers are more if frequency of encountered words is less) and age(people with learning disorders tend to enhance their reading skills with age), etc. © BEIESP.","Detection; Diagnosis; Dyslexia; Eye movements; Eye tracking; Machine learning; Prediction; Support vector machine",,Article,"Final","",Scopus,2-s2.0-85071430737
"Rehman H.U., Naeem M., Khan M., Sikander G., Anwar S.","57208408529;57208391573;57208395520;56038589900;56970729700;","Eye Tracking based Real- Time Non-Interfering Driver Fatigue Detection System",2019,"Proceedings of the 10th International Conference on Electronics, Computers and Artificial Intelligence, ECAI 2018",,,"8678951","","",,,"10.1109/ECAI.2018.8678951","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064650491&doi=10.1109%2fECAI.2018.8678951&partnerID=40&md5=b2822bf443c20f5d85f1dd0a1abc1432","Department of Mechatronics Engineering, University of Engineering and Technology, Peshawar, Pakistan","Rehman, H.U., Department of Mechatronics Engineering, University of Engineering and Technology, Peshawar, Pakistan; Naeem, M., Department of Mechatronics Engineering, University of Engineering and Technology, Peshawar, Pakistan; Khan, M., Department of Mechatronics Engineering, University of Engineering and Technology, Peshawar, Pakistan; Sikander, G., Department of Mechatronics Engineering, University of Engineering and Technology, Peshawar, Pakistan; Anwar, S., Department of Mechatronics Engineering, University of Engineering and Technology, Peshawar, Pakistan","Driver fatigue, cause fatal road accidents, and is a major socio-economic concern. The human body exhibits signs of fatigue, which can provide us with the information needed for fatigue detection. This study presents to develop an eye tracking based real time noninterfering driver fatigue detection system. In the proposed method Haar-like features based cascade classifiers are utilized to detect the face and eyes of the driver, skin colour based segmentation is used to calculate eye openness and Support Vector Machine (SVM) to deduce the eyes state as fatigued or non-fatigued. © 2018 IEEE.","Component; Fatigue detection; Machine learning; Skin colour segmentation; SVM; Viola jones","Artificial intelligence; Classification (of information); Learning systems; Support vector machines; Cascade classifiers; Component; Fatal road accidents; Fatigue detection; Haar-like features; Skin-colour segmentation; Socio-economics; Viola jones; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85064650491
"Yuan L., Reardon C., Warnell G., Loianno G.","57206466650;7005435537;26768341700;55209864600;","Human gaze-driven spatial tasking of an autonomous MAV",2019,"IEEE Robotics and Automation Letters","4","2","8626140","1343","1350",,7,"10.1109/LRA.2019.2895419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063310637&doi=10.1109%2fLRA.2019.2895419&partnerID=40&md5=1eeed636367e44ba9bb21a8d2bb7a717","GRASP Laboratory, University of Pennsylvania, Philadelphia, PA  19104, United States; U.S. Army Research Laboratory, Adelphi, MD  20783, United States; New York University, Tandon School of Engineering, Brooklyn, NY  11201, United States","Yuan, L., GRASP Laboratory, University of Pennsylvania, Philadelphia, PA  19104, United States; Reardon, C., U.S. Army Research Laboratory, Adelphi, MD  20783, United States; Warnell, G., U.S. Army Research Laboratory, Adelphi, MD  20783, United States; Loianno, G., New York University, Tandon School of Engineering, Brooklyn, NY  11201, United States","In this letter, we address the problem of providing human-assisted quadrotor navigation using a set of eye tracking glasses. The advent of these devices (i.e., eye tracking glasses, virtual reality tools, etc.) provides the opportunity to create new, noninvasive forms of interaction between humans and robots. We show how a set of glasses equipped with gaze tracker, a camera, and an inertial measurement unit (IMU) can be used to estimate the relative position of the human with respect to a quadrotor, and decouple the gaze direction from the head orientation, which allows the human to spatially task (i.e., send new 3-D navigation waypoints to) the robot in an uninstrumented environment. We decouple the gaze direction from head motion by tracking the human's head orientation using a combination of camera and IMU data. In order to detect the flying robot, we train and use a deep neural network. We experimentally evaluate the proposed approach, and show that our pipeline has the potential to enable gaze-driven autonomy for spatial tasking. The proposed approach can be employed in multiple scenarios including inspection and first response, as well as by people with disabilities that affect their mobility. © 2016 IEEE.",,"Cameras; Deep neural networks; Glass; Human robot interaction; Virtual reality; 3D navigation; Flying robots; Gaze direction; Gaze tracker; Inertial measurement unit; People with disabilities; Relative positions; Virtual reality tools; Eye tracking",Article,"Final","",Scopus,2-s2.0-85063310637
"Cercenelli L., Tiberi G., Bortolani B., Giannaccare G., Fresina M., Campos E., Marcelli E.","12777290700;57192114303;57194708553;55753474700;9435309300;57190684719;6701495843;","Gaze Trajectory Index (GTI): A novel metric to quantify saccade trajectory deviation using eye tracking",2019,"Computers in Biology and Medicine","107",,,"86","96",,2,"10.1016/j.compbiomed.2019.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061663543&doi=10.1016%2fj.compbiomed.2019.02.003&partnerID=40&md5=81df6ed64bf55e762de0de06e16b09d1","Laboratory of Bioengineering, Experimental Diagnostic and Specialty Medicine Dept. (DIMES), University of Bologna, Policlinico S. Orsola Malpighi, Via Massarenti 9, Bologna, 40138, Italy; Ophthalmology Unit, Experimental Diagnostic and Specialty Medicine Dept. (DIMES), University of Bologna, Policlinico S. Orsola Malpighi and S. Orsola-Malpighi Teaching Hospital, Bologna, Italy","Cercenelli, L., Laboratory of Bioengineering, Experimental Diagnostic and Specialty Medicine Dept. (DIMES), University of Bologna, Policlinico S. Orsola Malpighi, Via Massarenti 9, Bologna, 40138, Italy; Tiberi, G., Laboratory of Bioengineering, Experimental Diagnostic and Specialty Medicine Dept. (DIMES), University of Bologna, Policlinico S. Orsola Malpighi, Via Massarenti 9, Bologna, 40138, Italy; Bortolani, B., Laboratory of Bioengineering, Experimental Diagnostic and Specialty Medicine Dept. (DIMES), University of Bologna, Policlinico S. Orsola Malpighi, Via Massarenti 9, Bologna, 40138, Italy; Giannaccare, G., Ophthalmology Unit, Experimental Diagnostic and Specialty Medicine Dept. (DIMES), University of Bologna, Policlinico S. Orsola Malpighi and S. Orsola-Malpighi Teaching Hospital, Bologna, Italy; Fresina, M., Ophthalmology Unit, Experimental Diagnostic and Specialty Medicine Dept. (DIMES), University of Bologna, Policlinico S. Orsola Malpighi and S. Orsola-Malpighi Teaching Hospital, Bologna, Italy; Campos, E., Ophthalmology Unit, Experimental Diagnostic and Specialty Medicine Dept. (DIMES), University of Bologna, Policlinico S. Orsola Malpighi and S. Orsola-Malpighi Teaching Hospital, Bologna, Italy; Marcelli, E., Laboratory of Bioengineering, Experimental Diagnostic and Specialty Medicine Dept. (DIMES), University of Bologna, Policlinico S. Orsola Malpighi, Via Massarenti 9, Bologna, 40138, Italy","Background: Many different indexes have been proposed to quantify saccade curvature based on geometric properties of the saccade trajectory projected on the 2D plane. We introduce the Gaze Trajectory Index (GTI), a novel metric to quantify saccade trajectory deviation based on calculation of the rotational eye movements performed in 3D space while following a 2D saccade trajectory recorded with eye tracking (ET). Methods: We provided a description of GTI calculation. In 13 subjects with normal binocular vision we assessed GTI in single-target tests, then we evaluated GTI against previously proposed metrics (Maximum Deviation,MD; Area Curvature,AC; Quadratic Curvature,QC; Initial Direction,ID) using a distractor paradigm that elicited two types of saccade deviations, i.e.“inner-curved” and “outer-curved” saccades. Results: In single-target tests GTI showed that saccade curvature was significantly higher for oblique than for vertical saccades (0.86°±0.32 vs 0.55°±0.60,p < 0.05) and higher for vertical than for horizontal saccades (0.55°±0.60 vs 0.23°±0.17,p < 0.05), in accordance with previous studies. In distractor-based tests, for inner-curved saccades, GTI strongly correlated with MD (r = 0.965,p < 0.01), AC (r = 0.940,p < 0.01), QC (r = 0.866,p < 0.01), and Principal Component Analysis (PCA) confirmed that all these metrics reflect the same underlying phenomenon. For outer-curved trajectories, GTI showed poor correlation with MD and AC (r = 0.291 and 0.416,p < 0.01), however PCA included the three metrics in the same first component group. For outer-curved trajectories, GTI was the only metric showing strong correlation (r = 0.950,p < 0.05) with the overshoot degree of the trajectory. Conclusion: The novel GTI seems to have adjunctive potential, particularly for outer-curved trajectories, in the estimation of the absolute amount of saccade trajectory deviation. © 2019 Elsevier Ltd","Eye tracking; Gaze trajectories; Metrics; Saccade curvature; Saccades","Binocular vision; Eye tracking; Principal component analysis; Trajectories; 3-D space; Geometric properties; Metrics; Strong correlation; Trajectory deviation; Eye movements; adult; article; binocular vision; calculation; clinical article; controlled study; eye tracking; female; gaze; human; human experiment; male; principal component analysis; saccadic eye movement; eye fixation; image processing; middle aged; physiology; saccadic eye movement; videorecording; visual system examination; young adult; Adult; Diagnostic Techniques, Ophthalmological; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Middle Aged; Saccades; Video Recording; Young Adult",Article,"Final","",Scopus,2-s2.0-85061663543
"Salminen J., Kwak H., Jung S.-G., Nagpal M., An J., Jansen B.J.","57200315665;22835086700;57194276330;57212193148;55635855900;7202560690;","Confusion prediction from eye-tracking data: Experiments with machine learning",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"a5","","",,2,"10.1145/3361570.3361577","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076139447&doi=10.1145%2f3361570.3361577&partnerID=40&md5=14a93b26076a142e52b2217b822d61d3","Qatar Computing Research Institute, HBKU, Doha, Qatar; Turku School of Economics, Doha, Qatar; International Institute of Information Technology, Hyderabad, India","Salminen, J., Qatar Computing Research Institute, HBKU, Doha, Qatar, Turku School of Economics, Doha, Qatar; Kwak, H., Turku School of Economics, Doha, Qatar; Jung, S.-G., Turku School of Economics, Doha, Qatar; Nagpal, M., International Institute of Information Technology, Hyderabad, India; An, J., Turku School of Economics, Doha, Qatar; Jansen, B.J., Turku School of Economics, Doha, Qatar","Predicting user confusion can help improve information presentation on websites, mobile apps, and virtual reality interfaces. One promising information source for such prediction is eye-tracking data about gaze movements on the screen. Coupled with think-aloud records, we explore if user's confusion is correlated with primarily fixation-level features. We find that random forest achieves an accuracy of more than 70% when prediction user confusion using only fixation features. In addition, adding user-level features (age and gender) improves the accuracy to more than 90%. We also find that balancing the classes before training improves performance. We test two balancing algorithms, Synthetic Minority Over Sampling Technique (SMOTE) and Adaptive Synthetic Sampling (ADASYN) finding that SMOTE provides a higher performance increase. Overall, this research contains implications for researchers interested in inferring users' cognitive states from eye-tracking data. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","Confusion detection; Eye tracking; Machine learning","Balancing; Decision trees; Eye movements; Forecasting; Information systems; Information use; Learning systems; Machine learning; Predictive analytics; Balancing algorithms; Cognitive state; Gaze movements; Information presentation; Information sources; Synthetic minority over-sampling techniques; User levels; Virtual reality interfaces; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85076139447
"Chen W., Liao T., Li Z., Lin H., Xue H., Zhang L., Guo J., Cao Z.","57087444800;57205490097;57205495832;57205492327;57205491672;55844477500;56647243900;56438458900;","Using FTOC to track shuttlecock for the badminton robot",2019,"Neurocomputing","334",,,"182","196",,13,"10.1016/j.neucom.2019.01.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060241808&doi=10.1016%2fj.neucom.2019.01.023&partnerID=40&md5=bf7b2f311d28fcf6b33165fc731dbd17","School of Automation, Guangdong University of Technology, Waihuan Xi Road No.100, Panyu District, Guangzhou, Guangdong, China; School of Computers, Guangdong University of Technology, Waihuan Xi Road No.100, Panyu District, Guangzhou, Guangdong, China; The Advanced Digital Sciences Center (ADSC), the Singapore-based research center of the University of Illinois at Urbana-Champaign (UIUC), United States; Department of Industrial Systems Engineering and Management, National University of Singapore, Singapore","Chen, W., School of Automation, Guangdong University of Technology, Waihuan Xi Road No.100, Panyu District, Guangzhou, Guangdong, China; Liao, T., School of Automation, Guangdong University of Technology, Waihuan Xi Road No.100, Panyu District, Guangzhou, Guangdong, China; Li, Z., School of Automation, Guangdong University of Technology, Waihuan Xi Road No.100, Panyu District, Guangzhou, Guangdong, China; Lin, H., School of Automation, Guangdong University of Technology, Waihuan Xi Road No.100, Panyu District, Guangzhou, Guangdong, China; Xue, H., School of Computers, Guangdong University of Technology, Waihuan Xi Road No.100, Panyu District, Guangzhou, Guangdong, China; Zhang, L., The Advanced Digital Sciences Center (ADSC), the Singapore-based research center of the University of Illinois at Urbana-Champaign (UIUC), United States; Guo, J., School of Automation, Guangdong University of Technology, Waihuan Xi Road No.100, Panyu District, Guangzhou, Guangdong, China; Cao, Z., Department of Industrial Systems Engineering and Management, National University of Singapore, Singapore","Sport video analysis is gaining popularity recently owing to its importance in understanding sports and improving the performance of athletes. In this paper we focus on shuttlecock tracking algorithm. Particularly, a novel fast tracking based on object center (i.e., FTOC) method by fusing heterogeneous cues and AdaBoost algorithm are proposed to improve the tracking performance for a robot. Experimental results show that the proposed FTOC tracking method performs favorably against many other popular tracking approaches, such as TLD, MIL, KCF, DCF_CA, SMAF_CA, KCC, DSN, COKCF, etc., in term of speed, accuracy, and robustness, especially in challenging scenarios such as scale variations and background clutter. We further demonstrate the feasibility of the FTOC algorithm in a real-time ZED binocular camera based 3D shuttlecock tracking system for a robot. © 2019 Elsevier B.V.","AdaBoost; Badminton robot; FTOC; Tracking-by-detection; ZED binocular camera","Adaptive boosting; Binoculars; Cameras; Sports; Tracking (position); AdaBoost algorithm; Background clutter; Binocular camera; FTOC; Tracking algorithm; Tracking approaches; Tracking by detections; Tracking performance; Robots; article; badminton; eye tracking; feasibility study; human; robotics; velocity",Article,"Final","",Scopus,2-s2.0-85060241808
"Jigang L., Francis B.S.L., Rajan D.","36069339000;57189266609;7005909381;","Free-Head Appearance-Based Eye Gaze Estimation on Mobile Devices",2019,"1st International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2019",,,"8669057","232","237",,7,"10.1109/ICAIIC.2019.8669057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063896858&doi=10.1109%2fICAIIC.2019.8669057&partnerID=40&md5=92ced563b9bc24a52a9b9ca6eb8cf25a","School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore","Jigang, L., School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Francis, B.S.L., School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Rajan, D., School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore","Eye gaze tracking plays an important role in human-computer interaction applications. In recent years, many research have been performed to explore gaze estimation methods to handle free-head movement, most of which focused on gaze direction estimation. Gaze point estimation on the screen is another important application. In this paper, we proposed a two-step training network, called GazeEstimator, to improve the estimation accuracy of gaze location on mobile devices. The first step is to train an eye landmarks localization network on 300W-LP dataset [1], and the second step is to train a gaze estimation network on GazeCapture dataset [2]. Some processing operations are performed between the two networks for data cleaning. The first network is able to localize eye precisely on the image, while the gaze estimation network use only eye images and eye grids as inputs, and it is robust to facial expressions and occlusion.Compared with state-of-The-Art gaze estimation method, iTracker, our proposed deep network achieves higher accuracy and is able to estimate gaze location even in the condition that the full face cannot be detected. © 2019 IEEE.","CNN; Deep learning; Eye gaze estimation; Eye localization; Gaze location","Artificial intelligence; Cleaning; Deep learning; Human computer interaction; Location; Appearance based; Eye gaze tracking; Eye localization; Eye-gaze; Facial Expressions; Gaze point estimations; Processing operations; Two-step training; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85063896858
"Fu B., Steichen B.","57220885355;25928750600;","Using Behavior Data to Predict User Success in Ontology Class Mapping - An Application of Machine Learning in Interaction Analysis",2019,"Proceedings - 13th IEEE International Conference on Semantic Computing, ICSC 2019",,,"8665670","216","223",,3,"10.1109/ICOSC.2019.8665670","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064126106&doi=10.1109%2fICOSC.2019.8665670&partnerID=40&md5=c409f8ad9aad05f63bd48ad775160237","Computer Engineering and Computer Science, California State University, Long Beach, United States; Computer Science, California State Polytechnic University, Pomona, United States","Fu, B., Computer Engineering and Computer Science, California State University, Long Beach, United States; Steichen, B., Computer Science, California State Polytechnic University, Pomona, United States","Ontology visualization has played an important role in human data interaction by offering clarity and insight for complex structured datasets. Recent usability evaluations of ontology visualization techniques have added to our understanding of desired features when assisting users in the interactive process. However, user behavior data such as eye gaze and event logs have largely been used as indirect evidence to explain why a user may have carried out certain tasks in a controlled environment as opposed to direct input that informs the underlying visualization system. Although findings from usability studies have contributed to the refinement of ontology visualizations as a whole, the visualization techniques themselves remain a one-size-fits-all approach where all users are presented with the same visualizations and interactive features. By contrast, this paper investigates how user behavior data may offer real time indications as to how appropriate or effective a given visualization may be for a specific user at a moment in time, which in turn may inform the adaptation of the given visualization to the user on the fly. To this end, we apply established predictive modeling techniques in Machine Learning to predict user success using gaze data and event logs. We present a detailed analysis and demonstrate such predictions can be significantly better than a baseline classifier during visualization usage. These predictions can then be used to drive the adaptations of visual systems in providing ad hoc visualizations on a per user basis, which in turn may increase individual user success and performance. © 2019 IEEE.","Adaptive Visualization; Eye Tracking; Machine Learning; Ontology Visualization; User Prediction","Data visualization; Digital storage; Eye tracking; Forecasting; Learning systems; Machine learning; Ontology; Semantics; Visualization; Adaptive visualization; Controlled environment; Interaction analysis; Interactive features; Ontology visualizations; Usability evaluation; Visualization system; Visualization technique; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85064126106
"Fujii K., Rekimoto J.","56039515600;6603848632;","Subme an interactive subtitle system with english skill estimation using eye tracking",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"a23","","",,2,"10.1145/3311823.3311865","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062936751&doi=10.1145%2f3311823.3311865&partnerID=40&md5=3129b332df8fcb1656ef87c527b6e7cd","University of Tokyo Bunkyo-ku, Tokyo, Japan; University of Tokyo, Sony Computer Science Laboratories Inc., Tokyo, Japan","Fujii, K., University of Tokyo Bunkyo-ku, Tokyo, Japan; Rekimoto, J., University of Tokyo, Sony Computer Science Laboratories Inc., Tokyo, Japan","Owing to the improvement in accuracy of eye tracking devices, eye gaze movements occurring while conducting tasks are now a part of physical activities that can be monitored just like other life-logging data. Analyzing eye gaze movement data to predict reading comprehension has been widely explored and researchers have proven the potential of utilizing computers to estimate the skills and expertise level of users in various categories, including language skills. However, though many researchers have worked specifically on written texts to improve the reading skills of users, little research has been conducted to analyze eye gaze movements in correlation to watching movies, a medium which is known to be a popular and successful method of studying English as it includes reading, listening, and even speaking, the later of which is attributed to language shadowing. In this research, we focus on movies with subtitles due to the fact that they are very useful in order to grasp what is occurring on screen, and therefore, overall understanding of the content. We realized that the viewers' eye gaze movements are distinct depending on their English level. After retrieving the viewers' eye gaze movement data, we implemented a machine learning algorithm to detect their English levels and created a smart subtitle system called SubMe. The goal of this research is to estimate English levels through tracking eye movement. This was conducted by allowing the users to view a movie with subtitles. Our aim is create a system that can give the user certain feedback that can help improve their English studying methods. ©2019 Association for Computing Machinery.","Human computer interaction; Learning; User interface","Eye movements; Human computer interaction; Learning algorithms; Machine learning; Motion pictures; User interfaces; English skills; Eye tracking devices; Learning; Lifelogging; Physical activity; Reading comprehension; Reading skills; Written texts; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85062936751
"Elbalaoui A., Fakir M.","56611911200;6701779277;","Exudates detection in fundus images using mean-shift segmentation and adaptive thresholding",2019,"Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization","7","2",,"145","153",,4,"10.1080/21681163.2018.1463175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045845980&doi=10.1080%2f21681163.2018.1463175&partnerID=40&md5=54f3714e5e3dd985cbe77ec29e61acca","FST, Sultan Moulay Slimane University, Beni Mellal, Morocco","Elbalaoui, A., FST, Sultan Moulay Slimane University, Beni Mellal, Morocco; Fakir, M., FST, Sultan Moulay Slimane University, Beni Mellal, Morocco","Diabetic retinopathy (DR) affects changes to retinal blood vessels that can cause them to bleed or leak fluid and distorting vision. An early detection of exudates is a prerequisite for detecting and grading severe retinal lesions, like DR. This paper presents an automated method for detection of the exudates in digital fundus images. Our approach can be divided into four steps: shifting colour correction, Optic disc (OD) elimination, exudates segmentation and separation of exudates from background. In order to correct non-uniform illumination, we adopted the grey world method. Then, we must extract the OD prior to the process because it appears with similar colour, intensity and contrast to exudates. Next, to segment the exudates, we applied the mean-shift method. Finally, we used the maximum entropy thresholding to separate the exudates from background. The proposed method is tested on DIARETDB0 and DIARETDB1. Comparing to other recent methods available in the literature, our proposed approach obtains better exudate detection results in terms of sensitivity, specificity and accuracy. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.","diabetic retinopathy; entropy thresholding; Exudate detection; fundus image; mean-shift segmentation","Article; artificial neural network; blindness; comparative study; diabetic retinopathy; diagnostic accuracy; entropy; exudate; eye tracking; fundus imaging; human; image intensification; image processing; image quality; image segmentation; machine learning; mathematical phenomena; optic disk; priority journal; refraction error; retina image; sensitivity and specificity",Article,"Final","",Scopus,2-s2.0-85045845980
"Das P.J., Talukdar A.K., Sarma K.K.","57212081899;36019598100;35219168100;","A Framework for Human Behaviour Detection Using Combined Analysis of Facial Expression and Eye Gaze",2019,"Proceedings of 2nd International Conference on Innovations in Electronics, Signal Processing and Communication, IESC 2019",,,"8902367","154","160",,,"10.1109/IESPC.2019.8902367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075859067&doi=10.1109%2fIESPC.2019.8902367&partnerID=40&md5=fe9fa7cce04820c941a6775e344370dc","Gauhati University, Dept. of Electronics and Communication Engineering, Guwahati, 781014, India","Das, P.J., Gauhati University, Dept. of Electronics and Communication Engineering, Guwahati, 781014, India; Talukdar, A.K., Gauhati University, Dept. of Electronics and Communication Engineering, Guwahati, 781014, India; Sarma, K.K., Gauhati University, Dept. of Electronics and Communication Engineering, Guwahati, 781014, India","Facial appearances like a happy, sad, disgust, surprise, angry, fear and Eye Gaze Estimations are the fastest means of communication while conveying any type of information. Expressions not only expose the sensitivity or feelings of any person but can also be used to judge his/her mental states. Therefore, it has become a wide interest area of research due to its applications to the fields like Human Computer Interface (HCI), Man Machine Interface (MMI) etc. In order to get more effective human behavior identification, we have presented a combination of facial expression recognition and eye gaze estimation technique where we have to recognize the real time expressions by using Convolutional Neural Network (CNN) model with transfer learning method. After that, we determine the eye gaze by using Viola Jones, Circular Hough Transform (CHT) and a geometric method. With the combination of both processes, we able to predict the human behavior like drivers concentration levels. In this way experiments are carried out on MUG database for our own trained CNN model based on VGG16 (Visual Geometry Group) pre trained model and gives better performance with accuracy of 93% for training and 94% for overall process. © 2019 IEEE.","eye gaze estimation; Face detection; face recognition; facial expression; transfer learning CNN; VGG16","Behavioral research; Hough transforms; Learning systems; Neural networks; Circular Hough transforms; Convolutional neural network; Eye-gaze; Facial expression recognition; Facial Expressions; Human computer interfaces; Transfer learning; VGG16; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85075859067
"Alghofaili R., Solah M.S., Huang H.","57209395756;57209397544;57192586295;","Optimizing visual element placement via visual attention analysis",2019,"26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings",,,"8797816","464","473",,9,"10.1109/VR.2019.8797816","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071882641&doi=10.1109%2fVR.2019.8797816&partnerID=40&md5=826aca18513b4aa5006fbbcf79b18de6","Yasuhito Sawahata Japan Broadcasting Corporation, George Mason University, United States","Alghofaili, R., Yasuhito Sawahata Japan Broadcasting Corporation, George Mason University, United States; Solah, M.S., Yasuhito Sawahata Japan Broadcasting Corporation, George Mason University, United States; Huang, H., Yasuhito Sawahata Japan Broadcasting Corporation, George Mason University, United States","Eye-tracking enables researchers to conduct complex analysis on human behavior. With the recent introduction of eye-tracking into consumer-grade virtual reality headsets, the barrier of entry to visual attention analysis in virtual environments has been lowered significantly. Whether for arranging artwork in a virtual museum, posting banners for virtual events or placing advertisements in virtual worlds, analyzing visual attention patterns provides a powerful means for guiding visual element placement. In this work, we propose a novel data-driven optimization approach for automatically analyzing visual attention and placing visual elements in 3D virtual environments. Using an eye-tracking virtual reality headset, we collect eye-tracking data which we use to train a regression model for predicting gaze duration. We then use the predicted gaze duration output of our regressors to optimize the placement of visual elements with respect to certain visual attention and design goals. Through experiments in several virtual environments, we demonstrate the effectiveness of our optimization approach for predicting gaze duration and for placing visual elements in different practical scenarios. Our approach is implemented as a useful plug-in that level designers can use to automatically populate visual elements in 3D virtual environments. © 2019 IEEE.","Computer graphics; Computing methodologies; Graphics systems and interfacesvirtual reality","Computer graphics; Eye tracking; Regression analysis; User interfaces; Virtual reality; 3-D virtual environment; Computing methodologies; Data-driven optimization; Graphics systems; Optimization approach; Regression model; Virtual-reality headsets; Visual Attention; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85071882641
"Yoshimura A., Khokhar A., Borst C.W.","57210910262;57210910170;9736479200;","Eye-gaze-triggered visual cues to restore attention in educational VR",2019,"26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings",,,"8798327","1255","1256",,9,"10.1109/VR.2019.8798327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071879282&doi=10.1109%2fVR.2019.8798327&partnerID=40&md5=3caa289b210bac5e8099fb6bf3bd8d10","University of Louisiana, Lafayette, United States","Yoshimura, A., University of Louisiana, Lafayette, United States; Khokhar, A., University of Louisiana, Lafayette, United States; Borst, C.W., University of Louisiana, Lafayette, United States","In educational virtual reality, it is important to deal with problems of student inattention to presented content. We are developing attention-restoring visual cues for display when gaze tracking detects that student focus shifts away from critical objects. These cues include novel aspects and variations of standard cues that performed well in prior work on visual guidance. For the longer term, we propose experiments to compare various cues and their parameters to assess effectiveness and tradeoffs, and to assess the impact of eye tracking. Eye tracking is used to both detect inattention and to control the appearance and location of cues. © 2019 IEEE.","Attention; Educational VR; Eye tracking; Visual cues","Students; User interfaces; Virtual reality; Attention; Educational VR; Eye-gaze; Gaze tracking; Visual cues; Visual guidance; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85071879282
"Mardanbegi D., Mayer B., Pfeuffer K., Jalaliniya S., Gellersen H., Perzl A.","42761947400;57198766623;36141954200;55536725000;6701531333;57210918908;","EyeSeeThrough: Unifying tool selection and application in virtual environments",2019,"26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings",,,"8797988","474","483",,15,"10.1109/VR.2019.8797988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071868958&doi=10.1109%2fVR.2019.8797988&partnerID=40&md5=ca422840e190e4a78f91cbf52978c7a9","Lancaster University, Lancaster, United Kingdom; LMU Munich, Munich, Germany; Bundeswehr University Munich, Munich, Germany; Centennial College, Toronto, Canada","Mardanbegi, D., Lancaster University, Lancaster, United Kingdom; Mayer, B., LMU Munich, Munich, Germany; Pfeuffer, K., Bundeswehr University Munich, Munich, Germany; Jalaliniya, S., Lancaster University, Lancaster, United Kingdom; Gellersen, H., Lancaster University, Lancaster, United Kingdom; Perzl, A., Centennial College, Toronto, Canada","In 2D interfaces, actions are often represented by fixed tools arranged in menus, palettes, or dedicated parts of a screen, whereas 3D interfaces afford their arrangement at different depths relative to the user and the user can move them relative to each other. In this paper, we introduce EyeSeeThrough as a novel interaction technique that utilizes eye-tracking in VR. The user can apply an action to an intended object by visually aligning the object with the tool at the line-of-sight, and then issue a confirmation command. The underlying idea is to merge the two-step process of 1) selection of a mode in a menu and 2) applying it to a target, into one unified interaction. We present a user study where we compare the method to the baseline two-step selection. The results of our user study showed that our technique outperforms the two step selection in terms of speed and comfort. We further developed a prototype of a virtual living room to demonstrate the practicality of the proposed technique. © 2019 IEEE.","Centered computing; Centeredcomputing; Gestural input; Human; Human","Eye tracking; Virtual reality; 3D interface; Centered computing; Gestural input; Human; Line of Sight; Novel interaction techniques; Tool selection; Two-step process; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85071868958
"Wong E.T., Yean S., Hu Q., Lee B.S., Liu J., Deepu R.","57209509367;57192383054;57202648497;7405441352;57209506625;57209509412;","Gaze Estimation Using Residual Neural Network",2019,"2019 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2019",,,"8730846","411","414",,5,"10.1109/PERCOMW.2019.8730846","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067968733&doi=10.1109%2fPERCOMW.2019.8730846&partnerID=40&md5=86eebd4a37148352c6baf1e802b92b3f","Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore","Wong, E.T., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore; Yean, S., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore; Hu, Q., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore; Lee, B.S., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore; Liu, J., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore; Deepu, R., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore","Eye gaze tracking has become an prominent research topic in human-computer interaction and computer vision. It is due to its application in numerous fields, such as the market research, medical, neuroscience and psychology. Eye gaze tracking is implemented by estimating gaze (gaze estimation) for each individual frame in offline or real-time video captured. Therefore, in order to produce the secure the accurate tracking, especially in the emerging use in medical and community, innovation on the gaze estimation posts a challenge in research field. In this paper, we explored the use of the deep learning model, Residual Neural Network (ResNet-18), to predict the eye gaze on mobile device. The model is trained using the large-scale eye tracking public dataset called GazeCapture. We aim to innovate by incorporating methods/techniques of removing the blinking data, applying image histogram normalisation, head pose, and face grid features. As a result, we achieved 3.05cm average error, which is better performance than iTracker (4.11cm average error), the recent gaze tracking deep-learning model using AlexNet architecture. Upon observation, adaptive normalisation of the images was found to produce better results compared to histogram normalisation. Additionally, we found that head pose information was useful contribution to the proposed deep-learning network, while face grid information does not help to reduce test error. © 2019 IEEE.","deep learning; eye track; mobile; ResNet","Deep learning; Errors; Graphic methods; Human computer interaction; Large dataset; Neural networks; Ubiquitous computing; Accurate tracking; Eye gaze tracking; Grid information; Learning network; Market researches; mobile; Real time videos; ResNet; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85067968733
"Seychell D., Debono C.J.","55513973400;6603113105;","Ranking regions of visual saliency in RGB-D content",2019,"2018 International Conference on 3D Immersion, IC3D 2018 - Proceedings",,,"8657902","","",,4,"10.1109/IC3D.2018.8657902","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063542204&doi=10.1109%2fIC3D.2018.8657902&partnerID=40&md5=368099ddd952675fba48619bb05f1ec8","Department of Computer and Communications Engineering, Faculty of ICT, University of Malta, Malta","Seychell, D., Department of Computer and Communications Engineering, Faculty of ICT, University of Malta, Malta; Debono, C.J., Department of Computer and Communications Engineering, Faculty of ICT, University of Malta, Malta","Effective immersion takes place when the user can relate to the 3D environment presented and interact with key objects. Efficiently predicting which objects in a scene are in the user's attention, without using additional hardware, such as eye tracking solutions, provides an opportunity for creating more immersive scenes in real time and at lower costs. This is nonetheless algorithmically challenging. In this paper, we are proposing a technique that efficiently and effectively identifies the most salient objects in a scene. We show how it accurately matches user selection within 0.04s and is over 95% faster than other saliency algorithms while also providing a ranking of the most salient segments in a scene. © 2018 IEEE.","Immersion; Saliency Ranking; Segmentation","Image segmentation; 3-D environments; Immersion; Immersive; Key object; Saliency Ranking; Salient objects; User selection; Visual saliency; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85063542204
"Król M., Król M.","36677429600;57208580432;","Learning From Peers’ Eye Movements in the Absence of Expert Guidance: A Proof of Concept Using Laboratory Stock Trading, Eye Tracking, and Machine Learning",2019,"Cognitive Science","43","2","e12716","","",,10,"10.1111/cogs.12716","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062026448&doi=10.1111%2fcogs.12716&partnerID=40&md5=825fad1c54842d21d060cb09fd501c93","Wrocław Faculty of Psychology, SWPS University of Social Sciences and Humanities in Wrocław, Poland; Department of Economics, School of Social Sciences, University of Manchester, United Kingdom","Król, M., Wrocław Faculty of Psychology, SWPS University of Social Sciences and Humanities in Wrocław, Poland; Król, M., Department of Economics, School of Social Sciences, University of Manchester, United Kingdom","Existing research shows that people can improve their decision skills by learning what experts paid attention to when faced with the same problem. However, in domains like financial education, effective instruction requires frequent, personalized feedback given at the point of decision, which makes it time-consuming for experts to provide and thus, prohibitively costly. We address this by demonstrating an automated feedback mechanism that allows amateur decision-makers to learn what information to attend to from one another, rather than from an expert. In the first experiment, eye movements of N = 100 subjects were recorded while they repeatedly performed a standard behavioral finance investment task. Consistent with previous studies, we found that a significant proportion of subjects were affected by decision bias. In the second experiment, a different group of N = 100 subjects faced the same task but, after each choice, they received individual, machine learning-generated feedback on whether their pre-decision eye movements resembled those made by Experiment 1 subjects prior to good decisions. As a result, Experiment 2 subjects learned to analyze information similarly to their successful peers, which in turn reduced their decision bias. Furthermore, subjects with low Cognitive Reflection Test scores gained more from the proposed form of process feedback than from standard behavioral feedback based on decision outcomes. © 2019 Cognitive Science Society, Inc.","Crowd sourced process feedback; Debiasing; Eye tracking; Learning from peers; Machine learning","attention; crowdsourcing; decision making; eye movement; feedback system; financial management; human; learning; machine learning; peer group; professional competence; Attention; Crowdsourcing; Decision Making; Eye Movements; Feedback; Financial Management; Humans; Learning; Machine Learning; Peer Group; Professional Competence",Article,"Final","",Scopus,2-s2.0-85062026448
"Ariz M., Villanueva A., Cabeza R.","57204069029;7101612861;36763933900;","Robust and accurate 2D-tracking-based 3D positioning method: Application to head pose estimation",2019,"Computer Vision and Image Understanding","180",,,"13","22",,10,"10.1016/j.cviu.2019.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060351964&doi=10.1016%2fj.cviu.2019.01.002&partnerID=40&md5=e07035c188f4c09306738b73cd46a60b","Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, 31006, Spain","Ariz, M., Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, 31006, Spain; Villanueva, A., Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, 31006, Spain; Cabeza, R., Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, 31006, Spain","Head pose estimation (HPE) is currently a growing research field, mainly because of the proliferation of human–computer interfaces (HCI) in the last decade. It offers a wide variety of applications, including human behavior analysis, driver assistance systems or gaze estimation systems. This article aims to contribute to the development of robust and accurate HPE methods based on 2D tracking of the face, enhancing performance of both 2D point tracking and 3D pose estimation. We start with a baseline method for pose estimation based on POSIT algorithm. A novel weighted variant of POSIT is then proposed, together with a methodology to estimate weights for the 2D–3D point correspondences. Further, outlier detection and correction methods are also proposed in order to enhance both point tracking and pose estimation. With the aim of achieving a wider impact, the problem is addressed using a global approach: all the methods proposed are generalizable to any kind of object for which an approximate 3D model is available. These methods have been evaluated for the specific task of HPE using two different head pose video databases; a recently published one that reflects the expected performance of the system in current technological conditions, and an older one that allows an extensive comparison with state-of-the-art HPE methods. Results show that the proposed enhancements improve the accuracy of both 2D facial point tracking and 3D HPE, with respect to the implemented baseline method, by over 15% in normal tracking conditions and over 30% in noisy tracking conditions. Moreover, the proposed HPE system outperforms the state of the art on the two databases. © 2019 The Authors","Facial point detection and tracking; Head tracking; Outlier correction; Pose estimation; POSIT","Automobile drivers; Behavioral research; Statistics; Facial point detections; Head tracking; Outlier correction; Pose estimation; POSIT; Face recognition",Article,"Final","",Scopus,2-s2.0-85060351964
"Gautam G., Mukhopadhyay S.","57202336101;7401807653;","An adaptive localization of pupil degraded by eyelash occlusion and poor contrast",2019,"Multimedia Tools and Applications","78","6",,"6655","6677",,3,"10.1007/s11042-018-6371-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050812159&doi=10.1007%2fs11042-018-6371-0&partnerID=40&md5=4c2388a9ffd242ac9d181af951d10fe9","Indian Institute of Technology, Dhanbad, 826004, India","Gautam, G., Indian Institute of Technology, Dhanbad, 826004, India; Mukhopadhyay, S., Indian Institute of Technology, Dhanbad, 826004, India","The inner boundary of iris represents the pupil’s edge. Hence, to work an Iris Recognition System (IRS) and the gaze tracking system expeditiously it is important to locate it as precisely as possible in a significant amout of time. In the presence of non-ideal constraints e.g. non-uniform illumination, poor contrast, eyelashes, hairs, glasses, off-angle orientation, these systems may not work well. In this paper we present an adaptive pupil localization method based on the roundness criteria. First, it applies a gray level inversion to suppress the reflections, then it performs Gray level co-occurrence matrix (GLCM) based contrast estimation. If this estimated contrast is lower than a certain threshold, the input image is made to undergo gamma correction to adjust the contrast. Subsequently, anisotropic diffusion filtering followed by log transformation is applied, which suppresses the effect of eyelash occlusion, limits the creation of small regions and highlight the dark pixels. Afterwards, a clean binary image with few regions is acquired using adaptive thresholding and some morphological operations. Finally, the roundness metric is computed for each of these regions and the region with largest roundness metric, also being greater than a prescribed threshold, declared as pupil. Experiments were carried out on few well known databases, NICE1, CASIA V3 lamp, MMU, WVU and IITD. The results are grounded upon subjective and objective evaluation; which in turn, indicate that our method outperforms a state-of-the-art approach and a deep learning approach in terms of localization capability in some unconstrained scenarios and shorter processing time. After assessing the performance of the proposed algorithm, it is manifested that it ensures a fast and robust localization of pupil in the presence of corneal reflection, poor contrast, glasses and eyelash occlusion. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Contrast estimation; Gray-level co-occurrence matrix; Iris biometric; Morphological reconstruction; Pupil localization","Binary images; Biometrics; Deep learning; Glass; Image segmentation; Mathematical morphology; Physical addresses; Anisotropic diffusion filtering; Gray level co occurrence matrix(GLCM); Gray level co-occurrence matrix; Iris biometrics; Morphological reconstruction; Pupil localization; State-of-the-art approach; Subjective and objective evaluations; Eye tracking",Article,"Final","",Scopus,2-s2.0-85050812159
"Hosseinkhani J., Joslin C.","55675854500;57200744209;","Investigating into Saliency Priority of Bottom-up Attributes in 2D Videos Without Cognitive Bias",2019,"2018 IEEE International Symposium on Signal Processing and Information Technology, ISSPIT 2018",,,"8642701","223","228",,,"10.1109/ISSPIT.2018.8642701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063479277&doi=10.1109%2fISSPIT.2018.8642701&partnerID=40&md5=0010228eecdc579f0ee622fc5ba6f3fc","Dept. Systems and Computer Eng., Carleton University, Ottawa, Canada; School of Information Technology, Carleton University, Ottawa, Canada","Hosseinkhani, J., Dept. Systems and Computer Eng., Carleton University, Ottawa, Canada; Joslin, C., School of Information Technology, Carleton University, Ottawa, Canada","Saliency in an image or video is a region of interest that stands out relative to its neighbors and consequently attracts more human attention. A key factor in designing an algorithm to measure the importance and distinctiveness (i.e. saliency) of different regions of a frame is to understand how different visual cues affect the human perceptual and visual system. To this end, we investigated bottom-up features including color, texture, and motion in 2D video sequences for both one-by-one and combined scenarios to provide a ranking system stating the most dominant circumstances for each feature individually and in combination with other features as well. In this work, we mostly considered the feature combination scenarios under conditions in which we had no cognitive bias. Human cognition refers to a systematic pattern of perceptual and rational judgements and decision-making actions. Since computers do not typically have this ability, we tried to minimize this bias in the design of our experiment. First, we modelled our test data as 2D images and videos in a virtual environment to avoid any cognitive bias. Then, we performed an experiment using human subjects to determine which colors, textures, motion directions, and motion speeds attract human attention more. The proposed ranking system of salient visual attention stimuli was achieved using an eye-tracking procedure. This work provides a benchmark to specify the most salient stimulus with comprehensive information for both static and dynamic scenes. The main goal of this work is to create the ability of assigning a ranking of saliency for the entirety of an image/video frame rather than simply extracting a salient object/area which is widely performed in the state-of-the-art. © 2018 IEEE.","Bottom-up Features; Cognitive Bias; Dynamic Scenes; Saliency Detection; Visual Attention Model","Decision making; Eye tracking; Image segmentation; Textures; Virtual reality; Bottom up; Cognitive bias; Dynamic scenes; Saliency detection; Visual attention model; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85063479277
"Török Z.G., Török A.","57197314873;55608559600;","Looking at the map - Or navigating in a virtual city: Interaction of visuospatial display and spatial strategies in VR",2019,"9th IEEE International Conference on Cognitive Infocommunications, CogInfoCom 2018 - Proceedings",,,"8639898","327","332",,6,"10.1109/CogInfoCom.2018.8639898","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063101033&doi=10.1109%2fCogInfoCom.2018.8639898&partnerID=40&md5=1faa836ce6e8f805387a21273b5f8f12","Department of Cartography and Geoinformatics, Eötvös Loránd University, Budapest, Hungary; Sytems and Control Laboratory, HAS Institute for Computer Science and Control, Budapest, Hungary","Török, Z.G., Department of Cartography and Geoinformatics, Eötvös Loránd University, Budapest, Hungary; Török, A., Sytems and Control Laboratory, HAS Institute for Computer Science and Control, Budapest, Hungary","To study geo-visualization processes a Cognitive Cartography Lab was established at Eötvös University, and the ""Virtual Tourist"" experiment was designed for the better understanding of actual map use during navigation. In this paper we present some preliminary results of the experiment. We explored the use of a static, north-oriented city map during navigation in an interactive, 3D town. Participants explored the virtual environment or followed verbal instructions before they completed spatial tasks. Their spatial behavior, verbal reactions were recorded, and also eye tracking data from 64 participants was collected. The experiment was designed by a multidisciplinary research group, including students of Eötvös Loránd University. © 2018 IEEE.","Eye Tracking; Navigation; Spatial Cognition; Virtual Reality; Visualization","Flow visualization; Maps; Navigation; Virtual reality; Visualization; City map; Geo visualizations; Multi-disciplinary research; Spatial behaviors; Spatial cognition; Spatial strategies; Verbal instructions; Virtual cities; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85063101033
"Micelli L., Acosta D., Uribe-Quevedo A., Lamberti F., Kapralos B.","57207777268;57207757011;55208144400;7006958191;57203087583;","Extending upper limb user interactions in AR, VR and MR headsets employing a custom-made wearable device",2019,"2018 9th International Conference on Information, Intelligence, Systems and Applications, IISA 2018",,,"8633693","","",,,"10.1109/IISA.2018.8633693","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062861034&doi=10.1109%2fIISA.2018.8633693&partnerID=40&md5=983d90353db1fba0895b1ef2ac174d7e","Politecnico di Torino, Dip. Automatica e Informatica, Torino, Italy; Universidad Militar Nueva Granada, Bogota, Colombia; University of Ontario, Institute of Technology, Oshawa, ON, Canada","Micelli, L., Politecnico di Torino, Dip. Automatica e Informatica, Torino, Italy; Acosta, D., Universidad Militar Nueva Granada, Bogota, Colombia; Uribe-Quevedo, A., University of Ontario, Institute of Technology, Oshawa, ON, Canada; Lamberti, F., Politecnico di Torino, Dip. Automatica e Informatica, Torino, Italy; Kapralos, B., University of Ontario, Institute of Technology, Oshawa, ON, Canada","Upper limb interactions play an important role in virtual, augmented, and mixed reality scenarios. Numerous sensors including optical, magnetic, mechanical, and myography, have been employed to provide more natural interactions in comparison to game controllers. Recently, virtual, augmented, and mixed reality headsets have started embedding hand tracking sensors to simplify the hardware requirements, thus easing operation and setup. The tracking integration is being referred to as inside/out tracking, whereby hand and eye tracking interactions without external sensors or controllers provide interactive freedom. However, the motion capture area of the inside/out sensors is limited to the field of view and technical features of the cameras, and restricted to a few hand tracking gestures. In this paper, we introduce a custom upper limb motion tracking device that extends the user's interaction range while employing a virtual, augmented, or mixed reality headset. Our 3D motion tracking system is a compact wireless wearable prototype that uses inertial measurement units providing orientation and position data employed for upper limb user interaction outside the field of view of the inside/out sensors. © 2018 IEEE",,"Eye tracking; Mixed reality; Motion analysis; Palmprint recognition; 3D motion tracking; External sensors; Inertial measurement unit; Interaction ranges; Natural interactions; Technical features; Upper limb motion; Wearable prototypes; Wearable sensors",Conference Paper,"Final","",Scopus,2-s2.0-85062861034
"Gündüz A., Najjar T.","57214541379;55791244100;","Analysis Eye Movements during Reading by Machine Learning Algorithms: A Review Paper",2019,"Proceedings of the 2018 IEEE Symposium Series on Computational Intelligence, SSCI 2018",,,"8628799","1069","1075",,,"10.1109/SSCI.2018.8628799","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062781694&doi=10.1109%2fSSCI.2018.8628799&partnerID=40&md5=f1450e06730229d60273780ba34a80e0","Computer Engineering, University of Ankara Yildirim Beyazit, Ankara, Turkey","Gündüz, A., Computer Engineering, University of Ankara Yildirim Beyazit, Ankara, Turkey; Najjar, T., Computer Engineering, University of Ankara Yildirim Beyazit, Ankara, Turkey","With today's eye-tracking technologies, it is possible to analyze cognitive processes such as reading. There are many studies on that topic. We have studied some of these studies that analyze eye movements recording during reading by various machine learning algorithms. © 2018 IEEE.",,"Eye movements; Eye tracking; Machine learning; Cognitive process; Eye tracking technologies; Review papers; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85062781694
"Van Huynh T., Yang H.-J., Lee G.-S., Kim S.-H., Na I.-S.","57208575536;7406566089;7404853051;57218000904;24825137400;","Emotion recognition by integrating eye movement analysis and facial expression model",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"166","169",,2,"10.1145/3310986.3311001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065198202&doi=10.1145%2f3310986.3311001&partnerID=40&md5=691e05634574bcc7319d88238acea4a2","School of Electronics and Computer Engineering, Chonnam National University, 77 Yongbong-ro, Buk-gu, Gwangju, 500-757, South Korea; Software Convergence Education Institute, Chosun University, 309 Pilmun-daero, Dong-gu, Gwangju, 61452, South Korea","Van Huynh, T., School of Electronics and Computer Engineering, Chonnam National University, 77 Yongbong-ro, Buk-gu, Gwangju, 500-757, South Korea; Yang, H.-J., School of Electronics and Computer Engineering, Chonnam National University, 77 Yongbong-ro, Buk-gu, Gwangju, 500-757, South Korea; Lee, G.-S., School of Electronics and Computer Engineering, Chonnam National University, 77 Yongbong-ro, Buk-gu, Gwangju, 500-757, South Korea; Kim, S.-H., School of Electronics and Computer Engineering, Chonnam National University, 77 Yongbong-ro, Buk-gu, Gwangju, 500-757, South Korea; Na, I.-S., Software Convergence Education Institute, Chosun University, 309 Pilmun-daero, Dong-gu, Gwangju, 61452, South Korea","This paper presents an emotion recognition method which combines knowledge from the face and eye movements to improve the system accuracy. Our method has three fundamental stages to recognize the emotion. Firstly, we use a deep learning model to obtain the probability of a sample belonging to each emotion. Then, the eye movement features are extracted from an open-source framework which implements algorithms that demonstrated state-of-the-art results in this task. A new set of 51 features have been used to obtain related information about each emotion for the corresponding sample. Finally, the emotion for a sample is recognized based on the combination of the knowledge from the two previous stages. Experiment on the validation set of Acted Facial Expressions in the Wild (AFEW) dataset shows that the eye movements can make 2.87%improvement in the accuracy for the face model. © 2019 Association for Computing Machinery.","Deep learning; Emotion recognition; Eye movements; Eye tracking; Multilayer perceptron; Neural network","Deep learning; Deep neural networks; Eye tracking; Face recognition; Learning systems; Multilayer neural networks; Neural networks; Soft computing; Speech recognition; Emotion recognition; Eye movement analysis; Face modeling; Facial Expressions; Learning models; Open source frameworks; State of the art; System accuracy; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85065198202
"Thapaliya S., Jayarathna S., Jaime M.","57205570858;36052654200;13007146600;","Evaluating the EEG and Eye Movements for Autism Spectrum Disorder",2019,"Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018",,,"8622501","2328","2336",,11,"10.1109/BigData.2018.8622501","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062625693&doi=10.1109%2fBigData.2018.8622501&partnerID=40&md5=4956b169e6bb3d9ff272af668b81dcb0","Department of Computer Science, California State Polytechnic University, Pomona, CA  91768, United States; Department of Computer Science, Old Dominion University, Norfolk, VA  23529, United States; Department of Psychology, Indiana Univeristy- Purdue University, Columbus, IN, United States","Thapaliya, S., Department of Computer Science, California State Polytechnic University, Pomona, CA  91768, United States; Jayarathna, S., Department of Computer Science, Old Dominion University, Norfolk, VA  23529, United States; Jaime, M., Department of Psychology, Indiana Univeristy- Purdue University, Columbus, IN, United States","Autism Spectrum Disorder is a developmental disorder that often impairs a child's normal development of the brain. Early Diagnosis is crucial in the long term treatment of ASD, but this is challenging due to the lack of a proper objective measures. Subjective measures often take more time, resources, and have false positives or false negatives. There is a need for efficient objective measures that can help in diagnosing this disease early as possible with less effort. This paper presents EEG and Eye movement data for the diagnosis of ASD using machine learning algorithms. There are number of studies on classification of ASD using EEG or Eye tracking data. However, all of them simply use either Eye movements or EEG data for the classification. In our study we combine Eye movements and EEG data to develop an efficient methodology for diagnosis. This paper presents several models based on EEG, and eye movements for the diagnosis of ASD. © 2018 IEEE.","autism spectrum disorder; EEG; eye movements","Big data; Diagnosis; Diseases; Electroencephalography; Eye tracking; Learning algorithms; Machine learning; Autism spectrum disorders; Developmental disorders; Early diagnosis; Eeg datum; Eye movement datum; False negatives; False positive; Objective measure; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85062625693
"Li P., Hou X., Wei L., Song G., Duan X.","56375954100;57207460293;57207453327;55218895500;55646016600;","Efficient and low-cost deep-learning based gaze estimator for surgical robot control",2019,"2018 IEEE International Conference on Real-Time Computing and Robotics, RCAR 2018",,,"8621810","58","63",,4,"10.1109/RCAR.2018.8621810","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062548732&doi=10.1109%2fRCAR.2018.8621810&partnerID=40&md5=62576ddafef02ee80f8af70f074656b2","School of Mechanical Engineering and Automation, Harbin Institute of Technology (ShenZhen), Shenzhen, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China","Li, P., School of Mechanical Engineering and Automation, Harbin Institute of Technology (ShenZhen), Shenzhen, China; Hou, X., School of Mechanical Engineering and Automation, Harbin Institute of Technology (ShenZhen), Shenzhen, China; Wei, L., School of Mechanical Engineering and Automation, Harbin Institute of Technology (ShenZhen), Shenzhen, China; Song, G., State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Duan, X., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China","Surgical robots are playing more and more important role in modern operating room. However, operations by using surgical robot are not easy to handle by doctors. Vision based human-computer interaction (HCI) is a way to ease the difficulty to control surgical robots. While the problem of this method is that eyes tracking devices are expensive. In this paper, a low cost and robust deep-learning based on gaze estimator is proposed to control surgical robots. By this method, doctors can easily control the robot by specifying the starting point and ending point of the surgical robot using eye gazing. Surgical robots can also be controlled to move in 9 directions using controllers' eyes gazing information. A Densely Connected convolutional Neural Networks (Dense CNN) model for 9-direction/36-direction gaze estimation is built. The Dense CNN architecture has much more less trainable parameters compared to traditional CNN network architecture (AlexNet like/VGG like) which is more feasible to deploy on the Field-Programmable Gate Array (FPGA) and other hardware with limited memories. © 2018 IEEE.","Convolutional Neural Neural; Deep Learning; Gaze estimation; Minimally Invasive Surgery; Surgical robot","Convolution; Cost estimating; Deep learning; Eye movements; Field programmable gate arrays (FPGA); Human computer interaction; Human robot interaction; Network architecture; Neural networks; Robotic surgery; Robotics; Transplantation (surgical); Convolutional neural network; Convolutional Neural Neural; Gaze estimation; Human computer interaction (HCI); Limited memory; Minimally invasive surgery; Tracking devices; Vision based; Surgical equipment",Conference Paper,"Final","",Scopus,2-s2.0-85062548732
"Ai G., Hagio M., Ichiki M., Wagatsuma H.","56592209500;57207104458;57192675460;6603005439;","Simultaneous Analysis of EEGs and Movements in Interative Hand Shaking Required Skills to Synchronize Cooperatively in Game",2019,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,,"8616105","590","594",,1,"10.1109/SMC.2018.00109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062241903&doi=10.1109%2fSMC.2018.00109&partnerID=40&md5=a2e2256010232e429fedcd8c7cf1a41f","Department of Computer Science and Technology, Neusoft Institute Guangdong, Foshan, Guangdong Province, China; Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology, Kitakyushu, Japan; RIKEN Brain Science Institute, Saitama, Japan; Artificial Intelligence Research Center, AIST, Tokyo, Japan","Ai, G., Department of Computer Science and Technology, Neusoft Institute Guangdong, Foshan, Guangdong Province, China; Hagio, M., Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology, Kitakyushu, Japan; Ichiki, M., Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology, Kitakyushu, Japan; Wagatsuma, H., Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology, Kitakyushu, Japan, RIKEN Brain Science Institute, Saitama, Japan, Artificial Intelligence Research Center, AIST, Tokyo, Japan","Data analysis in the simultaneous recording of behavioral and brain activities such as electroencephalography (EEG) is highly important not only for the fundamental analysis of the functional correspondence between brain signals and an actual behavior, but also patients who need rehabilitations due to a brain damage. In the case of the refractory epilepsy, patients are treated with direct electrical cortical stimulation to map higher-motor cortices for an estimation of the deficit level in function of skilled motion control (praxis) after the removal of target brain regions. In the present study, we focused the possibility of the simultaneous analysis of EEGs and hand shaking movements as a natural behavior in the interactive game known as Rock-Paper-Scissors, which requires a smooth skilled synchronous movement with counterpart each other for making the match in the final moment to judge which one is winner, and hypothesized the existence of a synchronous component in EEGs between counterparts with respect to frequency ranges around Mu rhythm, 7.5-12.5 (primarily 9-11). We applied the 3D motion capture system and EEG scalp recording system with noise removal methods into the experimental analysis. © 2018 IEEE.","3D motion capture system; EEG; Eye-tracking system; simultaneous recording; Wavelet coherence and crossspectrum","Cybernetics; Electroencephalography; Electrophysiology; Eye tracking; Patient rehabilitation; Patient treatment; 3D motion capture systems; Cross spectra; Experimental analysis; Eye tracking systems; Simultaneous analysis; Simultaneous recording; Synchronous components; Synchronous movement; Brain",Conference Paper,"Final","",Scopus,2-s2.0-85062241903
"Sun W., You S., Walker J., Li K., Barnes N.","57207105170;36027010700;9039358100;57201861088;55515924300;","Structural Edge Detection: A Dataset and Benchmark",2019,"2018 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2018",,,"8615801","","",,2,"10.1109/DICTA.2018.8615801","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062240825&doi=10.1109%2fDICTA.2018.8615801&partnerID=40&md5=afec6582467d1cc7355d5ade6dbe65aa","Data61 - CSIRO, Australia; Australian National University, Australia","Sun, W., Data61 - CSIRO, Australia, Australian National University, Australia; You, S., Data61 - CSIRO, Australia, Australian National University, Australia; Walker, J., Data61 - CSIRO, Australia; Li, K., Data61 - CSIRO, Australia, Australian National University, Australia; Barnes, N., Data61 - CSIRO, Australia, Australian National University, Australia","Edge detection is a fundamental problem in computer vision community. In this paper, we propose a novel concept for edge detection called Structural Edge. The Structural edges include occluding contours of objects as well as orientation discontinuities in surfaces that define the 3D structure of objects and their environments. This contrasts the semantic edge which is only the boundary between semantic areas. While existing edge detection methods focus on either semantic boundaries or low-level gradients, we focus on the structural edge. To achieve that, in this paper, we propose the structural edge dataset along with a benchmark. The structural edge dataset contains 600 images of natural indoor and outdoor scenes. The structural edges are labeled manually and validated by eye-tracking data from 10 participants with overall 20 trials. Later, we use the dataset to benchmark the existing edge detection methods. We benchmark both the learning based and non-learning based methods and draw the conclusion that existing methods cannot fully solve the structural edge detection. We encourage new research to exploit the proposed task. © 2018 IEEE.","benchmark; Edge detection; edge detection dataset; structural edge","Benchmarking; Eye tracking; Image processing; Semantics; 3D Structure; Edge detection methods; Learning-based methods; Novel concept; Outdoor scenes; Semantic boundary; structural edge; Vision communities; Edge detection",Conference Paper,"Final","",Scopus,2-s2.0-85062240825
"Obo T., Adachi K.","35243395400;57203284169;","Multi-modal Sensing System for Unilateral Spatial Neglect in Computational System Rehabilitation",2019,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,,"8616408","2400","2405",,,"10.1109/SMC.2018.00412","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062213709&doi=10.1109%2fSMC.2018.00412&partnerID=40&md5=10cac5011b7b917dc1025c2e3fd150d3","Department of Applied Computer Science, Tokyo Polytechnic University, Kanagawa, Japan","Obo, T., Department of Applied Computer Science, Tokyo Polytechnic University, Kanagawa, Japan; Adachi, K., Department of Applied Computer Science, Tokyo Polytechnic University, Kanagawa, Japan","This paper presents a multi-modal sensing system for USN assessment with eye tracker, 3D image sensor and tablet PC. First, we introduce the multi-modal sensing system based on the concept of computational system rehabilitation. Next, we propose a computational approach to extract behavioral and perceptional features of USN patients from the heterogeneous data. Furthermore, we show an experimental example to discuss the effectiveness and applicability to the feature extraction. © 2018 IEEE.","computational system rehabilitation; multi-modal sensing; rehabilitaion suppport; unilateral spatial neglect","Cybernetics; Personal computers; Computational approach; Computational system; Eye trackers; Heterogeneous data; Multi-modal sensing; rehabilitaion suppport; Tablet PCs; unilateral spatial neglect; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85062213709
"Zhou H., Wei L., Cao R., Hanoun S., Bhatti A., Tai Y., Nahavandi S.","57198957382;35773955300;57207107724;24780376000;12345621500;36984437500;55992860000;","The Study of Using Eye Movements to Control the Laparoscope under a Haptically-Enabled Laparoscopic Surgery Simulation Environment",2019,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,,"8616509","3022","3026",,1,"10.1109/SMC.2018.00513","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062210153&doi=10.1109%2fSMC.2018.00513&partnerID=40&md5=aae4d43e327bd6e8ac29373bce8d40ec","Deakin University, Geelong, Australia; Surgery Department, First People's Hospital of YunNan Province New KunHua Hospital, YunNan, China","Zhou, H., Deakin University, Geelong, Australia; Wei, L., Deakin University, Geelong, Australia; Cao, R., Surgery Department, First People's Hospital of YunNan Province New KunHua Hospital, YunNan, China; Hanoun, S., Deakin University, Geelong, Australia; Bhatti, A., Deakin University, Geelong, Australia; Tai, Y., Deakin University, Geelong, Australia; Nahavandi, S., Deakin University, Geelong, Australia","The purpose of this study is to investigate the possibility to use eye movements to control the laparoscope during a laparoscopic surgery. Laparoscopic surgery usually needs at least two doctors, a surgeon and a laparoscope assistant. The view of the operating surgeon is provided by the laparoscope assistant. As misunderstandings or conflicts of cooperation may happen, an ideal way is that the surgeon has a full control of all the instruments including the surgical tools and laparoscope. To achieve it, an eye based interaction method is introduced in this paper that allows surgeons to control the view by themselves. With recent developments in the eye tracker platforms and associated eye tracking technologies, many non-contact eye tracking systems are available. It can record where a person is looking at any time and a sequence of eye movements. This information can be used to know where is the attention and interest of the person on a display. As such, surgeon's attention can be captured and then be followed by moving the laparoscope to the region of interest. To have a safe and efficient evaluation on the usability, a virtual reality based laparoscopic surgery simulation is built. It is based on Unity with two haptic devices simulating the surgical tools, a 3D mouse providing 6 degrees-of-freedom control of the camera and an eye tracker capturing eyes' positions on a display. Experiments on moving a camera left, right, up, down, in, out and to specified locations using eyes are conducted, and moreover the performances of the proposed eye based self-control and the 3D mouse based other-control are compared. The results are promising where the proposed pointing method leads to 43.6% faster completion of the tasks against the traditional other-control method using the 3D mouse. © 2018 IEEE.","eye gaze; haptic; laparoscopic surgery simulation","Cameras; Cybernetics; Degrees of freedom (mechanics); Display devices; Eye tracking; Image segmentation; Laparoscopy; Mammals; Surgery; Surgical equipment; Virtual reality; Eye tracking systems; Eye tracking technologies; Eye-gaze; haptic; Interaction methods; Laparoscopic surgery; Pointing methods; Region of interest; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85062210153
"Yin Y., Juan C., Chakraborty J., McGuire M.P.","57201739020;57207114226;23476701400;16203300300;","Classification of Eye Tracking Data Using a Convolutional Neural Network",2019,"Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018",,,"8614110","530","535",,6,"10.1109/ICMLA.2018.00085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062237930&doi=10.1109%2fICMLA.2018.00085&partnerID=40&md5=c1c73d0675ce108dd7192e9e0ae02507","Department of Computer and Information Sciences, Towson University, Towson, United States","Yin, Y., Department of Computer and Information Sciences, Towson University, Towson, United States; Juan, C., Department of Computer and Information Sciences, Towson University, Towson, United States; Chakraborty, J., Department of Computer and Information Sciences, Towson University, Towson, United States; McGuire, M.P., Department of Computer and Information Sciences, Towson University, Towson, United States","Historically, eye tracking analysis has been a useful approach to identify areas of interest (AOIs) where users have specific regions of the user interface (UI) in which they are interested. Many algorithms have been proposed to analyze eye tracking data in order to make user interfaces more effective. The objective of this study is to use convolutional neural networks (CNNs) to classify eye tracking data. First, a CNN was used to classify two different web interfaces for browsing news data. Then in a second experiment, a CNN was used to classify the nationalities of users. In addition, techniques of data-preprocessing and feature-engineering were applied. The algorithm used in this research is convolutional neural network (CNN), which is famous in deep learning field. Keras framework running on top of TensorFlow was used to define and train our CNN model. The purpose of this research is to explore how feature-engineering can affect evaluation metrics about our model. The results of the study show a number of interesting patterns and generally that deep learning shows promise in the analysis of eye tracking data. © 2018 IEEE.","CNN; Deep learning; Eye tracking; Feature-engineering; Gaze point; Keras; TensorFlow","Classification (of information); Convolution; Deep learning; Machine learning; Neural networks; User interfaces; Convolutional neural network; Data preprocessing; Evaluation metrics; Eye-tracking analysis; Feature engineerings; Gaze point; Keras; TensorFlow; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85062237930
"Barbosa Monforte P.H., Matos Araujo G., Azevedo De Lima A.","57207112505;11539108500;24733621100;","Evaluation of a New Kernel-Based Classifier in Eye Pupil Detection",2019,"Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018",,,"8614088","380","385",,1,"10.1109/ICMLA.2018.00063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062230155&doi=10.1109%2fICMLA.2018.00063&partnerID=40&md5=a0859b6c189a69687c57b4b66cd46877","Centro Federal de Educação Tecnolgica Celso Suckow da Fonseca, Estrada de Adrianópolis, 1.317, Nova Iguaçu - RJ, CEP: 26041-271, Brazil","Barbosa Monforte, P.H., Centro Federal de Educação Tecnolgica Celso Suckow da Fonseca, Estrada de Adrianópolis, 1.317, Nova Iguaçu - RJ, CEP: 26041-271, Brazil; Matos Araujo, G., Centro Federal de Educação Tecnolgica Celso Suckow da Fonseca, Estrada de Adrianópolis, 1.317, Nova Iguaçu - RJ, CEP: 26041-271, Brazil; Azevedo De Lima, A., Centro Federal de Educação Tecnolgica Celso Suckow da Fonseca, Estrada de Adrianópolis, 1.317, Nova Iguaçu - RJ, CEP: 26041-271, Brazil","Accurate pupil location is paramount to applications such as gaze estimation, assistive technologies and several man-machine interfaces as the ones found in smartphones and VR applications. We introduce a new classifier stemmed from the Inner Product Detector and investigate its features on the challenging task of pupil localization. IPD (Inner Product Detector) is a classifier with high potential in facial landmarks detection. It is robust to variations in the desired pattern while maintaining good generalization and computational efficiency. However, one possible limitation is its linear behavior, which could be overcome by aggregating non-linear techniques, such as kernel methods. Although kernel classifiers have been exhaustively studied in the past two decades, it was not analyzed or applied with IPD, yet. The proposed KIPD achieves in the worst case an accuracy of 97.41% on the BioID dataset and 93.71% in LFPW dataset both at 10% of the interocular distance. In this paper the KIPD is compared to the state of the art methods, including the ones using deep learning, being competitive in terms of accuracy as well as computational complexity. © 2018 IEEE.","Correlation filtes; Kernel machines; Machine learning","Computational efficiency; Learning systems; Machine learning; Assistive technology; Eye pupil detections; Kernel based classifiers; Kernel classifiers; Kernel machine; Man machine interface; Nonlinear techniques; State-of-the-art methods; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85062230155
"Ni H., Wang J., Wang L., Yan N.","57206904861;57206898295;55851945581;7102919410;","Track Your Emotional Perception of 3-D Virtual Talking Head in Human-computer Interaction",2019,"2018 IEEE International Conference on Cyborg and Bionic Systems, CBS 2018",,,"8612271","298","303",,3,"10.1109/CBS.2018.8612271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062057004&doi=10.1109%2fCBS.2018.8612271&partnerID=40&md5=3283f5264e00f42e545253929291ad7a","Shenzhen Institutes of Advanced Technology, Shenzhen, China; Key Laboratory of Fiber Optic Sensing Technology and Information Processing, Wuhan University of Technology, School of Information Engineering Wuhan, Shenzhen Institutes of Advanced Technology, Shenzhen, China; CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Shenzhen, China","Ni, H., Shenzhen Institutes of Advanced Technology, Shenzhen, China, Key Laboratory of Fiber Optic Sensing Technology and Information Processing, Wuhan University of Technology, School of Information Engineering Wuhan, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Wang, J., CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Wang, L., CAS Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Yan, N., Shenzhen Institutes of Advanced Technology, Shenzhen, China","To investigate how emotions are identified from the avatar character in the natural scene during human-computer interaction. In current paper, a novel 3-D virtual talking head system with dynamic emotional facial expression for applying to human-robot communication was developed. Furthermore, eye tracking experiment and subjective evaluation experiment were utilized to explore the emotional perception of the 3-D virtual talking head. The results showed that there was no significant difference of observation mode between audio-visual animation of 3-D virtual talking head videos (AV3D) and audio-visual human face videos (AVHF). Besides, the recognition accuracy of HF was higher than 3D and almost all the accuracy of emotions had been improved when adding audio to videos. Finally, the results demonstrated that happiness was identified the best whether watching 3-D virtual talking head videos (3D) or human face videos (HF). These results implied that the 3-D talking head has potentially been as a suitable natural communication form in human-computer interaction. © 2018 IEEE.",,"Behavioral research; Cyborgs; Electric circuit breakers; Eye tracking; Robots; Virtual reality; Audio-visual; Facial Expressions; Human-robot communication; Natural communication; Natural scenes; Recognition accuracy; Subjective evaluations; Talking heads; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85062057004
"Tong H., Wan Q., Kaszowska A., Panetta K., Taylor H.A., Agaian S.","57211084616;57188747389;57203481497;6507727793;7403057334;35321805400;","ARFurniture: Augmented reality interior decoration style colorization",2019,"IS and T International Symposium on Electronic Imaging Science and Technology","2019","2","175","","",,4,"10.2352/ISSN.2470-1173.2019.2.ERVR-175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080106107&doi=10.2352%2fISSN.2470-1173.2019.2.ERVR-175&partnerID=40&md5=90c8441c262b585d9b61b56c32a32e63","Department of Electrical and Computer Engineering, Tufts University, Medford, MA, United States; Department of Psychology, Tufts University, Medford, MA, United States; Computer Science, City University of New York, New York City, NY, United States","Tong, H., Department of Electrical and Computer Engineering, Tufts University, Medford, MA, United States; Wan, Q., Department of Electrical and Computer Engineering, Tufts University, Medford, MA, United States; Kaszowska, A., Department of Psychology, Tufts University, Medford, MA, United States; Panetta, K., Department of Electrical and Computer Engineering, Tufts University, Medford, MA, United States; Taylor, H.A., Department of Psychology, Tufts University, Medford, MA, United States; Agaian, S., Computer Science, City University of New York, New York City, NY, United States","Augmented Reality (AR) can seamlessly create an illusion of virtual elements blended into the real world scene, which is one of the most fascinating human-machine interaction technologies. AR has been utilized in a variety of real-life applications including immersive collaborative gaming, fashion appreciation, interior design, and assistive devices for individuals with vision impairments. This paper contributes a real-time AR application, ARFurniture, which will allow the users to envision furniture-of-interests in different colors and different styles, all from their smart devices. The core software architecture consists of deep-learning based semantic segmentation and fast-speed color transformation. Our software architecture allows the user prompt the system to colorize the style of the furniture-of-interest within the scene on their mobile devices, and has been successfully deployed on mobile devices. In addition, using eye gaze as a pointing indicator, a head-mounted user-centric augmented reality based indoor decoration style colorization concept is discussed. Related algorithms, system design, and simulation results for ARFurniture are presented. Furthermore, a no-reference image quality measure, Naturalness Image Quality Evaluator (NIQE), was utilized to evaluate the immersiveness and naturalness of ARFuniture. The results demonstrate that ARFurniture has game-changing value to enhance user experience in indoor decoration. © 2019, Society for Imaging Science and Technology.",,"Architectural design; Augmented reality; Deep learning; Image quality; Semantics; Software architecture; User experience; Virtual reality; Collaborative gaming; Color transformation; Human machine interaction; No-reference images; Real-life applications; Related algorithms; Semantic segmentation; Vision impairments; Quality control",Conference Paper,"Final","",Scopus,2-s2.0-85080106107
"Cazzato D., Castro S.M., Agamennoni O., Fernández G., Voos H.","55866556300;55171401100;6604079588;55550165900;6603280387;","A non-invasive tool for attention-deficit disorder analysis based on gaze tracks.",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"5","","",,,"10.1145/3309772.3309777","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070542762&doi=10.1145%2f3309772.3309777&partnerID=40&md5=dd2cbc528abc85226d38b86ce5bb14bd","Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg, Luxembourg; Universidad Nacional del Sur, Bahía Blanca, Argentina","Cazzato, D., Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg, Luxembourg; Castro, S.M., Universidad Nacional del Sur, Bahía Blanca, Argentina; Agamennoni, O., Universidad Nacional del Sur, Bahía Blanca, Argentina; Fernández, G., Universidad Nacional del Sur, Bahía Blanca, Argentina; Voos, H., Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg, Luxembourg","Attention deficit hyperactivity disorder (ADHD) is a neurodevelop-mental disability characterized by difficulties in keeping concentration, excessive activity and difficulties controlling behaviour not appropriate to the person’s age. It is estimated to affect between 4-9% of youths and 2-5% of adults. Assistive technologies can help people with ADHD to reach goals, stay organized and even fight the urge to succumb to forms of distraction. This work introduces a tool designed for people with ADHD aimed at detecting and training their ability to follow a target in a screen. The tool is based on noninvasive monocular gaze estimation technique without constraints in terms of user dependent calibration or appearance. The system has been employed and validated in a human-computer interaction (HCI) scenario with the aim of evaluating the user visual exploration. Results show that the tool can be used in complex tasks like monitoring a user progress comparing performance after different sessions. © 2019 Association for Computing Machinery.","Attention-deficit hyperactivity disorder (adhd); Classification; Gaze estimation; Random forests","Classification (of information); Decision trees; Diseases; Human computer interaction; Intelligent systems; Random forests; Assistive technology; Attention deficit disorder; Attention deficit hyperactivity disorder; Gaze estimation; Human computer interaction (HCI); Mental disabilities; User-dependent; Visual exploration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85070542762
"MacHado M., Aresta G., Leitão P., Carvalho A.S., Rodrigues M., Ramos I., Cunha A., Campilho A.","57206243536;57190949419;57193756731;57193760052;57199723236;57191864412;7103392597;57200232252;","Radiologists' Gaze Characterization during Lung Nodule Search in Thoracic CT",2019,"Proceedings - ICGI 2018: International Conference on Graphics and Interaction",,,"8602697","","",,4,"10.1109/ITCGI.2018.8602697","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061660900&doi=10.1109%2fITCGI.2018.8602697&partnerID=40&md5=6c44bc1610478063760b329c2dac1c1f","Instituto de Engenharia de Sistemas e Computadores - Tecnologia e Ciência (INESC-TEC), Porto, Portugal; Dept. de Radiologia Do Centro, Hospitalar São João, Porto, Portugal","MacHado, M., Instituto de Engenharia de Sistemas e Computadores - Tecnologia e Ciência (INESC-TEC), Porto, Portugal; Aresta, G., Instituto de Engenharia de Sistemas e Computadores - Tecnologia e Ciência (INESC-TEC), Porto, Portugal; Leitão, P., Dept. de Radiologia Do Centro, Hospitalar São João, Porto, Portugal; Carvalho, A.S., Dept. de Radiologia Do Centro, Hospitalar São João, Porto, Portugal; Rodrigues, M., Dept. de Radiologia Do Centro, Hospitalar São João, Porto, Portugal; Ramos, I., Dept. de Radiologia Do Centro, Hospitalar São João, Porto, Portugal; Cunha, A., Instituto de Engenharia de Sistemas e Computadores - Tecnologia e Ciência (INESC-TEC), Porto, Portugal; Campilho, A., Instituto de Engenharia de Sistemas e Computadores - Tecnologia e Ciência (INESC-TEC), Porto, Portugal","Lung cancer diagnosis is made by radiologists through nodule search in chest Computed Tomography (CT) scans. This task is known to be difficult and prone to errors that can lead to late diagnosis. Although Computer-Aided Diagnostic (CAD) systems are promising tools to be used in clinical practice, experienced radiologists continue to perform better diagnosis than CADs. This paper proposes a methodology for characterizing the radiologist's gaze during nodules search in chest CT scans. The main goals are to identify regions that attract the radiologists' attention, which can then be used for improving a lung CAD system, and to create a tool to assist radiologists during the search task. For that purpose, the methodology processes the radiologists' gaze and their mouse coordinates during the nodule search. The resulting data is then processed to obtain a 3D gaze path from which relevant attention studies can be derived. To better convey the found information, a reference model of the lung that eases the communication of the location of relevant anatomical/pathological findings is also proposed. The methodology is tested on a set of 24 real-practice gazes, recorded via an Eye tracker, from 3 radiologists. © 2018 IEEE.","eye tracking; lung division model; pulmonary nodules","Biological organs; Computer aided diagnosis; Eye tracking; Mammals; Chest CT scans; Clinical practices; Computed tomography scan; Computer aided diagnostics; Lung cancer diagnosis; Pulmonary nodules; Reference modeling; Search tasks; Computerized tomography",Conference Paper,"Final","",Scopus,2-s2.0-85061660900
"Gan L., Liu W., Luo Y., Wu X., Lu B.-L.","57211156391;56796035400;57204648704;57209182435;26435347900;","A cross-culture study on multimodal emotion recognition using deep learning",2019,"Communications in Computer and Information Science","1142 CCIS",,,"670","680",,,"10.1007/978-3-030-36808-1_73","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089606996&doi=10.1007%2f978-3-030-36808-1_73&partnerID=40&md5=b6284685f49f1002ab1d5039320c0fa9","Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognition Engineering, Shanghai Jiao Tong University, Shanghai, China; Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, China","Gan, L., Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Liu, W., Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Luo, Y., Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Wu, X., Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Lu, B.-L., Center for Brain-Like Computing and Machine Intelligence, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China, Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognition Engineering, Shanghai Jiao Tong University, Shanghai, China, Brain Science and Technology Research Center, Shanghai Jiao Tong University, Shanghai, China","In this paper, we aim to investigate the similarities and differences of multimodal signals between Chinese and French on three emotions recognition task using deep learning. We use videos including positive, neutral and negative emotions as stimuli material. Both Chinese and French subjects wear electrode caps and eye tracking glass while doing experiments to collect electroencephalography (EEG) and eye movement data. To deal with the problem of lacking data for training deep neural networks, conditional Wasserstein generative adversarial network is adopted to generate EEG and eye movement data. The EEG and eye movement features are fused by using Deep Canonical Correlation Analysis to analyze the relationship between EEG and eye movement data. Our experimental results show that French has higher classification accuracy on beta frequency band while Chinese performs better on gamma frequency band. In addition, EEG signals and eye movement data of French participants have complementary characteristics in discriminating positive and negative emotions. © Springer Nature Switzerland AG 2019.","Chinese; Cross-culture; Deep learning; EEG; Emotion recognition; Eye movement; French","Biomedical signal processing; Deep neural networks; Electroencephalography; Electrophysiology; Eye movements; Eye tracking; Adversarial networks; Canonical correlation analysis; Classification accuracy; Complementary characteristics; Emotions recognition; Eye movement datum; Multimodal emotion recognition; Positive and negative emotions; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85089606996
"Luo M., Liu X., Wang W., Huang W.","57206482771;57209845642;56948398100;56195325600;","Improved capsule network for gaze estimation in wireless sensor networks",2019,"Proceedings of the 12th EAI International Conference on Mobile Multimedia Communications, MOBIMEDIA 2019",,,,"307","326",,,"10.4108/eai.29-6-2019.2282839","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088231435&doi=10.4108%2feai.29-6-2019.2282839&partnerID=40&md5=e19ef3c672ec5ad449f5be0c591758e6","School of Information Engineering, Nanchang University, Nanchang, China; School of Economics and Management, Chang’an University, Xi’an, China","Luo, M., School of Information Engineering, Nanchang University, Nanchang, China; Liu, X., School of Information Engineering, Nanchang University, Nanchang, China; Wang, W., School of Economics and Management, Chang’an University, Xi’an, China; Huang, W., School of Information Engineering, Nanchang University, Nanchang, China","In this study, aiming at the problem of gaze estimation in the wireless sensor network in the car, we use image-based method to estimate gaze based on the single camera sensor. We use the deep learning model and propose the improved model from three aspects based on the original capsule network. The first is to increase the convolution layer, the second is to increase the capsule layer, and the third is to widen the capsule layer in the network. Through many contrast experiments, it is proved that the appropriate use of the first or second improved method can achieve performance over other comparison models, and the prediction results of gaze estimation are almost no different from the real gaze direction. © 2019 EAI.","Capsule Network; Gaze estimation; Multi-layer Capsule Network","Deep learning; Multimedia systems; Network layers; Comparison models; Contrast experiment; Gaze direction; Gaze estimation; Image-based methods; Learning models; Single cameras; Wireless sensor networks",Conference Paper,"Final","",Scopus,2-s2.0-85088231435
"Huang K., Bryant T., Schneider B.","57217054528;57211289353;55051404100;","Identifying collaborative learning states using unsupervised machine learning on eye-tracking, physiological and motion sensor data",2019,"EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining",,,,"318","323",,7,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086002316&partnerID=40&md5=d0b948cf82eb49891510a9efe11ef98d","Harvard University, United States","Huang, K., Harvard University, United States; Bryant, T., Harvard University, United States; Schneider, B., Harvard University, United States","With the advent of new data collection techniques, there has been a growing interest in studying co-located groups of students using Multimodal Learning Analytics [3] to automatically identify collaborative learning states. In this paper, we analyze a multimodal dataset (N=84) made of eye-tracking, physiological and motion sensing data. We leverage unsupervised machine learning algorithms to find (un)productive collaborative states. We found a three-states solution where different states (and transitions between them) were significantly correlated with task performance, collaboration quality and learning gains. We interpret these findings in light of collaborative learning theories and discuss their implications for studying groups of students using MMLA. © EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining. All rights reserved.","Eye-tracking; Motion sensing; Multimodal learning analytics; Physiological sensing; Unsupervised machine learning","Data mining; Eye tracking; Learning algorithms; Motion sensors; Motion tracking; Physiology; Students; Collaborative learning; Data collection; Learning gain; Motion sensing; Multi-modal dataset; Multi-modal learning; Task performance; Unsupervised machine learning; Machine learning",Conference Paper,"Final","",Scopus,2-s2.0-85086002316
"Angert T., Schneider B.","57217057430;55051404100;","Augmenting transcripts with natural language processing and multimodal data",2019,"EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining",,,,"496","499",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085993280&partnerID=40&md5=fea03d96adac79c4e68e8799236ee11f","Harvard Graduate School of Education, United States","Angert, T., Harvard Graduate School of Education, United States; Schneider, B., Harvard Graduate School of Education, United States","In this paper we explore preliminary applications of augmenting transcripts with multimodal data. In a previous study, pairs of participants (dyads) learned how to program a robot to navigate a maze using a block-based programming language. As the dyads completed the task, their transcripts and various multimodal data were captured, creating a synced dataset of speech, 3D motion capture points, electrodermal activity, heart rate, and eye tracking. In this paper, we describe a simple visualization method to more easily analyze conversation during collaboration: a ""Convergence chart"" which visualizes changes in biometrics between speakers throughout a conversation. These visualizations allow researchers to see high level trends in transcripts by using a combination of Natural Language Processing (NLP) and physiological data to generate new insights on how collaboration changes over time. We conclude with how to use these visualizations to create new metrics to understand group dynamics. © EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining. All rights reserved.","Biometrics; Computer supported collaborative learning; Data visualization; Natural language processing","Data mining; Eye tracking; High level languages; Motion tracking; Natural language processing systems; Robot programming; Visualization; 3D motion capture; Block based; Electrodermal activity; Group dynamics; Multi-modal data; NAtural language processing; Physiological data; Visualization method; Data handling",Conference Paper,"Final","",Scopus,2-s2.0-85085993280
"Pichitwong W., Chamnongthai K.","57191333091;57202765861;","An eye-tracker-based 3D point-of-gaze estimation method using head movement",2019,"IEEE Access","7",,"8764337","99086","99098",,2,"10.1109/ACCESS.2019.2929195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084926489&doi=10.1109%2fACCESS.2019.2929195&partnerID=40&md5=fb008a16311ea8d0e5c61ac9d1d58804","Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, 10140, Thailand","Pichitwong, W., Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, 10140, Thailand; Chamnongthai, K., Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, 10140, Thailand","Eye trackers are currently used to sense the positions of both the centers of the pupils and the point-of-gaze (POG) position on a screen, in keeping with the original objective for which they were designed; however, it remains difficult to measure the positions of three-dimensional (3D) POGs. This paper proposes a method for 3D gaze estimation by using head movement, pupil position data, and POGs on a screen. The method assumes that a person, usually unintentionally, moves his or her head a short distance such that multiple straight lines can be drawn from the center point between the two pupils to the POG. When the person is continuously focusing on a given 3D POG while moving, these lines represent the lines of sight that intersect at a 3D POG . That 3D POG can, therefore, be found from the intersection of several lines of sight formed by head movements. To evaluate the performance of the proposed method, experimental equipment was constructed, and experiments with five male and five female participants were performed in which the participants looked at nine test points in a 3D space for approximately 20 s each. The experimental results reveal that the proposed method can measure 3D POGs with average distance errors of 13.36 cm, 7.58 cm, 5.72 cm, 3.97 cm, and 3.52 cm for head movement distances of 1 cm, 2 cm, 3 cm, 4 cm, and 5 cm, respectively. © 2019 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","3D gaze estimation; Eye tracker; Gaze tracking","Eye movements; Average Distance; Experimental equipments; Gaze estimation; Head movements; Lines-of-sight; Point of gaze; Position data; Threedimensional (3-d); Eye tracking",Article,"Final","",Scopus,2-s2.0-85084926489
"Zhang Z., Bambach S., Yu C., Crandall D.J.","57201864135;55975996000;16032623800;8732077700;","From coarse attention to fine-grained gaze: A two-stage 3D fully convolutional network for predicting eye gaze in first person video",2019,"British Machine Vision Conference 2018, BMVC 2018",,,,"","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084015337&partnerID=40&md5=2bbefadf4bb5da8c9eabb9256f7d17c4","School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, United States; Psychological and Brain Sciences Indiana University, Bloomington, IN, United States","Zhang, Z., School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, United States; Bambach, S., School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, United States; Yu, C., Psychological and Brain Sciences Indiana University, Bloomington, IN, United States; Crandall, D.J., School of Informatics, Computing, and Engineering, Indiana University, Bloomington, IN, United States","While predicting where people will look when viewing static scenes has been well-studied, a more challenging problem is to predict gaze within the first-person, ego-centric field of view as people go about daily life. This problem is difficult because where a person looks depends not just on their visual surroundings, but also on the task they have in mind, their own internal state, their past gaze patterns and actions, and non-visual cues (e.g., sounds) that might attract their attention. Using data from head-mounted cameras and eye trackers that record people's egocentric fields of view and gaze, we propose and learn a two-stage 3D fully convolutional network to predict gaze in each egocentric frame. The model estimates a coarse attention region in the first stage, combining it with spatial and temporal features to predict a more precise gaze point in the second stage. We evaluate on a public dataset in which adults carry out specific tasks as well as on a new challenging dataset in which parents and toddlers freely interact with toys and each other, and demonstrate that our model outperforms state-of-the-art baselines. © 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.",,"Computer vision; Convolution; Convolutional networks; Fields of views; Head mounted Camera; Internal state; Model estimates; Public dataset; State of the art; Temporal features; Forecasting",Conference Paper,"Final","",Scopus,2-s2.0-85084015337
"Palmero C., Selva J., Bagheri M.A., Escalera S.","57188829002;57225366949;57139919400;22634035000;","Recurrent CNN for 3D gaze estimation using appearance and shape cues",2019,"British Machine Vision Conference 2018, BMVC 2018",,,,"","",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084012458&partnerID=40&md5=15c1ac9116bcb0a38ee15bd698b0e258","Dept. Mathematics and Informatics Universitat de Barcelona, Spain; Computer Vision Center Campus UAB, Bellaterra, Spain; Dept. Electrical and Computer Eng, University of Calgary, Canada; Dept. Engineering University of Larestan, Iran","Palmero, C., Dept. Mathematics and Informatics Universitat de Barcelona, Spain, Computer Vision Center Campus UAB, Bellaterra, Spain; Selva, J., Dept. Mathematics and Informatics Universitat de Barcelona, Spain; Bagheri, M.A., Dept. Electrical and Computer Eng, University of Calgary, Canada, Dept. Engineering University of Larestan, Iran; Escalera, S., Dept. Mathematics and Informatics Universitat de Barcelona, Spain, Computer Vision Center Campus UAB, Bellaterra, Spain","Gaze behavior is an important non-verbal cue in social signal processing and human-computer interaction. In this paper, we tackle the problem of person- and head pose-independent 3D gaze estimation from remote cameras, using a multi-modal recurrent convolutional neural network (CNN). We propose to combine face, eyes region, and face landmarks as individual streams in a CNN to estimate gaze in still images. Then, we exploit the dynamic nature of gaze by feeding the learned features of all the frames in a sequence to a many-to-one recurrent module that predicts the 3D gaze vector of the last frame. Our multi-modal static solution is evaluated on a wide range of head poses and gaze directions, achieving a significant improvement of 14.6% over the state of the art on EYEDIAP dataset, further improved by 4% when the temporal modality is included. © 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.",,"Computer vision; Human computer interaction; 3D gaze vectors; Convolutional neural network; Gaze direction; Gaze estimation; Social signal processing; State of the art; Static solutions; Temporal modalities; Recurrent neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85084012458
"Lian D., Zhang Z., Luo W., Hu L., Wu M., Li Z., Yu J., Gao S.","57203743979;57204289144;57206747739;57203744059;57207758944;36731281100;8569656400;35224747100;","RGBD based gaze estimation via multi-task CNN",2019,"33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",,,,"2488","2495",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083883072&partnerID=40&md5=749d3864ba090e5b3a101c86731a3312","ShanghaiTech University, China; Nanjing University of Science and Technology, China","Lian, D., ShanghaiTech University, China; Zhang, Z., ShanghaiTech University, China; Luo, W., ShanghaiTech University, China; Hu, L., ShanghaiTech University, China; Wu, M., ShanghaiTech University, China; Li, Z., Nanjing University of Science and Technology, China; Yu, J., ShanghaiTech University, China; Gao, S., ShanghaiTech University, China","This paper tackles RGBD based gaze estimation with Convolutional Neural Networks (CNNs). Specifically, we propose to decompose gaze point estimation into eyeball pose, head pose, and 3D eye position estimation. Compared with RGB image-based gaze tracking, having depth modality helps to facilitate head pose estimation and 3D eye position estimation. The captured depth image, however, usually contains noise and black holes which noticeably hamper gaze tracking. Thus we propose a CNN-based multi-task learning framework to simultaneously refine depth images and predict gaze points. We utilize a generator network for depth image generation with a Generative Neural Network (GAN), where the generator network is partially shared by both the gaze tracking network and GAN-based depth synthesizing. By optimizing the whole network simultaneously, depth image synthesis improves gaze point estimation and vice versa. Since the only existing RGBD dataset (EYEDIAP) is too small, we build a large-scale RGBD gaze tracking dataset for performance evaluation. As far as we know, it is the largest RGBD gaze dataset in terms of the number of participants. Comprehensive experiments demonstrate that our method outperforms existing methods by a large margin on both our dataset and the EYEDIAP dataset. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org).",,"Convolutional neural networks; Image enhancement; Large dataset; Multi-task learning; Black holes; Depth image; Eye position; Gaze estimation; Gaze point estimations; Gaze tracking; Head Pose Estimation; Large margins; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85083883072
"Wang Z., Li P., Zhang L., Shao L.","57195973746;57210250079;35231925400;55643855000;","Community-aware Photo Quality Evaluation by Deeply Encoding Human Perception",2019,"IEEE Transactions on Multimedia",,,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082027086&partnerID=40&md5=a1eb123d18e220aebfed22a2d8779139","Department of CSIE, Hefei University of Technology, Hefei, China.; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China, and also with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China.; College of Computer Sciences, Zhejiang University, Hangzhou, China.; Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, United Arab Emirates. He is also with the School of Computing Sciences, University of East Anglia, Norwich, UK.","Wang, Z., Department of CSIE, Hefei University of Technology, Hefei, China.; Li, P., School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou 310018, China, and also with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China.; Zhang, L., College of Computer Sciences, Zhejiang University, Hangzhou, China.; Shao, L., Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, United Arab Emirates. He is also with the School of Computing Sciences, University of East Anglia, Norwich, UK.","Computational photo quality evaluation is a useful technique in many tasks of computer vision and graphics, <formula><tex>$e.g.$</tex></formula>, photo retaregeting, 3D rendering, and fashion recommendation. Conventional photo quality models are designed by characterizing pictures from all communities (<formula><tex>$e.g.$</tex></formula>, &#x201C;architecture&#x201D; and &#x201C;colorful&#x201D;) indiscriminately, wherein community-specific features are not encoded explicitly. In this work, we develop a new community-aware photo quality evaluation framework. It uncovers the latent community-specific topics by a regularized latent topic model (LTM), and captures human visual quality perception by exploring multiple attributes. More specifically, given massive-scale online photos from multiple communities, a novel ranking algorithm is proposed to measure the visual/semantic attractiveness of regions inside each photo. Meanwhile, three attributes: photo quality scores, weak semantic tags, and inter-region correlations, are seamlessly and collaboratively incorporated during ranking. Subsequently, we construct gaze shifting path (GSP) for each photo by sequentially linking the top-ranking regions from each photo, and an aggregation-based deep CNN calculates the deep representation for each GSP. Based on this, an LTM is proposed to model the GSP distribution from multiple communities in the latent space. To mitigate the overfitting problem caused by communities with very few photos, a regularizer is added into our LTM. Finally, given a test photo, we obtain its deep GSP representation and its quality score is determined by the posterior probability of the regularized LTM. Comprehensive comparative studies on four image sets have shown the competitiveness of our method. Besides, eye tracking experiments demonstrated that our ranking-based GSPs are highly consistent with real human gaze movements. IEEE","Community; Deep feature; Gaze behavior; Machine learning; Quality model; Topic model","Eye movements; Eye tracking; Learning systems; Semantics; Signal encoding; Three dimensional computer graphics; Community; Deep feature; Gaze behavior; Quality modeling; Topic Modeling; Quality control",Article,"Article in Press","",Scopus,2-s2.0-85082027086
"Baharom N.H., Aid S.R., Amin M.K.M., Wibirama S., Mikami O.","57212646640;57194261972;57195992059;26654457700;57215580894;","Exploring the eye tracking data of human behaviour on consumer merchandise product",2019,"Journal of Advanced Manufacturing Technology","13","Special Issue 2",,"69","79",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081246846&partnerID=40&md5=9df65398e17bd867d6d0f730d9f59ffd","Malaysian-Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Department of Optical and Imaging Science and Technology, School of Engineering, Tokai University, Hiratsuka, Kanagawa, 259-1292, Japan","Baharom, N.H., Malaysian-Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Aid, S.R., Malaysian-Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Amin, M.K.M., Malaysian-Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Wibirama, S., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Mikami, O., Department of Optical and Imaging Science and Technology, School of Engineering, Tokai University, Hiratsuka, Kanagawa, 259-1292, Japan","This article presents an exploration of the human eye tracking data towards consumer products. The study aim to investigate the data attributes of the cognitive processes and focused on the visual attention of the participants when choosing a shampoo brand which is commonly available in Malaysia. However, eye tracking datasets has a wealth of data on the eyes visual attention, fixation, saccade and scan path gaze. Therefore, this paper aims to solve this problem to minimize the datasets by using clustering machine learning approach. This is to observe the relation of these data attributes and possibly predict the possible solution contributing to cognitive processing. Tobii TX300 Eye-tracker was used in this experiment and the eyes tracking data were gathered particularly related to the eyes fixation and saccades by using the Tobii I-VT filter. Sixty subjects participated in this study. K-means clustering was used as statistical analysis to cluster the huge datasets from the eye tracking data. The relationship of the consumer cognitive processes with visual attention was understood when most of the participants chose the most popular shampoo brand such as Head & Shoulder. Further visual analysis on the data attributes results showed that K-means clustering has the potential to cluster and minimize the huge datasets and predicts consumer preferences. © 2018 Penerbit Universiti Teknikal Malaysia Melaka.","Exploring data; Eye tracking; Human behavior; Visual attention",,Article,"Final","",Scopus,2-s2.0-85081246846
"Berga D.","57204681680;","Understanding eye movements: Psychophysics and a model of primary visual cortex",2019,"Electronic Letters on Computer Vision and Image Analysis","18","Specialissue2",,"","",2,,"10.5565/rev/elcvia.1193","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080920947&doi=10.5565%2frev%2felcvia.1193&partnerID=40&md5=98db507a5eb33edcc4e7b78103e1a47b","Department of Computer Science, Universitat Autònoma de Barcelona, Bellaterra, Spain; Computer Vision Center, Edifici O Campus UAB, Bellaterra, Spain","Berga, D., Department of Computer Science, Universitat Autònoma de Barcelona, Bellaterra, Spain, Computer Vision Center, Edifici O Campus UAB, Bellaterra, Spain","Humans move their eyes in order to learn visual representations of the world. These eye movements depend on distinct factors, either by the scene that we perceive or by our own decisions. To select what is relevant to attend is part of our survival mechanisms and the way we build reality, as we constantly react both consciously and unconsciously to all the stimuli that is projected into our eyes. In this thesis [1] we try to explain (1) how we move our eyes, (2) how to build machines that understand visual information and deploy eye movements, and (3) how to make these machines understand tasks in order to decide for eye movements. (1)We provided the analysis of eye movement behavior elicited by low-level feature distinctiveness with a dataset of 230 synthetically-generated image patterns [2]. A total of 15 types of stimuli has been generated (e.g. orientation, brightness, color, size, etc.), with 7 feature contrasts for each feature category. Eyetracking data was collected from 34 participants during the viewing of the dataset, using Free-Viewing and Visual Search task instructions. Results showed that saliency is predominantly and distinctively influenced by: 1. feature type, 2. feature contrast, 3. temporality of fixations, 4. task difficulty and 5. center bias. From such dataset (SID4VAM), we have computed a benchmark of saliency models by testing performance using psychophysical patterns [3]. Model performance has been evaluated considering model inspiration and consistency with human psychophysics. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation. (2) Computations in the primary visual cortex (area V1 or striate cortex) have long been hypothesized to be responsible, among several visual processing mechanisms, of bottom-up visual attention (also named saliency). In order to validate this hypothesis, images from eye tracking datasets have been processed with a biologically plausible model of V1 (named Neurodynamic Saliency Wavelet Model or NSWAM)[4]. Following Li's neurodynamic model, we define V1's lateral connections with a network of firing rate neurons, sensitive to visual features such as brightness, color, orientation and scale. Early subcortical processes (i.e. retinal and thalamic) are functionally simulated. The resulting saliency maps are generated from the model output, representing the neuronal activity of V1 projections towards brain areas involved in eye movement control. We want to pinpoint that our unified computational architecture is able to reproduce several visual processes (i.e. brightness, chromatic induction and visual discomfort) without applying any type of training or optimization and keeping the same parametrization. The model has been extended (NSWAMCM)[ 5] with an implementation of the cortical magnification function to define the retinotopical projections towards V1, processing neuronal activity for each distinct view during scene observation. Novel computational definitions of top-down inhibition (in terms of inhibition of return and selection mechanisms), are also proposed to predict attention in Free-Viewing and Visual Search conditions. Results show that our model outpeforms other biologically-inpired models of saliency prediction as well as to predict visual saccade sequences, specifically for nature and synthetic images. We also show how temporal and spatial characteristics of inhibition of return can improve prediction of saccades, as well as how distinct search strategies (in terms of feature-selective or category-specific inhibition) predict attention at distinct image contexts. (3) Although previous scanpath models have been able to efficiently predict saccades during Free- Viewing, it is well known that stimulus and task instructions can strongly affect eye movement patterns. In particular, task priming has been shown to be crucial to the deployment of eye movements, involving interactions between brain areas related to goal-directed behavior, working and long-term memory in combination with stimulus-driven eye movement neuronal correlates. In our latest study [6] we proposed an extension of the Selective Tuning Attentive Reference Fixation Controller Model based on task demands (STAR-FCT), describing novel computational definitions of Long-Term Memory, Visual Task Executive and Task Working Memory. With these modules we are able to use textual instructions in order to guide the model to attend to specific categories of objects and/or places in the scene. We have designed our memory model by processing a visual hierarchy of low- and high-level features. The relationship between the executive task instructions and the memory representations has been specified using a tree of semantic similarities between the learned features and the object category labels. Results reveal that by using this model, the resulting object localization maps and predicted saccades have a higher probability to fall inside the salient regions depending on the distinct task instructions compared to saliency. © 2019 Universitat Autonoma de Barcelona.","Attention; Eye movements; Firing rate; Freeviewing; Horizontal connections; Neural networks; Psychophysics; Saliency; visual cortex; Visual search",,Article,"Final","",Scopus,2-s2.0-85080920947
"Li W., Dong Q., Jia H., Zhao S., Wang Y., Xie L., Pan Q., Duan F., Liu T.","55657973600;57190214381;57213687667;56014643800;57213689160;55238809400;57213688492;24537140100;57203377182;","Training a camera to perform long-distance eye tracking by another eye-tracker",2019,"IEEE Access","7",,"8880636","155313","155324",,5,"10.1109/ACCESS.2019.2949150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077956670&doi=10.1109%2fACCESS.2019.2949150&partnerID=40&md5=d35b587d72b4dea2035a85dc0e17af07","Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, 300350, China; Cortical Architecture Imaging and Discovery Lab, University of Georgia, Athens, GA  30602, United States; School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Department of Instrument Science and Technology, Zhejiang University, Hangzhou, 310027, China; Xi'an ISoftStone Network Technology Company Ltd., Xi'an, 710100, China","Li, W., Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, 300350, China; Dong, Q., Cortical Architecture Imaging and Discovery Lab, University of Georgia, Athens, GA  30602, United States; Jia, H., Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, 300350, China; Zhao, S., School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Wang, Y., Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, 300350, China; Xie, L., Department of Instrument Science and Technology, Zhejiang University, Hangzhou, 310027, China; Pan, Q., Xi'an ISoftStone Network Technology Company Ltd., Xi'an, 710100, China; Duan, F., Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, 300350, China; Liu, T., Cortical Architecture Imaging and Discovery Lab, University of Georgia, Athens, GA  30602, United States","Appearance-based gaze estimation techniques have been greatly advanced in these years. However, using a single camera for appearance-based gaze estimation has been limited to short distance in previous studies. In addition, labeling of training samples has been a time-consuming and unfriendly step in previous appearance-based gaze estimation studies. To bridge these significant gaps, this paper presents a new long-distance gaze estimation paradigm: Train a camera to perform eye tracking by another eye tracker, named Learning-based Single Camera eye tracker (LSC eye-tracker). In the training stage, the LSC eye-tracker simultaneously acquired gaze data by a commercial trainer eye tracker and face appearance images by a long-distance trainee camera, based on which deep convolutional neural network (CNN) models are utilized to learn the mapping from appearance images to gazes. In the application stage, the LSC eye-tracker works alone to predict gazes based on the acquired appearance images by the single camera and the trained CNN models. Our experimental results show that the LSC eye-tracker enables both population-based eye tracking and personalized eye tracking with promising accuracy and performance. © 2013 IEEE.","Eye tracking; gaze estimation; human-computer interaction; machine learning","Cameras; Deep neural networks; Human computer interaction; Learning systems; Neural networks; Appearance based; CNN models; Convolutional neural network; Eye trackers; Gaze estimation; Single cameras; Training sample; Eye tracking",Article,"Final","",Scopus,2-s2.0-85077956670
"Zhdanov A., Zhdanov D., Potemin I., Bogdanov N., Bykovskii S.","56801420900;57213542823;54391601500;57202741634;57216469537;","Possibility of vergence disagreement reducing on the base of approximate restoration of the depth map",2019,"Proceedings of SPIE - The International Society for Optical Engineering","11185",,"1118517","","",,,"10.1117/12.2537753","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077790426&doi=10.1117%2f12.2537753&partnerID=40&md5=fe9c0cc1fa86d3ded65cd7d72de96cb6","ITMO University, 49 Kronverksky Pr., St. Petersburg, 197101, Russian Federation","Zhdanov, A., ITMO University, 49 Kronverksky Pr., St. Petersburg, 197101, Russian Federation; Zhdanov, D., ITMO University, 49 Kronverksky Pr., St. Petersburg, 197101, Russian Federation; Potemin, I., ITMO University, 49 Kronverksky Pr., St. Petersburg, 197101, Russian Federation; Bogdanov, N., ITMO University, 49 Kronverksky Pr., St. Petersburg, 197101, Russian Federation; Bykovskii, S., ITMO University, 49 Kronverksky Pr., St. Petersburg, 197101, Russian Federation","The article describes the approach that allows to reconstruct the image formed by the video see-through mixed reality system corresponding to the convergence of the device user eyes. Convergence is defined by the user eye pupils position acquired from the mixed reality device eye tracking system. The image reconstruction method is based on the use of an extended (2.5-dimensional) representation of the image obtained, for example, using a 3D scanner that builds a depth map of the scene. In the proposed solution, lens optical systems that form images of the real world on LCD screens and eyepieces that project these images into the user eyes do not change their characteristics and position. The image is reconstructed by projecting the points of the original image to the image points corresponding to the required convergence by the method of ""refocusing"" at a distance for each point. The advantages and disadvantages of this method are shown. An approach is proposed that reduces visual perception discomfort caused by an ambiguous distance to the image point, for example, in the case of mirror or transparent objects. Virtual prototyping of the mixed reality system showed the benefits of the proposed approach to reduce the visual perception discomfort caused by the mismatch between the convergence of human eyes and the images formed by the lenses of the mixed reality system. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Eye tracking; Mixed reality; Photorealistic rendering; Vergence conflict; Virtual reality; Visual perception","Eye tracking; Lenses; Liquid crystal displays; Mixed reality; Optical design; Virtual reality; Vision; Eye tracking systems; Image reconstruction methods; Mixed reality systems; Original images; Photorealistic rendering; Transparent objects; Vergences; Visual perception; Image reconstruction",Conference Paper,"Final","",Scopus,2-s2.0-85077790426
"Xu B., Li X., Wang Y.","56780818100;57213267610;56032563800;","Wide color gamut autostereoscopic 2d-3d switchable display based on dynamic subpixel arrangement",2019,"IEEE Access","7",,"8902114","167860","167868",,,"10.1109/ACCESS.2019.2953796","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077746404&doi=10.1109%2fACCESS.2019.2953796&partnerID=40&md5=882b6ac60057285cf82053bba51198bd","School of Electronic Science and Engineering, Nanjing University, Nanjing, 210023, China","Xu, B., School of Electronic Science and Engineering, Nanjing University, Nanjing, 210023, China; Li, X., School of Electronic Science and Engineering, Nanjing University, Nanjing, 210023, China; Wang, Y., School of Electronic Science and Engineering, Nanjing University, Nanjing, 210023, China","This paper is a report on the mathematical analysis and working principle of a wide color gamut autostereoscopic 2D-3D switchable display based on the dynamic subpixel arrangement method. The display prototype discussed in the paper has three major distinctions. First, the use of dynamic subpixel arrangement method and eye-tracking system has improved its performances in the resolution, crosstalk and 2D/3D switching statistics. Second, a design of Quantum-Dot-Polymer (QDP) film and optical layer combined backlight has enhanced its viewing angle and color gamut. Third, the application of parallel computing to the dynamic subpixel arrangement method has improved its real-time performance. Base on observation from finished fabrication and experiment, this prototype has already demonstrated noticeable enhancement in terms of color gamut expansion-reaching 77.98% according to ITU-R Recommendation BT.2020 (Rec.2020), and crosstalk reduction-with the minimum crosstalk rate at nearly 6%. Close comparison with two other commercial 3D displays (BENQ XL2707-B and View Sonic VX2268WM) are also presented in the paper for sufficiency. © 2013 IEEE.","3-D display; color gamut; dynamic subpixel arrangement; quantum dot","Color; Crosstalk; Eye tracking; Frequency bands; Nanocrystals; Pixels; Semiconductor quantum dots; Stereo image processing; 3-D displays; Color gamuts; Crosstalk reduction; Eye tracking systems; ITU-R recommendation; Mathematical analysis; Real time performance; Sub pixels; Three dimensional displays",Article,"Final","",Scopus,2-s2.0-85077746404
"Sun Z., Wang X., Zhang Q., Jiang J.","57213195468;35316453700;57192588517;57193403110;","Real-Time Video Saliency Prediction Via 3D Residual Convolutional Neural Network",2019,"IEEE Access","7",,"8863376","147743","147754",,5,"10.1109/ACCESS.2019.2946479","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077718706&doi=10.1109%2fACCESS.2019.2946479&partnerID=40&md5=8eaf8d95fcf68cd2218ae5598ae3dd12","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Department of Computer Science, City University of Hong Kong, Hong Kong, Hong Kong","Sun, Z., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Wang, X., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Zhang, Q., Department of Computer Science, City University of Hong Kong, Hong Kong, Hong Kong; Jiang, J., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China","Attention is a fundamental attribute of human visual system that plays important roles in many visual perception tasks. The key issue of video saliency lies in how to efficiently exploit the temporal information. Instead of singling out the temporal saliency maps, we propose a real-time end-to-end video saliency prediction model via 3D residual convolutional neural network (3D-ResNet), which incorporates the prediction of spatial and temporal saliency maps into one single process. In particular, a multi-scale feature representation scheme is employed to further boost the model performance. Besides, a frame skipping strategy is proposed for speeding up the saliency map inference process. Moreover, a new challenging eye tracking database with 220 video clips is established to facilitate the research of video saliency prediction. Extensive experimental results show our model outperforms the state-of-the-art methods over the eye fixation datasets in terms of both prediction accuracy and inference speed. © 2013 IEEE.","3D residual convolutional neural network; eye fixation dataset; Video saliency prediction","Convolution; Eye tracking; Forecasting; Image segmentation; Neural networks; Convolutional neural network; Eye fixations; Fundamental attributes; Human Visual System; Multi-scale features; State-of-the-art methods; Temporal information; Video saliencies; 3D modeling",Article,"Final","",Scopus,2-s2.0-85077718706
"Tanaka M., Lanaro M.P., Horiuchi T., Rizzi A.","55502310300;57195470572;55420359700;56962787100;","Random spray retinex extensions considering region of interest and eyemovements",2019,"Journal of Imaging Science and Technology","63","6","060403-1","","",,,"10.2352/J.ImagingSci.Technol.2019.63.6.060403","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077471900&doi=10.2352%2fJ.ImagingSci.Technol.2019.63.6.060403&partnerID=40&md5=d35a586054bc0df4d966624a3e9c398b","College of Liberal Arts and Sciences, Chiba University, Chiba, Japan; Department of Computer Science, University of Milano, Milano, Italy; Graduate School of Engineering, Chiba University, Chiba, Japan","Tanaka, M., College of Liberal Arts and Sciences, Chiba University, Chiba, Japan; Lanaro, M.P., Department of Computer Science, University of Milano, Milano, Italy; Horiuchi, T., Graduate School of Engineering, Chiba University, Chiba, Japan; Rizzi, A., Department of Computer Science, University of Milano, Milano, Italy","The Random spray Retinex (RSR) algorithm was developed by taking into consideration the mathematical description of Milano-Retinex. The RSR substituted random paths with random sprays. Mimicking some characteristics of the human visual system (HVS), this article proposes two variants of RSR adding a mechanism of region of interest (ROI). In the first proposed model, a cone distribution based on anatomical data is considered as ROI. In the second model, the visual resolution depending on the visual field based on the knowledge of visual information processing is considered as ROI. We have measured actual eye movements using an eye-tracking system. By using the eye-tracking data, we have simulated the HVS using test images. Results show an interesting qualitative computation of the appearance of the processed area around real gaze points. © 2019 Society for Imaging Science and Technology.",,"Eye movements; Image segmentation; Spectral resolution; Eye tracking systems; Human visual systems; Mathematical descriptions; Random paths; Region of interest; Visual fields; Visual information processing; Visual resolutions; Eye tracking",Article,"Final","",Scopus,2-s2.0-85077471900
"Song H., Lee K., Moon N.","57194339274;57212487483;36176323600;","User modeling using user preference and user life pattern based on personal bio data and sns data",2019,"Journal of Information Processing Systems","15","3",,"1","10",,4,"10.3745/JIPS.01.0044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077236364&doi=10.3745%2fJIPS.01.0044&partnerID=40&md5=c3da05633625514860826b3161be1927","Dept. of Computer and Engineering, Hoseo University, Asan, South Korea","Song, H., Dept. of Computer and Engineering, Hoseo University, Asan, South Korea; Lee, K., Dept. of Computer and Engineering, Hoseo University, Asan, South Korea; Moon, N., Dept. of Computer and Engineering, Hoseo University, Asan, South Korea","The purpose of this study was to collect and analyze personal bio data and social network services (SNS) data, derive user preference and user life pattern, and propose intuitive and precise user modeling. This study not only tried to conduct eye tracking experiments using various smart devices to be the ground of the recommendation system considering the attribute of smart devices, but also derived classification preference by analyzing eye tracking data of collected bio data and SNS data. In addition, this study intended to combine and analyze preference of the common classification of the two types of data, derive final preference by each smart device, and based on user life pattern extracted from final preference and collected bio data (amount of activity, sleep), draw the similarity between users using Pearson correlation coefficient. Through derivation of preference considering the attribute of smart devices, it could be found that users would be influenced by smart devices. With user modeling using user behavior pattern, eye tracking, and user preference, this study tried to contribute to the research on the recommendation system that should precisely reflect user tendency. © 2019 KIPS.","Bio data; Data tracking; Life pattern; Machine learning; Social behavior analysis; User modeling","Classification (of information); Clustering algorithms; Correlation methods; Eye tracking; Learning systems; Recommender systems; Social networking (online); Bio data; Data-tracking; Life pattern; Social behavior; User Modeling; Behavioral research",Article,"Final","",Scopus,2-s2.0-85077236364
"Chen H.-H., Hwang B.-J., Hsu W.-H., Kao C.-W., Chen W.-T.","37035935400;7201453946;17434693900;53984348300;57212604646;","Focus on Area Tracking Based on Deep Learning for Multiple Users",2019,"IEEE Access","7",,"8918265","179477","179491",,,"10.1109/ACCESS.2019.2956953","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077187881&doi=10.1109%2fACCESS.2019.2956953&partnerID=40&md5=227dff81a68d4857b8c9487c6794b3a0","Department of Computer and Communication Engineering, Ming Chuan University, Taipei, 111, Taiwan; Department of Information Management, National Taichung University of Science and Technology, Taichung, 404, Taiwan; Department of Applied Information Technology, Hsing Wu University of Science and Technology, Taipei, 24452, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taoyuan, 320, Taiwan","Chen, H.-H., Department of Computer and Communication Engineering, Ming Chuan University, Taipei, 111, Taiwan; Hwang, B.-J., Department of Computer and Communication Engineering, Ming Chuan University, Taipei, 111, Taiwan; Hsu, W.-H., Department of Information Management, National Taichung University of Science and Technology, Taichung, 404, Taiwan; Kao, C.-W., Department of Applied Information Technology, Hsing Wu University of Science and Technology, Taipei, 24452, Taiwan, Department of Computer Science and Information Engineering, National Central University, Taoyuan, 320, Taiwan; Chen, W.-T., Department of Computer and Communication Engineering, Ming Chuan University, Taipei, 111, Taiwan","Most eye-tracking experiments are limited to single subjects because gaze points are difficult to track when multiple users are involved and environmental factors might cause interference. To overcome this problem, this paper proposes a method for gaze tracking that can be applied for multiple users simultaneously. Four models, including FASEM, FAEM and FAFRCM in the signal-user environment, as well as FAEM and FAMAM in the multiple-user environment, are proposed, and we collected raw data of gazing behaviors to train the models. Through a modified VGG19 architecture and adjusting the Number of Convolutional Layers (NoCL), we obtained and compared the accuracy of various models to determine the most suitable architecture. Since data for multiple-users is not easy to obtain, in this paper, we trained the model first with single users, then extended it to multiple users with transfer learning. Finally, we propose an adaptive method to integrate the benefits of FAEM and FAMAM. © 2013 IEEE.","deep learning; gaze-tracking; Multiple users; position clustering","Eye tracking; Network-on-chip; Adaptive methods; Environmental factors; Gaze point; Gaze tracking; Multiple user; position clustering; Single users; Transfer learning; Deep learning",Article,"Final","",Scopus,2-s2.0-85077187881
"Wang C.-C., Hung J.C., Wang S.-C., Huang Y.-M.","7501632228;7201963626;57212476915;8630348700;","Visual Attention Analysis During Program Debugging Using Virtual Reality Eye Tracker",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11937 LNCS",,,"97","106",,1,"10.1007/978-3-030-35343-8_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076750485&doi=10.1007%2f978-3-030-35343-8_11&partnerID=40&md5=ccba9694bbc0ec730b0975f972d94f9f","School of Information and Design, Chang Jung Christian University, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Taichung University of Science and Technology, Taichung, Taiwan; Department of Engineering Science, National Cheng Kung University, Tainan, Taiwan","Wang, C.-C., School of Information and Design, Chang Jung Christian University, Tainan, Taiwan; Hung, J.C., Department of Computer Science and Information Engineering, National Taichung University of Science and Technology, Taichung, Taiwan; Wang, S.-C., Department of Engineering Science, National Cheng Kung University, Tainan, Taiwan; Huang, Y.-M., Department of Engineering Science, National Cheng Kung University, Tainan, Taiwan","The immersion of virtual reality (VR) has transcended the existing experience of multimedia teaching. This paper aims to design a virtual reality eye tracker device to analyze the cognitive process of program debugging by adopting virtual reality technology to build a 3D code rendering system and, at the same time, using eye tracking technology to study visual attention as well as to analyze and compare the differences in internal behavioral cognition in terms of program debugging. This paper has 32 students as participants who have studied C++ programming language courses for more than one year in the department of computer science. With Unity 3D development tool, the experiment creates a virtual classroom scene and C++ programming language code. The participants’ eye movements are recorded by an eye tracker device integrated in a Head-Mounted Display (HMD). The eye movement defines the regions of interest (ROIs) according to the division of the program’s function, and the difference in visual attention between various ROIs in the code is discussed when the participant performs the program debugging task. The finding results are expected to improve the dilemma of the existing programming teaching, so that the instructors can provide appropriate teaching aids for students to achieve the purpose of programming teaching and improving the students’ programming competence. © Springer Nature Switzerland AG 2019.","Eye movement analysis; Program debugging; Virtual reality; Visual behavior","Behavioral research; Codes (symbols); Computer aided instruction; Engineering education; Eye movements; Eye tracking; Helmet mounted displays; Program debugging; Students; Teaching; Three dimensional computer graphics; Virtual reality; Eye movement analysis; Eye tracking technologies; Head mounted displays; Multimedia teachings; Programming teaching; Regions of interest; Virtual reality technology; Visual behavior; C++ (programming language)",Conference Paper,"Final","",Scopus,2-s2.0-85076750485
"Ebrahimpour M.K., Falandays J.B., Spevack S., Noelle D.C.","57208018803;57202309741;57191309692;6602598032;","Do Humans Look Where Deep Convolutional Neural Networks “Attend”?",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11845 LNCS",,,"53","65",,,"10.1007/978-3-030-33723-0_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076254946&doi=10.1007%2f978-3-030-33723-0_5&partnerID=40&md5=0fb66dc560cb0ff05fd7d4922e5970d9","EECS, University of California, Merced, United States; Cognitive and Information Sciences, University of California, Merced, United States","Ebrahimpour, M.K., EECS, University of California, Merced, United States; Falandays, J.B., Cognitive and Information Sciences, University of California, Merced, United States; Spevack, S., Cognitive and Information Sciences, University of California, Merced, United States; Noelle, D.C., EECS, University of California, Merced, United States, Cognitive and Information Sciences, University of California, Merced, United States","Deep Convolutional Neural Networks (CNNs) have recently begun to exhibit human level performance on some visual perception tasks. Performance remains relatively poor, however, on some vision tasks, such as object detection: specifying the location and object class for all objects in a still image. We hypothesized that this gap in performance may be largely due to the fact that humans exhibit selective attention, while most object detection CNNs have no corresponding mechanism. In examining this question, we investigated some well-known attention mechanisms in the deep learning literature, identifying their weaknesses and leading us to propose a novel attention algorithm called the Densely Connected Attention Model. We then measured human spatial attention, in the form of eye tracking data, during the performance of an analogous object detection task. By comparing the learned representations produced by various CNN architectures with that exhibited by human viewers, we identified some relative strengths and weaknesses of the examined computational attention mechanisms. Some CNNs produced attentional patterns somewhat similar to those of humans. Others focused processing on objects in the foreground. Still other CNN attentional mechanisms produced usefully interpretable internal representations. The resulting comparisons provide insights into the relationship between CNN attention algorithms and the human visual system. © 2019, Springer Nature Switzerland AG.","Class Activation Maps; Computer vision; Convolutional Neural Networks; Densely connected attention maps; Sensitivity analysis; Visual spatial attention","Activation analysis; Computer vision; Convolution; Eye tracking; Neural networks; Object detection; Object recognition; Sensitivity analysis; Spatial variables measurement; Activation maps; Attention mechanisms; Attentional mechanism; Convolutional neural network; Human-level performance; Internal representation; Selective attention; Visual spatial attention; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85076254946
"Chen M., Jin Y., Goodall T., Yu X., Bovik A.C.","57212195714;57225874348;55513291000;57203982855;56984291600;","Study of 3D Virtual Reality Picture Quality",2019,"IEEE Journal on Selected Topics in Signal Processing",,,,"","",,2,"10.1109/JSTSP.2019.2956408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076160873&doi=10.1109%2fJSTSP.2019.2956408&partnerID=40&md5=efdb4e9b39d805b12bbbca6643069594","Electrical and Computer Engineering, University of Texas at Austin, 12330 Austin, Texas United States 78712-1139 (e-mail: chenmx@utexas.edu); Electrical and Computer Engineering, University of Texas at Austin, 12330 Austin, Texas United States (e-mail: yizejin@utexas.edu); Facebook Reality Labs, California United States (e-mail: Todd.Goodall@oculus.com); Electrical and Computer Engineering, The University of Texas at Austin, Austin, Texas United States 78712 (e-mail: yuxiangxu@utexas.edu); Dept of Electrical &amp; Computer Engineering, University of Texas-Austin, Austin, Texas United States 78712-1084 (e-mail: bovik@ece.utexas.edu)","Chen, M., Electrical and Computer Engineering, University of Texas at Austin, 12330 Austin, Texas United States 78712-1139 (e-mail: chenmx@utexas.edu); Jin, Y., Electrical and Computer Engineering, University of Texas at Austin, 12330 Austin, Texas United States (e-mail: yizejin@utexas.edu); Goodall, T., Facebook Reality Labs, California United States (e-mail: Todd.Goodall@oculus.com); Yu, X., Electrical and Computer Engineering, The University of Texas at Austin, Austin, Texas United States 78712 (e-mail: yuxiangxu@utexas.edu); Bovik, A.C., Dept of Electrical &amp; Computer Engineering, University of Texas-Austin, Austin, Texas United States 78712-1084 (e-mail: bovik@ece.utexas.edu)","Virtual Reality (VR) and its applications have attracted significant and increasing attention. However, the requirements of much larger file sizes, different storage formats, and immersive viewing conditions pose significant challenges to the goals of acquiring, transmitting, compressing and displaying high quality VR content. Towards meeting these challenges, it is important to be able to understand the distortions that arise and that can affect the perceived quality of displayed VR content. It is also important to develop ways to automatically predict VR picture quality. Meeting these challenges requires basic tools in the form of large, representative subjective VR quality databases on which VR quality models can be developed and which can be used to benchmark VR quality prediction algorithms. Towards making progress in this direction, here we present the results of an immersive 3D subjective image quality assessment study. In the study, 450 distorted images obtained from 15 pristine 3D VR images modified by 6 types of distortion of varying severities were evaluated by 42 subjects in a controlled VR setting. Both the subject ratings as well as eye tracking data were recorded and made available as part of the new database, in hopes that the relationships between gaze direction and perceived quality might be better understood. We also evaluated several public available IQA models on the new database, and also report a statistical evaluation of the performances of the compared IQA models. IEEE","full reference; human perception; image quality assessment; immersive image database; virtual reality","Benchmarking; Database systems; Digital storage; Eye tracking; Virtual reality; 3D virtual reality; Full references; Human perception; Image database; Image quality assessment; Statistical evaluation; Subjective image quality; Viewing conditions; Image quality",Article,"Article in Press","",Scopus,2-s2.0-85076160873
"Patra A., Cai Y., Chatelain P., Sharma H., Drukker L., Papageorghiou A.T., Noble J.A.","57195771466;57202376461;55891653300;56272987100;36241434600;57194082999;56185660000;","Efficient ultrasound image analysis models with sonographer gaze assisted distillation",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11767 LNCS",,,"394","402",,2,"10.1007/978-3-030-32251-9_43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075690650&doi=10.1007%2f978-3-030-32251-9_43&partnerID=40&md5=48fdff2f6f8aa3560fc7dff1ba24458e","University of Oxford, Oxford, OX3 7DQ, United Kingdom","Patra, A., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Cai, Y., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Chatelain, P., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Sharma, H., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Drukker, L., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Papageorghiou, A.T., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Noble, J.A., University of Oxford, Oxford, OX3 7DQ, United Kingdom","Recent automated medical image analysis methods have attained state-of-the-art performance but have relied on memory and compute-intensive deep learning models. Reducing model size without significant loss in performance metrics is crucial for time and memory-efficient automated image-based decision-making. Traditional deep learning based image analysis only uses expert knowledge in the form of manual annotations. Recently, there has been interest in introducing other forms of expert knowledge into deep learning architecture design. This is the approach considered in the paper where we propose to combine ultrasound video with point-of-gaze tracked for expert sonographers as they scan to train memory-efficient ultrasound image analysis models. Specifically we develop teacher-student knowledge transfer models for the exemplar task of frame classification for the fetal abdomen, head, and femur. The best performing memory-efficient models attain performance within 5% of conventional models that are 1000× larger in size. © Springer Nature Switzerland AG 2019.","Expert knowledge; Gaze tracking; Model compression","Decision making; Deep learning; Distillation; Distilleries; Eye tracking; Knowledge management; Medical computing; Medical imaging; Ultrasonics; Conventional models; Expert knowledge; Gaze tracking; Learning architectures; Model compression; Performance metrics; State-of-the-art performance; Ultrasound image analysis; Image analysis",Conference Paper,"Final","",Scopus,2-s2.0-85075690650
"Kogkas A., Ezzat A., Thakkar R., Darzi A., Mylonas G.","56624483200;57221503521;57212005836;14633357600;13905959400;","Free-View, 3D Gaze-Guided Robotic Scrub Nurse",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11768 LNCS",,,"164","172",,2,"10.1007/978-3-030-32254-0_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075688399&doi=10.1007%2f978-3-030-32254-0_19&partnerID=40&md5=a1e6b32733bff5988273b0b403e6c561","HARMS Lab, Department of Surgery and Cancer, Imperial College London, St Mary’s Hospital, London, United Kingdom; St George’s, University of London, London, United Kingdom; Department of Surgery and Cancer, Imperial College London, St Mary’s Hospital, London, United Kingdom","Kogkas, A., HARMS Lab, Department of Surgery and Cancer, Imperial College London, St Mary’s Hospital, London, United Kingdom; Ezzat, A., HARMS Lab, Department of Surgery and Cancer, Imperial College London, St Mary’s Hospital, London, United Kingdom; Thakkar, R., St George’s, University of London, London, United Kingdom; Darzi, A., Department of Surgery and Cancer, Imperial College London, St Mary’s Hospital, London, United Kingdom; Mylonas, G., HARMS Lab, Department of Surgery and Cancer, Imperial College London, St Mary’s Hospital, London, United Kingdom","We introduce a novel 3D gaze-guided robotic scrub nurse (RN) and test the platform in simulated surgery to determine usability and acceptability with clinical teams. Surgeons and trained scrub nurses performed an ex vivo task on pig colon. Surgeons used gaze via wearable eye-tracking glasses to select surgical instruments on a screen, in turn initiating RN to deliver the instrument. Comparison was done between human- and robot-assisted tasks (HT vs RT). Real-time gaze-screen interaction was based on a framework developed with synergy of conventional wearable eye-tracking, motion capture system and RGB-D cameras. NASA-TLX and Van der Laan’s technology acceptance questionnaires were collected and analyzed. 10 teams of surgical trainees (ST) and scrub nurses (HN) participated. Overall, NASA-TLX feedback was positive. ST and HN revealed no statistically significant difference in overall task load. Task performance feedback was unaffected. Frustration was reported by ST. Overall, Van der Laan’s scores showed positive usefulness and satisfaction scores following RN use. There was no significant difference in task interruptions across HT vs RT. Similarly, no statistical difference was found in duration to task completion in both groups. Quantitative and qualitative feedback was positive. The source of frustration has been understood. Importantly, there was no significant difference in task workflow or operative time, with overall perceptions towards task performance remaining unchanged in HT vs RT. © 2019, Springer Nature Switzerland AG.","Gaze interactions; Robotic scrub nurse; Smart operating room","Mammals; Medical computing; Medical imaging; Motion tracking; NASA; Nursing; Robotic surgery; Robotics; Surgery; Surgical equipment; Surveys; Wearable technology; Gaze interaction; Motion capture system; Qualitative feedback; Robotic scrub nurse; Statistical differences; Statistically significant difference; Surgical instrument; Technology acceptance; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85075688399
"Lu C., Uchiyama H., Thomas D., Shimada A., Taniguchi R.-I.","57204773485;35318642200;36100817900;57222646656;7005112455;","Multi-pedestrian tracking system based on asynchronized IMUs",2019,"CEUR Workshop Proceedings","2498",,,"447","454",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075634985&partnerID=40&md5=3176c9decbc4a240dda415b18efb5504","Kyushu University, Fukuoka, Japan","Lu, C., Kyushu University, Fukuoka, Japan; Uchiyama, H., Kyushu University, Fukuoka, Japan; Thomas, D., Kyushu University, Fukuoka, Japan; Shimada, A., Kyushu University, Fukuoka, Japan; Taniguchi, R.-I., Kyushu University, Fukuoka, Japan","We propose a multi-pedestrian tracking system based on MEMS based IMUs as a novel tool for human behavior analysis. With asynchronized multiple IMUs, our system can track IMU-attached pedestrians in synchronization at a high frame rate in the large environment, compared with vision based approaches. The output data is similar to standard PDR systems as follows: the time-series position, velocity, and heading of the pedestrians in the 3D space. To realize our system, we propose a simple but effective calibration technique for synchronizing the timelines of the asynchronized IMUs. With our system, users can analyze the detailed motion behaviors of the people who participate in a group work or a collective activity, quantitatively. By combining with other sensors such as an eye tracker, our system can further provide more comprehensive data in the experiments. © 2019 CEUR Workshop Proceedings. All rights reserved.","Calibration; Inertial navigation; Multi-pedestrian","Behavioral research; Calibration; Navigation; Paper; Calibration techniques; High frame rate; Human behavior analysis; Inertial navigations; Motion behavior; Multi-pedestrian; Pedestrian tracking; Vision-based approaches; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85075634985
"Lyudvichenko V.A., Vatolin D.S.","14067590000;6506528788;","Predicting video saliency using crowdsourced mouse-tracking data",2019,"CEUR Workshop Proceedings","2485",,,"127","130",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074734896&partnerID=40&md5=5f8a764e6262cb58cccf93474185eeba","Lomonosov Moscow State University, Moscow, Russian Federation","Lyudvichenko, V.A., Lomonosov Moscow State University, Moscow, Russian Federation; Vatolin, D.S., Lomonosov Moscow State University, Moscow, Russian Federation","This paper presents a new way of getting high-quality saliency maps for video, using a cheaper alternative to eye-tracking data. We designed a mouse-contingent video viewing system which simulates the viewers’ peripheral vision based on the position of the mouse cursor. The system enables the use of mouse-tracking data recorded from an ordinary computer mouse as an alternative to real gaze fixations recorded by a more expensive eye-tracker. We developed a crowdsourcing system that enables the collection of such mouse-tracking data at large scale. Using the collected mouse-tracking data we showed that it can serve as an approximation of eye-tracking data. Moreover, trying to increase the efficiency of collected mouse-tracking data we proposed a novel deep neural network algorithm that improves the quality of mouse-tracking saliency maps. Copyright © 2019 for this paper by its authors.","Crowdsourcing; Deep learning; Eye tracking; Mouse tracking; Saliency; Visual attention","Behavioral research; Computer graphics; Computer vision; Crowdsourcing; Deep learning; Deep neural networks; Eye tracking; Computer mouse; Neural network algorithm; Peripheral vision; Saliency; Tracking data; Video saliencies; Viewing systems; Visual Attention; Mammals",Conference Paper,"Final","",Scopus,2-s2.0-85074734896
"Shen R., Weng D., Guo J., Fang H., Jiang H.","57192313769;24386043400;57189033244;57211428575;57208692158;","Effects of Dynamic Disparity on Visual Fatigue Caused by Watching 2D Videos in HMDs",2019,"Communications in Computer and Information Science","1043",,,"310","321",,2,"10.1007/978-981-13-9917-6_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073908573&doi=10.1007%2f978-981-13-9917-6_30&partnerID=40&md5=fc057f7f62b974bb357c72480e8711c0","Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology, Beijing, China; AICFVE of Beijing Film Academy, 4, Xitucheng Rd, Haidian, Beijing, China","Shen, R., Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology, Beijing, China; Weng, D., Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology, Beijing, China, AICFVE of Beijing Film Academy, 4, Xitucheng Rd, Haidian, Beijing, China; Guo, J., Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology, Beijing, China; Fang, H., Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology, Beijing, China; Jiang, H., Beijing Engineering Research Center of Mixed Reality and Advanced Display, School of Optics and Photonics, Beijing Institute of Technology, Beijing, China","As working at a video display terminal (VDT) for a long time can induce visual fatigue, this paper proposed a method to use dynamic disparity on the situation of video watching in head-mounted displays (HMD), based on the accommodative training. And an experiment was designed to evaluate whether it can alleviate visual fatigue. Subjective and objective methods were combined in the experiment under different disparity conditions to evaluate the visual fatigue of the subjects. The objective assessment was the blink frequency of the subjects, achieved by the eye tracker. The subjective assessment was questionnaire. However, we came to the conclusion that dynamic disparity caused by the movement of left and right eye images in the HMD can’t effectively alleviate visual fatigue. According to the change of the average eye blink frequency ratio of the subjects during the experiment, the change of the visual fatigue over time was analyzed. © 2019, Springer Nature Singapore Pte Ltd.","Accommodative training; Blink; Dynamic disparity; Eye tracking; HMD; Visual fatigue","Computer terminals; Eye movements; Eye tracking; Blink; Blink frequencies; Head mounted displays; Objective assessment; Objective methods; Subjective assessments; Video display terminals; Visual fatigue; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85073908573
"Favorskaya M.N., Jain L.C.","36598108700;57210558830;","Saliency detection in deep learning era: Trends of development",2019,"Informatsionno-Upravliaiushchie Sistemy",,"3",,"10","36",,3,"10.31799/1684-8853-2019-3-10-36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073831232&doi=10.31799%2f1684-8853-2019-3-10-36&partnerID=40&md5=5330cca2974f05faf4299c7500c5083b","Reshetnev Siberian State University of Science and Technology, 31, Krasnoyarsky Rabochy Ave., Krasnoyarsk, 660037, Russian Federation; University of Canberra, 11 Kirinari St., Bruce, Canberra, ACT  2617, Australia; Liverpool Hope University, Hope Park, Liverpool, L16 9JD, United Kingdom; University of Technology Sydney, PO Box 123, Broadway, Sydney, NSW  2007, Australia","Favorskaya, M.N., Reshetnev Siberian State University of Science and Technology, 31, Krasnoyarsky Rabochy Ave., Krasnoyarsk, 660037, Russian Federation; Jain, L.C., University of Canberra, 11 Kirinari St., Bruce, Canberra, ACT  2617, Australia, Liverpool Hope University, Hope Park, Liverpool, L16 9JD, United Kingdom, University of Technology Sydney, PO Box 123, Broadway, Sydney, NSW  2007, Australia","Introduction: Saliency detection is a fundamental task of computer vision. Its ultimate aim is to localize the objects of interest that grab human visual attention with respect to the rest of the image. A great variety of saliency models based on different approaches was developed since 1990s. In recent years, the saliency detection has become one of actively studied topic in the theory of Convolutional Neural Network (CNN). Many original decisions using CNNs were proposed for salient object detection and, even, event detection. Purpose: A detailed survey of saliency detection methods in deep learning era allows to understand the current possibilities of CNN approach for visual analysis conducted by the human eyes' tracking and digital image processing. Results: A survey reflects the recent advances in saliency detection using CNNs. Different models available in literature, such as static and dynamic 2D CNNs for salient object detection and 3D CNNs for salient event detection are discussed in the chronological order. It is worth noting that automatic salient event detection in durable videos became possible using the recently appeared 3D CNN combining with 2D CNN for salient audio detection. Also in this article, we have presented a short description of public image and video datasets with annotated salient objects or events, as well as the often used metrics for the results' evaluation. Practical relevance: This survey is considered as a contribution in the study of rapidly developed deep learning methods with respect to the saliency detection in the images and videos. © 2019 Saint Petersburg State University of Aerospace Instrumentation. All rights reserved.","Convolutional neural network; Deep learning; Feature extraction; Salient event detection; Salient object detection; Salient region detection",,Article,"Final","",Scopus,2-s2.0-85073831232
"Ahn Y.-K., Park Y.-C.","8666104800;11040176400;","Natural user interface-based car infotainment control system",2019,"Multi Conference on Computer Science and Information Systems, MCCSIS 2019 - Proceedings of the International Conferences on Interfaces and Human Computer Interaction 2019, Game and Entertainment Technologies 2019 and Computer Graphics, Visualization, Computer Vision and Image Processing 2019",,,,"363","367",,1,"10.33965/ihci2019_201906c049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073113068&doi=10.33965%2fihci2019_201906c049&partnerID=40&md5=24b9226c63309db9de14ee94b88b1612","Korea Electronics Technology Institute, 121-835, 8th Floor, #1599, Sangam-Dong, Mapo-Gu, Seoul, South Korea","Ahn, Y.-K., Korea Electronics Technology Institute, 121-835, 8th Floor, #1599, Sangam-Dong, Mapo-Gu, Seoul, South Korea; Park, Y.-C., Korea Electronics Technology Institute, 121-835, 8th Floor, #1599, Sangam-Dong, Mapo-Gu, Seoul, South Korea","This article proposes a natural user interface (NUI) system for the control of infotainment content in a smart car that recognizes a user's emotions and gaze with a 2D camera, an eye camera. The configuration of the system used to recognize a user's emotions and gaze is introduced. The methods for recognizing a user's emotions and gaze are described. An experiment was performed to evaluate the performance of emotions and gaze recognition, and to demonstrate the user performance of the system proposed in this article. © Copyright 2019 IADIS Press. All rights reserved.","Eye Tracking; Face Recognition; Natural User Interface; Smart Car","Cameras; Computer games; Computer graphics; Computer vision; Eye tracking; Face recognition; Human computer interaction; Visualization; Car infotainment; Eye camera; Infotainment; Natural user interfaces; Smart car; User performance; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85073113068
"Wang H., Chen W.-W., Sun C.-T.","55579674000;57211240614;57211242649;","How gaming experience influences new game learning",2019,"Multi Conference on Computer Science and Information Systems, MCCSIS 2019 - Proceedings of the International Conferences on Interfaces and Human Computer Interaction 2019, Game and Entertainment Technologies 2019 and Computer Graphics, Visualization, Computer Vision and Image Processing 2019",,,,"381","385",,,"10.33965/g2019_201906c053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073095940&doi=10.33965%2fg2019_201906c053&partnerID=40&md5=775e6ef495b021383ba77f3f27518f5a","Department of Computer Science, National Chiao Tung University, 1001, Ta Hsieh Road, Hsinchu City, Taiwan","Wang, H., Department of Computer Science, National Chiao Tung University, 1001, Ta Hsieh Road, Hsinchu City, Taiwan; Chen, W.-W., Department of Computer Science, National Chiao Tung University, 1001, Ta Hsieh Road, Hsinchu City, Taiwan; Sun, C.-T., Department of Computer Science, National Chiao Tung University, 1001, Ta Hsieh Road, Hsinchu City, Taiwan","A large number of new video games are specially designed to meet the needs of players with different levels of gaming experience. To provide ideal learning environments, designers must understand differences in how experienced/less experienced players learn new games. Using a sample of players with different experience levels, our goal is to understand learning processes for a new real-time strategy game. Data from observations, post-game interviews, and eye movement recordings indicate that the majority of study participants relied on a trial-and-error approach, with more experienced gamers using a structured mental model involving feedback and expectations about making progress. Specifically, experienced gamers in the sample tended to use a top-down learning style emphasizing connections between goals and available actions, and to focus on the functions of game objects. In comparison, players with little or no gaming experience were more likely to focus on appearance and textual descriptions. Our findings suggest that in-game information interface designers need to consider player experience level in order to facilitate learning. Our results raise questions about whether gaming experience affects student classroom learning styles, as well as expectations regarding teachers and curriculums. © Copyright 2019 IADIS Press. All rights reserved.","Eye-Tracking; Game Interface; Learning Strategies; Mental Model","Cognitive systems; Computer aided instruction; Computer games; Computer vision; Eye movements; Eye tracking; Interactive computer graphics; Learning systems; Students; Visualization; Game interfaces; Information interfaces; Learning environments; Learning strategy; Mental model; Real-time strategy games; Textual description; Trial-and-error approach; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85073095940
"Shan H., Liu Y., Stefanov T.","57204467445;57200045231;6602079223;","Ensemble of Convolutional Neural Networks for P300 Speller in Brain Computer Interface",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11730 LNCS",,,"376","394",,2,"10.1007/978-3-030-30490-4_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072867415&doi=10.1007%2f978-3-030-30490-4_31&partnerID=40&md5=d4b7d1a79d999b47190164b9f8e25810","Leiden University, Leiden, Netherlands; KU Leuven, Leuven, Belgium","Shan, H., Leiden University, Leiden, Netherlands; Liu, Y., KU Leuven, Leuven, Belgium; Stefanov, T., Leiden University, Leiden, Netherlands","A Brain Computer Interface (BCI) speller allows human-beings to directly spell characters using eye-gazes, thereby building communication between the human brain and a computer. Convolutional Neural Networks (CNNs) have shown better ability than traditional machine learning methods to increase the character spelling accuracy for the BCI speller. Unfortunately, current CNNs can not learn well the features related to the target signal of the BCI speller. This issue limits these CNNs from further character spelling accuracy improvements. To address this issue, we propose a network, which combines our proposed two CNNs, with an existing CNN. These three CNNs of our network extract different features related to the target BCI signal. Our network uses the ensemble of the features extracted by these CNNs for BCI character spelling. Experimental results on three benchmark datasets show that our network outperforms other methods in most cases, with a significant spelling accuracy improvement up to 38.72%. In addition, the communication speed of the P300 speller based on our network is up to 2.56 times faster than the communication speed of the P300 speller based on other methods. © 2019, Springer Nature Switzerland AG.",,"Convolution; Machine learning; Neural networks; Accuracy Improvement; Benchmark datasets; Communication speed; Convolutional neural network; Human being; Human brain; Machine learning methods; Target signals; Brain computer interface",Conference Paper,"Final","",Scopus,2-s2.0-85072867415
"Fuhl W., Rosenstiel W., Kasneci E.","56770084800;7006528940;56059892600;","500,000 Images Closer to Eyelid and Pupil Segmentation",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11678 LNCS",,,"336","347",,10,"10.1007/978-3-030-29888-3_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072858556&doi=10.1007%2f978-3-030-29888-3_27&partnerID=40&md5=d806ec303877e7b34956b7ed3b749580","Eberhard Karls University Tübingen, Tübingen, 72076, Germany","Fuhl, W., Eberhard Karls University Tübingen, Tübingen, 72076, Germany; Rosenstiel, W., Eberhard Karls University Tübingen, Tübingen, 72076, Germany; Kasneci, E., Eberhard Karls University Tübingen, Tübingen, 72076, Germany","Human gaze behavior is not the only important aspect about eye tracking. The eyelids reveal additional important information; such as fatigue as well as the pupil size holds indications of the workload. The current state-of-the-art datasets focus on challenges in pupil center detection, whereas other aspects, such as the lid closure and pupil size, are neglected. Therefore, we propose a fully convolutional neural network for pupil and eyelid segmentation as well as eyelid landmark and pupil ellipsis regression. The network is jointly trained using the Log loss for segmentation and L1 loss for landmark and ellipsis regression. The application of the proposed network is the offline processing and creation of datasets. Which can be used to train resource-saving and real-time machine learning algorithms such as random forests. In addition, we will provide the worlds largest eye images dataset with more than 500,000 images DOWNLOAD. © 2019, Springer Nature Switzerland AG.","Eye tracking; Eyelid opening; Eyelid regression; Eyelid segmentation; Landmark detection; Landmark regression; Pupil ellipses regression; Pupil segmentation","Behavioral research; Decision trees; Image analysis; Image segmentation; Learning algorithms; Machine learning; Neural networks; Regression analysis; Eyelid opening; Eyelid regression; Eyelid segmentation; Landmark detection; Landmark regression; Pupil ellipses regression; Pupil segmentation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85072858556
"Martínez Zárate J., Mateus Santiago S.","57211167759;25522291400;","Sentiment Analysis Through Machine Learning for the Support on Decision-Making in Job Interviews",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11786 LNCS",,,"202","213",,1,"10.1007/978-3-030-30033-3_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072854620&doi=10.1007%2f978-3-030-30033-3_16&partnerID=40&md5=6ce7f801f9d146247720fec4d7cca7c6","Politécnico Colombiano Jaime Isaza Cadavid, Medellín, Colombia","Martínez Zárate, J., Politécnico Colombiano Jaime Isaza Cadavid, Medellín, Colombia; Mateus Santiago, S., Politécnico Colombiano Jaime Isaza Cadavid, Medellín, Colombia","In this paper, we propose a sentiment analysis model using machine learning for the support on decision-making in the process of job interviews. To do this, a characterization of the analysis of sentiments, job interviews and machine learning algorithms is first performed. Then, supervised machine learning with artificial neural networks is implemented in a prototype, due to the non-linear behavior described in the variables taken in the study and applying the Eye tracking technique. Finally, tests are carried out with people, in which, by asking questions of these, the involuntary movements of the pupil of the eye are analyzed, through the processing of a volume of data and the results of the ocular patterns are interpreted. Correlated with the questions of the test and with it, a final judgment is presented for the support of the decision making. © 2019, Springer Nature Switzerland AG.","Eye tracking; Job interview; Machine learning; Neural networks; Sentiment analysis","Behavioral research; Decision making; Eye movements; Eye tracking; Human computer interaction; Learning algorithms; Learning systems; Neural networks; Sentiment analysis; Supervised learning; Involuntary movements; Job interviews; Nonlinear behavior; Supervised machine learning; Machine learning",Conference Paper,"Final","",Scopus,2-s2.0-85072854620
"Ye X., König M.","57210795262;8883542500;","Applying eye tracking in virtual construction environments to improve cognitive data collection and human-computer interaction of site hazard identification",2019,"Proceedings of the 36th International Symposium on Automation and Robotics in Construction, ISARC 2019",,,,"1073","1080",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071467612&partnerID=40&md5=e60ede51eb7fda93d4c4b2ffb59cc495","Department of Civil and Environmental Engineering, Ruhr-University Bochum, Germany","Ye, X., Department of Civil and Environmental Engineering, Ruhr-University Bochum, Germany; König, M., Department of Civil and Environmental Engineering, Ruhr-University Bochum, Germany","In the Architecture, Engineering and Construction (AEC) field, eye tracking technology is being applied more frequently in cognitive research such as hazard identification. These studies typically use eye tracking in a diagnostic way and pay less attention to the application of virtual environment. However, in virtual environment, eye tracking not only can enhance the study of the cognitive process but also improves the human-computer interaction. Therefore, this paper elaborates on how we use eye tracking devices to track 3D objects in virtual environments diagnostically and interactively. First, we analyze the existing research gaps of using eye tracking in the construction industry. Then, we follow 3D object identification, diagnostic mode and interactive mode to develop a methodology by HTC VIVE device with Pupil Labs HTC Vive Binocular Add-on based on the research gaps. Finally, an example experiment is provided to demonstrate studying hazard identification using eye tracking in construction safety. For analyzing the eye movement data from the participants, we offer the number of confirmations, the scan path and the 3D heatmap of objects in both static and dynamic construction site scenes. This paper provides an approach of applying eye tracking to gather more data in virtual environment for the future cognitive studies and explores the possibility to improve human-computer interaction using eye tracking in the construction industry. © 2019 International Association for Automation and Robotics in Construction I.A.A.R.C. All rights reserved.","Eye tracking; Human-computer interaction; Site hazard identification; Virtual environment","Construction industry; Eye movements; Hazardous materials; Hazards; Human computer interaction; Robotics; Virtual reality; Architecture , engineering and constructions; Construction safety; Dynamic construction; Eye movement datum; Eye tracking devices; Eye tracking technologies; Hazard identification; Virtual construction; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85071467612
"Jiang J., Zhou X., Chan S., Chen S.","57211817476;55743240400;57188979700;24491760700;","Appearance-based gaze tracking: A brief review",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11745 LNAI",,,"629","640",,4,"10.1007/978-3-030-27529-7_53","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070721293&doi=10.1007%2f978-3-030-27529-7_53&partnerID=40&md5=c12c4495fefdfe42eeb9ff5515532bf9","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Electrical and Information Engineering, Quzhou University, Quzhou, China; School of Computer Communication and Engineering, Tianjin University of Technology, Tianjin, China","Jiang, J., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Zhou, X., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China, College of Electrical and Information Engineering, Quzhou University, Quzhou, China; Chan, S., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Chen, S., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China, School of Computer Communication and Engineering, Tianjin University of Technology, Tianjin, China","Human gaze tracking plays an important role in the field of Human-Computer Interaction. This paper presents a brief review on appearance-based gaze tracking. Based on the appearance of human eyes, input features can be classified into three categories according to the different ways of extracting human eyes features, namely, complete human eye image, pixel-based feature and 3D reconstruction image. The estimation process from human eye feature to fixation point mainly uses different mapping functions. In this paper, common mapping functions and related algorithms are described in detail: k-nearest neighbor (KNN), random forest (RF) regression, gaussian process (GP) regression, support vector machines (SVM) and artificial neural networks (ANN). This paper evaluates the performance of these gaze tracking algorithms using different mapping functions. Based on the results of the evaluation, potential challenges are summarized and the future directions of gaze estimation are prospected. © Springer Nature Switzerland AG 2019.","Appearance-based; Gaze tracking; HCI; Mapping","Decision trees; Human computer interaction; Mapping; Nearest neighbor search; Neural networks; Robotics; Support vector machines; 3-D reconstruction images; Appearance based; Estimation process; Gaussian process; Gaze tracking; K nearest neighbor (KNN); Mapping functions; Related algorithms; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85070721293
"Feidakis M., Rangoussi M., Kasnesis P., Patrikakis C., Kogias D.G., Charitopoulos A.","36998244700;6602664727;57044812500;8244299800;56896083200;36995957700;","Affective assessment in distance learning: A semi-explicit approach",2019,"International Journal of Technologies in Learning","26","1",,"19","34",,,"10.18848/2327-0144/CGP/V26I01/19-34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070311024&doi=10.18848%2f2327-0144%2fCGP%2fV26I01%2f19-34&partnerID=40&md5=8b7b589331a3df6dbaef581c3f5ecb9a","Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece","Feidakis, M., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece; Rangoussi, M., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece; Kasnesis, P., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece; Patrikakis, C., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece; Kogias, D.G., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece; Charitopoulos, A., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece","Modern e-learning and distance learning systems suffer severe lack of affect-aware interaction: the typical system is irresponsive to the affective state of the user, while even an inadequate human tutor would respond to it and even adapt his/her instruction accordingly. The main goal of this paper is to describe a scenario that deploys state-of-theart technologies to ""sense"" or ""gauge"" the affective state of a remote class of learners while they participate in a distance learning course, either synchronous or asynchronous, and provide feedback to all stakeholders (individual learner, peers, class tutor) through intuitive, easy-to-grasp visualisations. Both semi-automated, smart (selfreporting/ explicit) solutions through gestures and fully automated (user-transparent/implicit) solutions are sought through fusion of a number of ""experts"" (monitored features of the learner) that feed a decision-making algorithm after suitable processing. © Common Ground Research Networks, Michalis Feidakis, Maria Rangoussi, Panagiotis Kasnesis, Charalampos Patrikakis, Dimitrios G. Kogias, Angelos Charitopoulos.","Affective; Affective Computing; Analytics; Artificial Intelligence; Assessment; Deep Learning; Distance Learning; Emotion; Eye Gaze; Feedback; Gesture Analysis; Learning; State",,Article,"Final","",Scopus,2-s2.0-85070311024
"Alghamdi N., Alhalabi W.","57210260488;35316856000;","Fixation detection with ray-casting in immersive virtual reality",2019,"International Journal of Advanced Computer Science and Applications","10","7",,"60","65",,1,"10.14569/ijacsa.2019.0100710","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070105246&doi=10.14569%2fijacsa.2019.0100710&partnerID=40&md5=0b1c3a8272b85f2545aa7c627a3c1751","Computer Science Department, King Abdulaziz University, Jeddah, Saudi Arabia","Alghamdi, N., Computer Science Department, King Abdulaziz University, Jeddah, Saudi Arabia; Alhalabi, W., Computer Science Department, King Abdulaziz University, Jeddah, Saudi Arabia","This paper demonstrates the application of a proposed eye fixation detection algorithm to eye movement recorded during eye gaze input within immersive Virtual Reality and compares it with the standard frame-by-frame analysis for validation. Pearson correlations and a sample paired t-test indicated strong correlations between the two analysis methods in terms of fixation duration. The results showed that the principle of eye movement event detection in 2D can be applied successfully in a 3D environment and ensures efficient detection when combined with ray-casting and event time. © 2018 The Science and Information (SAI) Organization Limited.","Eye Movement; Eye Tracking; Fixation Detection; HMD; Virtual Reality",,Article,"Final","",Scopus,2-s2.0-85070105246
"Bamidele A.A., Kamardin K., Aziz N.S.N.A., Sam S.M., Ahmed I.S., Azizan A., Bani N.A., Kaidi H.M.","57210261072;35618313700;57210259952;24465516500;57210255803;15130531400;57189337505;57215690928;","Non-intrusive driver drowsiness detection based on face and eye tracking",2019,"International Journal of Advanced Computer Science and Applications","10","7",,"549","569",,4,"10.14569/ijacsa.2019.0100775","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070087301&doi=10.14569%2fijacsa.2019.0100775&partnerID=40&md5=4bab7955e28b51f7d52e4743a806b615","Razak School of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Malaysia-Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Wireless Communication Centre, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia","Bamidele, A.A., Razak School of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Kamardin, K., Malaysia-Japan International Institute of Technology, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia, Wireless Communication Centre, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Aziz, N.S.N.A., Razak School of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Sam, S.M., Razak School of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Ahmed, I.S., Razak School of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Azizan, A., Razak School of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia, Wireless Communication Centre, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Bani, N.A., Razak School of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia; Kaidi, H.M., Razak School of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia, Wireless Communication Centre, Universiti Teknologi Malaysia, Kuala Lumpur, 54100, Malaysia","The rate of annual road accidents attributed to drowsy driving are significantly high. Due to this, researchers have proposed several methods aimed at detecting drivers' drowsiness. These methods include subjective, physiological, behavioral, vehicle-based, and hybrid methods. However, recent reports on road safety are still indicating drowsy driving as a major cause of road accidents. This is plausible because the current driver drowsiness detection (DDD) solutions are either intrusive or expensive, thus hindering their ubiquitous nature. This research serves to bridge this gap by providing a test-bed for achieving a non-intrusive and low-cost DDD solution. A behavioral DDD solution is proposed based on tracking the face and eye state of the driver. The aim is to make this research an inception to DDD pervasiveness. To achieve this, National Tsing Hua University (NTHU) Computer Vision Lab's driver drowsiness detection video dataset was utilized. Several video and image processing operations were performed on the videos so as to detect the drivers' eye state. From the eye states, three important drowsiness features were extracted: percentage of eyelid closure (PERCLOS), blink frequency (BF), and Maximum Closure Duration (MCD) of the eyes. These features were then fed as inputs into several machine learning models for drowsiness classification. Models from the K-nearest Neighbors (KNN), Support Vector Machine (SVM), Logistic Regression, and Artificial Neural Networks (ANN) machine learning algorithms were experimented. These models were evaluated by calculating their accuracy, sensitivity, specificity, miss rate, and false alarm rate values. Although these five metrics were evaluated, the focus was more on getting optimal accuracies and miss rates. The result shows that the best models were a KNN model when k = 31 and an ANN model that used an Adadelta optimizer with 3 hidden layer network of 3, 27, and 9 neurons respective. The KNN model obtained an accuracy of 72.25% with a miss rate of 16.67%, while the ANN model obtained 71.61% and 14.44% accuracy and miss rate respectively. © 2018 The Science and Information (SAI) Organization Limited.","Artificial Neural Networks (ANN); Driver Drowsiness Detection (DDD); Eye tracking; Face tracking; K-nearest Neighbors (KNN); Logistic Regression; Support Vector Machine (SVM)",,Article,"Final","",Scopus,2-s2.0-85070087301
"Xu Q., Ragan E.D.","57194038669;26667185300;","Effects of Character Guide in Immersive Virtual Reality Stories",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11574 LNCS",,,"375","391",,2,"10.1007/978-3-030-21607-8_29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069700181&doi=10.1007%2f978-3-030-21607-8_29&partnerID=40&md5=3562af033f3881ad2acee4082afc9bbd","Department of Visualization, Texas A&M University, College Station, TX  77840, United States; Department of Computer and Information Science and Engineering, University of Florida, Gainesville, FL  32611, United States","Xu, Q., Department of Visualization, Texas A&M University, College Station, TX  77840, United States; Ragan, E.D., Department of Computer and Information Science and Engineering, University of Florida, Gainesville, FL  32611, United States","Bringing cinematic experiences from traditional film screens into Virtual Reality (VR) has become increasingly popular in recent years. However, striking a balance between storytelling and user interaction can cause a big challenge for filmmakers. In this paper, we present a media review on the common strategies that constructed the existing framework of computer generated cinematic VR by evaluating over 80 real-time rendered interactive experiences across different media. We summarized the most-used methods, which creators applied to maintain a relative control when presenting a narrative experience in VR, that were associated with story-progression strategies and attention guidance techniques. We then approach the problem of guiding the audience through major events of a story in VR by using a virtual character as a travel guide providing assistance in directing viewers attention to the target. To assess the effectiveness of this technique, we performed a controlled experiment applying the method in three VR videos. The experiment compared three variations of the character guide: (1) no guide, (2) a guide with a matching art style to the video, and (3) a guide with a non-matching art style. The experiment results provided insights for future directors and designers into how to draw viewers attention to a target point within a narrative VE, such as what could be improved and what should be avoided. © 2019, Springer Nature Switzerland AG.","3D interface; Character design; Gaze redirection; Interaction design; Interactive storytelling; Virtual Reality","Human computer interaction; Motion pictures; User interfaces; Virtual reality; 3D interface; Character designs; Gaze redirection; Interaction design; Interactive storytelling; Mixed reality",Conference Paper,"Final","",Scopus,2-s2.0-85069700181
"Hansberger J.T., Peng C., Blakely V., Meacham S., Cao L., Diliberti N.","6507505097;56921123800;57194689311;57194687255;57192903935;57210159309;","A Multimodal Interface for Virtual Information Environments",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11574 LNCS",,,"59","70",,5,"10.1007/978-3-030-21607-8_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069684664&doi=10.1007%2f978-3-030-21607-8_5&partnerID=40&md5=50f538de312dca8d631ef54ae86234ef","Army Research Laboratory, Huntsville, AL  35816, United States; University of Alabama in Huntsville, Huntsville, AL  35816, United States","Hansberger, J.T., Army Research Laboratory, Huntsville, AL  35816, United States; Peng, C., University of Alabama in Huntsville, Huntsville, AL  35816, United States; Blakely, V., University of Alabama in Huntsville, Huntsville, AL  35816, United States; Meacham, S., University of Alabama in Huntsville, Huntsville, AL  35816, United States; Cao, L., University of Alabama in Huntsville, Huntsville, AL  35816, United States; Diliberti, N., University of Alabama in Huntsville, Huntsville, AL  35816, United States","Continuing advances in multimodal technology, machine learning, and virtual reality are providing the means to explore and develop multimodal interfaces that are faster, more accurate, and more meaningful in the interactions they support. This paper describes an ongoing effort to develop an interface using input from voice, hand gestures, and eye gaze to interact with information in a virtual environment. A definition for a virtual environment tailored for the presentation and manipulation of information is introduced along with a new metaphor for multimodal interactions within a virtual environment. © 2019, Springer Nature Switzerland AG.","Gesture recognition; Multimodal interface; Virtual environment","Gesture recognition; Human computer interaction; Interactive computer graphics; Interactive computer systems; User interfaces; Virtual reality; Eye-gaze; Hand gesture; Multi-modal; Multi-Modal Interactions; Multi-modal interfaces; Virtual information; Mixed reality",Conference Paper,"Final","",Scopus,2-s2.0-85069684664
"Kato K., Prima O.D.A., Ito H.","57210157299;14060899300;56200735700;","3D Eye Tracking for Visual Imagery Measurements",2019,"Communications in Computer and Information Science","1033",,,"231","237",,,"10.1007/978-3-030-23528-4_32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069682208&doi=10.1007%2f978-3-030-23528-4_32&partnerID=40&md5=76fa6641e977739366adabfb08c5d02f","Graduate School of Software and Information Science, Iwate Prefectural University, Takizawa, Japan","Kato, K., Graduate School of Software and Information Science, Iwate Prefectural University, Takizawa, Japan; Prima, O.D.A., Graduate School of Software and Information Science, Iwate Prefectural University, Takizawa, Japan; Ito, H., Graduate School of Software and Information Science, Iwate Prefectural University, Takizawa, Japan","Experiences of visual imagery, the ability to see in the mind’s eye, occur in various situations. The Vividness of Visual Imagery Questionnaire (VVIQ) has been widely used to subjectively measure the vividness of visualizers based on its score. For objective measurements, studies show that functional Magnetic Resonance Imaging (fMRI) can be used to measure individual variabilities of the vividness of visual imagery. However, questions are remained on how the visualizers see the images spatially. This study proposes a method to measure the spatial distribution of gaze in 3-dimensional space of an object seen by a visualizer using a glass-typed 3D eye tracker. The eye tracker estimated gaze in 3D based on vergence eye movements. Thus, if the visualizer reports good visual imagery of a given image, the eye tracker will be able to estimate the location of the object in 3-dimensional space. The eye tracker is equipped with polarizing lenses to enable the visualizer to see the given stimuli in both virtual and real worlds. Here, a 3D television (3DTV) is used to present the stimuli virtually. Ten introductory students completed the VVIQ and divided into two groups: High and low vividness visualizers, based on total scores of the VVIQ. Experiment results show that 3D gaze fixations of subjects who reported good visual imagery were relatively distributed around the location of the given stimuli. © Springer Nature Switzerland AG 2019.","3D-gaze; Eye tracking; Visual imagery; VVIQ","Eye movements; Human computer interaction; Magnetic resonance imaging; 3-dimensional spaces; 3D-gaze; Functional magnetic resonance imaging; Individual variability; Objective measurement; Vergence eye movements; Visual imagery; VVIQ; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069682208
"Hirata Y., Soma H., Takimoto M., Kambayashi Y.","57210164277;57210164616;14632606500;57211703965;","Virtual Space Pointing Based on Vergence",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11567 LNCS",,,"259","269",,3,"10.1007/978-3-030-22643-5_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069675230&doi=10.1007%2f978-3-030-22643-5_21&partnerID=40&md5=40a3ae5b53897e54017e027037a37824","Department of Information Sciences, Tokyo University of Science, Chiba, Japan; Department of Computer and Information Engineering, Nippon Institute of Technology, Saitama, Japan","Hirata, Y., Department of Information Sciences, Tokyo University of Science, Chiba, Japan; Soma, H., Department of Information Sciences, Tokyo University of Science, Chiba, Japan; Takimoto, M., Department of Information Sciences, Tokyo University of Science, Chiba, Japan; Kambayashi, Y., Department of Computer and Information Engineering, Nippon Institute of Technology, Saitama, Japan","Recent virtual reality (VR) headsets make users perceive the three-dimensional (3D) virtual space through their parallax. The 3D space has been used for only passive use such as representing something put into intensive reality. We propose a new manner for pointing objects at specific locations in 3D space through vergence. In order to achieve this new manner, we have paid a close attention to the directions of eyes through parallax. Using the pointing manner, we cannot only drag icons or windows to any locations in 3D space, but also intuitively perform most desktop operations such as pilling up a pop-up menu and pushing a button. Because it is not easy for most people to control their vergence angle as they want, in our pointing manner, we present a feedback system of degree of vergence through an indicator appearing around a pointing cursor. In order to show the effectiveness of our proposal, we have implemented the pointing manner in a VR headset with an eye-tracker, and we have conducted numerical experiments. The experimental results show that our vergence based operations are feasible. © 2019, Springer Nature Switzerland AG.","Eye tracking; Indicator; Vergence; Virtual reality","Eye tracking; Geometrical optics; Indicators (instruments); Virtual reality; 3-D space; Eye trackers; Feedback systems; Numerical experiments; Specific location; Threedimensional (3-d); Vergences; Virtual spaces; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85069675230
"Tangnimitchok S., O-larnnithipong N., Ratchatanantakit N., Barreto A.","56964500200;56902559100;57203136289;7007109179;","Affective Monitor: A Process of Data Collection and Data Preprocessing for Building a Model to Classify the Affective State of a Computer User",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11567 LNCS",,,"179","190",,1,"10.1007/978-3-030-22643-5_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069673144&doi=10.1007%2f978-3-030-22643-5_14&partnerID=40&md5=cd4be864a5c68bfc747ab835cfcf8572","Florida International University, Miami, FL  33174, United States","Tangnimitchok, S., Florida International University, Miami, FL  33174, United States; O-larnnithipong, N., Florida International University, Miami, FL  33174, United States; Ratchatanantakit, N., Florida International University, Miami, FL  33174, United States; Barreto, A., Florida International University, Miami, FL  33174, United States","This paper outlines the first phase of our implementation of a system for non-intrusive estimation of a computer user’s affective state based on the Circumplex Model of Affect [1], from monitoring the user’s pupil diameter and facial expression [2]. The details of the original design plan for this system have been described previously [2]. The outline describes each part of data collecting process including: Obtaining 3D facial coordinates by Kinect, recording the pupil diameter signal, embedding the facial expression to Facial Animation Parameter indices, and the description of how the experiment will be setup. © 2019, Springer Nature Switzerland AG.","Affective computing; AffectiveMonitor; Eye-gaze tracking; Facial expression recognition","Data acquisition; Eye tracking; Face recognition; Affective Computing; AffectiveMonitor; Data preprocessing; Eye gaze tracking; Facial animation parameters; Facial expression recognition; Facial Expressions; Non-intrusive estimation; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85069673144
"Renker J., Kreutzfeldt M., Rinkenauer G.","56938677800;55881042000;6603586280;","Eye Blinks Describing the State of the Learner Under Uncertainty",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11597 LNCS",,,"444","454",,1,"10.1007/978-3-030-22341-0_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069672671&doi=10.1007%2f978-3-030-22341-0_35&partnerID=40&md5=d3c8ff6318cafb2011f65674578fb5eb","Leibniz Research Centre for Working Environments and Human Factors, Dortmund, Germany","Renker, J., Leibniz Research Centre for Working Environments and Human Factors, Dortmund, Germany; Kreutzfeldt, M., Leibniz Research Centre for Working Environments and Human Factors, Dortmund, Germany; Rinkenauer, G., Leibniz Research Centre for Working Environments and Human Factors, Dortmund, Germany","Adaptive systems are able to support the human-machine interaction in a great manner. However, the question arises which parameter are useful to gain insights into the user and can be easily implemented in the adaptive system. Eye blinks are frequent and most of the time automatic actions that reflect attentional and cognitive processes. They have not gained much attention in the context of adaptive systems until now. Thus, the current experiment investigated the number of blinks as an indicator of the state of the user while interacting with a technical system. Participants had to perform a dynamic visual spatial search task while their eye blinks were tracked. The task is to predict the appearance of target objects and thereby to learn a probability concept in order to improve the prediction. Results showed that eye blinks could distinguish between good and poor learner and increased parallel to the increasing task performance. Further, eye blinks reflected the information processing during the performance of a trial and the completion of the task. Thus, eye blinks might inform about the needs of the user with regard to the amount and detailedness of new information as well as additional help. However, the individual variability necessitates a separate baseline to be determined for each user. Further research is needed to foster the results also in more applied settings. © 2019, Springer Nature Switzerland AG.","Blink rate; Eyetracking; Mental representation; Probability learning; Uncertainty","Adaptive systems; Eye tracking; Blink rates; Cognitive process; Human machine interaction; Individual variability; Mental representations; Probability concepts; Probability learning; Uncertainty; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85069672671
"Min W., Park K., Wiggins J., Mott B., Wiebe E., Boyer K.E., Lester J.","55790560900;57209640340;55790031200;57203231751;7005357155;57203215959;57203179695;","Predicting dialogue breakdown in conversational pedagogical agents with multimodal LSTMs",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11626 LNAI",,,"195","200",,5,"10.1007/978-3-030-23207-8_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068335512&doi=10.1007%2f978-3-030-23207-8_37&partnerID=40&md5=813401ee166721e6c751d9b8fbcbecc4","Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States; Department of Computer and Information Science and Engineering, University of Florida, Gainsville, FL  32601, United States","Min, W., Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States; Park, K., Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States; Wiggins, J., Department of Computer and Information Science and Engineering, University of Florida, Gainsville, FL  32601, United States; Mott, B., Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States; Wiebe, E., Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States; Boyer, K.E., Department of Computer and Information Science and Engineering, University of Florida, Gainsville, FL  32601, United States; Lester, J., Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States","Recent years have seen a growing interest in conversational pedagogical agents. However, creating robust dialogue managers for conversational pedagogical agents poses significant challenges. Agents’ misunderstandings and inappropriate responses may cause breakdowns in conversational flow, lead to breaches of trust in agent-student relationships, and negatively impact student learning. Dialogue breakdown detection (DBD) is the task of predicting whether an agent’s utterance will cause a breakdown in an ongoing conversation. A robust DBD framework can support enhanced user experiences by choosing more appropriate responses, while also offering a method to conduct error analyses and improve dialogue managers. This paper presents a multimodal deep learning-based DBD framework to predict breakdowns in student-agent conversations. We investigate this framework with dialogues between middle school students and a conversational pedagogical agent in a game-based learning environment. Results from a study with 92 middle school students demonstrate that multimodal long short-term memory network (LSTM)-based dialogue breakdown detectors incorporating eye gaze features achieve high predictive accuracies and recall rates, suggesting that multimodal detectors can play an important role in designing conversational pedagogical agents that effectively engage students in dialogue. © Springer Nature Switzerland AG 2019.","Conversational pedagogical agent; Dialogue breakdown detection; Gaze; Multimodal; Natural language processing","Computer aided instruction; Deep learning; Forecasting; Long short-term memory; Managers; Natural language processing systems; Speech recognition; Breakdown detection; Gaze; Multi-modal; NAtural language processing; Pedagogical agents; Students",Conference Paper,"Final","",Scopus,2-s2.0-85068335512
"Yamagishi K., Takemura K.","57201291604;8575290600;","A hybrid method for remote eye tracking using RGB-IR Camera",2019,"VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","4",,,"591","596",,,"10.5220/0007582705910596","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068241443&doi=10.5220%2f0007582705910596&partnerID=40&md5=b9d82d3859a5887d16c523ecf8665113","Graduate School of Engineering, Tokai University, Hiratsuka, Japan","Yamagishi, K., Graduate School of Engineering, Tokai University, Hiratsuka, Japan; Takemura, K., Graduate School of Engineering, Tokai University, Hiratsuka, Japan","Methods for eye tracking using images can be divided largely into two categories: Methods using a near-infrared image and methods using a visible image. These images have been used independently in conventional eye-tracking methods; however, each category of methods have different advantageous features. Therefore, we propose using these images simultaneously to compensate for the weak points in each technique, and an RGB-IR camera, which can capture visible and near-infrared images simultaneously, is employed. Pupil detection can yield better results than iris detection because the eyelid often occludes the iris. On the other hand, the iris area can be used for model fitting because the iris size is constant. The model fitting can be automated at initialization; thus, the relationship between the 3D eyeball model and eye camera is solved. Additionally, the positions of the eye and gaze vectors are estimated continuously using these images for tracking. We conducted several experiments for evaluating the proposed method and confirmed its feasibility. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved","3D Eye Model; Eye-tracking; RGB-IR Camera","3D modeling; Cameras; Computer graphics; Computer vision; Infrared devices; Infrared imaging; 3D eye models; Eye tracking methods; IR camera; Iris detection; Model fitting; Near- infrared images; Pupil detection; Visible and near infrared; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85068241443
"Zdebskyi P., Vysotska V., Peleshchak R., Peleshchak I., Demchuk A., Krylyshyn M.","57209528737;24484045400;6602604864;57188576351;57204568083;57209531720;","An application development for recognizing of view in order to control the mouse pointer",2019,"CEUR Workshop Proceedings","2386",,,"55","74",,23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068053967&partnerID=40&md5=40e9d8c4412ac9c9c2daab72c2496f1f","Lviv Polytechnic National University, Lviv, Ukraine; Ivan Franko Drohobych State Pedagogical University, Drohobych, Ukraine","Zdebskyi, P., Lviv Polytechnic National University, Lviv, Ukraine; Vysotska, V., Lviv Polytechnic National University, Lviv, Ukraine; Peleshchak, R., Ivan Franko Drohobych State Pedagogical University, Drohobych, Ukraine; Peleshchak, I., Lviv Polytechnic National University, Lviv, Ukraine; Demchuk, A.; Krylyshyn, M., Lviv Polytechnic National University, Lviv, Ukraine","The purpose of this article is to develop an application for recognizing the user's point of view in order to control the mouse pointer. In the course of the task, an analysis of the subject area and technologies for implementation of the application was conducted. The algorithms of machine learning for solving the problem were considered. The input data of system are 50 coordinates placed on the face, which include the contours of the face, eyebrows, eyes, nose and coordinates of pupils. Finding the required coordinates occur without use of special devices, but only with webcam used for recognition. The application is implemented in the form of two modules, one of which is responsible for training system for the recognition of the view; the other one is responsible for controlling the mouse cursor with a view. The product can be operated on any operational system: the main requirement is the presence of an interpreter for the Python programming language, which can be downloaded for free from the official site. © 2019 CEUR-WS. All rights reserved.","Artificial neural network; Component called viewpointdata; Control the Mouse Pointer; Convolutional neural network; Data analyzation; Data stream mining; Demand knowledge; Eye recognition; Eye tracking; Facial feature; Functional requirement; Human eye; Image processing; Image recognition; Machine Learning; Machine learning algorithm; Mouse cursor; Mouse pointer; Neural Network; Open source; Python; Real time; Sequence action response; System component; Third party device; Trademark office; User perspective; User's Point; Web camera; Weight coefficient","Eye tracking; High level languages; Image processing; Image recognition; Learning systems; Machine learning; Mammals; Neural networks; Open source software; Open systems; Program interpreters; Component called viewpointdata; Convolutional neural network; Data analyzation; Data stream mining; Demand knowledge; Eye recognition; Facial feature; Functional requirement; Human eye; Mouse cursor; Mouse pointers; Open sources; Python; Real time; Sequence action response; System components; Third parties; User perspectives; User's Point; Web camera; Weight coefficients; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85068053967
"Jothi Prabha A., Bhargavi R.","57204137234;36661898100;","Prediction of Dyslexia from Eye Movements Using Machine Learning",2019,"IETE Journal of Research",,,,"","",,5,"10.1080/03772063.2019.1622461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067573687&doi=10.1080%2f03772063.2019.1622461&partnerID=40&md5=7b42f48c702d989da9da37595c9da03e","School of Computing Sciences and Engineering Department, Vellore Institute of Technology, Chennai, 600 127, India","Jothi Prabha, A., School of Computing Sciences and Engineering Department, Vellore Institute of Technology, Chennai, 600 127, India; Bhargavi, R., School of Computing Sciences and Engineering Department, Vellore Institute of Technology, Chennai, 600 127, India","Dyslexia is a reading disability and a language disorder where the individual exhibits difficulty in reading, writing, speaking, and trouble in spelling words. Early prediction of dyslexia can help dyslexics to get early support or intervention through remedial teaching. There is no remarkable computational model for the prediction of dyslexia in the literature. Existing methods to diagnose dyslexia include oral and written assessments, analysis and interpretation of Magnetic Resonance Imaging (MRI), functional MRI (fMRI), and Electroencephalogram (EEG). These methods require every instance to be interpreted by the domain expert in all stages whereas rigorously trained and tested computational models need subject expert intervention only at the end. In this paper, a prediction model has been proposed that uses statistical methods to differentiate dyslexics from non-dyslexics using their eye movement. The eye movements are tracked with an eye tracker. Eye movement has many features like fixations, saccades, transients, and distortions. From the raw data of eye tracker, high-level features are extracted using Principal Component Analysis. This paper proposes a Particle Swarm Optimization (PSO)-based Hybrid Kernel SVM-PSO for the prediction of dyslexia in individuals. The proposed model gives better predictive accuracy of 95% compared to a Linear SVM model. The proposed model is validated on 187 subjects by tracking their eye movements while reading. It is observed that eye movement data along with machine learning can be used for building models of high predictive accuracy. The proposed model can be used as a screening tool for the diagnosis of dyslexia in schools. © 2019, © 2019 IETE.","Classification; cross validation; dyslexia; eye tracking; feature extraction; Particle Swarm Optimization; Principal Component Analysis; Support Vector Machine","Classification (of information); Computation theory; Computational methods; Electroencephalography; Eye tracking; Feature extraction; Forecasting; Machine learning; Magnetic resonance imaging; Particle swarm optimization (PSO); Principal component analysis; Support vector machines; Computational model; Cross validation; dyslexia; Electro-encephalogram (EEG); Functional MRI (fMRI); High-level features; Language disorders; Predictive accuracy; Eye movements",Article,"Article in Press","",Scopus,2-s2.0-85067573687
"Findling R.D., Nguyen L.N., Sigg S.","55582066100;57225962998;17435716100;","Closed-Eye Gaze Gestures: Detection and Recognition of Closed-Eye Movements with Cameras in Smart Glasses",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11506 LNCS",,,"322","334",,3,"10.1007/978-3-030-20521-8_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067469791&doi=10.1007%2f978-3-030-20521-8_27&partnerID=40&md5=b91cd87ad364433cd6103e41e0bf88d9","Ambient Intelligence Group, Department of Communications and Networking, Aalto University, Maarintie 8, Espoo, 02150, Finland","Findling, R.D., Ambient Intelligence Group, Department of Communications and Networking, Aalto University, Maarintie 8, Espoo, 02150, Finland; Nguyen, L.N., Ambient Intelligence Group, Department of Communications and Networking, Aalto University, Maarintie 8, Espoo, 02150, Finland; Sigg, S., Ambient Intelligence Group, Department of Communications and Networking, Aalto University, Maarintie 8, Espoo, 02150, Finland","Gaze gestures bear potential for user input with mobile devices, especially smart glasses, due to being always available and hands-free. So far, gaze gesture recognition approaches have utilized open-eye movements only and disregarded closed-eye movements. This paper is a first investigation of the feasibility of detecting and recognizing closed-eye gaze gestures from close-up optical sources, e.g. eye-facing cameras embedded in smart glasses. We propose four different closed-eye gaze gesture protocols, which extend the alphabet of existing open-eye gaze gesture approaches. We further propose a methodology for detecting and extracting the corresponding closed-eye movements with full optical flow, time series processing, and machine learning. In the evaluation of the four protocols we find closed-eye gaze gestures to be detected 82.8%–91.6% of the time, and extracted gestures to be recognized correctly with an accuracy of 92.9%–99.2%. © 2019, Springer Nature Switzerland AG.","Closed eyes; Gaze gestures; Machine learning; Mobile computing; Recognition; Smart glasses; Time series analysis","Cameras; Gesture recognition; Glass; Learning systems; Machine learning; Mobile computing; Mobile telecommunication systems; Neural networks; Time series analysis; Closed eyes; Eye-gaze; Gaze gestures; Hands-free; Recognition; Smart glass; Time series processing; User input; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85067469791
"Abdessalem H.B., Chaouachi M., Boukadida M., Frasson C.","57202437771;57117835200;57202430900;7003506234;","Toward real-time system adaptation using excitement detection from eye tracking",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11528 LNCS",,,"214","223",,1,"10.1007/978-3-030-22244-4_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067237439&doi=10.1007%2f978-3-030-22244-4_26&partnerID=40&md5=7cebdf3492a340ebd302d47fecde1c5f","Department of Computer Science and Operations Research, University of Montreal, Montreal, H3C 3J7, Canada","Abdessalem, H.B., Department of Computer Science and Operations Research, University of Montreal, Montreal, H3C 3J7, Canada; Chaouachi, M., Department of Computer Science and Operations Research, University of Montreal, Montreal, H3C 3J7, Canada; Boukadida, M., Department of Computer Science and Operations Research, University of Montreal, Montreal, H3C 3J7, Canada; Frasson, C., Department of Computer Science and Operations Research, University of Montreal, Montreal, H3C 3J7, Canada","Users’ performance is known to be impacted by their emotional states. To better understand this relationship, different situations could be simulated during which the users’ emotional reactions are analyzed through sensors like eye tracking and EEG. In addition, virtual reality environments provide an immersive simulation context that induces high intensity emotions such as excitement. Extracting excitement from EEG provides more precise measures then other methods, however it is not always possible to use EEG headset in virtual reality environment. In this paper we present an alternative approach to the use of EEG for excitement detection using only eye movements. Results showed that there is a correlation between eye movements and excitement index extracted from EEG. Five machine learning algorithms were used in order to predict excitement trend exclusively from eye tracking. Results revealed that we can detect the offline excitements trend directly from eye movements with a precision of 92% using deep neural network. © Springer Nature Switzerland AG 2019.","Artificial intelligence; EEG; Emotional intelligence; Excitement; Eye tracking; Real-time adaptation; Virtual reality","Artificial intelligence; Computer aided instruction; Deep neural networks; Electroencephalography; Emotional intelligence; Eye movements; Intelligent vehicle highway systems; Interactive computer systems; Learning algorithms; Machine learning; Real time systems; Virtual reality; Emotional reactions; Emotional state; Excitement; High intensity; Immersive; Offline; Real-time adaptation; Virtual-reality environment; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85067237439
"Chen Z., Shi B.E.","56808413900;7402547071;","Appearance-Based Gaze Estimation Using Dilated-Convolutions",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11366 LNCS",,,"309","324",,6,"10.1007/978-3-030-20876-9_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066949725&doi=10.1007%2f978-3-030-20876-9_20&partnerID=40&md5=0f7c751e25d068f3df4b284b1949a027","The Hong Kong University of Science and Technology, Kowloon, Hong Kong","Chen, Z., The Hong Kong University of Science and Technology, Kowloon, Hong Kong; Shi, B.E., The Hong Kong University of Science and Technology, Kowloon, Hong Kong","Appearance-based gaze estimation has attracted more and more attention because of its wide range of applications. The use of deep convolutional neural networks has improved the accuracy significantly. In order to improve the estimation accuracy further, we focus on extracting better features from eye images. Relatively large changes in gaze angles may result in relatively small changes in eye appearance. We argue that current architectures for gaze estimation may not be able to capture such small changes, as they apply multiple pooling layers or other downsampling layers so that the spatial resolution of the high-level layers is reduced significantly. To evaluate whether the use of features extracted at high resolution can benefit gaze estimation, we adopt dilated-convolutions to extract high-level features without reducing spatial resolution. In cross-subject experiments on the Columbia Gaze dataset for eye contact detection and the MPIIGaze dataset for 3D gaze vector regression, the resulting Dilated-Nets achieve significant (upÂ to 20.8%) gains when compared to similar networks without dilated-convolutions. Our proposed Dilated-Net achieves state-of-the-art results on both the Columbia Gaze and the MPIIGaze datasets. © 2019, Springer Nature Switzerland AG.","Appearance-based gaze estimation; Dilated-convolutions","Computer vision; Deep neural networks; Image enhancement; Image resolution; Neural networks; Appearance based; Convolutional neural network; Gaze estimation; High resolution; High-level features; Spatial resolution; State of the art; Subject experiment; Convolution",Conference Paper,"Final","",Scopus,2-s2.0-85066949725
"Tsujino T., Nakamura H., Fujishima T., Hamagishi G., Yoshimoto K., Takahashi H., Matsumoto T., Kusafuka K.","57208882469;57208888581;57208885909;6603319161;56492328000;7405468853;57203999911;57203998567;","3D display with active parallax barrier using the monochromatic LC panel of specifications same as the image display panel",2019,"Proceedings of SPIE - The International Society for Optical Engineering","10942",,"1094211","","",,1,"10.1117/12.2508357","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066055219&doi=10.1117%2f12.2508357&partnerID=40&md5=4142da0b3aac8345f675f6eadf8f4d21","Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Kyocera Corp., 402-1 Nakayama-cho, Midori-ku, Yokohama, 226-8512, Japan","Tsujino, T., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Nakamura, H., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Fujishima, T., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Hamagishi, G., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Yoshimoto, K., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Takahashi, H., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Matsumoto, T., Kyocera Corp., 402-1 Nakayama-cho, Midori-ku, Yokohama, 226-8512, Japan; Kusafuka, K., Kyocera Corp., 402-1 Nakayama-cho, Midori-ku, Yokohama, 226-8512, Japan","In the stereoscopic 3D display using the parallax barrier, the active barrier method can expand the viewing area if the barrier pattern optimizes corresponding to the position of the observer by eye tracking. However, requiring not only the LC display panel but also the specially shaped LC panel for the active barrier, this system should be very expensive. We propose using the active barrier which is a monochromatic panel of the same pixel shape as the image display panel. In the proposed method, it is easy to manufacture panels for the active barrier, and the 3D display provide the wide viewing area and high quality 3D images for observers. When the active barrier is a monochromatic panel having the same pixel shape as the image display panel, basically the barrier pitch cannot realize the ideal value. Thus, the observer cannot observe the stereoscopic image in the full screen. In order to realize stereoscopic observation, we apply the cycle pitch composing the stereoscopic image. The cycle pitch composing the stereoscopic image is the method to bring the pitch constituting the L/R image closer to the ideal value by periodically increasing the number of dots constituting the L/R image. To confirm the effectiveness of the proposed method, the crosstalk of the prototypes using film barriers were measured. Crosstalk was less than or equal to 10% at the viewing distance of 421 mm to 1238 mm. That crosstalk can be reduced regardless of observation distance was confirmed. © 2019 SPIE.","Active barrier; Crosstalk; Cycle pitch; Eye tracking; Glasses-free; Parallax barrier; Stereoscopic display","Crosstalk; Eye tracking; Geometrical optics; Pixels; Stereo image processing; Active barrier; Barrier patterns; Cycle pitch; Parallax barriers; Stereoscopic 3-D display; Stereoscopic display; Stereoscopic image; Viewing distance; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85066055219
"Zabels R., Osmanis K., Narels M., Smukulis R., Osmanis I.","28268175300;55578677300;56913268700;57203585967;55933860100;","Integrated head-mounted display system based on a multi-planar architecture",2019,"Proceedings of SPIE - The International Society for Optical Engineering","10942",,"1094208","","",,4,"10.1117/12.2509954","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066049992&doi=10.1117%2f12.2509954&partnerID=40&md5=781a1c62884a6cd32144c9f360072bec","LightSpace Technologies, 1 Ziedleju, Marupe, LV-2167, Latvia","Zabels, R., LightSpace Technologies, 1 Ziedleju, Marupe, LV-2167, Latvia; Osmanis, K., LightSpace Technologies, 1 Ziedleju, Marupe, LV-2167, Latvia; Narels, M., LightSpace Technologies, 1 Ziedleju, Marupe, LV-2167, Latvia; Smukulis, R., LightSpace Technologies, 1 Ziedleju, Marupe, LV-2167, Latvia; Osmanis, I., LightSpace Technologies, 1 Ziedleju, Marupe, LV-2167, Latvia","LightSpace Technologies have developed a prototype of integrated head-mounted stereoscopic display system based on a proprietary multi-plane optical diffuser technology. The system is entirely solid-state and has six focal planes which covers ∼3 diopters (from 32 cm to 8 m). For the operation no eye-tracking is utilized. The new display system virtually entirely eliminates vergence-accommodation conflict and adds a monocular accommodation as an important depth cue for improved 3D realism. In regards to content rendering the processing load in contrast to conventional single-focalplane stereoscopic displays with similar image resolution is only slightly increased. The differences in terms of comparative performance are the worst in the case of simple 3D scenes, while for high-complexity scenes this difference has a tendency to slightly decrease. On average the processing burden for multi-plane stereoscopic displays is no more than 1.5% higher than for conventional stereoscopic displays. Furthermore, increasing a number of physical focal planes doesn't notably worsen the image rendering performance allowing the display device to be efficiently driven by already readily available hardware-including high-performance mobile platforms. Overall, the user feedback about the developed multi-plane stereoscopic 3D display prototype confirms prior proposed assumptions of multi-plane architecture yielding higher acceptance rate due to improved 3D realism and eradicated vergence-accommodation conflict, thus currently being one of the most noteworthy advancements in the field of 3D stereoscopic displays. © 2019 SPIE.","3D; Diffuser; Head-mounted display; Liquid crystal; Multi-plane; Stereo; Virtual reality; Volumetric","Diffusers (fluid); Diffusers (optical); Eye tracking; Focusing; Helmet mounted displays; Image resolution; Liquid crystals; Rendering (computer graphics); Stereo image processing; Three dimensional computer graphics; Virtual reality; Comparative performance; Head mounted display systems; Head mounted displays; Multi planes; Stereo; Stereoscopic 3-D display; Stereoscopic display; Volumetric; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85066049992
"Fujishima T., Nakamura H., Tsujino T., Hamagishi G., Yoshimoto K., Takahashi H., Matsumoto T., Kusafuka K.","57208885909;57208888581;57208882469;6603319161;56492328000;7405468853;57203999911;57203998567;","A novel control method of the combination of simple active barrier pitch control and image processing to extremely expand the viewing zone in forward and backward directions of stereoscopic 3D displays",2019,"Proceedings of SPIE - The International Society for Optical Engineering","10942",,"109420M","","",,1,"10.1117/12.2509038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066029308&doi=10.1117%2f12.2509038&partnerID=40&md5=a44dbd060e0093b8cff149b8def10c5a","Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Kyocera Corporation, 402-1 Nakayamacho, Midori-ku, Yokohama, 226-8512, Japan","Fujishima, T., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Nakamura, H., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Tsujino, T., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Hamagishi, G., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Yoshimoto, K., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Takahashi, H., Dept. of Electrical and Information Engineering, Graduate School of Engineering, Osaka City Univ., 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Matsumoto, T., Kyocera Corporation, 402-1 Nakayamacho, Midori-ku, Yokohama, 226-8512, Japan; Kusafuka, K., Kyocera Corporation, 402-1 Nakayamacho, Midori-ku, Yokohama, 226-8512, Japan","We have previously proposed eye tracking system to expand viewing area in all directions for glasses-free 3D display. In this system, since the parallax barrier was fixed, the viewing zone was expanded by image processing corresponding to the viewing position. Thus, there was a limit to expanding the viewing zone only by image processing. On the other hand, we can expand the viewing zone by applying an active barrier that changes to the optimum barrier pattern corresponding to the viewing position by eye tracking. However, to change the active barrier pattern, complex calculation and a specially designed active barrier LC panel are required. To overcome this problem, we propose a novel control method to expand the viewing zone of stereoscopic 3D displays with active parallax barrier in depth direction. The proposed method is the combination of the simple generation method of a barrier pattern and the synthesis method of Left/Right synthetic image corresponding to the viewing position. In this method, let the optimum barrier pitch be Bp at the viewing distance d, we set the barrier pitch to x∗Bp and synthesize the Left/Right image so that crosstalk is low at the same time at the viewing distance d/x. To verify the effectiveness of the proposed method, we measured the crosstalk of the prototype 3D display. The crosstalk ratios at the optimum viewing distance 1092 mm (d), 774 mm (d∗3/4) and 546 mm (d∗1/2) were 4.28%, 3.82% and 5.02%, respectively. Therefore, low crosstalk 3D images could be observed. © 2019 SPIE.","Active barrier; Crosstalk; Eye tracking; Glass-less; Parallax barrier; Stereoscopic display","Crosstalk; Eye tracking; Geometrical optics; Glass; Stereo image processing; Active barrier; Eye tracking systems; Forward-and-backward; Generation method; Glasses-free 3-D displays; Parallax barriers; Stereoscopic 3-D display; Stereoscopic display; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85066029308
"Cadena S.A., Denfield G.H., Walker E.Y., Gatys L.A., Tolias A.S., Bethge M., Ecker A.S.","57204284701;36175486400;56572021600;56719697600;57207588001;57210225326;23099898900;","Deep convolutional models improve predictions of macaque V1 responses to natural images",2019,"PLoS Computational Biology","15","4","e1006897","","",,46,"10.1371/journal.pcbi.1006897","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065598006&doi=10.1371%2fjournal.pcbi.1006897&partnerID=40&md5=813e67e56fb146e251ffc32c23e8e1f8","Centre for Integrative Neuroscience and Institute for Theoretical Physics, University of Tübingen, Tübingen, Germany; Bernstein Center for Computational Neuroscience, Tübingen, Germany; Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, United States; Department of Neuroscience, Baylor College of Medicine, Houston, Houston, TX, United States; Department of Electrical and Computer Engineering, Rice University, Houston, Houston, TX, United States; Max Planck Institute for Biological Cybernetics, Tübingen, Germany","Cadena, S.A., Centre for Integrative Neuroscience and Institute for Theoretical Physics, University of Tübingen, Tübingen, Germany, Bernstein Center for Computational Neuroscience, Tübingen, Germany, Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, United States; Denfield, G.H., Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, United States, Department of Neuroscience, Baylor College of Medicine, Houston, Houston, TX, United States; Walker, E.Y., Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, United States, Department of Neuroscience, Baylor College of Medicine, Houston, Houston, TX, United States; Gatys, L.A., Centre for Integrative Neuroscience and Institute for Theoretical Physics, University of Tübingen, Tübingen, Germany, Bernstein Center for Computational Neuroscience, Tübingen, Germany; Tolias, A.S., Bernstein Center for Computational Neuroscience, Tübingen, Germany, Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, United States, Department of Neuroscience, Baylor College of Medicine, Houston, Houston, TX, United States, Department of Electrical and Computer Engineering, Rice University, Houston, Houston, TX, United States; Bethge, M., Centre for Integrative Neuroscience and Institute for Theoretical Physics, University of Tübingen, Tübingen, Germany, Bernstein Center for Computational Neuroscience, Tübingen, Germany, Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, United States, Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Ecker, A.S., Centre for Integrative Neuroscience and Institute for Theoretical Physics, University of Tübingen, Tübingen, Germany, Bernstein Center for Computational Neuroscience, Tübingen, Germany, Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, United States","Despite great efforts over several decades, our best models of primary visual cortex (V1) still predict spiking activity quite poorly when probed with natural stimuli, highlighting our limited understanding of the nonlinear computations in V1. Recently, two approaches based on deep learning have emerged for modeling these nonlinear computations: transfer learning from artificial neural networks trained on object recognition and data-driven convolutional neural network models trained end-to-end on large populations of neurons. Here, we test the ability of both approaches to predict spiking activity in response to natural images in V1 of awake monkeys. We found that the transfer learning approach performed similarly well to the data-driven approach and both outperformed classical linear-nonlinear and waveletbased feature representations that build on existing theories of V1. Notably, transfer learning using a pre-trained feature space required substantially less experimental time to achieve the same performance. In conclusion, multi-layer convolutional neural networks (CNNs) set the new state of the art for predicting neural responses to natural images in primate V1 and deep features learned for object recognition are better explanations for V1 computation than all previous filter bank theories. This finding strengthens the necessity of V1 models that are multiple nonlinearities away from the image domain and it supports the idea of explaining early visual cortex based on high-level functional goals. © 2019 Cadena et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"animal experiment; Article; controlled study; convolutional neural network; eye tracking; male; nonhuman; prediction; receptive field; recognition; rhesus monkey; striate cortex; transfer of learning; visual stimulation; visual system; algorithm; animal; artificial neural network; biological model; biology; nerve cell; physiology; vision; visual cortex; Algorithms; Animals; Computational Biology; Macaca mulatta; Male; Models, Neurological; Neural Networks (Computer); Neurons; Visual Cortex; Visual Perception",Article,"Final","",Scopus,2-s2.0-85065598006
"Yoo S., Jeong D.K., Jang Y.","57192084306;57205652159;36152811100;","The Study of a Classification Technique for Numeric Gaze-Writing Entry in Hands-Free Interface",2019,"IEEE Access","7",,"6287639","49125","49134",,,"10.1109/ACCESS.2019.2909573","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065069026&doi=10.1109%2fACCESS.2019.2909573&partnerID=40&md5=10a3848e6e7ad0290dd9a08ea2e7ea3f","Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea","Yoo, S., Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea; Jeong, D.K., Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea; Jang, Y., Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea","Recently, many applications are developed in numerous domains with various environments. Since some environments require hands-free applications, new technology is needed for the input interfaces other than the mouse and keyboard. Therefore, to meet the needs, many researchers have begun to investigate the gaze and voice for the input technology. In particular, there are many approaches to render virtual keyboards with the gaze. However, since the virtual keyboards hide the screen space, this technique can only be applied in limited environments. In this paper, we propose a classification technique for gaze-written numbers as the hands-free interface. Since the gaze-writing is less accurate compared to the virtual keyboard typing, we apply the convolutional neural network (CNN) deep learning algorithm to recognize the gaze-writing and improve the classification accuracy. Besides, we create new gaze-writing datasets for training, gaze MNIST (gMNIST), by modifying the MNIST data with features of the gaze movement patterns. For the evaluation, we compare our approach with the basic CNN structures using the original MNIST dataset. Our study will allow us to have more options for the input interfaces and expand our choices in hands-free environments. © 2013 IEEE.","Eye tracking; Gaze-writing; Input technique; Machine learning; MNIST","Computer keyboards; Deep learning; Learning algorithms; Learning systems; Mammals; Neural networks; Classification accuracy; Classification technique; Convolutional neural network; Gaze movements; Input interface; Input techniques; MNIST; Virtual Keyboards; Eye tracking",Article,"Final","",Scopus,2-s2.0-85065069026
"Sledzianowski A., Szymanski A., Drabik A., Szlufik S., Koziorowski D.M., Przybyszewski A.W.","57201155975;56304189900;6506466125;55334567200;7801382272;6603763540;","Measurements of Antisaccades Parameters Can Improve the Prediction of Parkinson’s Disease Progression",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11432 LNAI",,,"602","614",,1,"10.1007/978-3-030-14802-7_52","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064557072&doi=10.1007%2f978-3-030-14802-7_52&partnerID=40&md5=da36e8c4b9e0022ffc5dfa37a228eaf8","Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Neurology, Faculty of Health Science, Medical University of Warsaw, Warsaw, 03-242, Poland","Sledzianowski, A., Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Szymanski, A., Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Drabik, A., Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Szlufik, S., Neurology, Faculty of Health Science, Medical University of Warsaw, Warsaw, 03-242, Poland; Koziorowski, D.M., Neurology, Faculty of Health Science, Medical University of Warsaw, Warsaw, 03-242, Poland; Przybyszewski, A.W., Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland","In this text we present the results of oculometric experiment consisting the registration of anitsaccades of patients with Parkinson’s Disease (PD) in relation to their neurological data. PD is an important and incurable neurodegenerative disease and we are looking for methods optimizing the treatment. In our previous works we used Reflexive Saccades (RS) and Pursuit Ocular Movements (POM) to check what it can tell us about the disease’s progression expressed in the Unified Parkinson’s Disease Rating Scale (UPDRS). The UPDRS is the most commonly used scale in the clinical studies of Parkinson’s disease. In this experiment we examined antisaccades (AS) of 11 PD patients who performed eye movement tests in controlled conditions. We correlated neurological measurements of patient’s motoric abilities and data describing their treatment with values of AS parameters. We used RSES and for prediction of the UPDRS scoring groups and Weka methods for presentation of the results. We achieved good results with accuracy of 91% and coverage of 100%. The AS test is a relatively easy and non-invasive method that can be used in the telemedicine in the future. © 2019, Springer Nature Switzerland AG.","Antisaccades; Data mining; Eye tracking; Machine learning; Parkinson’s disease","Data mining; Database systems; Eye movements; Eye tracking; Learning systems; Neurology; Noninvasive medical procedures; Parameter estimation; Antisaccades; Clinical study; Controlled conditions; Disease progression; Disease ratings; Measurements of; Noninvasive methods; Ocular movements; Neurodegenerative diseases",Conference Paper,"Final","",Scopus,2-s2.0-85064557072
"Mutlu-Bayraktar D.","55443406000;","Change blindness in multimedia learning environment",2019,"Journal of Educational Multimedia and Hypermedia","28","1",,"75","97",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064400642&partnerID=40&md5=763d09aa067cfcfaee28e12f8fe47089","Istanbul University, Cerrahpasa, Turkey","Mutlu-Bayraktar, D., Istanbul University, Cerrahpasa, Turkey","This study investigates the change blindness that may occur in multimedia learning environments. For this purpose, a multimedia animation which had some changes was designed. The eye movements were examined during the process of detecting the changes in multimedia via eye tracking technics. The research model was defined as a controlled experiment method. Fifteen ungraduated students participated in the experiment. Attention levels of participants were determined by d2 Attention Test. Change detection numbers of participants were analyzed according to their attention level and their gender. The appearance of a major object on the scene was the most detected change and the change on the detail object was detected less. According to findings about the attention level and change detection, the participants at high attention level were more successful at detecting change in multimedia. Females were more successful in detecting change than males. © 2019 Association for the Advancement of Computing in Education. All Rights Reserved.","Attention; Change blindness; Change detection; Multimedia learning environment",,Article,"Final","",Scopus,2-s2.0-85064400642
"Li P., Hou X., Duan X., Yip H., Song G., Liu Y.","56375954100;57207460293;55646016600;54793800500;55218895500;57203136059;","Appearance-Based Gaze Estimator for Natural Interaction Control of Surgical Robots",2019,"IEEE Access","7",,"8648437","25095","25110",,6,"10.1109/ACCESS.2019.2900424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062733925&doi=10.1109%2fACCESS.2019.2900424&partnerID=40&md5=8a8a3370e0ff0ec00fa2ecefea1e8cb3","School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China","Li, P., School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China; Hou, X., School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China; Duan, X., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; Yip, H., Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong; Song, G., State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China; Liu, Y., Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong","Robots are playing an increasingly important role in modern surgery. However, conventional human-computer interaction methods, such as joystick control and sound control, have some shortcomings, and medical personnel are required to specifically practice operating the robot. We propose a human-computer interaction model based on eye movement with which medical staff can conveniently use their eye movements to control the robot. Our algorithm requires only an RGB camera to perform tasks without requiring expensive eye-tracking devices. Two kinds of eye control modes are designed in this paper. The first type is the pick and place movement, with which the user uses eye gaze to specify the point where the robotic arm is required to move. The second type is user command movement, with which the user can use eye gaze to select the direction in which the user desires the robot to move. The experimental results demonstrate the feasibility and convenience of these two modes of movement. © 2013 IEEE.","convolutional neural network; Deep learning; gaze estimation; surgical robot","Deep learning; Eye movements; Eye tracking; Human computer interaction; Human robot interaction; Neural networks; Robotic surgery; Surgery; Appearance based; Convolutional neural network; Eye tracking devices; Gaze estimation; Joystick control; Medical personnel; Natural interactions; Pick and place; Surgical equipment",Article,"Final","",Scopus,2-s2.0-85062733925
"Yang B., Zhang X., Li Z., Du S., Wang F.","57205176402;16032700200;57207687484;15073648200;57221087918;","An Accurate and Robust Gaze Estimation Method Based on Maximum Correntropy Criterion",2019,"IEEE Access","7",,"8629993","23291","23302",,1,"10.1109/ACCESS.2019.2896303","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062704122&doi=10.1109%2fACCESS.2019.2896303&partnerID=40&md5=f43f1e0d33e8ebb22448ee327b83ca01","National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China; School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China","Yang, B., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China; Zhang, X., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China; Li, Z., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China; Du, S., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China; Wang, F., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China","Accurately estimating the user's gaze is important in many applications, such as human-computer interaction. Due to great convenience, appearance-based methods for gaze estimation have been a popular subject of research for many years. However, the greatest challenges in the appearance-based gaze estimation in a desktop environment are how to simplify the calibration process and deal with other issues such as image noise and low resolution. To address the problems, we adopt a mapping relationship between the high-dimensional eye image features space and the low-dimensional gaze positions and propose a robust and accurate method for gaze estimation with a webcam. First, we utilize Kullback-Leibler divergence to reduce feature dimension and keep similarity between the feature space and the gaze space. Then, we construct the objective function using the maximum correntropy criterion instead of mean squared error, which can enhance the anti-noise ability, especially for outliers or pixel corruption. A regularization term is adopted to adaptively select the sparse training samples for gaze estimation. We conducted extensive experiments in a desktop environment, which verified that the proposed method was robust and efficient in dealing with sparse training samples, pixel corruption, and low-resolution problems in gaze estimation. © 2013 IEEE.","Appearance-based method; gaze estimation; human computer interaction; maximum correntropy criterion","Crime; Mean square error; Pixels; Sampling; Appearance-based methods; Calibration process; Correntropy; Gaze estimation; Kullback Leibler divergence; Mapping relationships; Objective functions; Regularization terms; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85062704122
"Li J., Shan Y., Li S., Chen T.","57203736355;57201580351;16202805500;57201241944;","Gaze estimation using a head-mounted single full-view camera",2019,"Journal of Electronic Imaging","28","1","013002","","",,1,"10.1117/1.JEI.28.1.013002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062644140&doi=10.1117%2f1.JEI.28.1.013002&partnerID=40&md5=391ced3d88ac894f57ec02a521a7121e","Southwest University, School of Electronic and Information Engineering, Chongqing Key Laboratory of Nonlinear Circuit and Intelligent Information Processing, Chongqing, China; Hiroshima City University, Graduate School of Information Sciences, Hiroshima, Japan","Li, J., Southwest University, School of Electronic and Information Engineering, Chongqing Key Laboratory of Nonlinear Circuit and Intelligent Information Processing, Chongqing, China; Shan, Y., Hiroshima City University, Graduate School of Information Sciences, Hiroshima, Japan; Li, S., Southwest University, School of Electronic and Information Engineering, Chongqing Key Laboratory of Nonlinear Circuit and Intelligent Information Processing, Chongqing, China, Hiroshima City University, Graduate School of Information Sciences, Hiroshima, Japan; Chen, T., Southwest University, School of Electronic and Information Engineering, Chongqing Key Laboratory of Nonlinear Circuit and Intelligent Information Processing, Chongqing, China","We present a gaze estimation method for a head-mounted full-view egocentric camera that can capture egocentric video together with users' gaze cues. While the conventional gaze recording device has two cameras, an eye camera and a scene camera, the proposed gaze recording device has only a single spherical camera with a full field of view. To determine the point of gaze on full-view images, we present an eye-model-based gaze estimation method by means of three-dimensional iris projection. First, an eye model is built according to the precalibration process of an eyeball center and the eyeball biological parameters; then, we express the 3D iris contour in the eye model and project it back to the spherical camera model. since 2D iris contours can be detected directly on images and can also be expressed under the spherical model, the relationship between the 3D iris contour and the 2D iris contour can be found. By solving this problem, the gaze direction under the camera model is determined; subsequently, the point of gaze on the images can be inferred. since in the proposed method, a single spherical camera can play the role of the conventional two cameras, not only do the proposed method results have a simpler system structure but also the cumbersome operation of the calibration for the conventional two-camera gaze measurement devices becomes unnecessary. The effectiveness of the proposed method is shown by the experimental results. © 2019 SPIE and IS and T.","eye-model-based gaze estimation; head-mounted camera; point of gaze; spherical camera model","3D modeling; Spheres; Biological parameter; Camera model; Gaze estimation; Head mounted Camera; Measurement device; Point of gaze; Recording devices; System structures; Cameras",Article,"Final","",Scopus,2-s2.0-85062644140
"Yu Y., Liu G., Odobez J.-M.","57188644020;56420692700;57203103085;","Deep multitask gaze estimation with a constrained Landmark-Gaze model",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11130 LNCS",,,"456","474",,8,"10.1007/978-3-030-11012-3_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061782488&doi=10.1007%2f978-3-030-11012-3_35&partnerID=40&md5=a9f7442a575a13638c64cef81bc25f70","Idiap Research Institute, Martigny, Switzerland; EPFL, Lausanne, Switzerland","Yu, Y., Idiap Research Institute, Martigny, Switzerland, EPFL, Lausanne, Switzerland; Liu, G., Idiap Research Institute, Martigny, Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, Switzerland, EPFL, Lausanne, Switzerland","As an indicator of attention, gaze is an important cue for human behavior and social interaction analysis. Recent deep learning methods for gaze estimation rely on plain regression of the gaze from images without accounting for potential mismatches in eye image cropping and normalization. This may impact the estimation of the implicit relation between visual cues and the gaze direction when dealing with low resolution images or when training with a limited amount of data. In this paper, we propose a deep multitask framework for gaze estimation, with the following contributions. (i) we proposed a multitask framework which relies on both synthetic data and real data for end-to-end training. During training, each dataset provides the label of only one task but the two tasks are combined in a constrained way. (ii) we introduce a Constrained Landmark-Gaze Model (CLGM) modeling the joint variation of eye landmark locations (including the iris center) and gaze directions. By relating explicitly visual information (landmarks) to the more abstract gaze values, we demonstrate that the estimator is more accurate and easier to learn. (iii) by decomposing our deep network into a network inferring jointly the parameters of the CLGM model and the scale and translation parameters of eye regions on one hand, and a CLGM based decoder deterministically inferring landmark positions and gaze from these parameters and head pose on the other hand, our framework decouples gaze estimation from irrelevant geometric variations in the eye image (scale, translation), resulting in a more robust model. Thorough experiments on public datasets demonstrate that our method achieves competitive results, improving over state-of-the-art results in challenging free head pose gaze estimation tasks and on eye landmark localization (iris location) ones. © 2019, Springer Nature Switzerland AG.",,"Behavioral research; Computer vision; Geometric variations; Landmark localization; Landmark locations; Low resolution images; Social interactions; State of the art; Translation parameters; Visual information; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85061782488
"Makowski S., Jäger L.A., Abdelwahab A., Landwehr N., Scheffer T.","57205689294;56503248300;57205686686;10244371900;55122621900;","A discriminative model for identifying readers and assessing text comprehension from eye movements",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11051 LNAI",,,"209","225",,6,"10.1007/978-3-030-10925-7_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061120127&doi=10.1007%2f978-3-030-10925-7_13&partnerID=40&md5=da5582db44db12e98b85d517978d0f18","Department of Computer Science, University of Potsdam, August-Bebel-Straße 89, Potsdam, 14482, Germany; Department of Linguistics, University of Potsdam, Karl-Liebknecht-Straße 24–25, Potsdam, 14476, Germany; Weizenbaum Institute for the Networked Society, Hardenbergstraße 32, Berlin, 10623, Germany; Leibniz Institute for Agricultural Engineering and Bioeconomy, Max-Eyth-Allee 100, Potsdam, 14469, Germany","Makowski, S., Department of Computer Science, University of Potsdam, August-Bebel-Straße 89, Potsdam, 14482, Germany; Jäger, L.A., Department of Computer Science, University of Potsdam, August-Bebel-Straße 89, Potsdam, 14482, Germany, Department of Linguistics, University of Potsdam, Karl-Liebknecht-Straße 24–25, Potsdam, 14476, Germany, Weizenbaum Institute for the Networked Society, Hardenbergstraße 32, Berlin, 10623, Germany; Abdelwahab, A., Department of Computer Science, University of Potsdam, August-Bebel-Straße 89, Potsdam, 14482, Germany, Leibniz Institute for Agricultural Engineering and Bioeconomy, Max-Eyth-Allee 100, Potsdam, 14469, Germany; Landwehr, N., Department of Computer Science, University of Potsdam, August-Bebel-Straße 89, Potsdam, 14482, Germany, Leibniz Institute for Agricultural Engineering and Bioeconomy, Max-Eyth-Allee 100, Potsdam, 14469, Germany; Scheffer, T., Department of Computer Science, University of Potsdam, August-Bebel-Straße 89, Potsdam, 14482, Germany","We study the problem of inferring readers’ identities and estimating their level of text comprehension from observations of their eye movements during reading. We develop a generative model of individual gaze patterns (scanpaths) that makes use of lexical features of the fixated words. Using this generative model, we derive a Fisher-score representation of eye-movement sequences. We study whether a Fisher-SVM with this Fisher kernel and several reference methods are able to identify readers and estimate their level of text comprehension based on eye-tracking data. While none of the methods are able to estimate text comprehension accurately, we find that the SVM with Fisher kernel excels at identifying readers. © 2019, Springer Nature Switzerland AG.",,"Eye tracking; Machine learning; Discriminative models; Fisher kernels; Fisher score; Generative model; Lexical features; Reference method; Text comprehensions; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85061120127
"Drimalla H., Landwehr N., Baskow I., Behnia B., Roepke S., Dziobek I., Scheffer T.","56829587000;10244371900;57205691393;56050614900;57213425403;14023902800;55122621900;","Detecting autism by analyzing a simulated social interaction",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11051 LNAI",,,"193","208",,5,"10.1007/978-3-030-10925-7_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061120065&doi=10.1007%2f978-3-030-10925-7_12&partnerID=40&md5=a787080e4bb38067d8a8d7daffe00c4c","Department of Computer Science, University of Potsdam, Potsdam, Germany; Department of Psychology, Humboldt-Universität zu Berlin, Berlin, Germany; Berlin School of Mind and Brain, Humboldt-Universität zu Berlin, Berlin, Germany; Department of Psychiatry and Psychotherapy, Campus Benjamin Franklin, Charité-Universitätsmedizin Berlin, Berlin, Germany; Leibniz Institute for Agricultural Engineering and Bioeconomy, Potsdam, Germany","Drimalla, H., Department of Computer Science, University of Potsdam, Potsdam, Germany, Department of Psychology, Humboldt-Universität zu Berlin, Berlin, Germany, Berlin School of Mind and Brain, Humboldt-Universität zu Berlin, Berlin, Germany; Landwehr, N., Department of Computer Science, University of Potsdam, Potsdam, Germany, Leibniz Institute for Agricultural Engineering and Bioeconomy, Potsdam, Germany; Baskow, I., Department of Psychology, Humboldt-Universität zu Berlin, Berlin, Germany; Behnia, B., Department of Psychiatry and Psychotherapy, Campus Benjamin Franklin, Charité-Universitätsmedizin Berlin, Berlin, Germany; Roepke, S., Department of Psychiatry and Psychotherapy, Campus Benjamin Franklin, Charité-Universitätsmedizin Berlin, Berlin, Germany; Dziobek, I., Department of Psychology, Humboldt-Universität zu Berlin, Berlin, Germany, Berlin School of Mind and Brain, Humboldt-Universität zu Berlin, Berlin, Germany; Scheffer, T., Department of Computer Science, University of Potsdam, Potsdam, Germany","Diagnosing autism spectrum conditions takes several hours by well-trained practitioners; therefore, standardized questionnaires are widely used for first-level screening. Questionnaires as a diagnostic tool, however, rely on self-reflection—which is typically impaired in individuals with autism spectrum condition. We develop an alternative screening mechanism in which subjects engage in a simulated social interaction. During this interaction, the subjects’ voice, eye gaze, and facial expression are tracked, and features are extracted that serve as input to a predictive model. We find that a random-forest classifier on these features can detect autism spectrum condition accurately and functionally independently of diagnostic questionnaires. We also find that a regression model estimates the severity of the condition more accurately than the reference screening method. © 2019, Springer Nature Switzerland AG.",,"Decision trees; Diseases; Machine learning; Regression analysis; Diagnostic tools; Facial Expressions; Predictive modeling; Random forest classifier; Regression model; Screening mechanism; Screening methods; Social interactions; Surveys",Conference Paper,"Final","",Scopus,2-s2.0-85061120065
"Bhattacharjee A., Pal S.","57194030254;57193511741;","Attention of viewers while viewing paintings changes with the different ccts of exhibition light: A quantitative approach with eye-tracking method",2019,"Smart Innovation, Systems and Technologies","135",,,"487","496",,,"10.1007/978-981-13-5977-4_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060329504&doi=10.1007%2f978-981-13-5977-4_41&partnerID=40&md5=1277d787da27581f0339fe6c975252fb","Department of Design, IIT Guwahati, Guwahati, Assam, India","Bhattacharjee, A., Department of Design, IIT Guwahati, Guwahati, Assam, India; Pal, S., Department of Design, IIT Guwahati, Guwahati, Assam, India","Light influences the appearance of paintings in any exhibition. Few studies have experimented with correlated colour temperature (CCT) and illuminance of light to understand the lighting preference of viewers while viewing paintings. However, effect of only CCT on viewers’ perception is still a debatable issue. Also, previous studies in this regard have taken subjective approach with category rating that may lead to inconsistent conclusion. Therefore, a study has been designed with quantitative approach using eye-tracking method (N = 10) to verify the effect of different CCTs on viewers’ attention. The experimental result shows that viewers’ attention while viewing similar paintings changes with different CCTs of exhibition light having all other light parameters constant. © Springer Nature Singapore Pte Ltd 2019.","Attention; Light; Painting","Exhibitions; Light; Lighting; Painting; Attention; Category ratings; Eye tracking methods; Quantitative approach; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85060329504
"Shojaeizadeh M., Djamasbi S., Paffenroth R.C., Trapp A.C.","56964307900;18433762500;6506146565;24780062400;","Detecting task demand via an eye tracking machine learning system",2019,"Decision Support Systems","116",,,"91","101",,16,"10.1016/j.dss.2018.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056696188&doi=10.1016%2fj.dss.2018.10.012&partnerID=40&md5=7d466b5ecc52212aac68e9845b5a331a","Foisie School of Business, Worcester Polytechnic Institute, Worcester, MA, United States; Department of Mathematical Sciences, Data Science Program, Worcester Polytechnic Institute, Worcester, MA, United States; Foisie School of Business, Department of Mathematical Sciences, Data Science Program, Worcester Polytechnic Institute, Worcester, MA, United States","Shojaeizadeh, M., Foisie School of Business, Worcester Polytechnic Institute, Worcester, MA, United States; Djamasbi, S., Foisie School of Business, Worcester Polytechnic Institute, Worcester, MA, United States; Paffenroth, R.C., Department of Mathematical Sciences, Data Science Program, Worcester Polytechnic Institute, Worcester, MA, United States; Trapp, A.C., Foisie School of Business, Department of Mathematical Sciences, Data Science Program, Worcester Polytechnic Institute, Worcester, MA, United States","Computerized systems play a significant role in today's fast-paced digital economy. Because task demand is a major factor that influences how computerized systems are used to make decisions, identifying task demand automatically provides an opportunity for designing advanced decision support systems that can respond to user needs at a personalized level. A first step for designing such advanced decision tools is to investigate possibilities for developing automatic task load detectors. Grounded in decision making, eye tracking, and machine learning literature, we argue that task demand can be detected automatically, reliably, and unobtrusively using eye movements only. To investigate this possibility, we developed an eye tracking task load detection system and tested its effectiveness. Our results revealed that our task load detection system reliably predicted increased task demand from users' eye movement data. These results and their implications for research and practice are discussed. © 2018 Elsevier B.V.","Adaptive decision making; Cognitive effort; Eye tracking; Human computer interaction; Machine learning; Task demand","Artificial intelligence; Decision making; Decision support systems; Eye movements; Human computer interaction; Learning systems; Adaptive decision making; Cognitive efforts; Computerized systems; Digital economy; Eye movement datum; Load detection; Machine learning literature; Task demand; Eye tracking",Article,"Final","",Scopus,2-s2.0-85056696188
"Khosravan N., Celik H., Turkbey B., Jones E.C., Wood B., Bagci U.","57195066219;57212691442;9435311800;36071904900;7401873523;24176491700;","A collaborative computer aided diagnosis (C-CAD) system with eye-tracking, sparse attentional model, and deep learning",2019,"Medical Image Analysis","51",,,"101","115",,25,"10.1016/j.media.2018.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055895805&doi=10.1016%2fj.media.2018.10.010&partnerID=40&md5=f9f5b157dddcbc6a837b8b7787f1aba3","Center for Research in Computer Vision, University of Central FloridaFL, United States; Clinical Center, National Institutes of Health, Bethesda, MD, United States","Khosravan, N., Center for Research in Computer Vision, University of Central FloridaFL, United States; Celik, H., Clinical Center, National Institutes of Health, Bethesda, MD, United States; Turkbey, B., Clinical Center, National Institutes of Health, Bethesda, MD, United States; Jones, E.C., Clinical Center, National Institutes of Health, Bethesda, MD, United States; Wood, B., Clinical Center, National Institutes of Health, Bethesda, MD, United States; Bagci, U., Center for Research in Computer Vision, University of Central FloridaFL, United States","Computer aided diagnosis (CAD) tools help radiologists to reduce diagnostic errors such as missing tumors and misdiagnosis. Vision researchers have been analyzing behaviors of radiologists during screening to understand how and why they miss tumors or misdiagnose. In this regard, eye-trackers have been instrumental in understanding visual search processes of radiologists. However, most relevant studies in this aspect are not compatible with realistic radiology reading rooms. In this study, we aim to develop a paradigm shifting CAD system, called collaborative CAD (C-CAD), that unifies CAD and eye-tracking systems in realistic radiology room settings. We first developed an eye-tracking interface providing radiologists with a real radiology reading room experience. Second, we propose a novel algorithm that unifies eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a graph model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve their diagnostic decisions. The C-CAD uses radiologists’ search efficiency by processing their gaze patterns. Furthermore, the C-CAD incorporates a deep learning algorithm in a newly designed multi-task learning platform to segment and diagnose suspicious areas simultaneously. The proposed C-CAD system has been tested in a lung cancer screening experiment with multiple radiologists, reading low dose chest CTs. Promising results support the efficiency, accuracy and applicability of the proposed C-CAD system in a real radiology room setting. We have also shown that our framework is generalizable to more complex applications such as prostate cancer screening with multi-parametric magnetic resonance imaging (mp-MRI). © 2018 Elsevier B.V.","Attention; Eye-tracking; Graph sparsification; Lung cancer screening; Multi-task deep learning; Prostate cancer screening","Biological organs; Clustering algorithms; Computer aided instruction; Deep learning; Diseases; Efficiency; Eye movements; Eye tracking; Graphic methods; Learning algorithms; Magnetic resonance imaging; Radiation; Radiology; Tumors; Urology; Attention; Computer Aided Diagnosis(CAD); Eye tracking technologies; Graph sparsification; Graph-based clustering; Lung cancer screening; Prostate cancers; Radiology reading rooms; Computer aided diagnosis; Article; cancer screening; collaborative learning; computer assisted diagnosis; computer assisted tomography; conceptual framework; diagnostic accuracy; eye tracking; learning algorithm; lung cancer; machine learning; multiparametric magnetic resonance imaging; priority journal; prostate cancer; qualitative analysis; quantitative analysis; radiologist; technology; algorithm; computer assisted diagnosis; diagnostic error; diagnostic imaging; early cancer diagnosis; eye movement; female; human; lung tumor; male; nuclear magnetic resonance imaging; prevention and control; procedures; prostate tumor; x-ray computed tomography; Algorithms; Deep Learning; Diagnosis, Computer-Assisted; Diagnostic Errors; Early Detection of Cancer; Eye Movements; Female; Humans; Lung Neoplasms; Magnetic Resonance Imaging; Male; Prostatic Neoplasms; Tomography, X-Ray Computed",Article,"Final","",Scopus,2-s2.0-85055895805
"Mutlu-Bayraktar D.","55443406000;","Evaluation of change blindness in multimedia learning environment with cognitive process",2019,"Interactive Learning Environments","27","7",,"871","894",,1,"10.1080/10494820.2018.1530682","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054555754&doi=10.1080%2f10494820.2018.1530682&partnerID=40&md5=4d7da6f17ae326ec372677d04910af78","Istanbul University Cerrahpasa, Istanbul, Turkey","Mutlu-Bayraktar, D., Istanbul University Cerrahpasa, Istanbul, Turkey","This study aims to investigate the change blindness and cognitive processes with eye-tracking method in multimedia learning environment. For this purpose, a multimedia animation which had some changes was designed. The eye movements were examined during the process of detecting the changes in multimedia via eye-tracking technics. The research model was defined as controlled experiment and survey methods. Twenty-one ungraduated students participated in the experiment. Attention and perception levels of participants were determined by d2 Attention Test and Group Embedded Figures Test. Change detection numbers of participants were analyzed according to their attention level and field dependence. The appearance of a major object on the scene was the most detected change and the change on the detail object was detected less. According to findings about the attention level and change detection, the participants at high attention level were more successful at detecting change in multimedia. It was observed that field-independent individuals could detect the change much more compared to the field-dependent individuals. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.","attention; Change blindness; change detection; field dependence; multimedia learning environment",,Article,"Final","",Scopus,2-s2.0-85054555754
"Park S.J., Hong S., Kim D., Hussain I., Seo Y.","57191670651;56161261300;57193995429;57201649041;57201649886;","Intelligent in-car health monitoring system for elderly drivers in connected car",2019,"Advances in Intelligent Systems and Computing","823",,,"40","44",,7,"10.1007/978-3-319-96074-6_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051816235&doi=10.1007%2f978-3-319-96074-6_4&partnerID=40&md5=19c5dd33bb44ec0c414336e219521545","Korea Research Institute of Standards and Science, Daejeon, South Korea; Electronics Telecommunication Research Institute, Daejeon, South Korea; University of Science and Technology, Daejeon, South Korea","Park, S.J., Korea Research Institute of Standards and Science, Daejeon, South Korea, Electronics Telecommunication Research Institute, Daejeon, South Korea, University of Science and Technology, Daejeon, South Korea; Hong, S., Korea Research Institute of Standards and Science, Daejeon, South Korea, Electronics Telecommunication Research Institute, Daejeon, South Korea; Kim, D., Korea Research Institute of Standards and Science, Daejeon, South Korea, Electronics Telecommunication Research Institute, Daejeon, South Korea; Hussain, I., Korea Research Institute of Standards and Science, Daejeon, South Korea, Electronics Telecommunication Research Institute, Daejeon, South Korea, University of Science and Technology, Daejeon, South Korea; Seo, Y., Korea Research Institute of Standards and Science, Daejeon, South Korea","Introduction: Health has become a major concern nowadays. People pass significant amount of time of daily life on driving seat. Some health complexity happens during driving like heart problem, stroke etc. Driver’s health abnormality may also effect safety of other vehicles. So, automotive manufacturers and users are interested to include real-time health monitoring in car system. Intelligent in-car health monitoring is considered most innovative technology which is able to measure real-time physiological parameters of drivers, feed data to web cloud, analysis using machine learning, artificial intelligence and big data. Brain stroke is most deadly diseases and effected persons lose conscience and ability to contact emergency services or hospital. Emergency medical assistance is necessary in order to survive from any kind of disability due to stroke. Purpose: The aim of our study is to develop a health monitoring system for elderly drivers using air cushion car seat and embedded IoT (Internet of Things) devices in order to detect stroke onset during driving. Method: Real-time monitoring is desired to detect stroke onset during regular activities like driving. Abnormal physiological signals, face pattern generated during stroke onset can be traced by real-time monitoring using sensors. Here, we have suggested a framework of stroke onset detection using sensors and developed a system suitable for elderly drivers. This system can measure and analyze data of ECG, EEG, heart rate, seat pressure balance data, face/eye tracking etc. using IoT sensors. Physiological data will be feed to cloud and compared with reference normal person data. Findings: If any health abnormality such as stroke is found in real-time monitoring, system will predict type and severity of stroke and suggest possible steps. System may switch car control to autonomous driving mode if available and move the car to safe place. System may also generate alarm and send message with available information such as position to relatives and emergency services to provide emergency assistance so that effected driver can be transferred to hospital/clinic. © Springer Nature Switzerland AG 2019.","Brain stroke; Elderly healthcare; Internet of Things; Real-time monitoring","Air cushion vehicles; Artificial intelligence; Automobile manufacture; Automobile safety devices; Big data; Emergency services; Ergonomics; Health; Hospitals; Internet of things; Learning systems; Physiological models; Physiology; Automotive manufacturers; Brain strokes; Health monitoring system; Innovative technology; Physiological parameters; Physiological signals; Real time monitoring; Real-time health monitoring; Monitoring",Conference Paper,"Final","",Scopus,2-s2.0-85051816235
"Lotz A., Weissenberger S.","57202890385;57202887872;","Predicting Take-Over Times of Truck Drivers in Conditional Autonomous Driving",2019,"Advances in Intelligent Systems and Computing","786",,,"329","338",,4,"10.1007/978-3-319-93885-1_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049683422&doi=10.1007%2f978-3-319-93885-1_30&partnerID=40&md5=8ad0bae2fe18b94df2a23bbd51e6d0d0","Daimler AG, Daimler Trucks Advanced Engineering, HPC T332, Stuttgart, 70546, Germany","Lotz, A., Daimler AG, Daimler Trucks Advanced Engineering, HPC T332, Stuttgart, 70546, Germany; Weissenberger, S., Daimler AG, Daimler Trucks Advanced Engineering, HPC T332, Stuttgart, 70546, Germany","Conditional autonomous driving requires the description of sufficient time reserves for drivers in take-over situations. The definition of this time reserve has not been addressed for the truck context thus far. Through the observation of physiological measures, the possibility of estimating reaction times is considered. Driver data is collected with a remote eye-tracker and body posture camera. Empirical data from a simulator study is utilized to train and compare four machine learning algorithms and generate driver features. The estimation of take-over times is defined as a classification problem with four reaction time classes, leading to a misclassification rate of a linear support vector machine (SVM) of 38.7%. Utility of driver features for reaction time estimation are discussed. © Springer International Publishing AG, part of Springer Nature 2019.","Conditional autonomous driving; Control transition; Human factors; Machine learning; Take-over prediction","Artificial intelligence; Driver training; Eye tracking; Human engineering; Learning algorithms; Learning systems; Support vector machines; Truck drivers; Trucks; Autonomous driving; Body postures; Empirical data; Eye trackers; Linear Support Vector Machines; Misclassification rates; Physiological measures; Time estimation; Human reaction time",Conference Paper,"Final","",Scopus,2-s2.0-85049683422
"Peng H., Liu S., Zhang T.","43261755000;57200211055;57202834812;","Study on human-computer interaction in the design of public self-service equipment",2019,"Advances in Intelligent Systems and Computing","794",,,"174","183",,1,"10.1007/978-3-319-94947-5_17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049536930&doi=10.1007%2f978-3-319-94947-5_17&partnerID=40&md5=c1e099511eebaa84bfa75c8a801c55b6","School of Design, Guangzhou Higher Education Mega Center, South China University of Technology, Panyu District, Guangzhou, 510006, China","Peng, H., School of Design, Guangzhou Higher Education Mega Center, South China University of Technology, Panyu District, Guangzhou, 510006, China; Liu, S., School of Design, Guangzhou Higher Education Mega Center, South China University of Technology, Panyu District, Guangzhou, 510006, China; Zhang, T., School of Design, Guangzhou Higher Education Mega Center, South China University of Technology, Panyu District, Guangzhou, 510006, China","Public self-service equipment has a fixed use environment, complex product function, broad age group of users and other remarkable features compared to other products. This paper takes the increased amount of information and complicated operation of self-service equipment nowadays as the breakthrough point to conduct theory, case study and evaluation. This article studies the human-computer interaction factors in public self-service equipment from two aspects: appearance function design and interface interaction design. taking the bank self-service bank card machine as an example, and several simulation interface interactive systems are designed, then through the eye-tracking for testing, to analyze the data from the tests, according to the theoretical and experimental research results, the appearance and interactive interface of bank self-service card machine are designed. The results of this article greatly enhance the users’ interactive experience, and achieve the optimization and upgrading of human-computer interaction, the theoretical results of this article are of reference, the experimental results are repeatable, which are conducive to different disciplines of reference and using for reference. © 2019, Springer International Publishing AG, part of Springer Nature.","Interaction design; Public self-service equipment","Eye tracking; Human engineering; User interfaces; Amount of information; Breakthrough point; Experimental research; Interaction design; Interactive experiences; Interactive interfaces; Interface interaction; Public selves; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85049536930
"Zhang X., Sugano Y., Fritz M., Bulling A.","57142162900;7005470045;14035495500;6505807414;","MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation",2019,"IEEE Transactions on Pattern Analysis and Machine Intelligence","41","1","8122058","162","175",,116,"10.1109/TPAMI.2017.2778103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037648078&doi=10.1109%2fTPAMI.2017.2778103&partnerID=40&md5=07963eacac14fe52a5800aa8c93b3c84","Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, 66123, Germany; Graduate School of Information Science and Technology, Osaka University, Osaka, 565-0871, Japan","Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, 66123, Germany; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Osaka, 565-0871, Japan; Fritz, M., Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, 66123, Germany; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, 66123, Germany","Learning-based methods are believed to work well for unconstrained gaze estimation, i.e. gaze estimation from a monocular RGB camera without assumptions regarding user, environment, or camera. However, current gaze datasets were collected under laboratory conditions and methods were not evaluated across multiple datasets. Our work makes three contributions towards addressing these limitations. First, we present the MPIIGaze dataset, which contains 213,659 full face images and corresponding ground-truth gaze positions collected from 15 users during everyday laptop use over several months. An experience sampling approach ensured continuous gaze and head poses and realistic variation in eye appearance and illumination. To facilitate cross-dataset evaluations, 37,667 images were manually annotated with eye corners, mouth corners, and pupil centres. Second, we present an extensive evaluation of state-of-the-art gaze estimation methods on three current datasets, including MPIIGaze. We study key challenges including target gaze range, illumination conditions, and facial appearance variation. We show that image resolution and the use of both eyes affect gaze estimation performance, while head pose and pupil centre information are less informative. Finally, we propose GazeNet, the first deep appearance-based gaze estimation method. GazeNet improves on the state of the art by 22 percent (from a mean error of 13.9 degrees to 10.8 degrees) for the most challenging cross-dataset evaluation. © 1979-2012 IEEE.","convolutional neural network; cross-dataset evaluation; deep learning; Unconstrained gaze estimation","Cameras; Data structures; Estimation; Image resolution; Lighting; Magnetic heads; Neural networks; Three dimensional displays; Convolutional neural network; Cross-dataset evaluation; Experience sampling; Gaze estimation; Head; Illumination conditions; Laboratory conditions; Learning-based methods; Deep learning",Article,"Final","",Scopus,2-s2.0-85037648078
"Lahiri A., Agarwalla A., Biswas P.K.","56572183500;57195936642;7202443668;","Unsupervised Domain Adaptation for Learning Eye Gaze from a Million Synthetic Images: An Adversarial Approach",2018,"ACM International Conference Proceeding Series",,,"3293423","","",,1,"10.1145/3293353.3293423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098160381&doi=10.1145%2f3293353.3293423&partnerID=40&md5=2843b93eb2f12752a766f484f38bced9","Dept. of EandECE, Iit Kharagpur, India; Microsoft Idc Hyderabad and Iit, Kharagpur, India","Lahiri, A., Dept. of EandECE, Iit Kharagpur, India; Agarwalla, A., Microsoft Idc Hyderabad and Iit, Kharagpur, India; Biswas, P.K., Dept. of EandECE, Iit Kharagpur, India","With contemporary advancements of graphics engines, recent trend in deep learning community is to train models on automatically annotated simulated examples and apply on real data during test time. This alleviates the burden of manual annotation. However, there is an inherent difference of distributions between images coming from graphics engine and real world. Such domain difference deteriorates test time performances of models trained on synthetic examples. In this paper we address this issue with unsupervised adversarial feature adaptation across synthetic and real domain for the special use case of eye gaze estimation which is an essential component for various downstream HCI tasks. We initially learn a gaze estimator on annotated synthetic samples rendered from a 3D game engine and then adapt the features of unannotated real samples via a zero-sum minmax adversarial game against a domain discriminator following the recent paradigm of generative adversarial networks. Such adversarial adaptation forces features of both domains to be indistinguishable which enables us to use regression models trained on synthetic domain to be used on real samples. On the challenging MPIIGaze real life dataset, we outperform recent fully supervised methods trained on manually annotated real samples by appreciable margins and also achieve 13% more relative gain after adaptation compared to the current benchmark method of SimGAN [31]. Codes available at: https://github.com/abhinavagarwalla/adversarial_da_icvgip18. © 2018 ACM.","Adversarial Learning; Domain Adaptation; Domain Adversarial Networks; Gaze Prediction; Generative Adversarial Networks; MPIIGaze; UnityEyes","Computer vision; Regression analysis; Adversarial networks; Domain adaptation; Domain differences; Feature adaptation; Learning community; Manual annotation; Supervised methods; Synthetic images; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85098160381
"Wang K., Zhao R., Ji Q.","56637259500;56461916600;18935108400;","A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation",2018,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",,,"8578151","440","448",,21,"10.1109/CVPR.2018.00053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062837341&doi=10.1109%2fCVPR.2018.00053&partnerID=40&md5=619c2f3f5b07ac60f394d0c872e5fd9d","ECSE, Rensselaer Polytechnic Institute, Troy, NY, United States","Wang, K., ECSE, Rensselaer Polytechnic Institute, Troy, NY, United States; Zhao, R., ECSE, Rensselaer Polytechnic Institute, Troy, NY, United States; Ji, Q., ECSE, Rensselaer Polytechnic Institute, Troy, NY, United States","In this work, we introduce a Hierarchical Generative Model (HGM) to enable realistic forward eye image synthesis, as well as effective backward eye gaze estimation. The proposed HGM consists of a hierarchical generative shape model (HGSM), and a conditional bidirectional generative adversarial network (c-BiGAN). The HGSM encodes eye geometry knowledge and relates eye gaze with eye shape, while c-BiGAN leverages on big data and captures the dependency between eye shape and eye appearance. As an intermediate component, eye shape connects knowledge-based model (HGSM) with data-driven model (c-BiGAN) and enables bidirectional inference. Through a top-down inference, the HGM can synthesize eye images consistent with the given eye gaze. Through a bottom-up inference, HGM can infer eye gaze effectively from a given eye image. Qualitative and quantitative evaluations on benchmark datasets demonstrate our model's effectiveness on both eye image synthesis and eye gaze estimation. In addition, the proposed model is not restricted to eye images only. It can be adapted to face images and any shape-appearance related fields. © 2018 IEEE.",,"Computer vision; Knowledge based systems; Adversarial networks; Benchmark datasets; Data-driven model; Generative model; Intermediate components; Knowledge-based model; Quantitative evaluation; Top-down inference; C (programming language)",Conference Paper,"Final","",Scopus,2-s2.0-85062837341
"Xu Y., Dong Y., Wu J., Sun Z., Shi Z., Yu J., Gao S.","57192081433;57193158532;57206989929;57207759513;56510716400;8569656400;35224747100;","Gaze Prediction in Dynamic 360° Immersive Videos",2018,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",,,"8578657","5333","5342",,76,"10.1109/CVPR.2018.00559","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061647977&doi=10.1109%2fCVPR.2018.00559&partnerID=40&md5=b38ffd0e5c42da06f5bc212c487fcf7d","Shanghai Tech University, China","Xu, Y., Shanghai Tech University, China; Dong, Y., Shanghai Tech University, China; Wu, J., Shanghai Tech University, China; Sun, Z., Shanghai Tech University, China; Shi, Z., Shanghai Tech University, China; Yu, J., Shanghai Tech University, China; Gao, S., Shanghai Tech University, China","This paper explores gaze prediction in dynamic 360° immersive videos, i.e., based on the history scan path and VR contents, we predict where a viewer will look at an upcoming time. To tackle this problem, we first present the large-scale eye-tracking in dynamic VR scene dataset. Our dataset contains 208 360° videos captured in dynamic scenes, and each video is viewed by at least 31 subjects. Our analysis shows that gaze prediction depends on its history scan path and image contents. In terms of the image contents, those salient objects easily attract viewers' attention. On the one hand, the saliency is related to both appearance and motion of the objects. Considering that the saliency measured at different scales is different, we propose to compute saliency maps at different spatial scales: The sub-image patch centered at current gaze point, the sub-image corresponding to the Field of View (FoV), and the panorama image. Then we feed both the saliency maps and the corresponding images into a Convolutional Neural Network (CNN) for feature extraction. Meanwhile, we also use a Long-Short-Term-Memory (LSTM) to encode the history scan path. Then we combine the CNN features and LSTM features for gaze displacement prediction between gaze point at a current time and gaze point at an upcoming time. Extensive experiments validate the effectiveness of our method for gaze prediction in dynamic VR scenes. © 2018 IEEE.",,"Computer vision; Eye tracking; Forecasting; Large dataset; Visual communication; Convolutional neural network; Displacement prediction; Dynamic scenes; Field of views; Image content; Panorama images; Salient objects; Spatial scale; Long short-term memory",Conference Paper,"Final","",Scopus,2-s2.0-85061647977
"Ranjan R., De Mello S., Kautz J.","57212394654;57201314496;7006458237;","Light-weight head pose invariant gaze tracking",2018,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2018-June",,"8575461","2237","2245",,26,"10.1109/CVPRW.2018.00290","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060851979&doi=10.1109%2fCVPRW.2018.00290&partnerID=40&md5=8d0704aa71f4d6f1536afa3152268dd2","University of Maryland, United States; NVIDIA, United States","Ranjan, R., University of Maryland, United States; De Mello, S., NVIDIA, United States; Kautz, J., NVIDIA, United States","Unconstrained remote gaze tracking using off-the-shelf cameras is a challenging problem. Recently, promising algorithms for appearance-based gaze estimation using convolutional neural networks (CNN) have been proposed. Improving their robustness to various confounding factors including variable head pose, subject identity, illumination and image quality remain open problems. In this work, we study the effect of variable head pose on machine learning regressors trained to estimate gaze direction. We propose a novel branched CNN architecture that improves the robustness of gaze classifiers to variable head pose, without increasing computational cost. We also present various procedures to effectively train our gaze network including transfer learning from the more closely related task of object viewpoint estimation and from a large high-fidelity synthetic gaze dataset, which enable our ten times faster gaze network to achieve competitive accuracy to its current state-of-the-art direct competitor. © 2018 IEEE.",,"Computer vision; Gesture recognition; Image enhancement; Large dataset; Learning systems; Neural networks; Appearance based; Computational costs; Convolutional neural network; Gaze direction; Gaze estimation; High-fidelity; State of the art; Transfer learning; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85060851979
"Koutras P., Panagiotaropoulou G., Tsiami A., Maragos P.","56242401400;56230903700;56414740900;35243026700;","Audio-visual temporal saliency modeling validated by fMRI data",2018,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2018-June",,"8575437","2081","2091",,2,"10.1109/CVPRW.2018.00269","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060846276&doi=10.1109%2fCVPRW.2018.00269&partnerID=40&md5=69edcd1f731e396591128a3c599d037b","School of E.C.E., National Technical University of Athens, Greece","Koutras, P., School of E.C.E., National Technical University of Athens, Greece; Panagiotaropoulou, G., School of E.C.E., National Technical University of Athens, Greece; Tsiami, A., School of E.C.E., National Technical University of Athens, Greece; Maragos, P., School of E.C.E., National Technical University of Athens, Greece","In this work we propose an audio-visual model for predicting temporal saliency in videos, that we validate and evaluate in an alternative way by employing fMRI data. We intend to bridge the gap between the large improvements achieved during the last years in computational modeling, especially in deep learning, and the neurobiological and behavioral research regarding human vision. The proposed audio-visual model incorporates both state-of-the-art deep architectures for visual saliency, which were trained on eye-tracking data, and behavioral findings concerning audio-visual integration in multimedia stimuli. A new fMRI database has been collected for evaluation purposes, that includes various videos and subjects. This dataset may prove useful not only for saliency but for other computer vision problems as well. The evaluation of our model using the new fMRI database under a mixed-effect analysis shows that the proposed saliency model has strong correlation with both the visual and audio brain areas, that confirms its effectiveness and appropriateness in predicting audio-visual saliency for dynamic stimuli. © 2018 IEEE.",,"Behavioral research; Deep learning; Eye tracking; Visualization; Audio-visual integration; Computational model; Computer vision problems; Deep architectures; Dynamic stimuli; Saliency modeling; State of the art; Strong correlation; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85060846276
"Sun H.-P., Yang C.-H., Lai S.-H.","57205559589;56039197700;7402937330;","A deep learning approach to appearance-based gaze estimation under head pose variations",2018,"Proceedings - 4th Asian Conference on Pattern Recognition, ACPR 2017",,,"8575948","941","946",,,"10.1109/ACPR.2017.155","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060553240&doi=10.1109%2fACPR.2017.155&partnerID=40&md5=befade363acc2cc67a50394704d3b86e","National Tsing Hua Univ., Computer Science Dept., Hsinchu, Taiwan","Sun, H.-P., National Tsing Hua Univ., Computer Science Dept., Hsinchu, Taiwan; Yang, C.-H., National Tsing Hua Univ., Computer Science Dept., Hsinchu, Taiwan; Lai, S.-H., National Tsing Hua Univ., Computer Science Dept., Hsinchu, Taiwan","In this paper, we propose a deep learning based gaze estimation algorithm that estimates the gaze direction from a single face image. The proposed gaze estimation algorithm is based on using multiple convolutional neural networks (CNN) to learn the regression networks for gaze estimation from the eye images. The proposed algorithm can provide accurate gaze estimation for users with different head poses, since it explicitly includes the head pose information into the proposed gaze estimation framework. The proposed algorithm can be widely used for appearance-based gaze estimation in practice. Our experimental results show that the proposed gaze estimation system improves the accuracy of appearance-based gaze estimation under head pose variations compared to the previous methods. © 2017 IEEE.","Convolutional neural network; Deep learning; Gaze estimation","Convolution; Neural networks; Pattern recognition; Appearance based; Convolutional neural network; Eye images; Face images; Gaze direction; Gaze estimation; Head pose; Learning approach; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85060553240
"Ruiz N., Chong E., Rehg J.M.","57204290088;57194267364;7004835775;","Fine-grained head pose estimation without keypoints",2018,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2018-June",,"8575451","2155","2164",,124,"10.1109/CVPRW.2018.00281","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051963262&doi=10.1109%2fCVPRW.2018.00281&partnerID=40&md5=62954cd30ec3feedbca4541457229e13","Georgia Institute of Technology, United States","Ruiz, N., Georgia Institute of Technology, United States; Chong, E., Georgia Institute of Technology, United States; Rehg, J.M., Georgia Institute of Technology, United States","Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models. © 2018 IEEE.",,"3D modeling; Classification (of information); Computer vision; Large dataset; Neural networks; Open systems; Statistical tests; Benchmark datasets; Convolutional neural network; Correspondence problems; Head Pose Estimation; Image intensities; Landmark detection; Pose classifications; Training and testing; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85051963262
"Yamamoto T., Seo M., Kitajima T., Chen Y.-W.","57205504201;35280835900;57205503712;56036268200;","Eye Gaze Correction Using Generative Adversarial Networks",2018,"2018 IEEE 7th Global Conference on Consumer Electronics, GCCE 2018",,,"8574844","431","432",,1,"10.1109/GCCE.2018.8574844","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060291711&doi=10.1109%2fGCCE.2018.8574844&partnerID=40&md5=2c1c1bdeb582d4457b83d5b288d4a465","Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan; Sumsung RD Institute Japan, Osaka, Japan","Yamamoto, T., Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan; Seo, M., Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan; Kitajima, T., Sumsung RD Institute Japan, Osaka, Japan; Chen, Y.-W., Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan","Eye gaze correction is an important topic in video teleconference and video chart in order to keep the eye contact. In this paper, we propose to use a generative adversarial networks for eye gaze correction. We use pairs of front facial image (idea camera setting) and real facial image (real camera setting) to training the network. By using the trained network, we can generate a gaze corrected facial image (front facial image) for any real facial image. Experiments demonstrated the effectiveness of our proposed method. © 2018 IEEE.","Conditional GAN; Deep learning; Gaze correction; Generative Adversarial Net(GAN); Image-to-image translation","Cameras; Adversarial networks; Camera settings; Conditional GAN; Eye contact; Eye-gaze; Facial images; Generative Adversarial Net(GAN); Image translation; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85060291711
"Schwehr J., Willert V.","57204616940;8987807400;","Multi-Hypothesis Multi-Model Driver's Gaze Target Tracking",2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2018-November",,"8569655","1427","1434",,3,"10.1109/ITSC.2018.8569655","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060442457&doi=10.1109%2fITSC.2018.8569655&partnerID=40&md5=f3de811a107ffeb281186e5bedcb69f1","Control Methods and Robotics, TU Darmstadt, Germany","Schwehr, J., Control Methods and Robotics, TU Darmstadt, Germany; Willert, V., Control Methods and Robotics, TU Darmstadt, Germany","For a safe handover of the driving task or driver-adaptive warning strategies the driver's situation awareness is a helpful source of information. In order to estimate and track the driver's focus of attention over time in a dynamic automotive scene, a Multi-Hypothesis Multi-Model probabilistic tracking framework was developed in which we postulate consistency between machine and human perception during gaze fixations. Within this framework, we explicitly included target object motion in the spatial transition step and integrated spatiotemporal models of human-like gaze behavior for fixations and saccades in the motion transition. This elaborate design makes the target estimation robust and yet flexible. At the same time, the representation in continuous 2D coordinates makes the algorithm run in real time on a standard laptop. By incorporating dynamic and static potential gaze targets from an object list and a free space spline, the algorithm is in principle independent from the applied sensor setup. The benefit of the proposed model is presented on real world data where the filter's tracking performance as well as the driver's visual sampling are presented based on an exemplary scene. © 2018 IEEE.",,"Behavioral research; Eye tracking; Human computer interaction; Intelligent systems; Intelligent vehicle highway systems; Interoperability; Focus of Attention; Motion transition; Multi-hypothesis; Probabilistic tracking; Situation awareness; Spatio-temporal models; Target estimations; Tracking performance; Target tracking",Conference Paper,"Final","",Scopus,2-s2.0-85060442457
"Wang X., Koch S., Holmqvist K., Alexa M.","57190735109;57198210324;8357720500;7003588954;","Tracking the gaze on objects in 3D: How do people really look at the bunny?",2018,"SIGGRAPH Asia 2018 Technical Papers, SIGGRAPH Asia 2018",,,"188","","",,4,"10.1145/3272127.3275094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066106177&doi=10.1145%2f3272127.3275094&partnerID=40&md5=d8d064393a2e099e90bc18c3a91c3373","TU Berlin, Department of Computer Science and Electrical Engineering, Sekretariat MAR 6-6, Marchstr. 23, Berlin, 10587, Germany; Universität Regensburg, Institute für Psy-chologie, Universitätsstrasse 31, Regensburg, 93053, Germany","Wang, X., TU Berlin, Department of Computer Science and Electrical Engineering, Sekretariat MAR 6-6, Marchstr. 23, Berlin, 10587, Germany; Koch, S., TU Berlin, Department of Computer Science and Electrical Engineering, Sekretariat MAR 6-6, Marchstr. 23, Berlin, 10587, Germany; Holmqvist, K., Universität Regensburg, Institute für Psy-chologie, Universitätsstrasse 31, Regensburg, 93053, Germany; Alexa, M., TU Berlin, Department of Computer Science and Electrical Engineering, Sekretariat MAR 6-6, Marchstr. 23, Berlin, 10587, Germany","We provide the first large dataset of human fixations on physical 3D objects presented in varying viewing conditions and made of different materials. Our experimental setup is carefully designed to allow for accurate calibration and measurement. We estimate a mapping from the pair of pupil positions to 3D coordinates in space and register the presented shape with the eye tracking setup. By modeling the fixated positions on 3D shapes as a probability distribution, we analysis the similarities among different conditions. The resulting data indicates that salient features depend on the viewing direction. Stable features across different viewing directions seem to be connected to semantically meaningful parts. We also show that it is possible to estimate the gaze density maps from view dependent data. The dataset provides the necessary ground truth data for computational models of human perception in 3D. © 2018 Association for Computing Machinery.","3D object viewing; Eye tracking; Mesh saliency","Interactive computer graphics; Large dataset; Probability distributions; 3D object; Calibration and measurements; Computational model; Ground truth data; Mesh saliencies; Salient features; Viewing conditions; Viewing directions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85066106177
"Nagano K., Seo J., Xing J., Wei L., Li Z., Saito S., Agarwal A., Fursund J., Li H.","55825559100;35786690300;57215048703;55877551400;57200614728;56312543900;57208443430;55965826400;55082661800;","Pagan: Real-time avatars using dynamic textures",2018,"SIGGRAPH Asia 2018 Technical Papers, SIGGRAPH Asia 2018",,,"258","","",,23,"10.1145/3272127.3275075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066081751&doi=10.1145%2f3272127.3275075&partnerID=40&md5=20793bfbdf3d073b54a6e63e7eace5f5","Pinscreen, United States; USC Institute for Creative Technologies, United States; University of Southern California, United States","Nagano, K., Pinscreen, United States, USC Institute for Creative Technologies, United States; Seo, J., Pinscreen, United States; Xing, J., USC Institute for Creative Technologies, United States; Wei, L., Pinscreen, United States; Li, Z., University of Southern California, United States; Saito, S., Pinscreen, United States, University of Southern California, United States; Agarwal, A., Pinscreen, United States; Fursund, J., Pinscreen, United States; Li, H., Pinscreen, United States, USC Institute for Creative Technologies, United States, University of Southern California, United States","With the rising interest in personalized VR and gaming experiences comes the need to create high quality 3D avatars that are both low-cost and variegated. Due to this, building dynamic avatars from a single unconstrained input image is becoming a popular application. While previous techniques that attempt this require multiple input images or rely on transferring dynamic facial appearance from a source actor, we are able to do so using only one 2D input image without any form of transfer from a source image. We achieve this using a new conditional Generative Adversarial Network design that allows fine-scale manipulation of any facial input image into a new expression while preserving its identity. Our photoreal avatar GAN (paGAN) can also synthesize the unseen mouth interior and control the eye-gaze direction of the output, as well as produce the final image from a novel viewpoint. The method is even capable of generating fully-controllable temporally stable video sequences, despite not using temporal information during training. After training, we can use our network to produce dynamic image-based avatars that are controllable on mobile devices in real time. To do this, we compute a fixed set of output images that correspond to key blendshapes, from which we extract textures in UV space. Using a subject's expression blendshapes at run-time, we can linearly blend these key textures together to achieve the desired appearance. Furthermore, we can use the mouth interior and eye textures produced by our network to synthesize on-the-fly avatar animations for those regions. Our work produces state-of-the-art quality image and video synthesis, and is the first to our knowledge that is able to generate a dynamically textured avatar with a mouth interior, all from a single image. © 2018 Association for Computing Machinery.","Digital avatar; Facial animation; Generative adversarial network; Image-based rendering; Texture synthesis","Image segmentation; Interactive computer graphics; Textures; Three dimensional computer graphics; Adversarial networks; Digital avatar; Facial animation; Image based rendering; Texture synthesis; Image texture",Conference Paper,"Final","",Scopus,2-s2.0-85066081751
"Anwar S., Milanova M., Svetleff Z., Abdulla S.","57193015705;7003785945;57200761694;57205575562;","Real Time Eye Gaze Estimation",2018,"Proceedings - 2017 International Conference on Computational Science and Computational Intelligence, CSCI 2017",,,"8560846","526","531",,1,"10.1109/CSCI.2017.89","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060582787&doi=10.1109%2fCSCI.2017.89&partnerID=40&md5=c898a6e1182551b901dfc753cffdf3e7","Computer Science Department, UA Little RockAR, United States; Salahaddin University, Kurdistan Region/Erbil, Iraq; Department of Education Psychology and Higher Eduaction, University of Nevada, Las Vegas, United States; Information Technolgy Department, Polytechnic University, Kurdistan Region, Erbil, Iraq","Anwar, S., Computer Science Department, UA Little RockAR, United States, Salahaddin University, Kurdistan Region/Erbil, Iraq; Milanova, M., Computer Science Department, UA Little RockAR, United States; Svetleff, Z., Department of Education Psychology and Higher Eduaction, University of Nevada, Las Vegas, United States; Abdulla, S., Information Technolgy Department, Polytechnic University, Kurdistan Region, Erbil, Iraq","In this paper we used a set of searching techniques that allow to gain information about eye movement, its location and point of view in real time. The algorithm determines the point on the monitor at which the user is looking at. To obtain such data it is necessary to determine the relative position of the eye and the head. The first step is the initialization during which a head model is created. After initialization, the tracking phase is started using an Active Appearance Models (AAM) and Pose from Orthography and Scaling with ITerations (POSIT) algorithm for head position estimation. The purpose of eye gaze estimation or eye tracking can be used for testing the effectiveness of the text, game, or advertising message. The aim of this work is to develop and implement a system for real time eye gaze estimation using PC's webcam only without any additional hardware. © 2017 IEEE.","AAM; Active Appearance Model; eye gaze; gaze estimation; POSIT","Artificial intelligence; Eye movements; Image recognition; Active appearance models; Eye-gaze; Gain information; Gaze estimation; Head position; POSIT; Relative positions; Searching techniques; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85060582787
"Ruan L., Chen B., Lam M.-L.","57205468831;56742504100;7202630301;","Human-computer interaction by voluntary vergence control",2018,"SIGGRAPH Asia 2018 Posters, SA 2018",,,"3283356","","",,,"10.1145/3283289.3283356","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060137263&doi=10.1145%2f3283289.3283356&partnerID=40&md5=db9eddc3e75da9f4bded54604ca6d339","City University of Hong Kong, Hong Kong; City University of Hong Kong CityU, Shenzhen Research Institute, Hong Kong","Ruan, L., City University of Hong Kong, Hong Kong; Chen, B., City University of Hong Kong, Hong Kong; Lam, M.-L., City University of Hong Kong CityU, Shenzhen Research Institute, Hong Kong","Most people can voluntarily control vergence eye movements. However, the interaction possibility of using vergence as an active input remain largely unexplored. We present a novel human-computer interaction technique which allows a user to control the depth position of an object based on voluntary vergence of the eyes. Our technique is similar to the mechanism for seeing the intended 3D image of an autostereogram, which requires cross-eyed or walleyed viewing. We invite the user to look at a visual target that is mounted on a linear motor, then consciously control the eye convergence to focus at a point in front of or behind the target. A camera is used to measure the eye convergence and control the motion of the linear motor dynamically based on the measured distance. Our technique can enhance existing eye-tracking methods by providing additional information in the depth dimension, and has great potential for hands-free interaction and assistive applications. © 2018 Copyright held by the owner/author(s).","Eye tracking; Human computer interaction; Voluntary vergence","Eye movements; Eye tracking; Interactive computer graphics; Linear motors; Assistive applications; Eye tracking methods; Hands-free interactions; Object based; Vergence control; Vergence eye movements; Vergences; Visual targets; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85060137263
"Cha X., Yang X., Feng Z., Xu T., Fan X., Tian J.","57214798390;55683790100;7403443516;56683649700;57197729874;57208750027;","Calibration-Free Gaze Zone Estimation Using Convolutional Neural Network",2018,"2018 International Conference on Security, Pattern Analysis, and Cybernetics, SPAC 2018",,,"8965441","481","484",,1,"10.1109/SPAC46244.2018.8965441","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079151448&doi=10.1109%2fSPAC46244.2018.8965441&partnerID=40&md5=4aed1162415507b4c3ef9bf1ad22e6c3","University of Jinan, School of Information Science and Engineering, Jinan, 250022, China","Cha, X., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China; Yang, X., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China; Feng, Z., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China; Xu, T., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China; Fan, X., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China; Tian, J., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China","In this paper we propose a gaze zone estimation method using deep learning. Compared with traditional method, our method does not need the procedure of calibration. In the proposed method, a Kinect is used to capture the video of a computer user, which is pre-processed to suppress illumination variations. After that, haar cascade classifier is adopted to detect the face region and eye region. Then, the eye region is used to estimate the gaze zone on the monitor via a trained CNN (Convolution Neural Network). Experimental results show that the proposed method has a high accuracy, which can be applied in human-computer interaction. © 2018 IEEE.","convolutional neural network; deep learning; eye tracking; Gaze estimation","Calibration; Classification (of information); Convolution; Deep learning; Deep neural networks; Eye tracking; Human computer interaction; Calibration free; Computer users; Convolution neural network; Estimation methods; Gaze estimation; Haar cascade classifiers; High-accuracy; Illumination variation; Convolutional neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85079151448
"Ghiass R.S., Laurendeau D.","25654959400;7004448364;","Highly accurate and fully automatic 3D head pose estimation and eye gaze estimation using RGB-3D sensors and 3D morphable models",2018,"Sensors (Switzerland)","18","12","4280","","",,3,"10.3390/s18124280","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058845756&doi=10.3390%2fs18124280&partnerID=40&md5=da7b483286b03a8a4a99066047cd8bba","Computer Vision and Systems Laboratory, Laval University, Universite Laval, 1665 Rue de l’Universite, Quebec City, QC  G1V 0A6, Canada","Ghiass, R.S., Computer Vision and Systems Laboratory, Laval University, Universite Laval, 1665 Rue de l’Universite, Quebec City, QC  G1V 0A6, Canada; Laurendeau, D., Computer Vision and Systems Laboratory, Laval University, Universite Laval, 1665 Rue de l’Universite, Quebec City, QC  G1V 0A6, Canada","This work addresses the problem of automatic head pose estimation and its application in 3D gaze estimation using low quality RGB-D sensors without any subject cooperation or manual intervention. The previous works on 3D head pose estimation using RGB-D sensors require either an offline step for supervised learning or 3D head model construction, which may require manual intervention or subject cooperation for complete head model reconstruction. In this paper, we propose a 3D pose estimator based on low quality depth data, which is not limited by any of the aforementioned steps. Instead, the proposed technique relies on modeling the subject’s face in 3D rather than the complete head, which, in turn, relaxes all of the constraints in the previous works. The proposed method is robust, highly accurate and fully automatic. Moreover, it does not need any offline step. Unlike some of the previous works, the method only uses depth data for pose estimation. The experimental results on the Biwi head pose database confirm the efficiency of our algorithm in handling large pose variations and partial occlusion. We also evaluated the performance of our algorithm on IDIAP database for 3D head pose and eye gaze estimation. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","3D eye gaze estimation; 3D head pose estimation; 3D morphable models; Iterative closest point; RGB-D sensors","Iterative methods; 3D head; 3D Morphable model; Eye-gaze; Iterative Closest Points; Rgb-d sensors; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85058845756
"Lodato C., Ribino P.","25123054300;23393633000;","A Novel Vision-Enhancing Technology for Low-Vision Impairments",2018,"Journal of Medical Systems","42","12","256","","",,5,"10.1007/s10916-018-1108-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056238685&doi=10.1007%2fs10916-018-1108-1&partnerID=40&md5=169bd81e68213c2e30aeb726249001d6","Istituto di Calcolo e Reti ad Alte Prestazioni, via Ugo La Malfa 153, Palermo, Italy","Lodato, C., Istituto di Calcolo e Reti ad Alte Prestazioni, via Ugo La Malfa 153, Palermo, Italy; Ribino, P., Istituto di Calcolo e Reti ad Alte Prestazioni, via Ugo La Malfa 153, Palermo, Italy","Ocular disorders such as vitreoretinal pathologies are widespread, especially in older adults. In particular, degenerative diseases of the retina such as macular senile degenerations are on the rise and affect millions of people with hundreds of thousands of new cases each year. These diseases can cause profoundly disabling visual impairments, in some cases severely compromising the central and/or the peripheral vision in one or both eyes. In this paper, we present a novel vision aids technology that allows for correcting or attenuating the perception of visual field defects due to ocular pathologies of diverse origins or traumas by using techniques of 3D visualisation, eye tracking, and image processing. The presented technology is mainly conceived for providing vision aids that can significantly improve the quality of life of people with this kind of visual disorders. As well, it could be employed for supporting the diagnosis of ocular dysfunctions and for monitoring the progression of diseases. The technology shown in this work is protected by an International Application in Patent Cooperation Treaty (PCT). © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Binocular vision; Low-vision aids; Visual field defects; Vitreoretinal impairments","Article; binocular vision; digital filtering; eye fixation; eye tracking; human; low vision; monocular vision; optical tomography; peripheral vision; quality of life; retina degeneration; retina injury; stereoscopic vision; three dimensional imaging; videorecording; vision; visual field; visual field defect; visual system examination; visual system parameters; equipment design; eye movement; image processing; low vision; procedures; sensory aid; visually impaired person; Equipment Design; Eye Movements; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Quality of Life; Sensory Aids; Vision, Binocular; Vision, Low; Visually Impaired Persons",Article,"Final","",Scopus,2-s2.0-85056238685
"Lejeune L., Grossrieder J., Sznitman R.","57195776416;57203819458;36100657400;","Iterative multi-path tracking for video and volume segmentation with sparse point supervision",2018,"Medical Image Analysis","50",,,"65","81",,6,"10.1016/j.media.2018.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053059454&doi=10.1016%2fj.media.2018.08.007&partnerID=40&md5=066ab3c8dedcda3dbfe434009150ef9d","Ophthalmic Technology Laboratory, ARTORG Center, University of Bern, Murtenstrasse 50, 3008 Bern, Switzerland","Lejeune, L., Ophthalmic Technology Laboratory, ARTORG Center, University of Bern, Murtenstrasse 50, 3008 Bern, Switzerland; Grossrieder, J., Ophthalmic Technology Laboratory, ARTORG Center, University of Bern, Murtenstrasse 50, 3008 Bern, Switzerland; Sznitman, R., Ophthalmic Technology Laboratory, ARTORG Center, University of Bern, Murtenstrasse 50, 3008 Bern, Switzerland","Recent machine learning strategies for segmentation tasks have shown great ability when trained on large pixel-wise annotated image datasets. It remains a major challenge however to aggregate such datasets, as the time and monetary cost associated with collecting extensive annotations is extremely high. This is particularly the case for generating precise pixel-wise annotations in video and volumetric image data. To this end, this work presents a novel framework to produce pixel-wise segmentations using minimal supervision. Our method relies on 2D point supervision, whereby a single 2D location within an object of interest is provided on each image of the data. Our method then estimates the object appearance in a semi-supervised fashion by learning object-image-specific features and by using these in a semi-supervised learning framework. Our object model is then used in a graph-based optimization problem that takes into account all provided locations and the image data in order to infer the complete pixel-wise segmentation. In practice, we solve this optimally as a tracking problem using a K-shortest path approach. Both the object model and segmentation are then refined iteratively to further improve the final segmentation. We show that by collecting 2D locations using a gaze tracker, our approach can provide state-of-the-art segmentations on a range of objects and image modalities (video and 3D volumes), and that these can then be used to train supervised machine learning classifiers. © 2018 Elsevier B.V.","Multi-path tracking; Point-wise supervision; Semantic segmentation; Semi-supervised learning","Artificial intelligence; Eye tracking; Graphic methods; Iterative methods; Learning algorithms; Location; Pixels; Semantics; Supervised learning; Multipaths; Optimization problems; Point wise; Semantic segmentation; Semi- supervised learning; Supervised machine learning; Volume segmentation; Volumetric images; Image segmentation; article; classifier; gaze; human; human experiment; supervised machine learning; videorecording; algorithm; machine learning; procedures; three dimensional imaging; Algorithms; Humans; Imaging, Three-Dimensional; Machine Learning; Supervised Machine Learning",Article,"Final","",Scopus,2-s2.0-85053059454
"John B., Banerjee A., Raiturkar P., Jain E.","57205639875;57201100950;57193240475;36715118000;","An evaluation of pupillary light response models for 2D screens and VR HMDs",2018,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,"3281538","","",,8,"10.1145/3281505.3281538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060940034&doi=10.1145%2f3281505.3281538&partnerID=40&md5=0f13eb73d5d1bb2a7acafcb88eea3bb3","University of Florida, United States","John, B., University of Florida, United States; Banerjee, A., University of Florida, United States; Raiturkar, P., University of Florida, United States; Jain, E., University of Florida, United States","Pupil diameter changes have been shown to be indicative of user engagement and cognitive load for various tasks and environments. However, it is still not the preferred physiological measure for applied settings. This reluctance to leverage the pupil as an index of user engagement stems from the problem that in scenarios where scene brightness cannot be controlled, the pupil light response confounds the cognitive-emotional response. What if we could predict the light response of an individual’s pupil, thus creating the opportunity to factor it out of the measurement? In this work, we lay the groundwork for this research by evaluating three models of pupillary light response in 2D, and in a virtual reality (VR) environment. Our results show that either a linear or an exponential model can be fit to an individual participant with an easy-to-use calibration procedure. This work opens several new research directions in VR relating to performance analysis and inspires the use of eye tracking beyond gaze as a pointer and foveated rendering. © 2018 Association for Computing Machinery.","Eyetracking; Light response; Pupil dilation; Videos; Virtual reality","Virtual reality; Calibration procedure; Emotional response; Exponential models; Light response; Performance analysis; Physiological measures; Pupil dilation; Videos; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85060940034
"Jyoti S., Dhall A.","57202800224;35229206900;","Automatic Eye Gaze Estimation using Geometric Texture-based Networks",2018,"Proceedings - International Conference on Pattern Recognition","2018-August",,"8545162","2474","2479",,3,"10.1109/ICPR.2018.8545162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059762588&doi=10.1109%2fICPR.2018.8545162&partnerID=40&md5=09ed648b4f7ffb8f46c7e2014fa92ef9","Learning Affect and SemantIc Imaging (LASII) Group, Indian Institute of Technology Ropar, India","Jyoti, S., Learning Affect and SemantIc Imaging (LASII) Group, Indian Institute of Technology Ropar, India; Dhall, A., Learning Affect and SemantIc Imaging (LASII) Group, Indian Institute of Technology Ropar, India","Eye gaze estimation is an important problem in automatic human behavior understanding. This paper proposes a deep learning based method for inferring the eye gaze direction. The method is based on the use of ensemble of networks, which capture both the geometric and texture information. Firstly, a Deep Neural Network (DNN) is trained using the geometric features that are extracted from the facial landmark locations. Secondly, for the texture based features, three Convolutional Neural Networks (CNN) are trained i.e. For the patch around the left eye, right eye, and the combined eyes, respectively. Finally, the information from the four channels is fused with concatenation and dense layers are trained to predict the final eye gaze. The experiments are performed on the two publicly available datasets: Columbia eye gaze and TabletGaze. The extensive evaluation shows the superior performance of the proposed framework. We also evaluate the performance of the recently proposed swish activation function as compared to Rectified Linear Unit (ReLU) for eye gaze estimation. © 2018 IEEE.",,"Behavioral research; Deep neural networks; Geometry; Neural networks; Activation functions; Convolutional Neural Networks (CNN); Facial landmark; Geometric feature; Geometric texture; Human behavior understanding; Learning-based methods; Texture information; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85059762588
"Cao L., Gou C., Wang K., Xiong G., Wang F.-Y.","57191250405;56320227000;55901133200;55733323100;57211758869;","Gaze-Aided Eye Detection via Appearance Learning",2018,"Proceedings - International Conference on Pattern Recognition","2018-August",,"8545635","1965","1970",,4,"10.1109/ICPR.2018.8545635","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058784655&doi=10.1109%2fICPR.2018.8545635&partnerID=40&md5=6bc55a11c210cd1b3f430118ccfdd305","Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Qingdao Academy of Intelligent Industries, Qingdao, China; Cloud Computing Center, Chinese Academy of Sciences, Dongguan, China","Cao, L., Institute of Automation, Chinese Academy of Sciences, Beijing, China, University of Chinese Academy of Sciences, Beijing, China; Gou, C., Institute of Automation, Chinese Academy of Sciences, Beijing, China, Qingdao Academy of Intelligent Industries, Qingdao, China; Wang, K., Institute of Automation, Chinese Academy of Sciences, Beijing, China, Qingdao Academy of Intelligent Industries, Qingdao, China; Xiong, G., Institute of Automation, Chinese Academy of Sciences, Beijing, China, Cloud Computing Center, Chinese Academy of Sciences, Dongguan, China; Wang, F.-Y., Institute of Automation, Chinese Academy of Sciences, Beijing, China, Qingdao Academy of Intelligent Industries, Qingdao, China","Image based eye detection and gaze estimation have a wide range of potential applications, such as medical treatment, biometrics recognition, human-computer interaction. Though a large number of researchers have attempted to solve the two problems, they still exist some challenges due to the variation in appearance and lack of annotated images. In addition, most related work perform eye detection first, followed by gaze estimation via appearance learning. In this paper, we propose a unified framework to execute the gaze estimation and the eye detection simultaneously by learning the cascade regression models from appearance around the eye related key points. Intuitively, there is coupled relationship among location of eye center, shape of eye related key points, appearance representation and gaze information. To incorporate these information, at each cascade level, we first learn a model to map the shape and appearance around current eye related key points to the three dimension gaze update. Then, with the help of estimated gaze, we further learn a regression model to map the gaze, shape and appearance information to eye location update. By leveraging the power of cascade learning, the proposed method can alternatively optimize the two tasks of eye detection and gaze estimation. The experiments are conducted on benchmarks of GI4E and MPIIGaze. Experimental results show that our proposed method can achieve preferable results in gaze estimation and outperform the state-of-the-art methods in eye detection. © 2018 IEEE.",,"Human computer interaction; Medical imaging; Optical character recognition; Regression analysis; Appearance learning; Eye detection; Gaze estimation; Medical treatment; Regression model; State-of-the-art methods; Three dimensions; Unified framework; Eye protection",Conference Paper,"Final","",Scopus,2-s2.0-85058784655
"Muralidhar S., Siegfried R., Gatica-Perez D., Odobez J.-M.","57193554182;57195685304;6602722700;57203103085;","Facing employers and customers: What do gaze and expressions tell about soft skills?",2018,"ACM International Conference Proceeding Series",,,,"121","126",,4,"10.1145/3282894.3282925","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059971715&doi=10.1145%2f3282894.3282925&partnerID=40&md5=c17fbb13c0c5dc6c0ee972b0d692a7ee","Idiap, EPFL, Switzerland","Muralidhar, S., Idiap, EPFL, Switzerland; Siegfried, R., Idiap, EPFL, Switzerland; Gatica-Perez, D., Idiap, EPFL, Switzerland; Odobez, J.-M., Idiap, EPFL, Switzerland","Eye gaze and facial expressions are central to face-to-face social interactions. These behavioral cues and their connections to first impressions have been widely studied in psychology and computing literature, but limited to a single situation. Utilizing ubiquitous multimodal sensors coupled with advances in computer vision and machine learning, we investigate the connections between these behavioral cues and perceived soft skills in two diverse workplace situations (job interviews and reception desk). Pearson’s correlation analysis shows a moderate connection between certain facial expressions, eye gaze cues and perceived soft skills in job interviews (r 2 [30, 30]) and desk (r 2 [20, 36]) situations. Results of our computational framework to infer perceived soft skills indicates a low predictive power of eye gaze, facial expressions, and their combination in both interviews (R 2 2 [0.02, 0.21]) and desk (R 2 2 [0.05, 0.15]) situations. Our work has important implications for employee training and behavioral feedback systems. © 2018 Association for Computing Machinery. All Rights Reserved.","Eye gaze; Facial expressions; First impressions; Hirability; Hospitality; Job performance; Multimodal interaction; Social computing","Feedback control; Learning systems; Eye-gaze; Facial Expressions; First impressions; Hirability; Hospitality; Job performance; Multi-Modal Interactions; Social computing; Personnel training",Conference Paper,"Final","",Scopus,2-s2.0-85059971715
"Singh M., Walia G.S., Goswami A.","57201344513;16240294500;56022767100;","Using Supervised Learning to Guide the Selection of Software Inspectors in Industry",2018,"Proceedings - 29th IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2018",,,"8539156","12","17",,2,"10.1109/ISSREW.2018.00-38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059854124&doi=10.1109%2fISSREW.2018.00-38&partnerID=40&md5=d03d6b4ca09268fafc04329f81300cb3","Department of Computer Science, North Dakota State University, Fargo, United States; Department of Computer Science, Bennett University, Greater Noida, India","Singh, M., Department of Computer Science, North Dakota State University, Fargo, United States; Walia, G.S., Department of Computer Science, North Dakota State University, Fargo, United States; Goswami, A., Department of Computer Science, Bennett University, Greater Noida, India","Software development is a multi-phase process that starts with requirement engineering. Requirements elicited from different stakeholders are documented in natural language (NL) software requirement specification (SRS) document. Due to the inherent ambiguity of NL, SRS is prone to faults (e.g., ambiguity, incorrectness, inconsistency). To find and fix faults early (where they are cheapest to find), companies routinely employ inspections, where skilled inspectors are selected to review the SRS and log faults. While other researchers have attempted to understand the factors (experience and learning styles) that can guide the selection of effective inspectors but could not report improved results. This study analyzes the reading patterns (RPs) of inspectors recorded by eye-tracking equipment and evaluates their abilities to find various fault-types. The inspectors' characteristics are selected by employing ML algorithms to find the most common RPs w.r.t each fault-types. Our results show that our approach could guide the inspector selection with an accuracy ranging between 79.3% and 94% for various fault-types. © 2018 IEEE.","classifiers; eye tracking; Fault types; inspector selection; machine learning; reading patterns","Classifiers; Eye tracking; Learning systems; Requirements engineering; Software design; Technical presentations; Fault types; inspector selection; Learning Style; Natural languages; Reading patterns; Requirement engineering; Selection of software; Software requirement specification; Software reliability",Conference Paper,"Final","",Scopus,2-s2.0-85059854124
"Yun X., Sun Y., Wang S., Shi Y., Lu N.","41462242100;14519670400;57203137800;57203124329;36634780500;","Multi-layer convolutional network-based visual tracking via important region selection",2018,"Neurocomputing","315",,,"145","156",,2,"10.1016/j.neucom.2018.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050635192&doi=10.1016%2fj.neucom.2018.07.010&partnerID=40&md5=cce4c3267b5c0431c93e4acfbed40092","School of Information and Control Engineering, China University of Mining and Technology, 1 Daxue Road, Jiangsu, Xuzhou, 221116, China","Yun, X., School of Information and Control Engineering, China University of Mining and Technology, 1 Daxue Road, Jiangsu, Xuzhou, 221116, China; Sun, Y., School of Information and Control Engineering, China University of Mining and Technology, 1 Daxue Road, Jiangsu, Xuzhou, 221116, China; Wang, S., School of Information and Control Engineering, China University of Mining and Technology, 1 Daxue Road, Jiangsu, Xuzhou, 221116, China; Shi, Y., School of Information and Control Engineering, China University of Mining and Technology, 1 Daxue Road, Jiangsu, Xuzhou, 221116, China; Lu, N., School of Information and Control Engineering, China University of Mining and Technology, 1 Daxue Road, Jiangsu, Xuzhou, 221116, China","The convolutional network-based tracking (CNT) algorithm provides a training network with warped target regions in the first frame instead of large auxiliary datasets, which solves the problem of convolutional neural network (CNN)-based tracking requiring very long training time and a large number of auxiliary training samples. However, the two-layer CNT uses only gray feature that causes sensitivity to appearance variations. Besides, some samples with useless information should be removed to avoid drifting problems. For these reasons, a multi-layer convolutional network-based visual tracking algorithm via important region selection (IRST) is proposed in this paper. The proposed important region selection model is built via high entropy selection and background discrimination, which enables the training samples to be informative in order to provide enough stable information and also be discriminative so as to resist distractors. The feature maps are also obtained by weighting the template filters with cluster weights. Instead of simple gray features, IRST adds the Gabor layer to explore the texture feature of the target that is effective on coping with illumination and rotation variations. Extensive experiments show that the proposed algorithm achieves superior performances in many challenging visual tracking tasks. © 2018 Elsevier B.V.","Convolutional network-based tracker; Convolutional neural network; Gabor filter; Important region selection; Visual tracking","Convolution; Gabor filters; Neural networks; Sampling; Sensitivity analysis; Background discrimination; Convolutional networks; Convolutional neural network; Convolutional Neural Networks (CNN); Region selections; Training network; Visual Tracking; Visual tracking algorithm; Network layers; accuracy; Article; classification algorithm; convolutional neural network; entropy; eye tracking; illumination; image segmentation; machine learning; mathematical model; nonlinear system; priority journal; probability; reliability; rotation; sensitivity analysis",Article,"Final","",Scopus,2-s2.0-85050635192
"Cai L., Yang R., Tao Z.","57205193872;56768521700;57195031884;","A new method of evaluating signage system using mixed reality and eye tracking",2018,"Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience, EM-GIS 2018",,,"a2","","",,3,"10.1145/3284103.3284105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059003950&doi=10.1145%2f3284103.3284105&partnerID=40&md5=a2cc63b29070c4a341325791f6f13376","Engineering Physics Tsinghua University, Beijing, China","Cai, L., Engineering Physics Tsinghua University, Beijing, China; Yang, R., Engineering Physics Tsinghua University, Beijing, China; Tao, Z., Engineering Physics Tsinghua University, Beijing, China","Signage system is utilized to identify the emergency exit when the accident occurs. To evaluate the design of indoor signage system, a new mobile eye tracking method, that integrated eye tracker in a mixed reality application, was proposed in mixed reality environment. A virtual scene was reconstructed from the real world using mixed reality technique. Moreover, different virtual exit signs were placed on the wall in the mixed reality environment. Accurate measurement of movement and gaze with a precise timestamp were obtained in the environment. The data was used to project 3D fixation point onto the environment. The amount of fixation point and fixation time were simultaneously counted to evaluate the signage system. To demonstrate the feasibility of the method, an experiment was conducted on the 10th floor of LiuQing building in Tsinghua University. The results showed that the new mobile eye tracking method can not only easily set up the experimental evacuation environment, but also capture quantitative fixation data, providing technical support for evaluating signage system. © 2018 Association for Computing Machinery.","Emergency evacuation; Eye tracking; Mixed reality; Signage system; Way finding","Geographic information systems; Mixed reality; Accurate measurement; Emergency evacuation; Mixed-reality environment; Mobile eye-tracking; Signage system; Technical support; Tsinghua University; Way finding; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85059003950
"Castillo S., Hahn P., Legde K., Cunningham D.W.","56312658200;57205073831;56841631400;7402319584;","Personality analysis of embodied conversational agents",2018,"Proceedings of the 18th International Conference on Intelligent Virtual Agents, IVA 2018","2018-May",,,"227","232",,4,"10.1145/3267851.3267853","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069738303&doi=10.1145%2f3267851.3267853&partnerID=40&md5=41c92c41f75c04aea0c8028d61f4ec43","BTU Cottbus-Senftenberg, Germany","Castillo, S., BTU Cottbus-Senftenberg, Germany; Hahn, P., BTU Cottbus-Senftenberg, Germany; Legde, K., BTU Cottbus-Senftenberg, Germany; Cunningham, D.W., BTU Cottbus-Senftenberg, Germany","People tend to personify machines. Giving machines the ability to actually produce social information can help improve human-machine interactions. Embodied Conversational Agents (ECAs) are virtual software agents that can process and produce speech, facial expressions, gestures and eye gaze, enabling natural, multimodal, human-machine communication. On the one hand, the field of personality psychology provides insights into how we could describe and measure the virtual personality of ECAs. On the other hand, ECAs provide a method to systematically examine how different factors affect the perception of personality. This paper shows that standardized, validated personality questionnaires can be used to evaluate ECAs psychologically, and that state of the art ECAs can manipulate their perceived personality through appearance and behavior. © 2018 Association for Computing Machinery.","Embodied Conversational Agents; Multimodal communication; Personality","Human computer interaction; Software agents; Speech communication; Surveys; Appearance and behavior; Embodied conversational agent; Facial Expressions; Human machine interaction; Human-machine communication; Multimodal communications; Personality; Social information; Intelligent virtual agents",Conference Paper,"Final","",Scopus,2-s2.0-85069738303
"Hahn P., Castillo S., Cunningham D.W.","57205073831;56312658200;7402319584;","Look Me in the lines: The impact of stylization on the recognition of expressions and perceived personality",2018,"Proceedings of the 18th International Conference on Intelligent Virtual Agents, IVA 2018",,,,"339","340",,1,"10.1145/3267851.3267881","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058453254&doi=10.1145%2f3267851.3267881&partnerID=40&md5=4dc2cd9c16a82a2a73c0e0b87db54f15","BTU Cottbus-Senftenberg, Germany","Hahn, P., BTU Cottbus-Senftenberg, Germany; Castillo, S., BTU Cottbus-Senftenberg, Germany; Cunningham, D.W., BTU Cottbus-Senftenberg, Germany","We are increasingly approaching the point where computer-based technology is truly ambient and omnipresent. People tend to personify their technical servants, including giving them human names as well as attributing personality traits and intentions to them. The more those devices advance from simple tools to intelligent assistants the more seriously we need to take this personification. That is, if the computers perform human-like tasks in collaboration with humans, and humans already tend to treat computers as human-like, it is only reasonable to give those devices a human-like appearance and conversational abilities. Therefore, one approach to design advanced human-machine interfaces relies heavily on the so-called Embodied Conversational Agents (ECAs). An ECA is a virtual software agent that can process and produce speech, facial expressions, gestures and eye-gaze and, as a result, enables natural, multimodal, human-machine communication. Decades of research in psychology and related fields have shown that the visual channel is especially important in human-human-communication, with subtle changes in both appearance and motion altering how a conversational partner is perceived. In this work, we examine the effectiveness of modifying a virtual character’s visual appearance using well-known stylization techniques in order to alter its perceived personality. We also explore the effect of these techniques on the recognizability, intensity and sincerity of the character’s displayed emotions. © 2018 Copyright held by the owner/author(s).","Character Design; Embodied Conversational Agents; Emotions; Personality; Stylization","Human computer interaction; Software agents; Speech communication; User interfaces; Character designs; Embodied conversational agent; Emotions; Personality; Stylization; Intelligent virtual agents",Conference Paper,"Final","",Scopus,2-s2.0-85058453254
"Zalake M., Kapoor A., Woodward J., Lok B.","57202542910;57201773758;57193574146;57203616548;","Assessing the impact of virtual human’s appearance on users’ trust levels",2018,"Proceedings of the 18th International Conference on Intelligent Virtual Agents, IVA 2018",,,,"329","330",,2,"10.1145/3267851.3267863","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058446164&doi=10.1145%2f3267851.3267863&partnerID=40&md5=132b7b437b92eb8c4b1ff4a3f9a6196f","University of Florida, Gainesville, FL, United States","Zalake, M., University of Florida, Gainesville, FL, United States; Kapoor, A., University of Florida, Gainesville, FL, United States; Woodward, J., University of Florida, Gainesville, FL, United States; Lok, B., University of Florida, Gainesville, FL, United States","Virtual humans are used to facilitate interactions in sensitive contexts such as health-care. In such contexts, trust in the information source plays an important role in reception of the information. Prior work has shown that physical appearance affects trustworthiness in human-human interactions; therefore, we examined the effect of virtual human’s appearance on users’ trust. We ran a between-users study with 12 adult participants, who watched a video of a virtual human with professional attire (e.g., lab coat) or with general attire (e.g., button-down shirt). We examined the duration of eye fixation on the virtual human’s face along with participants’ self-reported trust levels. We found that there was no statistical difference in eye contact or trust between the two test conditions. © 2018 Copyright held by the owner/author(s).","Eye-tracking; Health-care; User trust; Virtual Human","Eye tracking; Health care; Eye contact; Eye fixations; Human-human interactions; Information sources; Statistical differences; Test condition; User trust; Virtual humans; Intelligent virtual agents",Conference Paper,"Final","",Scopus,2-s2.0-85058446164
"Griswold-Steiner I., Fyke Z., Ahmed M., Serwadda A.","57201857962;57210821829;57210822296;56824295100;","Morph-a-Dope: Using Pupil Manipulation to Spoof Eye Movement Biometrics",2018,"2018 9th IEEE Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2018",,,"8796625","543","552",,2,"10.1109/UEMCON.2018.8796625","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071554311&doi=10.1109%2fUEMCON.2018.8796625&partnerID=40&md5=d23a1fc35381187b2bc628ac99997c0a","Department of Computer Science, Texas Tech University, Lubbock, TX  79409, United States","Griswold-Steiner, I., Department of Computer Science, Texas Tech University, Lubbock, TX  79409, United States; Fyke, Z., Department of Computer Science, Texas Tech University, Lubbock, TX  79409, United States; Ahmed, M., Department of Computer Science, Texas Tech University, Lubbock, TX  79409, United States; Serwadda, A., Department of Computer Science, Texas Tech University, Lubbock, TX  79409, United States","Eye Tracking Authentication - a mechanism where eye movement patterns are used to verify a user's identity - is increasingly being explored for use as a layer of security in computing systems. Despite being widely studied, there is barely any research investigating how these systems could be attacked by a determined attacker. In particular, the relationship between pupil characteristics and lighting is one that could lead to vulnerabilities in improperly secured systems.This paper presents Morph-a-Dope, an attack that leverages lighting manipulations to defeat eye tracking authentication systems that heavily rely on features derived from pupil sizes. Across 20 attacker-victim pairs, the attack increased the EER by an average of over 50% as compared to the zero-effort attack by the overall population, and as much as 500% for individual victims. Our research calls for a greater emphasis on manipulation-resistant pupil size features or system designs that otherwise avoid such vulnerabilities. © 2018 IEEE.","behavioral biometrics; continuous authentication; eye tracking biometrics; machine learning; spoof attack","Authentication; Biometrics; Eye tracking; Learning systems; Lighting; Mobile telecommunication systems; Ubiquitous computing; Authentication systems; Behavioral biometrics; Computing system; Continuous authentications; Eye movement patterns; Pupil size; spoof attack; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85071554311
"Wang X., Koch S., Holmqvist K., Alexa M.","57190735109;57198210324;8357720500;7003588954;","Tracking the gaze on objects in 3d: How do people really look at the bunny?",2018,"ACM Transactions on Graphics","37","6","188","","",,3,"10.1145/3272127.3275094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064816296&doi=10.1145%2f3272127.3275094&partnerID=40&md5=8cec5dcd806fc5b7a762e9b6fed5c352","TU Berlin, Department of Computer Science and Electrical Engineering, Sekretariat MAR 6-6, Marchstr. 23, Berlin, 10587, Germany; Universität Regensburg, Institute für Psychologie, Universitätsstrasse 31, Regensburg, 93053, Germany","Wang, X., TU Berlin, Department of Computer Science and Electrical Engineering, Sekretariat MAR 6-6, Marchstr. 23, Berlin, 10587, Germany; Koch, S., TU Berlin, Department of Computer Science and Electrical Engineering, Sekretariat MAR 6-6, Marchstr. 23, Berlin, 10587, Germany; Holmqvist, K., Universität Regensburg, Institute für Psychologie, Universitätsstrasse 31, Regensburg, 93053, Germany; Alexa, M., TU Berlin, Department of Computer Science and Electrical Engineering, Sekretariat MAR 6-6, Marchstr. 23, Berlin, 10587, Germany","We provide the first large dataset of human fixations on physical 3D objects presented in varying viewing conditions and made of different materials. Our experimental setup is carefully designed to allow for accurate calibration and measurement. We estimate a mapping from the pair of pupil positions to 3D coordinates in space and register the presented shape with the eye tracking setup. By modeling the fixated positions on 3D shapes as a probability distribution, we analysis the similarities among different conditions. The resulting data indicates that salient features depend on the viewing direction. Stable features across different viewing directions seem to be connected to semantically meaningful parts. We also show that it is possible to estimate the gaze density maps from view dependent data. The dataset provides the necessary ground truth data for computational models of human perception in 3D. © 2018 Association for Computing Machinery.","3D object viewing; Eye tracking; Mesh saliency","Large dataset; Probability distributions; 3D object; Calibration and measurements; Computational model; Ground truth data; Mesh saliencies; Salient features; Viewing conditions; Viewing directions; Eye tracking",Article,"Final","",Scopus,2-s2.0-85064816296
"Nagano K., Seo J., Xing J., Wei L., Li Z., Saito S., Agarwal A., Fursund J., Li H.","55825559100;35786690300;57215048703;55877551400;57200614728;56312543900;57208443430;55965826400;55082661800;","PaGAN: Real-time avatars using dynamic textures",2018,"ACM Transactions on Graphics","37","6","258","","",,32,"10.1145/3272127.3275075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064803718&doi=10.1145%2f3272127.3275075&partnerID=40&md5=ac385eb6dd4aa0980545e1bbdc728cb7","Pinscreen, USC Institute for Creative Technologies, United States; Pinscreen, United States; USC University of Southern California, United States; University of Southern California, United States; University of Southern California, Pinscreen, United States; University of Southern California, USC Institute for Creative Technologies, United States","Nagano, K., Pinscreen, USC Institute for Creative Technologies, United States; Seo, J., Pinscreen, United States; Xing, J., USC University of Southern California, United States; Wei, L., Pinscreen, United States; Li, Z., University of Southern California, United States; Saito, S., University of Southern California, Pinscreen, United States; Agarwal, A., Pinscreen, United States; Fursund, J., Pinscreen, United States; Li, H., University of Southern California, USC Institute for Creative Technologies, United States","With the rising interest in personalized VR and gaming experiences comes the need to create high quality 3D avatars that are both low-cost and variegated. Due to this, building dynamic avatars from a single unconstrained input image is becoming a popular application. While previous techniques that attempt this require multiple input images or rely on transferring dynamic facial appearance from a source actor, we are able to do so using only one 2D input image without any form of transfer from a source image. We achieve this using a new conditional Generative Adversarial Network design that allows fine-scale manipulation of any facial input image into a new expression while preserving its identity. Our photoreal avatar GAN (paGAN) can also synthesize the unseen mouth interior and control the eye-gaze direction of the output, as well as produce the final image from a novel viewpoint. The method is even capable of generating fully-controllable temporally stable video sequences, despite not using temporal information during training. After training, we can use our network to produce dynamic image-based avatars that are controllable on mobile devices in real time. To do this, we compute a fixed set of output images that correspond to key blendshapes, from which we extract textures in UV space. Using a subject's expression blendshapes at run-time, we can linearly blend these key textures together to achieve the desired appearance. Furthermore, we can use the mouth interior and eye textures produced by our network to synthesize on-The-fly avatar animations for those regions. Our work produces state-ofthe-art quality image and video synthesis, and is the first to our knowledge that is able to generate a dynamically textured avatar with a mouth interior, all from a single image. © 2018 Association for Computing Machinery.","Digital avatar; Facial animation; Generative adversarial network; Imagebased rendering; Texture synthesis","Image segmentation; Interactive computer graphics; Textures; Adversarial networks; Digital avatar; Facial animation; Image based rendering; Texture synthesis; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85064803718
"Fang Y., Zhang X., Imamoglu N.","8435698900;57203362331;38061650600;","A novel superpixel-based saliency detection model for 360-degree images",2018,"Signal Processing: Image Communication","69",,,"1","7",,22,"10.1016/j.image.2018.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051383029&doi=10.1016%2fj.image.2018.07.009&partnerID=40&md5=879de84fee9e978a8ac0d0431820dbed","School of Information Technology, Jiangxi University of Finance and Economics, Jiangxi, Nanchang, 330032, China; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan","Fang, Y., School of Information Technology, Jiangxi University of Finance and Economics, Jiangxi, Nanchang, 330032, China; Zhang, X., School of Information Technology, Jiangxi University of Finance and Economics, Jiangxi, Nanchang, 330032, China; Imamoglu, N., National Institute of Advanced Industrial Science and Technology, Tokyo, Japan","Effective visual attention modeling is a key factor that helps enhance the overall Quality of Experience (QoE) of VR/AR data. Although a huge number of algorithms have been developed in recent years to detect salient regions in flat-2D images, the research on 360-degree image saliency is limited. In this study, we propose a superpixel-level saliency detection model for 360-degree images by figure-ground law of Gestalt theory. First, the input image is segmented into superpixels. CIE Lab color space is then used to extract the perceptual features. We extract luminance and texture features for 360-degree images from L channel, while color features are extracted from a and b channels. We compute two components for saliency prediction by figure-ground law of Gestalt theory: feature contrast and boundary connectivity. The feature contrast is computed on superpixel level by luminance and color features. The boundary connectivity is predicted for background measure and it describes the spatial layout of image region with two image boundaries (upper and lower boundary). The final saliency map of 360-degree image is calculated by fusing feature contrast map and boundary connectivity map. Experimental results on a public eye tracking database of 360-degree images show promising performance of saliency prediction from the proposed method. © 2018 Elsevier B.V.","360-degree image; Boundary connectivity; Figure-ground law; Gestalt theory; Saliency detection; Visual attention","Behavioral research; Color; Eye tracking; Image segmentation; Luminance; Pixels; Quality of service; 360-degree image; Boundary connectivity; Figure ground; Gestalt theory; Saliency detection; Visual Attention; Superpixels",Article,"Final","",Scopus,2-s2.0-85051383029
"Masse B., Ba S., Horaud R.","57191161185;7003397447;7003505326;","Tracking Gaze and Visual Focus of Attention of People Involved in Social Interaction",2018,"IEEE Transactions on Pattern Analysis and Machine Intelligence","40","11","8194910","2711","2724",,23,"10.1109/TPAMI.2017.2782819","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038832500&doi=10.1109%2fTPAMI.2017.2782819&partnerID=40&md5=6970b5bb5e22f62ccb797f6dad983a65","INRIA Grenoble Rhone-Alpes and with, Universite Grenoble Alpes, Montbonnot, Saint-Martin, 38400, France; Dailymotion, Paris, 75017, France","Masse, B., INRIA Grenoble Rhone-Alpes and with, Universite Grenoble Alpes, Montbonnot, Saint-Martin, 38400, France; Ba, S., Dailymotion, Paris, 75017, France; Horaud, R., INRIA Grenoble Rhone-Alpes and with, Universite Grenoble Alpes, Montbonnot, Saint-Martin, 38400, France","The visual focus of attention (VFOA) has been recognized as a prominent conversational cue. We are interested in estimating and tracking the VFOAs associated with multi-party social interactions. We note that in this type of situations the participants either look at each other or at an object of interest; therefore their eyes are not always visible. Consequently both gaze and VFOA estimation cannot be based on eye detection and tracking. We propose a method that exploits the correlation between eye gaze and head movements. Both VFOA and gaze are modeled as latent variables in a Bayesian switching state-space model (also referred switching Kalman filter). The proposed formulation leads to a tractable learning method and to an efficient online inference procedure that simultaneously tracks gaze and visual focus. The method is tested and benchmarked using two publicly available datasets, Vernissage and LAEO, that contain typical multi-party human-robot and human-human interactions. © 1979-2012 IEEE.","dynamic Bayesian models; eye gaze; head pose; human-robot interaction; multi-party interaction; switching state-space models; Visual focus of attention","Bayesian networks; Eye movements; Eye protection; Eye tracking; Human computer interaction; State space methods; Bayesian model; Eye-gaze; Head pose; multi-party dialog; Visual focus of attentions; Human robot interaction; algorithm; attention; Bayes theorem; eye fixation; eye movement; factual database; female; human; human relation; machine learning; male; Markov chain; normal distribution; psychophysics; robotics; Algorithms; Attention; Bayes Theorem; Databases, Factual; Eye Movements; Female; Fixation, Ocular; Humans; Interpersonal Relations; Machine Learning; Male; Markov Chains; Normal Distribution; Psychophysics; Robotics",Article,"Final","",Scopus,2-s2.0-85038832500
"Kononenko D., Ganin Y., Sungatullina D., Lempitsky V.","57142551900;56938634700;55821404200;22234735100;","Photorealistic Monocular Gaze Redirection Using Machine Learning",2018,"IEEE Transactions on Pattern Analysis and Machine Intelligence","40","11","8010348","2696","2710",,6,"10.1109/TPAMI.2017.2737423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028454003&doi=10.1109%2fTPAMI.2017.2737423&partnerID=40&md5=5967e9341855f072d35ffd09c0f030c4","Skolkovo Institute of Science and Technology, Moscow, 143026, Russian Federation; Universite de Montreal, Montreal, Quebec  136800, Canada","Kononenko, D., Skolkovo Institute of Science and Technology, Moscow, 143026, Russian Federation; Ganin, Y., Universite de Montreal, Montreal, Quebec  136800, Canada; Sungatullina, D., Skolkovo Institute of Science and Technology, Moscow, 143026, Russian Federation; Lempitsky, V., Skolkovo Institute of Science and Technology, Moscow, 143026, Russian Federation","We propose a general approach to the gaze redirection problem in images that utilizes machine learning. The idea is to learn to re-synthesize images by training on pairs of images with known disparities between gaze directions. We show that such learning-based re-synthesis can achieve convincing gaze redirection based on monocular input, and that the learned systems generalize well to people and imaging conditions unseen during training. We describe and compare three instantiations of our idea. The first system is based on efficient decision forest predictors and redirects the gaze by a fixed angle in real-time (on a single CPU), being particularly suitable for the videoconferencing gaze correction. The second system is based on a deep architecture and allows gaze redirection by a range of angles. The second system achieves higher photorealism, while being several times slower. The third system is based on real-time decision forests at test time, while using the supervision from a 'teacher' deep network during training. The third system approaches the quality of a teacher network in our experiments, and thus provides a highly realistic real-time monocular solution to the gaze correction problem. We present in-depth assessment and comparisons of the proposed systems based on quantitative measurements and a user study. © 1979-2012 IEEE.","deep learning; Gaze redirection; image resynthesis; machine learning; random forest; weakly-supervised learning","Artificial intelligence; Decision trees; Image processing; Learning systems; Gaze direction; Gaze redirection; Imaging conditions; Photo-realistic; Random forests; Resynthesis; Weakly supervised learning; Deep learning; anatomy and histology; artificial neural network; decision tree; eye fixation; eye movement; face; facial expression; factual database; female; human; image processing; machine learning; male; procedures; videoconferencing; Databases, Factual; Decision Trees; Deep Learning; Eye Movements; Face; Facial Expression; Female; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Machine Learning; Male; Neural Networks (Computer); Videoconferencing",Article,"Final","",Scopus,2-s2.0-85028454003
"Lemley J., Kar A., Corcoran P.","23005117400;56956378200;57190839462;","Eye Tracking in Augmented Spaces: A Deep Learning Approach",2018,"2018 IEEE Games, Entertainment, Media Conference, GEM 2018",,,"8516529","396","401",,4,"10.1109/GEM.2018.8516529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057042612&doi=10.1109%2fGEM.2018.8516529&partnerID=40&md5=812f1f4908c656975d3169bbda75becb","NUI Galway College of Engineering Informatics, Center for Cognitive Connected Computational Imaging, Galway, Ireland","Lemley, J., NUI Galway College of Engineering Informatics, Center for Cognitive Connected Computational Imaging, Galway, Ireland; Kar, A., NUI Galway College of Engineering Informatics, Center for Cognitive Connected Computational Imaging, Galway, Ireland; Corcoran, P., NUI Galway College of Engineering Informatics, Center for Cognitive Connected Computational Imaging, Galway, Ireland","The use of deep learning for estimating eye gaze in augmented spaces is investigated in this work. There are two primary ways of interacting with augmented spaces. The first involves the use of AR/VR systems; the second involves devices that respond to the user's gaze directly. This domain can overlap with AR/VR environments but is not exclusive to them and contains its own unique set of issues. Deep learning methods for eye tracking that are capable of performing with minimal power consumption are investigated for both problems. © 2018 IEEE.","Augmented reality; Convolutional neural networks; Deep learning; Gaze estimation; Smart spaces; Virtual reality","Augmented reality; Eye tracking; Neural networks; Virtual reality; Convolutional neural network; Eye-gaze; Gaze estimation; Learning approach; Learning methods; Smart space; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85057042612
"Xia Y., Lou J., Dong J., Li G., Yu H.","57192671097;57192670261;22634069200;13408307700;56115992300;","SDM-Based means of gradient for eye center localization",2018,"Proceedings - IEEE 16th International Conference on Dependable, Autonomic and Secure Computing, IEEE 16th International Conference on Pervasive Intelligence and Computing, IEEE 4th International Conference on Big Data Intelligence and Computing and IEEE 3rd Cyber Science and Technology Congress, DASC-PICom-DataCom-CyberSciTec 2018",,,"8511989","857","861",,4,"10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00-17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056894893&doi=10.1109%2fDASC%2fPiCom%2fDataCom%2fCyberSciTec.2018.00-17&partnerID=40&md5=ae3f253f305eefa034765cf20d9ece07","1School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Key Laboratory of Metallurgical Equipment and Control Technology, Ministry of Education, Wuhan University of Science and Technology, Wuhan, 430081, China","Xia, Y., 1School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Lou, J., 1School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Dong, J., 1School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Li, G., Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Yu, H., Key Laboratory of Metallurgical Equipment and Control Technology, Ministry of Education, Wuhan University of Science and Technology, Wuhan, 430081, China","For eye gaze estimation and eye tracking, localizing eye center is a crucial requirement. This task is challenging work because of the significant variability of eye appearance in illumination, shape, color and viewing angles. In this paper, we improve the performance of means of gradient method in low resolution images, which could locate the eye center more accurately. The proposed method applies Supervised Descent Method (SDM), which has remarkable achievement in the field of face alignment, to improve the traditional means of gradient method in localizing eye center. We extensively evaluate our method on BioID database which is very challenging and realistic for eye center localization. Moreover, we have com-pared our method with existing state of the art methods and the results of the experiment confirm that the proposed method is an attractive alternative for eye center localization. © 2018 IEEE.","Eye Center Localization; Eye Gaze Estimation; Means of Gradient.","Big data; Gradient methods; Image enhancement; Descent method; Experiment confirm; Eye Center Localization; Eye-gaze; Face alignment; Low resolution images; State-of-the-art methods; Viewing angle; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85056894893
"Melo E.V.","23469045400;","Improving collaborative filtering-based image recommendation through use of Eye Gaze tracking",2018,"Information (Switzerland)","9","11","262","","",,3,"10.3390/info9110262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056106595&doi=10.3390%2finfo9110262&partnerID=40&md5=c7cd1561ebff1e25d7eaa393366d4638","Department of Computer Engineering, Instituto Federal do Triângulo Mineiro, Uberaba, 38064-190, Brazil","Melo, E.V., Department of Computer Engineering, Instituto Federal do Triângulo Mineiro, Uberaba, 38064-190, Brazil","Due to the overwhelming variety of products and services currently available on electronic commerce sites, the consumer finds it difficult to encounter products of preference. It is common that product preference be influenced by the visual appearance of the image associated with the product. In this context, Recommendation Systems for products that are associated with Images (IRS) become vitally important in aiding consumers to find those products considered as pleasing or useful. In general, these IRS use the Collaborative Filtering technique that is based on the behaviour passed on by users. One of the principal challenges found with this technique is the need for the user to supply information concerning their preference. Therefore, methods for obtaining implicit information are desirable. In this work, the author proposes an investigation to discover to which extent information concerning user visual attention can aid in producing a more precise IRS. This work proposes therefore a new approach, which combines the preferences passed on from the user, by means of ratings and visual attention data. The experimental results show that our approach exceeds that of the state of the art. © 2018 by the authors.","Collaborative filtering; Image recommendation; Image similarity; Recommendation systems; Visual attention","Behavioral research; Collaborative filtering; Image enhancement; Recommender systems; Collaborative filtering techniques; Eye gaze tracking; Image recommendation; Image similarity; Implicit informations; Information concerning; Products and services; Visual Attention; Eye tracking",Article,"Final","",Scopus,2-s2.0-85056106595
"Song G., Zheng J., Cai J., Zhang J., Cham T.-J., Fuchs H.","57204979749;57194827755;7403153287;25423412200;7003391055;7201917402;","Real-time 3D face-eye performance capture of a person wearing vr headset",2018,"MM 2018 - Proceedings of the 2018 ACM Multimedia Conference",,,,"923","931",,2,"10.1145/3240508.3240570","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058230557&doi=10.1145%2f3240508.3240570&partnerID=40&md5=a7f50144a948cc2cd3d8ce9c2f4638e5","Nanyang Technological University, China; University of Science and Technology of China, China; University of North Carolina, Chapel Hill, United States","Song, G., Nanyang Technological University, China; Zheng, J., Nanyang Technological University, China; Cai, J., Nanyang Technological University, China; Zhang, J., University of Science and Technology of China, China; Cham, T.-J., Nanyang Technological University, China; Fuchs, H., University of North Carolina, Chapel Hill, United States","Teleconference or telepresence based on virtual reality (VR) head-mount display (HMD) device is a very interesting and promising application since HMD can provide immersive feelings for users. However, in order to facilitate face-to-face communications for HMD users, real-time 3D facial performance capture of a person wearing HMD is needed, which is a very challenging task due to the large occlusion caused by HMD. The existing limited solutions are very complex either in setting or in approach as well as lacking the performance capture of 3D eye gaze movement. In this paper, we propose a convolutional neural network (CNN) based solution for real-time 3D face-eye performance capture of HMD users without complex modification to devices. To address the issue of lacking training data, we generate massive pairs of HMD face-label dataset by data synthesis as well as collecting VR-IR eye dataset from multiple subjects. Then, we train a dense-fitting network for facial region and an eye gaze network to regress 3D eye model parameters. Extensive experimental results demonstrate that our system can efficiently and effectively produce in real time a vivid personalized 3D avatar with the correct identity, pose, expression and eye motion corresponding to the HMD user. © 2018 Association for Computing Machinery.","3D facial reconstruction; Gaze estimation; HMDs","Complex networks; Eye movements; Neural networks; Three dimensional computer graphics; Virtual reality; Visual communication; Wear of materials; 3d facial reconstruction; Convolutional Neural Networks (CNN); Data synthesis; Face-to-face communications; Gaze estimation; Head-mount displays; Large occlusion; Performance capture; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85058230557
"Li T., Zhou X.","55837695100;56271430300;","Battery-free eye tracker on glasses",2018,"Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM",,,,"67","82",,10,"10.1145/3241539.3241578","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056897666&doi=10.1145%2f3241539.3241578&partnerID=40&md5=a1d83adff6477b46b9cd8805df4196e2","Department of Computer Science, Dartmouth College, Hanover, NH, United States","Li, T., Department of Computer Science, Dartmouth College, Hanover, NH, United States; Zhou, X., Department of Computer Science, Dartmouth College, Hanover, NH, United States","This paper presents a battery-free wearable eye tracker that tracks both the 2D position and diameter of a pupil based on its light absorption property. With a few near-infrared (NIR) lights and photodiodes around the eye, NIR lights sequentially illuminate the eye from various directions while photodiodes sense spatial patterns of reflected light, which are used to infer pupil's position and diameter on the fly via a lightweight inference algorithm. The system also exploits characteristics of different eye movement stages and adjusts its sensing and computation accordingly for further energy savings. A prototype is built with off-the-shelf hardware components and integrated into a regular pair of glasses. Experiments with 22 participants showthat the system achieves 0.8-mm mean error in tracking pupil position (2.3 mm at the 95th percentile) and 0.3-mm mean error in tracking pupil diameter (0.9 mm at the 95th percentile) at 120-Hz output frame rate, consuming 395μW mean power supplied by two small, thin solar cells on glasses side arms. © 2018 Association for Computing Machinery.",,"Electric batteries; Energy conservation; Eye movements; Glass; Inference engines; Infrared devices; Light absorption; Mobile computing; Photodiodes; Absorption property; Inference algorithm; Near infrared light; Off-the-shelf hardwares; Pupil diameter; Reflected light; Regular pairs; Spatial patterns; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85056897666
"Ishibashi T., Sugano Y., Matsushita Y.","57204728003;7005470045;35956654700;","Gaze-guided Image Classification for Reflecting Perceptual Class Ambiguity",2018,"UIST 2018 Adjunct - Adjunct Publication of the 31st Annual ACM Symposium on User Interface Software and Technology",,,,"26","28",,,"10.1145/3266037.3266090","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056867038&doi=10.1145%2f3266037.3266090&partnerID=40&md5=8e980b6228df1d2185aa748bfa89883c","Graduate School of Information Science and Technology, Osaka University, Japan","Ishibashi, T., Graduate School of Information Science and Technology, Osaka University, Japan; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Japan; Matsushita, Y., Graduate School of Information Science and Technology, Osaka University, Japan","Despite advances in machine learning and deep neural networks, there is still a huge gap between machine and human image understanding. One of the causes is the annotation process used to label training images. In most image categorization tasks, there is a fundamental ambiguity between some image categories and the underlying class probability differs from very obvious cases to ambiguous ones. However, current machine learning systems and applications usually work with discrete annotation processes and the training labels do not reflect this ambiguity. To address this issue, we propose an new image annotation framework where labeling incorporates human gaze behavior. In this framework, gaze behavior is used to predict image labeling difficulty. The image classifier is then trained with sample weights defined by the predicted difficulty. We demonstrate our approach's effectiveness on four-class image classification tasks. © 2018 Copyright held by the owner/author(s).","Computer vision; Eye tracking; Machine learning","Artificial intelligence; Behavioral research; Computer vision; Deep neural networks; Eye tracking; Learning systems; User interfaces; Class probabilities; Gaze behavior; Human image understanding; Image Categorization; Image Classifiers; Image labeling; Training image; Image classification",Conference Paper,"Final","",Scopus,2-s2.0-85056867038
"Hirzle T., Gugenheimer J., Geiselhart F., Bulling A., Rukzio E.","57203517985;55876769400;56940701200;6505807414;18233783900;","Towards a symbiotic human-machine depth sensor: Exploring 3D gaze for object reconstruction",2018,"UIST 2018 Adjunct - Adjunct Publication of the 31st Annual ACM Symposium on User Interface Software and Technology",,,,"114","116",,4,"10.1145/3266037.3266119","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056818060&doi=10.1145%2f3266037.3266119&partnerID=40&md5=3924bece370197460581f4ae33633180","Institute of Media Informatics, Ulm University, Germany; Institute for Visualisation and Interactive Systems, University of Stuttgart, Germany","Hirzle, T., Institute of Media Informatics, Ulm University, Germany; Gugenheimer, J., Institute of Media Informatics, Ulm University, Germany; Geiselhart, F., Institute of Media Informatics, Ulm University, Germany; Bulling, A., Institute for Visualisation and Interactive Systems, University of Stuttgart, Germany; Rukzio, E., Institute of Media Informatics, Ulm University, Germany","Eye tracking is expected to become an integral part of future augmented reality (AR) head-mounted displays (HMDs) given that it can easily be integrated into existing hardware and provides a versatile interaction modality. To augment objects in the real world, AR HMDs require a three-dimensional understanding of the scene, which is currently solved using depth cameras. In this work we aim to explore how 3D gaze data can be used to enhance scene understanding for AR HMDs by envisioning a symbiotic human-machine depth camera, fusing depth data with 3D gaze information. We present a first proof of concept, exploring to what extend we are able to recognise what a user is looking at by plotting 3D gaze data. To measure 3D gaze, we implemented a vergence-based algorithm and built an eye tracking setup consisting of a Pupil Labs headset and an OptiTrack motion capture system, allowing us to measure 3D gaze inside a 50x50x50 cm volume. We show first 3D gaze plots of ""gazed-at"" objects and describe our vision of a symbiotic human-machine depth camera that combines a depth camera and human 3D gaze information. © 2018 Copyright held by the owner/author(s).","3D gaze; Eye-based interaction; Human-machine symbiosis","Augmented reality; Cameras; Helmet mounted displays; Image reconstruction; User interfaces; 3D gaze; Eye-based interaction; Head mounted displays; Human-machine; Motion capture system; Object reconstruction; Proof of concept; Scene understanding; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85056818060
"Herurkar D., Dengel A., Ishimaru S.","57205026556;6603764314;55876558900;","Poster: Combining software-based eye tracking and a wide-angle lens for sneaking detection",2018,"UbiComp/ISWC 2018 - Adjunct Proceedings of the 2018 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2018 ACM International Symposium on Wearable Computers",,,,"54","57",,,"10.1145/3267305.3267675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058301836&doi=10.1145%2f3267305.3267675&partnerID=40&md5=f60bf3385c3001e314707ee84a306a32","University of Kaiserslautern, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany","Herurkar, D., University of Kaiserslautern, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Dengel, A., University of Kaiserslautern, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Ishimaru, S., University of Kaiserslautern, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany","This paper proposes Sneaking Detector, a system which recognizes sneaking on a laptop screen by other people and alerts the owner through several interventions. We utilize a pre-trained deep learning network to estimate eye gaze of sneakers captured by a front-facing camera. Since most of the cameras equipped on laptop computers cannot cover a wide enough range, a commercial wide-angle lens attachment and an image processing are applied in our system. On the dataset involving nine participants following four experiments, it has been realized that our system can estimate the horizontal eye gaze and recognizes whether a sneaker is looking at a screen or not with 78% accuracy. © Copyright held by the owner/author(s).","Deep learning; Eye tracking; Image processing; Sneaking","Cameras; Deep learning; Image processing; Laptop computers; Ubiquitous computing; Wearable computers; Eye-gaze; Learning network; Sneaking; Wide-angle lens; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85058301836
"Hosseinkhani J., Joslin C.","55675854500;57200744209;","Significance of Bottom-Up Attributes in Video Saliency Detection without Cognitive Bias",2018,"Proceedings of 2018 IEEE 17th International Conference on Cognitive Informatics and Cognitive Computing, ICCI*CC 2018",,,"8482037","606","613",,2,"10.1109/ICCI-CC.2018.8482037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056484124&doi=10.1109%2fICCI-CC.2018.8482037&partnerID=40&md5=b1b7a737020225f14a1749254d043850","Dept. Systems and Computer Engineering, Carleton University, Ottawa, Canada; School of Information Technology, Carleton University, Ottawa, Canada","Hosseinkhani, J., Dept. Systems and Computer Engineering, Carleton University, Ottawa, Canada; Joslin, C., School of Information Technology, Carleton University, Ottawa, Canada","Saliency in an image or video is the region of interest that stands out relative to its neighbors and consequently attracts more human attention. To determine the salient areas within a scene, visual importance and distinctiveness of the regions must be measured. A key factor in designing saliency detection algorithms for videos is to understand how different visual cues affect the human perceptual and visual system. To this end, we investigated the bottom-up features including color, texture, and motion in video sequences for both one-by-one and combined scenarios to provide a ranking system stating the most dominant circumstances for each feature individually and in combination with other features as well. In this work, we only considered the individual features and various visual saliency attributes investigated under conditions in which we had no cognitive bias. Human cognition refers to a systematic pattern of perceptual and rational judgements and decision-making actions. Since computers do not typically have this ability, we tried to minimize this bias in the design of our experiment. First, we modelled our test data as 2D images and videos in a virtual environment to avoid any cognitive bias. Then, we performed an experiment using human subjects to determine which colors, textures, motion directions, and motion speeds attract human attention more. The proposed ranking system of salient visual attention stimuli was achieved using an eye tracking procedure. This work provides a benchmark to specify the most salient stimulus with comprehensive information. © 2018 IEEE.","Bottom-up Features; Cognitive Bias; Human Visual System; Saliency Detection; Semantic Analysis; Visual Attention Model","Decision making; Eye tracking; Image segmentation; Object recognition; Semantics; Virtual reality; Bottom up; Cognitive bias; Human Visual System; Saliency detection; Semantic analysis; Visual attention model; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85056484124
"Guo W., Wang J.","55574221946;56962595200;","Understanding mobile reading via camera based gaze tracking and kinematic touch modeling",2018,"ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction",,,,"288","297",,3,"10.1145/3242969.3243011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056637949&doi=10.1145%2f3242969.3243011&partnerID=40&md5=e87291a8dc7e898111cbb3671dc74269","Google Cloud AI, Beijing, China; University of Pittsburgh, Pittsburgh, PA, United States","Guo, W., University of Pittsburgh, Pittsburgh, PA, United States; Wang, J., Google Cloud AI, Beijing, China","Despite the ubiquity and rapid growth of mobile reading activities, researchers and practitioners today either rely on coarse-grained metrics such as click-through-rate (CTR) and dwell time, or expensive equipment such as gaze trackers to understand users' reading behavior on mobile devices. We present Lepton, an intelligent mobile reading system and a set of dual-channel sensing algorithms to achieve scalable and fine-grained understanding of users' reading behaviors, comprehension, and engagement on unmodified smartphones. Lepton tracks the periodic lateral patterns, i.e. saccade, of users' eye gaze via the front camera, and infers their muscle stiffness during text scrolling via a Mass-Spring-Damper (MSD) based kinematic model from touch events. Through a 25-participant study, we found that both the periodic saccade patterns and muscle stiffness signals captured by Lepton can be used as expressive features to infer users' comprehension and engagement in mobile reading. Overall, our new signals lead to significantly higher performances in predicting users' comprehension (correlation: 0.36 vs. 0.29), concentration (0.36 vs. 0.16), confidence (0.5 vs. 0.47), and engagement (0.34 vs. 0.16) than using traditional dwell-time based features via a user-independent model. © 2018 Association for Computing Machinery.","Gaze tracking; Intelligent user interfaces; Machine learning; Mass-Spring-Damper; Mobile computing; Text reading","Cameras; Elementary particles; Interactive computer systems; Kinematics; Learning systems; Mobile computing; Mobile telecommunication systems; Muscle; Stiffness; User interfaces; Click-through rate; Expensive equipments; Intelligent User Interfaces; Mass spring damper; Muscle stiffness; Reading activities; Text reading; User independents; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85056637949
"Li T., Zhou X.","55837695100;56271430300;","Battery-free eye tracker on glasses",2018,"Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM",,,,"27","29",,4,"10.1145/3264877.3264885","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061499320&doi=10.1145%2f3264877.3264885&partnerID=40&md5=f9b5673510ef827bccfd3b949a083074","Department of Computer Science, Dartmouth College, Hanover, NH, United States","Li, T., Department of Computer Science, Dartmouth College, Hanover, NH, United States; Zhou, X., Department of Computer Science, Dartmouth College, Hanover, NH, United States","We propose the design of a battery-free wearable eye tracker that achieves sub-millimeter tracking accuracy at high track ing rates. It tracks pupil’s 2D position based on pupil’s light absorption effect. With a few near-infrared (NIR) lights and photodiodes around the eye, NIR lights sequentially illu minate the eye from various directions while photodiodes sense spatial patterns of reflected light, which are used to infer pupil positions on the fly through a lightweight infer ence algorithm. The system also exploits characteristics of different eye movement stages and adjusts its sensing and computation accordingly for further energy savings. We have built a prototype with off-the-shelf hardware components and integrated it into a regular pair of glasses. Experiments with ten participants show that the system achieves 0.9-mm mean tracking accuracy (2.4 mm at the 95th percentile) at 120-Hz output frame rate, consuming 395µW mean power supplied by two small, thin solar cells on glasses side arms. © 2018 Association for Computing Machinery.","Gaze tracking; Infrared light sensing; Low power sensing","Electric batteries; Energy conservation; Eye movements; Glass; Infrared devices; Light absorption; Photodiodes; Students; Absorption effects; Gaze tracking; Infrared light; Low-power sensing; Near infrared light; Off-the-shelf hardwares; Spatial patterns; Tracking accuracy; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85061499320
"Banitalebi-Dehkordi A., Nasiopoulos P.","55760854100;6701848250;","Saliency inspired quality assessment of stereoscopic 3D video",2018,"Multimedia Tools and Applications","77","19",,"26055","26082",,3,"10.1007/s11042-018-5837-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052690051&doi=10.1007%2fs11042-018-5837-4&partnerID=40&md5=86d522e443358f46847622d805ce5998","Electrical Engineering Department and ICICS, University of British Columbia, Vancouver, BC  V6T 1Z4, Canada","Banitalebi-Dehkordi, A., Electrical Engineering Department and ICICS, University of British Columbia, Vancouver, BC  V6T 1Z4, Canada; Nasiopoulos, P., Electrical Engineering Department and ICICS, University of British Columbia, Vancouver, BC  V6T 1Z4, Canada","To study the visual attentional behavior of Human Visual System (HVS) on 3D content, eye tracking experiments are performed and Visual Attention Models (VAMs) are designed. One of the main applications of these VAMs is in quality assessment of 3D video. The usage of 2D VAMs in designing 2D quality metrics is already well explored. This paper investigates the added value of incorporating 3D VAMs into Full-Reference (FR) and No-Reference (NR) quality assessment metrics for stereoscopic 3D video. To this end, state-of-the-art 3D VAMs are integrated to quality assessment pipeline of various existing FR and NR stereoscopic video quality metrics. Performance evaluations using a large scale database of stereoscopic videos with various types of distortions demonstrated that using saliency maps generally improves the performance of the quality assessment task for stereoscopic video. However, depending on the type of distortion, utilized metric, and VAM, the amount of improvement will change. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","3D video; Quality assessment; Saliency prediction; Stereoscopic video; Visual attention modeling","Behavioral research; Eye tracking; Image quality; Stereo image processing; 3-D videos; Human visual system (HVS); Large-scale database; Performance evaluations; Quality assessment; State of the art; Stereoscopic video; Visual attention model; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85052690051
"Nai K., Li Z., Li G., Wang S.","57192112808;57020064900;57202613103;57202607734;","Robust Object Tracking via Local Sparse Appearance Model",2018,"IEEE Transactions on Image Processing","27","10",,"4958","4970",,26,"10.1109/TIP.2018.2848465","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048880393&doi=10.1109%2fTIP.2018.2848465&partnerID=40&md5=a94c02c7a7a2535111bcfb5f0d91a50e","School of Information Science and Engineering, Hunan University, Changsha, 410082, China","Nai, K., School of Information Science and Engineering, Hunan University, Changsha, 410082, China; Li, Z., School of Information Science and Engineering, Hunan University, Changsha, 410082, China; Li, G., School of Information Science and Engineering, Hunan University, Changsha, 410082, China; Wang, S., School of Information Science and Engineering, Hunan University, Changsha, 410082, China","In this paper, we propose a novel local sparse representation-based tracking framework for visual tracking. To deeply mine the appearance characteristics of different local patches, the proposed method divides all local patches of a candidate target into three categories, which are stable patches, valid patches, and invalid patches. All these patches are assigned different weights to consider the different importance of the local patches. For stable patches, we introduce a local sparse score to identify them, and discriminative local sparse coding is developed to decrease the weights of background patches among the stable patches. For valid patches and invalid patches, we adopt local linear regression to distinguish the former from the latter. Furthermore, we propose a weight shrinkage method to determine weights for different valid patches to make our patch weight computation more reasonable. Experimental results on public tracking benchmarks with challenging sequences demonstrate that the proposed method performs favorably against other state-of-the-art tracking methods. © 2018 IEEE.","discriminative local sparse coding (DLSC); local linear regression (LLR); local sparse score; Visual tracking; weight shrinkage method","Codes (symbols); Lunar surface analysis; Shrinkage; Local linear regression; Local sparse coding; local sparse score; Shrinkage methods; Visual Tracking; Tracking (position); article; eye tracking; linear regression analysis",Article,"Final","",Scopus,2-s2.0-85048880393
"Wyder S., Cattin P.C.","57190963125;6506422723;","Eye tracker accuracy: quantitative evaluation of the invisible eye center location",2018,"International Journal of Computer Assisted Radiology and Surgery","13","10",,"1651","1660",,1,"10.1007/s11548-018-1808-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048687400&doi=10.1007%2fs11548-018-1808-5&partnerID=40&md5=1feb3ae7b26db525ecfa89d009cf3a0f","Department of Biomedical Engineering, University of Basel, Gewerbestrasse 14, Allschwil, 4123, Switzerland","Wyder, S., Department of Biomedical Engineering, University of Basel, Gewerbestrasse 14, Allschwil, 4123, Switzerland; Cattin, P.C., Department of Biomedical Engineering, University of Basel, Gewerbestrasse 14, Allschwil, 4123, Switzerland","Purpose: We present a new method to evaluate the accuracy of an eye tracker-based eye localization system. Measuring the accuracy of an eye tracker’s primary intention, the estimated point of gaze, is usually done with volunteers and a set of fixation points used as ground truth. However, verifying the accuracy of the location estimate of a volunteer’s eye center in 3D space is not easily possible. This is because the eye center, the center of corneal curvature, is an intangible point. Methods: We evaluate the eye location accuracy by using an eye phantom instead of eyes of volunteers. For this, we developed a testing stage with a realistic artificial eye and a corresponding kinematic model, which we trained with μCT data. This enables us to precisely evaluate the eye location estimate of an eye tracker. Results: We show that the proposed testing stage with the corresponding kinematic model is suitable for such a validation. Further, we evaluate a particular eye tracker-based navigation system and show that this system is able to successfully determine the eye center with a mean accuracy of 0.68 mm. Conclusion: We show the suitability of the evaluated eye tracker for eye interventions, using the proposed testing stage and the corresponding kinematic model. The results further enable specific enhancements of the navigation system to potentially get even better results. © 2018, CARS.","3D eye tracking; Eye tracking evaluation; Invisible eye center location; Kinematic model; Proton therapy; Testing stage","accuracy; Article; calibration; cornea curvature; eye position; eye tracking; image processing; kinematics; micro-computed tomography; priority journal; three dimensional printing; visual orientation; artificial eye; biomechanics; devices; eye movement; human; imaging phantom; oculography; Biomechanical Phenomena; Eye Movement Measurements; Eye Movements; Eye, Artificial; Humans; Phantoms, Imaging; X-Ray Microtomography",Article,"Final","",Scopus,2-s2.0-85048687400
"Xiao F., Huang K., Qiu Y., Shen H.","57206593016;54581052800;57196198075;13309317500;","Accurate iris center localization method using facial landmark, snakuscule, circle fitting and binary connected component",2018,"Multimedia Tools and Applications","77","19",,"25333","25353",,8,"10.1007/s11042-018-5787-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042348432&doi=10.1007%2fs11042-018-5787-x&partnerID=40&md5=0de6e43eb531ff84fbe273f7237800c6","Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China","Xiao, F., Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China; Huang, K., Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China; Qiu, Y., Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China; Shen, H., Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China","Iris centers have been widely used in machine vision for face matching, gaze estimation, etc. However, in low resolution eye images, the iris and its surrounding region present a variety of appearance characteristics, which make it difficult to accurately locate the iris center. In this paper, we propose a robust, accurate and real-time iris center localization method by combining the facial landmark, snakuscule, circle fitting and binary connected component. Facial landmarks are used to extract an accurate eye Region of Interest (ROI). Thereafter, a fixed size circle-based active contour snakuscule is used to detect the iris center. Based on the snakuscule center and inner radius, a novel method is proposed to extract accurate iris edges for circle fitting. In addition, the quality of the detected iris center is evaluated by a circle-binary quality evaluation method. Binary connected component method is used to improve the accuracies in those unqualified images. The proposed method is tested on three publicly available databases BioID, GI4E and Talking Face Video. The result shows that it could achieve an accuracy of 94.35% on the BioID database when the normalized error is smaller than 0.05, which outperforms all state-of-the-art methods. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Binary connected component; Circle fitting; Facial landmark; Iris centers; Snakuscule","Image enhancement; Image segmentation; Circle fitting; Connected component; Facial landmark; Iris centers; Snakuscule; Quality control",Article,"Final","",Scopus,2-s2.0-85042348432
"Wan Z., Wang X., Yin L., Zhou K.","56105053500;55736887200;57201689515;57194266680;","A method of free-space point-of-regard estimation based on 3D eye model and stereo vision",2018,"Applied Sciences (Switzerland)","8","10","1769","","",,,"10.3390/app8101769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054065503&doi=10.3390%2fapp8101769&partnerID=40&md5=73c68a6f9096eb45a1fc9b7c0d1af322","State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, No. 92Weijin Road, Nankai District, Tianjin, 300072, China; MOEMS Education Ministry Key Laboratory, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin, 300072, China","Wan, Z., State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, No. 92Weijin Road, Nankai District, Tianjin, 300072, China, MOEMS Education Ministry Key Laboratory, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin, 300072, China; Wang, X., State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, No. 92Weijin Road, Nankai District, Tianjin, 300072, China, MOEMS Education Ministry Key Laboratory, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin, 300072, China; Yin, L., State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, No. 92Weijin Road, Nankai District, Tianjin, 300072, China, MOEMS Education Ministry Key Laboratory, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin, 300072, China; Zhou, K., State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, No. 92Weijin Road, Nankai District, Tianjin, 300072, China, MOEMS Education Ministry Key Laboratory, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin, 300072, China","This paper proposes a 3D point-of-regard estimation method based on 3D eye model and a corresponding head-mounted gaze tracking device. Firstly, a head-mounted gaze tracking system is given. The gaze tracking device uses two pairs of stereo cameras to capture the left and right eye images, respectively, and then sets a pair of scene cameras to capture the scene images. Secondly, a 3D eye model and the calibration process are established. Common eye features are used to estimate the eye model parameters. Thirdly, a 3D point-of-regard estimation algorithm is proposed. Three main parts of this method are summarized as follows: (1) the spatial coordinates of the eye features are directly calculated by using stereo cameras; (2) the pupil center normal is used to the initial value for the estimation of optical axis; (3) a pair of scene cameras are used to solve the actual position of the objects being watched in the calibration process, and the calibration for the proposed eye model does not need the assistance of the light source. Experimental results show that the proposed method can output the coordinates of 3D point-of-regard more accurately. © 2018 by the authors.","3D point-of-regard; Binocular eye model; Gaze estimation; Stereo vision",,Article,"Final","",Scopus,2-s2.0-85054065503
"Beelders T.R., Du Plessis J.-P.L.","23395659400;57038251500;","Reading usability of ereaders and ebooks for Information technology students",2018,"ACM International Conference Proceeding Series",,,,"205","214",,,"10.1145/3278681.3278706","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058654612&doi=10.1145%2f3278681.3278706&partnerID=40&md5=25da4d570835ee0d67b0f90e743bb378","University of the Free State, Department of Computer Science and Informatics, Bloemfontein, South Africa","Beelders, T.R., University of the Free State, Department of Computer Science and Informatics, Bloemfontein, South Africa; Du Plessis, J.-P.L., University of the Free State, Department of Computer Science and Informatics, Bloemfontein, South Africa","Electronic reading devices and books present a convenient, cheap solution to university students who are required to buy expensive textbooks. However, these devices may not be suitable for reading academic texts, particularly in specialized fields such as Information Technology. This study investigated whether reading IT academic texts are influenced by the presentation medium, namely a Kindle eReader, an iPad, an Android tablet, a PC and paper. Eye gaze is an established means of detecting whether reading difficulty is being experienced. Reading speed on the text portions was not significantly different between devices. However, reading speed on the iPad was significantly faster than on the Kindle. paper and PC for the entire text. No significant difference was found in fixation durations when reading code, but there was a significant difference when reading text only, where, fixations were, on average, significantly longer on the PC than the other devices. When reading code, the PC had significantly fewer fixations and visits but visits were longer when reading text. The tablet, had significantly more fixations and longer visits than the iPad, Kindle and paper and the Kindle had significantly fewer fixations than the paper. Reactions to all the devices were very positive in terms of ease of use, readability and appearance. The appearance was also positively experienced and navigation was found to be easy. © 2018 Association for Computing Machinery.","eBooks; EReaders; Eye tracking; Reading; Region of interest; Usability","Electronic publishing; Engineers; Hand held computers; Image segmentation; E-books; EReaders; Reading; Region of interest; Usability; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85058654612
"Feng P., Xu C., Zhao Z., Liu F., Guo J., Yuan C., Wang T., Duan K.","57202074446;55878062700;56969972300;57091781700;57199055234;56834954400;55833111900;57190139449;","A deep features based generative model for visual tracking",2018,"Neurocomputing","308",,,"245","254",,7,"10.1016/j.neucom.2018.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047096128&doi=10.1016%2fj.neucom.2018.05.007&partnerID=40&md5=05a85b02725290ab5eed8ead3386fa75","Huazhong University of Science and Technology School Hospital, Wuhan, 430074, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China","Feng, P., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Xu, C., School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China; Zhao, Z., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Liu, F., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Guo, J., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Yuan, C., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Wang, T., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Duan, K., Huazhong University of Science and Technology School Hospital, Wuhan, 430074, China","In this work, we propose a novel visual tracking algorithm based on a framework of generative model. In order to make the algorithm robust to various challenging appearance changes, we adopt the powerful deep features in the description of tracking object appearance. The features are extracted from a Convolutional Neural Network (CNN), which is a modified one based on the VGG-M nets but constructed with fewer convolution layers and sequences exclusively full connection layers. In the pretraining process, we add a special convolution layer called coefficients layer before the full connection layers. In the tracking process after the network being pretrained, we remove the coefficients layer and just update the full connection layers conditionally. To decide the new target's positions, we compute the compositive similarity scores containing three kinds of similarities with different weights. The first kind is similarities between candidates and the target in the first frame, and the second kind is between candidates and tracking results in the last frame. The third kind is related to the important object appearance variations in the tracking process. We design a simple mechanism to produce a collection to record those historical templates when the object appearance changed largely. With similarities between candidates and the historical templates, the drift problem can be alleviated to some extent, because similar historical appearances sometimes appear repeatedly and the recorded historical templates can provide important information. We use the outputs of the convolution part before the full connection layers as features and weight them with the coefficients layer's filter weights to compute all similarities. Finally, candidates with the highest scores will be regarded as new targets in the current frame. The evaluated results on CVPR2013 Online Object Tracking Benchmark show that our algorithm can achieve outstanding performance compared with state-of-the-art trackers. © 2018 Elsevier B.V.","Deep features; Generative model; Visual tracking","Benchmarking; Neural networks; Tracking (position); Convolutional Neural Networks (CNN); Deep features; Generative model; Object appearance; Online object tracking; Similarity scores; Visual Tracking; Visual tracking algorithm; Convolution; algorithm; Article; artificial neural network; classification; classifier; conceptual framework; eye tracking; information processing; priority journal; scoring system; support vector machine",Article,"Final","",Scopus,2-s2.0-85047096128
"Wu L., Xu M., Zhu G., Wang J., Rao T.","57191158052;55643281400;56013201400;16069376500;57189580521;","Appearance features in Encoding Color Space for visual surveillance",2018,"Neurocomputing","308",,,"21","30",,4,"10.1016/j.neucom.2018.04.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047062691&doi=10.1016%2fj.neucom.2018.04.019&partnerID=40&md5=a304863ce483ab6f9986a6d3b192ea35","GBDTC, Faculty of Engineering and IT, University of Technology Sydney, Australia; Research Center for Brain-inspired Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China","Wu, L., GBDTC, Faculty of Engineering and IT, University of Technology Sydney, Australia; Xu, M., GBDTC, Faculty of Engineering and IT, University of Technology Sydney, Australia; Zhu, G., Research Center for Brain-inspired Intelligence, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; Wang, J., National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; Rao, T., GBDTC, Faculty of Engineering and IT, University of Technology Sydney, Australia","Person re-identification and visual tracking are two important tasks in video surveillance. Many works have been done on appearance modeling for these two tasks. However, existing feature descriptors are mainly constructed on three-channel color spaces, such like RGB, HSV and XYZ. These color spaces somehow enable meaningful representation for color, yet may lack distinctiveness for real-world tasks. In this paper, we propose a multi-channel Encoding Color Space (ECS), and consider the color distinction with the design of image feature descriptor. In order to overcome the illumination variation and shape deformation, we design features on the basis of the Encoding Color Space and Histogram of Oriented Gradient (HOG), which enables rich color-gradient characteristics. Additionally, we extract Second Order Histogram (SOH) on the descriptor constructed to capture abstract information with layout constrains. Exhaustive experiments are performed on datasets VIPeR, CAVIAR, CUHK01 and Visual Tracking Benchmark. Experimental results on these datasets show that our feature descriptors could achieve promising performance. © 2018 Elsevier B.V.","Encoding color space; HOG; Person re-identification; Tracking","Encoding (symbols); Graphic methods; Security systems; Signal encoding; Surface discharges; Appearance modeling; Color space; Feature descriptors; Histogram of oriented gradients (HOG); Illumination variation; Person re identifications; Video surveillance; Visual surveillance; Color; Article; color; Encoding Color Space; eye tracking; histogram; Histogram of Oriented Gradient; HSV color space; human; identity recognition; illumination; image analysis; information processing; mathematical model; priority journal; RGB color space; Second Order Histogram; space; videorecording; XYZ color space",Article,"Final","",Scopus,2-s2.0-85047062691
"Díaz-Vilariño L., González-De Santos L., Verbree E., Michailidou G., Zlatanova S.","57204491819;57202647606;56119056300;57202942471;15830792100;","From point clouds to 3D isovists in indoor environments",2018,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","4",,"225","232",,2,"10.5194/isprs-archives-XLII-4-149-2018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056198160&doi=10.5194%2fisprs-archives-XLII-4-149-2018&partnerID=40&md5=c1466db0b4df30f4c557c76b89393f6a","Applied Geotechnologies Group, Dept. of Natural Resources and Environmental Engineering, University of Vigo, Spain; GIS Technology, OTB Research Institute for the Built Environment, Delft University of Technology, Julianalaan 134, Delft, Netherlands; University of New South Wales, Built Environment, Red Centre, Kensington Campus, Sydney, NSW, Australia","Díaz-Vilariño, L., Applied Geotechnologies Group, Dept. of Natural Resources and Environmental Engineering, University of Vigo, Spain, GIS Technology, OTB Research Institute for the Built Environment, Delft University of Technology, Julianalaan 134, Delft, Netherlands; González-De Santos, L., Applied Geotechnologies Group, Dept. of Natural Resources and Environmental Engineering, University of Vigo, Spain; Verbree, E., GIS Technology, OTB Research Institute for the Built Environment, Delft University of Technology, Julianalaan 134, Delft, Netherlands; Michailidou, G., GIS Technology, OTB Research Institute for the Built Environment, Delft University of Technology, Julianalaan 134, Delft, Netherlands; Zlatanova, S., University of New South Wales, Built Environment, Red Centre, Kensington Campus, Sydney, NSW, Australia","Visibility is a common measure to describe the spatial properties of an environment related to the spatial behaviour. Isovists represent the space that can be seen from one observation point, and they are used to analyse the existence of obstacles affecting or blocking intervisibility in an area. Although point clouds depict the as-built reality in a very detailed and accurate way, literature addressing the analysis of visibility in 3D, and more specifically the usage of point clouds to visibility analysis, is rather limited. In this paper, a methodology to evaluate visibility from point clouds in indoor environments is proposed, resulting in the creation of 3D isovists. Point cloud is firstly discretized in a voxel-based structure and voxels are labelled into ‘exterior’, ‘occupied’, ‘visible’ and ‘occluded’ based on an occupancy followed by a visibility analysis performed from a ray-tracing algorithm. 3D Isovists are created from the boundary of visible voxels from an observer position and considering as input parameters the visual angle, maximum line of sight, and eye gaze direction. © Authors 2018.","Navigation; Obstacle detection; Path complexity; Spatial analysis; Visibility graphs","Communication channels (information theory); Navigation; Obstacle detectors; Ray tracing; Visibility; Indoor environment; Obstacle detection; Path complexity; Ray-tracing algorithm; Spatial analysis; Spatial properties; Visibility analysis; Visibility graphs; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85056198160
"Cantoni V., Lacovara T., Porta M., Wang H.","7004438014;57205687635;35100711800;57208732603;","A study on gaze-controlled pin input with biometric data analysis",2018,"ACM International Conference Proceeding Series",,,,"99","103",,6,"10.1145/3274005.3274029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061146323&doi=10.1145%2f3274005.3274029&partnerID=40&md5=5a271afa954f5ad32bc76ec6c9ab7b89","Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Via A. Ferrata 5, Pavia, 27100, Italy","Cantoni, V., Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Via A. Ferrata 5, Pavia, 27100, Italy; Lacovara, T., Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Via A. Ferrata 5, Pavia, 27100, Italy; Porta, M., Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Via A. Ferrata 5, Pavia, 27100, Italy; Wang, H., Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Via A. Ferrata 5, Pavia, 27100, Italy","Common methods for checking a user's identity (e.g., passwords) do not consider personal elements characterizing a subject. In this paper, we present a study on the exploitation of eye information for biometric purposes. Data is acquired when the user enters a PIN (Personal Identification Number) through the gaze, by means of an on-screen virtual numeric keypad. Both identification (i.e., the recognition of a subject in a group) and verification (i.e., the confirmation of an individual's claimed identity) are considered. Using machine learning algorithms, we performed two kinds of analysis, one for the entire PIN sequence and one for each key (i.e., digit) in the series. Overall, the achieved results can be considered satisfying in the context of “soft biometrics”, which does not require very high success rates and is meant to be used along with other identification or verification techniques-in our case, the PIN itself-as an additional security level. © 2018 Association for Computing Machinery. ACM","Eye tracking; Gaze communication; Soft biometrics","Biometrics; Learning algorithms; Machine learning; Network security; Biometric data; Gaze communications; Personal identification number; Security level; Soft biometrics; Verification techniques; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85061146323
"Hong S., Cheng H., Mao B.","57202045745;57204071120;35753612000;","Visual Saliency Detection Framework for 3D Environment using Virtual Reality Devices",2018,"Proceedings of the 2018 IEEE 22nd International Conference on Computer Supported Cooperative Work in Design, CSCWD 2018",,,"8465363","666","671",,,"10.1109/CSCWD.2018.8465363","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054391290&doi=10.1109%2fCSCWD.2018.8465363&partnerID=40&md5=731c9c6df49ae819b076891234c4b52b","College of Information Engineering, Collaborative Innovation Center for Modern Grain Circulation and Safety, Jiangsu Key Laboratory of Grain Big-data Mining, Nanjing University of Finance Economics, Nanjing, China","Hong, S., College of Information Engineering, Collaborative Innovation Center for Modern Grain Circulation and Safety, Jiangsu Key Laboratory of Grain Big-data Mining, Nanjing University of Finance Economics, Nanjing, China; Cheng, H., College of Information Engineering, Collaborative Innovation Center for Modern Grain Circulation and Safety, Jiangsu Key Laboratory of Grain Big-data Mining, Nanjing University of Finance Economics, Nanjing, China; Mao, B., College of Information Engineering, Collaborative Innovation Center for Modern Grain Circulation and Safety, Jiangsu Key Laboratory of Grain Big-data Mining, Nanjing University of Finance Economics, Nanjing, China","Visual saliency is essential for attention analysis and it is usually measured by eye tracking system, which is widely used in the fields of design evaluation, user's gaze and behavior analysis. This study combines virtual reality with eye tracking technology and applies HTC Vive VR helmet to simulate eye tracker for both two-dimensional and three-dimensional cases study. We collect user's gaze points data and draw the scatter diagram and the thermodynamic map to analyze the user's visual saliency in completely 3D environment. The quantitative error of user's visual saliency data in virtual reality environment is analyzed by setting the average offset distance and offset degree indexes. The experiment results indicate that the proposed framework can be used to detect the visual saliency in 3D environment accurately that is has practical significance to the application of user's visual saliency analysis in the virtual reality environment. © 2018 IEEE.","Average Offset Distance; Offset; Virtual Reality; Visual Saliency",,Conference Paper,"Final","",Scopus,2-s2.0-85054391290
"Geisler D., Fox D., Kasneci E.","57189847283;7402074129;56059892600;","Real-time 3D Glint Detection in Remote Eye Tracking Based on Bayesian Inference",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,,"8460800","7119","7126",,2,"10.1109/ICRA.2018.8460800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063155664&doi=10.1109%2fICRA.2018.8460800&partnerID=40&md5=85671ef423e7a1fa14d46221395e7ecb","Department of Perception Engineering, University of Tuebingen, Germany; Department of Computer Science Engineering, University of Washington, Seattle, United States","Geisler, D., Department of Perception Engineering, University of Tuebingen, Germany; Fox, D., Department of Computer Science Engineering, University of Washington, Seattle, United States; Kasneci, E., Department of Perception Engineering, University of Tuebingen, Germany","As human gaze provides information on our cognitive states, actions, and intentions, gaze-based interaction has the potential to enable a fluent and natural human-robot collaboration. In this work, we focus on reliable gaze estimation in remote eye tracking based on calibration-free methods. Although these methods work well in controlled settings, they fail when illumination conditions change or other objects induce noise. We propose a novel, adaptive method based on a probabilistic model, which reliably detects glints from stereo images and evaluate our method using a data set that contains different challenges with regarding to light and reflections. © 2018 IEEE.",,"Bayesian networks; Human robot interaction; Inference engines; Robotics; Stereo image processing; Adaptive methods; Bayesian inference; Calibration-free methods; Gaze estimation; Gaze-based interaction; Human-robot collaboration; Illumination conditions; Probabilistic modeling; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85063155664
"Carette R., Elbattah M., Dequen G., Guerin J.-L., Cilia F.","57200860085;57163770900;23396657900;57200857001;57200855464;","Visualization of eye-tracking patterns in autism spectrum disorder: Method and dataset",2018,"2018 13th International Conference on Digital Information Management, ICDIM 2018",,,"8846967","248","253",,5,"10.1109/ICDIM.2018.8846967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064701723&doi=10.1109%2fICDIM.2018.8846967&partnerID=40&md5=0973fc40cb774c9f8cf2dc0e496bced6","Laboratoire Modélisation Information et Systèmes (MIS), Université de Picardie Jules Verne, Amiens, France; Laboratoire CRP-CPO, Université de Picardie Jules Verne, Amiens, France","Carette, R., Laboratoire Modélisation Information et Systèmes (MIS), Université de Picardie Jules Verne, Amiens, France; Elbattah, M., Laboratoire Modélisation Information et Systèmes (MIS), Université de Picardie Jules Verne, Amiens, France; Dequen, G., Laboratoire Modélisation Information et Systèmes (MIS), Université de Picardie Jules Verne, Amiens, France; Guerin, J.-L., Laboratoire Modélisation Information et Systèmes (MIS), Université de Picardie Jules Verne, Amiens, France; Cilia, F., Laboratoire CRP-CPO, Université de Picardie Jules Verne, Amiens, France","Autism spectrum disorder (ASD) is a lifelong condition generally characterized by social and communication impairments. One of the characteristic hallmarks of ASD is the difficulty of making or maintaining eye contact. In this respect, the eye-tracking technology has come into prominence to support the study and analysis of autism. This paper develops a methodology to visualize the eye-tracking patterns of ASD-diagnosed individuals with particular focus on children at early stages of development. The key idea is to transform the dynamics of eye motion into a visual representation, and hence diagnosis-related tasks could be approached using image-based techniques. The visualizations produced are made publicly available in an image dataset to be used by other studies aiming to experiment the potentials of eye-tracking within the ASD context. It is believed that the dataset can allow for developing further useful applications or discovering interesting insights using Machine Learning or data mining techniques. © 2018 IEEE.","Autism Spectrum Disorder; Eye-Tracking; Machine Learning; Visualization","Data mining; Diseases; Flow visualization; Information management; Learning systems; Machine learning; Visualization; Autism spectrum disorders; Eye contact; Eye tracking technologies; Image datasets; Image-based techniques; Visual representations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85064701723
"Koskela M.K., Immonen K.V., Viitanen T.T., Jääskeläinen P.O., Multanen J.I., Takala J.H.","57162886900;57194785625;36678169200;14056269000;57188831090;7103084368;","Instantaneous foveated preview for progressive Monte Carlo rendering",2018,"Computational Visual Media","4","3",,"267","276",,,"10.1007/s41095-018-0113-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053369380&doi=10.1007%2fs41095-018-0113-0&partnerID=40&md5=95b12dbc01066974ce5150dbe1d06e75","Tampere University of Technology, Tampere, 33720, Finland","Koskela, M.K., Tampere University of Technology, Tampere, 33720, Finland; Immonen, K.V., Tampere University of Technology, Tampere, 33720, Finland; Viitanen, T.T., Tampere University of Technology, Tampere, 33720, Finland; Jääskeläinen, P.O., Tampere University of Technology, Tampere, 33720, Finland; Multanen, J.I., Tampere University of Technology, Tampere, 33720, Finland; Takala, J.H., Tampere University of Technology, Tampere, 33720, Finland","Progressive rendering, for example Monte Carlo rendering of 360° content for virtual reality headsets, is a time-consuming task. If the 3D artist notices an error while previewing the rendering, they must return to editing mode, make the required changes, and restart rendering. We propose the use of eye-tracking-based optimization to significantly speed up previewing of the artist’s points of interest. The speed of the preview is further improved by sampling with a distribution that closely follows the experimentally measured visual acuity of the human eye, unlike the piecewise linear models used in previous work. In a comprehensive user study, the perceived convergence of our proposed method was 10 times faster than that of a conventional preview, and often appeared to be instantaneous. In addition, the participants rated the method to have only marginally more artifacts in areas where it had to start rendering from scratch, compared to conventional rendering methods that had already generated image content in those areas. © 2018, The Author(s).","360° content; foveated rendering; Monte Carlo rendering; preview; progressive rendering","Eye tracking; Monte Carlo methods; Piecewise linear techniques; Three dimensional computer graphics; Virtual reality; foveated rendering; Monte-Carlo rendering; Piecewise linear models; Points of interest; preview; Progressive rendering; Time-consuming tasks; Virtual-reality headsets; Rendering (computer graphics)",Article,"Final","",Scopus,2-s2.0-85053369380
"Hoonkwon K., Oh H.M., Kim M.Y.","57203978980;56028920100;56739349100;","Multiple RGB-D camera-based user intent position and object estimation",2018,"IEEE/ASME International Conference on Advanced Intelligent Mechatronics, AIM","2018-July",,"8452320","176","180",,1,"10.1109/AIM.2018.8452320","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053906824&doi=10.1109%2fAIM.2018.8452320&partnerID=40&md5=0e3122b18ebec242ff7ce21dfa9d760d","School of Electronics Engineering, IT College, Kyungpook National University, Research Center for Neurosurgical Robotic System, 80 Daehak-ro, Buk-gu, Daegu, 41566, South Korea","Hoonkwon, K., School of Electronics Engineering, IT College, Kyungpook National University, Research Center for Neurosurgical Robotic System, 80 Daehak-ro, Buk-gu, Daegu, 41566, South Korea; Oh, H.M., School of Electronics Engineering, IT College, Kyungpook National University, Research Center for Neurosurgical Robotic System, 80 Daehak-ro, Buk-gu, Daegu, 41566, South Korea; Kim, M.Y., School of Electronics Engineering, IT College, Kyungpook National University, Research Center for Neurosurgical Robotic System, 80 Daehak-ro, Buk-gu, Daegu, 41566, South Korea","Human gaze represents the area of interests of the person. By analyzing a time-series of these areas, it is possible to obtain user behavioral pattern that can be used in various fields. Well-known techniques for estimating human gaze are inconvenient because they require a wearable device, or the measurement area is relatively narrow. In this paper, a method to implement gaze estimation system using 3D view tracking with multi RGB-D camera is proposed. Surround 3D cameras are used to extract the region of interest of the user from 3D gaze estimation without wearable device in living space. To implement proposed method, first, 3D space mapping through multiple RGB-D camera calibration is performed. The resulting 3D map is the measurement area, which depends on the number and specifications of the RGB-D cameras used for this purpose. Then, when a person enters the 3D map, the face region is detected using both 2D/3D data, and 3D view tracking is implemented by detecting the gaze vector using the facial feature point and the head data center point extracted from the 3D map. Finally, when the gaze vector line intersects a specific point within the mapping space, the image coordinates corresponding to that point are extracted to implement user Intent position estimation. Applying object detection and classification algorithm to the extracted image can also estimate the intent object at that time. © 2018 IEEE.",,"Cameras; Data mining; Image segmentation; Intelligent mechatronics; Mapping; Object detection; Vector spaces; Wearable technology; Behavioral patterns; Classification algorithm; Facial feature points; Image coordinates; Object estimation; Position estimation; Region of interest; Wearable devices; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85053906824
"Ortega M., Stuerzlinger W.","15023288300;55902405400;","Pointing at Wiggle 3D Displays",2018,"25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings",,,"8447552","335","340",,1,"10.1109/VR.2018.8447552","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053859150&doi=10.1109%2fVR.2018.8447552&partnerID=40&md5=565bca2bb6121f2051f9435e5075270e","CNRS, Grenoble INP, University Grenoble Alpes, Grenoble, F-38000, France; Simon Fraser University, School of Interactive Arts + Technology, Vancouver, Canada","Ortega, M., CNRS, Grenoble INP, University Grenoble Alpes, Grenoble, F-38000, France; Stuerzlinger, W., Simon Fraser University, School of Interactive Arts + Technology, Vancouver, Canada","This paper presents two new pointing techniques for wiggle 3D displays, which present the 2D projection of 3D content with automatic (rotatory) motion parallax. Standard pointing at targets in wiggle 3D displays is challenging as the content is constantly in motion. The two pointing techniques presented here take advantage of the cursor's current position or the user's gaze direction for collocating the wiggle rotation center and potential targets. We evaluate the performance of the pointing techniques with a novel methodology that integrates 3D distractors into the ISO-9241-9 standard task. The experimental results indicate that the new techniques are significantly more efficient than standard pointing techniques in wiggle 3D displays. Given that we observed no performance variation for different targets, our new techniques seem to negate any interaction performance penalties of wiggle 3D displays. © 2018 IEEE.","3D Interaction Technique; Eye Tracking; H.5.2. Information Interfaces and Presentation: User Interfaces-Interaction styles; Input devices and strategies; Pointing","Eye tracking; Geometrical optics; User interfaces; Virtual reality; Wire pointing; 3D interaction technique; Input devices and strategies; Interaction styles; Novel methodology; Performance penalties; Performance variations; Potential targets; Rotation centers; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85053859150
"Chen S.-Y., Gao L., Lai Y.-K., Rosin P.L., Xia S.","57191372162;57191372232;14035747100;7004945120;7202893266;","Real-Time 3D Face Reconstruction and Gaze Tracking for Virtual Reality",2018,"25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings",,,"8446494","525","526",,5,"10.1109/VR.2018.8446494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053859106&doi=10.1109%2fVR.2018.8446494&partnerID=40&md5=165e1d79ef69970d879bbfa6f414daaf","Chinese Academy of Sciences, Beijing Key Laboratory of Mobile Computing and Pervasive Device Institute of Computing Technology, China; Cardiff University, School of Computer Science Informatics, United Kingdom","Chen, S.-Y., Chinese Academy of Sciences, Beijing Key Laboratory of Mobile Computing and Pervasive Device Institute of Computing Technology, China, Cardiff University, School of Computer Science Informatics, United Kingdom; Gao, L., Chinese Academy of Sciences, Beijing Key Laboratory of Mobile Computing and Pervasive Device Institute of Computing Technology, China; Lai, Y.-K., Chinese Academy of Sciences, Beijing Key Laboratory of Mobile Computing and Pervasive Device Institute of Computing Technology, China; Rosin, P.L., Cardiff University, School of Computer Science Informatics, United Kingdom; Xia, S., Chinese Academy of Sciences, Beijing Key Laboratory of Mobile Computing and Pervasive Device Institute of Computing Technology, China","With the rapid development of virtual reality (VR) technology, VR glasses, a.k.a. Head-Mounted Displays (HMDs) are widely available, allowing immersive 3D content to be viewed. A natural need for truly immersive VR is to allow bidirectional communication: The user should be able to interact with the virtual world using facial expressions and eye gaze, in addition to traditional means of interaction. Typical application scenarios include VR virtual conferencing and virtual roaming, where ideally users are able to see other users' expressions and have eye contact with them in the virtual world. Despite significant achievements in recent years for reconstruction of 3D faces from RGB or RGB-D images, it remains a challenge to reliably capture and reconstruct 3D facial expressions including eye gaze when the user is wearing VR glasses, because the majority of the face is occluded, especially those areas around the eyes which are essential for recognizing facial expressions and eye gaze. In this paper, we introduce a novel real-Time system that is able to capture and reconstruct 3D faces wearing HMDs and robustly recover eye gaze. We demonstrate the effectiveness of our system using live capture and more results are shown in the accompanying video. © 2018 IEEE.","Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality; Human-centered computing-Human computer interaction (HCI)-Interaction techniques-Gestural input","Glass; Helmet mounted displays; Human computer interaction; Image reconstruction; Interactive computer systems; Real time systems; User interfaces; Virtual reality; Wear of materials; 3-d facial expressions; 3D face reconstruction; Bi-directional communication; Facial Expressions; Graphics systems; Head mounted displays; Human Computer Interaction (HCI); Typical application; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85053859106
"Mei C., Zahed B.T., Mason L., Ouarles J.","56421487500;54928797700;39061658300;57203969899;","Towards Joint Attention Training for Children with ASD-a VR Game Approach and Eye Gaze Exploration",2018,"25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings",,,"8446242","289","296",,11,"10.1109/VR.2018.8446242","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053823448&doi=10.1109%2fVR.2018.8446242&partnerID=40&md5=6bcaca4c7c2a2696d1ba10180e2f1571","Kennesaw State Univ, United States","Mei, C., Kennesaw State Univ, United States; Zahed, B.T., Kennesaw State Univ, United States; Mason, L., Kennesaw State Univ, United States; Ouarles, J., Kennesaw State Univ, United States","Joint attention is critical to the education and development of a child. Deficits in joint attention are considered by many researchers to be an early predictor of children with Autism Spectrum Disorder (ASD). Training of joint attention have been a significant topic in ASD intervention education research. We propose a novel joint attention training approach using a Customizable Virtual Human (CVH) and a Virtual Reality (VR) game to assist with joint attention training. Previous work has shown that CVHs potentially help the users with ASD to increase their performance in hand-eye coordination, motivate the users to play longer, as well as improve user experience in a training game. Based upon these discovered CVH benefits, we hypothesize that CVHs may also be beneficial in training joint attention for users with ASD. To test our hypothesis, we developed a CVH with customizable facial features in an educational game-Imagination Drums-and conducted a user study on adolescents with high functioning ASD to investigate the effects of CVHs. We collected users' eye-gaze data and task performance during the game to evaluate the users' joint attention with CVHs and the effectiveness of CVHs compared with Non-Customizable Virtual Humans (NCVHs). The study results showed that the CVH make the participants gaze less at the irrelevant area of the game's storyline (i.e. background), but surprisingly, also provided evidence that participants react slower to the CVH's joint attention bids, compared with NCVH. Overall, the study reveals insights of how users with ASD interact with CVHs and how these interactions affect joint attention. © 2018 IEEE.","Customizable virtual human. Autism Spectrum Disorder. 3D interaction; H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/methodology","Diseases; Virtual reality; Children with autisms; Education research; Educational game; H.5.2 [Information Interfaces and Presentation]: User Interfaces - Evaluation/methodology; Hand eye coordination; Joint attention; Task performance; Virtual humans; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85053823448
"Pfeil K., Taranta E.M., II, Kulshreshth A., Wisniewski P., LaViola J.J., Jr.","55642509800;56182091700;55315352400;57293685900;6602792780;","A comparison of eye-head coordination between virtual and physical realities",2018,"Proceedings - SAP 2018: ACM Symposium on Applied Perception",,,"a18","","",,12,"10.1145/3225153.3225157","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056779160&doi=10.1145%2f3225153.3225157&partnerID=40&md5=c854f0f09a37ac1bb175c9a7347fde2a","University of Central Florida, Orlando, FL, United States; University of Louisiana, Lafayette Lafayette, LA, United States","Pfeil, K., University of Central Florida, Orlando, FL, United States; Taranta, E.M., II, University of Central Florida, Orlando, FL, United States; Kulshreshth, A., University of Louisiana, Lafayette Lafayette, LA, United States; Wisniewski, P., University of Central Florida, Orlando, FL, United States; LaViola, J.J., Jr., University of Central Florida, Orlando, FL, United States","Past research has shown that humans exhibit certain eye-head responses to the appearance of visual stimuli, and these natural reactions change during different activities. Our work builds upon these past observations by offering new insight to how humans behave in Virtual Reality (VR) compared to Physical Reality (PR). Using eye- and head- tracking technology, and by conducting a study on two groups of users - participants in VR or PR - we identify how often these natural responses are observed in both environments. We find that users statistically move their heads more often when viewing stimuli in VR than in PR, and VR users also move their heads more in the presence of text. We open a discussion for identifying the HWD factors that cause this difference, as this may not only affect predictive models using eye movements as features, but also VR user experience overall. © 2018 Association for Computing Machinery.","Eye tracking; Head tracking; User study","Eye tracking; Virtual reality; Eye-head coordination; Head tracking; Natural response; Physical reality; Predictive models; User experience; User study; Visual stimulus; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85056779160
"Ivorra E., Ortega M., Alcaniz M., Garcia-Aracil N.","55639705900;35488807900;7003335420;15762531900;","Multimodal Computer Vision Framework for Human Assistive Robotics",2018,"2018 Workshop on Metrology for Industry 4.0 and IoT, MetroInd 4.0 and IoT 2018 - Proceedings",,,"8428330","18","22",,7,"10.1109/METROI4.2018.8428330","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052543791&doi=10.1109%2fMETROI4.2018.8428330&partnerID=40&md5=b2121674ae73427c63fc18332307fdb7","Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, Spain; Biomedical Neuroengineering Group, Universidad Miguel Hernandez de Elche, Elche, Spain","Ivorra, E., Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, Spain; Ortega, M., Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, Spain; Alcaniz, M., Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, Spain; Garcia-Aracil, N., Biomedical Neuroengineering Group, Universidad Miguel Hernandez de Elche, Elche, Spain","This paper presents a multimodal computer vision framework for human assistive robotics with the purpose of giving accessibility to persons with disabilities. The user is capable of interacting with the system just by staring. Specifically, it is possible to select the desired object as well as to indicate the intention to grasp it just by staring at it. This gaze information is provided by ©Tobii Glasses 2 that in combination with a deep learning algorithm gives the class id of the desirable object. Later, the object's pose is estimated using a RGB-D camera with a new developed technique. This technique mixes a template based algorithm with a deep learning algorithm giving a precise, realtime method for pose estimation. Once the pose is obtained, it is transformed to a grasping position in the coordinate system of the assistive robot that performs the grasping operation. © 2018 IEEE.","Assistive Robotics; Deep Learning; Eye Tracking; Object Pose Detection","Deep learning; Eye tracking; Gesture recognition; Industry 4.0; Internet of things; Robotics; Assistive robotics; Multi-modal; Object pose; Persons with disabilities; Vision frameworks; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85052543791
"Kulkarni S.V., Sangeeta K.","57204030510;6507904443;","Techniques for visual analysis of eye tracking data",2018,"Proceedings of the 2nd International Conference on Green Computing and Internet of Things, ICGCIoT 2018",,,"8753026","525","530",,,"10.1109/ICGCIoT.2018.8753026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069456476&doi=10.1109%2fICGCIoT.2018.8753026&partnerID=40&md5=e7fb9038fc41837b117a6e3a6a702abc","Department of Computer Science and Engineering, Amrita Vishwa Vidyapeetham, India","Kulkarni, S.V., Department of Computer Science and Engineering, Amrita Vishwa Vidyapeetham, India; Sangeeta, K., Department of Computer Science and Engineering, Amrita Vishwa Vidyapeetham, India","Eye tracking is the process of estimating as well as recording gaze positions and eye movements of an individual. Eye tracking technology has many statistical factors which are significant in generating knowledge and values. In most of the approaches an insight is presented with the help of traditional attention maps as well as gaze plots. There is no any single visualization type for all possible requirements. The appropriate choice of a visualization method depends on the format of the data, analysis task specific to the requirements. The objective of this work is to visualize eye tracking data using various visualization especially 3D visuals and animation of eye gazes. These implementations have respective benefits over the other methods of eye tracking visualizations and can be used to generate more knowledge and value extraction from eye tracking metrics. © 2018 IEEE.","Data Science; Data Visualization; Eye Tracking.","Data Science; Data visualization; Eye movements; Green computing; Internet of things; Three dimensional computer graphics; Visualization; Eye tracking technologies; Eye-gaze; Visual analysis; Visualization method; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069456476
"Al Arabi A., Tipu R.S., Bashar M.R., Barman B., Monica S.A., Amin M.A.","57194715430;57194716938;57213289907;57215879960;57217353299;56779456900;","Implementation of Low Cost Stereo Humanoid Adaptive Vision for 3D Positioning and Distance Measurement for Robotics Application with Self-Calibration",2018,"AMS 2017 - Asia Modelling Symposium 2017 and 11th International Conference on Mathematical Modelling and Computer Simulation",,,"8424311","83","88",,,"10.1109/AMS.2017.21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051525933&doi=10.1109%2fAMS.2017.21&partnerID=40&md5=f6a0b175d96393c4d795d362e0e25c92","Computer Vision and Cybernetics Group, Department of Computer Science and Engineering, Independent University Bangladesh, Bashundhara R/A, Dhaka, Bangladesh; Mawlana Bhashani Science and Technology University, Bangladesh; Rangpur Medical College, Bangladesh","Al Arabi, A., Computer Vision and Cybernetics Group, Department of Computer Science and Engineering, Independent University Bangladesh, Bashundhara R/A, Dhaka, Bangladesh; Tipu, R.S., Computer Vision and Cybernetics Group, Department of Computer Science and Engineering, Independent University Bangladesh, Bashundhara R/A, Dhaka, Bangladesh; Bashar, M.R., Computer Vision and Cybernetics Group, Department of Computer Science and Engineering, Independent University Bangladesh, Bashundhara R/A, Dhaka, Bangladesh; Barman, B., Mawlana Bhashani Science and Technology University, Bangladesh; Monica, S.A., Rangpur Medical College, Bangladesh; Amin, M.A., Computer Vision and Cybernetics Group, Department of Computer Science and Engineering, Independent University Bangladesh, Bashundhara R/A, Dhaka, Bangladesh","Robots are getting smarter everyday with the implementation of computer vision system in it. It is now highly required for any robot to have a natural vision system or more likely humanoid vision system to interact with real life incidents. On the perspective of such imaging and vision, we propose an efficient method in order to determine the absolute view point of any desired image location. We used self calibration system and humanoid vision mechanism via stereo cameras to find the region of convergent of an object which with the help of a mathematical model can measure the distance of the object. With comparing different objects position it is also possible to determine the relative distance of the objects. Our system shows that, the real human eye tracking system used, can be possible for getting a realistic view of the image at the 3D point positioning. © 2017 IEEE.","distance measurement; humanoid vision; image processing; Movable multi-cameras; robotic vision; self calibration; Stereo Vision","Calibration; Cameras; Computer vision; Costs; Distance measurement; Eye tracking; Image processing; Robotics; Stereo vision; Computer vision system; Humanoid vision; Multi-cameras; Natural vision system; Relative distances; Robotic vision; Robotics applications; Self calibration; Stereo image processing",Conference Paper,"Final","",Scopus,2-s2.0-85051525933
"Ivorra E., Ortega M., Catalán J.M., Ezquerro S., Lledó L.D., Garcia-Aracil N., Alcañiz M.","55639705900;35488807900;56992476300;56993859700;55388884000;15762531900;7003335420;","Intelligent multimodal framework for human assistive robotics based on computer vision algorithms",2018,"Sensors (Switzerland)","18","8","2408","","",,5,"10.3390/s18082408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050735680&doi=10.3390%2fs18082408&partnerID=40&md5=a56d5a0456eb5453cb0ca42ae4c56d7b","Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, 46022, Spain; Biomedical Neuroengineering Group, Universidad Miguel Hernández de Elche, Elche, 03202, Spain","Ivorra, E., Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, 46022, Spain; Ortega, M., Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, 46022, Spain; Catalán, J.M., Biomedical Neuroengineering Group, Universidad Miguel Hernández de Elche, Elche, 03202, Spain; Ezquerro, S., Biomedical Neuroengineering Group, Universidad Miguel Hernández de Elche, Elche, 03202, Spain; Lledó, L.D., Biomedical Neuroengineering Group, Universidad Miguel Hernández de Elche, Elche, 03202, Spain; Garcia-Aracil, N., Biomedical Neuroengineering Group, Universidad Miguel Hernández de Elche, Elche, 03202, Spain; Alcañiz, M., Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, 46022, Spain","Assistive technologies help all persons with disabilities to improve their accessibility in all aspects of their life. The AIDE European project contributes to the improvement of current assistive technologies by developing and testing a modular and adaptive multimodal interface customizable to the individual needs of people with disabilities. This paper describes the computer vision algorithms part of the multimodal interface developed inside the AIDE European project. The main contribution of this computer vision part is the integration with the robotic system and with the other sensory systems (electrooculography (EOG) and electroencephalography (EEG)). The technical achievements solved herein are the algorithm for the selection of objects using the gaze, and especially the state-of-the-art algorithm for the efficient detection and pose estimation of textureless objects. These algorithms were tested in real conditions, and were thoroughly evaluated both qualitatively and quantitatively. The experimental results of the object selection algorithm were excellent (object selection over 90%) in less than 12 s. The detection and pose estimation algorithms evaluated using the LINEMOD database were similar to the state-of-the-art method, and were the most computationally efficient. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","3D object detection and pose estimation; Assistive robotics; Computer interface; Eye-tracking; Human","Electroencephalography; Electrophysiology; Eye tracking; Gesture recognition; Interactive computer systems; Interfaces (computer); Modal analysis; Robotics; Assistive robotics; Computationally efficient; Computer vision algorithms; Human; Persons with disabilities; Pose estimation; Pose estimation algorithm; State-of-the-art algorithms; Computer vision; algorithm; brain computer interface; electroencephalography; electrooculography; human; procedures; robotics; vision; Algorithms; Brain-Computer Interfaces; Electroencephalography; Electrooculography; Humans; Robotics; Vision, Ocular",Article,"Final","",Scopus,2-s2.0-85050735680
"Mollahosseini A., Abdollahi H., Sweeny T.D., Cole R., Mahoor M.H.","57204081302;57193025926;23029335500;7401590871;8423254700;","Role of embodiment and presence in human perception of robots’ facial cues",2018,"International Journal of Human Computer Studies","116",,,"25","39",,11,"10.1016/j.ijhcs.2018.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046493075&doi=10.1016%2fj.ijhcs.2018.04.005&partnerID=40&md5=1ab8eaacd53d69d281b34e80f5314e28","Daniel Felix Ritchie School of Engineering & Computer Science, University of Denver, Denver, CO  80208, United States; Boulder Learning Inc., Boulder, CO  80301, United States; Department of Psychology, University of Denver, Denver, CO  80208, United States","Mollahosseini, A., Daniel Felix Ritchie School of Engineering & Computer Science, University of Denver, Denver, CO  80208, United States; Abdollahi, H., Daniel Felix Ritchie School of Engineering & Computer Science, University of Denver, Denver, CO  80208, United States; Sweeny, T.D., Department of Psychology, University of Denver, Denver, CO  80208, United States; Cole, R., Boulder Learning Inc., Boulder, CO  80301, United States; Mahoor, M.H., Daniel Felix Ritchie School of Engineering & Computer Science, University of Denver, Denver, CO  80208, United States","Both robotic and virtual agents could one day be equipped with social abilities necessary for effective and natural interaction with human beings. Although virtual agents are relatively inexpensive and flexible, they lack the physical embodiment present in robotic agents. Surprisingly, the role of embodiment and physical presence for enriching human-robot-interaction is still unclear. This paper explores how these unique features of robotic agents influence three major elements of human-robot face-to-face communication, namely the perception of visual speech, facial expression, and eye-gaze. We used a quantitative approach to disentangle the role of embodiment from the physical presence of a social robot, called Ryan, with three different agents (robot, telepresent robot, and virtual agent), as well as with an actual human. We used a robot with a retro-projected face for this study, since the same animation from a virtual agent could be projected to this robotic face, thus allowing comparison of the virtual agent's animation behaviors with both telepresent and the physically present robotic agents. The results of our studies indicate that the eye gaze and certain facial expressions are perceived more accurately when the embodied agent is physically present than when it is displayed on a 2D screen either as a telepresent or a virtual agent. Conversely, we find no evidence that either the embodiment or the presence of the robot improves the perception of visual speech, regardless of syntactic or semantic cues. Comparison of our findings with previous studies also indicates that the role of embodiment and presence should not be generalized without considering the limitations of the embodied agents. © 2018 Elsevier Ltd","Embodiment; Physical presence; Retro-projected robots; Social robot","Animation; Robotics; Semantics; Speech communication; Virtual reality; Embodiment; Facial Expressions; Human perception; Natural interactions; Physical presence; Quantitative approach; Social abilities; Social robots; Human robot interaction",Article,"Final","",Scopus,2-s2.0-85046493075
"Qi Y., Qin L., Zhang J., Zhang S., Huang Q., Yang M.-H.","55977742700;49561776300;57204328730;23391616000;8435766200;7404927015;","Structure-aware local sparse coding for visual tracking",2018,"IEEE Transactions on Image Processing","27","8",,"3857","3869",,40,"10.1109/TIP.2018.2797482","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040996089&doi=10.1109%2fTIP.2018.2797482&partnerID=40&md5=1badf4621e9286fa67973aac8f50e47c","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China; Visual Computing Center, King Abdullah University of Science and Technology, Thuwal, 23955-6900, Saudi Arabia; School of Computer Science and Technology, Harbin Institute of Technology, Weihai, 264209, China; School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, 100049, China; School of Engineering, University of California at Merced, Merced, CA  95344, United States","Qi, Y., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; Qin, L., Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China; Zhang, J., Visual Computing Center, King Abdullah University of Science and Technology, Thuwal, 23955-6900, Saudi Arabia; Zhang, S., School of Computer Science and Technology, Harbin Institute of Technology, Weihai, 264209, China; Huang, Q., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China, School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, 100049, China; Yang, M.-H., School of Engineering, University of California at Merced, Merced, CA  95344, United States","Sparse coding has been applied to visual tracking and related vision problems with demonstrated success in recent years. Existing tracking methods based on local sparse coding sample patches from a target candidate and sparsely encode these using a dictionary consisting of patches sampled from target template images. The discriminative strength of existing methods based on local sparse coding is limited as spatial structure constraints among the template patches are not exploited. To address this problem, we propose a structure-aware local sparse coding algorithm, which encodes a target candidate using templates with both global and local sparsity constraints. For robust tracking, we show the local regions of a candidate region should be encoded only with the corresponding local regions of the target templates that are the most similar from the global view. Thus, a more precise and discriminative sparse representation is obtained to account for appearance changes. To alleviate the issues with tracking drifts, we design an effective template update scheme. Extensive experiments on challenging image sequences demonstrate the effectiveness of the proposed algorithm against numerous state-of-the-art methods. © 1992-2012 IEEE.","local sparse coding; spatial structure information; template update; Visual tracking","Codes (symbols); Encoding (symbols); Flow visualization; Glossaries; Image processing; Image reconstruction; Target tracking; Indexes; Local sparse coding; Object Tracking; Spatial structure information; Template updates; Visual Tracking; Image coding; algorithm; article; coding algorithm; eye tracking",Article,"Final","",Scopus,2-s2.0-85040996089
"Kamimura R., Takeuchi H.","26643093500;7403412373;","Autoeconder-Based Excessive Information Generation for Improving and Interpreting Multi-layered Neural Networks",2018,"Proceedings - 2018 7th International Congress on Advanced Applied Informatics, IIAI-AAI 2018",,,"8693245","518","523",,,"10.1109/IIAI-AAI.2018.00112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065163260&doi=10.1109%2fIIAI-AAI.2018.00112&partnerID=40&md5=243d9888fe0943f56072d3ebd52ed416","IT Education Center, Tokai Univerisity, Japan; Human Informatics Research Institute, National Institute of Advanced Industrial Science and Technology, Japan","Kamimura, R., IT Education Center, Tokai Univerisity, Japan; Takeuchi, H., Human Informatics Research Institute, National Institute of Advanced Industrial Science and Technology, Japan","The present paper aims to propose a new type of learning method to increase information content in input patterns with multiple steps to be used in supervised learning. Unsupervised pre-training to train multi-layered neural networks turned out to be not so effective as has been expected, because connection weights obtained by the unsupervised learning tend to lose their original characteristics immediately in supervised training. To keep original information by unsupervised learning, we here try to increase information in input patterns as much as possible to overcome the vanishing information problem. In particular, for acquiring detailed information more appropriately, we gradually increases detailed information through multiple steps. We applied the method to the actual real data set of the eye-tracking, and two step information augmentation approach was taken. The results confirmed that generalization performance could be improved. In addition, we could interpret the importance of input variables more easily by treating all connection weights collectively. © 2018 IEEE.","Autoencoder; Excessive information; Generalization; Interpretation; Styling","Eye tracking; Machine learning; Unsupervised learning; Auto encoders; Excessive information; Generalization; Interpretation; Styling; Network layers",Conference Paper,"Final","",Scopus,2-s2.0-85065163260
"Villamor M., Rodrigo M.M.","56533988000;7101831103;","Predicting successful collaboration in a pair programming eye tracking experiment",2018,"UMAP 2018 - Adjunct Publication of the 26th Conference on User Modeling, Adaptation and Personalization",,,,"263","268",,9,"10.1145/3213586.3225234","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051473466&doi=10.1145%2f3213586.3225234&partnerID=40&md5=a7f2c83841cfb162b2316d22f4be22a1","Ateneo de Manila University, University of Southeastern Philippines, Davao City, Philippines; Ateneo de Manila University, Quezon City, Philippines","Villamor, M., Ateneo de Manila University, University of Southeastern Philippines, Davao City, Philippines; Rodrigo, M.M., Ateneo de Manila University, Quezon City, Philippines","The context of collaboration is of great importance. Attempts have been made to objectively define what comprises a successful collaboration. Questions like ""When can we say that a collaboration is successful?"" or ""Is there a way to predict that a collaboration would be successful?"" have been asked. In this paper, we look at the output of the collaboration, which are the debugging scores of the pairs, and we consider a collaboration to be successful if it leads to good debugging scores. We choose pair programming because it is an example of a collaboration paradigm. In order to find out what are the potential factors that could possibly predict success in the context of a pair program tracing and debugging task, we performed a dual eye tracking experiment on pairs of novice programmers. We tracked and recorded their fixation sequences and analyzed them using Cross-Recurrence Quantification Analysis (CRQA). Two machine learning algorithms were used, such as Naive Bayes and Logistic Regression. Our findings reveal that CRQA results alone are inadequate to come up with a model with an acceptable performance. Hence, we added the pairs' proficiency level to the model. Between the two models, the Logistic Regression model turned out to be the better model. However, the performance is still not quite unacceptable to predict success so other features are needed to enhance the model. © 2018 Association for Computing Machinery.","Collaboration; Cross-recurrence quantification analysis; Eye tracking; Pair programming","Forecasting; Learning algorithms; Learning systems; Multivariable control systems; Object oriented programming; Program debugging; Regression analysis; Acceptable performance; Collaboration; Collaboration paradigm; Cross recurrences; Dual eye tracking; Logistic Regression modeling; Logistic regressions; Pair-programming; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85051473466
"Smith J., Legg P., Matovic M., Kinsey K.","7410180217;36026676100;57209691115;6603565273;","Predicting user confidence during visual decision making",2018,"ACM Transactions on Interactive Intelligent Systems","8","2","10","","",,5,"10.1145/3185524","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061741550&doi=10.1145%2f3185524&partnerID=40&md5=c5918c3ff1d24a1b36dd6579e45fc300","Department of Computer Science and Creative Technologies, University of the West of England, Bristol, BS161QY, United Kingdom; Department of Psychology, University of the West of England, Bristol, BS161QY, United Kingdom","Smith, J., Department of Computer Science and Creative Technologies, University of the West of England, Bristol, BS161QY, United Kingdom; Legg, P., Department of Computer Science and Creative Technologies, University of the West of England, Bristol, BS161QY, United Kingdom; Matovic, M., Department of Computer Science and Creative Technologies, University of the West of England, Bristol, BS161QY, United Kingdom; Kinsey, K., Department of Psychology, University of the West of England, Bristol, BS161QY, United Kingdom","People are not infallible consistent “oracles”: their confidence in decision-making may vary significantly between tasks and over time. We have previously reported the benefits of using an interface and algorithms that explicitly captured and exploited users’ confidence: error rates were reduced by up to 50% for an industrial multi-class learning problem; and the number of interactions required in a design-optimisation context was reduced by 33%. Having access to users’ confidence judgements could significantly benefit intelligent interactive systems in industry, in areas such as intelligent tutoring systems and in health care. There are many reasons for wanting to capture information about confidence implicitly. Some are ergonomic, but others are more “social”—such as wishing to understand (and possibly take account of) users’ cognitive state without interrupting them. We investigate the hypothesis that users’ confidence can be accurately predicted from measurements of their behaviour. Eye-tracking systems were used to capture users’ gaze patterns as they undertook a series of visual decision tasks, after each of which they reported their confidence on a 5-point Likert scale. Subsequently, predictive models were built using “conventional” machine learning approaches for numerical summary features derived from users’ behaviour. We also investigate the extent to which the deep learning paradigm can reduce the need to design features specific to each application by creating “gaze maps”—visual representations of the trajectories and durations of users’ gaze fixations—and then training deep convolutional networks on these images. Treating the prediction of user confidence as a two-class problem (confident/not confident), we attained classification accuracy of 88% for the scenario of new users on known tasks, and 87% for known users on new tasks. Considering the confidence as an ordinal variable, we produced regression models with a mean absolute error of ≈0.7 in both cases. Capturing just a simple subset of non-task-specific numerical features gave slightly worse, but still quite high accuracy (e.g., MAE ≈ 1.0). Results obtained with gaze maps and convolutional networks are competitive, despite not having access to longer-term information about users and tasks, which was vital for the “summary” feature sets. This suggests that the gaze-map-based approach forms a viable, transferable alternative to handcrafting features for each different application. These results provide significant evidence to confirm our hypothesis, and offer a way of substantially improving many interactive artificial intelligence applications via the addition of cheap non-intrusive hardware and computationally cheap prediction algorithms. © 2018 ACM","Confidence; Human-centred machine learning","Behavioral research; Computer aided instruction; Convolution; Decision making; Deep learning; Forecasting; Machine learning; Regression analysis; Classification accuracy; Confidence; Convolutional networks; Intelligent interactive systems; Intelligent tutoring system; Machine learning approaches; Prediction algorithms; Visual representations; Eye tracking",Article,"Final","",Scopus,2-s2.0-85061741550
"Thies J., Zollhöfer M., Stamminger M., Theobalt C., Niebner M.","56312656700;36245738500;55906526700;6507027272;35772871600;","FaceVR: Real-time gaze-aware facial reenactment in virtual reality",2018,"ACM Transactions on Graphics","37","2","25","","",,43,"10.1145/3182644","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055774377&doi=10.1145%2f3182644&partnerID=40&md5=959185ee903b2bb25d37bafb9a78d8b8","Technical University of Munich, Department of Informatics, Boltzmannstrasse 3, Garching, 85748, Germany; Department of Computer Science, Computer Graphics Laboratory, 353 Serra Mall, Stanford, CA  94305, United States; Lehrstuhl für Informatik 9, Cauerstrasse 11, Erlangen  91058, Germany; MPI Informatik, Saarland Informatics Campus, Campus E 1.4, Saarbruecken, 66123, Germany","Thies, J., Technical University of Munich, Department of Informatics, Boltzmannstrasse 3, Garching, 85748, Germany; Zollhöfer, M., Department of Computer Science, Computer Graphics Laboratory, 353 Serra Mall, Stanford, CA  94305, United States; Stamminger, M., Lehrstuhl für Informatik 9, Cauerstrasse 11, Erlangen  91058, Germany; Theobalt, C., MPI Informatik, Saarland Informatics Campus, Campus E 1.4, Saarbruecken, 66123, Germany; Niebner, M., Technical University of Munich, Department of Informatics, Boltzmannstrasse 3, Garching, 85748, Germany","We propose FaceVR, a novel image-based method that enables video teleconferencing in VR based on self-reenactment. State-of-the-art face tracking methods in the VR context are focused on the animation of rigged 3D avatars (Li et al. 2015; Olszewski et al. 2016). Although they achieve good tracking performance, the results look cartoonish and not real. In contrast to these model-based approaches, FaceVR enables VR teleconferencing using an image-based technique that results in nearly photo-realistic outputs. The key component of FaceVR is a robust algorithm to perform realtime facial motion capture of an actor who is wearing a head-mounted display (HMD), as well as a new data-driven approach for eye tracking from monocular videos. Based on reenactment of a prerecorded stereo video of the person without the HMD, FaceVR incorporates photo-realistic re-rendering in real time, thus allowing artificial modifications of face and eye appearances. For instance, we can alter facial expressions or change gaze directions in the prerecorded target video. In a live setup, we apply these newly introduced algorithmic components. © 2018 ACM.","Eye tracking; Face tracking; Virtual reality","Eye tracking; Helmet mounted displays; Stereo image processing; Teleconferencing; Three dimensional computer graphics; Virtual reality; Data-driven approach; Face Tracking; Facial motion capture; Head mounted displays; Image-based techniques; Model based approach; Tracking performance; Video teleconferencing; Face recognition",Article,"Final","",Scopus,2-s2.0-85055774377
"Martinikorena I., Cabeza R., Villanueva A., Urtasun I., Larumbe A.","56655118600;36763933900;7101612861;57202137012;57210106737;","Fast and robust ellipse detection algorithm for head-mounted eye tracking systems",2018,"Machine Vision and Applications","29","5",,"845","860",,9,"10.1007/s00138-018-0940-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047243390&doi=10.1007%2fs00138-018-0940-0&partnerID=40&md5=fb88f15c7d5c0d0405deccc03dde584e","Electrical and Electronics Engineering Department, Public University of Navarre, Pamplona, Spain","Martinikorena, I., Electrical and Electronics Engineering Department, Public University of Navarre, Pamplona, Spain; Cabeza, R., Electrical and Electronics Engineering Department, Public University of Navarre, Pamplona, Spain; Villanueva, A., Electrical and Electronics Engineering Department, Public University of Navarre, Pamplona, Spain; Urtasun, I., Electrical and Electronics Engineering Department, Public University of Navarre, Pamplona, Spain; Larumbe, A., Electrical and Electronics Engineering Department, Public University of Navarre, Pamplona, Spain","In head-mounted eye tracking systems, the correct detection of pupil position is a key factor in estimating gaze direction. However, this is a challenging issue when the videos are recorded in real-world conditions, due to the many sources of noise and artifacts that exist in these scenarios, such as rapid changes in illumination, reflections, occlusions and an elliptical appearance of the pupil. Thus, it is an indispensable prerequisite that a pupil detection algorithm is robust in these challenging conditions. In this work, we present one pupil center detection method based on searching the maximum contribution point to the radial symmetry of the image. Additionally, two different center refinement steps were incorporated with the aim of adapting the algorithm to images with highly elliptical pupil appearances. The performance of the proposed algorithm is evaluated using a dataset consisting of 225,569 head-mounted annotated eye images from publicly available sources. The results are compared with the better algorithm found in the bibliography, with our algorithm being shown as superior. © 2018, The Author(s).","Eye tracking; Head mounted; Pupil detection","Signal detection; Ellipse detection; Elliptical pupil; Gaze direction; Head mounted; Head-mounted eye tracking; Pupil detection; Radial symmetrys; Refinement step; Eye tracking",Article,"Final","",Scopus,2-s2.0-85047243390
"Wang K., Ji Q.","56637259500;18935108400;","3D gaze estimation without explicit personal calibration",2018,"Pattern Recognition","79",,,"216","227",,18,"10.1016/j.patcog.2018.01.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044650376&doi=10.1016%2fj.patcog.2018.01.031&partnerID=40&md5=398ebfcd4005aa8dd4cd7e59246830a5","Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, 110 Eighth Street, Troy, NY  12180, United States","Wang, K., Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, 110 Eighth Street, Troy, NY  12180, United States; Ji, Q., Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, 110 Eighth Street, Troy, NY  12180, United States","Model-based 3D gaze estimation represents a dominant technique for eye gaze estimation. It allows free head movement and gives good estimation accuracy. But it requires a personal calibration, which may significantly limit its practical utility. Various techniques have been proposed to replace intrusive and subject-unfriendly calibration methods. In this paper, we introduce a new implicit calibration method that takes advantage of four natural constraints during eye gaze tracking. The first constraint is based on two complementary gaze estimation methods. The underlying assumption is that different gaze estimation methods, though based on different principles and mechanisms, ideally predict exactly the same gaze point at the same time. The second constraint is inspired by the well-known center prior principle, it is assumed that most fixations are concentrated on the center of the screen with natural viewing scenarios. The third constraint arises from the fact that for console based eye tracking, human's attention/gaze are always within the screen region. The final constraint comes from eye anatomy, where the value of eye parameters must be within certain regions. The four constraints are integrated jointly and help formulate the implicit calibration as a constrained unsupervised regression problem, which can be effectively solved through the proposed iterative hard EM algorithm. Experiments on two everyday interactions Web-browsing and Video-watching demonstrate the effectiveness of the proposed implicit calibration method. © 2018 Elsevier Ltd","Gaze estimation; Human computer interaction; Implicit calibration; Natural constraints","Calibration; Eye movements; Human computer interaction; Iterative methods; Calibration method; EM algorithms; Eye gaze tracking; Eye parameters; Gaze estimation; Model-based OPC; Natural constraints; Regression problem; Eye tracking",Article,"Final","",Scopus,2-s2.0-85044650376
"Amrouche S., Ferscha A., Gollan B., Heftberger J.","57195488098;6701318941;48361077700;57202944151;","Activity segmentation and identification based on eye gaze features",2018,"ACM International Conference Proceeding Series",,,,"75","82",,1,"10.1145/3197768.3197775","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049881106&doi=10.1145%2f3197768.3197775&partnerID=40&md5=944b91fd4322d78117d57f5045984ff2","Pervasive Computing Applications, Research Studios Austria FG mbH, Vienna, Austria; Insitute for Pervasive Computing, JKU Linz, Linz, Austria; Fischer Sports GmbH, Ried am Innkreis, Austria","Amrouche, S., Pervasive Computing Applications, Research Studios Austria FG mbH, Vienna, Austria; Ferscha, A., Insitute for Pervasive Computing, JKU Linz, Linz, Austria; Gollan, B., Pervasive Computing Applications, Research Studios Austria FG mbH, Vienna, Austria; Heftberger, J., Fischer Sports GmbH, Ried am Innkreis, Austria","In coherence with the ongoing digitalization of production processes, Human Computer Interaction (HCI) technologies have evolved rapidly in industrial applications, providing abundant numbers of the versatile tracking and monitoring devices suitable to address complex challenges. This paper focuses on Activity Segmentation and Activity Identification as one of the most crucial challenges in pervasive computing, applying only visual attention features captured through mobile eye-tracking sensors. We propose a novel, application-independent approach towards segmentation of task executions in semi-manual industrial assembly setup via exploiting the expressive properties of the distribution-based gaze feature Nearest Neighbor Index (NNI) to build a dynamic activity segmentation algorithm. The proposed approach is enriched with a machine learning validation model acting as a feedback loop to classify segments qualities. The approach is evaluated in an alpine ski assembly scenario with real-world data reaching an overall of 91% detection accuracy. © 2018 Association for Computing Machinery.","Activity identification; Activity segmentation; Gaze behavior analysis; Human-centered computing","Behavioral research; Functional assessment; Human computer interaction; Learning systems; Ubiquitous computing; Activity segmentation; Detection accuracy; Gaze behavior analysis; Human Computer Interaction (HCI); Human-centered computing; Industrial assemblies; Mobile eye-tracking; Production process; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049881106
"Augereau O., Jacquet C., Kise K., Journet N.","25421349800;57203014565;16178222100;55953663100;","Vocabulometer: A web platform for document and reader mutual analysis",2018,"Proceedings - 13th IAPR International Workshop on Document Analysis Systems, DAS 2018",,,,"109","114",,5,"10.1109/DAS.2018.59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050275693&doi=10.1109%2fDAS.2018.59&partnerID=40&md5=f64a3e30a3c244d482bfab59d7ed44f4","Osaka Prefecture University, IDAKS, Sakai, Japan; Bordeaux University, LaBRI, Talence, France","Augereau, O., Osaka Prefecture University, IDAKS, Sakai, Japan; Jacquet, C., Osaka Prefecture University, IDAKS, Sakai, Japan; Kise, K., Osaka Prefecture University, IDAKS, Sakai, Japan; Journet, N., Bordeaux University, LaBRI, Talence, France","We present the Vocabulometer, a reading assistant system designed to record the reading activity of a user with an eye tracker and to extract mutual information about the users and the read documents. The Vocabulometer stands as a web platform and can be used for analyzing the comprehension of the user, the comprehensibility of the document, predicting the difficult words, recommending document according to the reader's in order to increase his skills, etc. Since the last years, with the development of low-cost eye trackers, the technology is now accessible for many people, which will allow using data mining and machine learning algorithms for the mutual analysis of documents and readers. © 2018 IEEE.","document analysis; eye tracking; mutual analysis; reader analysis; web platform","Data mining; Eye movements; Learning algorithms; Learning systems; Document analysis; Eye trackers; Low costs; mutual analysis; Mutual informations; reader analysis; Reading activities; web platform; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85050275693
"Behroozi M., Parnin C.","56039074500;15136883200;","Can we predict stressful technical interview settings through eye-tracking?",2018,"Proceedings - EMIP 2018: Eye Movements in Programming",,,"3216729","","",,3,"10.1145/3216723.3216729","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063588398&doi=10.1145%2f3216723.3216729&partnerID=40&md5=9125b2040d85545073dbf9ad8301afe0","North Carolina State University, United States","Behroozi, M., North Carolina State University, United States; Parnin, C., North Carolina State University, United States","Recently, eye-tracking analysis for finding the cognitive load and stress while problem-solving on the whiteboard during a technical interview is finding its way in software engineering society. However, there is no empirical study on analyzing how much the interview setting characteristics affect the eye-movement measurements. Without knowing that, the results of a research on eye-movement measurements analysis for stress detection will not be reliable. In this paper, we analyzed the eye-movements of 11 participants in two interview settings, one on the whiteboard and the other on the paper, to find out if the characteristics of the interview settings affect the analysis of participants' stress. To this end, we applied 7 Machine Learning classification algorithms on three different labeling strategies of the data to suggest researchers of the domain a useful practice of checking the reliability of the eye-measurements before reporting any results. © 2018 ACM.","Data mining; Eye-tracking; Machine learning; Stress detection; Technical interviews","Data mining; Eye tracking; Learning systems; Machine learning; Problem solving; Software engineering; Stresses; Cognitive loads; Empirical studies; Eye movement measurement; Eye-tracking analysis; Labeling strategy; Machine learning classification; Stress detection; Technical interviews; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85063588398
"Popelka S.","55341416700;","Eye-tracking evaluation of 3D thematic maps",2018,"Proceedings - ETVIS 2018: Eye Tracking and Visualization",,,"3205932","","",,2,"10.1145/3205929.3205932","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063566570&doi=10.1145%2f3205929.3205932&partnerID=40&md5=2e9c0976726f78e6b18fbc04a23614cd","Palacky University Olomouc, Olomouc, Czech Republic","Popelka, S., Palacky University Olomouc, Olomouc, Czech Republic","Although many 3D thematic cartography methods exist, the effectiveness of their use is not known. The described experiment comprised two parts focusing on the evaluation of two 3D thematic cartography methods (Prism Map and Illuminated Choropleth Map) compared to a simple choropleth map. The task in both parts of the experiment was to determine which of the marked areas showed a higher value of the displayed phenomenon. The correctness of answers, response time and selected eye-tracking metrics were analysed. In the first part of the experiment, a higher number of correct answers was found for Prism Maps than for simple choropleth maps, but it required more time to solve the task. The Illuminated Choropleth Map showed a higher proportion of correct answers than a simple choropleth map. During evaluation of the eye-tracking metrics, a statistically significant difference was not found in most cases. © 2018 ACM.","Eye-tracking; Illuminated choropleth map; Prism map; Thematic maps; Usability","Maps; Prisms; Visualization; Choropleth maps; Statistically significant difference; Thematic cartography; Thematic maps; Usability; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85063566570
"Martinikorena I., Villanueva A., Cabeza R., Porta S.","56655118600;7101612861;36763933900;7005292345;","Introducing I2head database",2018,"Proceedings - PETMEI 2018: Pervasive Eye Tracking and Mobile Eye-Based Interaction",,,,"","",,7,"10.1145/3208031.3208033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052022779&doi=10.1145%2f3208031.3208033&partnerID=40&md5=b34e5283333fc19d073b639538ee40f8","Electrical and Eletronics Engineering Department, Public University of Navarre Pamplona, Navarre, Spain","Martinikorena, I., Electrical and Eletronics Engineering Department, Public University of Navarre Pamplona, Navarre, Spain; Villanueva, A., Electrical and Eletronics Engineering Department, Public University of Navarre Pamplona, Navarre, Spain; Cabeza, R., Electrical and Eletronics Engineering Department, Public University of Navarre Pamplona, Navarre, Spain; Porta, S., Electrical and Eletronics Engineering Department, Public University of Navarre Pamplona, Navarre, Spain","I2Head database has been created with the aim to become an optimal reference for low cost gaze estimation. It exhibits the following outstanding characteristics: it takes into account key aspects of low resolution eye tracking technology; it combines images of users gazing at different grids of points from alternative positions with registers of user’s head position and it provides calibration information of the camera and a simple 3D head model for each user. Hardware used to build the database includes a 6D magnetic sensor and a webcam. A careful calibration method between the sensor and the camera has been developed to guarantee the accuracy of the data. Different sessions have been recorded for each user including not only static head scenarios but also controlled displacements and even free head movements. The database is an outstanding framework to test both gaze estimation algorithms and head pose estimation methods. © 2018 Copyright held by the owner/author(s).","Database; Gaze estimation evaluation; Head pose estimation; Low cost gaze estimation","Calibration; Cameras; Cost estimating; Database systems; Image recognition; 3D head model; Calibration information; Calibration method; Eye tracking technologies; Gaze estimation; Head Pose Estimation; Head position; Low resolution; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85052022779
"Elmadjian C., Shukla P., Tula A.D., Morimoto C.H.","57202983651;57189354558;55848005200;7102275798;","3D gaze estimation in the scene volume with a head-mounted eye tracker",2018,"Proceedings - COGAIN 2018: Communication by Gaze Interaction",,,"a3","","",,16,"10.1145/3206343.3206351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050119904&doi=10.1145%2f3206343.3206351&partnerID=40&md5=e8dc8145990a83fb5b3de01102ad534f","Computer Science Department, University of São Paulo, Brazil; Computer Science Department, University of California, Santa Barbara, United States","Elmadjian, C., Computer Science Department, University of São Paulo, Brazil; Shukla, P., Computer Science Department, University of California, Santa Barbara, United States; Tula, A.D., Computer Science Department, University of São Paulo, Brazil; Morimoto, C.H., Computer Science Department, University of São Paulo, Brazil","Most applications involving gaze-based interaction are supported by estimation techniques that find a mapping between gaze data and corresponding targets on a 2D surface. However, in Virtual and Augmented Reality (AR) environments, interaction occurs mostly in a volumetric space, which poses a challenge to such techniques. Accurate point-of-regard (PoR) estimation, in particular, is of great importance to AR applications, since most known setups are prone to parallax error and target ambiguity. In this work, we expose the limitations of widely used techniques for PoR estimation in 3D and propose a new calibration procedure using an uncalibrated headmounted binocular eye tracker coupled with an RGB-D camera to track 3D gaze within the scene volume. We conducted a study to evaluate our setup with real-world data using a geometric and an appearance-based method. Our results show that accurate estimation in this setting still is a challenge, though some gaze-based interaction techniques in 3D should be possible.","3D dataset; Calibration; Gaze estimation; Head-mounted eye tracking","Augmented reality; Calibration; Geometrical optics; Stereo vision; 3D dataset; Appearance-based methods; Calibration procedure; Estimation techniques; Gaze estimation; Gaze-based interaction; Head-mounted eye tracking; Virtual and augmented reality; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85050119904
"Larumbe A., Cabeza R., Villanueva A.","57210106737;36763933900;7101612861;","Supervised Descent Method (SDM) applied to accurate pupil detection in off-the-shelf eye tracking systems",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a7","","",,4,"10.1145/3204493.3204551","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049697298&doi=10.1145%2f3204493.3204551&partnerID=40&md5=f6e07f12192683fa6d2b60d9ee644e04","Public University of Navarre, Pamplona, Spain","Larumbe, A., Public University of Navarre, Pamplona, Spain; Cabeza, R., Public University of Navarre, Pamplona, Spain; Villanueva, A., Public University of Navarre, Pamplona, Spain","The precise detection of pupil/iris center is key to estimate gaze accurately. This fact becomes specially challenging in low cost frameworks in which the algorithms employed for high performance systems fail. In the last years an outstanding effort has been made in order to apply training-based methods to low resolution images. In this paper, Supervised Descent Method (SDM) is applied to GI4E database. The 2D landmarks employed for training are the corners of the eyes and the pupil centers. In order to validate the algorithm proposed, a cross validation procedure is performed. The strategy employed for the training allows us to affirm that our method can potentially outperform the state of the art algorithms applied to the same dataset in terms of 2D accuracy. The promising results encourage to carry on in the study of training-based methods for eye tracking. © 2018 Copyright held by the owner/author(s).","2D iris center estimation; Cascaded Regres-sors; Eye tracking; SDM; Supervised Descent Method","Cascaded Regres-sors; Center estimations; Cross validation; Descent method; Eye tracking systems; High performance systems; Low resolution images; State-of-the-art algorithms; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049697298
"Nakayama M.","7401792114;","Ocular reactions in response to impressions of emotion-evoking pictures",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a42","","",,1,"10.1145/3204493.3204574","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049695415&doi=10.1145%2f3204493.3204574&partnerID=40&md5=b54ebb5eca182776bbe291e316def3f6","Information and Communications Engineering, Tokyo Institute of Technology, Japan","Nakayama, M., Information and Communications Engineering, Tokyo Institute of Technology, Japan","Oculomotor indicies in response to emotional stimuli were analysed chronologically in order to investigate the relationships between eye behaviour and emotional activity in human visual perception. Seven participants classified visual stimuli into two emotional groups using subjective ratings of images, such as “Pleasant” and “Unpleasant”. Changes in both eye movements and pupil diameters between the two groups of images were compared. Both the mean saccade lengths and the cross power spectra of eye movements for “Unpleasant” ratings were significantly higher than for other ratings of eye movements in regards to certain the duration of certain pictures shown. Also, both mean pupil diameters and their power spectrum densities were significantly higher when the durations of pictures presented were lengthened. When comparing the response latencies, pupil reactions followed the appearance of changes in the direction of eye movements. The results suggest that at specific latencies, “Unpleasant” images activate both eye movements and pupil dilations. © 2018 Association for Computing Machinery.","Emotional assessment; Eye movements; Frequency analysis; Pupil response; Subjective assessment","Behavioral research; Eye tracking; Power spectrum; Emotional assessment; Emotional stimulus; Frequency Analysis; Human visual perception; Power spectrum density; Pupil response; Subjective assessments; Subjective rating; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85049695415
"Hiroe M., Yamamoto M., Nagamatsu T.","57202892342;56328923300;23398000100;","Implicit user calibration for gaze-tracking systems using an averaged saliency map around the optical axis of the eye",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a56","","",,2,"10.1145/3204493.3204572","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049690021&doi=10.1145%2f3204493.3204572&partnerID=40&md5=6b56e7e2ff3b33ff60fe4e67befcb494","Kobe University, Kobe, Japan; Kwansei Gakuin University, Sanda, Japan","Hiroe, M., Kobe University, Kobe, Japan; Yamamoto, M., Kwansei Gakuin University, Sanda, Japan; Nagamatsu, T., Kobe University, Kobe, Japan","A 3D gaze-tracking method that uses two cameras and two light sources can measure the optical axis of the eye without user calibration. The visual axis of the eye (line of sight) is estimated by conducting a single-point user calibration. This single-point user calibration estimates the angle ? that is offset between the optical and visual axes of the eye, which is a user-dependent parameter. We have proposed an implicit user calibration method for gaze-tracking systems using a saliency map around the optical axis of the eye. We assume that the peak of the average of the saliency maps indicates the visual axis of the eye in the eye coordinate system. We used both-eye restrictions effectively. The experimental result shows that the proposed system could estimate angle ? without explicit personal calibration. © 2018 Copyright held by the owner/author(s).","Calibration; Eye tracking; Saliency map","Calibration; Image segmentation; Light sources; Eye coordinates; Gaze tracking system; Line of Sight; Optical axis; Saliency map; Single point; User calibration; User-dependent; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049690021
"Keyvanara M., Allison R.S.","57195734927;7101890629;","Sensitivity to natural 3D image transformations during eye movements",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a64","","",,2,"10.1145/3204493.3204583","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049686635&doi=10.1145%2f3204493.3204583&partnerID=40&md5=91a47633ba51c43deb9c44204dd948d9","Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada","Keyvanara, M., Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada; Allison, R.S., Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada","The saccadic suppression effect, in which visual sensitivity is reduced significantly during saccades, has been suggested as a mechanism for masking graphic updates in a 3D virtual environment. In this study, we investigate whether the degree of saccadic suppression depends on the type of image change, particularly between different natural 3D scene transformations. The user observed 3D scenes and made a horizontal saccade in response to the displacement of a target object in the scene. During this saccade the entire scene translated or rotated. We studied six directions of transformation corresponding to the canonical directions for the six degrees of freedom. Following each trial, the user made a forced-choice indication of direction of the scene change. Results show that during horizontal saccades, the most recognizable changes were rotations along the roll axis. © 2018 Copyright held by the owner/author(s).","Eye Tracking; Image Transformations; Saccadic Suppression; Virtual Environments","Degrees of freedom (mechanics); Eye tracking; Three dimensional computer graphics; Virtual reality; 3-D virtual environment; Image transformations; Roll axis; Saccadic suppression; Scene change; Six degrees of freedom; Target object; Visual sensitivity; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85049686635
"Bannier K., Jain E., Le Meur O.","57202887183;36715118000;8611330900;","DeepComics: Saliency estimation for comics",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a49","","",,4,"10.1145/3204493.3204560","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049684217&doi=10.1145%2f3204493.3204560&partnerID=40&md5=b963927f783a25624772e93a143b4046","Univ Rennes, CNRS, IRISA, Rennes, F-35000, France; University of Florida, Gainesville, United States","Bannier, K., Univ Rennes, CNRS, IRISA, Rennes, F-35000, France; Jain, E., University of Florida, Gainesville, United States; Le Meur, O., Univ Rennes, CNRS, IRISA, Rennes, F-35000, France","A key requirement for training deep learning saliency models is large training eye tracking datasets. Despite the fact that the accessibility of eye tracking technology has greatly increased, collecting eye tracking data on a large scale for very specific content types is cumbersome, such as comic images, which are different from natural images such as photographs because text and pictorial content is integrated. In this paper, we show that a deep network trained on visual categories where the gaze deployment is similar to comics outperforms existing models and models trained with visual categories for which the gaze deployment is dramatically different from comics. Further, we find that it is better to use a computationally generated dataset on visual category close to comics one than real eye tracking data of a visual category that has different gaze deployment. These findings hold implications for the transference of deep networks to different domains. © 2018 Association for Computing Machinery.","Comics; Deep learning; Eye-movements; Saliency","Deep learning; Eye movements; Comics; Deep networks; Different domains; Eye tracking technologies; Natural images; Saliency; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049684217
"Outram B.I., Pai Y.S., Person T., Minamizawa K., Kunze K.","54411048800;56267209600;57195529262;24587799600;21743317500;","AnyOrbit: Orbital navigation in virtual environments with eye-tracking",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a99","","",,7,"10.1145/3204493.3204555","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049681384&doi=10.1145%2f3204493.3204555&partnerID=40&md5=bb5d77b0024ef82652ac3d791ba3ea28","Keio University, Graduate School of Media Design, Japan","Outram, B.I., Keio University, Graduate School of Media Design, Japan; Pai, Y.S., Keio University, Graduate School of Media Design, Japan; Person, T., Keio University, Graduate School of Media Design, Japan; Minamizawa, K., Keio University, Graduate School of Media Design, Japan; Kunze, K., Keio University, Graduate School of Media Design, Japan","Gaze-based interactions promise to be fast, intuitive and effective in controlling virtual and augmented environments. Yet, there is still a lack of usable 3D navigation and observation techniques. In this work: 1) We introduce a highly advantageous orbital navigation technique, AnyOrbit, providing an intuitive and hands-free method of observation in virtual environments that uses eye-tracking to control the orbital center of movement; 2) The versatility of the technique is demonstrated with several control schemes and use-cases in virtual/augmented reality head-mounted-display and desktop setups, including observation of 3D astronomical data and spectator sports. © 2018 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.","3D navigation; 3D user interface; Augmented reality; Eye tracking; Orbital mode; Orbiting; Virtual reality","Augmented reality; Eye movements; Helmet mounted displays; Navigation; User interfaces; Virtual reality; 3D navigation; 3D user interface; Augmented environments; Gaze-based interaction; Navigation in virtual en-vironments; Observation techniques; Orbital mode; Orbiting; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049681384
"Fuhl W., Geisler D., Santini T., Appel T., Rosenstiel W., Kasneci E.","56770084800;57189847283;54881866000;57191500368;7006528940;56059892600;","CBF: Circular binary features for robust and real-time pupil center detection",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a8","","",,20,"10.1145/3204493.3204559","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049678289&doi=10.1145%2f3204493.3204559&partnerID=40&md5=c49bdcd517ad6ff75612fbb94e723bc5","University Tuebingen, Perception Engineering, Tuebingen, Baden-Wuerttemberg, Germany; University Tuebingen, LEAD Graduate School, Tuebingen, Baden-Wuerttemberg, Germany; University Tuebingen, Technical Computer Science, Tuebingen, Baden-Wuerttemberg, Germany","Fuhl, W., University Tuebingen, Perception Engineering, Tuebingen, Baden-Wuerttemberg, Germany; Geisler, D., University Tuebingen, Perception Engineering, Tuebingen, Baden-Wuerttemberg, Germany; Santini, T., University Tuebingen, Perception Engineering, Tuebingen, Baden-Wuerttemberg, Germany; Appel, T., University Tuebingen, LEAD Graduate School, Tuebingen, Baden-Wuerttemberg, Germany; Rosenstiel, W., University Tuebingen, Technical Computer Science, Tuebingen, Baden-Wuerttemberg, Germany; Kasneci, E., University Tuebingen, Perception Engineering, Tuebingen, Baden-Wuerttemberg, Germany","Modern eye tracking systems rely on fast and robust pupil detection, and several algorithms have been proposed for eye tracking under real world conditions. In this work, we propose a novel binary feature selection approach that is trained by computing conditional distributions. These features are scalable and rotatable, allowing for distinct image resolutions, and consist of simple intensity comparisons, making the approach robust to different illumination conditions as well as rapid illumination changes. The proposed method was evaluated on multiple publicly available data sets, considerably outperforming state-of-the-art methods, and being real-time capable for very high frame rates. Moreover, our method is designed to be able to sustain pupil center estimation even when typical edge-detection-based approaches fail – e.g., when the pupil outline is not visible due to occlusions from reflections or eye lids / lashes. As a consequece, it does not attempt to provide an estimate for the pupil outline. Nevertheless, the pupil center suffices for gaze estimation – e.g., by regressing the relationship between pupil center and gaze point during calibration. © 2018 Association for Computing Machinery.","Conditional distrubution; Machine learning; Pupil detection; Random ferns; Rotatable features; Scaleable features; Statistics","Edge detection; Feature extraction; Image resolution; Learning systems; Statistics; Conditional distrubution; Pupil detection; Random ferns; Rotatable features; Scaleable; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049678289
"Park S., Zhang X., Bulling A., Hilliges O.","57195422868;57142162900;6505807414;14041644100;","Learning to find eye region landmarks for remote gaze estimation in unconstrained settings",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a21","","",,47,"10.1145/3204493.3204545","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049678032&doi=10.1145%2f3204493.3204545&partnerID=40&md5=df2fb633eb35f354c0906b81c91ded45","ETH Zurich, Switzerland; MPI for Informatics, United States","Park, S., ETH Zurich, Switzerland; Zhang, X., MPI for Informatics, United States; Bulling, A., MPI for Informatics, United States; Hilliges, O., ETH Zurich, Switzerland","Conventional feature-based and model-based gaze estimation methods have proven to perform well in settings with controlled illumination and specialized cameras. In unconstrained real-world settings, however, such methods are surpassed by recent appearance-based methods due to difficulties in modeling factors such as illumination changes and other visual artifacts. We present a novel learning-based method for eye region landmark localization that enables conventional methods to be competitive to latest appearance-based methods. Despite having been trained exclusively on synthetic data, our method exceeds the state of the art for iris localization and eye shape registration on real-world imagery. We then use the detected landmarks as input to iterative model-fitting and lightweight learning-based gaze estimation methods. Our approach outperforms existing model-fitting and appearance-based methods in the context of person-independent and personalized gaze estimation. © 2018 Copyright held by the owner/author(s).","Eye region landmark localization; Gaze estimation","Iterative methods; Appearance-based methods; Conventional methods; Gaze estimation; Illumination changes; Landmark localization; Learning-based methods; Person-independent; Remote gaze estimation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049678032
"Velloso E., Coutinho F.L., Kurauchi A., Morimoto C.H.","53364337000;22233254400;57095009900;7102275798;","Circular orbits detection for gaze interaction using 2D correlation and profile matching algorithms",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a25","","",,8,"10.1145/3204493.3204524","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049677580&doi=10.1145%2f3204493.3204524&partnerID=40&md5=87814a822f03ed6d785f144942eafacd","Interaction Design Lab, University of Melbourne, Australia; School of Arts, Sciences and Humanities, University of São Paulo, Brazil; Computer Science Department, University of São Paulo, Brazil","Velloso, E., Interaction Design Lab, University of Melbourne, Australia; Coutinho, F.L., School of Arts, Sciences and Humanities, University of São Paulo, Brazil; Kurauchi, A., Computer Science Department, University of São Paulo, Brazil; Morimoto, C.H., Computer Science Department, University of São Paulo, Brazil","Recently, interaction techniques in which the user selects screen targets by matching their movement with the input device have been gaining popularity, particularly in the context of gaze interaction (e.g. Pursuits, Orbits, AmbiGaze, etc.). However, though many algorithms for enabling such interaction techniques have been proposed, we still lack an understanding of how they compare to each other. In this paper, we introduce two new algorithms for matching eye movements: Profile Matching and 2D Correlation, and present a systematic comparison of these algorithms with two other state-ofthe-art algorithms: The Basic Correlation algorithm used in Pursuits and the Rotated Correlation algorithm used in PathSync. We also examine the effects of two thresholding techniques and post-hoc filtering. We evaluated the algorithms on a user dataset and found the 2D Correlation with one-level thresholding and post-hoc filtering to be the best performing algorithm. © 2018 Copyright held by the owner/author(s).","Eye tracking; Gaze interaction; Orbits; Pursuits; Smooth pursuits","Eye movements; Orbits; Stress intensity factors; Circular orbit; Correlation algorithm; Gaze interaction; Interaction techniques; Matching algorithm; Pursuits; Smooth pursuit; Thresholding techniques; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049677580
"Zhang X., Sugano Y., Bulling A.","57142162900;7005470045;6505807414;","Revisiting data normalization for appearance-based gaze estimation",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a12","","",,23,"10.1145/3204493.3204548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049676828&doi=10.1145%2f3204493.3204548&partnerID=40&md5=6a4945958ee922517dd62622a2db6c96","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Graduate School of Information Science and Technology, Osaka University, Japan","Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Japan; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Appearance-based gaze estimation is promising for unconstrained real-world settings, but the significant variability in head pose and user-camera distance poses significant challenges for training generic gaze estimators. Data normalization was proposed to cancel out this geometric variability by mapping input images and gaze labels to a normalized space. Although used successfully in prior works, the role and importance of data normalization remains unclear. To fill this gap, we study data normalization for the first time using principled evaluations on both simulated and real data. We propose a modification to the current data normalization formulation by removing the scaling factor and show that our new formulation performs significantly better (between 9.5% and 32.7%) in the different evaluation settings. Using images synthesized from a 3D face model, we demonstrate the benefit of data normalization for the efficiency of the model training. Experiments on real-world images confirm the advantages of data normalization in terms of aze estimation performance. © 2018 Copyright held by the owner/author(s).","Appearance-based gaze estimation; Eye tracking; Machine learning","Learning systems; 3-D face modeling; Appearance based; Data normalization; Estimation performance; Gaze estimation; Real world setting; Real-world image; Scaling factors; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049676828
"Steil J., Huang M.X., Bulling A.","57170107900;55258532000;6505807414;","Fixation detection for head-mounted eye tracking based on visual similarity of gaze targets",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a23","","",,21,"10.1145/3204493.3204538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049676653&doi=10.1145%2f3204493.3204538&partnerID=40&md5=09072c84bcfdcc700a7bac0de8cc1f8c","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Steil, J., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Huang, M.X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Fixations are widely analysed in human vision, gaze-based interaction, and experimental psychology research. However, robust fixation detection in mobile settings is profoundly challenging given the prevalence of user and gaze target motion. These movements feign a shift in gaze estimates in the frame of reference defined by the eye tracker’s scene camera. To address this challenge, we present a novel fixation detection method for head-mounted eye trackers. Our method exploits that, independent of user or gaze target motion, target appearance remains about the same during a fixation. It extracts image information from small regions around the current gaze position and analyses the appearance similarity of these gaze patches across video frames to detect fixations. We evaluate our method using fine-grained fixation annotations on a five-participant indoor dataset (MPIIEgoFixation) with more than 2,300 fixations in total. Our method outperforms commonly used velocity- and dispersion-based algorithms, which highlights its significant potential to analyse scene image information for eye movement detection. © 2018 Copyright held by the owner/author(s).","Egocentric vision; Mobile eye tracking; Visual focus of attention","Eye movements; Image analysis; Stereo vision; Target tracking; Appearance similarities; Frame of reference; Gaze-based interaction; Head-mounted eye tracking; Image information; Mobile eye-tracking; Movement detection; Visual focus of attentions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049676653
"Dierkes K., Kassner M., Bulling A.","57202889988;56406193700;6505807414;","A novel approach to single camera, glint-free 3D eye model fitting including corneal refraction",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a9","","",,16,"10.1145/3204493.3204525","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049676019&doi=10.1145%2f3204493.3204525&partnerID=40&md5=9f276594f1e85e9ab8561a953d731269","Pupil Labs Research, Berlin, Germany","Dierkes, K., Pupil Labs Research, Berlin, Germany; Kassner, M., Pupil Labs Research, Berlin, Germany; Bulling, A., Pupil Labs Research, Berlin, Germany","Model-based methods for glint-free gaze estimation typically infer eye pose using pupil contours extracted from eye images. Existing methods, however, either ignore or require complex hardware setups to deal with refraction effects occurring at the corneal interfaces. In this work we provide a detailed analysis of the effects of refraction in glint-free gaze estimation using a single near-eye camera, based on the method presented by [Swirski and Dodgson 2013]. We demonstrate systematic deviations in inferred eyeball positions and gaze directions with respect to synthetic ground-truth data and show that ignoring corneal refraction can result in angular errors of several degrees. Furthermore, we quantify gaze direction dependent errors in pupil radius estimates. We propose a novel approach to account for corneal refraction in 3D eye model fitting and by analyzing synthetic and real images show that our new method successfully captures refraction effects and helps to overcome the shortcomings of the state of the art approach. © 2018 Copyright held by the owner/author(s).","3D eye model; Contour-based; Eye tracking; Glint-free; Pupil detection; Refraction","Cameras; Refraction; Systematic errors; 3D eye models; Contour-based; Corneal refraction; Glint-free; Pupil detection; Refraction effects; State-of-the-art approach; Systematic deviation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049676019
"Wang H., Pi J., Qin T., Shen S., Shi B.E.","56809110800;57195220788;57194712639;55325638900;7402547071;","SLAM-based localization of 3D gaze using a mobile eye tracker",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a65","","",,18,"10.1145/3204493.3204584","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049673814&doi=10.1145%2f3204493.3204584&partnerID=40&md5=d577a55a59834f1957c2cd0ded9a18f7","Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong","Wang, H., Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Pi, J., Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Qin, T., Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Shen, S., Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Shi, B.E., Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong","Past work in eye tracking has focused on estimating gaze targets in two dimensions (2D), e.g. on a computer screen or scene camera image. Three-dimensional (3D) gaze estimates would be extremely useful when humans are mobile and interacting with the real 3D environment. We describe a system for estimating the 3D locations of gaze using a mobile eye tracker. The system integrates estimates of the user’s gaze vector from a mobile eye tracker, estimates of the eye tracker pose from a visual-inertial simultaneous localization and mapping (SLAM) algorithm, a 3D point cloud map of the environment from a RGB-D sensor. Experimental results indicate that our system produces accurate estimates of 3D gaze over a much larger range than remote eye trackers. Our system will enable applications, such as the analysis of 3D human attention and more anticipative human robot interfaces. © 2018 Copyright held by the owner/author(s).","3D Gaze Estimation; Eye Tracker; Human-Robot Interaction; Point Cloud; RGB-D camera; SLAM","Cameras; Eye movements; Human robot interaction; Robotics; Eye trackers; Gaze estimation; Point cloud; Rgb-d cameras; SLAM; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049673814
"Startsev M., Agtzidis I., Dorr M.","57189849471;56241483100;10244404800;","Deep learning vs. manual annotation of eye movements",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a101","","",,,"10.1145/3204493.3208346","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049672990&doi=10.1145%2f3204493.3208346&partnerID=40&md5=142906d2aeeb9ff6884d99a9229f8023","Technical University of Munich, Munich, Germany","Startsev, M., Technical University of Munich, Munich, Germany; Agtzidis, I., Technical University of Munich, Munich, Germany; Dorr, M., Technical University of Munich, Munich, Germany","Deep Learning models have revolutionized many research fields already. However, the raw eye movement data is still typically processed into discrete events via threshold-based algorithms or manual labelling. In this work, we describe a compact 1D CNN model, which we combined with BLSTM to achieve end-to-end sequence-to-sequence learning. We discuss the acquisition process for the ground truth that we use, as well as the performance of our approach, in comparison to various literature models and manual raters. Our deep method demonstrates superior performance, which brings us closer to human-level labelling quality. © 2018 Copyright held by the owner/author(s).","Event detection; Eye movement classification; Smooth pursuit","Deep learning; Eye tracking; Acquisition process; Event detection; Eye movement classifications; Eye movement datum; Literature models; Manual annotation; Sequence learning; Smooth pursuit; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85049672990
"Eivazi S., Santini T., Kübler T.C., Kasneci E.","37019970200;54881866000;55701951700;56059892600;","An inconspicuous and modular head-mounted eye tracker",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a106","","",,3,"10.1145/3204493.3208345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049670163&doi=10.1145%2f3204493.3208345&partnerID=40&md5=60bcb950ff95f723a6ec4550fc64b07a","University of Tübingen, Perception Engineering, Germany","Eivazi, S., University of Tübingen, Perception Engineering, Germany; Santini, T., University of Tübingen, Perception Engineering, Germany; Kübler, T.C., University of Tübingen, Perception Engineering, Germany; Kasneci, E., University of Tübingen, Perception Engineering, Germany","State of the art head mounted eye trackers employ glasses like frames, making their usage uncomfortable or even impossible for prescription eyewear users. Nonetheless, these users represent a notable portion of the population (e.g. the Prevent Blindness America organization reports that about half of the USA population use corrective eyewear for refractive errors alone). Thus, making eye tracking accessible for eyewear users is paramount to not only improve usability, but is also key for the ecological validity of eye tracking studies. In this work, we report on a novel approach for eye tracker design in the form of a modular and inconspicuous device that can be easily attached to glasses; for users without glasses, we also provide a 3D printable frame blueprint. Our prototypes include both low cost Commerical Out of The Shelf (COTS) and more expensive Original Equipment manufacturer (OEM) cameras, with sampling rates ranging between 30 and 120 fps and multiple pixel resolutions. © 2018 Copyright held by the owner/author(s).","Eye tracker; Gaze tracking; Hardware design; Head-mounted","Eye movements; Glass; Ecological validity; Eye trackers; Eye-tracking studies; Hardware design; Head-mounted; Original equipment manufacturers; Refractive error; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049670163
"Hild J., Kühnle C., Voit M., Beyerer J.","44161228300;56453159000;36812047700;6603794653;","Predicting observer’s task from eye movement patterns during motion image analysis",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a58","","",,3,"10.1145/3204493.3204575","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049668477&doi=10.1145%2f3204493.3204575&partnerID=40&md5=3a616912cdc98680d39ac88ff5dc900d","Fraunhofer IOSB, Karlsruhe, Germany; Fraunhofer IOSB, Vision and Fusion Lab, Karlsruhe Institute of Technology, Germany","Hild, J., Fraunhofer IOSB, Karlsruhe, Germany; Kühnle, C., Fraunhofer IOSB, Karlsruhe, Germany; Voit, M., Fraunhofer IOSB, Karlsruhe, Germany; Beyerer, J., Fraunhofer IOSB, Vision and Fusion Lab, Karlsruhe Institute of Technology, Germany","Predicting an observer’s tasks from eye movements during several viewing tasks has been investigated by several authors. This contribution adds task prediction from eye movements tasks occurring during motion image analysis: Explore, Observe, Search, and Track. For this purpose, gaze data was recorded from 30 human observers viewing a motion image sequence once under each task. For task decoding, the classification methods Random Forest, LDA, and QDA were used; features were fixation- or saccade-related measures. Best accuracy for prediction of the three tasks Observe, Search, Track from the 4-minute gaze data samples was 83.7% (chance level 33%) using Random Forest. Best accuracy for prediction of all four tasks from the gaze data samples containing the first 30 seconds of viewing was 59.3% (chance level 25%) using LDA. Accuracy decreased significantly for task prediction on small gaze data chunks of 5 and 3 seconds, being 45.3% and 38.0% (chance 25%) for the four tasks, and 52.3% and 47.7% (chance 33%) for the three tasks. © 2018 Copyright held by the owner/author(s).","Experiment; Eye movements; Machine learning; Motion image analysis; Task prediction","Decision trees; Experiments; Eye tracking; Forecasting; Learning systems; Motion analysis; Classification methods; Data chunks; Data sample; Eye movement patterns; Human observers; Motion images; Random forests; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85049668477
"Orlov P., Shafti A., Auepanwiriyakul C., Songur N., Faisal A.A.","56319626100;56183259000;57202891739;57202892275;6602900233;","A Gaze-contingent Intention Decoding Engine for human augmentation",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a91","","",,4,"10.1145/3204493.3208350","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049667235&doi=10.1145%2f3204493.3208350&partnerID=40&md5=a85e4a3971c6ff4077cb2770ef393ea5","Brain and Behaviour Lab, Imperial College London, United Kingdom","Orlov, P., Brain and Behaviour Lab, Imperial College London, United Kingdom; Shafti, A., Brain and Behaviour Lab, Imperial College London, United Kingdom; Auepanwiriyakul, C., Brain and Behaviour Lab, Imperial College London, United Kingdom; Songur, N., Brain and Behaviour Lab, Imperial College London, United Kingdom; Faisal, A.A., Brain and Behaviour Lab, Imperial College London, United Kingdom","Humans process high volumes of visual information to perform everyday tasks. In a reaching task, the brain estimates the distance and position of the object of interest, to reach for it. Having a grasp intention in mind, human eye-movements produce specific relevant patterns. Our Gaze-Contingent Intention Decoding Engine uses eye-movement data and gaze-point position to indicate the hidden intention. We detect the object of interest using deep convolution neural networks and estimate its position in a physical space using 3D gaze vectors. Then we trigger the possible actions from an action grammar database to perform an assistive movement of the robotic arm, improving action performance in physically disabled people. This document is a short report to accompany the Gaze-contingent Intention Decoding Engine demonstrator, providing details of the setup used and results obtained. © 2018 Copyright held by the owner/author(s).","Assistive robotics; Eye hand interaction; Eye-movements; Gaze-contingent systems","Decoding; Engines; Eye tracking; Object detection; Robotics; Vector spaces; Action performance; Assistive robotics; Convolution neural network; Eye hand interaction; Eye movement datum; Gaze-contingent; Relevant patterns; Visual information; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85049667235
"Singh K., Kalash M., Bruce N.","57213994821;57193707631;8347469300;","Capturing real-world gaze behaviour: Live and unplugged",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a20","","",,2,"10.1145/3204493.3204528","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049666506&doi=10.1145%2f3204493.3204528&partnerID=40&md5=f272700ff280e8efda8290e652d11e38","Department of Computer Science, University of Manitoba, Canada; Department of Computer Science, Ryerson University, United States","Singh, K., Department of Computer Science, University of Manitoba, Canada; Kalash, M., Department of Computer Science, University of Manitoba, Canada; Bruce, N., Department of Computer Science, Ryerson University, United States","Understanding human gaze behaviour has benefits from scientific understanding to many application domains. Current practices constrain possible use cases, requiring experimentation restricted to a lab setting or controlled environment. In this paper, we demonstrate a flexible unconstrained end-to-end solution that allows for collection and analysis of gaze data in real-world settings. To achieve these objectives, rich 3D models of the real world are derived along with strategies for associating experimental eye-tracking data with these models. In particular, we demonstrate the strength of photogrammetry in allowing these capabilities to be realized, and demonstrate the first complete solution for 3D gaze analysis in large-scale outdoor environments using standard camera technology without fiducial markers. The paper also presents techniques for quantitative analysis and visualization of 3D gaze data. As a whole, the body of techniques presented provides a foundation for future research, with new opportunities for experimental studies and computational modeling efforts. © 2018 Copyright held by the owner/author(s).","3D Modeling; Eye tracking; Gaze analysis; Gaze visualization","Behavioral research; Data visualization; Three dimensional computer graphics; Visualization; 3-d modeling; Complete solutions; Computational model; Controlled environment; End-to-end solutions; Gaze analysis; Outdoor environment; Real world setting; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049666506
"He H., She Y., Xiahou J., Yao J., Li J., Hong Q., Ji Y.","56939561900;56156530900;36465827100;16644372400;57207781546;36623542100;57211723326;","Real-time eye-gaze based interaction for human intention prediction and emotion analysis",2018,"ACM International Conference Proceeding Series",,,,"185","194",,5,"10.1145/3208159.3208180","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058287110&doi=10.1145%2f3208159.3208180&partnerID=40&md5=5fc9676df91c1a46b16d13af01d2ea84","Software School, Xiamen University, Quanzhou Institute of Equipment, Manufacturing CAS, China","He, H., Software School, Xiamen University, Quanzhou Institute of Equipment, Manufacturing CAS, China; She, Y., Software School, Xiamen University, Quanzhou Institute of Equipment, Manufacturing CAS, China; Xiahou, J., Software School, Xiamen University, Quanzhou Institute of Equipment, Manufacturing CAS, China; Yao, J., Software School, Xiamen University, Quanzhou Institute of Equipment, Manufacturing CAS, China; Li, J., Software School, Xiamen University, Quanzhou Institute of Equipment, Manufacturing CAS, China; Hong, Q., Software School, Xiamen University, Quanzhou Institute of Equipment, Manufacturing CAS, China; Ji, Y., Software School, Xiamen University, Quanzhou Institute of Equipment, Manufacturing CAS, China","The human eye's state of motion and content of interest can express people's cognitive status and emotional status based on their situation. When observing the surrounding things, the human eyes make different eye movements according to the observed objects which reflects human's attention and interest. In this paper, we capture and analyze patterns of human eye-gaze behavior and head motion and classify them into different categories. Besides, we compute and train the eye-object movement attention model and eye-object feature preference model based on different peoples' eye-gaze behaviors by using machine learning algorithms. These models are used to predict humans' object of interest and the interaction intention according to people's real-time situation. Furthermore, the eye-gaze behavior and head motion patterns can be used as a modality of non-verbal information in the computing of human emotional states based on the PAD affective computing model. Our methodology analyzes human emotion and cognition status from the aspect of eye-gaze behavior and head motion, understands the cognitive information that human eyes can express, and effectively improves the efficiency of human-computer interaction in different circumstances. © 2018 ACM.","Eye-gaze interaction; Machine learning; Robot and vision","Behavioral research; Computer graphics; Computer vision; Eye movements; Learning algorithms; Learning systems; Machine learning; Affective Computing; Cognitive information; Emotion analysis; Eye gaze interactions; Human intentions; Non-verbal information; Object movements; Preference modeling; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85058287110
"Eigbe N., Baltrušaitis T., Morency L.-P., Pestian J.","57202803035;36696075900;6603047400;8058174500;","Toward visual behavior markers of suicidal ideation",2018,"Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018",,,,"530","534",,2,"10.1109/FG.2018.00085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049402323&doi=10.1109%2fFG.2018.00085&partnerID=40&md5=e350bbf0ce3467ebf5cc0193499df986","Rice University, Houston, United States; Microsoft, Cambridge, United Kingdom; Language Technologies Institute, Carnegie Mellon University, Pittsburgh, United States; Cincinnati Childrens Hospital Medical Center, Cincinnati, United States","Eigbe, N., Rice University, Houston, United States; Baltrušaitis, T., Microsoft, Cambridge, United Kingdom, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, United States; Morency, L.-P., Language Technologies Institute, Carnegie Mellon University, Pittsburgh, United States; Pestian, J., Cincinnati Childrens Hospital Medical Center, Cincinnati, United States","Suicide is an increasingly present issue in our society whose eradication could be greatly aided by decision support technologies that can objectively identify behavior markers of suicidal ideation. In this paper, we examine the predictive ability of a variety of smiling and eye gaze behaviors in categorizing hospital patients by mental health status: patients with suicidal ideation, patients with other mental illnesses such as depression, or control group without suicidal ideation or mental illness. We study three main research questions related to suicide behavior markers: (1) Do people with suicidal ideation smile with different dynamics (e.g. genuine vs fake smile)? (2) Do smiles while speaking, listening, and laughing show different levels of occurrence between the three groups? (3) Is gaze aversion (e.g. looking down) also a useful behavior marker? To answer these questions, we created new behavioral annotations on 74 semi-structured interviews from hospital patients, each of them within one of the three mental health conditions. Our data analysis identified behavior markers of mental health status from both smiling and eye gaze behaviors. Using these behavioral features, we created predictive models that show promising results when distinguishing between these three mental health conditions, especially when differentiating suicidal from non-suicidal patients. © 2018 IEEE.","Depression; Diagnosis; Facial behaviors; Gaze; Machine learning; Mental health; Mental illness; Smiling; Suicidal ideation; Visual behaviors","Decision support systems; Diagnosis; Diseases; Gesture recognition; Hospitals; Learning systems; Depression; Facial behaviors; Gaze; Mental health; Mental illness; Smiling; Suicidal ideation; Visual behavior; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85049402323
"Kononenko D., Lempitsky V.","57142551900;22234735100;","Semi-supervised learning for monocular gaze redirection",2018,"Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018",,,,"535","539",,,"10.1109/FG.2018.00086","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049399614&doi=10.1109%2fFG.2018.00086&partnerID=40&md5=685514a389780bf69c802c52f247ce84","Skolkovo Institute of Science and Technology (Skoltech), Moscow, Russian Federation","Kononenko, D., Skolkovo Institute of Science and Technology (Skoltech), Moscow, Russian Federation; Lempitsky, V., Skolkovo Institute of Science and Technology (Skoltech), Moscow, Russian Federation","We present a new approach to monocular learning-based gaze redirection problem in images that is able to train on raw sequences of eye images with unknown gaze directions and a small amount of eye images, where the gaze direction is known. The proposed approach is based on a pair of deep networks, where the first encoder-like network maps eye images to a latent space, while the second network maps pairs of latent representations to warping fields implementing the transformation between the pair of the original images. In the proposed system, both networks are trained in an unsupervised manner, while the gaze-annotated images are only used to estimate displacements in the latent space that are characteristic to certain gaze redirections. Quantitative and qualitative evaluation suggests that such characteristic displacement vectors in the learned latent space can be learned from few examples and are transferable across different people and different imaging conditions. © 2018 IEEE.","Gaze redirection; Image resynthesis; Machine learning; Unsupervised learning","Gesture recognition; Learning algorithms; Learning systems; Unsupervised learning; Vector spaces; Displacement vectors; Gaze direction; Gaze redirection; Imaging conditions; Original images; Qualitative evaluations; Resynthesis; Semi- supervised learning; Supervised learning",Conference Paper,"Final","",Scopus,2-s2.0-85049399614
"Baltrusaitis T., Zadeh A., Lim Y.C., Morency L.-P.","36696075900;57144043100;57201857690;6603047400;","OpenFace 2.0: Facial behavior analysis toolkit",2018,"Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018",,,,"59","66",,395,"10.1109/FG.2018.00019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049395496&doi=10.1109%2fFG.2018.00019&partnerID=40&md5=a0bb1171e618014a5d8c3e3b0545b3da","Microsoft, Cambridge, United Kingdom; Carnegie Mellon University, Pittsburgh, United States","Baltrusaitis, T., Microsoft, Cambridge, United Kingdom, Carnegie Mellon University, Pittsburgh, United States; Zadeh, A., Carnegie Mellon University, Pittsburgh, United States; Lim, Y.C., Carnegie Mellon University, Pittsburgh, United States; Morency, L.-P., Carnegie Mellon University, Pittsburgh, United States","Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace 2.0 - a tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace 2.0 is an extension of OpenFace toolkit and is capable of more accurate facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace 2.0 demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, unlike a lot of modern approaches or toolkits, OpenFace 2.0 source code for training models and running them is freely available for research purposes. © 2018 IEEE.","Eye gaze; Facial behavior analysis; Head pose; Landmark detection","Behavioral research; Computer vision; Image recognition; Learning systems; Behavior analysis; Computer vision algorithms; Eye-gaze; Facial action unit recognition; Facial landmark detection; Head pose; Interactive applications; Landmark detection; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-85049395496
"Spiliotopoulos K., Rigou M., Sirmakessis S.","57215078106;6505802206;6506362420;","A comparative study of skeuomorphic and flat design from a ux perspective",2018,"Multimodal Technologies and Interaction","2","2","31","","",,11,"10.3390/mti2020031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074752151&doi=10.3390%2fmti2020031&partnerID=40&md5=2c3df5f4607ada8eaac636fe395428fb","School of Science and Technology, Hellenic Open University, Parodos Aristotelous 18, Patra, 26335, Greece; Department of Computer Engineering and Informatics, University of Patras, Patras, 26504, Greece; Computer and Informatics Engineering Department, Western Greece University of Applied Sciences (TEI of Western Greece), Meg. Alexandrou 1, Patra, 26334, Greece","Spiliotopoulos, K., School of Science and Technology, Hellenic Open University, Parodos Aristotelous 18, Patra, 26335, Greece; Rigou, M., Department of Computer Engineering and Informatics, University of Patras, Patras, 26504, Greece; Sirmakessis, S., Computer and Informatics Engineering Department, Western Greece University of Applied Sciences (TEI of Western Greece), Meg. Alexandrou 1, Patra, 26334, Greece","A key factor influencing the effectiveness of a user interface is the usability resulting from its design, and the overall experience generated while using it, through any kind of device. The two main design trends that prevail in the field of user interface design is skeuomorphism and flat design. Skeuomorphism was used in UI design long before flat design and it is built upon the notion of metaphors and affordances. Flat design is the main design trend used in most UIs today and, unlike skeuomorphic design, it is considered as a way to explore the digital medium without trying to reproduce the appearance of the physical world. This paper investigates how users perceive the two design approaches at the level of icon design (in terms of icon recognizability, recall and effectiveness) based on series of experiments and on data collected via a Tobii eye tracker. Moreover, the paper poses the question whether users perceive an overall flat design as more aesthetically attractive or more usable than a skeuomorphic equivalent. All tested hypotheses regarding potential effect of design approach on icon recognizability, task completion time, or number of errors were rejected but users perceived flat design as more usable. The last issue considered was how users respond to functionally equivalent flat and skeuomorphic variations of websites when given specific tasks to execute. Most tested hypotheses that website design affects task completion durations, user expected and experienced difficulty, or SUS (System Usability Scale) and meCUE questionnaires scores were rejected but there was a correlation between skeuomorphic design and increased experienced difficulty, as well as design type and SUS scores but not in both websites examined. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Effectiveness; Eye tracking; Flat design; MeCUE; Metaphor; Recall; Skeuomorphic design; SUS",,Article,"Final","",Scopus,2-s2.0-85074752151
"Sharma A., Abrol P.","57169198100;26632764700;","Design and analysis of improved iris-based gaze estimation model",2018,"Journal of Computing Science and Engineering","12","2",,"77","89",,,"10.5626/JCSE.2018.12.2.77","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049211140&doi=10.5626%2fJCSE.2018.12.2.77&partnerID=40&md5=2cc20c9dc98b02543a341bda41e8ee79","Department of Computer Science, GGM Science College, Jammu, India; Department of Computer Science and IT, University of Jammu, Jammu, India","Sharma, A., Department of Computer Science, GGM Science College, Jammu, India; Abrol, P., Department of Computer Science and IT, University of Jammu, Jammu, India","The detection accuracy of gaze direction mainly depends on the performance of features extracted from eye images. Limitations on the estimation of gaze direction include harmful infrared (IR) light, expensive devices, static thresholding, inappropriate and complex segmentation techniques, corneal reflections, etc. In this study, an efficient appearance cum feature-based detection model, namely, iris center-based gaze estimation (ICGE), has been proposed. The model is an extension of the earlier proposed glint-based gaze direction estimation (GDE) model and overcomes the above limitations. The ICGE model has been analyzed for GDE based on iris center coordinates using a local adaptive thresholding technique. An indigenous database using more than two hundred images of different subjects on a five quadrant map screen generates almost 90% accurate results for iris and gaze quadrant detection. The distinguishing features of the low cost, non-intrusive proposed model include a lack of IR and affordable ubiquitous H/W designing, large subject-camera distance and screen dimensions, no glint dependency, and many more. The proposed model also shows significantly better results in the lower periphery corners of the quadrant map than traditional models. In addition, aside from the comparison with the GDE model, the proposed model has also been compared with other existing techniques. © 2018. The Korean Institute of Information Scientists and Engineers.","Adaptive thresholding; Gaze quadrant detection; Glint; Iris center; Iris center based gaze estimation (ICGE) model; Non-intrusive","Image segmentation; Adaptive thresholding; Gaze estimation; Glint; Iris center; Non-intrusive; Feature extraction",Article,"Final","",Scopus,2-s2.0-85049211140
"Vance A., Jenkins J.L., Anderson B.B., Bjornn D.K., Kirwan C.B.","24330327900;36713489900;36986090000;57074795300;6508279742;","Tuning out security warnings: A longitudinal examination of habituation through fMRI, eye tracking, and field experiments",2018,"MIS Quarterly: Management Information Systems","42","2",,"355","380",,37,"10.25300/MISQ/2018/14124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047006261&doi=10.25300%2fMISQ%2f2018%2f14124&partnerID=40&md5=fd70be5ac3f539b52d6cd0d2abfc0a71","Information Systems Department, Marriott School of Business, Brigham Young University, Provo, UT  84602, United States; Department of Psychology, Brigham Young University, Provo, UT  84602, United States; Department of Psychology and Neuroscience Center, Brigham Young University, Provo, UT  84602, United States","Vance, A., Information Systems Department, Marriott School of Business, Brigham Young University, Provo, UT  84602, United States; Jenkins, J.L., Information Systems Department, Marriott School of Business, Brigham Young University, Provo, UT  84602, United States; Anderson, B.B., Information Systems Department, Marriott School of Business, Brigham Young University, Provo, UT  84602, United States; Bjornn, D.K., Department of Psychology, Brigham Young University, Provo, UT  84602, United States; Kirwan, C.B., Department of Psychology and Neuroscience Center, Brigham Young University, Provo, UT  84602, United States","Research in the fields of information systems and human-computer interaction has shown that habituation-decreased response to repeated stimulation-is a serious threat to the effectiveness of security warnings. Although habituation is a neurobiological phenomenon that develops over time, past studies have only examined this problem cross-sectionally. Further, past studies have not examined how habituation influences actual security warning adherence in the field. For these reasons, the full extent of the problem of habituation is unknown. We address these gaps by conducting two complementary longitudinal experiments. First, we performed an experiment collecting fMRI and eye-tracking data simultaneously to directly measure habituation to security warnings as it develops in the brain over a five-day workweek. Our results show not only a general decline of participants' attention to warnings over time but also that attention recovers at least partially between workdays without exposure to the warnings. Further, we found that updating the appearance of a warning-that is, a polymorphic design-substantially reduced habituation of attention. Second, we performed a three-week field experiment in which users were naturally exposed to privacy permission warnings as they installed apps on their mobile devices. Consistent with our fMRI results, users' warning adherence substantially decreased over the three weeks. However, for users who received polymorphic permission warnings, adherence dropped at a substantially lower rate and remained high after three weeks, compared to users who received standard warnings. Together, these findings provide the most complete view yet of the problem of habituation to security warnings and demonstrate that polymorphic warnings can substantially improve adherence. © 2018. The Authors.","Eye tracking; Field experiment; Functional magnetic resonance imaging (fMRI); Habituation; Information security behavior; Longitudinal experimental design; Mobile computing; NeuroIS; Security warnings","Design of experiments; Functional neuroimaging; Human computer interaction; Magnetic resonance imaging; Mobile computing; Security of data; Field experiment; Functional magnetic resonance imaging; Habituation; NeuroIS; Security warning; Eye tracking",Article,"Final","",Scopus,2-s2.0-85047006261
"Li N., Busso C.","55805334600;35742852700;","Calibration free, user-independent gaze estimation with tensor analysis",2018,"Image and Vision Computing","74",,,"10","20",,2,"10.1016/j.imavis.2018.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046809989&doi=10.1016%2fj.imavis.2018.04.001&partnerID=40&md5=68cc0f83b589de7fcaf8bec3f6c62729","The University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States","Li, N., The University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States; Busso, C., The University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States","Human gaze directly signals visual attention, therefore, estimation of gaze has been an important research topic in fields such as human attention modeling and human-computer interaction. Accurate gaze estimation requires user, system and even session dependent parameters, which can be obtained by calibration process. However, this process has to be repeated whenever the parameter changes (head movement, camera movement, monitor movement). This study aims to eliminate the calibration process of gaze estimation by building a user-independent, appearance-based gaze estimation model. The system is ideal for multimodal interfaces, where the gaze is tracked without the cooperation from the users. The main goal is to capture the essential representation of the gaze appearance of the target user. We investigate the tensor analysis framework that decomposes the high dimension gaze data into different factors including individual differences, gaze differences, user-screen distances and session differences. The axis that is representative for a particular subject is automatically chosen in the tensor analysis framework using LASSO regression. The proposed approaches show promising results on capturing the test subject gaze changes. To address the estimation shift caused by the variations in individual heights, or relative position to the monitor, we apply domain adaptation to adjust the gaze estimation, observing further improvements. These promising results suggest that the proposed gaze estimation approach is a feasible and flexible scheme to facilitate gaze-based multimodal interfaces. © 2018 Elsevier B.V.","Domain adaptation; Human computer interaction; LASSO regression; Tensor analysis; User-independent gaze estimation","Behavioral research; Calibration; Interactive computer systems; Regression analysis; Tensors; Calibration process; Domain adaptation; Gaze estimation; Human attention model; Individual Differences; Lasso regressions; Multi-modal interfaces; Tensor analysis; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85046809989
"Khamis M., Oechsner C., Alt F., Bulling A.","35243028400;57144592500;27267528900;6505807414;","VRPursuits: Interaction in virtual reality using smooth pursuit eye movements",2018,"Proceedings of the Workshop on Advanced Visual Interfaces AVI",,,"a18","","",,44,"10.1145/3206505.3206522","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048897321&doi=10.1145%2f3206505.3206522&partnerID=40&md5=9cc1f1bbbe81aa7146f6c886b1c0a87e","LMU Munich, Munich, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Khamis, M., LMU Munich, Munich, Germany; Oechsner, C., LMU Munich, Munich, Germany; Alt, F., LMU Munich, Munich, Germany; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Gaze-based interaction using smooth pursuit eye movements (Pursuits) is attractive given that it is intuitive and overcomes the Midas touch problem. At the same time, eye tracking is becoming increasingly popular for VR applications. While Pursuits was shown to be effective in several interaction contexts, it was never explored in-depth for VR before. In a user study (N=26), we investigated how parameters that are specific to VR settings influence the performance of Pursuits. For example, we found that Pursuits is robust against different sizes of virtual 3D targets. However performance improves when the trajectory size (e.g., radius) is larger, particularly if the user is walking while interacting. While walking, selecting moving targets via Pursuits is generally feasible albeit less accurate than when stationary. Finally, we discuss the implications of these findings and the potential of smooth pursuits for interaction in VR by demonstrating two sample use cases: 1) gaze-based authentication in VR, and 2) a space meteors shooting game. © 2018 Copyright held by the owner/author(s).","Eye Tracking; Gaze Interaction; Pursuits; Virtual Reality","Eye tracking; Stereo vision; Virtual reality; Different sizes; Gaze interaction; Gaze-based interaction; Interaction context; Midas touch problems; Pursuits; Smooth pursuit eye movement; VR applications; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85048897321
"Morales A., Costela F.M., Tolosana R., Woods R.L.","24476050500;55908194000;55605251600;7401707048;","Saccade landing point prediction: A novel approach based on recurrent neural networks",2018,"ACM International Conference Proceeding Series",,,,"1","5",,4,"10.1145/3231884.3231890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055434531&doi=10.1145%2f3231884.3231890&partnerID=40&md5=26ead8d3af87186d9e58fbaf801c72fb","BiDA-Lab, Department of Electrical Engineering, Universidad Autonoma de Madrid, Madrid, Spain; Schepens Eye Research Institute, Mass Eye and Ear, Boston, MA, United States; Department of Ophthalmology, Harvard Medical School, Boston, MA, United States","Morales, A., BiDA-Lab, Department of Electrical Engineering, Universidad Autonoma de Madrid, Madrid, Spain, Schepens Eye Research Institute, Mass Eye and Ear, Boston, MA, United States; Costela, F.M., Schepens Eye Research Institute, Mass Eye and Ear, Boston, MA, United States, Department of Ophthalmology, Harvard Medical School, Boston, MA, United States; Tolosana, R., BiDA-Lab, Department of Electrical Engineering, Universidad Autonoma de Madrid, Madrid, Spain; Woods, R.L., Schepens Eye Research Institute, Mass Eye and Ear, Boston, MA, United States, Department of Ophthalmology, Harvard Medical School, Boston, MA, United States","A saccade is a fast eye movement that allows the change of visualfixation from one object of interest to another. These movementsare characterized by very high angular velocity peaks that canreach up to 1,000°/s, making them as one of the fastestneuromotor activities in the human body. Modeling such acomplex movement remains a challenge. Saccadic eye movementscan be defined by initial and landing points, duration, amplitude,and velocity profile. The landing point is important as it definesthe new fixation region and, therefore, the region of interest of theviewer. Its prediction may reduce problems caused by displayupdate latency in gaze-contingent systems that make real-timechanges in the display based on eye tracking. The maincontribution of this work is to propose the use of state-of-the-artmachine learning techniques (i.e., Recurrent Neural Networks) forsaccade landing point prediction in real-world scenarios. Ourmethod was evaluated using 220,000 saccades from 75 subjectsacquired during viewing video from ""Hollywood"" movies. Theresults obtained using our proposed methods outperform existingapproaches with improvements of up to 40% error reduction. Ourresults show that dynamic temporal relationships exploited byRecurrent Neural Networks can improve the performance oftraditional Feed Forward Neural Networks. © 2018 Association for Computing Machinery.","Deep Learning; Eye movement; Gaze-contingent; LSTM; Recurrent Neural Networks; Saccade","Angular velocity; Deep learning; Eye tracking; Forecasting; Image segmentation; Landing; Long short-term memory; Recurrent neural networks; Error reduction; Gaze-contingent; Learning techniques; LSTM; Real-world scenario; Region of interest; Temporal relationships; Velocity profiles; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85055434531
"Gupta P., Gupta S., Jayagopal A., Pal S., Sinha R.","57226374129;57203225029;57203223080;57203223677;14053056000;","Saliency prediction for mobile user interfaces",2018,"Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018","2018-January",,,"1529","1538",,6,"10.1109/WACV.2018.00171","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050986169&doi=10.1109%2fWACV.2018.00171&partnerID=40&md5=8743f4b647c09e8db4f864f0de5fb843","Adobe Research, India; IIT, Kanpur, India; IIT, Madras, India; IIT, Kharagpur, India","Gupta, P., Adobe Research, India; Gupta, S., IIT, Kanpur, India; Jayagopal, A., IIT, Madras, India; Pal, S., IIT, Kharagpur, India; Sinha, R., Adobe Research, India","We introduce models for saliency prediction for mobile user interfaces. A mobile interface may include elements like buttons and text in addition to natural images which enable performing a variety of tasks. Saliency in natural images is a well studied topic. However, given the difference in what constitutes a mobile interface, and the usage context of these devices, we postulate that saliency prediction for mobile interface images requires a fresh approach. Mobile interface design involves operating on elements, the building blocks of the interface. We first collected eye-gaze data from mobile devices for a free viewing task. Using this data, we develop a novel autoencoder based multi-scale deep learning model that provides saliency prediction at the mobile interface element level. Compared to saliency prediction approaches developed for natural images, we show that our approach performs significantly better on a range of established metrics. © 2018 IEEE.",,"Computer vision; Deep learning; Forecasting; Auto encoders; Building blockes; Learning models; Mobile interface; Mobile interface design; Mobile user interface; Natural images; Usage context; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85050986169
"Kowalski M., Nasarzewski Z., Galinski G., Garbat P.","56519214200;57203223638;22334224600;6506642638;","HoloFace: Augmenting Human-to-Human Interactions on HoloLens",2018,"Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018","2018-January",,,"141","149",,7,"10.1109/WACV.2018.00022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050958372&doi=10.1109%2fWACV.2018.00022&partnerID=40&md5=258f1fd7bd664c48518e6bd97cf3e54c","Warsaw University of Technology, Poland","Kowalski, M., Warsaw University of Technology, Poland; Nasarzewski, Z., Warsaw University of Technology, Poland; Galinski, G., Warsaw University of Technology, Poland; Garbat, P., Warsaw University of Technology, Poland","We present HoloFace, an open-source framework for face alignment, head pose estimation and facial attribute retrieval for Microsoft HoloLens. HoloFace implements two state-of-the-art face alignment methods which can be used interchangeably: one running locally and one running on a remote backend. Head pose estimation is accomplished by fitting a deformable 3D model to the landmarks localized using face alignment. The head pose provides both the rotation of the head and a position in the world space. The parameters of the fitted 3D face model provide estimates of facial attributes such as mouth opening or smile. Together the above information can be used to augment the faces of people seen by the HoloLens user, and thus their interaction. Potential usage scenarios include facial recognition, emotion recognition, eye gaze tracking and many others. We demonstrate the capabilities of our framework by augmenting the faces of people seen through the HoloLens with various objects and animations. © 2018 IEEE.",,"Alignment; Computer vision; Eye tracking; 3-D face modeling; Emotion recognition; Eye gaze tracking; Facial recognition; Head Pose Estimation; Human-to-human interactions; Open source frameworks; Usage scenarios; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85050958372
"Yang H., Qu S., Zhu F., Zheng Z.","55968539600;7202361296;55985914800;56504310400;","Robust objectness tracking with weighted multiple instance learning algorithm",2018,"Neurocomputing","288",,,"43","53",,6,"10.1016/j.neucom.2017.02.106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039993526&doi=10.1016%2fj.neucom.2017.02.106&partnerID=40&md5=debd1517eeda88a9c86ea801dcd63e28","Department of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China; College of Economics, Center for Finance and Accounting Research, Shenzhen University, Shenzhen, Guangdong, China","Yang, H., Department of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China; Qu, S., Department of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, China; Zhu, F., College of Economics, Center for Finance and Accounting Research, Shenzhen University, Shenzhen, Guangdong, China; Zheng, Z., College of Economics, Center for Finance and Accounting Research, Shenzhen University, Shenzhen, Guangdong, China","A novel improved online weighted multiple instance learning algorithm(IWMIL) for visual tracking is proposed. In the IWMIL algorithm, the importance of each sample contributing to bag probability is evaluated based on the objectness estimation with object properties (superpixel straddling). To reduce the computation cost, a coarse-to-fine sample detection method is employed to detect sample for a new arriving frame. Then, an adaptive learning rate, which exploits the maximum classifier score to assign different weights to tracking result and template, is presented to update the classifiers. Furthermore, an object similarity constraint strategy is used to estimate tracking drift. Experimental results on challenging sequences show that the proposed method is robust to occlusion and appearance changes. © 2018 Elsevier B.V.","Adaptive learning rate; Object similarity constraint; Object tracking; Objectness; Weighted multiple instance learning algorithm","Learning systems; Adaptive learning rates; Multiple instance learning; Object similarity constraint; Object Tracking; Objectness; Learning algorithms; algorithm; Article; classifier; eye tracking; improved online weighted multiple instance learning algorithm; machine learning; online system; priority journal; process optimization",Article,"Final","",Scopus,2-s2.0-85039993526
"Ogawa T., Nakazawa A., Nishida T.","57195404621;35807510800;35595754400;","Point of gaze estimation using corneal surface reflection and omnidirectional camera image",2018,"IEICE Transactions on Information and Systems","E101D","5",,"1278","1287",,5,"10.1587/transinf.2017MVP0020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046297983&doi=10.1587%2ftransinf.2017MVP0020&partnerID=40&md5=24b92a5a09aaa043ea5610fad4b7a3b1","Graduate School of Informatics, Kyoto University, Kyoto-shi, 606-8501, Japan; RIKEN Center for Advanced Intelligence Project, Wako-shi, 351-0198, Japan","Ogawa, T., Graduate School of Informatics, Kyoto University, Kyoto-shi, 606-8501, Japan; Nakazawa, A., Graduate School of Informatics, Kyoto University, Kyoto-shi, 606-8501, Japan; Nishida, T., Graduate School of Informatics, Kyoto University, Kyoto-shi, 606-8501, Japan, RIKEN Center for Advanced Intelligence Project, Wako-shi, 351-0198, Japan","We present a human point of gaze estimation system using corneal surface reflection and omnidirectional image taken by spherical panorama cameras, which becomes popular recent years. Our system enables to find where a user is looking at only from an eye image in a 360° surrounding scene image, thus, does not need gaze mapping from partial scene images to a whole scene image that are necessary in conventional eye gaze tracking system. We first generate multiple perspective scene images from an omnidirectional (equirectangular) image and perform registration between the corneal reflection and perspective images using a corneal reflection-scene image registration technique. We then compute the point of gaze using a corneal imaging technique leveraged by a 3D eye model, and project the point to an omnidirectional image. The 3D eye pose is estimate by using the particle-filter-based tracking algorithm. In experiments, we evaluated the accuracy of the 3D eye pose estimation, robustness of registration and accuracy of PoG estimations using two indoor and five outdoor scenes, and found that gaze mapping error was 5.546 [deg] on average. © 2018 The Institute of Electronics, Information and Communication Engineers.","Corneal imaging; Corneal reflection; Eye tracking; Point of gaze estimation; Spherical panorama","Cameras; Image segmentation; Mapping; Corneal reflection; Multiple perspectives; Omnidirectional cameras; Omnidirectional image; Point of gaze; Spherical panorama; Surface reflections; Tracking algorithm; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85046297983
"Gessert N., Schlüter M., Schlaefer A.","57201201119;57195487877;12784041700;","A deep learning approach for pose estimation from volumetric OCT data",2018,"Medical Image Analysis","46",,,"162","179",,18,"10.1016/j.media.2018.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043755938&doi=10.1016%2fj.media.2018.03.002&partnerID=40&md5=fa40f75b64db16c60949b88917066b76","Hamburg University of Technology, Schwarzenbergstraße 95, Hamburg, 21073, Germany","Gessert, N., Hamburg University of Technology, Schwarzenbergstraße 95, Hamburg, 21073, Germany; Schlüter, M., Hamburg University of Technology, Schwarzenbergstraße 95, Hamburg, 21073, Germany; Schlaefer, A., Hamburg University of Technology, Schwarzenbergstraße 95, Hamburg, 21073, Germany","Tracking the pose of instruments is a central problem in image-guided surgery. For microscopic scenarios, optical coherence tomography (OCT) is increasingly used as an imaging modality. OCT is suitable for accurate pose estimation due to its micrometer range resolution and volumetric field of view. However, OCT image processing is challenging due to speckle noise and reflection artifacts in addition to the images’ 3D nature. We address pose estimation from OCT volume data with a new deep learning-based tracking framework. For this purpose, we design a new 3D convolutional neural network (CNN) architecture to directly predict the 6D pose of a small marker geometry from OCT volumes. We use a hexapod robot to automatically acquire labeled data points which we use to train 3D CNN architectures for multi-output regression. We use this setup to provide an in-depth analysis on deep learning-based pose estimation from volumes. Specifically, we demonstrate that exploiting volume information for pose estimation yields higher accuracy than relying on 2D representations with depth information. Supporting this observation, we provide quantitative and qualitative results that 3D CNNs effectively exploit the depth structure of marker objects. Regarding the deep learning aspect, we present efficient design principles for 3D CNNs, making use of insights from the 2D deep learning community. In particular, we present Inception3D as a new architecture which performs best for our application. We show that our deep learning approach reaches errors at our ground-truth label's resolution. We achieve a mean average error of 14.89 ± 9.3 µm and 0.096 ± 0.072° for position and orientation learning, respectively. © 2018 Elsevier B.V.","3D convolutional neural networks; 3D deep learning; Optical coherence tomography; Pose estimation","Convolution; Image processing; Network architecture; Neural networks; Optical data processing; Optical tomography; Convolutional neural network; Convolutional Neural Networks (CNN); Image guided surgery; In-depth analysis; Learning approach; Learning community; Pose estimation; Position and orientations; Deep learning; marker; accuracy; architecture; Article; comparative study; eye tracking; image processing; information; learning; optical coherence tomography; pose estimation; priority journal; radiological parameters; surface property; volumetric optical coherence tomography; algorithm; devices; human; optical coherence tomography; procedures; robotics; three dimensional imaging; Algorithms; Deep Learning; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Robotics; Tomography, Optical Coherence",Article,"Final","",Scopus,2-s2.0-85043755938
"Wibirama S., Nugroho H.A., Hamamoto K.","26654457700;57210591699;7102699225;","Depth gaze and ECG based frequency dynamics during motion sickness in stereoscopic 3D movie",2018,"Entertainment Computing","26",,,"117","127",,11,"10.1016/j.entcom.2018.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042728470&doi=10.1016%2fj.entcom.2018.02.003&partnerID=40&md5=a9b60a0f70c291979f31481ecf0aaf6b","Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Department of Information Media Technology, School of Information and Telecommunication Engineering, Tokai UniversityTokyo  108-8619, Japan","Wibirama, S., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Nugroho, H.A., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Hamamoto, K., Department of Information Media Technology, School of Information and Telecommunication Engineering, Tokai UniversityTokyo  108-8619, Japan","The simulator sickness questionnaire (SSQ) has been a prevalent method to observe motion sickness in stereoscopic 3D motion picture. However, previous works do not provide adequate comprehension of the relationship between SSQ, depth gaze behavior, and heart rate variability in the stereoscopic 3D motion picture. To fill this research gap, we present a novel investigation of motion sickness in stereoscopic 3D movies using SSQ, electrocardiography (ECG), and 3D gaze tracking. Forty participants (N=40) watched only one of two 3D contents—3D content with a strong or a moderate sensation of vection. We observed that viewers of the 3D content with an intense feeling of vection more frequently reported symptoms of nausea (p<0.005) and disorientation (p<0.05) than their counterpart. SSQ, ECG, and 3D gaze tracking data show that sickness level could be reduced by persistently gazing at a particular point during exposure of 3D contents (p < 0.001). Additionally, we found that individuals who were prone to motion sickness experienced depth gaze oscillation during several provoking scenes in dynamic 3D contents. Our experimental results may be used as a guideline in the development of a motion sickness predictor for various stereoscopic 3D motion pictures. © 2018 Elsevier B.V.","3D gaze analysis; ECG; Eye tracking; Heart rate variability; Motion sickness; Stereoscopic 3D; User experience","Diseases; Electrocardiography; Heart; Motion pictures; Stereo image processing; Tracking (position); User interfaces; Gaze analysis; Heart rate variability; Motion sickness; Stereoscopic 3d; User experience; Eye tracking",Article,"Final","",Scopus,2-s2.0-85042728470
"Yaneva V., An Ha L., Eraslan S., Yesilada Y., Mitkov R.","57003253500;57203461757;55785840000;8454176800;6602533142;","Etecting autism based on eye-tracking data from web searching tasks",2018,"Proceedings of the 15th Web for All Conference : Internet of Accessible Things, W4A 2018",,,"a16","","",,18,"10.1145/3192714.3192819","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051796947&doi=10.1145%2f3192714.3192819&partnerID=40&md5=293a9aa591c8d27e996876ba9cfa7b93","University of Wolverhampton, Wolverhampton, United Kingdom; Middle East Technical University, Northern Cyprus Campus, Mersin, Turkey","Yaneva, V., University of Wolverhampton, Wolverhampton, United Kingdom; An Ha, L., University of Wolverhampton, Wolverhampton, United Kingdom; Eraslan, S., Middle East Technical University, Northern Cyprus Campus, Mersin, Turkey; Yesilada, Y., Middle East Technical University, Northern Cyprus Campus, Mersin, Turkey; Mitkov, R., University of Wolverhampton, Wolverhampton, United Kingdom","The ASD diagnosis requires a long, elaborate, and expensive prnnocedure, which is subjective and is currently restricted to behavioural, historical, and parent-report information. In this paper, we present an alternative way for detecting the condition based on the atypical visual-attention patterns of people with autism. We collect gaze data from two different kinds of tasks related to processing of information from web pages: Browsing and Searching. The gaze data is then used to train a machine learning classifier whose aim is to distinguish between participants with autism and a control group of participants without autism. In addition, we explore the effects of the type of the task performed, different approaches to defining the areas of interest, gender, visual complexity of the web pages and whether or not an area of interest contained the correct answer to a searching task. Our best-performing classifier achieved 0.75 classification accuracy for a combination of selected web pages using all gaze features. These preliminary results show that the differences in the way people with autism process web content could be used for the future development of serious games for autism screening. The gaze data, R code, visual stimuli and task descriptionare made freely available for replication purposes. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Autism; Diagnostic Classification; Eye Tracking; Screening; Web","Behavioral research; Diseases; Eye movements; Learning systems; Screening; Serious games; Websites; Area of interest; Autism; Classification accuracy; Control groups; Searching task; Visual Attention; Visual complexity; Visual stimulus; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85051796947
"Arsenovic M., Sladojevic S., Stefanovic D., Anderla A.","55342797300;55243552000;57198355699;57191747561;","Deep neural network ensemble architecture for eye movements classification",2018,"2018 17th International Symposium on INFOTEH-JAHORINA, INFOTEH 2018 - Proceedings","2018-January",,,"1","4",,4,"10.1109/INFOTEH.2018.8345537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050950713&doi=10.1109%2fINFOTEH.2018.8345537&partnerID=40&md5=976daaf91428c0913d282ec87203de53","Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia","Arsenovic, M., Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia; Sladojevic, S., Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia; Stefanovic, D., Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia; Anderla, A., Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia","Up to now, eye tracking technologies have been used for different purposes in various industries, from medical to gaming. Eye tracking methods could include predicting fixations, gaze mapping or movement classification. Recent advances in deep learning techniques provide possibilities for solving many computer vision tasks with high accuracy. Authors of this paper propose a novel deep learning based architecture for eye movement classification task. Proposed architecture is an ensemble approach which employs deep convolutional neural networks that run in parallel, for both eyes separately, for visual feature extractions along with recurrent layers for temporal information gathering. Dataset images for training and validation were gathered from standard web camera and pre-processed automatically using dedicated tools. Overall accuracy of developed classifier on the validation set was 92%. Proposed architecture uses relatively small networks which brings the possibility of real time usage (successfully tested on 15-20fps) on regular CPU. Classifier achieved overall accuracy of 88% on the real-time test, using standard laptop and web camera. © 2018 IEEE.","convolutional networks; data deep learning; eye tracking; image classification; recurrent networks; time-series prediction","Cameras; Convolution; Deep neural networks; Eye tracking; Image classification; mHealth; Motion analysis; Network architecture; Recurrent neural networks; Convolutional networks; Deep convolutional neural networks; Eye movement classifications; Eye tracking technologies; Neural network ensembles; Recurrent networks; Time series prediction; Visual feature extraction; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85050950713
"Panwar P., Collins C.","55639234100;56760469700;","Detecting negative emotion for mixed initiative visual analytics",2018,"Conference on Human Factors in Computing Systems - Proceedings","2018-April",,"LBW004","","",,3,"10.1145/3170427.3188664","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052021356&doi=10.1145%2f3170427.3188664&partnerID=40&md5=0356410f5e8cc00f2498a50e444010e0","University of Ontario Institute of Technology, Oshawa, ON, Canada","Panwar, P., University of Ontario Institute of Technology, Oshawa, ON, Canada; Collins, C., University of Ontario Institute of Technology, Oshawa, ON, Canada","The paper describes an efficient model to detect negative mind states caused by visual analytics tasks. We have developed a method for collecting data from multiple sensors, including GSR and eye-tracking, and quickly generating labelled training data for the machine learning model. Using this method we have created a dataset from 28 participants carrying out intentionally difficult visualization tasks. We have concluded the paper by a discussing the best performing model, Random Forest, and its future applications for providing just-in-time assistance for visual analytics. Copyright held by the owner/author(s).","Data Analysis; Emotion Detection; Eye Tracking; GSR","Decision trees; Eye tracking; Human engineering; Future applications; Just in time; Machine learning models; Mixed initiative; Multiple sensors; Training data; Visual analytics; Visualization",Conference Paper,"Final","",Scopus,2-s2.0-85052021356
"Fridman L.","57185179400;","Deep learning for understanding the human",2018,"Conference on Human Factors in Computing Systems - Proceedings","2018-April",,"C12","","",,,"10.1145/3170427.3170661","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052018794&doi=10.1145%2f3170427.3170661&partnerID=40&md5=a87180435e429400a0b4beaa3e83ee0f","Massachusetts Institute of Technology (MIT), United States","Fridman, L., Massachusetts Institute of Technology (MIT), United States","We will explore how deep learning approaches can be used for perceiving and interpreting the state and behavior of human beings in images, video, audio, and text data. The course will cover how convolutional, recurrent and generative neural networks can be used for applications of face recognition, eye tracking, cognitive load estimation, emotion recognition, natural language processing, voice-based interaction, and activity recognition. The course is open to beginners and is designed for those who are new to deep learning, but it can also benefit advanced researchers in the field looking for a practical overview of deep learning methods and their application. Copyright held by the owner/author(s).","Computer vision; Deep learning; Human-centered artificial intelligence; Natural language processing","Eye tracking; Face recognition; Human engineering; Learning systems; Natural language processing systems; Recurrent neural networks; Speech recognition; Activity recognition; Cognitive loads; Emotion recognition; Human being; Learning approach; Learning methods; NAtural language processing; Text data; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85052018794
"Pfeuffer K., Li Y.","36141954200;57277705200;","Analysis and modeling of grid performance on touchscreen mobile devices",2018,"Conference on Human Factors in Computing Systems - Proceedings","2018-April",,,"","",,7,"10.1145/3173574.3173862","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046972759&doi=10.1145%2f3173574.3173862&partnerID=40&md5=d0d43bf1cbc6e6171044eddac248fe75","Google Research and Machine Intelligence, Mountain View, CA, United States; Lancaster University, Lancaster, United Kingdom","Pfeuffer, K., Google Research and Machine Intelligence, Mountain View, CA, United States, Lancaster University, Lancaster, United Kingdom; Li, Y., Google Research and Machine Intelligence, Mountain View, CA, United States","Touchscreen mobile devices can afford rich interaction behaviors but they are complex to model. Scrollable twodimensional grids are a common user interface on mobile devices that allow users to access a large number of items on a small screen by direct touch. By analyzing touch input and eye gaze of users during grid interaction, we reveal how multiple performance components come into play in such a task, including navigation, visual search and pointing. These findings inspired us to design a novel predictive model that combines these components for modeling grid tasks. We realized these model components by employing both traditional analytical methods and data-driven machine learning approaches. In addition to showing high accuracy achieved by our model in predicting human performance on a test dataset, we demonstrate how such a model can lead to a significant reduction in interaction time when used in a predictive user interface. © 2018 Copyright held by the owner/author(s).","Grid UI; Machine learning; Performance modeling; Predictive interfaces; Touchscreen mobile device","Human engineering; Learning systems; Machine learning; Statistical tests; Touch screens; User interfaces; Analysis and modeling; Grid UI; Human performance; Interaction behavior; Machine learning approaches; Performance Model; Predictive modeling; Two-dimensional grids; Predictive analytics",Conference Paper,"Final","",Scopus,2-s2.0-85046972759
"Zhang X., Huang M.X., Sugano Y., Bulling A.","57142162900;55258532000;7005470045;6505807414;","Training person-specific gaze estimators from user interactions with multiple devices",2018,"Conference on Human Factors in Computing Systems - Proceedings","2018-April",,,"","",,22,"10.1145/3173574.3174198","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046965951&doi=10.1145%2f3173574.3174198&partnerID=40&md5=e98b2223c456e377a0b73ae661096fac","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Graduate School of Information Science and Technology, Osaka University, Japan","Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Huang, M.X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Japan; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Learning-based gaze estimation has significant potential to enable attentive user interfaces and gaze-based interaction on the billions of camera-equipped handheld devices and ambient displays. While training accurate person- and device-independent gaze estimators remains challenging, person-specific training is feasible but requires tedious data collection for each target device. To address these limitations, we present the first method to train person-specific gaze estimators across multiple devices. At the core of our method is a single convolutional neural network with shared feature extraction layers and device-specific branches that we train from face images and corresponding on-screen gaze locations. Detailed evaluations on a new dataset of interactions with five common devices (mobile phone, tablet, laptop, desktop computer, smart TV) and three common applications (mobile game, text editing, media center) demonstrate the significant potential of cross-device training. We further explore training with gaze locations derived from natural interactions, such as mouse or touch input. © 2018 Copyright held by the owner/author(s).","Appearance-based gaze estimation; Multi-devices","Computer games; Convolutional neural networks; Human engineering; Mammals; Multilayer neural networks; User interfaces; Ambient displays; Attentive user interfaces; Gaze estimation; Gaze-based interaction; Multi-devices; Multiple devices; Natural interactions; User interaction; Display devices",Conference Paper,"Final","",Scopus,2-s2.0-85046965951
"Jeevithashree D.V., Ray P., Natarajan P., Pradipta B.","57201637559;57224709536;35840982100;14007579800;","Automating the process of gaze tracking data using soft clustering",2018,"2017 International Conference on Intelligent Computing, Instrumentation and Control Technologies, ICICICT 2017","2018-January",,,"449","456",,,"10.1109/ICICICT1.2017.8342605","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049314460&doi=10.1109%2fICICICT1.2017.8342605&partnerID=40&md5=8bf4cf016fc8d25de7b1a0096749a975","SCOPE, VIT University, Vellore, Tamil Nadu, India; CPDM, Indian Institute of Science, Bangalore, India","Jeevithashree, D.V., SCOPE, VIT University, Vellore, Tamil Nadu, India; Ray, P., CPDM, Indian Institute of Science, Bangalore, India; Natarajan, P., SCOPE, VIT University, Vellore, Tamil Nadu, India; Pradipta, B., CPDM, Indian Institute of Science, Bangalore, India","The aim of the paper is to automate the processing of gaze tracking data through soft clustering techniques. Standard analysis software for eye gaze tracking data requires users to define areas of interest, which may not be best option for exploratory analysis, where users may want to analyze eye gaze tracking data to know the area of interest. We have presented results on using Fuzzy c-means and Expectation Maximization algorithms on gaze tracking data and using an entropy based cluster validation index, we tried to automate identification of areas of interest. In our study, data from search task in digitally rendered 2D architectural plans have been explored and results indicated that irrespective of clustering technique, users fixated attention only 2 or 3 times for individual image. We have also presented GUI of a tool that can automatically identify areas of interest for any gaze tracking data sample using FCM or EM Algorithms. © 2017 IEEE.","Eye Gaze Tracker; Human Computer Interaction(HCI); Soft Clustering; Validation Metric; Visual Search Behaviour","Cluster analysis; Human computer interaction; Intelligent computing; Maximum principle; Eye gaze trackers; Human Computer Interaction (HCI); Soft clustering; Validation metric; Visual search; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049314460
"Bai B., Zhong B., Ouyang G., Wang P., Liu X., Chen Z., Wang C.","57203908518;24473810700;57196010486;57191519658;56180289800;56313442100;55978095000;","Kernel correlation filters for visual tracking with adaptive fusion of heterogeneous cues",2018,"Neurocomputing","286",,,"109","120",,23,"10.1016/j.neucom.2018.01.068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041672090&doi=10.1016%2fj.neucom.2018.01.068&partnerID=40&md5=66e884f252fc2973b4b755b7a2cde080","Department of Computer Science and Technology, Huaqiao University, Xiamen, Fujian  361021, China","Bai, B., Department of Computer Science and Technology, Huaqiao University, Xiamen, Fujian  361021, China; Zhong, B., Department of Computer Science and Technology, Huaqiao University, Xiamen, Fujian  361021, China; Ouyang, G., Department of Computer Science and Technology, Huaqiao University, Xiamen, Fujian  361021, China; Wang, P., Department of Computer Science and Technology, Huaqiao University, Xiamen, Fujian  361021, China; Liu, X., Department of Computer Science and Technology, Huaqiao University, Xiamen, Fujian  361021, China; Chen, Z., Department of Computer Science and Technology, Huaqiao University, Xiamen, Fujian  361021, China; Wang, C., Department of Computer Science and Technology, Huaqiao University, Xiamen, Fujian  361021, China","Although the correlation filter-based trackers have achieved competitive results both on accuracy and robustness, the performance of trackers can still be improved because the most existing trackers either use a fixed scale or a sole filtering template to represent a target object. In this paper, to effectively handle the scale variation and the drifting problem, we propose a correlation filter-based tracker by adaptively fusing the heterogeneous cues. Firstly, to tackle the problems of the fixed template size, the scale of a target object is estimated from a set of possible scales. Secondly, an adaptive set of filtering templates is learned to alleviate the drifting problem by carefully selecting object candidates in different situations to jointly capture the target appearance variations. Finally, a variety of simple yet effective features (e.g., the HOG and color name features) are effectively integrated into the learning process of filters to further improve the discriminative power of the filters. Consequently, the proposed correlation filter-based tracker can simultaneous utilizes different types of cues to effectively estimate the target's location and scale while alleviating the drifting problem. We have done extensive experiments on the CVPR2013 tracking benchmark dataset with 50 challenging sequences. The proposed tracker successfully tracked the targets in about 90% videos and outperformed the state-of-the-art trackers. © 2018 Elsevier B.V.","Correlation filter; Drifting problem; Multi-cue; Scale variation; Visual tracking","Adaptive filtering; Bandpass filters; Correlation filters; Drifting problem; Multi cues; Scale variation; Visual Tracking; Target tracking; Article; association; benchmarking; correlation filter; eye tracking; image analysis; intermethod comparison; kernel method; learning algorithm; mathematical model; priority journal; statistical parameters",Article,"Final","",Scopus,2-s2.0-85041672090
"Wirawan C., Qingyao H., Yi L., Yean S., Lee B.-S., Ran F.","57202648085;57202648497;57192561421;57192383054;7405441352;57202644576;","Pholder: An Eye-Gaze Assisted Reading Application on Android",2018,"Proceedings - 13th International Conference on Signal-Image Technology and Internet-Based Systems, SITIS 2017","2018-January",,,"350","353",,2,"10.1109/SITIS.2017.64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048944192&doi=10.1109%2fSITIS.2017.64&partnerID=40&md5=aa40b0412c1d606afb3426637611d5e1","School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore","Wirawan, C., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore; Qingyao, H., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore; Yi, L., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore; Yean, S., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore; Lee, B.-S., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore; Ran, F., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore","Eye-gaze has been used extensively in human computer interface design, web layout design and as assistive technology. We successfully built a reading application with automatic scrolling, using the images captured by the in-build camera to determine the eye-gaze. The application, Pholder, uses the appearance-based method for gaze estimation and tracking of gaze movement directions for scrolling of the screen. We used an innovative technique, using the integration of pixel intensity, for gaze movement estimation which is more robust then other techniques. © 2017 IEEE.","Appearance-based method; Gaze movement estimation; Mobile; Saccade","Android (operating system); Eye movements; Human computer interaction; Interfaces (computer); Appearance-based methods; Application on androids; Assistive technology; Gaze movements; Human computer interfaces; Innovative techniques; Mobile; Pixel intensities; Motion estimation",Conference Paper,"Final","",Scopus,2-s2.0-85048944192
"Watkins D., Gallardo G., Chau S.","57202449669;57202451705;57202458209;","Pilot Support System: A Machine Learning Approach",2018,"Proceedings - 12th IEEE International Conference on Semantic Computing, ICSC 2018","2018-January",,,"325","328",,2,"10.1109/ICSC.2018.00067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048418154&doi=10.1109%2fICSC.2018.00067&partnerID=40&md5=0e10bf063bbb4da9e3c83fbf02ea4ca8","AVIAGE SYSTEMS Advanced Design and Technology, GE AVIC Civil Avionics Systems, Phoenix, AZ, United States","Watkins, D., AVIAGE SYSTEMS Advanced Design and Technology, GE AVIC Civil Avionics Systems, Phoenix, AZ, United States; Gallardo, G., AVIAGE SYSTEMS Advanced Design and Technology, GE AVIC Civil Avionics Systems, Phoenix, AZ, United States; Chau, S., AVIAGE SYSTEMS Advanced Design and Technology, GE AVIC Civil Avionics Systems, Phoenix, AZ, United States","Pilots can be one of the factors in many air traffic accidents. When one or both pilots are impaired (e.g. fatigue, drunk or distracted), one or both pilots are disabled, one or both pilots are capable but wrong-headed, both pilots don't have sufficient training, both pilots are fully capable but distracted, both pilots miscommunicate with the air traffic controller, or both pilots follow wrong instructions from the air traffic controller, the risk of accident will increase dramatically. In some of these cases, the risk can be mitigated by using big data and machine learning. The learning machine will collect and analyze large amount of data about the state of the aircraft, e.g., the flight path, the immediate environment around the aircraft, the weather and terrain information, and the pilots' input to control the aircraft. Additional sensors such as eye tracking devices and biological monitor can also be added to determine the condition of the pilots. If the pilots' input do not match proper reaction to the situation or the pilots are impaired, the learning machine will first provide an advisory to the pilot. When the situation becomes more urgent, the advisory will be elevated to warning. If there is at least one capable pilot, these advisories and warnings may help the pilot take proper actions. If it is both pilots are impaired or incapable, a warning will be sent to the air traffic controllers so that they can take actions such as redirect the flight management system and activate the autopilot. © 2018 IEEE.","Avionics Systems; Big Data; Deep Learning; Machine Learning; Pilot Analytics","Air navigation; Aircraft accidents; Artificial intelligence; Aviation; Big data; Controllers; Deep learning; Eye tracking; Fighter aircraft; Learning systems; Semantics; Training aircraft; Air traffic controller; Avionics systems; Eye tracking devices; Flight management systems; Immediate environment; Learning machines; Machine learning approaches; Pilot Analytics; Air traffic control",Conference Paper,"Final","",Scopus,2-s2.0-85048418154
"Adithya B., Pavan Kumar B.N., Lee H., Kim J.Y., Moon J.C., Chai Y.H.","57211045502;57202496237;57202498242;57199864806;57202500124;7102457214;","An experimental study on relationship between foveal range and FoV of a human eye using eye tracking devices",2018,"International Conference on Electronics, Information and Communication, ICEIC 2018","2018-January",,,"1","5",,2,"10.23919/ELINFOCOM.2018.8330605","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048588074&doi=10.23919%2fELINFOCOM.2018.8330605&partnerID=40&md5=6b44432cb2aaa9674ae50ed88d6e41c7","Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 156-756, South Korea","Adithya, B., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 156-756, South Korea; Pavan Kumar, B.N., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 156-756, South Korea; Lee, H., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 156-756, South Korea; Kim, J.Y., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 156-756, South Korea; Moon, J.C., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 156-756, South Korea; Chai, Y.H., Graduate School of Advanced Imaging Science, Multimedia and Film Chung-Ang University, Seoul, 156-756, South Korea","Various methodologies have been scrutinized to model a human eye. Most of them have failed to consider aspects pertaining to free movement of the head and mainly focus on the gaze of a Human Eye. Today's eye trackers offer gaze data with respect to the normalized coordinate system. In this paper, experimental results are presented that infer that the point of gaze of a human eye, highly lies within the foveal view and drifts along the foveal view as the user traces the gaze points on the 2D plane. © 2018 Institute of Electronics and Information Engineers.","Eye Tracking; Field of View; Foveal Range; Virtual Eye","Eye movements; Co-ordinate system; Eye trackers; Eye tracking devices; Field of views; Foveal Range; Gaze point; Point of gaze; Virtual Eye; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85048588074
"Prieto L.P., Sharma K., Kidzinski Ł., Rodríguez-Triana M.J., Dillenbourg P.","36936110000;55903734200;56912415000;51161955600;8912010400;","Multimodal teaching analytics: Automated extraction of orchestration graphs from wearable sensor data",2018,"Journal of Computer Assisted Learning","34","2",,"193","203",,41,"10.1111/jcal.12232","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040984057&doi=10.1111%2fjcal.12232&partnerID=40&md5=96eb70385904a4aadbe270b48f4ba1d4","Tallinn University, Estonia; École Polytechnique Fédérale de Lausanne, Switzerland; Stanford UniversityCA, United States","Prieto, L.P., Tallinn University, Estonia; Sharma, K., École Polytechnique Fédérale de Lausanne, Switzerland; Kidzinski, Ł., Stanford UniversityCA, United States; Rodríguez-Triana, M.J., École Polytechnique Fédérale de Lausanne, Switzerland; Dillenbourg, P., École Polytechnique Fédérale de Lausanne, Switzerland","The pedagogical modelling of everyday classroom practice is an interesting kind of evidence, both for educational research and teachers' own professional development. This paper explores the usage of wearable sensors and machine learning techniques to automatically extract orchestration graphs (teaching activities and their social plane over time) on a dataset of 12 classroom sessions enacted by two different teachers in different classroom settings. The dataset included mobile eye-tracking as well as audiovisual and accelerometry data from sensors worn by the teacher. We evaluated both time-independent and time-aware models, achieving median F1 scores of about 0.7–0.8 on leave-one-session-out k-fold cross-validation. Although these results show the feasibility of this approach, they also highlight the need for larger datasets, recorded in a wider variety of classroom settings, to provide automated tagging of classroom practice that can be used in everyday practice across multiple teachers. © 2018 John Wiley & Sons Ltd","activity detection; eye-tracking; multimodal learning analytics; sensors; teaching analytics",,Article,"Final","",Scopus,2-s2.0-85040984057
"Hwang H., Kang D.","55555706100;57211898992;","User-friendly inter-pupillary distance calibration method using a single camera for autostereoscopic 3D displays",2018,"2018 IEEE International Conference on Consumer Electronics, ICCE 2018","2018-January",,,"1","3",,1,"10.1109/ICCE.2018.8326062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048816449&doi=10.1109%2fICCE.2018.8326062&partnerID=40&md5=84836341b86ccab8c241b297b52f3dee","Multimedia Processing Lab, Samsung Advanced Institute of Technology, Suwon, South Korea","Hwang, H., Multimedia Processing Lab, Samsung Advanced Institute of Technology, Suwon, South Korea; Kang, D., Multimedia Processing Lab, Samsung Advanced Institute of Technology, Suwon, South Korea","The accurate inter-pupillary distance (IPD) of a user plays an important role and is a prerequisite for eye-tracking-based autostereoscopic three-dimensional (3D) display systems by calculating the precise 3D eye position of the users. We aimed to develop a robust computer-aided algorithm for each user-specific IPD calibration using a single camera in a user-friendly manner. Our algorithm consists of eye tracking, pattern rendering, user pattern selection, and IPD adjustment according to the selected patterns. Two stereo patterns were designed to clearly show the IPD differences: A 3D stereo registration pattern and a complimentary stereo pattern. We applied this algorithm to 21 users. The reference standard was provided by a commercial pupilometer. The IPD values obtained by the proposed method and the reference standard IPD values were not statistically different (64.9 ± 4.1 mm from the algorithm and 64.2 ± 3.4 mm from the reference standard, p = 6.64) from the students' t-test. A good agreement was observed among the 21 users in using the IPD calibration software with an agreement of 94.8% (kappa 0.89, 95% confidence interval from 0.83 to 0.96, and p < 0.0001). Our algorithm shows promising results in IPD calibration using a single camera in a user-friendly manner. © 2018 IEEE.",,"Calibration; Cameras; Eye tracking; Stereo image processing; Auto stereoscopic; Autostereoscopic 3D displays; Calibration method; Calibration softwares; Confidence interval; Reference standard; Registration pattern; Three dimensional (3D) display systems; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85048816449
"Li C., Wu X., Zhao N., Cao X., Tang J.","56699429900;57197844002;57197816148;8920951000;24286986300;","Fusing two-stream convolutional neural networks for RGB-T object tracking",2018,"Neurocomputing","281",,,"78","85",,33,"10.1016/j.neucom.2017.11.068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038894733&doi=10.1016%2fj.neucom.2017.11.068&partnerID=40&md5=b66447cfcaaca062580dcc67db61a190","School of Computer Science and Technology, Anhui University, Hefei, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","Li, C., School of Computer Science and Technology, Anhui University, Hefei, China; Wu, X., School of Computer Science and Technology, Anhui University, Hefei, China; Zhao, N., School of Computer Science and Technology, Anhui University, Hefei, China; Cao, X., Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Tang, J., School of Computer Science and Technology, Anhui University, Hefei, China","This paper investigates how to integrate the complementary information from RGB and thermal (RGB-T) sources for object tracking. We propose a novel Convolutional Neural Network (ConvNet) architecture, including a two-stream ConvNet and a FusionNet, to achieve adaptive fusion of different source data for robust RGB-T tracking. Both RGB and thermal streams extract generic semantic information of the target object. In particular, the thermal stream is pre-trained on the ImageNet dataset to encode rich semantic information, and then fine-tuned using thermal images to capture the specific properties of thermal information. For adaptive fusion of different modalities while avoiding redundant noises, the FusionNet is employed to select most discriminative feature maps from the outputs of the two-stream ConvNet, and updated online to adapt to appearance variations of the target object. Finally, the object locations are efficiently predicted by applying the multi-channel correlation filter on the fused feature maps. Extensive experiments on the recently public benchmark GTOT verify the effectiveness of the proposed approach against other state-of-the-art RGB-T trackers. © 2017","Adaptive fusion; Convolutional neural network; Correlation filter; Object tracking; Thermal information","Aircraft propulsion; Convolution; Neural networks; Semantics; Tracking (position); Adaptive fusion; Convolutional neural network; Correlation filters; Object Tracking; Thermal information; Passive filters; Article; artificial neural network; convolutional neural network; correlational study; data processing; eye tracking; mathematical computing; priority journal; RGB T object tracking; thermal analysis",Article,"Final","",Scopus,2-s2.0-85038894733
"Robal T., Zhao Y., Lofi C., Hauff C.","21743794200;57043730400;23393135400;23392382200;","Webcam-based attention tracking in online learning: A feasibility study",2018,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,"189","197",,8,"10.1145/3172944.3172987","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045475458&doi=10.1145%2f3172944.3172987&partnerID=40&md5=33f8354d3346f6d5ca88493d108cb85e","Department of Computer Systems, Tallinn University of Technology, Tallinn, Estonia; Web Information Systems, Delft University of Technology, Delft, Netherlands","Robal, T., Department of Computer Systems, Tallinn University of Technology, Tallinn, Estonia; Zhao, Y., Web Information Systems, Delft University of Technology, Delft, Netherlands; Lofi, C., Web Information Systems, Delft University of Technology, Delft, Netherlands; Hauff, C., Web Information Systems, Delft University of Technology, Delft, Netherlands","A main weakness of the open online learning movement is retention: a small minority of learners (on average 5-10%, in extreme cases < 1%) that start a so-called Massive Open Online Course (MOOC) complete it successfully. There are many reasons why learners are unsuccessful, among the most important ones is the lack of self-regulation: learners are often not able to self-regulate their learning behavior. Designing tools that provide learners with a greater awareness of their learning is vital to the future success of MOOC environments. Detecting learners' loss of focus during learning is particularly important, as this can allow us to intervene and return the learners' attention to the learning materials. One technological affordance to detect such loss of focus are webcams-ubiquitous pieces of hardware available in almost all laptops today. In recent years, researchers have begun to exploit eye tracking and gaze data generated from webcams as part of complex machine learning solutions to detect inattention or loss of focus. Those approaches however tend to have a high detection lag, can be inaccurate, and are complex to design and maintain. In contrast, in this paper, we explore the possibility of a simple alternative-the presence or absence of a face- to detect a loss of focus in the online learning setting. To this end, we evaluate the performance of three consumer and professional eye/face-tracking frameworks using a benchmark suite we designed specifically for this purpose: it contains a set of common xMOOC user activities and behaviours. The results of our study show that even this basic approach poses a significant challenge to current hardware and software-based tracking solutions. © 2018 ACM.","Eye tracking; Face detection; MOOCs; Online learning","Behavioral research; Benchmarking; Eye tracking; Face recognition; Hardware; Learning systems; User interfaces; Feasibility studies; Hardware and software; Learning behavior; Learning materials; Massive open online course; MOOCs; Online learning; Tracking solutions; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85045475458
"Soleymani M., Riegler M., Halvorsen P.","57188866370;56411768700;35580847300;","Multimodal analysis of user behavior and browsed content under different image search intents",2018,"International Journal of Multimedia Information Retrieval","7","1",,"29","41",,7,"10.1007/s13735-018-0150-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041113912&doi=10.1007%2fs13735-018-0150-6&partnerID=40&md5=2203be1bba9fc89ddbce6a9309fae39d","Swiss Center for Affective Sciences, University of Geneva, Geneva, Switzerland; Simula Metropolitan Center for Digital Engineering, Oslo, Norway; Simula Research Laboratory, Oslo, Norway","Soleymani, M., Swiss Center for Affective Sciences, University of Geneva, Geneva, Switzerland; Riegler, M., Simula Metropolitan Center for Digital Engineering, Oslo, Norway; Halvorsen, P., Simula Research Laboratory, Oslo, Norway","The motivation or intent of a search for content may vary between users and use-cases. Knowledge and understanding of these underlying objectives may therefore be important in order to return appropriate search results, and studies of user search intent are emerging in information retrieval to understand why a user is searching for a particular type of content. In the context of image search, our work targets automatic recognition of users’ intent in an early stage of a search session. We have designed seven different search scenarios under the intent conditions of finding items, re-finding items and entertainment. Moreover, we have collected facial expressions, physiological responses, eye gaze and implicit user interactions from 51 participants who performed seven different search tasks on a custom-built image retrieval platform, and we have analyzed the users’ spontaneous and explicit reactions under different intent conditions. Finally, we trained different machine learning models to predict users’ search intent from the visual content of the visited images, the user interactions and the spontaneous responses. Our experimental results show that after fusing the visual and user interaction features, our system achieved the F-1 score of 0.722 for classifying three classes in a user-independent cross-validation. Eye gaze and implicit user interactions, including mouse movements and keystrokes are the most informative features for intent recognition. In summary, the most promising results are obtained by modalities that can be captured unobtrusively and online, and the results therefore demonstrate the potential of including intent-based methods in multimedia retrieval platforms. © 2018, Springer-Verlag London Ltd., part of Springer Nature.","Machine learning; Search intent; User behavior",,Article,"Final","",Scopus,2-s2.0-85041113912
"Perwög M., Bardosi Z., Freysinger W.","55586708400;14628454000;57194224765;","Experimental validation of predicted application accuracies for computer-assisted (CAS) intraoperative navigation with paired-point registration",2018,"International Journal of Computer Assisted Radiology and Surgery","13","3",,"425","441",,5,"10.1007/s11548-017-1653-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027375061&doi=10.1007%2fs11548-017-1653-y&partnerID=40&md5=c2e553f7e091516e217c0c745b771eda","Medical University Innsbruck, Anichstr. 35, Innsbruck, Austria","Perwög, M., Medical University Innsbruck, Anichstr. 35, Innsbruck, Austria; Bardosi, Z., Medical University Innsbruck, Anichstr. 35, Innsbruck, Austria; Freysinger, W., Medical University Innsbruck, Anichstr. 35, Innsbruck, Austria","Purpose: The target registration error (TRE) is a crucial parameter to estimate the potential usefulness of computer-assisted navigation intraoperatively. Both image-to-patient registration on base of rigid-body registration and TRE prediction methods are available for spatially isotropic and anisotropic data. This study presents a thorough validation of data obtained in an experimental operating room setting with CT images. Methods: Optical tracking was used to register a plastic skull, an anatomic specimen, and a volunteer to their respective CT images. Plastic skull and anatomic specimen had implanted bone fiducials for registration; the volunteer was registered with anatomic landmarks. Fiducial localization error, fiducial registration error, and total target error (TTE) were measured; the TTE was compared to isotropic and anisotropic error prediction models. Numerical simulations of the experiment were done additionally. Results: The user localization error and the TTE were measured and calculated using predictions, both leading to results as expected for anatomic landmarks and screws used as fiducials. TRE/TTE is submillimetric for the plastic skull and the anatomic specimen. In the experimental data a medium correlation was found between TRE and target localization error (TLE). Most of the predictions of the application accuracy (TRE) fall in the 68% confidence interval of the measured TTE. For the numerically simulated data, a prediction of TTE was not possible; TRE and TTE show a negligible correlation. Conclusion: Experimental application accuracy of computer-assisted navigation could be predicted satisfactorily with adequate models in an experimental setup with paired-point registration of CT images to a patient. The experimental findings suggest that it is possible to run navigation and prediction of navigation application accuracy basically defined by the spatial resolution/precision of the 3D tracker used. © 2017, The Author(s).","Accuracy; Anisotropy; Error analysis; Navigation; Registration; Surgical guidance","anatomic landmark; anisotropy; Article; computer assisted tomography; diagnostic accuracy; experimental study; eye tracking; human; intraoperative period; measurement error; paired point registration; priority journal; radiological parameters; total target error; validation study; anatomic landmark; computer assisted surgery; diagnostic imaging; fiducial marker; procedures; reproducibility; skull; surgery; x-ray computed tomography; Anatomic Landmarks; Fiducial Markers; Humans; Reproducibility of Results; Skull; Surgery, Computer-Assisted; Tomography, X-Ray Computed",Article,"Final","",Scopus,2-s2.0-85027375061
"Wang L., Lu H., Yang M.-H.","57142763100;8218163400;7404927015;","Constrained Superpixel Tracking",2018,"IEEE Transactions on Cybernetics","48","3","7875137","1030","1041",,23,"10.1109/TCYB.2017.2675910","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016485217&doi=10.1109%2fTCYB.2017.2675910&partnerID=40&md5=8c6768c98012be816921c4b1bf4a33d2","School of Information and Communication Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, 116024, China; Department of Electrical Engineering and Computer Science, University of California at Merced, Merced, CA  95344, United States","Wang, L., School of Information and Communication Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, 116024, China; Lu, H., School of Information and Communication Engineering, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, 116024, China; Yang, M.-H., Department of Electrical Engineering and Computer Science, University of California at Merced, Merced, CA  95344, United States","In this paper, we propose a constrained graph labeling algorithm for visual tracking where nodes denote superpixels and edges encode the underlying spatial, temporal, and appearance fitness constraints. First, the spatial smoothness constraint, based on a transductive learning method, is enforced to leverage the latent manifold structure in feature space by investigating unlabeled superpixels in the current frame. Second, the appearance fitness constraint, which measures the probability of a superpixel being contained in the target region, is developed to incrementally induce a long-term appearance model. Third, the temporal smoothness constraint is proposed to construct a short-term appearance model of the target, which handles the drastic appearance change between consecutive frames. All these three constraints are incorporated in the proposed graph labeling algorithm such that induction and transduction, short- A nd long-term appearance models are combined, respectively. The foreground regions inferred by the proposed graph labeling method are used to guide the tracking process. Tracking results, in turn, facilitate more accurate online update by filtering out potential contaminated training samples. Both quantitative and qualitative evaluations on challenging tracking data sets show that the proposed constrained tracking algorithm performs favorably against the state-of-the-art methods. © 2013 IEEE.","Smoothness constraints; superpixel; tracking; transduction","Appearance modeling; Foreground regions; Manifold structures; Qualitative evaluations; Smoothness constraints; State-of-the-art methods; Tracking algorithm; Transductive learning; Pixels; algorithm; article; eye tracking; filtration; learning; probability; qualitative analysis; quantitative analysis",Article,"Final","",Scopus,2-s2.0-85016485217
"Lee S., Hooshyar D., Ji H., Nam K., Lim H.","57191290060;56572940600;56585214600;55446701400;36028297500;","Mining biometric data to predict programmer expertise and task difficulty",2018,"Cluster Computing","21","1",,"1097","1107",,20,"10.1007/s10586-017-0746-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009885123&doi=10.1007%2fs10586-017-0746-2&partnerID=40&md5=cac80c9f323f97cae9458389c45d5b7f","Department of Computer Science and Engineering, Korea University, Seongbuk-gu, Seoul, South Korea; Department of Psychology, Korea University, Seongbuk-gu, Seoul, South Korea","Lee, S., Department of Computer Science and Engineering, Korea University, Seongbuk-gu, Seoul, South Korea; Hooshyar, D., Department of Computer Science and Engineering, Korea University, Seongbuk-gu, Seoul, South Korea; Ji, H., Department of Computer Science and Engineering, Korea University, Seongbuk-gu, Seoul, South Korea; Nam, K., Department of Psychology, Korea University, Seongbuk-gu, Seoul, South Korea; Lim, H., Department of Computer Science and Engineering, Korea University, Seongbuk-gu, Seoul, South Korea","Programming mistakes frequently waste software developers’ time and may lead to the introduction of bugs into their software, causing serious risks for their customers. Using the correlation between various software process metrics and defects, earlier work has traditionally attempted to spot such bug risks. However, this study departs from previous works in examining a more direct method of using psycho-physiological sensors data to detect the difficulty of program comprehension tasks and programmer level of expertise. By conducting a study with 38 expert and novice programmers, we investigated how well an electroencephalography and an eye-tracker can be utilized in predicting programmer expertise (novice/expert) and task difficulty (easy/difficult). Using data from both sensors, we could predict task difficulty and programmer level of expertise with 64.9 and 97.7% precision and 68.6 and 96.4% recall, respectively. The result shows it is possible to predict the perceived difficulty of a task and expertise level for developers using psycho-physiological sensors data. In addition, we found that while using single biometric sensor shows good results, the composition of both sensors lead to the best overall performance. © 2017, Springer Science+Business Media New York.","Biometric data; Code comprehension; Machine learning; Programming expertise; Task difficulty","Biometrics; Electroencephalography; Electrophysiology; Eye tracking; Forecasting; Learning systems; Biometric data; Code comprehension; Perceived difficulties; Program comprehension; Psycho-physiological; Software developer; Software Process metrics; Task difficulty; Program debugging",Article,"Final","",Scopus,2-s2.0-85009885123
"Han R., Xiao S.","57202158820;13409039200;","Human visual scanpath prediction based on RGB-D saliency",2018,"ACM International Conference Proceeding Series",,,,"180","184",,2,"10.1145/3191442.3191463","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047296321&doi=10.1145%2f3191442.3191463&partnerID=40&md5=2478464e480f086dcde55b4740d7755f","School of software, Shanghai Jiao Tong University, China","Han, R., School of software, Shanghai Jiao Tong University, China; Xiao, S., School of software, Shanghai Jiao Tong University, China","Human visual perception is considered as a dynamic process of information acquisition, while the visual scanpath can clearly reflect the shift of our eye fixations. In the previous study of visual attention, researchers generally do the saliency computation to predict where the regions of interest locate in the given scene, whereas less considering how our eyes saccade during the saliency generation. In this paper, we propose a novel model based on visual attention mechanism to predict the human visual scanpath of the given 3D scene. Our scanpath prediction model that can reasonably estimate the sequence of eye fixations when eyes saccade contains three important factors: RGB-D saliency computation, oculomotor biases and inhibition of return(IOR). In addition, we construct a small RGB-D eye tracking dataset with collecting eye tracking records from 91 people on 30 RGB-D images for our comparison experiments. The experiments demonstrate that our approach provides better prediction on human visual scanpath. © 2018 Association for Computing Machinery.","Eye movements; Eye tracking; Saliency; Scanpath prediction; Visual attention","Behavioral research; Eye tracking; Forecasting; Image processing; Human visual perception; Information acquisitions; Inhibition of returns (IOR); Regions of interest; Saliency; Scan path; Visual Attention; Visual attention mechanisms; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85047296321
"Cui W., Cui J., Zha H.","57201582993;15623000600;7006639394;","Specialized gaze estimation for children by convolutional neural network and domain adaptation",2018,"Proceedings - International Conference on Image Processing, ICIP","2017-September",,,"3305","3309",,3,"10.1109/ICIP.2017.8296894","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045304282&doi=10.1109%2fICIP.2017.8296894&partnerID=40&md5=c1e01e55f50d7dd164450d4298a32dfd","Key Laboratory of Machine Perception(MOE), Peking University, Beijing, 100871, China","Cui, W., Key Laboratory of Machine Perception(MOE), Peking University, Beijing, 100871, China; Cui, J., Key Laboratory of Machine Perception(MOE), Peking University, Beijing, 100871, China; Zha, H., Key Laboratory of Machine Perception(MOE), Peking University, Beijing, 100871, China","Children's social gaze behavior modeling and evaluation has obtained increasing attentions in various research areas. In psychology research, eye gaze behavior is very important to developmental disorders diagnosis and assessment. In robotics area, gaze interaction between children and robots also draws more and more attention. However, there exists no specific gaze estimator for children in social interaction context. Current approaches usually use models trained with adults' data to estimate children's gaze. Since gaze behaviors and eye appearances of children are different from those of adults, the current approaches, especially those with free-calibration assumptions which are utilized in usual human-robot interaction systems, will result in big errors. Note that children data is difficult to collect and label, so directly learning from children data is hard to achieve. We propose a new system to solve this problem, which combines a CNN feature extractor trained from adult data and a domain adaptation unit using geodesic flow kernel to adapt the source domain (adults) classifier to the target domain (children). Our system performs well in children's gaze estimation. © 2017 IEEE.","Computer Vision for Automation; Gaze Tracking; Neural Network; Semi-Supervised Learning","Human computer interaction; Human robot interaction; Image processing; Neural networks; Supervised learning; Convolutional neural network; Developmental disorders; Domain adaptation; Feature extractor; Gaze estimation; Gaze interaction; Semi- supervised learning; Social interactions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85045304282
"Lee J., Muñoz M., Fridman L., Victor T., Reimer B., Mehler B.","57189577286;57191817598;57185179400;8512910800;7003475727;26423894400;","Investigating the correspondence between driver head position and glance location",2018,"PeerJ Computer Science","2018","2","e146","","",,4,"10.7717/peerj-cs.146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043402342&doi=10.7717%2fpeerj-cs.146&partnerID=40&md5=fad4facb3f707d307cc4be8b05b53ccf","AgeLab and New England University Transportation Center, Massachusetts Institute of Technology, Cambridge, MA, United States; Technical University of Munich, Munich, Germany; University of Augsburg, Augsburg, Germany; SAFER Vehicle and Traffic Safety Center, Chalmers, Göteborg, Sweden","Lee, J., AgeLab and New England University Transportation Center, Massachusetts Institute of Technology, Cambridge, MA, United States; Muñoz, M., AgeLab and New England University Transportation Center, Massachusetts Institute of Technology, Cambridge, MA, United States, Technical University of Munich, Munich, Germany, University of Augsburg, Augsburg, Germany; Fridman, L., AgeLab and New England University Transportation Center, Massachusetts Institute of Technology, Cambridge, MA, United States; Victor, T., SAFER Vehicle and Traffic Safety Center, Chalmers, Göteborg, Sweden; Reimer, B., AgeLab and New England University Transportation Center, Massachusetts Institute of Technology, Cambridge, MA, United States; Mehler, B., AgeLab and New England University Transportation Center, Massachusetts Institute of Technology, Cambridge, MA, United States","The relationship between a driver's glance orientation and corresponding head rotation is highly complex due to its nonlinear dependence on the individual, task, and driving context. This paper presents expanded analytic detail and findings from an effort that explored the ability of head pose to serve as an estimator for driver gaze by connecting head rotation data with manually coded gaze region data using both a statistical analysis approach and a predictive (i.e., machine learning) approach. For the latter, classification accuracy increased as visual angles between two glance locations increased. In other words, the greater the shift in gaze, the higher the accuracy of classification. This is an intuitive but important concept that we make explicit through our analysis. The highest accuracy achieved was 83% using the method of Hidden Markov Models (HMM) for the binary gaze classification problem of (a) glances to the forward roadway versus (b) glances to the center stack. Results suggest that although there are individual differences in head-glance correspondence while driving, classifier models based on head-rotation data may be robust to these differences and therefore can serve as reasonable estimators for glance location. The results suggest that driver head pose can be used as a surrogate for eye gaze in several key conditions including the identification of high-eccentricity glances. Inexpensive driver head pose tracking may be a key element in detection systems developed to mitigate driver distraction and inattention. © 2018 Lee et al.","Driver distraction; Glance classification; Head movements; Head-glance correspondence","Learning systems; Location; Markov processes; Accuracy of classifications; Classification accuracy; Driver distractions; Head movements; Head-glance correspondence; Head-pose tracking; Individual Differences; Nonlinear dependence; Hidden Markov models",Article,"Final","",Scopus,2-s2.0-85043402342
"Hagihara K., Taniguchi K., Abibouraguimane I., Itoh Y., Higuchi K., Otsuka J., Sugimoto M., Sato Y.","57201306663;57203801620;57201318083;56154865900;37055797600;57191975273;9638451700;35230954300;","Object-wise 3d gaze mapping in physicalworkspace",2018,"ACM International Conference Proceeding Series",,,"a25","","",,2,"10.1145/3174910.3174921","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044317398&doi=10.1145%2f3174910.3174921&partnerID=40&md5=1f6943c6acee16d686aa64a65df3e3ea","Keio University, Japan; University of Tokyo, Japan; Tokyo Institute of Technology, Japan, Japan; University of Tokyo, Japan, Japan","Hagihara, K., Keio University, Japan; Taniguchi, K., University of Tokyo, Japan; Abibouraguimane, I., Keio University, Japan; Itoh, Y., Tokyo Institute of Technology, Japan, Japan; Higuchi, K., University of Tokyo, Japan, Japan; Otsuka, J., Keio University, Japan; Sugimoto, M., Keio University, Japan; Sato, Y., University of Tokyo, Japan, Japan","skill in human communication. Eye behavior is an important, yet implicit communication cue. In this work, we focus on enabling people to see the users' gaze associated with objects in the 3D space, namely, we present users the history of gaze linked to real 3D objects. Our 3D gaze visualization system automatically segments objects in the workspace and projects user's gaze trajectory onto the objects in 3D for visualizing user's intention. By combining automated object segmentation and head tracking via the firstperson video from a wearable eye tracker, our system can visualize user's gaze behavior more intuitively and efficiently compared to 2D based methods and 3D methods with manual annotation. We performed an evaluation of the system to measure the accuracy of object-wise gaze mapping. In the evaluation, the system achieved 94% accuracy of gaze mapping onto 40, 30, 20, 10-centimeter cubes. We also conducted a case study of through a case study where the user looks at food products, we showed that our system was ableto predict products that the user is interested in. ©2018 Copyright.","3D gaze; 3D segmentation; Gaze mapping","Eye tracking; Food products; Mapping; Three dimensional computer graphics; 3D gaze; 3D segmentation; Human communications; Implicit communications; Manual annotation; Object segmentation; User's intentions; Visualization system; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85044317398
"Filipovic N., Milosevic Z., Saveljic I., Nikolic D., Zdravkovic N., Kos A.","35749660900;36975934300;55565816700;37006898600;24479207600;56238460500;","Biomechanical model for detection of vertigo disease",2018,"Proceedings - 2016 International Conference on Identification, Information and Knowledge in the Internet of Things, IIKI 2016","2018-January",,,"255","260",,,"10.1109/IIKI.2016.59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050891716&doi=10.1109%2fIIKI.2016.59&partnerID=40&md5=7d3fcc5359dcf7091c59dee3d05291a1","Faculty of Engineering, University of Kragujevac, Kragujevac, Serbia; Faculty of Medical Sceince, University of Kragujevac, Kragujevac, Serbia; Faculty of Electrical Engineering, University of Ljubljana, Ljubljana, Slovenia","Filipovic, N., Faculty of Engineering, University of Kragujevac, Kragujevac, Serbia; Milosevic, Z., Faculty of Engineering, University of Kragujevac, Kragujevac, Serbia; Saveljic, I., Faculty of Engineering, University of Kragujevac, Kragujevac, Serbia; Nikolic, D., Faculty of Engineering, University of Kragujevac, Kragujevac, Serbia; Zdravkovic, N., Faculty of Medical Sceince, University of Kragujevac, Kragujevac, Serbia; Kos, A., Faculty of Electrical Engineering, University of Ljubljana, Ljubljana, Slovenia","Benign Paroxysmal Positional Vertigo (BPPV) is most common vestibular disorder influencing the quality of life to considerable percentage of population after the age of forty. In this study the three-dimensional biomechanical model of the semi-circular canal (SCC) is described with full 3D fluid-structure interaction of particles, wall, cupula deformation and endolymph fluid flow. Oculus Rift device was used for experimental results of head motion and eye tracking and correlation with biomechanical model. A full Navier-Stokes equations and continuity equations are used for fluid domain with Arbitrary-Lagrangian Eulerian (ALE) formulation for mesh motion. Fluid-structure interaction for fluid coupling with cupula deformation is used. Particle tracking algorithm has been used for particle motion. Different size and number of particles with their full interaction between themselves, wall and cupula deformation are used. Velocity distribution, shear stress and force from endolymph side are presented for parametric one SCC and patient specific three SCC. All the models are used for correlation with the same experimental protocols with head moving and nystagmus eye tracking. A good correlation was found with numerical simulation of membrane deflection and nystagmus response detected with tracking technology. It can be used for virtual games with detection of vestibular disorders to the users. © 2016 IEEE.","Biomechanical model; BPPV; Component; Fluid-structure interaction; Oculus rift device; Sedimenting particle; Semi-circular canals","Biomechanics; Canals; Computational fluid dynamics; Deformation; Eye movements; Eye tracking; Fluid structure interaction; Hydraulic structures; Internet of things; Occupational diseases; Shear flow; Shear stress; Bio-mechanical models; BPPV; Component; Oculus rift device; Sedimenting; Navier Stokes equations",Conference Paper,"Final","",Scopus,2-s2.0-85050891716
"Castellanos J.L., Gomez M.F., Adams K.D.","57194006121;56904136700;12902675100;","Using machine learning based on eye gaze to predict targets: An exploratory study",2018,"2017 IEEE Symposium Series on Computational Intelligence, SSCI 2017 - Proceedings","2018-January",,,"1","7",,5,"10.1109/SSCI.2017.8285207","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046110504&doi=10.1109%2fSSCI.2017.8285207&partnerID=40&md5=9675cbff922a736b08734bc5ec3a50fa","Faculty of Rehabilitation Science, University of Alberta, Edmonton, Canada","Castellanos, J.L., Faculty of Rehabilitation Science, University of Alberta, Edmonton, Canada; Gomez, M.F., Faculty of Rehabilitation Science, University of Alberta, Edmonton, Canada; Adams, K.D., Faculty of Rehabilitation Science, University of Alberta, Edmonton, Canada","Play is a crucial activity for child development. Play in children with physical disabilities may be compromised due to their physical limitations, such as having difficulties reaching and manipulating objects. Assistive technology robotic systems have been used as tools for children with disabilities to play and interact with the environment. Robots have shown a positive impact on children's independence, cognitive, and social skills. The present study is the first stage of a project to develop a telerobotic haptic system, with the goal of supporting the reaching of toys during play by children with severe physical disabilities. The end goal is to provide haptic guidance towards the toys that the children want to play with. The objective of this paper was to investigate the feasibility of predicting the selection of targets in a three-block task. This prediction was based on the Point of Gaze (POG) data of five participants while performing the task using a telerobotic haptic system. Two fixation-based algorithms, longest fixation and last fixation, and two learning algorithms, a Double Q-learning and a Multi-Layer Perceptron neural network, were implemented, tested, and compared. Results showed that the learning algorithms were better at predicting the targets than the fixation-based algorithms, with above 92% accuracy. This demonstrated that the learning algorithms can be utilized for activating haptic guidance towards the targets (toys). © 2017 IEEE.","Double Q-Learning; Eye tracking; Fixation; Haptics; Machine learning; Multi-Layer Perceptron; Point of gaze","Artificial intelligence; Eye tracking; Forecasting; Industrial robots; Learning systems; Network layers; Nitrogen fixation; Assistive technology; Children with disabilities; Haptics; Multi layer perceptron; Multi-layer perceptron neural networks; Physical limitations; Point of gaze; Q-learning; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85046110504
"Cazzato D., Dominio F., Manduchi R., Castro S.M.","55866556300;55914420500;7004297978;55171401100;","Real-time gaze estimation via pupil center tracking",2018,"Paladyn","9","1",,"6","18",,4,"10.1515/pjbr-2018-0002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048051474&doi=10.1515%2fpjbr-2018-0002&partnerID=40&md5=fdb5dc056c66f48f84c0d42313e8fd69","Interdisciplinary Centre for Security, Reliability and Trust, University of LuxembourgL-1359, Luxembourg; Urbana Smart Solutions Pte. Ltd, Singapore; Computer Engineering Department, University of California, Santa Cruz, United States; Universidad Nacional del Sur, Bahía Blanca, Argentina","Cazzato, D., Interdisciplinary Centre for Security, Reliability and Trust, University of LuxembourgL-1359, Luxembourg; Dominio, F., Urbana Smart Solutions Pte. Ltd, Singapore; Manduchi, R., Computer Engineering Department, University of California, Santa Cruz, United States; Castro, S.M., Universidad Nacional del Sur, Bahía Blanca, Argentina","Automatic gaze estimation not based on commercial and expensive eye tracking hardware solutions can enable several applications in the fields of human-computer interaction (HCI) and human behavior analysis. It is therefore not surprising that several related techniques and methods have been investigated in recent years. However, very few camera-based systems proposed in the literature are both real-time and robust. In this work, we propose a real-time user-calibration-free gaze estimation system that does not need person-dependent calibration, can deal with illumination changes and head pose variations, and can work with a wide range of distances from the camera. Our solution is based on a 3-D appearance-based method that processes the images from a built-in laptop camera. Real-time performance is obtained by combining head pose information with geometrical eye features to train a machine learning algorithm. Our method has been validated on a data set of images of users in natural environments, and shows promising results. The possibility of a real-time implementation, combined with the good quality of gaze tracking, make this system suitable for various HCI applications. © 2018 De Gruyter Open Ltd. All rights reserved.","Appearance-based method; Gaze estimation; Pupil tracking; Regression tree",,Article,"Final","",Scopus,2-s2.0-85048051474
"Jie L., Jian C., Lei W.","57203192691;57199888236;57203183507;","Design of multi-mode UAV human-computer interaction system",2018,"Proceedings of 2017 IEEE International Conference on Unmanned Systems, ICUS 2017","2018-January",,,"353","357",,4,"10.1109/ICUS.2017.8278368","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050858002&doi=10.1109%2fICUS.2017.8278368&partnerID=40&md5=9551335fabd6d09ad4818588f175ed24","Armament Science and Technology Department, Naval Aeronautical University, Yantai, Shandong, China","Jie, L., Armament Science and Technology Department, Naval Aeronautical University, Yantai, Shandong, China; Jian, C., Armament Science and Technology Department, Naval Aeronautical University, Yantai, Shandong, China; Lei, W., Armament Science and Technology Department, Naval Aeronautical University, Yantai, Shandong, China","With continuous function expansion of military UAV, the rapid increase in the number of airborne sensors, the complexity of UAV operation is increasing. To enhance future unmanned combat aircraft control and mission planning efficiency, multi-mode UAV control method will gradually become mainstream. In this paper a Multi-Mode UAV Human-Computer Interaction(HCI) System was designed with new technology achievements of virtual reality and artificial intelligence. Based on the design principle of usability, maintainability and reliability, according to the envisaged functions, the overall design of the system was completed. The four modules that divided into the base layer and the application layer with hierarchical structure was introduced. The base layer was composed of multi-mode HCI device and data recording processing subsystem. Acting as the system hardware I/O interface, the multi-mode HCI device use multi-mode interaction of immersion display, motion capture, eye tracking and voice recognition by means of intelligent devices such as VR HMD, 3D Sensor, VR Glove and intelligent speech recognition equipment. And according to the actual situation of GCS operation, the above hardware's selection was completed. Data recording processing subsystem provides the network and database to support the underlying. Human factors engineering evaluation subsystem and UAV simulation training subsystem constitute the function application layer, realize the system functions. The whole system can provide multi-mode interaction human factors engineering validation and evaluation platform, supporting the development of new HCI design, and high-frontal manned UAV operation training environment for supporting the new CGS ground operator personalized training. © 2017 IEEE.","Ground Control System(GCS); Human-Computer Interaction(HCI); Multi-Mode; UAV","Aircraft control; Computer control systems; Computer hardware; Data handling; Data recording; Display devices; Eye tracking; Flight control systems; Function evaluation; Hardware; Human engineering; Personnel training; Rock mechanics; Speech recognition; Training aircraft; Unmanned aerial vehicles (UAV); Virtual reality; Continuous functions; Ground control system; Hierarchical structures; Human Computer Interaction (HCI); Human-computer interaction system; Multi-mode interactions; Multimodes; Unmanned combat aircrafts; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85050858002
"Liu M., Wu W., Gu Z., Yu Z., Qi F., Li Y.","57195672820;56529849100;57204214680;57221260509;56530145300;55936283000;","Deep learning based on Batch Normalization for P300 signal detection",2018,"Neurocomputing","275",,,"288","297",,92,"10.1016/j.neucom.2017.08.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029534055&doi=10.1016%2fj.neucom.2017.08.039&partnerID=40&md5=97fc17035463e8d728ff4888951e0863","School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China","Liu, M., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Wu, W., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Gu, Z., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Yu, Z., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Qi, F., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Li, Y., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China","Detecting P300 signals from electroencephalography (EEG) is the key to establishing a P300 speller, which is a type of brain–computer interface (BCI) system based on the oddball paradigm that allows users to type messages simply by controlling eye-gazes. The convolutional neural network (CNN) is an approach that has achieved good P300 detection performances. However, the standard CNN may be prone to overfitting and the convergence may be slow. To address these issues, we develop a novel CNN, termed BN3, for detecting P300 signals, where Batch Normalization is introduced in the input and convolutional layers to alleviate over-fitting, and the rectified linear unit (ReLU) is employed in the convolutional layers to accelerate training. Since our model is fully data-driven, it is capable of automatically capturing the discriminative spatio-temporal features of the P300 signal. The results obtained on previous BCI competition P300 data sets show that BN3 both achieves the state-of-the-art character recognition performance and that it outperforms existing detection approaches with small flashing epoch numbers. BN3 can be used to improve the character recognition performance in P300 speller systems. © 2017 Elsevier B.V.","Brain–computer interface (BCI); Convolutional neural network (CNN); Deep learning; P300","Biomedical signal processing; Character recognition; Convolution; Deep learning; Electroencephalography; Electrophysiology; Interfaces (computer); Neural networks; Signal detection; Convolutional neural network; Detection approach; Detection performance; Linear units; Oddball paradigms; P300; Spatio temporal features; State of the art; Brain computer interface; Article; artificial neural network; batch normalization; batch normalized neural network; brain computer interface; deep learning; electroencephalography; event related potential; machine learning; mathematical computing; mathematical model; priority journal; recognition; signal detection",Article,"Final","",Scopus,2-s2.0-85029534055
"Lee K., Kim H., Suh C.","55638913300;34770685700;12238909500;","Simulated+unsupervised learning with adaptive data generation and bidirectional mappings",2018,"6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings",,,,"","",,10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083954405&partnerID=40&md5=199b28c1d2238dd35f9cf20d24766cbb","School of Electrical Engineering, KAIST, Daejeon, South Korea","Lee, K., School of Electrical Engineering, KAIST, Daejeon, South Korea; Kim, H., School of Electrical Engineering, KAIST, Daejeon, South Korea; Suh, C., School of Electrical Engineering, KAIST, Daejeon, South Korea","Collecting a large dataset with high quality annotations is expensive and time-consuming. Recently, Shrivastava et al. (2017) propose Simulated+Unsupervised (S+U) learning: It first learns a mapping from synthetic data to real data, translates a large amount of labeled synthetic data to the ones that resemble real data, and then trains a learning model on the translated data. Bousmalis et al. (2017b) propose a similar framework that jointly trains a translation mapping and a learning model. While these algorithms are shown to achieve the state-of-the-art performances on various tasks, it may have a room for improvement, as they do not fully leverage flexibility of data simulation process and consider only the forward (synthetic to real) mapping. Inspired by this limitation, we propose a new S+U learning algorithm, which fully leverage the flexibility of data simulators and bidirectional mappings between synthetic and real data. We show that our approach achieves the improved performance on the gaze estimation task, outperforming (Shrivastava et al., 2017). © Learning Representations, ICLR 2018 - Conference Track Proceedings.All right reserved.",,"Large dataset; Machine learning; Mapping; Bidirectional mapping; Data generation; Data simulation; Data simulators; Gaze estimation; Learning models; State-of-the-art performance; Synthetic and real data; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85083954405
"Matsumoto T., Kusafuka K., Nakamura H., Hamagishi G., Yoshimoto K., Takahashi H.","57203999911;57203998567;57208888581;6603319161;56492328000;7405468853;","Low crosstalk Glassless 3D HUD with expanded viewing area in all directions using novel eye tracking system",2018,"Proceedings of the International Display Workshops","2",,,"799","802",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072118377&partnerID=40&md5=016921aa3b661125fbed5a219afaabfd","Kyocera Corporation, 402-1 Nakayama-cho, Midori-ku, Yokohama, 226-8512, Japan; Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan","Matsumoto, T., Kyocera Corporation, 402-1 Nakayama-cho, Midori-ku, Yokohama, 226-8512, Japan; Kusafuka, K., Kyocera Corporation, 402-1 Nakayama-cho, Midori-ku, Yokohama, 226-8512, Japan; Nakamura, H., Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Hamagishi, G., Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Yoshimoto, K., Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Takahashi, H., Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan","We propose Glassless 3D HUD that enables binocular virtual image stereopsis with low crosstalk. In this system, 3D image processing algorithm and the crosstalk reduction method which perform processing based on the position of the driver's eye sensed by the driver monitor camera are applied. © 2018 International Display Workshops. All rights reserved.","3D; Eye tracking; Glassless; HUD (Head Up Display); Parallax barrier","Crosstalk; Geometrical optics; Glass; Head-up displays; Image processing; 3-D image processing; Crosstalk reduction; Eye tracking systems; Glassless; HUD (Head Up Display); Monitor cameras; Parallax barriers; Virtual images; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85072118377
"Nakamura H., Hamagishi G., Yoshimoto K., Takahashi H., Matsumoto T., Kusafuka K.","57208888581;6603319161;56492328000;7405468853;57203999911;57203998567;","A novel eye tracking system to expand viewing area in all directions for glasses-free 3D display displayable in both portrait and landscape modes",2018,"Proceedings of the International Display Workshops","2",,,"792","795",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066039843&partnerID=40&md5=841fe00b04fb69f22f8e2a88d24720e0","Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Kyocera Corporation, 402-1 Nakayama-cho, Midori-ku, Yokohama, 226-8512, Japan","Nakamura, H., Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Hamagishi, G., Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Yoshimoto, K., Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Takahashi, H., Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Matsumoto, T., Kyocera Corporation, 402-1 Nakayama-cho, Midori-ku, Yokohama, 226-8512, Japan; Kusafuka, K., Kyocera Corporation, 402-1 Nakayama-cho, Midori-ku, Yokohama, 226-8512, Japan","We propose a novel eye tracking system to expand the viewing area in all directions for glasses-free 3D display displayable in both portrait and landscape modes. Its viewing area is extremely expanded by dividing single screen into multiple areas and controlling binocular images positions of each area. © 2018 International Display Workshops. All rights reserved.","Eye tracking; Glasses-free; Portrait and landscape; Stereoscopic; Tablet","Eye tracking; Glass; Stereo image processing; Eye tracking systems; Glasses-free 3-D displays; Multiple areas; Portrait and landscape; Single screen; Stereoscopic; Tablet; Viewing area; Three dimensional displays",Conference Paper,"Final","",Scopus,2-s2.0-85066039843
"Bambach S., Crandall D.J., Smith L.B., Yu C.","55975996000;8732077700;7410395494;16032623800;","Toddler-inspired visual object learning",2018,"Advances in Neural Information Processing Systems","2018-December",,,"1201","1210",,12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064825990&partnerID=40&md5=fc33fa6587723d3103cda9f3e52535f8","School of Informatics, Computing, and Engineering, United States; Dept. of Psychological and Brain Sciences, Indiana University, Bloomington, United States","Bambach, S., School of Informatics, Computing, and Engineering, United States; Crandall, D.J., School of Informatics, Computing, and Engineering, United States; Smith, L.B., Dept. of Psychological and Brain Sciences, Indiana University, Bloomington, United States; Yu, C., Dept. of Psychological and Brain Sciences, Indiana University, Bloomington, United States","Real-world learning systems have practical limitations on the quality and quantity of the training datasets that they can collect and consider. How should a system go about choosing a subset of the possible training examples that still allows for learning accurate, generalizable models? To help address this question, we draw inspiration from a highly efficient practical learning system: the human child. Using head-mounted cameras, eye gaze trackers, and a model of foveated vision, we collected first-person (egocentric) images that represent a highly accurate approximation of the ""training data"" that toddlers' visual systems collect in everyday, naturalistic learning contexts. We used state-of-the-art computer vision learning models (convolutional neural networks) to help characterize the structure of these data, and found that child data produce significantly better object models than egocentric data experienced by adults in exactly the same environment. By using the CNNs as a modeling tool to investigate the properties of the child data that may enable this rapid learning, we found that child data exhibit a unique combination of quality and diversity, with not only many similar large, high-quality object views but also a greater number and diversity of rare views. This novel methodology of analyzing the visual ""training data"" used by children may not only reveal insights to improve machine learning, but also may suggest new experimental tools to better understand infant learning in developmental psychology. © 2018 Curran Associates Inc..All rights reserved.",,"Human computer interaction; Neural networks; Convolutional neural network; Developmental psychology; Eye gaze trackers; Head mounted Camera; Learning context; Novel methodology; Real-world learning; Training data sets; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85064825990
"Milosevic Z., Saveljic I., Nikolic D., Zdravkovic N., Filipovic N., Vidanovic N.","36975934300;55565816700;37006898600;24479207600;35749660900;57201365359;","Three-Dimensional Computer Model of Benign Paroxysmal Positional Vertigo in the Semi-Circular Canal",2018,"EAI Endorsed Transactions on Pervasive Health and Technology","4","13","e1","","",,,"10.4108/eai.28-2-2018.154142","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064603305&doi=10.4108%2feai.28-2-2018.154142&partnerID=40&md5=b8ff0a52ee528cf5167b1ef923214d4c","Faculty of Engineering, University of Kragujevac, Kragujevac, 34000, Serbia; Faculty of Medical Science, University of Kragujevac, Kragujevac, 34000, Serbia; BioIRC Research and Development Center for Bioengineering, Kragujevac, 34000, Serbia","Milosevic, Z., Faculty of Engineering, University of Kragujevac, Kragujevac, 34000, Serbia, BioIRC Research and Development Center for Bioengineering, Kragujevac, 34000, Serbia; Saveljic, I., Faculty of Engineering, University of Kragujevac, Kragujevac, 34000, Serbia, BioIRC Research and Development Center for Bioengineering, Kragujevac, 34000, Serbia; Nikolic, D., Faculty of Engineering, University of Kragujevac, Kragujevac, 34000, Serbia, BioIRC Research and Development Center for Bioengineering, Kragujevac, 34000, Serbia; Zdravkovic, N., Faculty of Medical Science, University of Kragujevac, Kragujevac, 34000, Serbia; Filipovic, N., Faculty of Engineering, University of Kragujevac, Kragujevac, 34000, Serbia, BioIRC Research and Development Center for Bioengineering, Kragujevac, 34000, Serbia; Vidanovic, N., Faculty of Engineering, University of Kragujevac, Kragujevac, 34000, Serbia, BioIRC Research and Development Center for Bioengineering, Kragujevac, 34000, Serbia","Benign Paroxysmal Positional Vertigo (BPPV) is the most common vestibular disorder. In this paper we tried to investigate a model of the semi-circular canal (SCC) with parametrically defined dimension and full 3D three SCC from patient-specific 3D reconstruction. Full Navier-Stokes equations and continuity equations are used for fluid domain with Arbitrary-Lagrangian Eulerian (ALE) formulation for mesh motion. Fluid-structure interaction for fluid coupling with cupula deformation is used. Particle tracking algorithm has been used for particle motion. Velocity distribution, shear stress and force from endolymph side are presented for one parametric SCC and three patient-specific SCC. All models are used for correlation with the same experimental protocols with head moving and nystagmus eye tracking. © 2018 Zarko Milosevic et al.","Biomechanical model; BPPV; Fluid-structure interaction; Sedimenting particle; Semi-circular canals","Canals; Eye tracking; Fluid structure interaction; Hydraulic structures; Motion tracking; Navier Stokes equations; Occupational diseases; Shear flow; Shear stress; 3D reconstruction; Arbitrary Lagrangian Eulerian formulations; Benign paroxysmal positional vertigo; Bio-mechanical models; BPPV; Continuity equations; Experimental protocols; Sedimenting; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85064603305
"González-Garduño A.V., Søgaard A.","57205543879;24336006300;","Learning to predict readability using eye-movement data from natives and learners",2018,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",,,,"5118","5124",,7,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060464573&partnerID=40&md5=ab375a27f974a6826569cd82b80a3f7a","Department of Computer Science, University of Copenhagen, Denmark","González-Garduño, A.V., Department of Computer Science, University of Copenhagen, Denmark; Søgaard, A., Department of Computer Science, University of Copenhagen, Denmark","Readability assessment can improve the quality of assisting technologies aimed at language learners. Eye-tracking data has been used for both inducing and evaluating general-purpose NLP/AI models, and below we show that unsurprisingly, gaze data from language learners can also improve multi-task readability assessment models. This is unsurprising, since the gaze data records the reading difficulties of the learners. Unfortunately, eye-tracking data from language learners is often much harder to obtain than eye-tracking data from native speakers. We therefore compare the performance of deep learning readability models that use native speaker eye movement data to models using data from language learners. Somewhat surprisingly, we observe no significant drop in performance when replacing learners with natives, making approaches that rely on native speaker gaze information, more scalable. In other words, our finding is that language learner difficulties can be efficiently estimated from native speakers, which suggests that, more generally, readily available gaze data can be used to improve educational NLP/AI models targeted towards language learners. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Deep learning; Eye movements; Natural language processing systems; Assessment models; Eye movement datum; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85060464573
"Tiwari A., Pal R.","57212912235;57204588196;","Gaze-Based Graphical Password Using Webcam",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11281 LNCS",,,"448","461",,1,"10.1007/978-3-030-05171-6_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059028852&doi=10.1007%2f978-3-030-05171-6_23&partnerID=40&md5=829f995f85539b3772226f9741c1dc5a","Institute for Development and Research in Banking Technology (IDRBT), Hyderabad, India; University of Hyderabad, Hyderabad, India","Tiwari, A., Institute for Development and Research in Banking Technology (IDRBT), Hyderabad, India, University of Hyderabad, Hyderabad, India; Pal, R., Institute for Development and Research in Banking Technology (IDRBT), Hyderabad, India","Authentication refers to verification of the identity of an user. There exist various types of authentication techniques, starting from simple password based authentication up to behavior biometric based authentication. In this paper, a new way of authentication is proposed where the user provides her password through eye gaze. It is based on graphical password scheme where she can choose her password from a large image data set. At the time of authentication, she needs to recall it and look at the chosen passcode appearing in a display in correct sequence. The method uses a machine learning technique where a convolutional neural network is used to determine gaze locations using inputs from a simple web camera. It takes her cropped eye as input and provides gaze location as output. Proposed method is cost effective solution as gaze tracking is done through a simple web camera. The proposed method is also free from attacks such as shoulder surfing, smudge, brute-force attacks. Experiments have been carried out to validate the system. It has been observed to perform accurately for all the volunteers. © 2018, Springer Nature Switzerland AG.","Authentication; Gaze tracking; Graphical password","Cameras; Cost effectiveness; Eye tracking; Information systems; Information use; Learning systems; Neural networks; Authentication techniques; Biometric based authentication; Brute-force attack; Convolutional neural network; Graphical password; Machine learning techniques; Password-based authentication; Shoulder surfing; Authentication",Conference Paper,"Final","",Scopus,2-s2.0-85059028852
"Sharma A., Kolay S., Ghosh J.K.","57210974678;56940605000;34968946700;","Dominance of perceptual grouping over functional category: An eye tracking study of high-resolution satellite images",2018,"Proceedings of SPIE - The International Society for Optical Engineering","10789",,"107891E","","",,,"10.1117/12.2325050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058989431&doi=10.1117%2f12.2325050&partnerID=40&md5=dfc460a73b06c0928b6e5a2fd0c44b2b","Civil Engineering Department, Indian Institute of Technology, Roorkee, 247667, India; Department of Architecture and Planning, Indian Institute of Technology, Roorkee, India","Sharma, A., Civil Engineering Department, Indian Institute of Technology, Roorkee, 247667, India; Kolay, S., Department of Architecture and Planning, Indian Institute of Technology, Roorkee, India; Ghosh, J.K., Civil Engineering Department, Indian Institute of Technology, Roorkee, 247667, India","The presented study is an attempt to determine the object grouping propensity of human visual system for further human-like analysis of High-Resolution Satellite (HRS) image by using an eye tracker. Perception, which is a basic characteristic of the human visual system, enables to part the visual stimuli into meaningful parts. This parting is either based on perceptual or functional grouping. However, these perceptual groups are often different from functional categories. All objects from one functional category are perceived equivalent, irrespective of their physical appearance variation. The recent advancements in eye tracking technology have provided an excellent modality to articulate the eye movement data with inner psychological factors. In this way, the presented study has exploited the fixation data to understand the perceptual organization for HRS images. The outcomes of this study has provided the analytical solution for grouping tendency of humans along with the heat maps as evidence. © 2018 SPIE.","Eye tracking; Free viewing; Functional category; High resolution satellite image; Perceptual grouping","Eye movements; Image processing; Remote sensing; Satellites; Eye tracking technologies; Free viewing; Functional category; High resolution satellite images; High resolution satellites; Perceptual grouping; Perceptual organization; Psychological factors; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85058989431
"Schweikert C., Gobin L., Xie S., Shimojo S., Frank Hsu D.","35146659300;57205100797;57205095111;7005960997;7202341455;","Preference prediction based on eye movement using multi-layer combinatorial fusion",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11309 LNAI",,,"282","293",,6,"10.1007/978-3-030-05587-5_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058569821&doi=10.1007%2f978-3-030-05587-5_27&partnerID=40&md5=988b1b1102952d9eed645a77452b195e","Division of Computer Science, Mathematics and Science, St. John’s University, Queens, NY  11439, United States; Laboratory of Informatics and Data Mining, Department of Computer and Information Science, Fordham University, New York, NY  10023, United States; Division of Biology and Biological Engineering/Computation and Neural Systems, California Institute of Technology, Pasadena, CA  91125, United States","Schweikert, C., Division of Computer Science, Mathematics and Science, St. John’s University, Queens, NY  11439, United States; Gobin, L., Laboratory of Informatics and Data Mining, Department of Computer and Information Science, Fordham University, New York, NY  10023, United States; Xie, S., Laboratory of Informatics and Data Mining, Department of Computer and Information Science, Fordham University, New York, NY  10023, United States; Shimojo, S., Division of Biology and Biological Engineering/Computation and Neural Systems, California Institute of Technology, Pasadena, CA  91125, United States; Frank Hsu, D., Laboratory of Informatics and Data Mining, Department of Computer and Information Science, Fordham University, New York, NY  10023, United States","Face image preference is influenced by many factors and can be detected by analyzing eye movement data. When comparing two face images, our gaze shifts within and between the faces. Eye tracking data can give us insights into the cognitive processes involved in forming a preference. In this paper, a gaze tracking dataset is analyzed using three machine learning algorithms (MLA): AdaBoost, Random Forest, and Mixed Group Ranks (MGR) as well as a newly developed machine learning framework called Multi-Layer Combinatorial Fusion (MCF) to predict a subject’s face image preference. Attributes constructed from the dataset are treated as input scoring systems. MCF involves a series of layers that consist of expansion and reduction processes. The expansion process involves performing exhaustive score and rank combinations, while the reduction process uses performance and diversity to select a subset of systems that will be passed onto the next layer of analysis. Performance and cognitive diversity are used in weighted scoring system combinations and system selection. The results outperform the Mixed Group Ranks algorithm, as well as our previous work using pairwise scoring system combinations. © 2018, Springer Nature Switzerland AG.","Cognitive diversity; Combinatorial Fusion Analysis (CFA); Machine learning; Multi-layer Combinatorial Fusion (MCF); Preference detection","Adaptive boosting; Artificial intelligence; Data reduction; Decision trees; Expansion; Eye tracking; Image analysis; Learning systems; Cognitive diversity; Cognitive process; Combinatorial Fusion Analysis (CFA); Expansion process; Eye movement datum; Preference predictions; Reduction process; System selection; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85058569821
"Vera-Olmos F.J., Pardo E., Melero H., Malpica N.","57195328347;57195324837;56563935200;6603593205;","DeepEye: Deep convolutional network for pupil detection in real environments",2018,"Integrated Computer-Aided Engineering","26","1",,"85","95",,47,"10.3233/ICA-180584","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058542215&doi=10.3233%2fICA-180584&partnerID=40&md5=6d2e54d8ddd0e1324be6fc536b5f929b","Laboratorio de Análisis de Imagen Médica y Biometría, Universidad Rey Juan Carlos, Calle Tulipán s/n, Madrid, 28933, Spain","Vera-Olmos, F.J., Laboratorio de Análisis de Imagen Médica y Biometría, Universidad Rey Juan Carlos, Calle Tulipán s/n, Madrid, 28933, Spain; Pardo, E., Laboratorio de Análisis de Imagen Médica y Biometría, Universidad Rey Juan Carlos, Calle Tulipán s/n, Madrid, 28933, Spain; Melero, H., Laboratorio de Análisis de Imagen Médica y Biometría, Universidad Rey Juan Carlos, Calle Tulipán s/n, Madrid, 28933, Spain; Malpica, N., Laboratorio de Análisis de Imagen Médica y Biometría, Universidad Rey Juan Carlos, Calle Tulipán s/n, Madrid, 28933, Spain","Robust identification and tracking of the pupil provides key information that can be used in several applications such as controlling gaze-based HMIs (human machine interfaces), designing new diagnostic tools for brain diseases, improving driver safety, detecting drowsiness, performing cognitive research, among others. We propose a deep convolutional neural network for eye-Tracking based on atrous convolutions and spatial pyramids. DeepEye is able to handle real world problems such as varying illumination, blurring and reflections. The proposed network was trained and evaluated on 94,000 images taken from 24 data sets recorded in real world scenarios. DeepEye outperforms previous eye-Tracking methods tested with these data sets. It improves the results of the current state of the art in a 26%, achieving an accuracy of more than 70% in almost every data set in terms of percentage of pupils detected with a distance error lower than 5 pixels. DeepEye can be downloaded at: https://github.com/Fjaviervera/DeepEye. © 2019-IOS Press and the authors. All rights reserved.","atrous; convolution; deep learning; Eye-Tracking; network","Convolution; Deep learning; Deep neural networks; Diagnosis; Human computer interaction; Networks (circuits); Neural networks; atrous; Convolutional networks; Deep convolutional neural networks; Eye tracking methods; Human Machine Interface; Real-world problem; Real-world scenario; Robust identification; Eye tracking",Article,"Final","",Scopus,2-s2.0-85058542215
"Ortiz J.S., Palacios-Navarro G., Carvajal C.P., Andaluz V.H.","56425030300;57211534046;57193056785;36163031900;","3D virtual path planning for people with amyotrophic lateral sclerosis through standing wheelchair",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11357 LNAI",,,"181","191",,2,"10.1007/978-3-030-05204-1_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058303174&doi=10.1007%2f978-3-030-05204-1_18&partnerID=40&md5=174c63d9ebf54bc45a1da5b04c1c7895","Department of Electronic Engineering and Communications, University of Zaragoza, Zaragoza, Spain; Universidad de Las Fuerzas Armadas ESPE, Sangolquí, Ecuador","Ortiz, J.S., Department of Electronic Engineering and Communications, University of Zaragoza, Zaragoza, Spain, Universidad de Las Fuerzas Armadas ESPE, Sangolquí, Ecuador; Palacios-Navarro, G., Department of Electronic Engineering and Communications, University of Zaragoza, Zaragoza, Spain; Carvajal, C.P., Universidad de Las Fuerzas Armadas ESPE, Sangolquí, Ecuador; Andaluz, V.H., Universidad de Las Fuerzas Armadas ESPE, Sangolquí, Ecuador","This article presents the development of an autonomous control system of an electric standing wheelchair for people with amyotrophic lateral sclerosis. The proposed control scheme is based on the autonomous maneuverability of the standing wheelchair, for which a path planner is implemented to which the desired 3D position is defined through the eye-tracking sensor. The eye-tracking is implemented in a virtual reality environment which allows selecting the desired position of the standing wheelchair. The wheelchair has a standing system that allows the user to position himself on the Z axis according to his needs independently of the displacement in the X-Y plane with respect to the inertial reference system <R>. To verify the performance of the proposed control scheme, several experimental tests are carried out. © 2018, Springer Nature Switzerland AG.","Amyotrophic lateral sclerosis; Standing wheelchair; Unity 3D; Virtual reality","Eye tracking; Flight control systems; Neurodegenerative diseases; Robotics; Virtual reality; Wheelchairs; Amyotrophic lateral sclerosis; Autonomous control systems; Control schemes; Desired position; Experimental test; Eye-tracking sensors; Inertial reference systems; Virtual-reality environment; Motion planning",Conference Paper,"Final","",Scopus,2-s2.0-85058303174
"Qiu S., Han T., Rauterberg M., Hu J.","57188752278;55425962400;6701479494;56442891500;","Impact of simulated gaze gestures on social interaction for people with visual impairments",2018,"Advances in Transdisciplinary Engineering","7",,,"249","258",,2,"10.3233/978-1-61499-898-3-249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057994253&doi=10.3233%2f978-1-61499-898-3-249&partnerID=40&md5=0be06a38f4121bfb8ae0c366cb5d3d48","Department of Industrial Design, Eindhoven University of Technology, Eindhoven, Netherlands; Department of Design, Shanghai Jiao Tong University, Shanghai, China","Qiu, S., Department of Industrial Design, Eindhoven University of Technology, Eindhoven, Netherlands; Han, T., Department of Design, Shanghai Jiao Tong University, Shanghai, China; Rauterberg, M., Department of Industrial Design, Eindhoven University of Technology, Eindhoven, Netherlands; Hu, J., Department of Industrial Design, Eindhoven University of Technology, Eindhoven, Netherlands","Gaze and eye contact have important social meanings in our daily lives. The sighted often uses gaze gestures in communication to convey nonverbal information that a blind interlocutor cannot access and react to. In many examples, blind people’s eyes are unattractive, and often with deformities, which makes the eye appearance less appealing to the sighted. All of these factors influence the smooth face-to-face communication between the blind and sighted people, which leads to blind people’s poor adaptions in the communication. We implemented a working prototype, namely E-Gaze (glasses), an assistive device based on an eye tracking system. E-Gaze attempts to simulate the natural gaze for blind people, especially establishing the “eye contact” between the blind and sighted people to enhance the engagement in the face-to-face communication. The interactive gaze behaviors of the E-Gaze are based on a model that combines a turn-taking strategy and the eye-contact mechanism. In order to test the impact of the interactive gaze model in the face-to-face communication, we conducted an experiment with sixteen participants. In the user experiment, participants had a monologue with a dummy wearing the E-Gaze. Two monologues took place under two experimental conditions (i.e., Interactive Gaze and Random Gaze) with counter balancing to avoid carry-over effects. Results well support the hypothesis that the interactive gaze model of the E-Gaze can enable the sighted to feel attention from the listener, enhancing the level of engagement in the face-to-face communication. We also obtain insights and design implications from participants’ comments. © 2018 The authors and IOS Press.","Communication quality; Eye tracking; Social interaction; Visual impairment","Balancing; Communication quality; Design implications; Experimental conditions; Eye tracking systems; Face-to-face communications; Non-verbal information; Social interactions; Visual impairment; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85057994253
"Wu J., Zhong S.-H., Ma Z., Heinen S.J., Jiang J.","57189367891;36844960400;57220884718;7003536110;57193403110;","Gaze aware deep learning model for video summarization",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11165 LNCS",,,"285","295",,3,"10.1007/978-3-030-00767-6_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057268424&doi=10.1007%2f978-3-030-00767-6_27&partnerID=40&md5=b0f061ebf99ff6425583ed4cc2ef3f50","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; The Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States","Wu, J., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Zhong, S.-H., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Ma, Z., The Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; Heinen, S.J., The Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; Jiang, J., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China","Video summarization is an ideal tool for skimming videos. Previous computational models extract explicit information from the input video, such as visual appearance, motion or audio information, in order to generate informative summaries. Eye gaze information, which is an implicit clue, has proved useful for indicating important content and the viewer’s interest. In this paper, we propose a novel gaze-aware deep learning model for video summarization. In our model, the position and velocity of the observers’ raw eye movements are processed by the deep neural network to indicate the users’ preferences. Experiments on two widely used video summarization datasets show that our model is more proficient than state-of-the-art methods in summarizing video for characterizing general preferences as well as for personal preferences. The results provide an innovative and improved algorithm for using gaze information in video summarization. © Springer Nature Switzerland AG 2018.","Convolutional neural networks; Gaze information; Video summarization","Eye movements; Neural networks; Video recording; Audio information; Computational model; Convolutional neural network; Explicit information; Gaze information; State-of-the-art methods; Video summarization; Visual appearance; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85057268424
"Kim H., Garrido P., Tewari A., Xu W., Thies J., Niessner M., Pérez P., Richardt C., Zollhöfer M., Theobalt C.","57193155212;57196554116;57200618272;57195993759;56312656700;35772871600;55431049800;35181960300;36245738500;6507027272;","Deep video portraits",2018,"ACM Transactions on Graphics","37","4","163","","",,133,"10.1145/3197517.3201283","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056648137&doi=10.1145%2f3197517.3201283&partnerID=40&md5=f9fb32e6290839356a315d1551cb2867","Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany; Technicolor, 975 Avenue des Champs Blancs, Cesson-Sévigné, 35576, France; Technical University of Munich, Boltzmannstraße 3, Garching, 85748, Germany; University of Bath, Claverton Down, Bath, BA2 7AY, United Kingdom; Stanford University, 353 Serra Mall, Stanford, CA  94305, United States","Kim, H., Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany; Garrido, P., Technicolor, 975 Avenue des Champs Blancs, Cesson-Sévigné, 35576, France; Tewari, A., Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany; Xu, W., Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany; Thies, J., Technical University of Munich, Boltzmannstraße 3, Garching, 85748, Germany; Niessner, M., Technical University of Munich, Boltzmannstraße 3, Garching, 85748, Germany; Pérez, P., Technicolor, 975 Avenue des Champs Blancs, Cesson-Sévigné, 35576, France; Richardt, C., University of Bath, Claverton Down, Bath, BA2 7AY, United Kingdom; Zollhöfer, M., Stanford University, 353 Serra Mall, Stanford, CA  94305, United States; Theobalt, C., Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany","We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted to manipulations of facial expressions only, we are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor. The realism in this rendering-to-video transfer is achieved by careful adversarial training, and as a result, we can create modified target videos that mimic the behavior of the synthetically-created input. In order to enable source-to-target video re-animation, we render a synthetic target video with the reconstructed head animation parameters from a source video, and feed it into the trained network - thus taking full control of the target. With the ability to freely recombine source and target parameters, we are able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background. For instance, we can reenact the full head using interactive user-controlled editing, and realize high-fidelity visual dubbing. To demonstrate the high quality of our output, we conduct an extensive series of experiments and evaluations, where for instance a user study shows that our video edits are hard to detect. © 2018 Association for Computing Machinery.","Conditional GAN; Deep learning; Dubbing; Facial reenactment; Rendering-to-video translation; Video portraits","Animation; Deep learning; Quality control; Animation parameter; Conditional GAN; Dubbing; Facial reenactment; Photo-realistic video; Space-time architecture; Synthetic rendering; Video portraits; Rendering (computer graphics)",Article,"Final","",Scopus,2-s2.0-85056648137
"Dawood A., Turner S., Perepa P.","57204557628;7402275376;55955334100;","Affective computational model to extract natural affective states of students with asperger syndrome (AS) in computer-based learning environment",2018,"IEEE Access","6",,"8522016","67026","67034",,5,"10.1109/ACCESS.2018.2879619","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056173148&doi=10.1109%2fACCESS.2018.2879619&partnerID=40&md5=19bda06948b39f6b5d41603b3c9ff899","Department of Computing, Faculty of Art, Science, and Technology, University of Northampton, Northampton, NN1 5PH, United Kingdom; Department of Special Education Needs and Inclusion, Faculty of Education and Humanities, University of Northampton, Northampton, NN1 5PH, United Kingdom","Dawood, A., Department of Computing, Faculty of Art, Science, and Technology, University of Northampton, Northampton, NN1 5PH, United Kingdom; Turner, S., Department of Computing, Faculty of Art, Science, and Technology, University of Northampton, Northampton, NN1 5PH, United Kingdom; Perepa, P., Department of Special Education Needs and Inclusion, Faculty of Education and Humanities, University of Northampton, Northampton, NN1 5PH, United Kingdom","This paper was inspired by looking at the central role of emotion in the learning process, its impact on students' performance; as well as the lack of affective computing models to detect and infer affective-cognitive states in real time for students with and without Asperger Syndrome (AS). This model overcomes gaps in other models that were designed for people with autism, which needed the use of sensors or physiological instrumentations to collect data. The model uses a webcam to capture students' affective-cognitive states of confidence, uncertainty, engagement, anxiety, and boredom. These states have a dominant effect on the learning process. The model was trained and tested on a natural-spontaneous affective dataset for students with and without AS, which was collected for this purpose. The dataset was collected in an uncontrolled environment and included variations in culture, ethnicity, gender, facial and hairstyle, head movement, talking, glasses, illumination changes, and background variation. The model structure used deep learning (DL) techniques like convolutional neural network and long short-term memory. The DL is the-state-of-art tool that used to reduce data dimensionality and capturing non-linear complex features from simpler representations. The affective model provides reliable results with accuracy 90.06%. This model is the first model to detected affective states for adult students with AS without physiological or wearable instruments. For the first time, the occlusions in this model, like hand over face or head were considered an important indicator for affective states like boredom, anxiety, and uncertainty. These occlusions have been ignored in most other affective models. The essential information channels in this model are facial expressions, head movement, and eye gaze. The model can serve as an aided-technology for tutors to monitor and detect the behaviors of all students at the same time and help in predicting negative affective states during learning process. © 2013 IEEE.","Affective model; affective-cognitive states; AS; Asperger Syndrome; autism; CNN; deep learning; LSTM","Arsenic; Computer aided instruction; Deep learning; Diseases; E-learning; Eye movements; Physiological models; Students; Affective model; Asperger syndromes; Autism; Cognitive state; LSTM; Long short-term memory",Article,"Final","",Scopus,2-s2.0-85056173148
"Le Minh T., Shimizu N., Miyazaki T., Shinoda K.","57219500324;57188969836;57193230054;7102950364;","Deep learning based multi-modal addressee recognition in visual scenes with utterances",2018,"IJCAI International Joint Conference on Artificial Intelligence","2018-July",,,"1546","1553",,6,"10.24963/ijcai.2018/214","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055714585&doi=10.24963%2fijcai.2018%2f214&partnerID=40&md5=e4e613e45d35c0537b1fdb782025a7e3","Tokyo Institute of Technology, Tokyo, Japan; Yahoo Japan Corporation, United Kingdom","Le Minh, T., Tokyo Institute of Technology, Tokyo, Japan; Shimizu, N., Yahoo Japan Corporation, United Kingdom; Miyazaki, T., Yahoo Japan Corporation, United Kingdom; Shinoda, K., Tokyo Institute of Technology, Tokyo, Japan","With the widespread use of intelligent systems, such as smart speakers, addressee recognition has become a concern in human-computer interaction, as more and more people expect such systems to understand complicated social scenes, including those outdoors, in cafeterias, and hospitals. Because previous studies typically focused only on pre-specified tasks with limited conversational situations such as controlling smart homes, we created a mock dataset called Addressee Recognition in Visual Scenes with Utterances (ARVSU) that contains a vast body of image variations in visual scenes with an annotated utterance and a corresponding addressee for each scenario. We also propose a multi-modal deep-learning-based model that takes different human cues, specifically eye gazes and transcripts of an utterance corpus, into account to predict the conversational addressee from a specific speaker's view in various real-life conversational scenarios. To the best of our knowledge, we are the first to introduce an end-to-end deep learning model that combines vision and transcripts of utterance for addressee recognition. As a result, our study suggests that future addressee recognition can reach the ability to understand human intention in many social situations previously unexplored, and our modality dataset is a first step in promoting research in this field. © 2018 International Joint Conferences on Artificial Intelligence. All right reserved.",,"Automation; Human computer interaction; Intelligent buildings; Intelligent systems; End to end; Human intentions; Image variations; Learning Based Models; Learning models; Multi-modal; Smart homes; Visual scene; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85055714585
"Jiang L., Xu M., Liu T., Qiao M., Wang Z.","57188642646;55703599800;57215375468;57203088889;24170127500;","Deepvs: A deep learning based video saliency prediction approach",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11218 LNCS",,,"625","642",,16,"10.1007/978-3-030-01264-9_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055713015&doi=10.1007%2f978-3-030-01264-9_37&partnerID=40&md5=38694bd2ac7a1329477dc4d4c9885f1b","Beihang University, Beijing, China","Jiang, L., Beihang University, Beijing, China; Xu, M., Beihang University, Beijing, China; Liu, T., Beihang University, Beijing, China; Qiao, M., Beihang University, Beijing, China; Wang, Z., Beihang University, Beijing, China","In this paper, we propose a novel deep learning based video saliency prediction method, named DeepVS. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which includes 32 subjects’ fixations on 538 videos. We find from LEDOV that human attention is more likely to be attracted by objects, particularly the moving objects or the moving parts of objects. Hence, an object-to-motion convolutional neural network (OM-CNN) is developed to predict the intra-frame saliency for DeepVS, which is composed of the objectness and motion subnets. In OM-CNN, cross-net mask and hierarchical feature normalization are proposed to combine the spatial features of the objectness subnet and the temporal features of the motion subnet. We further find from our database that there exists a temporal correlation of human attention with a smooth saliency transition across video frames. We thus propose saliency-structured convolutional long short-term memory (SS-ConvLSTM) network, using the extracted features from OM-CNN as the input. Consequently, the inter-frame saliency maps of a video can be generated, which consider both structured output with center-bias and cross-frame transitions of human attention maps. Finally, the experimental results show that DeepVS advances the state-of-the-art in video saliency prediction. © 2018, Springer Nature Switzerland AG.","Convolutional LSTM; Eye-tracking database; Saliency prediction","Computer vision; Convolution; Database systems; Eye movements; Eye tracking; Forecasting; Long short-term memory; Convolutional LSTM; Convolutional neural network; Hierarchical features; Prediction methods; Spatial features; Temporal correlations; Temporal features; Video saliencies; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85055713015
"Shan H., Liu Y., Stefanov T.","57204467445;57200045231;6602079223;","A simple convolutional neural network for accurate P300 detection and character spelling in brain computer interface",2018,"IJCAI International Joint Conference on Artificial Intelligence","2018-July",,,"1604","1610",,18,"10.24963/ijcai.2018/222","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055710615&doi=10.24963%2fijcai.2018%2f222&partnerID=40&md5=eea762094d7924d9b47023cc75db7a55","Leiden Institute of Advanced Computer Science, Leiden University, Leiden, Netherlands","Shan, H., Leiden Institute of Advanced Computer Science, Leiden University, Leiden, Netherlands; Liu, Y., Leiden Institute of Advanced Computer Science, Leiden University, Leiden, Netherlands; Stefanov, T., Leiden Institute of Advanced Computer Science, Leiden University, Leiden, Netherlands","A Brain Computer Interface (BCI) character speller allows human-beings to directly spell characters using eye-gazes, thereby building communication between the human brain and a computer. Convolutional Neural Networks (CNNs) have shown better performance than traditional machine learning methods for BCI signal recognition and its application to the character speller. However, current CNN architectures limit further accuracy improvements of signal detection and character spelling and also need high complexity to achieve competitive accuracy, thereby preventing the use of CNNs in portable BCIs. To address these issues, we propose a novel and simple CNN which effectively learns feature representations from both raw temporal information and raw spatial information. The complexity of the proposed CNN is significantly reduced compared with state-of-the-art CNNs for BCI signal detection. We perform experiments on three benchmark datasets and compare our results with those in previous research works which report the best results. The comparison shows that our proposed CNN can increase the signal detection accuracy by up to 15.61% and the character spelling accuracy by up to 19.35%. © 2018 International Joint Conferences on Artificial Intelligence. All right reserved.",,"Complex networks; Convolution; Learning systems; Neural networks; Signal detection; Accuracy Improvement; Benchmark datasets; Convolutional neural network; Detection accuracy; Feature representation; Machine learning methods; Spatial informations; Temporal information; Brain computer interface",Conference Paper,"Final","",Scopus,2-s2.0-85055710615
"Cheng Y., Lu F., Zhang X.","57220572010;54956194300;57142162900;","Appearance-based gaze estimation via evaluation-guided asymmetric regression",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11218 LNCS",,,"105","121",,15,"10.1007/978-3-030-01264-9_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055706658&doi=10.1007%2f978-3-030-01264-9_7&partnerID=40&md5=bc7304d3c97222eb8761960be77cfe15","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, Beijing, China; Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany","Cheng, Y., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China, Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, Beijing, China; Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany","Eye gaze estimation has been increasingly demanded by recent intelligent systems to accomplish a range of interaction-related tasks, by using simple eye images as input. However, learning the highly complex regression between eye images and gaze directions is nontrivial, and thus the problem is yet to be solved efficiently. In this paper, we propose the Asymmetric Regression-Evaluation Network (ARE-Net), and try to improve the gaze estimation performance to its full extent. At the core of our method is the notion of “two eye asymmetry” observed during gaze estimation for the left and right eyes. Inspired by this, we design the multi-stream ARE-Net; one asymmetric regression network (AR-Net) predicts 3D gaze directions for both eyes with a novel asymmetric strategy, and the evaluation network (E-Net) adaptively adjusts the strategy by evaluating the two eyes in terms of their performance during optimization. By training the whole network, our method achieves promising results and surpasses the state-of-the-art methods on multiple public datasets. © 2018, Springer Nature Switzerland AG.","Asymmetric regression; Eye appearance; Gaze estimation","Computer vision; Intelligent systems; Appearance based; Asymmetric regression; Eye appearance; Eye images; Gaze direction; Gaze estimation; Multi-stream; State-of-the-art methods; Regression analysis",Conference Paper,"Final","",Scopus,2-s2.0-85055706658
"Park S., Spurr A., Hilliges O.","57195422868;57200213697;14041644100;","Deep pictorial gaze estimation",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11217 LNCS",,,"741","757",,13,"10.1007/978-3-030-01261-8_44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055482000&doi=10.1007%2f978-3-030-01261-8_44&partnerID=40&md5=0035b584bb2613d865aaf249fcb8f24e","AIT Lab, Department of Computer Science, ETH Zurich, Zürich, Switzerland","Park, S., AIT Lab, Department of Computer Science, ETH Zurich, Zürich, Switzerland; Spurr, A., AIT Lab, Department of Computer Science, ETH Zurich, Zürich, Switzerland; Hilliges, O., AIT Lab, Department of Computer Science, ETH Zurich, Zürich, Switzerland","Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality. © Springer Nature Switzerland AG 2018.","Appearance-based gaze estimation; Eye tracking","Computer vision; Deep neural networks; Network architecture; Eye images; Gaze direction; Gaze estimation; Highly accurate; Ill posed problem; Pictorial representation; State of the art; Unobservable; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85055482000
"Brau E., Guan J., Jeffries T., Barnard K.","50561084900;50561334200;57204390437;7101814130;","Multiple-Gaze Geometry: Inferring Novel 3D Locations from Gazes Observed in Monocular Video",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11208 LNCS",,,"641","659",,1,"10.1007/978-3-030-01225-0_38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055458488&doi=10.1007%2f978-3-030-01225-0_38&partnerID=40&md5=d8f94d1c423fd2204692765dac1b4db3","CiBO Technologies, Cambridge, MA  02141, United States; University of Arizona, Tucson, AZ  85711, United States","Brau, E., CiBO Technologies, Cambridge, MA  02141, United States; Guan, J., CiBO Technologies, Cambridge, MA  02141, United States; Jeffries, T., University of Arizona, Tucson, AZ  85711, United States; Barnard, K., University of Arizona, Tucson, AZ  85711, United States","We develop using person gaze direction for scene understanding. In particular, we use intersecting gazes to learn 3D locations that people tend to look at, which is analogous to having multiple camera views. The 3D locations that we discover need not be visible to the camera. Conversely, knowing 3D locations of scene elements that draw visual attention, such as other people in the scene, can help infer gaze direction. We provide a Bayesian generative model for the temporal scene that captures the joint probability of camera parameters, locations of people, their gaze, what they are looking at, and locations of visual attention. Both the number of people in the scene and the number of extra objects that draw attention are unknown and need to be inferred. To execute this joint inference we use a probabilistic data association approach that enables principled comparison of model hypotheses. We use MCMC for inference over the discrete correspondence variables, and approximate the marginalization over continuous parameters using the Metropolis-Laplace approximation, using Hamiltonian (Hybrid) Monte Carlo for maximization. As existing data sets do not provide the 3D locations of what people are looking at, we contribute a small data set that does. On this data set, we infer what people are looking at with 59% precision compared with 13% for a baseline approach, and where those objects are within about 0.58 m. © 2018, Springer Nature Switzerland AG.","3D gaze estimation; 3D temporal scene understanding; Discovering objects; MCMC; Model selection; Monocular video","Cameras; Computer vision; Location; Discovering objects; Gaze estimation; MCMC; Model Selection; Monocular video; Scene understanding; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85055458488
"Fischer T., Chang H.J., Demiris Y.","57190126084;35168664400;6506125343;","RT-GENE: Real-time eye gaze estimation in natural environments",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11214 LNCS",,,"339","357",,17,"10.1007/978-3-030-01249-6_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055116507&doi=10.1007%2f978-3-030-01249-6_21&partnerID=40&md5=534fd9d65c4080b5a9b8d50fd6efd72c","Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom","Fischer, T., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom; Chang, H.J., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom; Demiris, Y., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom","In this work, we consider the problem of robust gaze estimation in natural environments. Large camera-to-subject distances and high variations in head pose and eye gaze angles are common in such environments. This leads to two main shortfalls in state-of-the-art methods for gaze estimation: hindered ground truth gaze annotation and diminished gaze estimation accuracy as image resolution decreases with distance. We first record a novel dataset of varied gaze and head pose images in a natural environment, addressing the issue of ground truth annotation by measuring head pose using a motion capture system and eye gaze using mobile eyetracking glasses. We apply semantic image inpainting to the area covered by the glasses to bridge the gap between training and testing images by removing the obtrusiveness of the glasses. We also present a new real-time algorithm involving appearance-based deep convolutional neural networks with increased capacity to cope with the diverse images in the new dataset. Experiments with this network architecture are conducted on a number of diverse eye-gaze datasets including our own, and in cross dataset evaluations. We demonstrate state-of-the-art performance in terms of estimation accuracy in all experiments, and the architecture performs well even on lower resolution images. © Springer Nature Switzerland AG 2018.","Convolutional neural network; Eyetracking glasses; Gaze dataset; Gaze estimation; Semantic inpainting","Computer vision; Convolution; Deep neural networks; Eye tracking; Glass; Image resolution; Neural networks; Semantics; Convolutional neural network; Cross-dataset evaluation; Deep convolutional neural networks; Gaze dataset; Gaze estimation; Inpainting; State-of-the-art methods; State-of-the-art performance; Network architecture",Conference Paper,"Final","",Scopus,2-s2.0-85055116507
"Li N., Zhao X., Ma B., Zou X.","57188738516;55352149700;57204294504;24480297000;","A Visual Attention Model Based on Human Visual Cognition",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10989 LNAI",,,"271","281",,3,"10.1007/978-3-030-00563-4_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055088539&doi=10.1007%2f978-3-030-00563-4_26&partnerID=40&md5=39973d6ac3c3e21e195fcfb614d5ad69","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","Li, N., School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Zhao, X., School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Ma, B., School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Zou, X., School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","Understanding where humans look in a scene is significant for many applications. Researches on neuroscience and cognitive psychology show that human brain always pays attention on special areas when they observe an image. In this paper, we recorded and analyzed human eye-tracking data, we found that these areas mainly were focus on semantic objects. Inspired by neuroscience, deep learning concept is proposed. Fully Convolutional Neural Networks (FCN) as one of methods of deep learning can solve image objects segmentation at semantic level efficiently. So we bring forth a new visual attention model which uses FCN to stimulate the cognitive processing of human free observing a natural scene and fuses attractive low-level features to predict fixation locations. Experimental results demonstrated our model has apparently advantages in biology. © 2018, Springer Nature Switzerland AG.","FCN; Human visual cognition; Visual attention model","Behavioral research; Brain; Deep learning; Eye tracking; Image segmentation; Neural networks; Neurology; Semantics; Cognitive processing; Cognitive psychology; Convolutional neural network; Human visual; Low-level features; Semantic levels; Semantic objects; Visual attention model; Cognitive systems",Conference Paper,"Final","",Scopus,2-s2.0-85055088539
"Recasens A., Kellnhofer P., Stent S., Matusik W., Torralba A.","57189096003;55250016000;56229805900;56230515000;7005432728;","Learning to zoom: A saliency-based sampling layer for neural networks",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11213 LNCS",,,"52","67",,5,"10.1007/978-3-030-01240-3_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055083447&doi=10.1007%2f978-3-030-01240-3_4&partnerID=40&md5=0fe1602f078822e8d8909aca2bb820b2","Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Toyota Research Institute, Cambridge, MA  02139, United States","Recasens, A., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Kellnhofer, P., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Stent, S., Toyota Research Institute, Cambridge, MA  02139, United States; Matusik, W., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Torralba, A., Massachusetts Institute of Technology, Cambridge, MA  02139, United States","We introduce a saliency-based distortion layer for convolutional neural networks that helps to improve the spatial sampling of input data for a given task. Our differentiable layer can be added as a preprocessing block to existing task networks and trained altogether in an end-to-end fashion. The effect of the layer is to efficiently estimate how to sample from the original data in order to boost task performance. For example, for an image classification task in which the original data might range in size up to several megapixels, but where the desired input images to the task network are much smaller, our layer learns how best to sample from the underlying high resolution data in a manner which preserves task-relevant information better than uniform downsampling. This has the effect of creating distorted, caricature-like intermediate images, in which idiosyncratic elements of the image that improve task performance are zoomed and exaggerated. Unlike alternative approaches such as spatial transformer networks, our proposed layer is inspired by image saliency, computed efficiently from uniformly downsampled data, and degrades gracefully to a uniform sampling strategy under uncertainty. We apply our layer to improve existing networks for the tasks of human gaze estimation and fine-grained object classification. Code for our method is available in: http://github.com/recasens/Saliency-Sampler. © Springer Nature Switzerland AG 2018.","Attention; Convolutional neural networks; Deep learning; Image sampling; Spatial transformer; Task saliency","Classification (of information); Computer vision; Convolution; Deep learning; Image enhancement; Image sampling; Neural networks; Attention; Convolutional neural network; High resolution data; Intermediate image; Object classification; Spatial transformer; Task saliency; Uniform sampling; Network layers",Conference Paper,"Final","",Scopus,2-s2.0-85055083447
"Chopade P., Khan S., Stoeffler K., Edwards D., Rosen Y., Von Davier A.","37460936700;57202790555;57193767208;57204202470;18042058000;6506976799;","Framework for effective teamwork assessment in collaborative learning and problem solving",2018,"CEUR Workshop Proceedings","2153",,,"48","59",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054937159&partnerID=40&md5=f7aee634697e0a440344a38a84e0093c","ACTNext, ACT Inc., Iowa City, IA, United States; ACT Inc., Iowa City, IA, United States","Chopade, P., ACTNext, ACT Inc., Iowa City, IA, United States; Khan, S., ACTNext, ACT Inc., Iowa City, IA, United States; Stoeffler, K., ACT Inc., Iowa City, IA, United States; Edwards, D., ACTNext, ACT Inc., Iowa City, IA, United States; Rosen, Y., ACTNext, ACT Inc., Iowa City, IA, United States; Von Davier, A., ACTNext, ACT Inc., Iowa City, IA, United States","This paper presents an interactive team collaborative learning and problem-solving (ITCLP) framework for effective teamwork learning and assessment. Modeling the dynamics of a collaborative, networked system involving multimodal data presents many challenges. This framework incorporates an Artificial Intelligence (AI), a Machine Learning (ML) and computational psycho- metrics (CP) based methodology, system architecture, and algorithms to find pat- terns of learning, interactions, relationships, and effective teamwork assessment from a collaborative learning environment (CLE). Collaborative learning may take place in peer-to-peer or in large groups, to discuss concepts, or find solutions to real-time problems or working on situational judgement task (SJT). Intelligent Tutoring Systems (ITSs) have been mostly used as a supportive system for the varied needs of individual learners. The ITCLP framework enables development of ITSs for team tutoring and facilitates collaborative problem solving (CPS) by creating interactions between team members. Our team model maps team knowledge, skills, interactions, behaviors, and shared knowledge of team tasks, and performance. We will collect the team interaction log data, user eye tracking, and user portrait video/audio and will map team skills evidence based on CPS, a broad range of cross-cutting capabilities, which is part of an even broader Holistic Framework (HF) proposed by Camara and colleagues [1]. © 2018 CEUR-WS. All Rights Reserved.","Artificial Intelligence; Collaborative Learning; Collaborative Problem Solving; Computational Psychometrics; Intelligent Tutoring Systems; Machine Learning; Team Tutoring; Teamwork","Artificial intelligence; Computer aided instruction; Distributed computer systems; Eye tracking; Learning systems; Collaborative learning; Collaborative problem solving; Computational Psychometrics; Intelligent tutoring system; Team Tutoring; Teamwork; Problem solving",Conference Paper,"Final","",Scopus,2-s2.0-85054937159
"Stauden S., Barz M., Sonntag D.","57204112887;57189847803;12241487800;","Visual search target inference using bag of deep visual words",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11117 LNAI",,,"297","304",,3,"10.1007/978-3-030-00111-7_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054489256&doi=10.1007%2f978-3-030-00111-7_25&partnerID=40&md5=b7efb0d1ba4b412d15a9b1fc10153789","German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany","Stauden, S., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Barz, M., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Sonntag, D., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany","Visual Search target inference subsumes methods for predicting the target object through eye tracking. A person intents to find an object in a visual scene which we predict based on the fixation behavior. Knowing about the search target can improve intelligent user interaction. In this work, we implement a new feature encoding, the Bag of Deep Visual Words, for search target inference using a pre-trained convolutional neural network (CNN). Our work is based on a recent approach from the literature that uses Bag of Visual Words, common in computer vision applications. We evaluate our method using a gold standard dataset. The results show that our new feature encoding outperforms the baseline from the literature, in particular, when excluding fixations on the target. © Springer Nature Switzerland AG 2018.","Deep learning; Eye tracking; Intelligent user interfaces; Search target inference; Visual attention","Behavioral research; Deep learning; Encoding (symbols); Eye movements; Eye tracking; Graphical user interfaces; Neural networks; Signal encoding; Bag-of-visual words; Computer vision applications; Convolutional Neural Networks (CNN); Gold standards; Intelligent User Interfaces; Search target inference; User interaction; Visual Attention; Target tracking",Conference Paper,"Final","",Scopus,2-s2.0-85054489256
"Takahashi Y., Yendo T.","57192577600;12766446000;","Study of eye tracking type super multi-view display using time division multiplexing",2018,"IS and T International Symposium on Electronic Imaging Science and Technology",,,,"4131","4134",,,"10.2352/ISSN.2470-1173.2018.04.SDA-413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052861743&doi=10.2352%2fISSN.2470-1173.2018.04.SDA-413&partnerID=40&md5=6254f8e763f86561b06143217c45e675","Nagaoka Univ. of Technology, 1603-1, Kamitomioka-machi, Nagaoka-city, Niigata-pref, 940-2188, Japan","Takahashi, Y., Nagaoka Univ. of Technology, 1603-1, Kamitomioka-machi, Nagaoka-city, Niigata-pref, 940-2188, Japan; Yendo, T., Nagaoka Univ. of Technology, 1603-1, Kamitomioka-machi, Nagaoka-city, Niigata-pref, 940-2188, Japan","In recent year, the Head-up display is studied actively. Among them, 3D HUD attracts rising attention. In HUD application, it is assumed that stereoscopic image is displayed at far distance. Super Multi-View display provides a smooth parallax. Even if 3D image is at far distance, it be able to display 3D image which has appropriate depth. In previous studies, high resolution required for SMV. However, there are restrictions to increase the resolution. Therefore, we propose a novel SMV display using time division multiplexing and eye tracking techniques. Our new system is not required high resolution display. The proposed display consists of DMD and light source array. The ray from the light source array is reflected at DMD, and form an image at the vicinity of the pupil. The position forming an image depends on light source position. The image which is display on the DMD is changed corresponding to the focal point. To confirm the principle of the proposed method, we experiment about creating a viewing zone only in the vicinity of the pupil. From the result, we confirmed 8 viewpoints in the horizontal direction at 18.8 mm viewing zone. © 2018, Society for Imaging Science and Technology.",,"Display devices; Geometrical optics; Light sources; Stereo image processing; Time division multiplexing; Focal points; Head up displays; High resolution; High resolution display; Source position; Stereoscopic image; Super-multi; Viewing zone; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85052861743
"Mahardika W., Wibirama S., Ferdiana R., Kusumawardani S.S.","57203689637;26654457700;49663102500;36809779800;","A novel user experience study of parallax scrolling using eye tracking and user experience questionnaire",2018,"International Journal on Advanced Science, Engineering and Information Technology","8","4",,"1226","1233",,1,"10.18517/ijaseit.8.4.6500","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052639151&doi=10.18517%2fijaseit.8.4.6500&partnerID=40&md5=dd2196b55391dc5cf87e0ebbd52123cf","Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia","Mahardika, W., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Wibirama, S., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Ferdiana, R., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Kusumawardani, S.S., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia","Parallax scrolling technique is being devoted as a unique and an innovative trend in the web design. Parallax scrolling provides 3D perception on a web page. Previous works observed user experience issues of parallax scrolling merely based on subjective questionnaires. Their findings leave a research question whether the results are valid, as participants may perceive a written questionnaire differently. Additionally, bias and ambiguity in the questionnaire can affect the research results significantly. To solve this research problem, we present a novel user experience study of parallax scrolling in storytelling and online shop website using eye tracking and User Experience Questionnaire (UEQ). Forty (N=40) participant joined the experiment on a voluntary basis. Each participant only interacted with one out of two websites (storytelling or online shop) and only one effect (with or without parallax scrolling). We found that parallax scrolling affected UEQ score of Attractiveness of the storytelling website (p < 0.05). Our findings suggest that parallax scrolling improves user engagement in storytelling website. We also observed that the participants spent time almost two times faster to find an object of interest in an online shop with parallax scrolling compared with the similar task in an online shop without parallax scrolling (p < 0.05). We thus argue that parallax scrolling is useful during interacting with particular websites that require visual object localization. In future, web designers should consider the appropriate usage of parallax scrolling to optimize user experience while avoiding additional distraction caused by this technique. © IJASEIT.","Eye tracking; Parallax scrolling; User experience; Web design",,Article,"Final","",Scopus,2-s2.0-85052639151
"Wan Q., Rajeev S., Kaszowska A., Panetta K., Taylor H.A., Agaian S.","57188747389;57191541248;57203481497;6507727793;7403057334;35321805400;","Fixation oriented object segmentation using mobile eye tracker",2018,"Proceedings of SPIE - The International Society for Optical Engineering","10668",,"106680D","","",,2,"10.1117/12.2304868","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051846131&doi=10.1117%2f12.2304868&partnerID=40&md5=4a2c95e3e9f146df773a83306b845162","Dept of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Dept of Psychology, Tufts University, Medford, MA  02155, United States; Distinguished Prof. of Computer Science, City University of New York, New York City, NY  10017, United States","Wan, Q., Dept of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Rajeev, S., Dept of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Kaszowska, A., Dept of Psychology, Tufts University, Medford, MA  02155, United States; Panetta, K., Dept of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Taylor, H.A., Dept of Psychology, Tufts University, Medford, MA  02155, United States; Agaian, S., Distinguished Prof. of Computer Science, City University of New York, New York City, NY  10017, United States","Eye tracking technology allows researchers to monitor position of the eye and infer one's gaze direction, which is used to understand the nature of human attention within psychology, cognitive science, marketing and artificial intelligence. Commercially available head-mounted eye trackers allow researchers to track pupil movements (saccades and fixations) using infrared camera and capture the field of vision by a front-facing scene camera. The wearable eye tracker opened a new way to research in unconstrained environment settings; however, the recorded scene video typically has non-uniform illumination, low quality image frames, and moving scene objects. One of the most important tasks for analyzing the recorded scene video data is finding the boundary between different objects in a single frame. This paper presents a multi-level fixation-oriented object segmentation method (MFoOS) to solve the above challenges in segmenting the scene objects in video data collected by the eye tracker in order to support cognition research. MFoOS shows its advancement in position-invariance, illumination, noise tolerance and is task-driven. The proposed method is tested using real-world case studies designed by our team of psychologists focused on understanding visual attention in human problem solving. The extensive computer simulation demonstrates the method's accuracy and robustness for fixation-oriented object segmentation. Moreover, a deep-learning image semantic segmentation combining MFoOS results as label data was explored to demonstrate the possibility of on-line deployment of eye tracker fixation-oriented object segmentation. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Cognitive science; Eye tracking technology; Image semantic segmentation; Multi-level fixation-oriented object segmentation; Online deployment","Behavioral research; Cameras; Cognitive systems; Deep learning; Eye movements; Image segmentation; Problem solving; Semantics; Video recording; Cognitive science; Eye tracking technologies; Image semantics; Object segmentation; Online deployment; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85051846131
"Wood E., Baltrušaitis T., Morency L.-P., Robinson P., Bulling A.","56145872800;36696075900;6603047400;57205369790;6505807414;","Gazedirector: Fully articulated eye gaze redirection in video",2018,"Computer Graphics Forum","37","2",,"217","225",,17,"10.1111/cgf.13355","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051832117&doi=10.1111%2fcgf.13355&partnerID=40&md5=42137a016fcd71cd98890c91e7b6490b","University of Cambridge, United Kingdom; Carnegie Mellon University, United States; Max Planck Institute for Informatics, Germany; Microsoft, United States","Wood, E., University of Cambridge, United Kingdom, Microsoft, United States; Baltrušaitis, T., Carnegie Mellon University, United States, Microsoft, United States; Morency, L.-P., Carnegie Mellon University, United States; Robinson, P., University of Cambridge, United Kingdom; Bulling, A., Max Planck Institute for Informatics, Germany","We present GazeDirector, a new approach for eye gaze redirection that uses model-fitting. Our method first tracks the eyes by fitting a multi-part eye region model to video frames using analysis-by-synthesis, thereby recovering eye region shape, texture, pose, and gaze simultaneously. It then redirects gaze by 1) warping the eyelids from the original image using a model-derived flow field, and 2) rendering and compositing synthesized 3D eyeballs onto the output image in a photorealistic manner. GazeDirector allows us to change where people are looking without person-specific training data, and with full articulation, i.e. we can precisely specify new gaze directions in 3D. Quantitatively, we evaluate both model-fitting and gaze synthesis, with experiments for gaze estimation and redirection on the Columbia gaze dataset. Qualitatively, we compare GazeDirector against recent work on gaze redirection, showing better results especially for large redirection angles. Finally, we demonstrate gaze redirection on YouTube videos by introducing new 3D gaze targets and by manipulating visual behavior. © 2017 The Authors and The Eurographics Association and John Wiley & Sons Ltd.",,"Behavioral research; Analysis by synthesis; Gaze direction; Gaze estimation; Gaze synthesis; New approaches; Original images; Photo-realistic; Visual behavior; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85051832117
"Parikh S., Kalva H.","57201063030;57295796500;","Eye gaze feature classification for predicting levels of learning",2018,"CEUR Workshop Proceedings","2141",,,"24","29",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050942634&partnerID=40&md5=15b5cc144fe41cc95ec17fa59a652f50","Florida Atlantic University, Boca Raton, FL, United States; Institute of Technology, Nirma University, Ahmedabad, Gujarat, India","Parikh, S., Florida Atlantic University, Boca Raton, FL, United States, Institute of Technology, Nirma University, Ahmedabad, Gujarat, India; Kalva, H., Florida Atlantic University, Boca Raton, FL, United States","E-Learning courses reach online to millions worldwide. Amidst the geo-flexibility of registering students, the main challenges are instructor feedback and student retention. Ability to predict difficult content in real time enables eLearning systems to adapt content to students' needs dynamically. Recently, we examined eye responses as an indicator of levels of learning and introduced a non-parametric, non-probabilistic and statistical feature weighted linguistics classifier (FWLC) capable of predicting difficult words (terms) and concepts. FWLC achieved 85% accuracy for predicting levels of learning of big words using eye responses. In this paper, we analyze the performance of FWLC with five machine learning classifiers. FWLC has a higher true positive rate (TPR) and a lower ratio of FNR/FPR (the novel is a positive class). FWLC achieves a TPR gain of 43% over the best performing machine learning classifier. Prediction accuracy of FWLC for big words is lower by 6.6% than the best performing machine learning classifier. However, this accuracy tradeoff is worth the higher TPR of FWLC as the objective is to predict novel words (positive class) more accurately so that content can adapt to student's need. © 2018 Copyright held by the owner/author(s).","Elearning; Eye and pupil response analysis; Machine learning classification; Predicting levels of learning","Artificial intelligence; Classification (of information); Computer aided instruction; Forecasting; Learning systems; Real time systems; Students; Feature classification; Machine learning classification; Non-probabilistic; Predicting levels of learning; Prediction accuracy; Pupil response; Statistical features; True positive rates; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85050942634
"Nagamune K., Takata K.","9334125600;57203131618;","Analysis of human motion and cognition ability with virtual reality system: Basic mechanism of human response",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10908 LNCS",,,"78","86",,,"10.1007/978-3-319-92052-8_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050613808&doi=10.1007%2f978-3-319-92052-8_7&partnerID=40&md5=ba0718a110a6a2cd3969f2eda617b7fd","Graduate School of Engineering, University of Fukui, Fukui, Japan","Nagamune, K., Graduate School of Engineering, University of Fukui, Fukui, Japan; Takata, K., Graduate School of Engineering, University of Fukui, Fukui, Japan","When grasping an object, a human needs to recognize the object. In general, after the center of gravity of the object is recognized from the object shape, the human grasps a position close to the center of gravity. This research analyzes the relationship between the object shape and the sight trajectory until grasping. The proposed method traces finger motions when grasping the virtual 3D objects displayed on a screen by using finger motion capture device. In the motion, the sight trajectory is also measured and analyzed by using the eye tracking device. We conducted experiments with five subjects and analyzed the relationship between the variation of the line of sight trajectory and the size of the grasped object. © Springer International Publishing AG, part of Springer Nature 2018.","Motion; Rehabilitation; Virtual reality","Eye tracking; Motion analysis; Patient rehabilitation; Trajectories; Virtual reality; Basic mechanism; Center of gravity; Eye tracking devices; Grasped object; Human response; Motion; Virtual 3D objects; Virtual reality system; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85050613808
"Ahmad B.I., Langdon P.M., Godsill S.J.","26422007200;55781400200;7004018739;","Stabilising touch interactions in cockpits, aerospace, and vibrating environments",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10907 LNCS",,,"133","145",,1,"10.1007/978-3-319-92049-8_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050590049&doi=10.1007%2f978-3-319-92049-8_10&partnerID=40&md5=f55faade413412e8494266b23c883a71","Signal Processing and Communications Laboratory (SigProC), Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Engineering Design Centre (EDC), Department of Engineering, University of Cambridge, Cambridge, United Kingdom","Ahmad, B.I., Signal Processing and Communications Laboratory (SigProC), Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Langdon, P.M., Engineering Design Centre (EDC), Department of Engineering, University of Cambridge, Cambridge, United Kingdom; Godsill, S.J., Signal Processing and Communications Laboratory (SigProC), Department of Engineering, University of Cambridge, Cambridge, United Kingdom","Incorporating touch screen interaction into cockpit flight systems is increasingly gaining traction given its several potential advantages to design as well as usability to pilots. However, perturbations to the user input are prevalent in such environments due to vibrations, turbulence and high accelerations. This poses particular challenges for interacting with displays in the cockpit, for example, accidental activation during turbulence or high levels of distraction from the primary task of airplane control to accomplish selection tasks. On the other hand, predictive displays have emerged as a solution to minimize the effort as well as cognitive, visual and physical workload associated with using in-vehicle displays under perturbations, induced by road and driving conditions. This technology employs gesture tracking in 3D and potentially eye-gaze as well as other sensory data to substantially facilitate the acquisition (pointing and selection) of an interface component by predicting the item the user intents to select on the display, early in the movements towards the screen. A key aspect is utilising principled Bayesian modelling to incorporate and treat the present perturbation, thus, it is a software-based solution that showed promising results when applied to automotive applications. This paper explores the potential of applying this technology to applications in aerospace and vibrating environments in general and presents design recommendations for such an approach to enhance interactions accuracy as well as safety. © Springer International Publishing AG, part of Springer Nature 2018.","Bayesian inference; Interactive displays; Target assistance; Turbulence","Aircraft control; Application programs; Bayesian networks; Eye movements; Inference engines; Touch screens; Turbulence; Automotive applications; Bayesian inference; Design recommendations; Interactive display; Pointing and selections; Software-based solutions; Touch-screen interaction; Vibrating environment; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85050590049
"Kim Y.-T., Seo J., Seo W., Sung G., Kim Y., Song H., An J., Choi C.-S., Kim S., Kim H., Kim Y., Kim Y., Lee H.-S.","55826365700;55471873500;56366599800;36843451400;57194380014;24176149200;23471722500;7402961271;57020247800;16063915100;57196171178;57191473529;17137243800;","Holographic augmented reality head-up display with eye tracking and steering light source",2018,"23rd International Display Workshops in conjunction with Asia Display, IDW/AD 2016","4",,,"2007","2010",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050571606&partnerID=40&md5=168f7786a9b9f0b1222f29afdd65d447","Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea","Kim, Y.-T., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Seo, J., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Seo, W., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Sung, G., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, Y., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Song, H., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; An, J., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Choi, C.-S., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, S., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, H., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, Y., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, Y., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Lee, H.-S., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea","We realized a holographic head-up display using a steering light source with eye position tracking. It can represent a real augmented reality which perfectly matches virtual graphic images to the real world. Further, for the determination of the position of the light source, 3D calibration method is proposed. copyright © 2016 Society of Information Display. All rights reserved.","Accommodation-vergence conflict; Augmented reality; Head-up display; Holographic","Augmented reality; Holographic displays; Light sources; 3D calibration; Eye position tracking; Graphic images; Head up displays; Holographic; Real-world; Vergences; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85050571606
"Fathaliyan A.H., Wang X., Santos V.J.","57202989154;57202988622;24465525600;","Exploiting three-dimensional gaze tracking for action recognition during bimanual manipulation to enhance human-robot collaboration",2018,"Frontiers Robotics  AI","5","APR","25","","",,9,"10.3389/frobt.2018.00025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050089757&doi=10.3389%2ffrobt.2018.00025&partnerID=40&md5=01b5f82fd6676ed61934d38306d07464","Biomechatronics Laboratory, Mechanical and Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA, United States","Fathaliyan, A.H., Biomechatronics Laboratory, Mechanical and Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA, United States; Wang, X., Biomechatronics Laboratory, Mechanical and Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA, United States; Santos, V.J., Biomechatronics Laboratory, Mechanical and Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA, United States","Human-robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze-object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The ""gaze object sequence"" was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments. © 2018 Haji Fathaliyan, Wang and Santos.","Action recognition; Bimanual manipulation; Eye tracking; Gaze fixation; Gaze object sequence; Gaze saliency map; Human-robot collaboration; Instrumental activity of daily living",,Article,"Final","",Scopus,2-s2.0-85050089757
"Lavoué G., Cordier F., Seo H., Larabi M.-C.","8279467500;7003284423;55392220600;6603685956;","Visual attention for rendered 3D shapes",2018,"Computer Graphics Forum","37","2",,"191","203",,11,"10.1111/cgf.13353","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049676757&doi=10.1111%2fcgf.13353&partnerID=40&md5=e4389fab69144dc830cc240c392e7044","CNRS, Univ. Lyon, LIRIS, France; University of Haute-Alsace, LMIA, France; CNRS, University of Strasbourg, ICube, France; CNRS, Univ. Poitiers, XLIM, UMR 7252, Poitiers, France","Lavoué, G., CNRS, Univ. Lyon, LIRIS, France; Cordier, F., University of Haute-Alsace, LMIA, France; Seo, H., CNRS, University of Strasbourg, ICube, France; Larabi, M.-C., CNRS, Univ. Poitiers, XLIM, UMR 7252, Poitiers, France","Understanding the attentional behavior of the human visual system when visualizing a rendered 3D shape is of great importance for many computer graphics applications. Eye tracking remains the only solution to explore this complex cognitive mechanism. Unfortunately, despite the large number of studies dedicated to images and videos, only a few eye tracking experiments have been conducted using 3D shapes. Thus, potential factors that may influence the human gaze in the specific setting of 3D rendering, are still to be understood. In this work, we conduct two eye-tracking experiments involving 3D shapes, with both static and time-varying camera positions. We propose a method for mapping eye fixations (i.e., where humans gaze) onto the 3D shapes with the aim to produce a benchmark of 3D meshes with fixation density maps, which is publicly available. First, the collected data is used to study the influence of shape, camera position, material and illumination on visual attention. We find that material and lighting have a significant influence on attention, as well as the camera path in the case of dynamic scenes. Then, we compare the performance of four representative state-of-the-art mesh saliency models in predicting ground-truth fixations using two different metrics. We show that, even combined with a center-bias model, the performance of 3D saliency algorithms remains poor at predicting human fixations. To explain their weaknesses, we provide a qualitative analysis of the main factors that attract human attention. We finally provide a comparison of human-eye fixations and Schelling points and show that their correlation is weak. © 2018 The Author(s)and 2018 The Eurographics Association and John Wiley & Sons Ltd.",,"Behavioral research; Cameras; Eye movements; Eye tracking; Rendering (computer graphics); Camera positions; Cognitive mechanisms; Computer graphics applications; Human Visual System; Mesh saliencies; Qualitative analysis; State of the art; Visual Attention; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85049676757
"Ching-En C.","57202790055;","Metacognitive experience modeling using eye-tracking",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10948 LNAI",,,"503","507",,,"10.1007/978-3-319-93846-2_94","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049379345&doi=10.1007%2f978-3-319-93846-2_94&partnerID=40&md5=04d17d8f8814912927a4d84830a7cf5f","UCL Knowledge Lab, UCL Institute of Education, London, United Kingdom","Ching-En, C., UCL Knowledge Lab, UCL Institute of Education, London, United Kingdom","Metacognitive experience (ME) is one of the key facets of metacognition, which serves a critical cuing function in the process of self-regulated learning process. However, the study of ME is hindered by its subjective and implicit nature of and the challenges that are associated with accessing such experiences. In exploring such experiences, eye-tracking offers certain advantages over self-reporting methods. However, to date most studies tend to focus on utilizing eye-tracking to explore metacognitive skills (MS) rather than ME, with those that do explore ME also tending to require participants to self-report rather than relying on observation of possible behavioural indicators of such metacognitive processes. Based on previous works in this field, the research proposed is based on the hypothesis that eye-tracking data can provide a crucial objective measure of learners’ implicit ME processes. The research will also investigate the extent to which such data can serve as the basis for automatically predicting the occurrence and intensity of ME during learning using machine learning, in a way that can support the delivery of adaptive domain-independent feedback in a variety of Intelligent Learning Environments (ILEs). © Springer International Publishing AG, part of Springer Nature 2018.","Eye-tracking; Metacognition; Metacognitive experience; User modelling","Artificial intelligence; Cognitive systems; Computer aided instruction; Learning systems; Domain independents; Intelligent learning environments; Metacognition; Metacognitive process; Metacognitive skills; Metacognitives; Self-regulated learning; User Modelling; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049379345
"Cheok K.C.","7005938198;","Eye-hand tracking simulator for training ai learning systems",2018,"Proceedings of the 33rd International Conference on Computers and Their Applications, CATA 2018","2018-March",,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048441996&partnerID=40&md5=b58a091372393d4700f0968cc0ae6444","Department of Electrical and Computer Engineering, School of Engineering and Computer Science, Oakland University, Rochester, MI  48309, United States","Cheok, K.C., Department of Electrical and Computer Engineering, School of Engineering and Computer Science, Oakland University, Rochester, MI  48309, United States","There are many techniques for modeling and characterizing system behaviors. These techniques always require a sequence of steps such as acquisition of training data, training of the model and verification of the trained model performance. In this paper, we present a Matlab/Simulink-based simulator for a video-based target tracking system that is used to train AI learning models including artificial neural networks (ANN). It is shown that an ANN could emulate the eye-hand tracking (EHT) coordination of a human operator. The simulator-based experiments can be used to reveal many insights for training of the different ANN configurations for human EHT action. Its uses can be extended to system identification, adaptive control, machine learning and deep learning. © 2018 The International Society for Computers and Their Applications (ISCA). All Rights Reserved.","Eye-hand tracking; Machine learning; Neural network control; Simulator; Trainer","Adaptive control systems; Deep learning; Learning systems; Neural networks; Palmprint recognition; Simulators; Target tracking; Adaptive Control; Hand tracking; Model performance; Modeling and verifications; Neural network control; System behaviors; Target tracking systems; Trainer; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85048441996
"Howe A., Nguyen P.","57201521212;57209915739;","SAT reading analysis using eye-gaze tracking technology and machine learning",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10858 LNCS",,,"332","338",,,"10.1007/978-3-319-91464-0_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048349861&doi=10.1007%2f978-3-319-91464-0_36&partnerID=40&md5=4320d17068c3ff47fa2a1dc90ffca04d","The American School in Japan, Tokyo, 106-0047, Japan; Tokyo Techies, Tokyo, 106-0031, Japan","Howe, A., The American School in Japan, Tokyo, 106-0047, Japan; Nguyen, P., Tokyo Techies, Tokyo, 106-0031, Japan","We propose a method using eye-gaze tracking technology and machine learning for the analysis of the reading section of the Scholastic Aptitude Test (SAT). An eye-gaze tracking device tracks where the reader is looking on the screen and provides the coordinates of the gaze. This collected data allows us to analyze the reading patterns of test takers and discover what features enable test takers to score higher. Using a machine learning approach, we found that the time spent on the passage at the beginning of the test (in minutes), number of times switching between the passage and the questions, and the total time spent doing the reading test (in minutes) have the greatest impact in distinguishing higher scores from lower scores. © Springer International Publishing AG, part of Springer Nature 2018.","Analysis; Eye-gaze tracking; Machine learning; Reading pattern; SAT reading","Artificial intelligence; Computer aided instruction; Intelligent vehicle highway systems; Learning systems; Analysis; Eye gaze tracking; Machine learning approaches; Reading patterns; SAT reading; Scholastic aptitude tests; Time spent; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85048349861
"Doumbouya R., Benlamine M.S., Dufresne A., Frasson C.","57195933410;57190031634;57203073258;7003506234;","Game scenes evaluation and player’s dominant emotion prediction",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10858 LNCS",,,"54","65",,3,"10.1007/978-3-319-91464-0_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048326937&doi=10.1007%2f978-3-319-91464-0_6&partnerID=40&md5=0311c8c32731dc396c3f4cbc72d26b0a","Heron Lab, Department of Computer Science, University of Montreal, Montreal, Canada","Doumbouya, R., Heron Lab, Department of Computer Science, University of Montreal, Montreal, Canada; Benlamine, M.S., Heron Lab, Department of Computer Science, University of Montreal, Montreal, Canada; Dufresne, A., Heron Lab, Department of Computer Science, University of Montreal, Montreal, Canada; Frasson, C., Heron Lab, Department of Computer Science, University of Montreal, Montreal, Canada","In this paper, we present a solution for computer assisted emotional analysis of game session. The proposed approach combines eye movements and facial expressions to annotate the perceived game objects with the expressed dominate emotions. Moreover, our system EMOGRAPH (Emotional Graph) gives easy access to information about user experience and predicts player’s emotions. The prediction mainly uses both subjective measures through questionnaire and objective measures through brain wave activity (electroencephalography - EEG) combined with eye tracking data. EMOGRAPH’s method was experimented on 21 participants playing horror game “Outlast”. Our results show the effectiveness of our method in the identification of the emotions and their triggers. We also present our emotion prediction approach using game scene’s design goal (defined by OCC variables from the model of emotions’ cognitive evaluation of Ortony, Clore and Collins [1]) to annotate the player’s situation in a scene and machine learning algorithms. The prediction results are promising and would widen possibilities in game design. © Springer International Publishing AG, part of Springer Nature 2018.","Affective computing; Cognitive evaluation (OCC); EEG; Player model; Video games","Behavioral research; Brain; Computer aided analysis; Computer aided instruction; Electroencephalography; Electrophysiology; Eye movements; Eye tracking; Forecasting; Human computer interaction; Intelligent vehicle highway systems; Learning algorithms; Learning systems; Affective Computing; Cognitive evaluation (OCC); Computer assisted; Emotion predictions; Emotional analysis; Facial Expressions; Player modeling; Video game; Computer games",Conference Paper,"Final","",Scopus,2-s2.0-85048326937
"Han Y.-J., Kim W., Park J.-S.","57202044585;55492052500;8704160200;","Efficient Eye-Blinking Detection on Smartphones: A Hybrid Approach Based on Deep Learning",2018,"Mobile Information Systems","2018",,"6929762","","",,8,"10.1155/2018/6929762","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048106877&doi=10.1155%2f2018%2f6929762&partnerID=40&md5=d74ed393a7a2ce9785db608fb176f803","Vieworks Co., Ltd., Gyeonggi, South Korea; Department of Computer Engineering, Gachon University, Gyeonggi, South Korea; Department of Computer Engineering, Hongik University, Seoul, South Korea","Han, Y.-J., Vieworks Co., Ltd., Gyeonggi, South Korea; Kim, W., Department of Computer Engineering, Gachon University, Gyeonggi, South Korea; Park, J.-S., Department of Computer Engineering, Hongik University, Seoul, South Korea","We propose an efficient method that can be used for eye-blinking detection or eye tracking on smartphone platforms in this paper. Eye-blinking detection or eye-tracking algorithms have various applications in mobile environments, for example, a countermeasure against spoofing in face recognition systems. In resource limited smartphone environments, one of the key issues of the eye-blinking detection problem is its computational efficiency. To tackle the problem, we take a hybrid approach combining two machine learning techniques: SVM (support vector machine) and CNN (convolutional neural network) such that the eye-blinking detection can be performed efficiently and reliably on resource-limited smartphones. Experimental results on commodity smartphones show that our approach achieves a precision of 94.4% and a processing rate of 22 frames per second. © 2018 Young-Joo Han et al.",,"Computational efficiency; Eye tracking; Face recognition; Neural networks; Smartphones; Support vector machines; Convolutional neural network; Detection problems; Face recognition systems; Frames per seconds; Hybrid approach; Mobile environments; Processing rates; SVM(support vector machine); Deep learning",Article,"Final","",Scopus,2-s2.0-85048106877
"Guezou-Philippe A., Huet S., Pellerin D., Graff C.","57202317000;8912301400;55725474200;7006516470;","Prototyping and evaluating sensory substitution devices by spatial immersion in virtual environments",2018,"VISIGRAPP 2018 - Proceedings of the 13th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","4",,,"596","602",,1,"10.5220/0006637705960602","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047872804&doi=10.5220%2f0006637705960602&partnerID=40&md5=861378c23df74c63d88d85cf463eeaff","Univ. Grenoble Alpes, CNRS, Grenoble Institute of Engineering, GIPSA-lab, Grenoble, 38000, France; Univ. Grenoble Alpes, Univ. Savoie Mont Blanc, CNRS, LPNC, Grenoble, 38000, France","Guezou-Philippe, A., Univ. Grenoble Alpes, CNRS, Grenoble Institute of Engineering, GIPSA-lab, Grenoble, 38000, France; Huet, S., Univ. Grenoble Alpes, CNRS, Grenoble Institute of Engineering, GIPSA-lab, Grenoble, 38000, France; Pellerin, D., Univ. Grenoble Alpes, CNRS, Grenoble Institute of Engineering, GIPSA-lab, Grenoble, 38000, France; Graff, C., Univ. Grenoble Alpes, Univ. Savoie Mont Blanc, CNRS, LPNC, Grenoble, 38000, France","Various audio-vision Sensory Substitution Devices (SSDs) are in development to assist people without sight. They all convert optical information extracted from a camera, into sound parameters but are evaluated for different tasks in different contexts. The use of 3D environments is proposed here to compare the advantages and disadvantages of not only software (transcoding) solutions but also of hardware (component) specifics, in various situations and activities. By use of a motion capture system, the whole person, not just a guided avatar, was immersed in virtual places that were modelled and that could be replicated at will. We evaluated the ability to hear depth for various tasks: detecting and locating an open window, moving and crossing an open door. Participants directed the modelled depth-camera with a real pointing device that was either held in the hand or fastened on the head. Mixed effects on response delays were analyzed with a linear model to highlight the respective importance of the pointing device, the target specifics and the individual participants. Results are encouraging to further exploit our prototyping set-up and test many solutions by implementing e.g., environments, sensor devices, transcoding rules, and pointing devices including the use of an eye-tracker. Copyright © 2018 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","Eye tracking; Motion capture; Pointing device; Sensory substitution; Virtual environments","Cameras; Eye tracking; Virtual reality; 3-D environments; Linear modeling; Motion capture; Motion capture system; Optical information; Pointing devices; Response delays; Sensory substitution; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85047872804
"Puttemans S., Callemein T., Goedeme T.","55748609400;57195409711;6506388986;","Building robust industrial applicable object detection models using transfer learning and single pass deep learning architectures",2018,"VISIGRAPP 2018 - Proceedings of the 13th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","5",,,"209","217",,2,"10.5220/0006562002090217","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047816144&doi=10.5220%2f0006562002090217&partnerID=40&md5=d4cf9dfc691c3c493dbfd1d38f9f581e","KU Leuven, EAVISE Research Group, Jan Pieter De Nayerlaan 5, Sint-Katelijne-Waver, Belgium","Puttemans, S., KU Leuven, EAVISE Research Group, Jan Pieter De Nayerlaan 5, Sint-Katelijne-Waver, Belgium; Callemein, T., KU Leuven, EAVISE Research Group, Jan Pieter De Nayerlaan 5, Sint-Katelijne-Waver, Belgium; Goedeme, T., KU Leuven, EAVISE Research Group, Jan Pieter De Nayerlaan 5, Sint-Katelijne-Waver, Belgium","The uprising trend of deep learning in computer vision and artificial intelligence can simply not be ignored. On the most diverse tasks, from recognition and detection to segmentation, deep learning is able to obtain state-of-the-art results, reaching top notch performance. In this paper we explore how deep convolutional neural networks dedicated to the task of object detection can improve our industrial-oriented object detection pipelines, using state-of-the-art open source deep learning frameworks, like Darknet. By using a deep learning architecture that integrates region proposals, classification and probability estimation in a single run, we aim at obtaining real-time performance. We focus on reducing the needed amount of training data drastically by exploring transfer learning, while still maintaining a high average precision. Furthermore we apply these algorithms to two industrially relevant applications, one being the detection of promotion boards in eye tracking data and the other detecting and recognizing packages of warehouse products for augmented advertisements. © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Deep learning; Industrial specific solutions; Object detection","Computer graphics; Computer vision; Deep learning; Deep neural networks; Eye tracking; Network architecture; Neural networks; Object recognition; Convolutional neural network; Learning architectures; Learning frameworks; Probability estimation; Real time performance; State of the art; Training data; Transfer learning; Object detection",Conference Paper,"Final","",Scopus,2-s2.0-85047816144
"Chanijani S.S.M., Raue F., Hassanzadeh S.D., Agne S., Bukhari S.S., Dengel A.","46661601300;57141992100;57201380984;55920412300;25924725100;6603764314;","Reading type classification based on generative models and bidirectional long short-term memory",2018,"CEUR Workshop Proceedings","2068",,,"","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044510460&partnerID=40&md5=b3e4343ff9010e2c515e60d2f8aadb40","TU Kaiserslautern, German Research Center for Artificial Intelligence, Germany; TU Clausthal, Germany","Chanijani, S.S.M., TU Kaiserslautern, German Research Center for Artificial Intelligence, Germany; Raue, F., TU Kaiserslautern, German Research Center for Artificial Intelligence, Germany; Hassanzadeh, S.D., TU Clausthal, Germany; Agne, S., TU Kaiserslautern, German Research Center for Artificial Intelligence, Germany; Bukhari, S.S., TU Kaiserslautern, German Research Center for Artificial Intelligence, Germany; Dengel, A., TU Kaiserslautern, German Research Center for Artificial Intelligence, Germany","Measuring the attention of users is necessary to design smart Human Computer Interaction (HCI) systems. Particularly, in reading, the reading types, so-called reading, skimming, and scanning are signs to express the degree of attentiveness. Eye movements are informative spatiotemporal data to measure quality of reading. Eye tracking technology is the tool to record eye movements. Even though there is increasing usage of eye trackers in research and especially in psycholinguistics, collecting appropriate task-specific eye movements data is expensitive and time consuming. Moreover, machine learning tools like Recurrent Neural Networks need large enough samples to be trained. Hence, designing a generative model in order to have reliable research-oriented synthetic eye movements is desirable. This paper has two main contributions. First, a generative model in order to synthesize reading, skimming, and scanning in reading is developed. Second, in order to evaluate the generative model, a bidirectional Long Short- Term Memory (BLSTM) is proposed. It was trained with synthetic data and tested with real-world eye movements to classify reading, skimming, and scanning where more than 95% classification accuracy is achieved. © 2018 Copyright for the individual papers remains with the authors.","Classification; Eye tracking; Gaussian mixture models; Generative models; Hierarchical hidden Markov models; LSTM; Reading; Reading type; Recurrent neural networks; Scanning; Skimming; Synthetic data","Brain; Classification (of information); Eye tracking; Hidden Markov models; Human computer interaction; Learning systems; Long short-term memory; Recurrent neural networks; Scanning; Trellis codes; User interfaces; Gaussian Mixture Model; Generative model; Hierarchical Hidden Markov Model; LSTM; Reading; Reading type; Skimming; Synthetic data; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85044510460
"Pérez-Belis V., Agost M.-J., Vergara M.","48162008200;13007931100;7005618198;","Consumers’ visual attention and emotional perception of sustainable product information: Case study of furniture",2018,"Advances in Intelligent Systems and Computing","739",,,"239","248",,1,"10.1007/978-981-10-8612-0_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044184931&doi=10.1007%2f978-981-10-8612-0_26&partnerID=40&md5=ea8a17b3f63855fc8ebb59efe94f89a4","Mechanical Engineering and Construction Department, Universitat Jaume I, Castellón, Spain","Pérez-Belis, V., Mechanical Engineering and Construction Department, Universitat Jaume I, Castellón, Spain; Agost, M.-J., Mechanical Engineering and Construction Department, Universitat Jaume I, Castellón, Spain; Vergara, M., Mechanical Engineering and Construction Department, Universitat Jaume I, Castellón, Spain","Transparency about product information is increasingly accessible, due to factors such as market globalization and the fast growing of new communication technologies. This fact has facilitated the appearance of well-informed consumers who are concerned about the repercussions of their purchasing choices on, for example, their own health, the environment, or even the social conditions of workers. Information about sustainable aspects of product lifecycle is often presented through labels located on the product itself or on its packaging. Apart from providing information to consumers, these labels need to connect to their environmental subjective perception, in order to integrate them into their purchase criteria. However, literature shows that some of them are not effective or well understood by consumers. Through the application of product semantics and eye-tracking, an objective technique measuring visual attention, this communication shows the results of a study on consumer perception of different sustainable labels applied to furniture. Twenty-six subjects were recruited to analyze their perception and knowledge about labels related to three different sustainable dimensions: environment, workers respect and customer health, through semantic differential evaluation. Besides, visual attention was measured using eye-tracking technology. Some descriptive and comparative statistical analyses about visual behavior on different areas of interest were completed considering eye-tracking metrics. Results show some significant differences in the time spent looking at label areas, and also in the number of right answers, depending on different factors. © 2018, Springer Nature Singapore Pte Ltd.","Eye-Tracking; Product Information; Sustainability; Visual Attention","Eye movements; Eye tracking; Life cycle; Sales; Semantics; Sustainable development; Visual communication; Communication technologies; Eye tracking technologies; Market globalization; Product information; Semantic differential; Subjective perceptions; Sustainable products; Visual Attention; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85044184931
"Śledzianowski A., Szymański A., Szlufik S., Koziorowski D.","57201155975;56304189900;55334567200;7801382272;","Rough Set Data Mining Algorithms and Pursuit Eye Movement Measurements Help to Predict Symptom Development in Parkinson’s Disease",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10752 LNAI",,,"428","435",,2,"10.1007/978-3-319-75420-8_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043593018&doi=10.1007%2f978-3-319-75420-8_41&partnerID=40&md5=e6c0f90067e3f7da295f0a65758ae73e","Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Neurology, Faculty of Health Science, Medical University of Warsaw, Warsaw, 03-242, Poland","Śledzianowski, A., Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Szymański, A., Polish-Japanese Academy of Information Technology, Warsaw, 02-008, Poland; Szlufik, S., Neurology, Faculty of Health Science, Medical University of Warsaw, Warsaw, 03-242, Poland; Koziorowski, D., Neurology, Faculty of Health Science, Medical University of Warsaw, Warsaw, 03-242, Poland","This article presents research on pursuit eye movements tests conducted on patients with Parkinson’s disease in various stages of disease and phases of treatment. The aim of described experiment was to develop algorithms allowing for measurements of parameters of pursuit eye movement in order to reference calculated results to the previously collected neurological data of patients. An additional objective of the experiment was to develop an example of data-mining procedure, allowing for classification of neurological symptoms based on oculometric measurements. Definition of such correlation enables assignment of particular patient to a given neurological group on the base of parameters values of pursuit eye movements. By using created decision table, we have achieved good results of prediction of the neurological parameter UPDRS, with total accuracy of 93.3% and total coverage of 60%. This allows for better evaluation of stage of the disease and its progression. It also might provide additional tool in determining efficacy of different disease treatments. © Springer International Publishing AG, part of Springer Nature 2018.","Data mining; Eye tracking; Machine learning; Neurodegenerative disease; Parkinson’s disease; Pursuit eye movements","Data mining; Database systems; Decision tables; Diseases; Eye tracking; Learning systems; Neurodegenerative diseases; Neurology; Patient treatment; Disease treatment; Eye movement measurement; Measurements of; Neurological symptoms; Parkinson; Rough set datum; Symptom development; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85043593018
"Zhao T., Wang Y., Fu X.","57192707963;55211773900;7402204912;","Refining eye synthetic images via coarse-to-fine adversarial networks for appearance-based gaze estimation",2018,"Communications in Computer and Information Science","819",,,"419","428",,3,"10.1007/978-981-10-8530-7_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043366675&doi=10.1007%2f978-981-10-8530-7_41&partnerID=40&md5=4e6fb13f8556f72404391f6bc1a071b6","Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China","Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Wang, Y., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China, School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Recently, several models have achieved great success in terms of reducing the gap between synthetic and real image distributions with large unlabeled real data. However, collecting such large amounts of real data costs a lot of labouring and training them requires high memory. To reduce the gap with less real data, we propose a coarse-to-fine refine eye image method combining coarse model net and fine model net through adversarial training. Coarse model net is a feed-forward convolutional neural network aiming to transform synthetic eye images into coarse images. Fine model net is a modified Generative Adversarial Networks (GANs) which add realism to coarse images using unlabeled real data. Experimental results show that the proposed method achieves similar distributions as recent work but decreasing real data at least one order of magnitude. In addition, a significant accuracy improvement for gaze estimation with refined synthetic eye images is observed. © Springer Nature Singapore Pte Ltd. 2018.","Feed-forward convolutional neural network; Generative adversarial networks; Image synthesis","Convolution; Neural networks; Accuracy Improvement; Adversarial networks; Appearance based; Coarse to fine; Convolutional neural network; Gaze estimation; Image synthesis; Synthetic images; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85043366675
"Carette R., Cilia F., Dequen G., Bosche J., Guerin J.-L., Vandromme L.","57200860085;57200855464;23396657900;6701595130;57200857001;57019167100;","Automatic Autism Spectrum Disorder Detection Thanks to Eye-Tracking and Neural Network-Based Approach",2018,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST","225",,,"75","81",,3,"10.1007/978-3-319-76213-5_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042554864&doi=10.1007%2f978-3-319-76213-5_11&partnerID=40&md5=19c4eadd5c1c22b26f3010f8ea583c8d","Laboratoire Modélisation, Information et Systèmes, Université de Picardie Jules Verne, Amiens, Picardie, France; Centre de Recherche en Psychologie: Cognition, Psychisme et Organisations, Université de Picardie Jules Verne, Amiens, Picardie, France","Carette, R., Laboratoire Modélisation, Information et Systèmes, Université de Picardie Jules Verne, Amiens, Picardie, France; Cilia, F., Centre de Recherche en Psychologie: Cognition, Psychisme et Organisations, Université de Picardie Jules Verne, Amiens, Picardie, France; Dequen, G., Laboratoire Modélisation, Information et Systèmes, Université de Picardie Jules Verne, Amiens, Picardie, France; Bosche, J., Laboratoire Modélisation, Information et Systèmes, Université de Picardie Jules Verne, Amiens, Picardie, France; Guerin, J.-L., Laboratoire Modélisation, Information et Systèmes, Université de Picardie Jules Verne, Amiens, Picardie, France; Vandromme, L., Centre de Recherche en Psychologie: Cognition, Psychisme et Organisations, Université de Picardie Jules Verne, Amiens, Picardie, France","Autism spectrum disorder (ASD) is a neurodevelopmental disorder quite wide and its numerous variations render diagnosis hard. Some works have proven that children suffering from autism have trouble keeping their attention and tend to have a less focused sight. On top of that, eye-tracking systems enable the recording of precise eye focus on a screen. This paper deals with automatic detection of autism spectrum disorder thanks to eye-tracked data and an original Machine Learning approach. Focusing on data that describes the saccades of the patient’s sight, we distinguish, out of our six test patients, young autistic individuals from those with no problems in 83% (five) of tested patients, with a results confidence up to 95%. © 2018, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","Autism spectrum disorder; Data processing; eHealth; Eye-tracking; Long Short-Term Memory (LSTM); Neural network","Data handling; Data processing; Diagnosis; Diseases; eHealth; Eye movements; Health care; Internet of things; Learning systems; Long short-term memory; Neural networks; Autism spectrum disorders; Automatic Detection; Eye tracking systems; Machine learning approaches; Network-based approach; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85042554864
"Wu X., Li J., Wu Q., Sun J., Yan H.","57200820459;55441752700;57043789700;12645161300;56471078100;","Block-Wise Gaze Estimation Based on Binocular Images",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10749 LNCS",,,"477","487",,,"10.1007/978-3-319-75786-5_38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042520818&doi=10.1007%2f978-3-319-75786-5_38&partnerID=40&md5=5625e45d5597f23ea58e0fb1562cc6f9","School of Information Science and Engineering, Shandong University, Jinan, China; School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, China","Wu, X., School of Information Science and Engineering, Shandong University, Jinan, China; Li, J., School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China; Wu, Q., School of Information Science and Engineering, Shandong University, Jinan, China; Sun, J., School of Information Science and Engineering, Shandong Normal University, Jinan, China; Yan, H., School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, China","Appearance-based gaze estimation methods have been proved to be highly effective. Different from the previous methods that estimate gaze direction based on left or right eye image separately, we propose a binocular-image based gaze estimation method. Considering the challenges in estimating the precise gaze points via regression models, we estimate the block-wise gaze position by classifying the binocular images via convolutional neural network (CNN) in the proposed method. We divide the screen of the desktop computer into 2 × 3 and 6 × 9 blocks respectively, label the binocular images with their corresponding gazed block positions, train a convolutional neural network model to classify the eye images according to their labels, and estimate the gazed block through the CNN-based classification. The experimental results demonstrate that the proposed gaze estimation method based on binocular images can reach higher accuracy than those based on monocular images. And the proposed method shows its great potential in practical touch screen-based applications. © 2018, Springer International Publishing AG, part of Springer Nature.","Appearance-based; Convolutional neural network (CNN); Eye image; Gaze block; Gaze estimation","Binoculars; Bins; Convolution; Estimation; Neural networks; Regression analysis; Touch screens; Appearance based; Convolutional Neural Networks (CNN); Eye images; Gaze block; Gaze estimation; Image classification",Conference Paper,"Final","",Scopus,2-s2.0-85042520818
"Podder P.K., Paul M., Murshed M.","56469795400;7402403635;6604064650;","A Novel No-reference Subjective Quality Metric for Free Viewpoint Video Using Human Eye Movement",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10749 LNCS",,,"237","251",,,"10.1007/978-3-319-75786-5_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042491464&doi=10.1007%2f978-3-319-75786-5_20&partnerID=40&md5=57da684259571601c66ebe9561758348","School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW  2795, Australia; School of Information Technology, Federation University Churchill, Churchill, VIC  3842, Australia","Podder, P.K., School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW  2795, Australia; Paul, M., School of Computing and Mathematics, Charles Sturt University, Bathurst, NSW  2795, Australia; Murshed, M., School of Information Technology, Federation University Churchill, Churchill, VIC  3842, Australia","The free viewpoint video (FVV) allows users to interactively control the viewpoint and generate new views of a dynamic scene from any 3D position for better 3D visual experience with depth perception. Multiview video coding exploits both texture and depth video information from various angles to encode a number of views to facilitate FVV. The usual practice for the single view or multiview quality assessment is characterized by evolving the objective quality assessment metrics due to their simplicity and real time applications such as the peak signal-to-noise ratio (PSNR) or the structural similarity index (SSIM). However, the PSNR or SSIM requires reference image for quality evaluation and could not be successfully employed in FVV as the new view in FVV does not have any reference view to compare with. Conversely, the widely used subjective estimator- mean opinion score (MOS) is often biased by the testing environment, viewers mode, domain knowledge, and many other factors that may actively influence on actual assessment. To address this limitation, in this work, we devise a no-reference subjective quality assessment metric by simply exploiting the pattern of human eye browsing on FVV. Over different quality contents of FVV, the participants eye-tracker recorded spatio-temporal gaze-data indicate more concentrated eye-traversing approach for relatively better quality. Thus, we calculate the Length, Angle, Pupil-size, and Gaze-duration features from the recorded gaze trajectory. The content and resolution invariant operation is carried out prior to synthesizing them using an adaptive weighted function to develop a new quality metric using eye traversal (QMET). Tested results reveal that the proposed QMET performs better than the SSIM and MOS in terms of assessing different aspects of coded video quality for a wide range of FVV contents. © 2018, Springer International Publishing AG, part of Springer Nature.","Eye-tracking; Eye-traversal; Free viewpoint video; Gaze-trajectory; HEVC; QMET; Quality assessment","Depth perception; Eye tracking; Image coding; Image quality; Quality control; Signal to noise ratio; Subjective testing; Video signal processing; Eye-traversal; Free-viewpoint video; HEVC; QMET; Quality assessment; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85042491464
"Li J., Xia C., Chen X.","37067453800;57141559400;13410318100;","A benchmark dataset and saliency-guided stacked autoencoders for video-based salient object detection",2018,"IEEE Transactions on Image Processing","27","1","8066351","349","364",,50,"10.1109/TIP.2017.2762594","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038407326&doi=10.1109%2fTIP.2017.2762594&partnerID=40&md5=e5d80c057bdda755c3f8a6994d35ac33","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China","Li, J., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China, International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; Xia, C., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; Chen, X., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China","Image-based salient object detection (SOD) has been extensively studied in past decades. However, video-based SOD is much less explored due to the lack of large-scale video datasets within which salient objects are unambiguously defined and annotated. Toward this end, this paper proposes a video-based SOD dataset that consists of 200 videos. In constructing the dataset, we manually annotate all objects and regions over 7650 uniformly sampled keyframes and collect the eye-tracking data of 23 subjects who free-view all videos. From the user data, we find that salient objects in a video can be defined as objects that consistently pop-out throughout the video, and objects with such attributes can be unambiguously annotated by combining manually annotated object/region masks with eye-tracking data of multiple subjects. To the best of our knowledge, it is currently the largest dataset for video-based salient object detection. Based on this dataset, this paper proposes an unsupervised baseline approach for video-based SOD by using saliency-guided stacked autoencoders. In the proposed approach, multiple spatiotemporal saliency cues are first extracted at the pixel, superpixel, and object levels. With these saliency cues, stacked autoencoders are constructed in an unsupervised manner that automatically infers a saliency score for each pixel by progressively encoding the high-dimensional saliency cues gathered from the pixel and its spatiotemporal neighbors. In experiments, the proposed unsupervised approach is compared with 31 state-of-the-art models on the proposed dataset and outperforms 30 of them, including 19 image-based classic (unsupervised or non-deep learning) models, six image-based deep learning models, and five video-based unsupervised models. Moreover, benchmarking results show that the proposed dataset is very challenging and has the potential to boost the development of video-based SOD. © 2017 IEEE.","Model benchmarking; Salient object detection; Stacked autoencoders; Video dataset","Animals; Benchmarking; Deep learning; Learning systems; Object recognition; Pixels; Statistical tests; Autoencoders; Benchmark testing; Model benchmarking; Motion segmentation; Salient object detection; Solid model; Spatiotemporal phenomena; Video dataset; Object detection",Article,"Final","",Scopus,2-s2.0-85038407326
"Wang Y., Zhao T., Ding X., Peng J., Bian J., Fu X.","55211773900;57192707963;57194286638;57192708626;57200854878;7402204912;","Learning a gaze estimator with neighbor selection from large-scale synthetic eye images",2018,"Knowledge-Based Systems","139",,,"41","49",,11,"10.1016/j.knosys.2017.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031806955&doi=10.1016%2fj.knosys.2017.10.010&partnerID=40&md5=92ec8e308d2b9c5a32ece920cb99fbaa","School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian 116024, China; Information Science and Technology College, Dalian Maritime University, Dalian 116026, China","Wang, Y., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian 116024, China, Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; Ding, X., Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; Peng, J., Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; Bian, J., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian 116024, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian 116026, China","Appearance-based gaze estimation works well in inferring human gaze under real-world condition. But one of the significant limitations in appearance-based methods is the need for huge amounts of training data. Eye image synthesis addresses this problem by generating huge amounts of synthetic eye images with computer graphics. To fully use the large-scale synthetic eye images, a simple-but-effective appearance-based gaze estimation framework with neighbor selection is proposed in this paper. The proposed framework hierarchically fuses multiple k-NN queries (in head pose, pupil center and eye appearance spaces) to choose closest samples with more relevant features. Considering the structure characters of the closet samples, neighbor regression methods then can be applied to predict the gaze directions. Experimental results demonstrate that the representative neighbor regression methods under the proposed framework achieve better performance for within-subject and cross-subject gaze estimation. © 2017 Elsevier B.V.","Cross-subject; Gaze estimation; Learning-by-synthesis; Neighbor selection","Computer graphics; Nearest neighbor search; Appearance based; Appearance-based methods; Cross-subject; Gaze estimation; Neighbor selection; Regression method; Relevant features; Structure character; Regression analysis",Article,"Final","",Scopus,2-s2.0-85031806955
"Sun Y., Li Q., Zhang H., Zou J.","57195263604;57219134628;57195263285;7401551846;","The application of eye tracking in education",2018,"Smart Innovation, Systems and Technologies","82",,,"27","33",,1,"10.1007/978-3-319-63859-1_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026675113&doi=10.1007%2f978-3-319-63859-1_4&partnerID=40&md5=2e87ebcffa13aca3055e0b6aa1a1ee47","School of Mathematical Sciences, Capital Normal University, Beijing, China; Institute of Image Processing and Pattern Recognition, North China University of Technology, Beijing, 100144, China","Sun, Y., School of Mathematical Sciences, Capital Normal University, Beijing, China; Li, Q., School of Mathematical Sciences, Capital Normal University, Beijing, China; Zhang, H., Institute of Image Processing and Pattern Recognition, North China University of Technology, Beijing, 100144, China; Zou, J., Institute of Image Processing and Pattern Recognition, North China University of Technology, Beijing, 100144, China","The application of emerging information technologies to traditional teaching methods can not only enhance the value of technologies, but also improve the teaching progress and integrate different fields in education with efficiency. 3D printing, virtual reality and eye tracking technology have been found more and more applications in education recently. In this paper, through the improvement of eye tracking algorithm, we developed an education software package based on eye tracking technology. By analyzing the students’ eye movement data, teachers are able to improve the teaching quality by improving the teaching framework. Students can also focus on their own interests more to develop a reasonable learning plan. The application of eye tracking technology in the field of education has a great potential to promote the application of technology and to improve the educational standards. © Springer International Publishing AG 2018.","Education; Eye; Software; Tracking","3D printers; Computer software; Education; Educational technology; Engineering education; Multimedia signal processing; Signal processing; Students; Surface discharges; Teaching; Virtual reality; 3-D printing; Education softwares; Educational standards; Emerging information technologies; Eye movement datum; Eye tracking technologies; Teaching methods; Teaching quality; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85026675113
"Zhang Y., Yang H., Xu Y., Feng L.","56964064500;36618479600;57194727939;57191069964;","Comparison of visual comfort and fatigue between watching different types of 3D TVS as measured by eye tracking",2018,"Advances in Intelligent Systems and Computing","586",,,"175","186",,1,"10.1007/978-3-319-60642-2_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021836298&doi=10.1007%2f978-3-319-60642-2_16&partnerID=40&md5=a7089c3a3a78c3e658296aeba4cb7d6f","AQSIQ Key Laboratory of Human Factor and Ergonomics, China National Institute of Standardization, Beijing, China; Academy of Psychology and Behavior, Tianjin Normal University, Tianjin, China","Zhang, Y., AQSIQ Key Laboratory of Human Factor and Ergonomics, China National Institute of Standardization, Beijing, China; Yang, H., Academy of Psychology and Behavior, Tianjin Normal University, Tianjin, China; Xu, Y., Academy of Psychology and Behavior, Tianjin Normal University, Tianjin, China; Feng, L., AQSIQ Key Laboratory of Human Factor and Ergonomics, China National Institute of Standardization, Beijing, China","An eye movement study was conducted to make clear whether different types of 3D TVs would help to relieve visual fatigue after watching films for a long time. 64 undergraduates and ordinary researchers were measured to compare the difference of watching different 3D TVs by Eye-tracking. 64 participants were divided into four groups after being matched, and the four matched groups were separately arranged to watch Switched 3D TV, polarized 3D TV, naked 3D TV and 2D TV. They watched the same video contents which were scenery video and a film, while eye movement data were recorded. The results showed that: (1) with the increase of watching time, participants’ fatigue also increased. The blink frequency, blink counts, blink total duration, saccade angle and saccade velocity of the four group participants who watched different types of 3D TVs remarkably increased in the overall trend with time; (2) There was a remarkable difference between the participants watching polarized 3D TV and others watching 3D TV and 2D TV in average saccade duration, saccade duration peak, average saccade angle and saccade total angle, which might indicate that the principle of polarized 3D TV would affect users’ saccade duration and saccade distance during watching videos. (3) The saccade amplitude of switched 3D TV was significantly higher than that of other three conditions, which might indicate a greater influence of saccade amplitude in switched 3D TV. © Springer International Publishing AG 2018.","3D TV; Eye-tracking; Visual comfort; Visual fatigue","Computer programming; Computer science; Blink frequencies; Eye movement datum; Eye-tracking; Polarized 3D; Saccade distances; Video contents; Visual comfort; Visual fatigue; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85021836298
"Wulff-Jensen A., Bruni L.E.","57194720473;36338567900;","Evaluating ann efficiency in recognizing eeg and eye-tracking evoked potentials in visual-game-events",2018,"Advances in Intelligent Systems and Computing","586",,,"262","274",,,"10.1007/978-3-319-60642-2_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021822056&doi=10.1007%2f978-3-319-60642-2_25&partnerID=40&md5=868344a41b863c9e130724eecad1a1d4","Department of Architecture, Design and Media Technology, Aalborg University Copenhagen, AC Meyervænge 15, Copenhagen, 2450, Denmark","Wulff-Jensen, A., Department of Architecture, Design and Media Technology, Aalborg University Copenhagen, AC Meyervænge 15, Copenhagen, 2450, Denmark; Bruni, L.E., Department of Architecture, Design and Media Technology, Aalborg University Copenhagen, AC Meyervænge 15, Copenhagen, 2450, Denmark","EEG and Eye-tracking signals have customarily been analyzed and inspected visually in order to be correlated to the controlled stimuli. This process has proven to yield valid results as long as the stimuli of the experiment are under complete control (e.g.: the order of presentation). In this study, we have recorded the subject’s electroencephalogram and eye-tracking data while they were exposed to a 2D platform game. In the game we had control over the design of each level by choosing the diversity of actions (i.e. events) afforded to the player. However we had no control over the order in which these actions were undertaken. The psychophysiological signals were synchronized to these game events and used to train and test an artificial neural network in order to evaluate how efficiently such a tool can help us in establishing the correlation, and therefore differentiating among the different categories of events. The highest average accuracies were between 60.25%–72.07%, hinting that it is feasible to recognize reactions to complex uncontrolled stimuli, like game events, using artificial neural networks. © Springer International Publishing AG 2018.","Artificial; Electroencephalogram; Eye-tracking; Game events; Games; Machine learning; Neural network; Psychophysiology; Pupillometry","Complex networks; Electrophysiology; Learning systems; Neural networks; Psychophysiology; Artificial; Eye-tracking; Game events; Games; Pupillometry; Electroencephalography",Conference Paper,"Final","",Scopus,2-s2.0-85021822056
"Jan F.","55247968500;","Pupil localization in image data acquired with near-infrared or visible wavelength illumination",2018,"Multimedia Tools and Applications","77","1",,"1041","1067",,7,"10.1007/s11042-016-4334-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008661148&doi=10.1007%2fs11042-016-4334-x&partnerID=40&md5=14058d34077ee3215eddd6d5c0de4740","Department of Physics, COMSATS Institute of Information Technology, Park Road, Chak Shahzad, Islamabad, 44000, Pakistan","Jan, F., Department of Physics, COMSATS Institute of Information Technology, Park Road, Chak Shahzad, Islamabad, 44000, Pakistan","Pupil localization in human face/eye images has numerous applications, e.g., eye tracking, iris recognition, cataract assessment and surgery, diabetic retinopathy screening, neuropsychiatric disorders diagnosing, and aliveness detection. In real scenario, the pupil localization task suffers from many complications such as pupil’s constriction and dilation moments, light reflections, eyelids and eyelashes, and cataract disease. To resolve this issue, this study proposes an accurate and fast pupil localization scheme. It performs relatively well for eyeimages acquired either with the near infrared (NIR) or visible wavelength (VW) illumination. First, it effectively preprocesses the input eyeimage. Next, it coarsely marks pupil location using a scheme comprising an adaptive threshold and two-dimensional (2D) object properties. Then, it validates pupil location via an effective test involving global gray-level statistics. If it finds pupil location invalid, then it localizes pupil through a hybrid of the Hough transform and image global gray-level statistics. Finally, it localizes the fine pupillary boundary through a hybrid of the Fourier series and image’s gradients. Its experimental results obtained on numerous publically available iris datasets demonstrate its superiority over most of the contemporary schemes. © 2017, Springer Science+Business Media New York.","Eye tracking; Gaze detection; Iris biometric systems; Pupil localization","Biometrics; Diagnosis; Eye protection; Feature extraction; Fourier series; Hough transforms; Location; Aliveness detection; Diabetic retinopathy screening; Eye-tracking; Gaze detection; Iris biometrics; Neuropsychiatric disorder; Pupil localization; Two Dimensional (2 D); Infrared devices",Article,"Final","",Scopus,2-s2.0-85008661148
"Zhong M., Lan X., Wang K.","57203021181;56114033200;57202214365;","Saliency-based joint distortion model for 3D video coding",2017,"Proceedings - 2017 Chinese Automation Congress, CAC 2017","2017-January",,,"720","725",,1,"10.1109/CAC.2017.8242861","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050299032&doi=10.1109%2fCAC.2017.8242861&partnerID=40&md5=7e0bf5006f1d13eb904e6bceaa098f38","Institute of Artificial Intelligence and Robotics, Xi'An Jiaotong University, Xi'an, China","Zhong, M., Institute of Artificial Intelligence and Robotics, Xi'An Jiaotong University, Xi'an, China; Lan, X., Institute of Artificial Intelligence and Robotics, Xi'An Jiaotong University, Xi'an, China; Wang, K., Institute of Artificial Intelligence and Robotics, Xi'An Jiaotong University, Xi'an, China","In 3D video system, storing and transmitting the large amount of high definition videos is the main challenge. To further reduce video content related redundant data in 3D video encoded with HEVC, this paper proposes an saliency-based joint distortion model for 3D video encoding, which jointly considers the distortion of texture and depth, as well as synthesis distortion. With this model, two optimization methods are proposed. One method weighs the distortion of each coding unit (CU) according to the saliency information to protect the salient regions. And the other method optimizes the distribution of depth video according to the locality of virtual synthesis distortion (VSD), and weighs the CU-level distortion of texture video and VSD with the saliency information of texture video. Different from existing methods, saliency information based CU-level bit allocation is used in both texture and depth videos. Then we optimize saliency-based joint distortion to minimize the bits while keeping the visual quality the same. The experimental results of visual quality show that the proposed methods have gains on eyetracking PSNR (EWPSNR) with the same bitrate as latest HEVC and an existing saliency encoding method for 3D video. Besides, with the same visual quality, our method costs fewer bits. © 2017 IEEE.","3D video; eye-tracking; HEVC; saliency information","Encoding (symbols); Eye tracking; Image coding; Signal encoding; Video cameras; 3-d video systems; 3-D videos; Distortion model; Encoding methods; HEVC; High definition video; Optimization method; saliency information; Video signal processing",Conference Paper,"Final","",Scopus,2-s2.0-85050299032
"Bilac M., Chamoux M., Lim A.","57201367669;57201364987;36801929900;","Gaze and filled pause detection for smooth human-robot conversations",2017,"IEEE-RAS International Conference on Humanoid Robots",,,,"297","304",,1,"10.1109/HUMANOIDS.2017.8246889","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044476245&doi=10.1109%2fHUMANOIDS.2017.8246889&partnerID=40&md5=d3699cec63430dbe27481c10dc501004","SoftBank Robotics Europe, 43 Rue Colonel Pierre Avia, Paris, 75015, France","Bilac, M., SoftBank Robotics Europe, 43 Rue Colonel Pierre Avia, Paris, 75015, France; Chamoux, M., SoftBank Robotics Europe, 43 Rue Colonel Pierre Avia, Paris, 75015, France; Lim, A., SoftBank Robotics Europe, 43 Rue Colonel Pierre Avia, Paris, 75015, France","Let the human speak! Interactive robots and voice interfaces such as Pepper, Amazon Alexa, and OK Google are becoming more and more popular, allowing for more natural interaction compared to screens or keyboards. One issue with voice interfaces is that they tend to require a 'robotic' flow of human speech. Humans must be careful to not produce disfluencies, such as hesitations or extended pauses between words. If they do, the agent may assume that the human has finished their speech turn, and interrupts them mid-Thought. Interactive robots often rely on the same limited dialogue technology built for speech interfaces. Yet humanoid robots have the potential to also use their vision systems to determine when the human has finished their speaking turn. In this paper, we introduce HOMAGE (Human-rObot Multimodal Audio and Gaze End-of-Turn), a multimodal turntaking system for conversational humanoid robots. We created a dataset of humans spontaneously hesitating when responding to a robot's open-ended questions such as, 'What was your favorite moment this year?'. Our analyses found that users produced both auditory filled pauses such as 'uhhh', as well as gaze away from the robot to keep their speaking turn. We then trained a machine learning system to detect the auditory filled pauses and integrated it along with gaze into the Pepper humanoid robot's real-Time dialog system. Experiments with 28 naive users revealed that adding auditory filled pause detection and gaze tracking significantly reduced robot interruptions. Furthermore, user turns were 2.1 times longer (without repetitions), suggesting that this strategy allows humans to express themselves more, toward less time pressure and better robot listeners. © 2017 IEEE.",,"Anthropomorphic robots; Robot learning; Robotics; Conversational humanoid robots; Dialog systems; Interactive robot; Natural interactions; Open-ended questions; Speech interface; Time pressures; Voice interfaces; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85044476245
"Deng H., Zhu W.","57200613552;55430100200;","Monocular Free-Head 3D Gaze Tracking with Deep Learning and Geometry Constraints",2017,"Proceedings of the IEEE International Conference on Computer Vision","2017-October",,"8237603","3162","3171",,63,"10.1109/ICCV.2017.341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041922315&doi=10.1109%2fICCV.2017.341&partnerID=40&md5=ee9e8db2c6173ac07937327fda39796e","Nanjing University, China; Tsinghua University, China","Deng, H., Nanjing University, China; Zhu, W., Tsinghua University, China","Free-head 3D gaze tracking outputs both the eye location and the gaze vector in 3D space, and it has wide applications in scenarios such as driver monitoring, advertisement analysis and surveillance. A reliable and low-cost monocular solution is critical for pervasive usage in these areas. Noticing that a gaze vector is a composition of head pose and eyeball movement in a geometrically deterministic way, we propose a novel gaze transform layer to connect separate head pose and eyeball movement models. The proposed decomposition does not suffer from head-gaze correlation overfitting and makes it possible to use datasets existing for other tasks. To add stronger supervision for better network training, we propose a two-step training strategy, which first trains sub-tasks with rough labels and then jointly trains with accurate gaze labels. To enable good cross-subject performance under various conditions, we collect a large dataset which has full coverage of head poses and eyeball movements, contains 200 subjects, and has diverse illumination conditions. Our deep solution achieves state-of-the-art gaze tracking accuracy, reaching 5.6° cross-subject prediction error using a small network running at 1000 fps on a single CPU (excluding face alignment time) and 4.3° cross-subject error with a deeper network. © 2017 IEEE.",,"Computer vision; Tracking (position); Vector spaces; Driver monitoring; Eyeball movements; Geometry constraints; Illumination conditions; Network training; Prediction errors; State of the art; Two-step training; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85041922315
"Jiang M., Zhao Q.","56027704500;55743334300;","Learning Visual Attention to Identify People with Autism Spectrum Disorder",2017,"Proceedings of the IEEE International Conference on Computer Vision","2017-October",,"8237616","3287","3296",,39,"10.1109/ICCV.2017.354","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041895766&doi=10.1109%2fICCV.2017.354&partnerID=40&md5=6c427becda9a0b584d4e44831c443891","University of Minnesota, United States","Jiang, M., University of Minnesota, United States; Zhao, Q., University of Minnesota, United States","This paper presents a novel method for quantitative and objective diagnoses of Autism Spectrum Disorder (ASD) using eye tracking and deep neural networks. ASD is prevalent, with 1.5% of people in the US. The lack of clinical resources for early diagnoses has been a long-lasting issue. This work differentiates itself with three unique features: first, the proposed approach is data-driven and free of assumptions, important for new discoveries in understanding ASD as well as other neurodevelopmental disorders. Second, we concentrate our analyses on the differences in eye movement patterns between healthy people and those with ASD. An image selection method based on Fisher scores allows feature learning with the most discriminative contents, leading to efficient and accurate diagnoses. Third, we leverage the recent advances in deep neural networks for both prediction and visualization. Experimental results show the superior performance of our method in terms of multiple evaluation metrics used in diagnostic tests. © 2017 IEEE.",,"Computer vision; Deep learning; Deep neural networks; Diseases; Eye movements; Autism spectrum disorders; Clinical resources; Diagnostic tests; Evaluation metrics; Eye movement patterns; Feature learning; Objective diagnosis; Visual Attention; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85041895766
"Wang K., Ji Q.","56637259500;18935108400;","Real Time Eye Gaze Tracking with 3D Deformable Eye-Face Model",2017,"Proceedings of the IEEE International Conference on Computer Vision","2017-October",,"8237376","1003","1011",,46,"10.1109/ICCV.2017.114","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041894764&doi=10.1109%2fICCV.2017.114&partnerID=40&md5=7688ca08813a7ad66987dd1f22cacf92","ECSE Department, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States","Wang, K., ECSE Department, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States; Ji, Q., ECSE Department, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States","3D model-based gaze estimation methods are widely explored because of their good accuracy and ability to handle free head movement. Traditional methods with complex hardware systems (Eg. infrared lights, 3D sensors, etc.) are restricted to controlled environments, which significantly limit their practical utilities. In this paper, we propose a 3D model-based gaze estimation method with a single web-camera, which enables instant and portable eye gaze tracking. The key idea is to leverage on the proposed 3D eye-face model, from which we can estimate 3D eye gaze from observed 2D facial landmarks. The proposed system includes a 3D deformable eye-face model that is learned offline from multiple training subjects. Given the deformable model, individual 3D eye-face models and personal eye parameters can be recovered through the unified calibration algorithm. Experimental results show that the proposed method outperforms state-of-the-art methods while allowing convenient system setup and free head movement. A real time eye tracking system running at 30 FPS also validates the effectiveness and efficiency of the proposed method. © 2017 IEEE.",,"Computer vision; Deformation; Tracking (position); Complex hardware; Controlled environment; Deformable modeling; Effectiveness and efficiencies; Eye gaze tracking; Real-time eye tracking; State-of-the-art methods; Unified calibration; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85041894764
"Dini A., Murko C., Yahyanejad S., Augsdorfer U., Hofbaur M., Paletta L.","57193756837;57193761146;36474258100;6507964057;6506482672;6602696802;","Measurement and prediction of situation awareness in human-robot interaction based on a framework of probabilistic attention",2017,"IEEE International Conference on Intelligent Robots and Systems","2017-September",,"8206301","4354","4361",,8,"10.1109/IROS.2017.8206301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041951948&doi=10.1109%2fIROS.2017.8206301&partnerID=40&md5=03cc91d533f4a91249162793ec1844c2","DIGITAL - Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, Austria; ROBOTICS - Institute for Robotics and Mechatronics, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Klagenfurt, Austria; Graz University of Technology, Institute for Computer Graphics and Knowledge Visualisation, Graz, Austria","Dini, A., DIGITAL - Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, Austria; Murko, C., DIGITAL - Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, Austria; Yahyanejad, S., ROBOTICS - Institute for Robotics and Mechatronics, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Klagenfurt, Austria; Augsdorfer, U., Graz University of Technology, Institute for Computer Graphics and Knowledge Visualisation, Graz, Austria; Hofbaur, M., ROBOTICS - Institute for Robotics and Mechatronics, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Klagenfurt, Austria; Paletta, L., DIGITAL - Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, Austria","Human attention processes play a major role in the optimization of human-robot interaction (HRI) systems. This work describes a novel methodology to measure and predict situation awareness and from this overall performance from gaze features in real-time. The awareness about scene objects of interest is described by 3D gaze analysis using data from wearable eye tracking glasses and a precise optical tracking system. A probabilistic framework of uncertainty considers coping with measurement errors in eye and position estimation. Comprehensive experiments on HRI were conducted with typical tasks including handover in a lab based prototypical manufacturing environment. The methodology is proven to predict standard measures of situation awareness (SAGAT, SART) as well as performance in the HRI task in real-time and will open new opportunities for human factors based performance optimization in HRI applications. © 2017 IEEE.",,"Forecasting; Human computer interaction; Intelligent robots; Man machine systems; Robots; Uncertainty analysis; Human robot Interaction (HRI); Manufacturing environments; Novel methodology; Optical tracking systems; Performance optimizations; Position estimation; Probabilistic framework; Situation awareness; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85041951948
"Matsui T., Yamada S.","57188760377;35418721700;","Entropy-based eye-tracking analysis when a user watches a PRVA's recommendations",2017,"RO-MAN 2017 - 26th IEEE International Symposium on Robot and Human Interactive Communication","2017-January",,,"23","28",,,"10.1109/ROMAN.2017.8172275","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045831244&doi=10.1109%2fROMAN.2017.8172275&partnerID=40&md5=21cee9f13d08744402ad6c93ebc7722c","National Institute of Informatics, 2-1-2 Hitotsubashi, Tokyo, 101-8430, Japan; Graduate University for Advanced Studies (SOKENDAI), 2-1-2 Hitotsubashi, Tokyo, 101-8430, Japan; Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan","Matsui, T., National Institute of Informatics, 2-1-2 Hitotsubashi, Tokyo, 101-8430, Japan; Yamada, S., National Institute of Informatics, 2-1-2 Hitotsubashi, Tokyo, 101-8430, Japan, Graduate University for Advanced Studies (SOKENDAI), 2-1-2 Hitotsubashi, Tokyo, 101-8430, Japan, Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan","We conducted three experiments to discover the effect of a virtual agent's state transition on a user's eye gaze. Many previous studies showed that an agent's state transition affects a user's state. We focused on two kinds of transitions, the internal state transition and appearance state transition. In this research, we used a product recommendation virtual agent (PRVA) and aimed to discover the effect of its state transitions on users' eye gaze as it made recommendations. We used entropy-based analysis to visualise the deviation of a user's fixations. In experiment 1, the PRVA made recommendations without state transitions. In experiment 2, the amount of the PRVA's knowledge transitioned from low to high during the recommendations. This is an internal state transition. In experiment 3, the PRVA's facial expressions and gestures transitioned from a neutral to positive emotion during the recommendations. This is an appearance state transition. As a result, both the entropy-based analysis and fixation duration based analysis showed significant differences in experiment 3. These results show that an agent's appearance state transitions cause a user's eye gaze to transition. © 2017 IEEE.",,"Entropy; Eye movements; Entropy-based analysis; Eye-tracking analysis; Facial Expressions; Fixation duration; Internal state; Positive emotions; Product recommendation; State transitions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85045831244
"Wen Q., Xu F., Yong J.-H.","57199711508;57189663151;13907549600;","Real-time 3D eye performance reconstruction for RGBD cameras",2017,"IEEE Transactions on Visualization and Computer Graphics","23","12","7790904","2586","2598",,6,"10.1109/TVCG.2016.2641442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038375487&doi=10.1109%2fTVCG.2016.2641442&partnerID=40&md5=b35132b879aeb7ea57e56016a190a014","School of Software, Tsinghua University, Beijing, 100084, China; Key Laboratory for Information System Security, Ministry of Education of China, Beijing, 100084, China; Tsinghua National Laboratory for Information Science and Technology, Beijing, 100084, China","Wen, Q., School of Software, Tsinghua University, Beijing, 100084, China, Key Laboratory for Information System Security, Ministry of Education of China, Beijing, 100084, China, Tsinghua National Laboratory for Information Science and Technology, Beijing, 100084, China; Xu, F., School of Software, Tsinghua University, Beijing, 100084, China, Key Laboratory for Information System Security, Ministry of Education of China, Beijing, 100084, China, Tsinghua National Laboratory for Information Science and Technology, Beijing, 100084, China; Yong, J.-H., School of Software, Tsinghua University, Beijing, 100084, China, Key Laboratory for Information System Security, Ministry of Education of China, Beijing, 100084, China, Tsinghua National Laboratory for Information Science and Technology, Beijing, 100084, China","This paper proposes a real-time method for 3D eye performance reconstruction using a single RGBD sensor. Combined with facial surface tracking, our method generates more pleasing facial performance with vivid eye motions. In our method, a novel scheme is proposed to estimate eyeball motions by minimizing the differences between a rendered eyeball and the recorded image. Our method considers and handles different appearances of human irises, lighting variations and highlights on images via the proposed eyeball model and the L0-based optimization. Robustness and real-time optimization are achieved through the novel 3D Taylor expansion-based linearization. Furthermore, we propose an online bidirectional regression method to handle occlusions and other tracking failures on either of the two eyes from the information of the opposite eye. Experiments demonstrate that our technique achieves robust and accurate eye performance reconstruction for different iris appearances, with various head/face/eye motions, and under different lighting conditions. © 1995-2012 IEEE.","Eye reconstruction; Facial animation; Gaze tracking; Multilinear model; RGBD camera","Cameras; Lighting; Regression analysis; Facial animation; Gaze tracking; Lighting conditions; Lighting variations; Multilinear models; Real-time optimization; Rgb-d cameras; Taylor expansions; Image reconstruction; anatomy and histology; computer graphics; eye; eye fixation; face; facial expression; human; physiology; procedures; three dimensional imaging; visual system function; Computer Graphics; Eye; Face; Facial Expression; Fixation, Ocular; Humans; Imaging, Three-Dimensional; Ocular Physiological Phenomena",Article,"Final","",Scopus,2-s2.0-85038375487
"Conti J., Ozell B., Paquette E., Renaud P.","57196399194;6602394588;56639920100;7103298339;","Adjusting stereoscopic parameters by evaluating the point of regard in a virtual environment",2017,"Computers and Graphics (Pergamon)","69",,,"24","35",,2,"10.1016/j.cag.2017.08.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032854314&doi=10.1016%2fj.cag.2017.08.017&partnerID=40&md5=529b21bdd959c209d79b5262fae53516","École Polytechnique de Montréal, 2900 Boulevard Edouard-Montpetit, Montréal, QC  H3T 1J4, Canada; École de technologie supérieure, 1100 Rue Notre-Dame Ouest, Montréal, QC  H3C 1K3, Canada; Institut Philippe-Pinel de Montréal, 10905 Boul Henri-Bourassa E, Montréal, QC  H1C 1H1, Canada","Conti, J., École Polytechnique de Montréal, 2900 Boulevard Edouard-Montpetit, Montréal, QC  H3T 1J4, Canada; Ozell, B., École Polytechnique de Montréal, 2900 Boulevard Edouard-Montpetit, Montréal, QC  H3T 1J4, Canada; Paquette, E., École de technologie supérieure, 1100 Rue Notre-Dame Ouest, Montréal, QC  H3C 1K3, Canada; Renaud, P., Institut Philippe-Pinel de Montréal, 10905 Boul Henri-Bourassa E, Montréal, QC  H1C 1H1, Canada","Despite the growth in research and development in the area of virtual reality over the past few years, virtual worlds do not yet convey a feeling of presence that matches reality. This is particularly due to the difference in visual perception of flat images as compared to actual 3D. We studied the impact of two parameters of the stereoscopic configuration, namely, the inter-camera distance (ICD) and the presence of a depth of field blur (DOF blur). We conducted an experiment involving 18 participants in order to evaluate this impact, based on both subjective and objective criteria. We examined six configurations which differed in the presence or absence of DOF blur and the value of the ICD: fixed and equal to the anatomical interpupillary distance, fixed and chosen by the participant, or variable, depending on the depth of the viewer's point of regard (POR). The DOF blur and the variable ICD require the use of an eye tracking system in order to be adjusted with respect to the POR. To our knowledge, no previously published research has tested a gaze-contingent variable ICD along with dynamic DOF blur in a Cave Automatic Virtual Environment. Our results show that the anatomical and variable ICD performed similarly regarding each criterion of the experiment, both being more efficient than the fixed ICD. Besides, as with earlier similar attempts, the configurations with DOF blur obtained lower subjective evaluations. Although mainly not significant, the results obtained by the variable ICD and DOF blur are likely due to a noticeable delay in the parameters update. We also designed a new methodology to objectively compare the geometry and depth rendering, based on the reproduction of the same scene in the real and virtual setups, and then on the study of resulting ocular convergence and angular deviation from a target. This leads to a new comparative criterion for the perceptual realism of immersive virtual environments based on the visual behavior similarity between real and virtual setups. © 2017 Elsevier Ltd","Depth of field blur; Inter-camera distance; Point of regard tracking; Stereoscopic parameters; Virtual reality","Cameras; Cell proliferation; Eye tracking; Stereo image processing; Cave automatic virtual environments; Depth-of-field blur; Immersive virtual environments; Inter-camera distance; Point of regards; Research and development; Stereoscopic parameters; Subjective evaluations; Virtual reality",Article,"Final","",Scopus,2-s2.0-85032854314
"Hsu C.-C., Fann S.-C., Chuang M.-C.","34880037300;57195638151;25821950900;","Relationship between eye fixation patterns and Kansei evaluation of 3D chair forms",2017,"Displays","50",,,"21","34",,21,"10.1016/j.displa.2017.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029456843&doi=10.1016%2fj.displa.2017.09.002&partnerID=40&md5=ddfa661014acc707644468399bb33051","National Chiao Tung University, Institute of Applied Arts, 1001 University Road, Hsinchu, Taiwan","Hsu, C.-C., National Chiao Tung University, Institute of Applied Arts, 1001 University Road, Hsinchu, Taiwan; Fann, S.-C., National Chiao Tung University, Institute of Applied Arts, 1001 University Road, Hsinchu, Taiwan; Chuang, M.-C., National Chiao Tung University, Institute of Applied Arts, 1001 University Road, Hsinchu, Taiwan","Understanding how to induce Kansei (emotion or affect) in consumers through form is critical in product design and development. Conventional Kansei evaluations, which involve subjectively evaluating the overall form of a product, do not clarify the effects of the individual parts of a product on people's Kansei evaluation. A microscale analysis of eye movement of people looking at product form may redeem this flaw in subjective evaluation. However, simultaneously recording eye movement when people making Kansei evaluation is challenging, previous studies have typically investigated either the relationship between form and eye movement or the relationship between form and Kansei separately. The eye movement of people while performing Kansei evaluations on product forms still has not been clarified. To address this issue, the present study used an eye tracking system to analyze the changes in the fixation points of people performing various Kansei evaluations. Twenty participants were recruited for 8 Kansei evaluations on the form of 16 chairs by using the semantic differential (SD) rating, while their eye movements on these evaluations were tracked simultaneously. Through factor analysis on the data of Kansei evaluations, two principal factors, valence (pleasure) and arousal, were extracted from the 8 Kansei scales to constitute a Kansei plane which is compatible to Russell's circumplex model (plane) of affect By adopting the factor scores of the 16 chairs as coordinates, the 16 chairs were mapped into the Kansei plane. Further analysis on the eye fixation on the chairs located in this plane concluded the following results: (a) Pleasure had a more significant effect on the participants’ visual attention compared to arousal; the participants required more fixation points when evaluating the chair form that induced displeasure. (b) The participants typically fixated on two parts of the chairs during their Kansei evaluations, namely the seat and the backrest, indicating that seats and backrests are the two primary features people consider when evaluating chairs. The results clarify the effect of various Kansei on eye movements; thereby enable predicting people's Kansei evaluations of product forms through analyzing their eye movement. © 2017","3D forms; Eye tracking; Kansei engineering; Kansei evaluation; Product design","Aircraft seats; Behavioral research; Product design; Semantics; Eye tracking systems; Eye-tracking; Kansei Engineering; Kansei evaluation; Micro-scale analysis; Product design and development; Semantic differential; Subjective evaluations; Eye movements",Article,"Final","",Scopus,2-s2.0-85029456843
"Ma K.-T., Xu Q., Lim R., Li L., Sim T., Kankanhalli M.","57199767431;7403743739;57190191829;7501444983;7005189207;7003629165;","Eye-2-I: Eye-tracking for just-in-time implicit user profiling",2017,"2017 IEEE 2nd International Conference on Signal and Image Processing, ICSIP 2017","2017-January",,,"311","315",,2,"10.1109/SIPROCESS.2017.8124555","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043486042&doi=10.1109%2fSIPROCESS.2017.8124555&partnerID=40&md5=36bd4c11860924753c8280128fa18bcf","Institute for Infocomm Research, A-STAR, 1 Fusionopolis Way, Singapore, 138632, Singapore; National University of Singapore, 13 Computing Drive, Singapore, 117417, Singapore","Ma, K.-T., Institute for Infocomm Research, A-STAR, 1 Fusionopolis Way, Singapore, 138632, Singapore; Xu, Q., Institute for Infocomm Research, A-STAR, 1 Fusionopolis Way, Singapore, 138632, Singapore; Lim, R., Institute for Infocomm Research, A-STAR, 1 Fusionopolis Way, Singapore, 138632, Singapore; Li, L., Institute for Infocomm Research, A-STAR, 1 Fusionopolis Way, Singapore, 138632, Singapore; Sim, T., National University of Singapore, 13 Computing Drive, Singapore, 117417, Singapore; Kankanhalli, M., National University of Singapore, 13 Computing Drive, Singapore, 117417, Singapore","For many applications, such as targeted advertising and content recommendation, knowing users' traits and interests is a prerequisite. User profiling is a helpful approach for this purpose. However, current methods, i.e. self-reporting, web-activity monitoring and social media mining are either intrusive or require data over long periods of time. Recently, there is growing evidence in cognitive science that a variety of users' profile is significantly correlated with eye-tracking data. A novel just-in-time implicit profiling method, Eye-2-I, which learns the user's demographic and personality traits from the eye-tracking data while the user is watching videos is proposed. Although seemingly conspicuous by closely monitoring the user's eye behaviors, the proposed method is unobtrusive and privacy-preserving owing to its unique combination of speed and implicitness. As a proof-of-concept, the proposed method is evaluated in a user study with 51 subjects. © 2017 IEEE.","Classification; Eye-gaze; Machine learning; User profiling","Behavioral research; Classification (of information); Data privacy; Image processing; Just in time production; Learning systems; Cognitive science; Content recommendations; Eye-gaze; Personality traits; Privacy preserving; Social media minings; Targeted advertising; User profiling; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85043486042
"Fang Y., Lei J., Li J., Xu L., Lin W., Callet P.L.","8435698900;14037882800;37067453800;55660493400;8574872000;57200770358;","Learning visual saliency from human fixations for stereoscopic images",2017,"Neurocomputing","266",,,"284","292",,10,"10.1016/j.neucom.2017.05.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019705975&doi=10.1016%2fj.neucom.2017.05.050&partnerID=40&md5=5d3c50807133bf7597319efc2bc7f811","Jiangxi Provincial Key Laboratory of Digital Media, School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, Jiangxi  330032, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, 300072, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; Key Laboratory of Solar Activity, National Astronomical Observatories, Chinese Academy of Sciences, Beijing, 100012, China; School of Computer Engineering, Nanyang Technological University, Singapore, 639798, Singapore; LUNAM Université, Université de Nantes, IRCCyN UMR CNRS 6597, Polytech Nantes, Nantes, France","Fang, Y., Jiangxi Provincial Key Laboratory of Digital Media, School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, Jiangxi  330032, China; Lei, J., School of Electrical and Information Engineering, Tianjin University, Tianjin, 300072, China; Li, J., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; Xu, L., Key Laboratory of Solar Activity, National Astronomical Observatories, Chinese Academy of Sciences, Beijing, 100012, China; Lin, W., School of Computer Engineering, Nanyang Technological University, Singapore, 639798, Singapore; Callet, P.L., LUNAM Université, Université de Nantes, IRCCyN UMR CNRS 6597, Polytech Nantes, Nantes, France","In the previous years, a lot of saliency detection algorithms have been designed for saliency computation of visual content. Recently, stereoscopic display techniques have developed rapidly, which results in much requirement of stereoscopic saliency detection for emerging stereoscopic applications. Different from 2D saliency prediction, stereoscopic saliency detection methods have to consider depth factor. We design a novel stereoscopic saliency detection algorithm by machine learning technique. First, the features of luminance, color and texture are extracted to calculate the feature contract for predicting feature maps of stereoscopic images. Furthermore, the depth features are extracted for depth feature map computation. Sematic features including the center-bias factor and other top-down cues are also applied as the features in the proposed stereoscopic saliency detection method. Support Vector Regression (SVR) is applied to learn the saliency detection model of stereoscopic images. Experimental results obtained on a public large-scale eye tracking database demonstrate that the proposed method can predict better saliency results for stereoscopic images than other existing ones. © 2017 Elsevier B.V.","3D image; Machine learning; Stereoscopic image; Stereoscopic saliency detection; Support Vector Regression; Visual attention","Artificial intelligence; Behavioral research; Eye movements; Forecasting; Learning systems; Signal detection; 3-D image; Saliency detection; Stereoscopic image; Support vector regression (SVR); Visual Attention; Stereo image processing; algorithm; Article; color; controlled study; data base; experimental study; eye fixation; eye tracking; imaging and display; luminance; machine learning; physical parameters; prediction; priority journal; regression analysis; stereoscopic image; texture; visual saliency",Article,"Final","",Scopus,2-s2.0-85019705975
"Siddek A.M., Rashwan M.A., Eshrah I.A.","57201302016;6603898658;6603190613;","3D camouflaging object using RGB-D sensors",2017,"2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017","2017-January",,,"1232","1237",,,"10.1109/SMC.2017.8122781","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044235921&doi=10.1109%2fSMC.2017.8122781&partnerID=40&md5=143d551503e29c85c45c5452556b13e9","Suez University, Suez, Egypt; Cairo University, Giza, Egypt","Siddek, A.M., Suez University, Suez, Egypt; Rashwan, M.A., Cairo University, Giza, Egypt; Eshrah, I.A., Cairo University, Giza, Egypt","This paper proposes a new optical camouflage system that uses RGB-D cameras, for acquiring point cloud of background scene, and tracking observers' eyes. This system enables a user to conceal an object located behind a display that surrounded by 3D objects. If we considered here the tracked point of observer's eyes is a light source, the system will work on estimating shadow shape of the display device that falls on the objects in background. The system uses the 3d observer's eyes and the locations of display corners to predict their shadow points which have nearest neighbors in the constructed point cloud of background scene. © 2017 IEEE.","Concealment; Optical camouflage; Point Cloud; RGB-D camera; Shadow shape","Cameras; Cybernetics; Eye tracking; Invisibility cloaks; Light sources; Concealment; Optical camouflages; Point cloud; Rgb-d cameras; Shadow shape; Display devices",Conference Paper,"Final","",Scopus,2-s2.0-85044235921
"Siddek A.M.","57201302016;","Depth-level based camouflaging using RGB-D sensor",2017,"2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017","2017-January",,,"2349","2354",,1,"10.1109/SMC.2017.8122973","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044215864&doi=10.1109%2fSMC.2017.8122973&partnerID=40&md5=dfde4db6a7d481ca0ac11ecf76040925","Suez University, Suez, Egypt","Siddek, A.M., Suez University, Suez, Egypt","Optical Camouflage is the process of concealing objects in visual spectrum range. This paper proposes a system which, can conceal any 2D object that is in front of an observer using RGB-D sensor and LCD display. This sensor is Kinect v2 sensor which, is used for depth sensing of background scene behind the object, and for 3D tracking of an observer's eyes. The LCD display covers the object which is required to be concealed. Images which are outputted on the display, are a real-time processing video frame of the background region, which are unseen by the observer and occluded by the object. These images should be observed from the viewpoints of the observer rather than camera's viewpoints. © 2017 IEEE.","Concealing; Hiding objects; Mediated reality; Optical camouflage; RGB-D sensor; Visual camouflage","Cybernetics; Eye tracking; Invisibility cloaks; Concealing; Hiding objects; Mediated reality; Optical camouflages; Rgb-d sensors; Liquid crystal displays",Conference Paper,"Final","",Scopus,2-s2.0-85044215864
"Wu X., Li J., Wu Q., Sun J.","57200820459;55441752700;57043789700;12645161300;","Appearance-Based gaze block estimation via CNN classification",2017,"2017 IEEE 19th International Workshop on Multimedia Signal Processing, MMSP 2017","2017-January",,,"1","5",,9,"10.1109/MMSP.2017.8122270","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043397053&doi=10.1109%2fMMSP.2017.8122270&partnerID=40&md5=fde3a862d69f1bb55d8e4e9b167d6e3a","School of Information Science and Engineering, Shandong University, Jinan, China; School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China","Wu, X., School of Information Science and Engineering, Shandong University, Jinan, China; Li, J., School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China; Wu, Q., School of Information Science and Engineering, Shandong University, Jinan, China; Sun, J., School of Information Science and Engineering, Shandong University, Jinan, China","Appearance-based gaze estimation methods have received increasing attention in the field of human-computer interaction (HCI). These methods tried to estimate the accurate gaze point via Convolutional Neural Network (CNN) model, but the estimated accuracy can't reach the requirement of gazebased HCI when the regression model is used in the output layer of CNN. Given the popularity of button-touch-based interaction, we propose an appearance-based gaze block estimation method, which aims to estimate the gaze block, not the gaze point. In the proposed method, we relax the estimation from point to block, so that the gaze block can be estimated by CNN-based classification instead of the previous regression model. We divide the screen into square blocks to imitate the button-touch interface, and build an eye-image dataset, which contains the eye images labelled by their corresponding gaze blocks on the screen. We train the CNN model according to this dataset to estimate the gaze block by classifying the eye images. The experiments on 6- A nd 54-block classifications demonstrate that the proposed method has high accuracy in gaze block estimation without any calibration, and it is promising in button-touch-based interaction. © 2017 IEEE.","Appearance-Based; Button-Touch-Based Interaction; CNN; Gaze block; Gaze Estimation","Calibration; Classification (of information); Multimedia signal processing; Neural networks; Regression analysis; Signal processing; Touch screens; Appearance based; Block estimations; Convolutional Neural Networks (CNN); Gaze block; Gaze estimation; Human Computer Interaction (HCI); Regression model; Touch based interactions; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85043397053
"Koskela M., Jääskeläinen P., Immonen K., Multanen J., Viitanen T., Takala J.","57162886900;14056269000;57194785625;57188831090;36678169200;7103084368;","Foveated instant preview for progressive rendering",2017,"SIGGRAPH Asia 2017 Technical Briefs, SA 2017",,,"10","","",,1,"10.1145/3145749.3149423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040364769&doi=10.1145%2f3145749.3149423&partnerID=40&md5=7b27422672808eb8130c8636aa54fefc","Tampere University of Technology, Finland; Tampere University of Technology, Finland; Tampere University of Technology, Finland; Tampere University of Technology, Finland; Tampere University of Technology, Finland; Tampere University of Technology, Finland","Koskela, M., Tampere University of Technology, Finland; Jääskeläinen, P., Tampere University of Technology, Finland; Immonen, K., Tampere University of Technology, Finland; Multanen, J., Tampere University of Technology, Finland; Viitanen, T., Tampere University of Technology, Finland; Takala, J., Tampere University of Technology, Finland","Progressive rendering, for example Monte Carlo rendering of 360? content for virtual reality headsets, is a time-consuming task. If the 3D artist notices an error while previewing the rendering, he or she must return to editing mode, do the required changes, and restart rendering. Restart is required because the rendering system cannot know which pixels are affected by the change. We propose the use of eye-tracking-based optimization to significantly speed up previewing the artist’s points of interest. Moreover, we derive an optimized version of the visual acuity model, which follows the original model more accurately than previous work. The proposed optimization was tested with a comprehensive user study. The participants felt that preview with the proposed method converged instantly, and the recorded split times show that the preview is 10 times faster than conventional preview. In addition, the system does not have measurable drawbacks on computational performance. © 2017 Copyright held by the owner/author(s).","Eye tracking; Foveated rendering; Preview; Progressive rendering","Interactive computer graphics; Virtual reality; Computational performance; Eye-tracking; Foveated rendering; Monte-Carlo rendering; Preview; Progressive rendering; Time-consuming tasks; Virtual-reality headsets; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85040364769
"Lee Y., Shin C., Piumsomboon T., Lee G., Billinghurst M.","55716114900;16480889000;54885265000;15021122000;7006142663;","Automated enabling of head mounted display using gaze-depth estimation",2017,"SIGGRAPH Asia 2017 Mobile Graphics and Interactive Applications, SA 2017",,,"3139201","","",,1,"10.1145/3132787.3139201","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040313980&doi=10.1145%2f3132787.3139201&partnerID=40&md5=a401fcce90d83ce061ff8a84fda01549","Mokpo National University Jeonnam, South Korea; KETI, Seoul, South Korea; University of South Australia, Adelaide, SA, Australia","Lee, Y., Mokpo National University Jeonnam, South Korea; Shin, C., KETI, Seoul, South Korea; Piumsomboon, T., University of South Australia, Adelaide, SA, Australia; Lee, G., University of South Australia, Adelaide, SA, Australia; Billinghurst, M., University of South Australia, Adelaide, SA, Australia","Recently, global companies have released OST-HMDs (Optical Seethrough Head Mounted Displays) for Augmented Reality. The main feature of these HMDs is that you can see virtual objects while seeing real space. However, if you do not want to see a virtual object and you want to focus on a real object, this functionality is inconvenient. In this paper, we propose a method to turn on off the screen of HMD according to user 's gaze when using an augmented reality HMD. The proposed method uses the eye-Tracker attached to the mobile HMD to determine the line of sight along the distance. We put this data into a neural network to create a learning model. After the learning is completed, the gaze data is input in real time to obtain the gaze predicted distance. Through various experiments, the possibilities and limits of machine learning algorithms are grasped and suggestions for improvement are suggested. © 2017 Copyright held by the owner/author(s).","Augmented reality; Focal length; Gaze tracking; Neural network","Augmented reality; Interactive computer graphics; Interactive devices; Learning algorithms; Learning systems; Neural networks; Sensory perception; Street traffic control; Depth Estimation; Focal lengths; Gaze tracking; Head mounted displays; Learning models; Off the screens; Optical see-through head-mounted displays; Virtual objects; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85040313980
"Wiesner C.A., Ruf M., Sirim D., Klinker G.","55787058900;57193616004;57224290451;6603530980;","3D-FRC: Depiction of the future road course in the head-up-display",2017,"Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2017",,,"8115412","136","143",,5,"10.1109/ISMAR.2017.30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041648681&doi=10.1109%2fISMAR.2017.30&partnerID=40&md5=e65ab8400eead938c3f0a907a472a146","Robert Bosch GmbH, Germany; Hochschule Karlsruhe, Germany; Technische Universität München, Germany","Wiesner, C.A., Robert Bosch GmbH, Germany; Ruf, M., Hochschule Karlsruhe, Germany; Sirim, D., Robert Bosch GmbH, Germany; Klinker, G., Technische Universität München, Germany","The introduction of Head-Up-Displays (HUDs) have opened up avenues for a whole range of novel AR applications. However, until these applications become available for the mass market a number of problems need to be tackled. For example, the field of view (FoV) of current HUDs is extremely limited, and real world tracking and 3D reconstruction are still not precise enough to show driving information embedded into wide areas of complex traffic environment. It is not possible to show true AR-visualizations in the display areas provided by the current FoVs. In this paper, we investigate how an AR-like visualization approach in current HUDs (with a limited FoV) can support drivers in foreseeing the future road course. This visualisation uses the already established concept of an electronic horizon. By complying with automotive standards, our application can be easily adapted for series production. With this visualisation we performed a user study, investigating the effect on drivers' gaze behaviour. For this reason the test subjects were equipped with an eye tracking system. The results showed a decrease in both, the number of gazes as well as total glance time on the head unit and the instrument cluster. We also investigated the test subjects' braking behaviour around sharp bends of the road which showed an overall improvement when the visualisation was enabled. Furthermore it showed an increase of the mean glance duration in the area of the HUD. Note that the eye tracking system is not capable of distinguishing between glances at the visualisation in the HUD and the users' glance at objects behind the visualisation - overlapping with the HUD. This would require tracking the test persons' depth of focus. The study showed that developers need to be concerned about not displaying excessively in the HUD, so as not to distract drivers. It furthermore showed that AR-like visualizations have the potential to decrease the time the driver is not looking at the road creating a safer driving experience. © 2017 IEEE.",,"Aircraft manufacture; Augmented reality; Optical design; Roads and streets; Tracking (position); Transportation; Visualization; 3D reconstruction; Automotive standards; Driving experiences; Eye tracking systems; Head up displays; Instrument clusters; Series production; Traffic environment; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85041648681
"Jang C., Bang K., Moon S., Kim J., Lee S., Lee B.","55821476600;57193717839;57111280200;57203325066;57202328063;8114205400;","Retinal 3D: Augmented reality near-eye display via pupil-tracked light field projection on retina",2017,"ACM Transactions on Graphics","36","6","a190","","",,102,"10.1145/3130800.3130889","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038962234&doi=10.1145%2f3130800.3130889&partnerID=40&md5=4221b6b663bcb2f3af33ec9ef54bde22","Seoul National University, South Korea; Seoul National University, South Korea; Seoul National University, South Korea; Seoul National University, South Korea; Seoul National University, South Korea; Seoul National University, South Korea","Jang, C., Seoul National University, South Korea; Bang, K., Seoul National University, South Korea; Moon, S., Seoul National University, South Korea; Kim, J., Seoul National University, South Korea; Lee, S., Seoul National University, South Korea; Lee, B., Seoul National University, South Korea","We introduce an augmented reality near-eye display dubbed “Retinal 3D.” Key features of the proposed display system are as follows: Focus cues are provided by generating the pupil-tracked light eld that can be directly projected onto the retina. Generated focus cues are valid over a large depth range since laser beams are shaped for a large depth of eld (DOF). Pupil-tracked light eld generation signicantly reduces the needed information/computation load. Also, it provides “dynamic eye-box” which can be a break-through that overcome the drawbacks of retinal projection-type displays. For implementation, we utilized a holographic optical element (HOE) as an image combiner, which allowed high transparency with a thin structure. Compared with current augmented reality displays, the proposed system shows competitive performances of a large eld of view (FOV), high transparency, high contrast, high resolution, as well as focus cues in a large depth range. Two prototypes are presented along with experimental results and assessments. Analysis on the DOF of light rays and validity of focus cue generation are presented as well. Combination of pupil tracking and advanced near-eye display technique opens new possibilities of the future augmented reality. © 2017 Copyright held by the owner/author(s).","Computational displays; Eye tracking; Holographic optical element; Near-eye display; Vergence-accommodation conict","Augmented reality; Display devices; Holographic displays; Holographic optical elements; Holography; Interactive computer graphics; Laser beams; Ophthalmology; Transparency; Competitive performance; Display system; Eye-tracking; High resolution; High transparency; Projection types; Pupil tracking; Vergences; Field emission displays",Conference Paper,"Final","",Scopus,2-s2.0-85038962234
"Zhang L., Surman P., Zheng Y.","57117212100;9132350600;7404837363;","Dynamic head tracked 3D display using fast spatial light modulator",2017,"2017 Opto-Electronics and Communications Conference, OECC 2017 and Photonics Global Conference, PGC 2017","2017-November",,,"1","3",,,"10.1109/OECC.2017.8114991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048219786&doi=10.1109%2fOECC.2017.8114991&partnerID=40&md5=393b4834652c304308f5fb26caee78c9","Advanced Display Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore, 639798, Singapore","Zhang, L., Advanced Display Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore, 639798, Singapore; Surman, P., Advanced Display Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore, 639798, Singapore; Zheng, Y., Advanced Display Lab, School of Electrical and Electronic Engineering, Nanyang Technological University, 50 Nanyang Avenue, Singapore, 639798, Singapore","A laser-based eye-tracked autostereoscopic three-dimensional (3D) display has been developed, which is composed of a scanning laser, a ferroelectric liquid crystal (FLC) spatial light modulator (SLM) array, an assembled liquid crystal display (LCD) screen and an eye tracker. In the proposed system, the SLM array generates dynamic aperture pairs to modulate the scanning laser. Each aperture pair projects two exist pupils for single viewer's left and right eyes. By tracking the positions of the viewers' eyes, the dynamic exist pupils are regenerated simultaneously, which allows free movements of multi-viewers. A prototype system has been set up to verify this method. The experimental result shows that the image resolution is greatly improved by the addition of time multiplexing technique. High brightness and low crosstalk are achieved due to collimated laser backlight. Multi users are enabled to move freely without wearing any glasses. © 2017 IEEE.","Autostereoscopic display; Eye tracker; Laser backlight; Spatial light modulator (SLM)","Eye movements; Eye tracking; Image enhancement; Image resolution; Light modulation; Liquid crystal displays; Photonics; Stereo image processing; Three dimensional displays; Auto stereoscopic; Auto-stereoscopic display; Eye trackers; Ferroelectric liquid crystal; Liquid crystal display screens; Spatial light modulators; Three dimensional (3D) display; Time multiplexing; Light modulators",Conference Paper,"Final","",Scopus,2-s2.0-85048219786
"Thomas C., Jayagopi D.B.","57192934138;23135366800;","Predicting student engagement in classrooms using facial behavioral cues",2017,"MIE 2017 - Proceedings of the 1st ACM SIGCHI International Workshop on Multimodal Interaction for Education, Co-located with ICMI 2017","2017-November",,,"33","40",,28,"10.1145/3139513.3139514","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046453661&doi=10.1145%2f3139513.3139514&partnerID=40&md5=d4318446a0f6a475de17421c01d93a62","International Institute of Information Technology, Bangalore, Karnataka, India","Thomas, C., International Institute of Information Technology, Bangalore, Karnataka, India; Jayagopi, D.B., International Institute of Information Technology, Bangalore, Karnataka, India","Student engagement is the key to successful classroom learning. Measuring or analyzing the engagement of students is very important to improve learning as well as teaching. In this work, we analyze the engagement or attention level of the students from their facial expressions, headpose and eye gaze using computer vision techniques and a decision is taken using machine learning algorithms. Since the human observers are able to well distinguish the attention level from student's facial expressions,head pose and eye gaze, we assume that machine will also be able to learn the behavior automatically. The engagement level is analyzed on 10 second video clips. The performance of the algorithm is better than the baseline results. Our best accuracy results are 10 % better than the baseline. The paper also gives a detailed review of works related to the analysis of student engagement in a classroom using vision based techniques. © 2017 Association for Computing Machinery.","Educational data mining; Student engagement; Video analysis","Data mining; Interactive computer systems; Learning algorithms; Learning systems; Teaching; Baseline results; Classroom learning; Computer vision techniques; Educational data mining; Engagement levels; Facial Expressions; Student engagement; Video analysis; Students",Conference Paper,"Final","",Scopus,2-s2.0-85046453661
"Hamotskyi S., Rojbi A., Stirenko S., Gordienko Y.","57200141431;56422273200;54421204800;6701855242;","Automatized generation of alphabets of symbols",2017,"Proceedings of the 2017 Federated Conference on Computer Science and Information Systems, FedCSIS 2017",,,"8104613","639","642",,2,"10.15439/2017F413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039922396&doi=10.15439%2f2017F413&partnerID=40&md5=9b7ed832964b7c7710b7af44c3bb6a0c","Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine; Laboratoire THIM, University of Paris, Paris, France","Hamotskyi, S., Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine; Rojbi, A., Laboratoire THIM, University of Paris, Paris, France; Stirenko, S., Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine; Gordienko, Y., Igor Sikorsky Kyiv Polytechnic Institute, Kyiv, Ukraine","In this paper, we discuss the generation of symbols (and alphabets) based on specific user requirements (medium, priorities, type of information that needs to be conveyed). A framework for the generation of alphabets is proposed, and its use for the generation of a shorthand writing system is explored. We discuss the possible use of machine learning and genetic algorithms to gather inputs for generation of such alphabets and for optimization of already generated ones. The alphabets generated using such methods may be used in very different fields, from the creation of synthetic languages and constructed scripts to the creation of sensible commands for multimodal interaction through Human-Computer Interfaces, such as mouse gestures, touchpads, body gestures, eye-tracking cameras, and brain-computing Interfaces, especially in applications for elderly care and people with disabilities. © 2017 PTI.",,"Genetic algorithms; Information systems; Learning systems; User interfaces; Brain computing; Elderly care; Human computer interfaces; Mouse gesture; Multi-Modal Interactions; People with disabilities; User requirements; Writing systems; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85039922396
"Pohl D., Jungmann D., Taudul B., Membarth R., Hariharan H., Herfet T., Grau O.","55919425800;57200140950;57193560896;35105509700;57195332384;6603053360;7003464927;","The next generation of in-home streaming: Light fields, 5K, 10 GbE, and foveated compression",2017,"Proceedings of the 2017 Federated Conference on Computer Science and Information Systems, FedCSIS 2017",,,"8104618","663","667",,4,"10.15439/2017F16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039908098&doi=10.15439%2f2017F16&partnerID=40&md5=048760bb79355f23617122cc8b01b7ce","Intel Corporation, Saarland Informatics Campus, Saarbruecken, Germany; ArKaos S.A. Chausseé de, Waterloo 198, Rhode-Saint-Genèse, B-1640, Belgium; Huuuge Games, Mickiewicza 53, Szczecin, Poland; DFKI, Saarland Informatics Campus, Saarbruecken, Germany","Pohl, D., Intel Corporation, Saarland Informatics Campus, Saarbruecken, Germany; Jungmann, D., ArKaos S.A. Chausseé de, Waterloo 198, Rhode-Saint-Genèse, B-1640, Belgium; Taudul, B., Huuuge Games, Mickiewicza 53, Szczecin, Poland; Membarth, R., DFKI, Saarland Informatics Campus, Saarbruecken, Germany; Hariharan, H., DFKI, Saarland Informatics Campus, Saarbruecken, Germany; Herfet, T., DFKI, Saarland Informatics Campus, Saarbruecken, Germany; Grau, O., Intel Corporation, Saarland Informatics Campus, Saarbruecken, Germany","Interacting with real-time rendered 3D content from powerful machines on smaller devices is becoming ubiquitous through commercial products that enable in-home streaming within the same local network. However, support for high resolution, low latency in-home streaming at high image quality is still a challenging problem. To enable this, we enhance an existing open source framework for in-home streaming. We add highly optimized DXT1 (DirectX Texture Compression) support for thin desktop and notebook clients. For rendered light fields, we improve the encoding algorithms for higher image quality. Within a 10 Gigabit Ethernet (10 GbE) network, we achieve streaming up to 5K resolution at 55 frames per second. Through new low-level algorithmic improvements, we increase the compression speed of ETC1 (Ericsson Texture Compression) by a factor of 5. We are the first to bring ETC2 compression to real-time speed, which increases the streamed image quality. Last, we reduce the required data rate by more than a factor of 2 through foveated compression with real-time eye tracking. © 2017 PTI.","DXT1; ETC1; ETC2; in-home streaming; light fields","Image quality; Information systems; Rendering (computer graphics); 10-Gigabit Ethernet (10-GbE); Commercial products; DXT1; ETC1; ETC2; Light fields; Open source frameworks; Real-time eye tracking; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85039908098
"Fornalczyk K., Wojciechowski A.","56495242800;15021546300;","Robust face model based approach to head pose estimation",2017,"Proceedings of the 2017 Federated Conference on Computer Science and Information Systems, FedCSIS 2017",,,"8104720","1291","1295",,13,"10.15439/2017F425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039899670&doi=10.15439%2f2017F425&partnerID=40&md5=0cc7d24cefa414428d2dfedbf3b67169","Lodz University of Technology, Institute of Information Technology, Wolczanska 215, Lodz, 90-924, Poland","Fornalczyk, K., Lodz University of Technology, Institute of Information Technology, Wolczanska 215, Lodz, 90-924, Poland; Wojciechowski, A., Lodz University of Technology, Institute of Information Technology, Wolczanska 215, Lodz, 90-924, Poland","Head pose estimation from camera images is a computational problem that may influence many sociological, cognitive, interaction and marketing researches. It is especially crucial in the process of visual gaze estimation which accuracy depends not only on eye region analysis, but head inferring as well. Presented method exploits a 3d head model for a user head pose estimation as it outperforms, in the context of performance, popular appearance based approaches and assures efficient face head pose analysis. The novelty of the presented approach lies in a default head model refinement according to the selected facial features localisation. The new method not only achieves very high precision (about 4°), but iteratively improves the reference head model. The results of the head pose inferring experiments were verified with professional Vicon motion tracking system and head model refinement accuracy was verified with high precision Artec structural light scanner. © 2017 PTI.",,"Information systems; Iterative methods; Marketing; Appearance based approach; Computational problem; Efficient faces; Gaze estimation; Head Pose Estimation; Marketing research; Motion tracking system; Structural lights; Image recognition",Conference Paper,"Final","",Scopus,2-s2.0-85039899670
"Li T., Liu Q., Zhou X.","55837695100;55712150300;56271430300;","Ultra-Low Power Gaze Tracking for Virtual Reality",2017,"SenSys 2017 - Proceedings of the 15th ACM Conference on Embedded Networked Sensor Systems","2017-January",,,"","",,15,"10.1145/3131672.3131682","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051511101&doi=10.1145%2f3131672.3131682&partnerID=40&md5=ed339d0e949b91f44c458dbdf6e8d8b2","Department of Computer Science, Dartmouth College, Hanover, NH, United States","Li, T., Department of Computer Science, Dartmouth College, Hanover, NH, United States; Liu, Q., Department of Computer Science, Dartmouth College, Hanover, NH, United States; Zhou, X., Department of Computer Science, Dartmouth College, Hanover, NH, United States","Tracking user’s eye fixation direction is crucial to virtual reality (VR): it eases user’s interaction with the virtual scene and enables intelligent rendering to improve user’s visual experiences and save system energy. Existing techniques commonly rely on cameras and active infrared emitters, making them too expensive and power-hungry for VR headsets (especially mobile VR headsets). We present LiGaze, a low-cost, low-power approach to gaze tracking tailored to VR. It relies on a few low-cost photodiodes, eliminating the need for cameras and active infrared emitters. Reusing light emitted from the VR screen, LiGaze leverages photodiodes around a VR lens to measure reflected screen light in different directions. It then infers gaze direction by exploiting pupil’s light absorption property. The core of LiGaze is to deal with screen light dynamics and extract changes in reflected light related to pupil movement. LiGaze infers a 3D gaze vector on the fly using a lightweight regression algorithm. We design and fabricate a LiGaze prototype using off-the-shelf photodiodes. Our comparison to a commercial VR eye tracker (FOVE) shows that LiGaze achieves 6.3◦ and 10.1◦ mean within-user and cross-user accuracy. Its sensing and computation consume 791µW in total and thus can be completely powered by a credit-card sized solar cell harvesting energy from indoor lighting. LiGaze’s simplicity and ultra-low power make it applicable in a wide range of VR headsets to better unleash VR’s potential. © 2017 Association for Computing Machinery.","Gaze tracking; Virtual reality; Visible light sensing","Cameras; Embedded systems; Infrared devices; Light; Light absorption; Photodiodes; Virtual reality; Absorption property; Harvesting energies; Indoor lightings; Infrared emitters; Reflected light; Regression algorithms; Visible light; Visual experiences; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85051511101
"Liu Y., Zhang S., Xu M., He X.","57190304672;57215328161;55703599800;7404407842;","Predicting salient face in multiple-face videos",2017,"Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017","2017-January",,,"3224","3232",,21,"10.1109/CVPR.2017.343","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030216504&doi=10.1109%2fCVPR.2017.343&partnerID=40&md5=d6b03e7e1c1b303a51ae8cfcf235ecba","Beihang University, Beijing, China; Shanghai Tech University, Shanghai, China","Liu, Y., Beihang University, Beijing, China; Zhang, S., Beihang University, Beijing, China, Shanghai Tech University, Shanghai, China; Xu, M., Beihang University, Beijing, China; He, X., Shanghai Tech University, Shanghai, China","Although the recent success of convolutional neural network (CNN) advances state-of-the-art saliency prediction in static images, few work has addressed the problem of predicting attention in videos. On the other hand, we and that the attention of different subjects consistently focuses on a single face in each frame of videos involving multiple faces. Therefore, we propose in this paper a novel deep learning (DL) based method to predict salient face in multiple-face videos, which is capable of learning features and transition of salient faces across video frames. In particular, we first learn a CNN for each frame to locate salient face. Taking CNN features as input, we develop a multiple-stream long short-term memory (M-LSTM) network to predict the temporal transition of salient faces in video sequences. To evaluate our DL-based method, we build a new eye-tracking database of multiple-face videos. The experimental results show that our method outperforms the prior state-of-the-art methods in predicting visual attention on faces in multipleface videos. © 2017 IEEE.",,"Behavioral research; Computer vision; Deep learning; Eye tracking; Forecasting; Neural networks; Pattern recognition; Convolutional Neural Networks (CNN); Multiple streams; State of the art; State-of-the-art methods; Static images; Video frame; Video sequences; Visual Attention; Long short-term memory",Conference Paper,"Final","",Scopus,2-s2.0-85030216504
"Siegfried R., Yu Y., Odobez J.-M.","57195685304;57188644020;57203103085;","Towards the use of social interaction conventions as prior for gaze model adaptation",2017,"ICMI 2017 - Proceedings of the 19th ACM International Conference on Multimodal Interaction","2017-January",,,"154","162",,7,"10.1145/3136755.3136793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046706383&doi=10.1145%2f3136755.3136793&partnerID=40&md5=d18da421a4a24520f35fe674f1cb89f0","Idiap Research Institute Martigny, École Polytechnique Fédéral de Lausanne, Switzerland","Siegfried, R., Idiap Research Institute Martigny, École Polytechnique Fédéral de Lausanne, Switzerland; Yu, Y., Idiap Research Institute Martigny, École Polytechnique Fédéral de Lausanne, Switzerland; Odobez, J.-M., Idiap Research Institute Martigny, École Polytechnique Fédéral de Lausanne, Switzerland","Gaze is an important non-verbal cue involved in many facets of social interactions like communication, attentiveness or attitudes. Nevertheless, extracting gaze directions visually and remotely usually suffers large errors because of lowresolution images, inaccurate eye cropping, or large eye shape variations across the population, amongst others. This paper hypothesizes that these challenges can be addressed by exploiting multimodal social cues for gaze model adaptation on top of an head-pose independent 3D gaze estimation framework. First, a robust eye cropping refinement is achieved by combining a semantic face model with eye landmark detections. Investigations on whether temporal smoothing can overcome instantaneous refinement limitations is conducted. Secondly, to study whether social interaction convention could be used as priors for adaptation, we exploited the speaking status and head pose constraints to derive soft gaze labels and infer person-specific gaze bias using robust statistics. Experimental results on gaze coding in natural interactions from two different settings demonstrate that the two steps of our gaze adaptation method contribute to reduce gaze errors by a large margin over the baseline and can be generalized to several identities in challenging scenarios. © 2017 ACM.","Appearance based model; Bias correction; Gaze estimation; Personinvariance; Rgb-d cameras","Interactive computer systems; Semantics; Appearance-based modeling; Bias correction; Gaze estimation; Personinvariance; Rgb-d cameras; Coding errors",Conference Paper,"Final","",Scopus,2-s2.0-85046706383
"Dietz M., Schork D., Damian I., Steinert A., Haesner M., André E.","57191041218;57192162031;35168857700;55949683100;51863437700;7006887726;","Automatic Detection of Visual Search for the Elderly using Eye and Head Tracking Data",2017,"KI - Kunstliche Intelligenz","31","4",,"339","348",,7,"10.1007/s13218-017-0502-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056615250&doi=10.1007%2fs13218-017-0502-z&partnerID=40&md5=4f5b84a4570f06c4868fd9f4a40b3ec3","Human Centered Multimedia, Augsburg University, Universitätsstraße 6a, Augsburg, 86159, Germany; Geriatrics Research Group, Charité-Universitätsmedizin Berlin, Reinickendorfer Straße 61, Berlin, 13347, Germany","Dietz, M., Human Centered Multimedia, Augsburg University, Universitätsstraße 6a, Augsburg, 86159, Germany; Schork, D., Human Centered Multimedia, Augsburg University, Universitätsstraße 6a, Augsburg, 86159, Germany; Damian, I., Human Centered Multimedia, Augsburg University, Universitätsstraße 6a, Augsburg, 86159, Germany; Steinert, A., Geriatrics Research Group, Charité-Universitätsmedizin Berlin, Reinickendorfer Straße 61, Berlin, 13347, Germany; Haesner, M., Geriatrics Research Group, Charité-Universitätsmedizin Berlin, Reinickendorfer Straße 61, Berlin, 13347, Germany; André, E., Human Centered Multimedia, Augsburg University, Universitätsstraße 6a, Augsburg, 86159, Germany","With increasing age we often find ourselves in situations where we search for certain items, such as keys or wallets, but cannot remember where we left them before. Since finding these objects usually results in a lengthy and frustrating process, we propose an approach for the automatic detection of visual search for older adults to identify the point in time when the users need assistance. In order to collect the necessary sensor data for the recognition of visual search, we develop a completely mobile eye and head tracking device specifically tailored to the requirements of older adults. Using this device, we conduct a user study with 30 participants aged between 65 and 80 years (avg= 71.7 , 50% female) to collect training and test data. During the study, each participant is asked to perform several activities including the visual search for objects in a real-world setting. We use the recorded data to train a support vector machine (SVM) classifier and achieve a recognition rate of 97.55% with the leave-one-user-out evaluation method. The results indicate the feasibility of an approach towards the automatic detection of visual search in the wild. © 2017, Springer-Verlag GmbH Deutschland.","Activity recognition; Eye tracking; Head tracking; Machine learning; Visual search",,Article,"Final","",Scopus,2-s2.0-85056615250
"Mussgnug M., Singer D., Lohmeyer Q., Meboldt M.","56100585200;57189454238;43461718100;22835485400;","Automated interpretation of eye–hand coordination in mobile eye tracking recordings: Identifying demanding phases in human–machine interactions",2017,"KI - Kunstliche Intelligenz","31","4",,"331","337",,10,"10.1007/s13218-017-0503-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053461185&doi=10.1007%2fs13218-017-0503-y&partnerID=40&md5=3e69dab45b949769712566a91d708804","ETH Zurich, Leonhardstrasse 21, Zurich, 8092, Switzerland","Mussgnug, M., ETH Zurich, Leonhardstrasse 21, Zurich, 8092, Switzerland; Singer, D., ETH Zurich, Leonhardstrasse 21, Zurich, 8092, Switzerland; Lohmeyer, Q., ETH Zurich, Leonhardstrasse 21, Zurich, 8092, Switzerland; Meboldt, M., ETH Zurich, Leonhardstrasse 21, Zurich, 8092, Switzerland","Mobile eye tracking is beneficial for the analysis of human–machine interactions of tangible products, as it tracks the eye movements reliably in natural environments, and it allows for insights into human behaviour and the associated cognitive processes. However, current methods require a manual screening of the video footage, which is time-consuming and subjective. This work aims to automatically detect cognitive demanding phases in mobile eye tracking recordings. The approach presented combines the user’s perception (gaze) and action (hand) to isolate demanding interactions based upon a multi-modal feature level fusion. It was validated in a usability study of a 3D printer with 40 participants by comparing the usability problems found to a thorough manual analysis. The new approach detected 17 out of 19 problems, while the time for manual analyses was reduced by 63%. More than eye tracking alone, adding the information of the hand enriches the insights into human behaviour. The field of AI could significantly advance our approach by improving the hand-tracking through region proposal CNNs, by detecting the parts of a product and mapping the demanding interactions to these parts, or even by a fully automated end-to-end detection of demanding interactions via deep learning. This could set the basis for machines providing real-time assistance to the machine’s users in cases where they are struggling. © 2017, Springer-Verlag GmbH Deutschland.","Cognitive processes; Event interpretation; Eye-hand coordination; Human–machine interaction; Mobile eye tracking; Usability testing",,Article,"Final","",Scopus,2-s2.0-85053461185
"Touyama H., Sakuda M.","15023608800;57198791029;","Online control of a virtual object with collaborative SSVEP",2017,"Journal of Advanced Computational Intelligence and Intelligent Informatics","21","7",,"1291","1297",,1,"10.20965/jaciii.2017.p1291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037119516&doi=10.20965%2fjaciii.2017.p1291&partnerID=40&md5=97e3697f99a5535ed43c2116b93f3a08","Toyama Prefectural University, 5180 Kurokawa, Imizu-city, Toyama, 939-0398, Japan","Touyama, H., Toyama Prefectural University, 5180 Kurokawa, Imizu-city, Toyama, 939-0398, Japan; Sakuda, M., Toyama Prefectural University, 5180 Kurokawa, Imizu-city, Toyama, 939-0398, Japan","In this paper, we propose a brain-computer interface (BCI) based on collaborative steady-state visually evoked potential (SSVEP). A technique for estimating the common direction of the gaze of multiple subjects is studied with a view to controlling a virtual object in a virtual environment. The electro-encephalograms (EEG) of eight volunteers are simultaneously recorded with two virtual cubes as visual stimuli. These two virtual cubes flicker at different rates, 6 Hz and 8 Hz, and the corresponding SSVEP is observed around the occipital area. The amplitude spectra of the EEG activity of individual subjects are analyzed, averaged, and synthesized to obtain the collaborative SSVEP. Machine learning is applied to estimate the common gaze direction of the eight subjects with the supervised data from fewer than eight subjects. The estimation accuracy is perfect only in the case of the collaborative SSVEP. One-dimensional control of a virtual ball is performed by controlling the common eye gaze direction, which induces the collaborative SSVEP.","Brain-computer interface (BCI); Collaborative SSVEP; Steady-state visually evoked potential (SSVEP); Virtual reality (VR)","Bioelectric potentials; Interface states; Interfaces (computer); Learning systems; One dimensional; Virtual reality; Amplitude spectra; Collaborative SSVEP; Electro-encephalogram (EEG); On-line controls; One-dimensional control; Steady state visually evoked potentials; Virtual objects; Visual stimulus; Brain computer interface",Article,"Final","",Scopus,2-s2.0-85037119516
"Kalaie S., Gooya A.","57195466645;14628736800;","Vascular tree tracking and bifurcation points detection in retinal images using a hierarchical probabilistic model",2017,"Computer Methods and Programs in Biomedicine","151",,,"139","149",,10,"10.1016/j.cmpb.2017.08.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028327168&doi=10.1016%2fj.cmpb.2017.08.018&partnerID=40&md5=f4eef29f08ff811fd79fcb7b93289a2b","Department of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Department of Electronic and Electrical Engineering, University of Sheffield, Sheffield, United Kingdom","Kalaie, S., Department of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Gooya, A., Department of Electronic and Electrical Engineering, University of Sheffield, Sheffield, United Kingdom","Background and Objective Retinal vascular tree extraction plays an important role in computer-aided diagnosis and surgical operations. Junction point detection and classification provide useful information about the structure of the vascular network, facilitating objective analysis of retinal diseases. Methods In this study, we present a new machine learning algorithm for joint classification and tracking of retinal blood vessels. Our method is based on a hierarchical probabilistic framework, where the local intensity cross sections are classified as either junction or vessel points. Gaussian basis functions are used for intensity interpolation, and the corresponding linear coefficients are assumed to be samples from class-specific Gamma distributions. Hence, a directed Probabilistic Graphical Model (PGM) is proposed and the hyperparameters are estimated using a Maximum Likelihood (ML) solution based on Laplace approximation. Results The performance of proposed method is evaluated using precision and recall rates on the REVIEW database. Our experiments show the proposed approach reaches promising results in bifurcation point detection and classification, achieving 88.67% precision and 88.67% recall rates. Conclusions This technique results in a classifier with high precision and recall when comparing it with Xu's method. © 2017 Elsevier B.V.","Bifurcation; Classification; Machine learning; Probabilistic graphical model; Retinal blood vessel tracking","Artificial intelligence; Bifurcation (mathematics); Classification (of information); Computer aided diagnosis; Diagnosis; Forestry; Graphic methods; Learning algorithms; Learning systems; Maximum likelihood; Ophthalmology; Surgery; Gaussian basis functions; Laplace approximation; Precision and recall; Probabilistic framework; Probabilistic graphical models; Probabilistic graphical models (PGM); Probabilistic modeling; Retinal blood vessels; Blood vessels; Article; cell junction; clinical assessment; clinical classification; conceptual framework; data base; eye tracking; kernel method; machine learning; mathematical analysis; mathematical parameters; pattern recognition; retina blood vessel; retina image; algorithm; computer assisted diagnosis; diagnostic imaging; human; machine learning; normal distribution; retina blood vessel; statistical model; Algorithms; Diagnosis, Computer-Assisted; Humans; Likelihood Functions; Machine Learning; Models, Statistical; Normal Distribution; Retinal Vessels",Article,"Final","",Scopus,2-s2.0-85028327168
"Kim H.-I., Kim J.-B., Park R.-H.","56115915100;56115923900;7401895798;","Efficient and Fast Iris Localization Using Binary Radial Gradient Features for Human-Computer Interaction",2017,"International Journal of Pattern Recognition and Artificial Intelligence","31","11","1756015","","",,3,"10.1142/S0218001417560158","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022325878&doi=10.1142%2fS0218001417560158&partnerID=40&md5=c53c2432e1c3302fe659fc5403dfe2a9","Department of Electronic Engineering, School of Engineering, Sogang University 35, Backbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 04107, South Korea","Kim, H.-I., Department of Electronic Engineering, School of Engineering, Sogang University 35, Backbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 04107, South Korea; Kim, J.-B., Department of Electronic Engineering, School of Engineering, Sogang University 35, Backbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 04107, South Korea; Park, R.-H., Department of Electronic Engineering, School of Engineering, Sogang University 35, Backbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 04107, South Korea","This paper proposes an efficient and fast iris localization method. It uses support vector machine learning of iris features that represent closed outer and inner iris boundaries encompassing a low-intensity region. In addition, depending on the location of the iris in an eye image, an iris detection method is proposed based on three sub-datasets of eye images (middle, right, and left sub-datasets) with different iris features. The proposed method is implemented using fast sliding window and fast computation of the iris detection score with binary features. Compared with state-of-the-art methods, experimental results show that the proposed method is twice as fast and has comparable accuracy, even when factoring in head rotation, glasses, and highlights. © 2017 World Scientific Publishing Company.","binary feature; fast sliding window; gaze estimation; human-computer interaction; Iris localization; radial gradient","Bins; Feature extraction; Binary features; Fast sliding; Gaze estimation; Iris localization; Radial gradient; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85022325878
"Banitalebi-Dehkordi A., Pourazad M.T., Nasiopoulos P.","55760854100;6507167513;6701848250;","A learning-based visual saliency prediction model for stereoscopic 3D video (LBVS-3D)",2017,"Multimedia Tools and Applications","76","22",,"23859","23890",,15,"10.1007/s11042-016-4155-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996946744&doi=10.1007%2fs11042-016-4155-y&partnerID=40&md5=fbfd38d360a39a1c092fd28db6f8376e","ECE Department, University of British Columbia, Vancouver, BC, Canada; ICICS, University of British Columbia (UBC), Vancouver, BC, Canada; TELUS Communications Inc., Vancouver, BC, Canada","Banitalebi-Dehkordi, A., ECE Department, University of British Columbia, Vancouver, BC, Canada, ICICS, University of British Columbia (UBC), Vancouver, BC, Canada; Pourazad, M.T., ICICS, University of British Columbia (UBC), Vancouver, BC, Canada, TELUS Communications Inc., Vancouver, BC, Canada; Nasiopoulos, P., ECE Department, University of British Columbia, Vancouver, BC, Canada, ICICS, University of British Columbia (UBC), Vancouver, BC, Canada","Saliency prediction models provide a probabilistic map of relative likelihood of an image or video region to attract the attention of the human visual system. Over the past decade, many computational saliency prediction models have been proposed for 2D images and videos. Considering that the human visual system has evolved in a natural 3D environment, it is only natural to want to design visual attention models for 3D content. Existing monocular saliency models are not able to accurately predict the attentive regions when applied to 3D image/video content, as they do not incorporate depth information. This paper explores stereoscopic video saliency prediction by exploiting both low-level attributes such as brightness, color, texture, orientation, motion, and depth, as well as high-level cues such as face, person, vehicle, animal, text, and horizon. Our model starts with a rough segmentation and quantifies several intuitive observations such as the effects of visual discomfort level, depth abruptness, motion acceleration, elements of surprise, size and compactness of the salient regions, and emphasizing only a few salient objects in a scene. A new fovea-based model of spatial distance between the image regions is adopted for considering local and global feature calculations. To efficiently fuse the conspicuity maps generated by our method to one single saliency map that is highly correlated with the eye-fixation data, a random forest based algorithm is utilized. The performance of the proposed saliency model is evaluated against the results of an eye-tracking experiment, which involved 24 subjects and an in-house database of 61 captured stereoscopic videos. Our stereo video database as well as the eye-tracking data are publicly available along with this paper. Experiment results show that the proposed saliency prediction method achieves competitive performance compared to the state-of-the-art approaches. © 2016, Springer Science+Business Media New York.","3D video; Random forests; Saliency prediction; Stereoscopic video; Visual attention modeling","Behavioral research; Decision trees; Forecasting; Image processing; Image segmentation; Object recognition; Video signal processing; 3-D videos; Competitive performance; Human Visual System; Low-level attributes; Random forests; State-of-the-art approach; Stereoscopic video; Visual attention model; Stereo image processing",Article,"Final","",Scopus,2-s2.0-84996946744
"Piumsomboon T., Dey A., Ens B., Lee G., Billinghurst M.","54885265000;35317145400;53867874600;15021122000;7006142663;","CoVAR: Mixed-Platform Remote Collaborative Augmented and Virtual Realities System with Shared Collaboration Cues",2017,"Adjunct Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2017",,,"8088489","218","219",,14,"10.1109/ISMAR-Adjunct.2017.72","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040220080&doi=10.1109%2fISMAR-Adjunct.2017.72&partnerID=40&md5=93f95fa665a3d66e3532a6aac8a66a9e","School of Information Technology and Mathematical Science, University of South Australia, Australia","Piumsomboon, T., School of Information Technology and Mathematical Science, University of South Australia, Australia; Dey, A., School of Information Technology and Mathematical Science, University of South Australia, Australia; Ens, B., School of Information Technology and Mathematical Science, University of South Australia, Australia; Lee, G., School of Information Technology and Mathematical Science, University of South Australia, Australia; Billinghurst, M., School of Information Technology and Mathematical Science, University of South Australia, Australia","We present CoVAR, a novel Virtual Reality (VR) and Augmented Reality (AR) system for remote collaboration. It supports collaboration between AR and VR users by sharing a 3D reconstruction of the AR user's environment. To enhance this mixed platform collaboration, it provides natural inputs such as eye-gaze and hand gestures, remote embodiment through avatar's head and hands, and awareness cues of field-of-view and gaze cue. In this paper, we describe the system architecture, setup and calibration procedures, input methods and interaction, and collaboration enhancement features. © 2017 IEEE.","Eye tracking; Mixed reality; Remote collaboration","Augmented reality; Interactive computer graphics; 3D reconstruction; Augmented and virtual realities; Augmented reality systems; Calibration procedure; Eye-tracking; Mixed reality; Remote collaboration; System architectures; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85040220080
"Zou J., Zhang H., Weng T.","7401551846;57195263285;57192375143;","New 2D pupil and spot center positioning technology under real-Time eye tracking",2017,"ICCSE 2017 - 12th International Conference on Computer Science and Education",,,"8085473","110","115",,,"10.1109/ICCSE.2017.8085473","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040127552&doi=10.1109%2fICCSE.2017.8085473&partnerID=40&md5=541d94d1f7670d3463eb7c45deaca6be","Institute of Image Processing and Pattern Recognition, North China University of Technology, Beijing, China","Zou, J., Institute of Image Processing and Pattern Recognition, North China University of Technology, Beijing, China; Zhang, H., Institute of Image Processing and Pattern Recognition, North China University of Technology, Beijing, China; Weng, T., Institute of Image Processing and Pattern Recognition, North China University of Technology, Beijing, China","Eye tracking technology is an important technology in the field of artificial intelligence(AI). Eye tracking will promote the development of human-computer interaction(HCI). Spot Center Corneal Reflex (PCCR) is an eye tracking technique that relies on pupils and reflected light spots. Therefore, it is significant to accurately locate the pupil position and reflected spot position. The traditional algorithm used the edge and the gray information of the image to extract the contours of the pupil and the spot, and then determine the location through the fitting. However, the collected images will be affected by many environmental factors, the boundary point and the fitting calculation will greatly affect the efficiency and stability of the algorithm. In this paper, a new method combining image gradient information with threshold segmentation is proposed. Gradient detection and threshold segmentation are carried out in the region of interest, and the pupil and reflection spot are extracted directly. So, this paper use the centroid method to calculate the center coordinates more accurately. The algorithm has a good robust performance to avoid noise and environmental effects. The algorithm used to develop human eye tracking system to achieve real-Time eye tracking, while ensuring accuracy. © 2017 IEEE.","Eye tracking; image gradient; pupil positioning; spot positioning","Education computing; Engineering education; Human computer interaction; Image segmentation; Tracking (position); Eye tracking technologies; Eye-tracking; Human computer interaction (HCI); Image gradients; pupil positioning; Real-time eye tracking; spot positioning; Threshold segmentation; Edge detection",Conference Paper,"Final","",Scopus,2-s2.0-85040127552
"Deng C., Wang B., Lin W., Huang G.-B., Zhao B.","25958671000;55843883600;8574872000;7403425167;7403059245;","Effective visual tracking by pairwise metric learning",2017,"Neurocomputing","261",,,"266","275",,4,"10.1016/j.neucom.2016.05.115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013436177&doi=10.1016%2fj.neucom.2016.05.115&partnerID=40&md5=ffaaeca9e41e640cfd9e054aba3459f0","School of Information and Electronics, Beijing Institute of Technology, Beijing, 100081, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore, 639798, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798, Singapore","Deng, C., School of Information and Electronics, Beijing Institute of Technology, Beijing, 100081, China; Wang, B., School of Information and Electronics, Beijing Institute of Technology, Beijing, 100081, China; Lin, W., School of Computer Science and Engineering, Nanyang Technological University, Singapore, 639798, Singapore; Huang, G.-B., School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798, Singapore; Zhao, B., School of Information and Electronics, Beijing Institute of Technology, Beijing, 100081, China","For robust visual tracking, appearance modeling should be able to well separate the object from its backgrounds, while accurately adapt to its appearance variations. However, most of the existing tracking methods mainly focus on one of the two aspects; or design two different modules to combine them with the price of double computational cost. In this paper, by using pairwise metric learning, we present a novel appearance model for robust visual tracking. Specifically, visual tracking is viewed as a pairwise regression problem, and extreme learning machine (ELM) is utilized to construct the pairwise regression framework. In ELM-based pairwise training, two constraints are enforced: the target observations must have different regression outputs from those background ones; while the various target observations during tracking should have approximate regression outputs. Thus, the discriminative and generative capabilities are fully considered in a single object tracking model. Moreover, online sequential ELM (OS-ELM) is used to update the resulting appearance model, thereby leading to a more robust tracking process. Extensive experimental evaluations on challenging video sequences demonstrate the effectiveness and efficiency of the proposed tracker. © 2017 Elsevier B.V.","Appearance modeling; Extreme learning machine; Online sequential updating; Pairwise metric learning; Robust visual tracking","Knowledge acquisition; Machine learning; Appearance modeling; Extreme learning machine; Metric learning; Online sequential updating; Visual Tracking; Object tracking; Article; conceptual framework; experimental study; eye tracking; machine learning; mathematical analysis; mathematical computing; mathematical parameters; online sequential extreme learning machine; online system; pairwise metric learning; priority journal; regression analysis; statistical model; videorecording",Article,"Final","",Scopus,2-s2.0-85013436177
"Zhang X., Sugano Y., Bulling A.","57142162900;7005470045;6505807414;","Everyday eye contact detection using unsupervised gaze target discovery",2017,"UIST 2017 - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology",,,,"193","203",,31,"10.1145/3126594.3126614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041540055&doi=10.1145%2f3126594.3126614&partnerID=40&md5=6aa3b167e94bd1667fbc857f26e69354","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Graduate School of Information Science and Technology, Osaka University, Japan","Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Japan; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Eye contact is an important non-verbal cue in social signal processing and promising as a measure of overt attention in human-object interactions and attentive user interfaces. However, robust detection of eye contact across different users, gaze targets, camera positions, and illumination conditions is notoriously challenging. We present a novel method for eye contact detection that combines a state-of-the-art appearancebased gaze estimator with a novel approach for unsupervised gaze target discovery, i.e. without the need for tedious and time-consuming manual data annotation. We evaluate our method in two real-world scenarios: detecting eye contact at the workplace, including on the main work display, from cameras mounted to target objects, as well as during everyday social interactions with the wearer of a head-mounted egocentric camera. We empirically evaluate the performance of our method in both scenarios and demonstrate its effectiveness for detecting eye contact independent of target object type and size, camera position, and user and recording environment. © 2017 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","Appearance-based gaze estimation; Attentive user interfaces; Eye contact; Social signal processing","Cameras; Human computer interaction; Signal processing; Attentive user interfaces; Eye contact; Gaze estimation; Human-object interaction; Illumination conditions; Recording environment; Social interactions; Social signal processing; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85041540055
"Bylinskii Z., Kim N., O'Donovan P., Alsheikh S., Madan S., Pfister H., Durand F., Russell B., Hertzmann A.","55906691600;56949540900;55998975800;56737092600;57200526074;23028620700;57203217631;7202346441;6601954186;","Learning visual importance for graphic designs and data visualizations",2017,"UIST 2017 - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology",,,,"57","69",,56,"10.1145/3126594.3126653","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040310828&doi=10.1145%2f3126594.3126653&partnerID=40&md5=fa3e757f32905397394800ea820c3506","MIT CSAIL, Cambridge, MA, United States; Harvard SEAS, Cambridge, MA, United States; Adobe Systems, Seattle, WA, United States; Adobe Research, San Francisco, CA, United States","Bylinskii, Z., MIT CSAIL, Cambridge, MA, United States; Kim, N., Harvard SEAS, Cambridge, MA, United States; O'Donovan, P., Adobe Systems, Seattle, WA, United States; Alsheikh, S., MIT CSAIL, Cambridge, MA, United States; Madan, S., Harvard SEAS, Cambridge, MA, United States; Pfister, H., Harvard SEAS, Cambridge, MA, United States; Durand, F., MIT CSAIL, Cambridge, MA, United States; Russell, B., Adobe Research, San Francisco, CA, United States; Hertzmann, A., Adobe Research, San Francisco, CA, United States","Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process. © 2017 ACM.","Computer vision; Deep learning; Eye tracking; Graphic design; Machine learning; Retargeting; Saliency; Visualization","Computer vision; Deep learning; Eye movements; Flow visualization; Forecasting; Learning systems; User interfaces; Visualization; Eye-tracking; Graphic design; Immediate feedbacks; Interactive design; Retargeting; Saliency; State-of-the-art methods; Visual importance; Design",Conference Paper,"Final","",Scopus,2-s2.0-85040310828
"Kupas D., Harangi B., Czifra G., Andrassy G.","57199324483;36198305300;57199323103;6603115507;","Decision support system for the diagnosis of neurological disorders based on gaze tracking",2017,"International Symposium on Image and Signal Processing and Analysis, ISPA",,,"8073565","37","40",,1,"10.1109/ISPA.2017.8073565","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037814516&doi=10.1109%2fISPA.2017.8073565&partnerID=40&md5=f71f8773600f1a6d3e4c4486007f6094","University of Debrecen, Faculty of Informatics, Debrecen, 4002, Hungary; University of Debrecen, Department of Psychiatry, Debrecen, 4012, Hungary","Kupas, D., University of Debrecen, Faculty of Informatics, Debrecen, 4002, Hungary; Harangi, B., University of Debrecen, Faculty of Informatics, Debrecen, 4002, Hungary; Czifra, G., University of Debrecen, Department of Psychiatry, Debrecen, 4012, Hungary; Andrassy, G., University of Debrecen, Department of Psychiatry, Debrecen, 4012, Hungary","Current diagnosis of neurological disorders is an expensive and time-consuming task. Our goal is to make this procedure easier and more accurate using a digital eye scanner. Our system can help in making diagnoses, assists in the practice and shortens the time needed to find the appropriate treatment. First and foremost we collect all important visual effects in the field of neurological examination and create a video to make possible the testing of the eye movement of the patient during the video. Their gaze data is collected by an appropriate eye tracker, then we analyze the gaze information in order to evaluate the mental state of the patient using machine learning based algorithms. According to the experimental results, our proposed technique can separate the healthy and ill patients from each other using their gaze data. © 2017 IEEE.","decision support system; eye tracking; machine learning; neurological disorders","Artificial intelligence; Diagnosis; Eye movements; Image analysis; Image processing; Learning systems; Neurology; Signal processing; Tracking (position); Eye trackers; Eye-tracking; Gaze tracking; Mental state; Neurological disorders; Neurological examination; Time-consuming tasks; Visual effects; Decision support systems",Conference Paper,"Final","",Scopus,2-s2.0-85037814516
"Al Raisi S.F., Edirisinghe E.","57202440820;6701576984;","A machine learning based approach to human observer behaviour analysis in CCTV video analytics & forensics",2017,"ACM International Conference Proceeding Series",,,"a58","","",,,"10.1145/3109761.3158376","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048366697&doi=10.1145%2f3109761.3158376&partnerID=40&md5=dcc102a68b61731bd67afb553bdb9884","Department of Computer Science, Loughborough University, United Kingdom","Al Raisi, S.F., Department of Computer Science, Loughborough University, United Kingdom; Edirisinghe, E., Department of Computer Science, Loughborough University, United Kingdom","Human observer behaviour analysis in image and video inspection in many areas of practical application is conducted based on using data captured by eye tracking devices. Such data is analysed using statistical approaches leading to the creation of useful information and the ability to make decisions about the content. CCTV observer behaviour analysis is one example of a most widely used application. Unfortunately, the information and knowledge that such statistical approaches to data analysis can create is rather limited, especially the trends and patterns of data cannot be easily analysed. Thus, important information and knowledge that the data can provide may not be identifiable. In this paper, we proposed a novel approach to human observer eye tracking data analysis based on machine learning algorithms. Further, in order to conduct a more detailed and practically useful data analysis, we specifically analyse the attention human observers given instructions to search for specified content. We provide experimental results to demonstrate the significance and novelty of the information and knowledge that this novel approach to data analysis can provide. To the authors' knowledge, there is no work in literature that has proposed the use of machine learning in eye tracking data analysis. © 2017 Association for Computing Machinery.","CCTV surveillance; Eye-tracking system; Machine learning","Artificial intelligence; Behavioral research; Digital forensics; Eye tracking; Information analysis; Internet of things; Learning systems; Network security; Behaviour analysis; CCTV surveillance; Eye tracking devices; Eye tracking systems; Human observers; Statistical approach; Video analytics; Video inspection; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85048366697
"Pfeuffer K., Mayer B., Mardanbegi D., Gellersen H.","36141954200;57198766623;42761947400;6701531333;","Gaze + Pinch interaction in virtual reality",2017,"SUI 2017 - Proceedings of the 2017 Symposium on Spatial User Interaction",,,,"99","108",,64,"10.1145/3131277.3132180","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037053971&doi=10.1145%2f3131277.3132180&partnerID=40&md5=db7584775f88e3424baa9ec900899870","Lancaster University, Lancaster, United Kingdom; University of Munich (LMU), Munich, Germany","Pfeuffer, K., Lancaster University, Lancaster, United Kingdom; Mayer, B., Lancaster University, Lancaster, United Kingdom, University of Munich (LMU), Munich, Germany; Mardanbegi, D., Lancaster University, Lancaster, United Kingdom; Gellersen, H., Lancaster University, Lancaster, United Kingdom","Virtual reality affords experimentation with human abilities beyond what’s possible in the real world, toward novel senses of interaction. In many interactions, the eyes naturally point at objects of interest while the hands skilfully manipulate in 3D space. We explore a particular combination for virtual reality, the Gaze + Pinch interaction technique. It integrates eye gaze to select targets, and indirect freehand gestures to manipulate them. This keeps the gesture use intuitive like direct physical manipulation, but the gesture’s effect can be applied to any object the user looks at - whether located near or far. In this paper, we describe novel interaction concepts and an experimental system prototype that bring together interaction technique variants, menu interfaces, and applications into one unified virtual experience. Proof-of-concept application examples were developed and informally tested, such as 3D manipulation, scene navigation, and image zooming, illustrating a range of advanced interaction capabilities on targets at any distance, without relying on extra controller devices. © 2017 Association for Computing Machinery.","Eye tracking; Freehand gesture; Gaze; Interaction technique; Menu; Multimodal interface; Pinch; Virtual reality","Eye-tracking; Freehand gesture; Gaze; Interaction techniques; Menu; Multi-modal interfaces; Pinch; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85037053971
"Yamamoto M., Sakiyama H., Fukumori S., Nagamatsu T.","56328923300;57198777265;39862796000;23398000100;","An unobservable and untraceable input method for public spaces by reconstructing points of gaze only on servers",2017,"SUI 2017 - Proceedings of the 2017 Symposium on Spatial User Interaction",,,,"155","",,,"10.1145/3131277.3134352","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037032244&doi=10.1145%2f3131277.3134352&partnerID=40&md5=f8babdb8c2bf553b12d84d490cd2bf5f","Kwansei Gakuin University, 2-1 Gakuen, Sanda, Hyogo, 669-1337, Japan; Kobe University, 5-1-1 Fukaeminami-machi, Higashinada-ku Kobe, Hyogo, 658-0022, Japan","Yamamoto, M., Kwansei Gakuin University, 2-1 Gakuen, Sanda, Hyogo, 669-1337, Japan; Sakiyama, H., Kwansei Gakuin University, 2-1 Gakuen, Sanda, Hyogo, 669-1337, Japan; Fukumori, S., Kwansei Gakuin University, 2-1 Gakuen, Sanda, Hyogo, 669-1337, Japan; Nagamatsu, T., Kobe University, 5-1-1 Fukaeminami-machi, Higashinada-ku Kobe, Hyogo, 658-0022, Japan","We propose a gaze-based secure interaction by measuring only the optical axis of the eye on a computer and reconstructing the visual axis of the eye and point of gaze on a server. This method enables unobservable and untraceable input in public spaces even when using public computers. © 2017 Copyright is held by the owner/author(s).","3D-model based eye tracking; Angle kappa; Untraceable input","Angle kappa; Eye-tracking; Input methods; Optical axis; Point of gaze; Public computers; Secure interactions; Untraceable input",Conference Paper,"Final","",Scopus,2-s2.0-85037032244
"Chen O.T.-C., Chen P.-C., Tsai Y.-T.","7006248359;57197770363;57197765158;","Attention estimation system via smart glasses",2017,"2017 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology, CIBCB 2017",,,"8058565","","",,1,"10.1109/CIBCB.2017.8058565","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034665806&doi=10.1109%2fCIBCB.2017.8058565&partnerID=40&md5=099ffe0ee0689d87ce40f613369307c9","Department of Electrical Engineering, National Chung Cheng University, Chiayi, 62102, Taiwan","Chen, O.T.-C., Department of Electrical Engineering, National Chung Cheng University, Chiayi, 62102, Taiwan; Chen, P.-C., Department of Electrical Engineering, National Chung Cheng University, Chiayi, 62102, Taiwan; Tsai, Y.-T., Department of Electrical Engineering, National Chung Cheng University, Chiayi, 62102, Taiwan","Attention plays a critical role in effective learning. By means of attention assessment, it helps learners improve and review their learning processes, and even discover Attention Deficit Hyperactivity Disorder (ADHD). Hence, this work employs modified smart glasses which have an inward facing camera for eye tracking, and an inertial measurement unit for head pose estimation. The proposed attention estimation system consists of eye movement detection, head pose estimation, and machine learning. In eye movement detection, the central point of the iris is found by the locally maximum curve via the Hough transform where the region of interest is derived by the identified left and right eye corners. The head pose estimation is based on the captured inertial data to generate physical features for machine learning. Here, the machine learning adopts Genetic Algorithm (GA)-Support Vector Machine (SVM) where the feature selection of Sequential Floating Forward Selection (SFFS) is employed to determine adequate features, and GA is to optimize the parameters of SVM. Our experiments reveal that the proposed attention estimation system can achieve the accuracy of 93.1% which is fairly good as compared to the conventional systems. Therefore, the proposed system embedded in smart glasses brings users mobile, convenient, and comfortable to assess their attention on learning or medical symptom checker. © 2017 IEEE.","ADHD; attention; eye tracking; GA; head pose estimation; SFFS; smart glasses; SVM","Artificial intelligence; Bioinformatics; Edge detection; Eye movements; Gallium; Genetic algorithms; Glass; Hough transforms; Image recognition; Image segmentation; Motion analysis; Support vector machines; Units of measurement; ADHD; attention; Eye-tracking; Head Pose Estimation; SFFS; Smart glass; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85034665806
"Fang Y., Zhang C., Li J., Lei J., Da Silva M.P., Le Callet P.","8435698900;57194773954;51461432900;14037882800;24337440300;57200770358;","Visual Attention Modeling for Stereoscopic Video: A Benchmark and Computational Model",2017,"IEEE Transactions on Image Processing","26","10","7961192","4684","4696",,48,"10.1109/TIP.2017.2721112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022068465&doi=10.1109%2fTIP.2017.2721112&partnerID=40&md5=9227df9243838a3cadbef8ec39384641","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, 330032, China; Polytech Nantes, Universit de Nantes, Nantes, 44306, France; School of Electrical and Information Engineering, Tianjin University, Tianjin, 300072, China","Fang, Y., School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, 330032, China; Zhang, C., School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, 330032, China; Li, J., Polytech Nantes, Universit de Nantes, Nantes, 44306, France; Lei, J., School of Electrical and Information Engineering, Tianjin University, Tianjin, 300072, China; Da Silva, M.P., Polytech Nantes, Universit de Nantes, Nantes, 44306, France; Le Callet, P., Polytech Nantes, Universit de Nantes, Nantes, 44306, France","In this paper, we investigate the visual attention modeling for stereoscopic video from the following two aspects. First, we build one large-scale eye tracking database as the benchmark of visual attention modeling for stereoscopic video. The database includes 47 video sequences and their corresponding eye fixation data. Second, we propose a novel computational model of visual attention for stereoscopic video based on Gestalt theory. In the proposed model, we extract the low-level features, including luminance, color, texture, and depth, from discrete cosine transform coefficients, which are used to calculate feature contrast for the spatial saliency computation. The temporal saliency is calculated by the motion contrast from the planar and depth motion features in the stereoscopic video sequences. The final saliency is estimated by fusing the spatial and temporal saliency with uncertainty weighting, which is estimated by the laws of proximity, continuity, and common fate in Gestalt theory. Experimental results show that the proposed method outperforms the state-of-the-art stereoscopic video saliency detection models on our built large-scale eye tracking database and one other database (DML-ITRACK-3D). © 1992-2012 IEEE.","gestalt theory; spatiotemporal saliency detection; stereoscopic video; Visual attention","Computation theory; Computational methods; Database systems; Discrete cosine transforms; Flow visualization; Image processing; Stereo image processing; Three dimensional computer graphics; Uncertainty analysis; Video recording; Computational model; Gestalt theory; Spatiotemporal saliency detections; Stereoscopic video; Three-dimensional display; Two-dimensional displays; Video sequences; Visual Attention; Video signal processing",Article,"Final","",Scopus,2-s2.0-85022068465
"Zhang L., Suganthan P.N.","55844477500;7003996538;","Visual Tracking with Convolutional Random Vector Functional Link Network",2017,"IEEE Transactions on Cybernetics","47","10","7543468","3243","3253",,62,"10.1109/TCYB.2016.2588526","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982218427&doi=10.1109%2fTCYB.2016.2588526&partnerID=40&md5=3149484aca87169d780115ac4a2a86f5","Advanced Digital Science Center, Singapore, Singapore; Department of Electrical and Computer Engineering, Nanyang Technological University, Singapore, 639798, Singapore","Zhang, L., Advanced Digital Science Center, Singapore, Singapore; Suganthan, P.N., Department of Electrical and Computer Engineering, Nanyang Technological University, Singapore, 639798, Singapore","Deep neural network-based methods have recently achieved excellent performance in visual tracking task. As very few training samples are available in visual tracking task, those approaches rely heavily on extremely large auxiliary dataset such as ImageNet to pretrain the model. In order to address the discrepancy between the source domain (the auxiliary data) and the target domain (the object being tracked), they need to be finetuned during the tracking process. However, those methods suffer from sensitivity to the hyper-parameters such as learning rate, maximum number of epochs, size of mini-batch, and so on. Thus, it is worthy to investigate whether pretraining and fine tuning through conventional back-prop is essential for visual tracking. In this paper, we shed light on this line of research by proposing convolutional random vector functional link (CRVFL) neural network, which can be regarded as a marriage of the convolutional neural network and random vector functional link network, to simplify the visual tracking system. The parameters in the convolutional layer are randomly initialized and kept fixed. Only the parameters in the fully connected layer need to be learned. We further propose an elegant approach to update the tracker. In the widely used visual tracking benchmark, without any auxiliary data, a single CRVFL model achieves 79.0% with a threshold of 20 pixels for the precision plot. Moreover, an ensemble of CRVFL yields comparatively the best result of 86.3%. © 2017 IEEE.","Convolutional neural network (CNN); convolutional random vector functional link (CRVFL); deep learning; random vector functional link (RVFL); visual tracking","Convolution; Neural networks; Salinity measurement; Time varying networks; Tracking (position); Convolutional neural network; Deep neural networks; Functional links; Functional-link network; Hyper-parameter; Tracking process; Training sample; Visual tracking systems; Target tracking; article; convolutional neural network; eye tracking; functional link artificial neural network; learning; marriage",Article,"Final","",Scopus,2-s2.0-84982218427
"Liu C., Herrup K., Shi B.E.","57194613111;7006404111;7402547071;","Remote gaze tracking system for 3D environments",2017,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,,"8037186","1768","1771",,4,"10.1109/EMBC.2017.8037186","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032225816&doi=10.1109%2fEMBC.2017.8037186&partnerID=40&md5=26d842e2fbf07868f8a61ea1dd51d943","Department of Electronic and Computer Engineering (CL BES), Division of Life Science (KH), Division of Biomedical Engineering (BES), Hong Kong University of Science and Technology, Kowloon, Hong Kong","Liu, C., Department of Electronic and Computer Engineering (CL BES), Division of Life Science (KH), Division of Biomedical Engineering (BES), Hong Kong University of Science and Technology, Kowloon, Hong Kong; Herrup, K., Department of Electronic and Computer Engineering (CL BES), Division of Life Science (KH), Division of Biomedical Engineering (BES), Hong Kong University of Science and Technology, Kowloon, Hong Kong; Shi, B.E., Department of Electronic and Computer Engineering (CL BES), Division of Life Science (KH), Division of Biomedical Engineering (BES), Hong Kong University of Science and Technology, Kowloon, Hong Kong","Eye tracking systems are typically divided into two categories: remote and mobile. Remote systems, where the eye tracker is located near the object being viewed by the subject, have the advantage of being less intrusive, but are typically used for tracking gaze points on fixed two dimensional (2D) computer screens. Mobile systems such as eye tracking glasses, where the eye tracker are attached to the subject, are more intrusive, but are better suited for cases where subjects are viewing objects in the three dimensional (3D) environment. In this paper, we describe how remote gaze tracking systems developed for 2D computer screens can be used to track gaze points in a 3D environment. The system is non-intrusive. It compensates for small head movements by the user, so that the head need not be stabilized by a chin rest or bite bar. The system maps the 3D gaze points of the user onto 2D images from a scene camera and is also located remotely from the subject. Measurement results from this system indicate that it is able to estimate gaze points in the scene camera to within one degree over a wide range of head positions. © 2017 IEEE.",,"eye fixation; head movement; three dimensional imaging; Fixation, Ocular; Head Movements; Imaging, Three-Dimensional",Conference Paper,"Final","",Scopus,2-s2.0-85032225816
"Wang H., Antonelli M., Shi B.E.","56809110800;56263003900;7402547071;","Using point cloud data to improve three dimensional gaze estimation",2017,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,,"8036944","795","798",,7,"10.1109/EMBC.2017.8036944","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032194852&doi=10.1109%2fEMBC.2017.8036944&partnerID=40&md5=5e18b3ee4ef51cb016633f91868ba9dd","Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","Wang, H., Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Antonelli, M., Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Shi, B.E., Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","This paper addresses the problem of estimating gaze location in the 3D environment using a remote eye tracker. Instead of relying only on data provided by the eye tracker, we investigate how to integrate gaze direction with the point-cloud-based representation of the scene provided by a Kinect sensor. The algorithm first combines the gaze vectors for the two eyes provided by the eye tracker into a single gaze vector emanating from a point in between the two eyes. The gaze target in the three dimensional environment is then identified by finding the point in the 3D point cloud that is closest to the gaze vector. Our experimental results demonstrate that the estimate of the gaze target location provided by this method is significantly better than that provided when considering gaze information alone. It is also better than two other methods for integrating point cloud information: (1) finding the 3D point closest to the gaze location as estimated by triangulating the gaze vectors from the two eyes, and (2) finding the 3D point with smallest average distance to the two gaze vectors considered individually. The proposed method has an average error of 1.7 cm in a workspace of 25 × 23 × 24 cm located at a distance of 60 cm from the user. © 2017 IEEE.","3D gaze estimation; Eye tracker; Human computer interaction; Point cloud","algorithm; eye; eye fixation; Algorithms; Eye; Fixation, Ocular",Conference Paper,"Final","",Scopus,2-s2.0-85032194852
"Ishimaru S., Dengel A.","55876558900;6603764314;","ARFLED: Ability recognition framework for learning and education",2017,"UbiComp/ISWC 2017 - Adjunct Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2017 ACM International Symposium on Wearable Computers",,,,"339","343",,1,"10.1145/3123024.3123200","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030869397&doi=10.1145%2f3123024.3123200&partnerID=40&md5=917848dfeef24511bb9fd08650f7e343","German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany","Ishimaru, S., German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Dengel, A., German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany","Learning is one of the vital behaviors of human beings. This paper demonstrates a framework to augment learning activities by packaging two key ideas: Eyetifact and HyperMind. Eyeti-fact is a system that converts data of eye movements beyond the difference of sensing devices to collect a large amount of training data for machine learning. HyperMind is a digital textbook that displays learning materials dynamically based on a learner's cognitive states as measured by several sensors. In order to implement these two ideas, we have conducted experiments related to eyewear computing, textbook reading behavior analysis, and stress sensing. The contributions of this research are to investigate approaches that recognize human abilities and to transfer them from experts to others. Copyright © 2017 ACM.","Activity recognition; Cognitive state; Education; Eye tracking; Eyewear computing; Learning; Quantified self; Reading activity","Behavioral research; Eye movements; Learning systems; Textbooks; Ubiquitous computing; Wearable computers; Wearable technology; Activity recognition; Cognitive state; Eye-tracking; Eyewear; Learning; Quantified self; Reading activities; Education",Conference Paper,"Final","",Scopus,2-s2.0-85030869397
"Bâce M.","44461063200;","Augmenting human interaction capabilities with proximity, natural gestures, and eye gaze",2017,"Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2017",,,"71","","",,1,"10.1145/3098279.3119924","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030308444&doi=10.1145%2f3098279.3119924&partnerID=40&md5=0a85a4ecbb335d048c2d7385739ee734","Department of Computer Science, ETH Zurich, Switzerland","Bâce, M., Department of Computer Science, ETH Zurich, Switzerland","Nowadays, humans are surrounded by many complex computer systems. When people interact among each other, they use multiple modalities including voice, body posture, hand gestures, facial expressions, or eye gaze. Currently, computers can only understand a small subset of these modalities, but such cues can be captured by an increasing number of wearable devices. This research aims to improve traditional human-human and human-machine interaction by augmenting humans with wearable technology and developing novel user interfaces. More specifically, (i) we investigate and develop systems that enable a group of people in close proximity to interact using in-air hand gestures and facilitate effortless information sharing. Additionally, we focus on (ii) eye gaze which can further enrich the interaction between humans and cyber-physical systems.","Augmented human; Collaboration; CPS; Deep learning; Eye gaze; HCI; Proximity detection; Wearable technology","Deep learning; Embedded systems; Mobile devices; User interfaces; Wearable technology; Augmented human; Collaboration; Complex computer systems; Eye-gaze; Human machine interaction; Information sharing; Multiple modalities; Proximity detection; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85030308444
"Potapova E., Zillich M., Vincze M.","35093071100;8586250500;7003730710;","Survey of recent advances in 3D visual attention for robotics",2017,"International Journal of Robotics Research","36","11",,"1159","1176",,3,"10.1177/0278364917726587","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029913575&doi=10.1177%2f0278364917726587&partnerID=40&md5=08ff8c92916dd911c1df32f87bafbf8c","Automation and Control Institute, Vienna University of Technology, Gusshausstrasse 27-29, Wien, 1040, Austria","Potapova, E., Automation and Control Institute, Vienna University of Technology, Gusshausstrasse 27-29, Wien, 1040, Austria; Zillich, M., Automation and Control Institute, Vienna University of Technology, Gusshausstrasse 27-29, Wien, 1040, Austria; Vincze, M., Automation and Control Institute, Vienna University of Technology, Gusshausstrasse 27-29, Wien, 1040, Austria","3D visual attention plays an important role in both human and robotics perception that yet has to be explored in full detail. However, the majority of computer vision and robotics methods are concerned only with 2D visual attention. This survey presents findings and approaches that cover 3D visual attention in both human and robot vision, summarizing the last 30 years of research and also looking beyond computational methods. First, we present work in such fields as biological vision and neurophysiology, studying 3D attention in human observers. This provides a view of the role attention plays at the system level for biological vision. Then, we cover computer and robot vision approaches that take 3D visual attention into account. We compare approaches with respect to different categories, such as feature-based, data-based, or depth-based visual attention, and draw conclusions on what advances will help robotics to cope better with complex real-world settings and tasks. © The Author(s) 2017.","computational models of visual attention; eye tracking in 3D; saliency maps; Visual attention in 3D","Computational methods; Computer vision; Robotics; Surveys; Biological visions; Computer and robot vision; Eye-tracking; Feature-based; Human observers; Real world setting; Saliency map; Visual Attention; Behavioral research",Article,"Final","",Scopus,2-s2.0-85029913575
"Breeden K., Hanrahan P.","55503896900;7006336787;","Gaze data for the analysis of attention in feature films",2017,"ACM Transactions on Applied Perception","14","4","23","","",,7,"10.1145/3127588","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029901079&doi=10.1145%2f3127588&partnerID=40&md5=b3355c24ff48828cb92a0954eae23ad7","Stanford University, United States; 353 Serra Mall, Stanford, CA  94305, United States","Breeden, K., Stanford University, United States, 353 Serra Mall, Stanford, CA  94305, United States; Hanrahan, P., Stanford University, United States, 353 Serra Mall, Stanford, CA  94305, United States","Film directors are masters at controlling what we look at when we watch a film. However, there have been few quantitative studies of how gaze responds to cinematographic conventions thought to influence attention. We have collected and are releasing a dataset designed to help investigate eye movements in response to higher level features such as faces, dialogue, camera movements, image composition, and edits. The dataset, which will be released to the community, includes gaze information for 21 viewers watching 15 clips from live action 2D films, which have been hand annotated for high level features. This work has implications for the media studies, display technology, immersive reality, and human cognition. © 2017 ACM.","Eye tracking; Film studies; Gaze behavior; Gaze direction; Psychophysics","Motion pictures; Eye-tracking; Film study; Gaze behavior; Gaze direction; Psychophysics; Eye movements",Article,"Final","",Scopus,2-s2.0-85029901079
"Lebeda K., Hadfield S., Bowden R.","55646340100;54957728500;7102165938;","TMAGIC: A Model-Free 3D Tracker",2017,"IEEE Transactions on Image Processing","26","9","7865912","4378","4388",,1,"10.1109/TIP.2017.2675343","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029213716&doi=10.1109%2fTIP.2017.2675343&partnerID=40&md5=36c86a4e701845ba8d49beb147f00b37","University of Surrey, Guildford, GU2 7XH, United Kingdom","Lebeda, K., University of Surrey, Guildford, GU2 7XH, United Kingdom; Hadfield, S., University of Surrey, Guildford, GU2 7XH, United Kingdom; Bowden, R., University of Surrey, Guildford, GU2 7XH, United Kingdom","Significant effort has been devoted within the visual tracking community to rapid learning of object properties on the fly. However, state-of-the-art approaches still often fail in cases such as rapid out-of-plane rotation, when the appearance changes suddenly. One of the major contributions of this paper is a radical rethinking of the traditional wisdom of modeling 3D motion as appearance changes during tracking. Instead, 3D motion is modeled as 3D motion. This intuitive but previously unexplored approach provides new possibilities in visual tracking research. First, 3D tracking is more general, as large out-of-plane motion is often fatal for 2D trackers, but helps 3D trackers to build better models. Second, the tracker's internal model of the object can be used in many different applications and it could even become the main motivation, with tracking supporting reconstruction rather than vice versa. This effectively bridges the gap between visual tracking and structure from motion. A new benchmark data set of sequences with extreme out-of-plane rotation is presented and an online leader-board offered to stimulate new research in the relatively underdeveloped area of 3D tracking. The proposed method, provided as a baseline, is capable of successfully tracking these sequences, all of which pose a considerable challenge to 2D trackers (error reduced by 46%). © 2017 IEEE.","3D tracking; Gaussian process; image motion; Machine vision; SLAM; structure from motion; visual tracking","Computer vision; Mathematical models; 3D tracking; Gaussian Processes; Image motion; SLAM; Structure from motion; Visual Tracking; Image processing; article; eye tracking; leadership; motivation; rotation",Article,"Final","",Scopus,2-s2.0-85029213716
"Deng S., Jiang N., Chang J., Guo S., Zhang J.J.","56410328700;56038315600;55514990300;55515147500;55912086700;","Understanding the impact of multimodal interaction using gaze informed mid-air gesture control in 3D virtual objects manipulation",2017,"International Journal of Human Computer Studies","105",,,"68","80",,16,"10.1016/j.ijhcs.2017.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019047276&doi=10.1016%2fj.ijhcs.2017.04.002&partnerID=40&md5=e5daf1c506a4936388f34f8151023d78","National Centre for Computer Animation, Bournemouth University, Poole, United Kingdom; Department of Computing and Informatics, Bournemouth University, Poole, United Kingdom; School of Software, Xiamen University, Xiamen, China","Deng, S., National Centre for Computer Animation, Bournemouth University, Poole, United Kingdom; Jiang, N., Department of Computing and Informatics, Bournemouth University, Poole, United Kingdom; Chang, J., National Centre for Computer Animation, Bournemouth University, Poole, United Kingdom; Guo, S., School of Software, Xiamen University, Xiamen, China; Zhang, J.J., National Centre for Computer Animation, Bournemouth University, Poole, United Kingdom","Multimodal interactions provide users with more natural ways to manipulate virtual 3D objects than using traditional input methods. An emerging approach is gaze modulated pointing, which enables users to perform object selection and manipulation in a virtual space conveniently through the use of a combination of gaze and other interaction techniques (e.g., mid-air gestures). As gaze modulated pointing uses different sensors to track and detect user behaviours, its performance relies on the user's perception on the exact spatial mapping between the virtual space and the physical space. An underexplored issue is, when the spatial mapping differs with the user's perception, manipulation errors (e.g., out of boundary errors, proximity errors) may occur. Therefore, in gaze modulated pointing, as gaze can introduce misalignment of the spatial mapping, it may lead to user's misperception of the virtual environment and consequently manipulation errors. This paper provides a clear definition of the problem through a thorough investigation on its causes and specifies the conditions when it occurs, which is further validated in the experiment. It also proposes three methods (Scaling, Magnet and Dual-gaze) to address the problem and examines them using a comparative study which involves 20 participants with 1040 runs. The results show that all three methods improved the manipulation performance with regard to the defined problem where Magnet and Dual-gaze delivered better performance than Scaling. This finding could be used to inform a more robust multimodal interface design supported by both eye tracking and mid-air gesture control without losing efficiency and stability. © 2017 Elsevier Ltd","3D interaction; Eye tracking; Mid-air gesture; Multimodal interfaces; Spatial misperception; Virtual reality","Errors; Interactive computer systems; Magnets; Mapping; User interfaces; Virtual reality; 3D interactions; Eye-tracking; Mid-air gesture; Multi-modal interfaces; Spatial misperception; Behavioral research",Article,"Final","",Scopus,2-s2.0-85019047276
"Bock A., Svensson Å., Kleiner A., Lundberg J., Ropinski T.","55415902200;57188810210;8938318300;55574123024;8883314300;","A Visualization-Based Analysis System for Urban Search & Rescue Mission Planning Support",2017,"Computer Graphics Forum","36","6",,"148","159",,4,"10.1111/cgf.12869","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963620305&doi=10.1111%2fcgf.12869&partnerID=40&md5=2981035d49b52a7c281e1e2624c6f169","Scientific Visualization Group, Linköping University, Sweden; Graphic Design Group, Linköping University, Sweden; iRobot, Pasadena, CA, United States; Visual Computing Group, Ulm University, Germany","Bock, A., Scientific Visualization Group, Linköping University, Sweden; Svensson, Å., Graphic Design Group, Linköping University, Sweden; Kleiner, A., iRobot, Pasadena, CA, United States; Lundberg, J., Graphic Design Group, Linköping University, Sweden; Ropinski, T., Scientific Visualization Group, Linköping University, Sweden, Visual Computing Group, Ulm University, Germany","We propose a visualization system for incident commanders (ICs) in urban search and rescue scenarios that supports path planning in post-disaster structures. Utilizing point cloud data acquired from unmanned robots, we provide methods for the assessment of automatically generated paths. As data uncertainty and a priori unknown information make fully automated systems impractical, we present the IC with a set of viable access paths, based on varying risk factors, in a 3D environment combined with visual analysis tools enabling informed decision making and trade-offs. Based on these decisions, a responder is guided along the path by the IC, who can interactively annotate and reevaluate the acquired point cloud and generated paths to react to the dynamics of the situation. We describe visualization design considerations for our system and decision support systems in general, technical realizations of the visualization components, and discuss the results of two qualitative expert evaluation; one online study with nine search and rescue experts and an eye-tracking study in which four experts used the system on an application case. © 2016 The Authors Computer Graphics Forum © 2016 The Eurographics Association and John Wiley & Sons Ltd.","Computer Graphics [I.3.7]: Three-Dimensional Graphics and Realism–Color, shading, shadowing and texture; urban search and rescue decision support application","Artificial intelligence; Automation; Decision making; Decision support systems; Economic and social effects; Motion planning; Online systems; Uncertainty analysis; Visualization; Automatically generated; Eye-tracking studies; Incident commander; Informed decision; Technical realization; Urban search and rescue; Visualization designs; Visualization system; Risk assessment",Article,"Final","",Scopus,2-s2.0-84963620305
"Lim S., Lee D.","57224753931;55698938100;","Real-time eye tracking using ir stereo camera for indoor and outdoor environments",2017,"KSII Transactions on Internet and Information Systems","11","8",,"3965","3983",,3,"10.3837/tiis.2017.08.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028644639&doi=10.3837%2ftiis.2017.08.012&partnerID=40&md5=b7b79838b99a34cc63aa90fb876a2f37","Department of Electronics and Radio Engineering, Kyung Hee University, Yongin, Gyeonggi  17104, South Korea; Humanitas College, Kyung Hee University, Yongin, Gyeonggi  17104, South Korea","Lim, S., Department of Electronics and Radio Engineering, Kyung Hee University, Yongin, Gyeonggi  17104, South Korea; Lee, D., Humanitas College, Kyung Hee University, Yongin, Gyeonggi  17104, South Korea","We propose a novel eye tracking method that can estimate 3D world coordinates using an infrared (IR) stereo camera for indoor and outdoor environments. This method first detects dark evidences such as eyes, eyebrows and mouths by fast multi-level thresholding. Among these evidences, eye pair evidences are detected by evidential reasoning and geometrical rules. For robust accuracy, two classifiers based on multiple layer perceptron (MLP) using gradient local binary patterns (GLBPs) verify whether the detected evidences are real eye pairs or not. Finally, the 3D world coordinates of detected eyes are calculated by region-based stereo matching. Compared with other eye detection methods, the proposed method can detect the eyes of people wearing sunglasses due to the use of the IR spectrum. Especially, when people are in dark environments such as driving at nighttime, driving in an indoor carpark, or passing through a tunnel, human eyes can be robustly detected because we use active IR illuminators. In the experimental results, it is shown that the proposed method can detect eye pairs with high performance in real-time under variable illumination conditions. Therefore, the proposed method can contribute to human-computer interactions (HCIs) and intelligent transportation systems (ITSs) applications such as gaze tracking, windshield head-up display and drowsiness detection. © 2017 KSII.","Evidential reasoning; Eye detection and tracking; Gradient local binary pattern; Multi-level thresholding; Multiple layer perceptron","Bins; Cameras; Content based retrieval; Eye protection; Image segmentation; Intelligent systems; Stereo image processing; Evidential reasoning; Eye detection; Local binary patterns; Multilevel thresholding; Multiple layer perceptron; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85028644639
"Zhang X., Sugano Y., Fritz M., Bulling A.","57142162900;7005470045;14035495500;6505807414;","It's Written All over Your Face: Full-Face Appearance-Based Gaze Estimation",2017,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2017-July",,"8015018","2299","2308",,114,"10.1109/CVPRW.2017.284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030228048&doi=10.1109%2fCVPRW.2017.284&partnerID=40&md5=f1e83f1920e0afa0235ad6d80256ae3a","Perceptual User Interfaces Group, Germany; Scalable Learning and Perception Group, Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Graduate School of Information Science and Technology, Osaka University, Japan","Zhang, X., Perceptual User Interfaces Group, Germany; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Japan; Fritz, M., Scalable Learning and Perception Group, Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Bulling, A., Perceptual User Interfaces Group, Germany","Eye gaze is an important non-verbal cue for human affect analysis. Recent gaze estimation work indicated that information from the full face region can benefit performance. Pushing this idea further, we propose an appearance-based method that, in contrast to a long-standing line of work in computer vision, only takes the full face image as input. Our method encodes the face image using a convolutional neural network with spatial weights applied on the feature maps to flexibly suppress or enhance information in different facial regions. Through extensive evaluation, we show that our full-face method significantly outperforms the state of the art for both 2D and 3D gaze estimation, achieving improvements of up to 14.3% on MPIIGaze and 27.7% on EYEDIAP for person-independent 3D gaze estimation. We further show that this improvement is consistent across different illumination conditions and gaze directions and particularly pronounced for the most challenging extreme head poses. © 2017 IEEE.",,"Computer vision; Image enhancement; Neural networks; Affect analysis; Appearance based; Appearance-based methods; Convolutional neural network; Full face method; Illumination conditions; Person-independent; State of the art; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85030228048
"Venkataraman H., Assfalg R.","16176653600;57190346704;","Driver performance detection & recommender system in vehicular environment using video streaming analytics",2017,"IEEE/ASME International Conference on Advanced Intelligent Mechatronics, AIM",,,"8013985","1","3",,2,"10.1109/AIM.2017.8013985","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028744556&doi=10.1109%2fAIM.2017.8013985&partnerID=40&md5=2a2a1203245c2f56aa4c7de9f9499119","Center for Smart Cities, Indian Institute of Information Technology (IIIT) Chittoor, India; Dual Hochschule baden Wurtemberg (DHBW), Heidenheim, Germany","Venkataraman, H., Center for Smart Cities, Indian Institute of Information Technology (IIIT) Chittoor, India, Dual Hochschule baden Wurtemberg (DHBW), Heidenheim, Germany; Assfalg, R., Dual Hochschule baden Wurtemberg (DHBW), Heidenheim, Germany","This tutorial deals with how the overall human performance while working can be detected, under different conditions and scenarios. The tutorial will talk about how regular and real-time monitoring of people can be carried out through eye-tracking and how it can be integrated with other environment factors to develop a recommender system. Particularly, the speakers would articulate a vehicular driving scenario and explain how a combined use of eye-tracking and face-tracking can not only help the drivers but also significantly assist in reducing the road accidents, thereby increase the road safety. Finally, the speakers will present the use of existing eye-trackers along with the integrated eye-tracker and face-tracker that is under indigenous development. This tutorial is intended for a wide section of audience - ranging from clusters of automobile designers/manufacturers, researchers in mechatronics and students researching on different aspects of connected cars, computer vision, signal processing, image processing and machine learning and data analytics. © 2017 IEEE.",,"Education; Eye movements; Face recognition; Highway accidents; Image processing; Intelligent mechatronics; Learning systems; Motor transportation; Recommender systems; Roads and streets; Signal processing; Video streaming; Data analytics; Driver performance; Environment factors; Eye trackers; Face Tracking; Human performance; Real time monitoring; Vehicular environments; Speech recognition",Conference Paper,"Final","",Scopus,2-s2.0-85028744556
"Serim B., Chech L., Vasilieva M., Papa L., Gamberini L., Jacucci G.","56446965000;55921308700;57195995670;57195999824;6603650210;56216682200;","Exploring gaze-adaptive features for interacting with multi-document visualizations",2017,"ACM International Conference Proceeding Series","Part F130152",,,"85","92",,1,"10.1145/3105971.3105977","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030754721&doi=10.1145%2f3105971.3105977&partnerID=40&md5=2b87816465db61584571014c7b64f6c8","Helsinki Institute for Information Technology HIIT, Dept. of Computer Science, University of Helsinki, Department of Design, Aalto University, Finland; Human Inspired Technology Research Centre, Universitá di Padova, Italy; Department of Computer Science, University of Helsinki, Finland","Serim, B., Helsinki Institute for Information Technology HIIT, Dept. of Computer Science, University of Helsinki, Department of Design, Aalto University, Finland; Chech, L., Human Inspired Technology Research Centre, Universitá di Padova, Italy; Vasilieva, M., Department of Computer Science, University of Helsinki, Finland; Papa, L., Human Inspired Technology Research Centre, Universitá di Padova, Italy; Gamberini, L., Human Inspired Technology Research Centre, Universitá di Padova, Italy; Jacucci, G., Helsinki Institute for Information Technology HIIT, Dept. of Computer Science, University of Helsinki, Department of Design, Aalto University, Finland","We present a preliminary design study for utilizing eye tracking to support interacting with a multi-document visualization. Complex information seeking tasks can involve collection and comparison of multiple documents, resulting in long and sustained search sessions. The sustained and evolving nature of the session also provides the search interface the opportunity to gather information on the user state and interaction history, which can be used to adapt the information content and representation. We designed a system to evaluate how eye tracking information can be used for adapting the visual salience of information entities. The interface features documents and related keywords that are arranged in a radial layout configuration called the intent radar. Reading history and visual attention, as registered by eye tracking data, are respectively used to trace read items and for visual cueing. We evaluated the interface with 16 participants to gather subjective feedback about specific components and features of the interface. The overall results show that the interface and gaze-related appearance of keywords was positively received by the users. © 2017 Copyright held by the owner/author(s).","Attention switching; Attentive interfaces; Eye tracking; Search interfaces","Behavioral research; Visual communication; Visualization; Attention switching; Attentive interfaces; Complex information; Eye-tracking; Information contents; Interaction history; Search interfaces; Subjective feedback; Interface states",Conference Paper,"Final","",Scopus,2-s2.0-85030754721
"Yun X., Xiao G.","41462242100;57203485735;","Spiral visual and motional tracking",2017,"Neurocomputing","249",,,"117","127",,1,"10.1016/j.neucom.2017.03.070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017374506&doi=10.1016%2fj.neucom.2017.03.070&partnerID=40&md5=ba5b8270b25d7f4a7b9cec45fadb2d97","School of Information and Electrical Engineering, China University of Mining and Technology, 1 Daxue Road, Xuzhou, Jiangsu  221116, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, 800 Dongchuan Street, Shanghai, 200240, China","Yun, X., School of Information and Electrical Engineering, China University of Mining and Technology, 1 Daxue Road, Xuzhou, Jiangsu  221116, China; Xiao, G., School of Aeronautics and Astronautics, Shanghai Jiao Tong University, 800 Dongchuan Street, Shanghai, 200240, China","Constructing a visual appearance model is essential for visual tracking. However, relying only on the visual model during appearance changes is insufficient and may even interfere with achieving good results. Although several visual tracking algorithms emphasize motional tracking that estimates the motion state of the object center between consecutive frames, they suffer from accumulated error during runtime. As neither visual nor motional trackers are capable of performing well separately, several groups have recently proposed simultaneous visual and motional tracking algorithms. However, because tracking problems are often NP-hard, these algorithms cannot provide good solutions for the reason that they are driven top-down with low flexibility and often encounter drift problems. This paper proposes a spiral visual and motional tracking (SVMT) algorithm which, unlike existing algorithms, builds a strong tracker by cyclically combining weak trackers from both the visual and motional layers. In the spiral-like framework, an iteration model is used to search for the optimum until convergence, with the potential for achieving optimization. Three learned procedures including visual classification, motional estimation, and risk analysis are integrated into the generalized framework and implement corresponding modifications with regard to their performances. The experimental results demonstrate that SVMT performs well in terms of accuracy and robustness. © 2017 Elsevier B.V.","Iteration model; Motional tracking; Risk function; Simultaneous visual and motional tracking; Visual tracking","Iterative methods; Motion analysis; Optimization; Risk analysis; Risk assessment; Risk perception; Tracking (position); Accumulated errors; Iteration model; Risk function; Tracking algorithm; Visual appearance; Visual classification; Visual Tracking; Visual tracking algorithm; Motion estimation; accuracy; algorithm; analytical parameters; Article; classification; conceptual framework; experimental study; eye tracking; mathematical computing; mathematical parameters; movement perception; priority journal; risk assessment; spiral visual and motional tracking algorithm; statistical model",Article,"Final","",Scopus,2-s2.0-85017374506
"Huang Q., Veeraraghavan A., Sabharwal A.","57194771951;6506177844;57223450164;","TabletGaze: dataset and analysis for unconstrained appearance-based gaze estimation in mobile tablets",2017,"Machine Vision and Applications","28","5-6",,"445","461",,60,"10.1007/s00138-017-0852-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021953913&doi=10.1007%2fs00138-017-0852-4&partnerID=40&md5=05ae0f1f70ce457e72048aa7807da7ae","ECE Department, Rice University, Houston, TX, United States","Huang, Q., ECE Department, Rice University, Houston, TX, United States; Veeraraghavan, A., ECE Department, Rice University, Houston, TX, United States; Sabharwal, A., ECE Department, Rice University, Houston, TX, United States","We study gaze estimation on tablets; our key design goal is uncalibrated gaze estimation using the front-facing camera during natural use of tablets, where the posture and method of holding the tablet are not constrained. We collected a large unconstrained gaze dataset of tablet users, labeled Rice TabletGaze dataset. The dataset consists of 51 subjects, each with 4 different postures and 35 gaze locations. Subjects vary in race, gender and in their need for prescription glasses, all of which might impact gaze estimation accuracy. We made three major observations on the collected data and employed a baseline algorithm for analyzing the impact of several factors on gaze estimation accuracy. The baseline algorithm is based on multilevel HoG feature and Random Forests regressor, which achieves a mean error of 3.17 cm. We perform extensive evaluation on the impact of various practical factors such as person dependency, dataset size, race, wearing glasses and user posture on the gaze estimation accuracy. © 2017, Springer-Verlag GmbH Germany.","Applications; Dataset; Eye; Gaze estimation/tracking; Mobile device","Applications; Decision trees; Glass; Appearance based; Data set size; Dataset; Design goal; Gaze estimation; Mean errors; Random forests; Uncalibrated; Mobile devices",Article,"Final","",Scopus,2-s2.0-85021953913
"Khalil A., Faisal A., Lai K.W., Ng S.C., Liew Y.M.","57197752803;35777992400;57194066657;43761330600;36673421800;","2D to 3D fusion of echocardiography and cardiac CT for TAVR and TAVI image guidance",2017,"Medical and Biological Engineering and Computing","55","8",,"1317","1326",,21,"10.1007/s11517-016-1594-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994452899&doi=10.1007%2fs11517-016-1594-6&partnerID=40&md5=7e93114d5b163ce11c136209bbbd4105","Department of Biomedical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, 50603, Malaysia","Khalil, A., Department of Biomedical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, 50603, Malaysia; Faisal, A., Department of Biomedical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, 50603, Malaysia; Lai, K.W., Department of Biomedical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, 50603, Malaysia; Ng, S.C., Department of Biomedical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, 50603, Malaysia; Liew, Y.M., Department of Biomedical Engineering, Faculty of Engineering, University of Malaya, Kuala Lumpur, 50603, Malaysia","This study proposed a registration framework to fuse 2D echocardiography images of the aortic valve with preoperative cardiac CT volume. The registration facilitates the fusion of CT and echocardiography to aid the diagnosis of aortic valve diseases and provide surgical guidance during transcatheter aortic valve replacement and implantation. The image registration framework consists of two major steps: temporal synchronization and spatial registration. Temporal synchronization allows time stamping of echocardiography time series data to identify frames that are at similar cardiac phase as the CT volume. Spatial registration is an intensity-based normalized mutual information method applied with pattern search optimization algorithm to produce an interpolated cardiac CT image that matches the echocardiography image. Our proposed registration method has been applied on the short-axis “Mercedes Benz” sign view of the aortic valve and long-axis parasternal view of echocardiography images from ten patients. The accuracy of our fully automated registration method was 0.81 ± 0.08 and 1.30 ± 0.13 mm in terms of Dice coefficient and Hausdorff distance for short-axis aortic valve view registration, whereas for long-axis parasternal view registration it was 0.79 ± 0.02 and 1.19 ± 0.11 mm, respectively. This accuracy is comparable to gold standard manual registration by expert. There was no significant difference in aortic annulus diameter measurement between the automatically and manually registered CT images. Without the use of optical tracking, we have shown the applicability of this technique for effective fusion of echocardiography with preoperative CT volume to potentially facilitate catheter-based surgery. © 2016, International Federation for Medical and Biological Engineering.","Cardiovascular; Computed tomography; Echocardiography; Fusion; Image registration","Blood vessels; Diagnosis; Echocardiography; Fusion reactions; Heart; Image fusion; Image matching; Image registration; Optimization; Surgery; Cardiovascular; Diameter Measurement; Echocardiography Images; Normalized mutual information; Pattern search optimizations; Spatial registrations; Temporal synchronization; Transcatheter aortic valves; Computerized tomography; aortic valve disease; Article; case report; echocardiography; eye tracking; human; priority journal; registration; time series analysis; transcatheter aortic valve implantation; x-ray computed tomography",Article,"Final","",Scopus,2-s2.0-84994452899
"Li J.","55900081000;","A synthetic research on the multimedia data encryption based mobile computing security enhancement model and multi-channel mobile human computer interaction framework",2017,"Multimedia Tools and Applications","76","16",,"16963","16987",,9,"10.1007/s11042-016-3662-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976303093&doi=10.1007%2fs11042-016-3662-1&partnerID=40&md5=da185e609b52518e31fe57bedd1579f8","Quanzhou Institute of Equipment Manufacturing, Haixi Institutes, Chinese Academy of Sciences, Quanzhou, 362200, China","Li, J., Quanzhou Institute of Equipment Manufacturing, Haixi Institutes, Chinese Academy of Sciences, Quanzhou, 362200, China","With the development of computer network, people could obtain information through the network with stronger impulsion, the dependence of the requirements also becomes higher, this is not only reflected in the increase of information, but to obtain and submit more reflected in real-time and easy to access to information on the pressing needs of the. Therefore, people devoted all aspects from the terminal, network and software platforms to make unremitting efforts. Under this basis, we conduct synthetic research on the data encryption based mobile computing security enhancement model and multi-channel mobile human computer interaction framework in this paper. We firstly introduce the discrete-time Hopfield neural network based data encryption algorithm beyond the analysis on the information flow security and type based model, data security review and attack model under mobile computing environment. We improve learning algorithm of the MD method to avoid the sample mobile and cross interference problems of Hebb rule. Later, we integrate the multi-channel concept to propose the new multi-channel man–machine interaction. In our framework, the 3D interactive technology, speech recognition and synthesis technology, natural language understanding and processing technology, eye tracking technology, posture, input, tactile, force display basic technology are taken into consideration for the synthetic analysis. The result from the experimental simulation proves that our methodology obtains better effectiveness and feasibility from both the angels of data security and interface experience. © 2016, Springer Science+Business Media New York.","Data encryption; Human computer interaction; Mobile computing; Mobile multimedia; Multi-channel framework; Security enhancement","Cryptography; Hopfield neural networks; Human computer interaction; Mobile computing; Security of data; Speech recognition; 3D interactive technologies; Data encryption; Mobile computing environment; Mobile human computer interaction; Mobile multimedia; Multi-channel frameworks; Natural language understanding; Security enhancements; Network security",Article,"Final","",Scopus,2-s2.0-84976303093
"Thies J., Zollhöfer M., Stamminger M., Theobalt C., Nießner M.","56312656700;36245738500;55906526700;6507027272;35772871600;","Demo of FaceVR: Real-time facial reenactment and eye gaze control in virtual reality",2017,"ACM SIGGRAPH 2017 Emerging Technologies, SIGGRAPH 2017",,,"a7","","",,6,"10.1145/3084822.3084841","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033411604&doi=10.1145%2f3084822.3084841&partnerID=40&md5=9e49a21ee33efffa7e0088fc3ec1f357","University of Erlangen-Nuremberg, Germany; Max Planck Institute for Informatics, Germany; Technical University of Munich, Germany","Thies, J., University of Erlangen-Nuremberg, Germany; Zollhöfer, M., Max Planck Institute for Informatics, Germany; Stamminger, M., University of Erlangen-Nuremberg, Germany; Theobalt, C., Max Planck Institute for Informatics, Germany; Nießner, M., Technical University of Munich, Germany","We introduce FaceVR, a novel method for gaze-aware facial reenactment in the Virtual Reality (VR) context. The key component of FaceVR is a robust algorithm to perform real-time facial motion capture of an actor who is wearing a head-mounted display (HMD), as well as a new data-driven approach for eye tracking from monocular videos. In addition to these face reconstruction components, FaceVR incorporates photo-realistic re-rendering in real time, thus allowing artificial modifications of face and eye appearances. For instance, we can alter facial expressions, change gaze directions, or remove the VR goggles in realistic re-renderings. In a live setup with a source and a target actor, we apply these newly-introduced algorithmic components. We assume that the source actor is wearing a VR device, and we capture his facial expressions and eye movement in real-time. For the target video, we mimic a similar tracking process; however, we use the source input to drive the animations of the target video, thus enabling gaze-aware facial reenactment. To render the modified target video on a stereo display, we augment our capture and reconstruction process with stereo data. In the end, FaceVR produces compelling results for a variety of applications, such as gaze-aware facial reenactment, reenactment in virtual reality, removal of VR goggles, and re-targeting of somebody's gaze direction in a video conferencing call. © 2017 Copyright held by the owner/author(s).","Expression transfer; Face capture; Facial reenactment","Computer graphics; Digital storage; Eye movements; Goggles; Helmet mounted displays; Interactive computer graphics; Rendering (computer graphics); Video conferencing; Virtual reality; Data-driven approach; Expression transfers; Face capture; Face reconstruction; Facial motion capture; Facial reenactment; Head mounted displays; Reconstruction process; Stereo image processing",Conference Paper,"Final","",Scopus,2-s2.0-85033411604
"Lee Y., Shin C., Plopski A., Itoh Y., Piumsomboon T., Dey A., Lee G., Kim S., Billinghurst M.","55716114900;16480889000;56023098100;56154865900;54885265000;35317145400;15021122000;55645936800;7006142663;","Estimating Gaze Depth Using Multi-Layer Perceptron",2017,"Proceedings - 2017 International Symposium on Ubiquitous Virtual Reality, ISUVR 2017",,,"7988648","26","29",,13,"10.1109/ISUVR.2017.13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028559607&doi=10.1109%2fISUVR.2017.13&partnerID=40&md5=12726caf1aa20e13e65af7d4fba43a13","Empathic Computing Lab., University of South Australia, Australia; VR/AR Research Center, KETI, South Korea; Interactive Media Design Lab, NAIST, Japan; Interactive Media Lab., Keio University, Japan; UVR Lab., Mokpo National University, South Korea","Lee, Y., Empathic Computing Lab., University of South Australia, Australia, UVR Lab., Mokpo National University, South Korea; Shin, C., VR/AR Research Center, KETI, South Korea; Plopski, A., Interactive Media Design Lab, NAIST, Japan; Itoh, Y., Interactive Media Lab., Keio University, Japan; Piumsomboon, T., Empathic Computing Lab., University of South Australia, Australia; Dey, A., Empathic Computing Lab., University of South Australia, Australia; Lee, G., Empathic Computing Lab., University of South Australia, Australia; Kim, S., Empathic Computing Lab., University of South Australia, Australia; Billinghurst, M., Empathic Computing Lab., University of South Australia, Australia","In this paper we describe a new method for determining gaze depth in a head mounted eye-tracker. Eye-trackers are being incorporated into head mounted displays (HMDs), and eye-gaze is being used for interaction in Virtual and Augmented Reality. For some interaction methods, it is important to accurately measure the x-and y-direction of the eye-gaze and especially the focal depth information. Generally, eye tracking technology has a high accuracy in x-and y-directions, but not in depth. We used a binocular gaze tracker with two eye cameras, and the gaze vector was input to an MLP neural network for training and estimation. For the performance evaluation, data was obtained from 13 people gazing at fixed points at distances from 1m to 5m. The gaze classification into fixed distances produced an average classification error of nearly 10%, and an average error distance of 0.42m. This is sufficient for some Augmented Reality applications, but more research is needed to provide an estimate of a user's gaze moving in continuous space. © 2017 IEEE.","3D gaze; Augmented Reality; Eye-gaze; Head-mounted display; Machine Learning","Augmented reality; Eye movements; Learning systems; Sensory perception; Street traffic control; Virtual reality; 3D gaze; Augmented reality applications; Classification errors; Eye tracking technologies; Eye-gaze; Head mounted displays; Multi layer perceptron; Virtual and augmented reality; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85028559607
"Zhou X., Cai H., Li Y., Liu H.","55743240400;56763253600;8589964900;54958434200;","Two-eye model-based gaze estimation from a Kinect sensor",2017,"Proceedings - IEEE International Conference on Robotics and Automation",,,"7989194","1646","1653",,20,"10.1109/ICRA.2017.7989194","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028021650&doi=10.1109%2fICRA.2017.7989194&partnerID=40&md5=a635e93025c60ae9e98c2eb7ab4e8e7d","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Computing, University of Portsmouth, Portsmouth, United Kingdom; Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Hong Kong, Hong Kong; State Key Laboratory of Mechanical Systems and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China","Zhou, X., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Cai, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom; Li, Y., Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Hong Kong, Hong Kong; Liu, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom, State Key Laboratory of Mechanical Systems and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China","In this paper, we present an effective and accurate gaze estimation method based on two-eye model of a subject with the tolerance of free head movement from a Kinect sensor. To accurately and efficiently determine the point of gaze, i) we employ two-eye model to improve the estimation accuracy; ii) we propose an improved convolution-based means of gradients method to localize the iris center in 3D space; iii) we present a new personal calibration method that only needs one calibration point. The method approximates the visual axis as a line from the iris center to the gaze point to determine the eyeball centers and the Kappa angles. The final point of gaze can be calculated by using the calibrated personal eye parameters. We experimentally evaluate the proposed gaze estimation method on eleven subjects. Experimental results demonstrate that our gaze estimation method has an average estimation accuracy around 1.99°, which outperforms many leading methods in the state-of-the-art. © 2017 IEEE.",,"Eye movements; Robotics; Calibration method; Calibration points; Eye parameters; Gaze estimation; Gaze point; Kinect sensors; Point of gaze; State of the art; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-85028021650
"Grammatikopoulou M., Yang G.-Z.","57192432518;55539304100;","Gaze contingent control for optical micromanipulation",2017,"Proceedings - IEEE International Conference on Robotics and Automation",,,"7989707","5989","5995",,7,"10.1109/ICRA.2017.7989707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028011013&doi=10.1109%2fICRA.2017.7989707&partnerID=40&md5=b0bc13543b3efaf2b442300eba4548ea","Hamlyn Centre for Robotic Surgery, Imperial College London, London, United Kingdom","Grammatikopoulou, M., Hamlyn Centre for Robotic Surgery, Imperial College London, London, United Kingdom; Yang, G.-Z., Hamlyn Centre for Robotic Surgery, Imperial College London, London, United Kingdom","Optical Tweezers (OT) have the advantage of non-contact interaction with target objects such as cells, overcoming the pitfall of obstructive adhesion forces which are present in contact micromanipulation. It is also feasible to manipulate a number of small microparts simultaneously or 3D structures by using multiple laser traps. These capabilities give rise to the potential to develop a human-robot interface to facilitate microassembly tasks. This paper presents a gaze contingent control framework and a method for 3D orientation estimation for optical micromanipulation. The proposed strategy aims to use OT as an interactive microassembly platform. The framework comprises I) a strategy to recognize the operator's intentions in order to interactively place and reconfigure the optical traps using the operator's eye fixation point, II) haptic constraints generated from the user's eye gaze to assist positioning of the assembled microparts and III) a method for 3D orientation estimation. The performance of the proposed framework is assessed through a set of experiments comparing it to the standard OT user interface. Three-dimensional manipulation and orientation estimation of a non-spherical microstructure are also performed. © 2017 IEEE.",,"Micromanipulators; Robotics; Robots; User interfaces; Adhesion forces; Control framework; Gaze-contingent; Human-Robot Interface; Micro manipulation; Optical micromanipulation; Orientation estimation; Three-dimensional manipulation; Optical tweezers",Conference Paper,"Final","",Scopus,2-s2.0-85028011013
"Penkov S., Bordallo A., Ramamoorthy S.","57195424725;57118029200;15042865300;","Physical symbol grounding and instance learning through demonstration and eye tracking",2017,"Proceedings - IEEE International Conference on Robotics and Automation",,,"7989697","5921","5928",,6,"10.1109/ICRA.2017.7989697","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027974879&doi=10.1109%2fICRA.2017.7989697&partnerID=40&md5=a8374dbae3536e907323003cb2dc391b","School of Informatics, University of EdinburghEH8 9AB, United Kingdom","Penkov, S., School of Informatics, University of EdinburghEH8 9AB, United Kingdom; Bordallo, A., School of Informatics, University of EdinburghEH8 9AB, United Kingdom; Ramamoorthy, S., School of Informatics, University of EdinburghEH8 9AB, United Kingdom","It is natural for humans to work with abstract plans which are often an intuitive and concise way to represent a task. However, high level task descriptions contain symbols and concepts which need to be grounded within the environment if the plan is to be executed by an autonomous robot. The problem of learning the mapping between abstract plan symbols and their physical instances in the environment is known as the problem of physical symbol grounding. In this paper, we propose a framework for Grounding and Learning Instances through Demonstration and Eye tracking (GLIDE). We associate traces of task demonstration to a sequence of fixations which we call fixation programs and exploit their properties in order to perform physical symbol grounding. We formulate the problem as a probabilistic generative model and present an algorithm for computationally feasible inference over the proposed model. A key aspect of our work is that we estimate fixation locations within the environment which enables the appearance of symbol instances to be learnt. Instance learning is a crucial ability when the robot does not have any knowledge about the model or the appearance of the symbols referred to in the plan instructions. We have conducted human experiments and demonstrate that GLIDE successfully grounds plan symbols and learns the appearance of their instances, thus enabling robots to autonomously execute tasks in initially unknown environments. © 2017 IEEE.",,"Eye movements; Inference engines; Robotics; Robots; Abstract plans; Eye-tracking; Generative model; Symbol grounding; Task description; Demonstrations",Conference Paper,"Final","",Scopus,2-s2.0-85027974879
"Deng S., Chang J., Hu S.-M., Zhang J.J.","56410328700;55514990300;34769829200;55912086700;","Gaze modulated disambiguation technique for gesture control in 3D virtual objects selection",2017,"2017 3rd IEEE International Conference on Cybernetics, CYBCONF 2017 - Proceedings",,,"7985779","","",,3,"10.1109/CYBConf.2017.7985779","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027891432&doi=10.1109%2fCYBConf.2017.7985779&partnerID=40&md5=b66b782053dc4355e6d7fc5b08b8ce17","National Centre for Computer Animation, Bournemouth University, Poole, Dorset, United Kingdom; National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China","Deng, S., National Centre for Computer Animation, Bournemouth University, Poole, Dorset, United Kingdom; Chang, J., National Centre for Computer Animation, Bournemouth University, Poole, Dorset, United Kingdom; Hu, S.-M., National Centre for Computer Animation, Bournemouth University, Poole, Dorset, United Kingdom; Zhang, J.J., National Laboratory for Information Science and Technology, Tsinghua University, Beijing, China","Inputs with multimodal information provide more natural ways to interact with virtual 3D environment. An emerging technique that integrates gaze modulated pointing with mid-air gesture control enables fast target acquisition and rich control expressions. The performance of this technique relies on the eye tracking accuracy which is not comparable with the traditional pointing techniques (e.g., mouse) yet. This will cause troubles when fine grainy interactions are required, such as selecting in a dense virtual scene where proximity and occlusion are prone to occur. This paper proposes a coarse-to-fine solution to compensate the degradation introduced by eye tracking inaccuracy using a gaze cone to detect ambiguity and then a gaze probe for decluttering. It is tested in a comparative experiment which involves 12 participants with 3240 runs. The results show that the proposed technique enhanced the selection accuracy and user experience but it is still with a potential to be improved in efficiency. This study contributes to providing a robust multimodal interface design supported by both eye tracking and mid-air gesture control. © 2017 IEEE.",,"Comparative experiments; Gesture control; Multi-modal information; Multimodal interface designs; Selection accuracy; User experience; Virtual 3d environments; Virtual objects; Cybernetics",Conference Paper,"Final","",Scopus,2-s2.0-85027891432
"Liu Y., Lee B.-S., McKeown M.","57192561421;7405441352;7005375626;","A new reconstruction method in gaze estimation with natural head movement",2017,"Proceedings of the 15th IAPR International Conference on Machine Vision Applications, MVA 2017",,,"7986840","219","222",,4,"10.23919/MVA.2017.7986840","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027858530&doi=10.23919%2fMVA.2017.7986840&partnerID=40&md5=f3c1fbff6fc07b3bcf7175c31fd55619","School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Medicine, University of BritishBC, Canada","Liu, Y., School of Computer Science and Engineering, Nanyang Technological University, Singapore; Lee, B.-S., School of Computer Science and Engineering, Nanyang Technological University, Singapore; McKeown, M., Department of Medicine, University of BritishBC, Canada","We present a novel reconstruction method for the appearance-based gaze estimation that allows inferring persons' gaze under natural head movement. We first study that the locally linear combination in the respective manifolds consisting of stable left and right eye appearances is efficient. The local structure of the manifolds is destroyed when there is head movement. This is due to the destruction of the intrinsic relations between the two eyes(left and right) when we do locally linear combinations. We then introduce a new combination of both eye appearances, which maintains the relation embedding into the reconstruction of the training stage. Through comparison with other well known methods, we show that the proposed method achieves an optimal performance with head pose variation. © 2017 MVA Organization All Rights Reserved.",,"Appearance based; Gaze estimation; Intrinsic relation; Linear combinations; Local structure; Optimal performance; Reconstruction method; Relation embedding; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85027858530
"Ogawa T., Nakazawa A., Nishida T.","57195404621;35807510800;35595754400;","Point of gaze estimation using corneal surface reflection and omnidirectional camera image",2017,"Proceedings of the 15th IAPR International Conference on Machine Vision Applications, MVA 2017",,,"7986842","227","230",,1,"10.23919/MVA.2017.7986842","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027843704&doi=10.23919%2fMVA.2017.7986842&partnerID=40&md5=e5ab26253748c27cf33e183250857bfc","Kyoto University, Japan","Ogawa, T., Kyoto University, Japan; Nakazawa, A., Kyoto University, Japan; Nishida, T., Kyoto University, Japan","We present a human point of gaze estimation system using corneal surface reflection and omni-directional image taken by a fish eye. Only capturing an eye image, our system entables to find where a user is looking in 360P surrounding scene image. We first generates multiple perspective scene images from an equi-rectangular image and perform registration between corneal reflection and perspective images. We then compute the point of gaze using a 3D eye model and project the point to an omni-directional image. We evaluated the robustness of registration and accuracy of PoG estimations using two indoor and five outdoor scenes, and found that gaze mapping error was 5.526[deg] on average. This result shows the potential to the marketing and outdoor training system. © 2017 MVA Organization All Rights Reserved.",,"Image segmentation; Corneal reflection; Multiple perspectives; Omni-directional; Omnidirectional cameras; Perspective image; Rectangular image; Surface reflections; Training Systems; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85027843704
"Hutt S., Mills C., Bosch N., Krasich K., Brockmole J., D'mello S.","57195276908;40661587800;55790622900;56536941600;6603687579;14053463100;","Out of the Fr-""Eye""-ing Pan: Towards gaze-based models of attention during learning with technology in the classroom",2017,"UMAP 2017 - Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization",,,,"94","103",,36,"10.1145/3079628.3079669","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026756708&doi=10.1145%2f3079628.3079669&partnerID=40&md5=f6239aa2a116f552f86cd5822bf00964","University of Notre Dame, United States; University of British Columbia, Canada; University of Illinois at Urbana-Champaign, United States","Hutt, S., University of Notre Dame, United States; Mills, C., University of British Columbia, Canada; Bosch, N., University of Illinois at Urbana-Champaign, United States; Krasich, K., University of Notre Dame, United States; Brockmole, J., University of Notre Dame, United States; D'mello, S., University of Notre Dame, United States","Attention is critical to learning. Hence, advanced learning technologies should benefit from mechanisms to monitor and respond to learners' attentional states. We study the feasibility of integrating commercial off-The-shelf (COTS) eye trackers to monitor attention during interactions with a learning technology called GuruTutor. We tested our implementation on 135 students in a noisy computer-enabled high school classroom and were able to collect a median 95% valid eye gaze data in 85% of the sessions where gaze data was successfully recorded. Machine learning methods were employed to develop automated detectors of mind wandering (MW) -A phenomenon involving a shift in attention from task-related to task-unrelated thoughts that is negatively correlated with performance. Our student-independent, gaze-based models could detect MW with an accuracy (F1 of MW = 0.59) significantly greater than chance (F1 of MW = 0.24). Predicted rates of mind wandering were negatively related to posttest performance, providing evidence for the predictive validity of the detector. We discuss next steps towards developing gaze-based, attention-Aware, learning technologies that can be deployed in noisy, real-world environments. ©2017 ACM.","Attention-Aware learning; Cyberlearning; Eye-gaze; Intelligent tutoring systems; Mind wandering","Computer aided instruction; Human computer interaction; Learning systems; Students; Teaching; Attention-Aware learning; Cyberlearning; Eye-gaze; Intelligent tutoring system; Mind wandering; Education",Conference Paper,"Final","",Scopus,2-s2.0-85026756708
"Garain U., Pandit O., Augereau O., Okoso A., Kise K.","6602234041;57200338284;25421349800;56150704300;16178222100;","Identification of Reader Specific Difficult Words by Analyzing Eye Gaze and Document Content",2017,"Proceedings of the International Conference on Document Analysis and Recognition, ICDAR","1",,,"1346","1351",,5,"10.1109/ICDAR.2017.221","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045183493&doi=10.1109%2fICDAR.2017.221&partnerID=40&md5=53ceab52f96a9a8806061cc74afbbfc1","Computer Vision and Pattern Recognition (CVPR) Unit, Indian Statistical Institute, Kolkata, 700108, India; Dept. of Computer Science and Intelligent Systems, Graduate School of Engineering, Osaka Prefecture University, Osaka, 599-8531, Japan","Garain, U., Computer Vision and Pattern Recognition (CVPR) Unit, Indian Statistical Institute, Kolkata, 700108, India; Pandit, O., Computer Vision and Pattern Recognition (CVPR) Unit, Indian Statistical Institute, Kolkata, 700108, India; Augereau, O., Dept. of Computer Science and Intelligent Systems, Graduate School of Engineering, Osaka Prefecture University, Osaka, 599-8531, Japan; Okoso, A., Dept. of Computer Science and Intelligent Systems, Graduate School of Engineering, Osaka Prefecture University, Osaka, 599-8531, Japan; Kise, K., Dept. of Computer Science and Intelligent Systems, Graduate School of Engineering, Osaka Prefecture University, Osaka, 599-8531, Japan","This paper presents an approach for identifying reader specific difficult words while someone is reading a textual document. The work is motivated by the need of developing human-document interaction systems, in general and creating person-specific online educational content, in particular. Eye gaze information gives person specific behavior whereas textual content is analyzed to get general linguistic aspect of the document content. These two pieces of information are fused together through machine learning algorithms to identify the set of difficult words for a particular reader reading a particular document. An annotated dataset has been created where each word in a document is marked with its bounding box information and each reader identifies a set of difficult words while reading the document. The dataset consists of sixteen documents and each document is read by five subjects. The method is evaluated through recall-precision analysis. The impressive precision at high recall attests the feasibility of building a practical application based on this research. The experiment further brings out several interesting facts about human reading behaviour. © 2017 IEEE.","Content personalisation; Dataset and evaluation; Difficult words; Eye tracking; Linguistic analysis; Machine learning; Reading behaviour analysis","Artificial intelligence; Behavioral research; Learning algorithms; Learning systems; Linguistics; Online systems; Behaviour analysis; Dataset and evaluation; Difficult words; Linguistic analysis; Personalisation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85045183493
"Fajnzylber V., Gonzalez L., Maldonado P., Del Villar R., Yanez R., Madariaga S., Magdics M., Sbert M.","57194057172;57215273330;7004089908;57202818671;57215290118;57202816595;36161116500;8427067100;","Augmented film narrative by use of non-photorealistic rendering",2017,"2017 International Conference on 3D Immersion, IC3D 2017 - Proceedings","2018-January",,,"1","8",,2,"10.1109/IC3D.2017.8251912","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049431827&doi=10.1109%2fIC3D.2017.8251912&partnerID=40&md5=7f9830730481f955d65fd8c1b13e575e","University of Chile, Film and Television School, Chile; University of Chile, Department of Computer Science, Chile; University of Chile, Biomedical Neuroscience Institute, Chile; University of Chile, Communication and Image Institute, Chile; Budapest University of Technology and Economics, Department of Control Engineering and Information Technology, Hungary; Tianjin University, School of Computer Science and Technology, China; University of Girona, Institute of Informatics and Applications, Spain","Fajnzylber, V., University of Chile, Film and Television School, Chile; Gonzalez, L., University of Chile, Department of Computer Science, Chile; Maldonado, P., University of Chile, Biomedical Neuroscience Institute, Chile; Del Villar, R., University of Chile, Communication and Image Institute, Chile; Yanez, R., University of Chile, Film and Television School, Chile; Madariaga, S., University of Chile, Biomedical Neuroscience Institute, Chile; Magdics, M., Budapest University of Technology and Economics, Department of Control Engineering and Information Technology, Hungary; Sbert, M., Tianjin University, School of Computer Science and Technology, China, University of Girona, Institute of Informatics and Applications, Spain","Our research is about the effects of non-photorealistic rendering in the perception of stereoscopic cinema. We conducted a study with 27 participants, using eye-tracker to evaluate ocular behavior during the free viewing of a stereoscopic film. We obtained ocular fixation data that were correlated with visual entropy. Gaze data leads us to believe that facing a highly abstract non-photorealistic scene the gaze seems to react under the effect of a perceptive anxiety. This ocular response seems to spontaneously assign greater sensorimotor resources to the analysis of the character's behavior, transforming his face into the main key to analyse the information and sense of the global scene. This behavior appears to us as an adaptive response to a visual environment endowed with abstraction and cognitive uncertainty. Our results could be useful for conceiving new non-photorealistic rendering tools for 3D films and cinematic virtual reality. © 2017 IEEE.","3D cinema; film perception; Non-photorealistic rendering; visual entropy","Entropy; Eye tracking; Stereo image processing; Three dimensional computer graphics; Virtual reality; Adaptive response; D-Cinema; Eye trackers; Non-Photorealistic Rendering; Photo-realistic; Stereoscopic cinema; Visual environments; Rendering (computer graphics)",Conference Paper,"Final","",Scopus,2-s2.0-85049431827
"Lee H., Li Y., Yeh S.-C., Huang Y., Wu Z., Du Z.","57215278389;57215279317;14619911100;57196145557;57202880044;57215291861;","ADHD assessment and testing system design based on virtual reality",2017,"Proceeding of 2017 2nd International Conference on Information Technology, INCIT 2017","2018-January",,,"1","5",,5,"10.1109/INCIT.2017.8257860","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049420741&doi=10.1109%2fINCIT.2017.8257860&partnerID=40&md5=b0f6c30b2029af7cae5e5ffe3a4eaba6","School of Information Science and Technoogy, Fudan University, China; Shanghai Institute of Intelligent Electronics and Systems, China; Huashan Hospital of Fudan University, China","Lee, H., School of Information Science and Technoogy, Fudan University, China; Li, Y., School of Information Science and Technoogy, Fudan University, China; Yeh, S.-C., School of Information Science and Technoogy, Fudan University, China, Shanghai Institute of Intelligent Electronics and Systems, China; Huang, Y., Huashan Hospital of Fudan University, China; Wu, Z., Huashan Hospital of Fudan University, China; Du, Z., Huashan Hospital of Fudan University, China","Attention deficit hyperactivity disorder (ADHD) is a common psychological and behavioral disorder in children, adolescents, and even some adults. Several symptoms are observed in ADHD patients, such as difficulty in paying attention, hyperactivity, impulsivity, and cognitive abnormalities. In the past, clinical diagnosis of ADHD was mainly conducted through a paper test scale based on relevant standards. Subsequently, although the time efficiency of test based on computer has improved, disadvantages have remained, for those tests being too tedious to attract the interest of individuals and being restricted by the content and the type. In this study, a combination of virtual reality, eye tracking, and electroencephalogram (EEG) signal acquisition technologies was used to conduct diagnostic assessment and testing on patients with ADHD. Selective attention, sustained attention, abstract reasoning, and cognitive transfer abilities were evaluated by performing visual and auditory continuous performance test (CPT) and the Wisconsin card sorting test (WCST) in a 3D virtual classroom with optional distraction factors. This study also preliminarily analyzed the eye tracking and EEG data and validated their effectiveness and convenience in ADHD diagnosis. The system may provide a deeper level of ADHD diagnosis and cognitive rehabilitation. © 2017 IEEE.","ADHD; continuous performance test; electroencephalogram; eye tracking; virtual reality","Behavioral research; Computer aided instruction; Diseases; Electroencephalography; Signal processing; Virtual reality; ADHD; Attention deficit hyperactivity disorder; Cognitive rehabilitation; Continuous Performance Test (CPT); Electroencephalogram signals; Performance tests; Selective attention; Sustained attention; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049420741
"Özacar K., Hincapié-Ramos J.D., Takashima K., Kitamura Y.","55836694300;36095742000;55211076700;7401509098;","3D Selection Techniques for Mobile Augmented Reality Head-Mounted Displays",2017,"Interacting with Computers","29","4",,"579","591",,8,"10.1093/iwc/iww035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021844714&doi=10.1093%2fiwc%2fiww035&partnerID=40&md5=3f548076f34809136a93e8d5e51648c7","Research Institute of Electrical Communication, Tohoku University, 2-1-1 Katahira, Aoba-ku, Sendai, 980-8577, Japan; Computer Engineering Department, Karabuk University, Karabuk, Turkey; Lenovo RandT, Beijing, China","Özacar, K., Research Institute of Electrical Communication, Tohoku University, 2-1-1 Katahira, Aoba-ku, Sendai, 980-8577, Japan, Computer Engineering Department, Karabuk University, Karabuk, Turkey; Hincapié-Ramos, J.D., Lenovo RandT, Beijing, China; Takashima, K., Research Institute of Electrical Communication, Tohoku University, 2-1-1 Katahira, Aoba-ku, Sendai, 980-8577, Japan; Kitamura, Y., Research Institute of Electrical Communication, Tohoku University, 2-1-1 Katahira, Aoba-ku, Sendai, 980-8577, Japan","We conducted a user study evaluating five selection techniques for augmented reality in optical see-through head-mounted displays (OST-HMDs). The techniques we studied aim at supporting mobile usage scenarios where the devices do not need external tracking tools or special environments, and therefore we selected techniques that rely solely on tracking technologies built into conventional commercially available OST-HMDs [i.e. gesture trackers, gaze tracking and inertial measurement units (IMUs)]. While two techniques are based on raycasting using built-in IMU sensing, three techniques are based on a hand-controlled 3D cursor using gestural tracking. We compared these techniques in an experiment with 12 participants. Our results show that raycasting using head orientation (i.e. IMU on the headset) was the fastest, fatigueless and the most preferable technique to select spatially arranged objects. We discuss the implications of our findings for design of interaction techniques in mobile OST-HMDs. © The Author 2016. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved.","human computer interaction (HCI); interaction techniques; mixed/augmented reality; pointing; ubiquitous and mobile devices; usability testing","Augmented reality; Eye tracking; Helmet mounted displays; Human computer interaction; Three dimensional displays; Wire pointing; Head mounted displays; Human computer interaction (HCI); Inertial measurement unit; Interaction techniques; mixed/augmented reality; Mobile augmented reality; Optical see-through head-mounted displays; Usability testing; Gesture recognition",Article,"Final","",Scopus,2-s2.0-85021844714
"Gosselin F., Faghel-Soubeyrand S.","7005257733;57194571896;","Stationary Objects Flashed Periodically Appear to Move During Smooth Pursuit Eye Movement",2017,"Perception","46","7",,"874","881",,,"10.1177/0301006617694188","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020903141&doi=10.1177%2f0301006617694188&partnerID=40&md5=6f0d8c6eb866939770bc25f9da5ccc83","Université de Montréal, Canada","Gosselin, F., Université de Montréal, Canada; Faghel-Soubeyrand, S., Université de Montréal, Canada","We discovered that a white disc flashed twice at the same location appears to move during smooth pursuit eye tracking in the direction opposite to that of the eye movement. We called this novel phenomenon movement-induced apparent motion (MIAM). Using the method of constant stimuli, we measured the required displacement of the second appearance of the disc in the pursuit direction to null the effect during the closed-loop stage of smooth pursuit eye tracking. We observed a strong linear relationship between the points of subjective stationarity and the inter-stimuli intervals for four smooth pursuit eye movement speeds. The slopes and y-intercepts of these linear fits were well predicted by the hypothesis according to which subjects saw illusory motion from the first to the second retinal projections of the flashed disc during smooth pursuit eye movement, with no extra-retinal signal compensation. © 2017, © The Author(s) 2017.","eye movements; motion; optic flow; perceptual organization; pursuit","adult; female; human; male; movement perception; oculography; optic flow; physiology; smooth pursuit eye movement; young adult; Adult; Eye Movement Measurements; Female; Humans; Male; Motion Perception; Optic Flow; Pursuit, Smooth; Young Adult",Article,"Final","",Scopus,2-s2.0-85020903141
"Kogkas A.A., Darzi A., Mylonas G.P.","56624483200;14633357600;13905959400;","Gaze-contingent perceptually enabled interactions in the operating theatre",2017,"International Journal of Computer Assisted Radiology and Surgery","12","7",,"1131","1140",,14,"10.1007/s11548-017-1580-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017335799&doi=10.1007%2fs11548-017-1580-y&partnerID=40&md5=1d3659a562449840ae7bcd7a030af620","HARMS Lab, Department of Surgery and Cancer, Imperial College London, St Mary’s Hospital, 20 South Wharf Road, 3rd Floor Paterson Centre, London, W21PF, United Kingdom","Kogkas, A.A., HARMS Lab, Department of Surgery and Cancer, Imperial College London, St Mary’s Hospital, 20 South Wharf Road, 3rd Floor Paterson Centre, London, W21PF, United Kingdom; Darzi, A., HARMS Lab, Department of Surgery and Cancer, Imperial College London, St Mary’s Hospital, 20 South Wharf Road, 3rd Floor Paterson Centre, London, W21PF, United Kingdom; Mylonas, G.P., HARMS Lab, Department of Surgery and Cancer, Imperial College London, St Mary’s Hospital, 20 South Wharf Road, 3rd Floor Paterson Centre, London, W21PF, United Kingdom","Purpose: Improved surgical outcome and patient safety in the operating theatre are constant challenges. We hypothesise that a framework that collects and utilises information —especially perceptually enabled ones—from multiple sources, could help to meet the above goals. This paper presents some core functionalities of a wider low-cost framework under development that allows perceptually enabled interaction within the surgical environment. Methods: The synergy of wearable eye-tracking and advanced computer vision methodologies, such as SLAM, is exploited. As a demonstration of one of the framework’s possible functionalities, an articulated collaborative robotic arm and laser pointer is integrated and the set-up is used to project the surgeon’s fixation point in 3D space. Results: The implementation is evaluated over 60 fixations on predefined targets, with distances between the subject and the targets of 92–212 cm and between the robot and the targets of 42–193 cm. The median overall system error is currently 3.98 cm. Its real-time potential is also highlighted. Conclusions: The work presented here represents an introduction and preliminary experimental validation of core functionalities of a larger framework under development. The proposed framework is geared towards a safer and more efficient surgical theatre. © 2017, The Author(s).","3D eye-tracking; Gaze contingent; Perceptually enabled interactions; Robot control; SLAM; Smart operating theatre","Article; diagnostic accuracy; eye tracking; gaze; gaze contingent; human; light; operating room; outcome assessment; patient safety; priority journal; robotics; spectacles; surface property; surgical patient; triangulation; computer assisted diagnosis; eye fixation; minimally invasive surgery; oculography; procedures; workflow; Eye Movement Measurements; Fixation, Ocular; Humans; Image Interpretation, Computer-Assisted; Minimally Invasive Surgical Procedures; Operating Rooms; Robotics; Workflow",Article,"Final","",Scopus,2-s2.0-85017335799
"Kim B.C., Ko D., Jang U., Han H., Lee E.C.","56726972600;56204492400;56728300700;14037180100;14009024200;","3D Gaze tracking by combining eye- and facial-gaze vectors",2017,"Journal of Supercomputing","73","7",,"3038","3052",,1,"10.1007/s11227-016-1817-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978785985&doi=10.1007%2fs11227-016-1817-5&partnerID=40&md5=7b71c750f0b470e5f6524228c93ef07f","Department of Computer Science, Graduate School, Sangmyung University, Seoul, South Korea; Department of Computer Science, Sangmyung University, Seoul, South Korea","Kim, B.C., Department of Computer Science, Graduate School, Sangmyung University, Seoul, South Korea; Ko, D., Department of Computer Science, Graduate School, Sangmyung University, Seoul, South Korea; Jang, U., Department of Computer Science, Graduate School, Sangmyung University, Seoul, South Korea; Han, H., Department of Computer Science, Sangmyung University, Seoul, South Korea; Lee, E.C., Department of Computer Science, Sangmyung University, Seoul, South Korea","We propose a 3D gaze-tracking method that combines accurate 3D eye- and facial-gaze vectors estimated from a Kinect v2 high-definition face model. Using accurate 3D facial and ocular feature positions, gaze positions can be calculated more accurately than with previous methods. Considering the image resolution of the face and eye regions, two gaze vectors are combined as a weighted sum, allocating more weight to facial-gaze vectors. Hence, the facial orientation mainly determines the gaze position, and eye-gaze vectors then perform minor manipulations. The 3D facial-gaze vector is first defined, and the 3D rotational center of the eyeball is then estimated; together, these define the 3D eye-gaze vector. Finally, the intersection point between the 3D gaze vector and the physical display plane is calculated as the gaze position. Experimental results show that the average gaze estimation root-mean-square error was approximately 23 pixels from the desired position at a resolution of 1920 × 1080. © 2016, Springer Science+Business Media New York.","3D gaze tracking; Eye-gaze vector; Facial-gaze vector; HD face model; Kinect v2","Digital television; Image resolution; Mean square error; Tracking (position); Desired position; Eye-gaze; Face modeling; Facial orientations; Gaze tracking; Intersection points; Kinect v2; Root mean square errors; Vectors",Article,"Final","",Scopus,2-s2.0-84978785985
"Papavlasopoulou S., Giannakos M.N., Sharma K., Jaccheri L.","57063398000;36462343600;55903734200;6508386763;","Using eye-tracking to unveil differences between kids and teens in coding activities",2017,"IDC 2017 - Proceedings of the 2017 ACM Conference on Interaction Design and Children",,,,"171","181",,24,"10.1145/3078072.3079740","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026320294&doi=10.1145%2f3078072.3079740&partnerID=40&md5=242b44128530757d5a9b29cf7c631431","Norwegian University of Science and Technology, Trondheim, Norway; Faculty of Business and Economics, University of Lausanne, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","Papavlasopoulou, S., Norwegian University of Science and Technology, Trondheim, Norway; Giannakos, M.N., Norwegian University of Science and Technology, Trondheim, Norway; Sharma, K., Faculty of Business and Economics, University of Lausanne, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Jaccheri, L., Norwegian University of Science and Technology, Trondheim, Norway","Computational thinking and coding is gradually becoming an important part of K-12 education. Most parents, policy makers, teachers, and industrial stakeholders want their children to attain computational thinking and coding competences, since learning how to code is emerging as an important skill for the 21st century. Currently, educators are leveraging a variety of technological tools and programming environments, which can provide challenging and dynamic coding experiences. Despite the growing research on the design of coding experiences for children, it is still difficult to say how children of different ages learn to code, and to cite differences in their task-based behaviour. This study uses eye-tracking data from 44 children (here divided into ""kids"" [age 8-12] and ""teens"" [age 13-17]) to understand the learning process of coding in a deeper way, and the role of gaze in the learning gain and the different age groups. The results show that kids are more interested in the appearance of the characters, while teens exhibit more hypothesis-testing behaviour in relation to the code. In terms of collaboration, teens spent more time overall performing the task than did kids (higher similarity gaze). Our results suggest that eye-tracking data can successfully reveal how children of different ages learn to code. © 2017 Copyright is held by the owner/author(s).","Coding; Eye-tracking; Kids; Maker movement; Teens","Codes (symbols); Eye movements; Teaching; Coding; Eye-tracking; Kids; Maker movement; Teens; Education",Conference Paper,"Final","",Scopus,2-s2.0-85026320294
"Soleymani M., Riegler M., Halvorsen P.","57188866370;56411768700;35580847300;","Multimodal analysis of image search intent: Intent recognition in image search from user behavior and visual content",2017,"ICMR 2017 - Proceedings of the 2017 ACM International Conference on Multimedia Retrieval",,,,"251","259",,7,"10.1145/3078971.3078995","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021841428&doi=10.1145%2f3078971.3078995&partnerID=40&md5=25b4ea9715dfaea6cb14a5ba2bc21ffc","Swiss Center for Affective Sciences, University of Geneva, Geneva, Switzerland; Simula Research Laboratory, University of Oslo, Oslo, Norway","Soleymani, M., Swiss Center for Affective Sciences, University of Geneva, Geneva, Switzerland; Riegler, M., Simula Research Laboratory, University of Oslo, Oslo, Norway; Halvorsen, P., Simula Research Laboratory, University of Oslo, Oslo, Norway","Users search for multimedia content with different underlying motivations or intentions. Study of user search intentions is an emerging topic in information retrieval since understanding why a user is searching for a content is crucial for satisfying the user's need. In this paper, we aimed at automatically recognizing a user's intent for image search in the early stage of a search session. We designed seven different search scenarios under the intent conditions of finding items, re-finding items and entertainment. We collected facial expressions, physiological responses, eye gaze and implicit user interactions from 51 participants who performed seven different search tasks on a custom-built image retrieval platform. We analyzed the users' spontaneous and explicit reactions under different intent conditions. Finally, we trained machine learning models to predict users' search intentions from the visual content of the visited images, the user interactions and the spontaneous responses. After fusing the visual and user interaction features, our system achieved the F-1 score of 0.722 for classifying three classes in a user-independent cross-validation. We found that eye gaze and implicit user interactions, including mouse movements and keystrokes are the most informative features. Given that the most promising results are obtained by modalities that can be captured unobtrusively and online, the results demonstrate the feasibility of deploying such methods for improving multimedia retrieval platforms. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Computer vision; Emotion; Experiment; Eye gaze; Facial expression; Intent; Multimedia; Search; User interaction","Computer vision; Experiments; Eye movements; Image retrieval; Modal analysis; Physiological models; Emotion; Eye-gaze; Facial Expressions; Intent; Multimedia; Search; User interaction; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85021841428
"Jiang Z., Das M., Gifford H.C.","56681544800;7402050566;7006524139;","Analyzing visual-search observers using eye-tracking data for digital breast tomosynthesis images",2017,"Journal of the Optical Society of America A: Optics and Image Science, and Vision","34","6",,"838","845",,7,"10.1364/JOSAA.34.000838","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020288080&doi=10.1364%2fJOSAA.34.000838&partnerID=40&md5=fecb9a30e685ebe71bb987d7b554aa08","School of Computer Science and Engineering, Changshu Institute of Technology, Changshu, 215500, China; Department of Physics, University of Houston, Houston, TX  77004, United States; Department of Biomedical Engineering, University of Houston, Houston, TX  77004, United States","Jiang, Z., School of Computer Science and Engineering, Changshu Institute of Technology, Changshu, 215500, China; Das, M., Department of Physics, University of Houston, Houston, TX  77004, United States, Department of Biomedical Engineering, University of Houston, Houston, TX  77004, United States; Gifford, H.C., Department of Biomedical Engineering, University of Houston, Houston, TX  77004, United States","Visual-search (VS) model observers have the potential to provide reliable predictions of human-observer performance in detection-localization tasks. The purpose of this work was to examine some characteristics of human gaze on breast images with the goal of informing the design of our VS observers. Using a helmet-mounted eye-tracking system, we recorded the movement of gaze from human observers as they searched for masses in sets of 2D digital breast tomosynthesis (DBT) images. The masses in this study were of a single profile. The DBT images were extracted from image volumes reconstructed with the filtered backprojection method. Fixation times associated with observer points of interest were computed from the observer data. We used the k-mean clustering algorithm to get dwell times of gaze data. The dwell times were then compared to sets of morphological feature values extracted from the images. These features, extracted as cross correlations involving the mass profile and the test image, included the matched filter (MF), gradient MF, Laplacian MF, and adaptive MF. The adaptive MF combining four feature maps was computed using a hotelling discriminant generated from training data. For this investigation, we computed correlation coefficients between the fixation times and the feature values. We also conducted a significance test by computing p-values of correlation coefficients for five features. Of all these features, the adaptive MF provided the highest correlation coefficients for DBT images with different densities. © 2017 Optical Society of America.",,"Clustering algorithms; Eye movements; Image processing; Matched filters; Medical imaging; Tomography; Correlation coefficient; Digital breast tomosynthesis; Digital breast tomosynthesis (DBT); Eye tracking systems; Filtered back-projection methods; Human-observer performance; K-mean clustering algorithm; Morphological features; Search engines; algorithm; breast tumor; computer assisted diagnosis; diagnostic imaging; eye fixation; eye movement; female; human; image enhancement; mammography; observer variation; physiology; procedures; receiver operating characteristic; Algorithms; Breast Neoplasms; Eye Movements; Female; Fixation, Ocular; Humans; Mammography; Observer Variation; Radiographic Image Enhancement; Radiographic Image Interpretation, Computer-Assisted; ROC Curve",Article,"Final","",Scopus,2-s2.0-85020288080
"Coates D.R., Wagemans J., Sayim B.","55760732200;7006148651;24438236100;","Diagnosing the periphery: Using the Rey-Osterrieth Complex Figure drawing test to characterize peripheral visual function",2017,"i-Perception","8","3",,"1","20",,14,"10.1177/2041669517705447","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020113413&doi=10.1177%2f2041669517705447&partnerID=40&md5=f9715be4cfec143c13cf9466f10c23ec","Laboratory of Experimental Psychology, KU Leuven, Leuven, Belgium; Institute of Psychology, University of Bern, Bern, Switzerland; SCALab - Sciences Cognitives et Sciences Affectives, Universités de Lille, Lille, France","Coates, D.R., Laboratory of Experimental Psychology, KU Leuven, Leuven, Belgium, Institute of Psychology, University of Bern, Bern, Switzerland; Wagemans, J., Laboratory of Experimental Psychology, KU Leuven, Leuven, Belgium; Sayim, B., Laboratory of Experimental Psychology, KU Leuven, Leuven, Belgium, Institute of Psychology, University of Bern, Bern, Switzerland, SCALab - Sciences Cognitives et Sciences Affectives, Universités de Lille, Lille, France","Peripheral vision is strongly limited by crowding, the deleterious influence of neighboring stimuli on target perception. Many quantitative aspects of this phenomenon have been characterized, but the specific nature of the perceptual degradation remains elusive. We utilized a drawing technique to probe the phenomenology of peripheral vision, using the Rey-Osterrieth Complex Figure, a standard neuropsychological clinical instrument. The figure was presented at 12° or 6° in the right visual field, with eye tracking to ensure that the figure was only presented when observers maintained stable fixation. Participants were asked to draw the figure with free viewing, capturing its peripheral appearance. A foveal condition was used to measure copying performance in direct view. To assess the drawings, two raters used standard scoring systems that evaluated feature positions, spatial distortions, and omission errors. Feature scores tended to decrease with increasing eccentricity, both within and between conditions, reflecting reduced resolution and increased crowding in peripheral vision. Based on evaluation of the drawings, we also identified new error classes unique to peripheral presentation, including number errors for adjacent similar features and distinctive spatial distortions. The multifaceted nature of the Rey- Osterrieth Complex Figure-containing configural elements, detached compound features, and texture-like components-coupled with the flexibility of the free-response drawing paradigm and the availability of standardized scoring systems, provides a promising method to probe peripheral perception and crowding. © The Author(s) 2017.","Crowding; Drawing; peripheral vision; Phenomenology",,Article,"Final","",Scopus,2-s2.0-85020113413
"Wibirama S., Nugroho H.A., Hamamoto K.","26654457700;57210591699;7102699225;","Evaluating 3D gaze tracking in virtual space: A computer graphics approach",2017,"Entertainment Computing","21",,,"11","17",,20,"10.1016/j.entcom.2017.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018286781&doi=10.1016%2fj.entcom.2017.04.003&partnerID=40&md5=d25951f1f76194b6eb2ca447d9a24f0c","Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Department of Information Media Technology, School of Information and Telecommunication Engineering, Tokai UniversityTokyo  108-8619, Japan","Wibirama, S., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Nugroho, H.A., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Hamamoto, K., Department of Information Media Technology, School of Information and Telecommunication Engineering, Tokai UniversityTokyo  108-8619, Japan","Increasing usage of stereoscopic 3D technology in virtual reality, video games, entertainment, and visualization has risen concern on development of gaze-based interaction. To develop intuitive and accurate gaze-based interaction, estimation of 3D gaze in virtual space should be validated experimentally. 3D gaze tracking is generally performed in real space with rigid object as validation target. Thus, researchers in virtual reality are constrained on choosing appropriate evaluation method when 3D gaze tracking has to be performed in virtual space. To fill this research gap, we present design and development of a new evaluation method for 3D gaze tracking in virtual space. We have implemented computer graphics technology to develop virtual plane containing virtual 3D object as validation target. Experimental results show that the proposed evaluation method was able to support real experiment by proving the accuracy of our 3D gaze tracking system with average Euclidean error less than 1 cm (Mean = 0.95 cm; S.D = 0.55 cm) in 74 cm depth of workspace. Compared with evaluation method for 3D gaze tracking in real space, the proposed method even can be implemented when space of experiment room is limited by adjusting the distance of virtual plane programmatically. © 2017 Elsevier B.V.","3D gaze estimation; Eye tracking; Quad-buffer rendering; Stereoscopic 3D; Validation method; Virtual reality","Computer graphics; Stereo image processing; Stereo vision; Target tracking; Tracking (position); Virtual reality; Computer graphics technology; Design and Development; Eye-tracking; Gaze estimation; Gaze-based interaction; Quad-buffer rendering; Stereoscopic 3-d technologies; Validation method; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85018286781
"El Hafi L., Ding M., Takamatsu J., Ogasawara T.","57188836029;39461142600;35243684200;7201579979;","Gaze tracking and object recognition from eye images",2017,"Proceedings - 2017 1st IEEE International Conference on Robotic Computing, IRC 2017",,,"7926555","310","315",,4,"10.1109/IRC.2017.44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020199058&doi=10.1109%2fIRC.2017.44&partnerID=40&md5=39c4bd663553cd6f2f37fd7d950031be","Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan","El Hafi, L., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Ding, M., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Takamatsu, J., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Ogasawara, T., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan","This paper introduces a method to identify the focused object in eye images captured from a single camera in order to enable intuitive eye-based interactions using wearable devices. Indeed, eye images allow to not only obtain natural user responses from eye movements, but also the scene reflected on the cornea without the need for additional sensors such as a frontal camera, thus making it more socially acceptable. The proposed method relies on a 3D eye model reconstruction to evaluate the gaze direction from the eye images. The gaze direction is then used in combination with deep learning algorithms to classify the focused object reflected on the cornea. Finally, the experimental results using a wearable prototype demonstrate the potential of the proposed method solely based on eye images captured from a single camera. © 2017 IEEE.","Corneal image; Eye model; Gaze tracking; Object recognition; Wearable device","Cameras; Object recognition; Robotics; Three dimensional computer graphics; Tracking (position); Wearable technology; Corneal images; Eye model; Focused object; Gaze direction; Gaze tracking; Single cameras; Wearable devices; Wearable prototypes; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85020199058
"Schoning J., Faion P., Heidemann G., Krumnack U.","57203051746;57189377947;56273420500;23397226400;","Providing video annotations in multimedia containers for visualization and research",2017,"Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017",,,"7926661","650","659",,8,"10.1109/WACV.2017.78","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016134466&doi=10.1109%2fWACV.2017.78&partnerID=40&md5=13c7a1070a1088dfac3d2b3cb2f0af52","Institute of Cognitive Science, Osnabrück University, Germany","Schoning, J., Institute of Cognitive Science, Osnabrück University, Germany; Faion, P., Institute of Cognitive Science, Osnabrück University, Germany; Heidemann, G., Institute of Cognitive Science, Osnabrück University, Germany; Krumnack, U., Institute of Cognitive Science, Osnabrück University, Germany","There is an ever increasing amount of video data sets which comprise additional metadata, such as object labels, tagged events, or gaze data. Unfortunately, metadata are usually stored in separate files in custom-made data formats, which reduces accessibility even for experts and makes the data inaccessible for non-experts. Consequently, we still lack interfaces for many common use cases, such as visualization, streaming, data analysis, machine learning, high-level understanding and semantic web integration. To bridge this gap, we want to promote the use of existing multimedia container formats to establish a standardized method of incorporating content and metadata. This will facilitate visualization in standard multimedia players, streaming via the Internet, and easy use without conversion, as shown in the attached demonstration video and files. In two prototype implementations, we embed object labels, gaze data from eye-Tracking and the corresponding video into a single multimedia container and visualize this data using a media player. Based on this prototype, we discuss the benefit of our approach as a possible standard. Finally, we argue for the inclusion of MPEG-7 in multimedia containers as a further improvement. © 2017 IEEE.",,"Computer vision; Containers; Electronic document exchange; Learning systems; Metadata; Motion Picture Experts Group standards; Visualization; Eye-tracking; Media players; Multimedia player; Prototype implementations; Standardized methods; Video annotations; Video data; Data visualization",Conference Paper,"Final","",Scopus,2-s2.0-85016134466
"Zhao Z., Feng P., Wang T., Liu F., Yuan C., Guo J., Zhao Z., Cui Z.","56969972300;57202074446;55833111900;57091781700;56834954400;57199055234;57192591248;55866427000;","Dual-scale structural local sparse appearance model for robust object tracking",2017,"Neurocomputing","237",,,"101","113",,10,"10.1016/j.neucom.2016.09.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006905544&doi=10.1016%2fj.neucom.2016.09.031&partnerID=40&md5=4d7d9668e0979dd00d4e49a652080e3d","School of Information Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China; School of Information Science and Technology, University of Jiujiang, Jiujiang, Jiangxi  332005, China; School of Computer and Information Engineering, Henan University, Kaifeng, HeNan  475004, China; Business School of Hunan University, Changsha, HuNan  410006, China","Zhao, Z., School of Information Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China, School of Information Science and Technology, University of Jiujiang, Jiujiang, Jiangxi  332005, China; Feng, P., School of Information Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China; Wang, T., School of Information Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China; Liu, F., School of Information Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China; Yuan, C., School of Information Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China, School of Computer and Information Engineering, Henan University, Kaifeng, HeNan  475004, China; Guo, J., School of Information Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China, School of Information Science and Technology, University of Jiujiang, Jiujiang, Jiangxi  332005, China; Zhao, Z., Business School of Hunan University, Changsha, HuNan  410006, China; Cui, Z., School of Information Science and Technology, University of Jiujiang, Jiujiang, Jiangxi  332005, China","Recently, sparse representation has been applied in object tracking successfully. However, the existing sparse representation captures either the holistic features of the target or the local features of the target. In this paper, we propose a dual-scale structural local sparse appearance (DSLSA) model based on overlapped patches, which can capture the quasi-holistic features and the local features of the target simultaneously. This paper first proposes two-scales structural local sparse appearance models based on overlapped patches. The larger-scale model is used to capture the structural quasi-holistic feature of the target, and the smaller-scale model is used to capture the structural local features of the target. Then, we propose a new mechanism to associate these two scale models as a new dual-scale appearance model. Both qualitative and quantitative analyses on challenging benchmark image sequences indicate that the tracker with our DSLSA model performs favorably against several state-of-the-art trackers. © 2016 Elsevier B.V.","Appearance model; Dual scale; Sparse representation; Visual tracking","Image processing; Appearance modeling; Appearance models; Dual scale; Holistic features; Qualitative and quantitative analysis; Sparse representation; State of the art; Visual Tracking; Tracking (position); adaptive structural local sparse appearance model; area under the curve; Article; controlled study; dual scale structural local sparse appearance model; eye tracking; mathematical model; measurement accuracy; measurement precision; qualitative analysis; quantitative analysis",Article,"Final","",Scopus,2-s2.0-85006905544
"Karolus J., Wozniak P.W., Chuang L.L., Schmidt A.","56644671700;55879280600;8370160400;55596321600;","Robust gaze features for enabling language proficiency awareness",2017,"Conference on Human Factors in Computing Systems - Proceedings","2017-May",,,"2998","3010",,14,"10.1145/3025453.3025601","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030855099&doi=10.1145%2f3025453.3025601&partnerID=40&md5=ff25cecdf4125fbf7710b5df1594ed17","University of Stuttgart, Stuttgart, Germany; Max Planck Institute for Biological Cybernetics, Tübingen, Germany","Karolus, J., University of Stuttgart, Stuttgart, Germany; Wozniak, P.W., University of Stuttgart, Stuttgart, Germany; Chuang, L.L., Max Planck Institute for Biological Cybernetics, Tübingen, Germany; Schmidt, A., University of Stuttgart, Stuttgart, Germany","We are often confronted with information interfaces designed in an unfamiliar language, especially in an increasingly globalized world, where the language barrier inhibits interaction with the system. In our work, we explore the design space for building interfaces that can detect the user's language proficiency. Specifically, we look at how a user's gaze properties can be used to detect whether the interface is presented in a language they understand. We report a study (N=21) where participants were presented with questions in multiple languages, whilst being recorded for gaze behavior. We identified fixation and blink durations to be effective indicators of the participants' language proficiencies. Based on these findings, we propose a classification scheme and technical guidelines for enabling language proficiency awareness on information displays using gaze data. © 2017 ACM.","Adaptive interfaces; Eye-tracking; Language-aware interfaces; Machine learning","Classification (of information); Human engineering; Learning systems; Adaptive interface; Classification scheme; Information display; Information interfaces; Language barriers; Language proficiency; Multiple languages; Technical guidelines; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85030855099
"Duffner S., Garcia C.","14035382100;7401485873;","Fast Pixelwise Adaptive Visual Tracking of Non-Rigid Objects",2017,"IEEE Transactions on Image Processing","26","5","7867761","2368","2380",,11,"10.1109/TIP.2017.2676346","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018512037&doi=10.1109%2fTIP.2017.2676346&partnerID=40&md5=fb8aa008c570e3d3649a15679a1b562e","Université de Lyon, CNRS, INSA Lyon, Lyon, F-69621, France","Duffner, S., Université de Lyon, CNRS, INSA Lyon, Lyon, F-69621, France; Garcia, C., Université de Lyon, CNRS, INSA Lyon, Lyon, F-69621, France","In this paper, we present a new algorithm for real-Time single-object tracking in videos in unconstrained environments. The algorithm comprises two different components that are trained 'in one shot' at the first video frame: A detector that makes use of the generalized Hough transform with color and gradient descriptors and a probabilistic segmentation method based on global models for foreground and background color distributions. Both components work at pixel level and are used for tracking in a combined way adapting each other in a co-Training manner. Moreover, we propose an adaptive shape model as well as a new probabilistic method for updating the scale of the tracker. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging benchmarks, and outperforms the state-of-The-Art tracking methods designed for the same task. Finally, a very efficient implementation of the proposed models allows for extremely fast tracking. © 1992-2012 IEEE.","image motion analysis; Image sequence analysis","Hough transforms; Image processing; Probability distributions; Adaptive visual tracking; Efficient implementation; Generalized Hough transform; Image motion analysis; Image sequence analysis; Non-rigid deformation; Probabilistic methods; Unconstrained environments; Tracking (position); algorithm; article; eye tracking; Hough transform; videorecording",Article,"Final","",Scopus,2-s2.0-85018512037
"Vu T., Tran H., Cho K.W., Song C., Lin F., Chen C.W., Hartley-Mcandrew M., Doody K.R., Xu W.","57211670069;57204881502;57190135629;56542250700;57188551353;57196286409;36113825700;57189294599;55546406300;","Effective and efficient visual stimuli design for quantitative autism screening: An exploratory study",2017,"2017 IEEE EMBS International Conference on Biomedical and Health Informatics, BHI 2017",,,"7897264","297","300",,6,"10.1109/BHI.2017.7897264","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018378472&doi=10.1109%2fBHI.2017.7897264&partnerID=40&md5=72ec660b0c5294fdf0ed2e1a78485bce","Department of Computer Science and Engineering, University at Buffalo (SUNY), Buffalo, NY  14260, United States; Department of Pediatrics, University at Buffalo (SUNY), Buffalo, NY  14260, United States; Department of Exceptional Education, Buffalo State University (SUNY), Buffalo, NY  14222, United States","Vu, T., Department of Computer Science and Engineering, University at Buffalo (SUNY), Buffalo, NY  14260, United States; Tran, H., Department of Computer Science and Engineering, University at Buffalo (SUNY), Buffalo, NY  14260, United States; Cho, K.W., Department of Computer Science and Engineering, University at Buffalo (SUNY), Buffalo, NY  14260, United States; Song, C., Department of Computer Science and Engineering, University at Buffalo (SUNY), Buffalo, NY  14260, United States; Lin, F., Department of Computer Science and Engineering, University at Buffalo (SUNY), Buffalo, NY  14260, United States; Chen, C.W., Department of Computer Science and Engineering, University at Buffalo (SUNY), Buffalo, NY  14260, United States; Hartley-Mcandrew, M., Department of Pediatrics, University at Buffalo (SUNY), Buffalo, NY  14260, United States; Doody, K.R., Department of Exceptional Education, Buffalo State University (SUNY), Buffalo, NY  14222, United States; Xu, W., Department of Computer Science and Engineering, University at Buffalo (SUNY), Buffalo, NY  14260, United States","Autism spectrum disorder (ASD) is one of the most common childhood developmental disorders. Early detection and intervention for ASD are critical for increasing child success. In the past decade, utilizing the abnormal eye gaze characteristics of children with autism in regard to certain visual stimuli is emerging as a screening approach due to its cost-efficiency and promising accuracy. However, the effect of visual stimulus on children with ASD has not been considered as a diagnostic consideration in the past. In this paper, we first create a visual stimuli database based on an extensive literature review, then we examine the impact of picture stimuli and exposure time on the quantitative accuracy of screenings for ASD. This is done by extracting gaze distribution in a 2D space and comparing children with ASD to typical peers using the 1st Wasserstein distance. A group of 32 participants with ASD and typical development (TD) were recruited for the study. The f-score accuracy results demonstrate the impact of implementing visual stimulus on screening for ASD. Our study demonstrates that the parsing of 'social scene' stimulus with 5-second exposure time has the best performance at 98.24%. © 2017 IEEE.",,"Autism spectrum disorders; Children with autisms; Developmental disorders; Exploratory studies; Quantitative accuracy; Screening approaches; Typical development; Wasserstein distance; Diseases",Conference Paper,"Final","",Scopus,2-s2.0-85018378472
"Zhang L., Wade J., Bian D., Fan J., Swanson A., Weitlauf A., Warren Z., Sarkar N.","55803949700;55803508600;55804018400;56963090300;53265026200;57191707622;25032282500;7201361624;","Cognitive Load Measurement in a Virtual Reality-Based Driving System for Autism Intervention",2017,"IEEE Transactions on Affective Computing","8","2","7495013","176","189",,34,"10.1109/TAFFC.2016.2582490","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028341537&doi=10.1109%2fTAFFC.2016.2582490&partnerID=40&md5=13b220fdd5ce50d5fd289322f35bf615","Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN  37203-5721, United States; Vanderbilt Kennedy Center, Treatment and Research Institute for Autism Spectrum Disorders, Vanderbilt University, Nashville, TN  37203-5721, United States; Department of Pediatrics, Vanderbilt Kennedy Center, Treatment and Research Institute for Autism Spectrum Disorders, Vanderbilt University, Nashville, TN  37203-5721, United States; Department of Electrical Engineering and Computer Science, Department of Mechanical Engineering, Vanderbilt University, Nashville, TN  37203-5721, United States","Zhang, L., Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN  37203-5721, United States; Wade, J., Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN  37203-5721, United States; Bian, D., Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN  37203-5721, United States; Fan, J., Department of Electrical Engineering and Computer Science, Vanderbilt University, Nashville, TN  37203-5721, United States; Swanson, A., Vanderbilt Kennedy Center, Treatment and Research Institute for Autism Spectrum Disorders, Vanderbilt University, Nashville, TN  37203-5721, United States; Weitlauf, A., Department of Pediatrics, Vanderbilt Kennedy Center, Treatment and Research Institute for Autism Spectrum Disorders, Vanderbilt University, Nashville, TN  37203-5721, United States; Warren, Z., Department of Pediatrics, Vanderbilt Kennedy Center, Treatment and Research Institute for Autism Spectrum Disorders, Vanderbilt University, Nashville, TN  37203-5721, United States; Sarkar, N., Department of Electrical Engineering and Computer Science, Department of Mechanical Engineering, Vanderbilt University, Nashville, TN  37203-5721, United States","Autism Spectrum Disorder (ASD) is a highly prevalent neurodevelopmental disorder with enormous individual and social cost. In this paper, a novel virtual reality (VR)-based driving system was introduced to teach driving skills to adolescents with ASD. This driving system is capable of gathering eye gaze, electroencephalography, and peripheral physiology data in addition to driving performance data. The objective of this paper is to fuse multimodal information to measure cognitive load during driving such that driving tasks can be individualized for optimal skill learning. Individualization of ASD intervention is an important criterion due to the spectrum nature of the disorder. Twenty adolescents with ASD participated in our study and the data collected were used for systematic feature extraction and classification of cognitive loads based on five well-known machine learning methods. Subsequently, three information fusion schemes - feature level fusion, decision level fusion and hybrid level fusion - were explored. Results indicate that multimodal information fusion can be used to measure cognitive load with high accuracy. Such a mechanism is essential since it will allow individualization of driving skill training based on cognitive load, which will facilitate acceptance of this driving system for clinical use and eventual commercialization. © 2010-2012 IEEE.","autism spectrum disorders; cognitive models; driving simulator; Multi-modal recognition; physiological measures; virtual realities","Automobile drivers; Diseases; Electroencephalography; Electrophysiology; Information fusion; Learning systems; Personnel training; Physiological models; Physiology; Virtual reality; Autism spectrum disorders; Cognitive model; Driving simulator; Multi-modal; Physiological measures; Psychophysiology",Article,"Final","",Scopus,2-s2.0-85028341537
"Orlosky J., Itoh Y., Ranchet M., Kiyokawa K., Morgan J., Devos H.","55641218100;56154865900;36810333400;7005065755;9272956400;23033673200;","Emulation of Physician Tasks in Eye-Tracked Virtual Reality for Remote Diagnosis of Neurodegenerative Disease",2017,"IEEE Transactions on Visualization and Computer Graphics","23","4","7829437","1302","1311",,22,"10.1109/TVCG.2017.2657018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017103301&doi=10.1109%2fTVCG.2017.2657018&partnerID=40&md5=aed47f286e6046dc6847b2c3d32643e1","Osaka University, Japan; Keio University, Japan; French Institute of Science and Technology for Transport Development and Networks, France; Augusta University, United States; University of Kansas Medical Center, United States","Orlosky, J., Osaka University, Japan; Itoh, Y., Keio University, Japan; Ranchet, M., French Institute of Science and Technology for Transport Development and Networks, France; Kiyokawa, K., Osaka University, Japan; Morgan, J., Augusta University, United States; Devos, H., University of Kansas Medical Center, United States","For neurodegenerative conditions like Parkinson's disease, early and accurate diagnosis is still a difficult task. Evaluations can be time consuming, patients must often travel to metropolitan areas or different cities to see experts, and misdiagnosis can result in improper treatment. To date, only a handful of assistive or remote methods exist to help physicians evaluate patients with suspected neurological disease in a convenient and consistent way. In this paper, we present a low-cost VR interface designed to support evaluation and diagnosis of neurodegenerative disease and test its use in a clinical setting. Using a commercially available VR display with an infrared camera integrated into the lens, we have constructed a 3D virtual environment designed to emulate common tasks used to evaluate patients, such as fixating on a point, conducting smooth pursuit of an object, or executing saccades. These virtual tasks are designed to elicit eye movements commonly associated with neurodegenerative disease, such as abnormal saccades, square wave jerks, and ocular tremor. Next, we conducted experiments with 9 patients with a diagnosis of Parkinson's disease and 7 healthy controls to test the system's potential to emulate tasks for clinical diagnosis. We then applied eye tracking algorithms and image enhancement to the eye recordings taken during the experiment and conducted a short follow-up study with two physicians for evaluation. Results showed that our VR interface was able to elicit five common types of movements usable for evaluation, physicians were able to confirm three out of four abnormalities, and visualizations were rated as potentially useful for diagnosis. © 1995-2012 IEEE.","diagnosis; eye tracking; Virtual reality; visualization","Eye movements; Eye tracking; Flow visualization; Image enhancement; Neurodegenerative diseases; Patient treatment; Virtual reality; Visualization; 3-D virtual environment; Clinical diagnosis; Clinical settings; Follow-up Studies; Metropolitan area; Neurodegenerative; Neurological disease; Parkinson's disease; Diagnosis; computer assisted diagnosis; degenerative disease; eye movement; female; human; male; Parkinson disease; pathophysiology; physiology; procedures; task performance; telemedicine; videorecording; virtual reality; Eye Movements; Female; Humans; Image Interpretation, Computer-Assisted; Male; Neurodegenerative Diseases; Parkinson Disease; Task Performance and Analysis; Telemedicine; Video Recording; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85017103301
"Lu F., Chen X., Sato Y.","54956194300;13410318100;35230954300;","Appearance-based gaze estimation via uncalibrated gaze pattern recovery",2017,"IEEE Transactions on Image Processing","26","4","7833091","1543","1553",,16,"10.1109/TIP.2017.2657880","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015728206&doi=10.1109%2fTIP.2017.2657880&partnerID=40&md5=16cea1ea9da7fe9964809b6957bdcaf9","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; Institute of Industrial Science, University of Tokyo, Tokyo, Japan","Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China, International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; Chen, X., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; Sato, Y., Institute of Industrial Science, University of Tokyo, Tokyo, Japan","Aiming at reducing the restrictions due to person/scene dependence, we deliver a novel method that solves appearance-based gaze estimation in a novel fashion. First, we introduce and solve an 'uncalibrated gaze pattern' solely from eye images independent of the person and scene. The gaze pattern recovers gaze movements up to only scaling and translation ambiguities, via nonlinear dimension reduction and pixel motion analysis, while no training/calibration is needed. This is new in the literature and enables novel applications. Second, our method allows simple calibrations to align the gaze pattern to any gaze target. This is much simpler than conventional calibrations which rely on sufficient training data to compute person and scene-specific nonlinear gaze mappings. Through various evaluations, we show that: 1) the proposed uncalibrated gaze pattern has novel and broad capabilities; 2) the proposed calibration is simple and efficient, and can be even omitted in some scenarios; and 3) quantitative evaluations produce promising results under various conditions. © 1992-2012 IEEE.","dimension reduction; Gaze estimation; uncalibrated gaze pattern","Image processing; Mathematical models; Appearance based; Dimension reduction; Gaze estimation; Nonlinear dimension; Novel applications; Quantitative evaluation; Translation ambiguities; Uncalibrated; Calibration; algorithm; anatomy and histology; automated pattern recognition; eye; eye fixation; human; image processing; physiology; procedures; Algorithms; Eye; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated",Article,"Final","",Scopus,2-s2.0-85015728206
"Liu R., Wang D., Han Y., Fan X., Luo Z.","36091355100;57188673057;57193347741;34770684500;55811864000;","Adaptive low-rank subspace learning with online optimization for robust visual tracking",2017,"Neural Networks","88",,,"90","104",,14,"10.1016/j.neunet.2017.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013167699&doi=10.1016%2fj.neunet.2017.02.002&partnerID=40&md5=d93b4fa26a826c332fa6817e89686425","School of Software Technology, Dalian University of Technology, Dalian, 116024, China; The State Key Laboratory of Integrated Services Networks, Xidian University, Xian, 710071, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, Dalian University of Technology, Dalian, 116024, China; School of Mathematical Sciences, Dalian University of Technology, Dalian, 116024, China","Liu, R., School of Software Technology, Dalian University of Technology, Dalian, 116024, China, The State Key Laboratory of Integrated Services Networks, Xidian University, Xian, 710071, China, Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, Dalian University of Technology, Dalian, 116024, China; Wang, D., School of Mathematical Sciences, Dalian University of Technology, Dalian, 116024, China; Han, Y., School of Mathematical Sciences, Dalian University of Technology, Dalian, 116024, China; Fan, X., School of Software Technology, Dalian University of Technology, Dalian, 116024, China, Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, Dalian University of Technology, Dalian, 116024, China; Luo, Z., School of Software Technology, Dalian University of Technology, Dalian, 116024, China, Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, Dalian University of Technology, Dalian, 116024, China, School of Mathematical Sciences, Dalian University of Technology, Dalian, 116024, China","In recent years, sparse and low-rank models have been widely used to formulate appearance subspace for visual tracking. However, most existing methods only consider the sparsity or low-rankness of the coefficients, which is not sufficient enough for appearance subspace learning on complex video sequences. Moreover, as both the low-rank and the column sparse measures are tightly related to all the samples in the sequences, it is challenging to incrementally solve optimization problems with both nuclear norm and column sparse norm on sequentially obtained video data. To address above limitations, this paper develops a novel low-rank subspace learning with adaptive penalization (LSAP) framework for subspace based robust visual tracking. Different from previous work, which often simply decomposes observations as low-rank features and sparse errors, LSAP simultaneously learns the subspace basis, low-rank coefficients and column sparse errors to formulate appearance subspace. Within LSAP framework, we introduce a Hadamard production based regularization to incorporate rich generative/discriminative structure constraints to adaptively penalize the coefficients for subspace learning. It is shown that such adaptive penalization can significantly improve the robustness of LSAP on severely corrupted dataset. To utilize LSAP for online visual tracking, we also develop an efficient incremental optimization scheme for nuclear norm and column sparse norm minimizations. Experiments on 50 challenging video sequences demonstrate that our tracker outperforms other state-of-the-art methods. © 2017 Elsevier Ltd","Adaptive penalization; Low-rank subspace learning; Online optimization; Visual tracking","E-learning; Video recording; Adaptive penalization; Incremental optimization; Online optimization; Optimization problems; State-of-the-art methods; Structure constraints; Subspace learning; Visual Tracking; Learning to rank; algorithm; Article; eye tracking; geometry; image analysis; low rank subspace learning with adaptive penalization; machine learning; mathematical analysis; online system; principal component analysis; qualitative analysis; quantitative analysis; robust principal component analysis; artificial intelligence; automated pattern recognition; human; procedures; theoretical model; Algorithms; Artificial Intelligence; Humans; Models, Theoretical; Pattern Recognition, Automated",Article,"Final","",Scopus,2-s2.0-85013167699
"Yafei W., Zhao T., Xueyan D., Bian J., Fu X.","55211773900;57192707963;57193958369;57200854878;7402204912;","Head pose-free eye gaze prediction for driver attention study",2017,"2017 IEEE International Conference on Big Data and Smart Computing, BigComp 2017",,,"7881713","42","46",,10,"10.1109/BIGCOMP.2017.7881713","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017628051&doi=10.1109%2fBIGCOMP.2017.7881713&partnerID=40&md5=a2387e8e2b90e3d95f07d77bd538bfdc","School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Yafei, W., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China, Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Xueyan, D., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Bian, J., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Driver's gaze direction is an indicator of driver state and plays a significantly role in driving safety. Traditional gaze zone estimation methods based on eye model have disadvantages due to the vulnerability under large head movement. Different from these methods, an appearance-based head pose-free eye gaze prediction method is proposed in this paper, for driver gaze zone estimation under free head movement. To achieve this goal, a gaze zone classifier is trained with head vectors and eye image features by random forest. The head vector is calculated by Pose from Orthography and Scaling with ITerations (POSIT) where a 3D face model is combined with facial landmark detection. And the eye image features are derived from eye images which extracted through eye region localization. These features are presented as the combination of sparse coefficients by sparse encoding with eye image dictionary, having good potential to carry information of the eye images. Experimental results show that the proposed method is applicable in real driving environment. © 2017 IEEE.","Dictionary learning; Driver state; Gaze zone; Head pose-free; Random forest","Automobile drivers; Big data; Decision trees; Image coding; 3-D face modeling; Dictionary learning; Driver state; Estimation methods; Eye-gaze predictions; Facial landmark detection; Head pose; Random forests; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85017628051
"Eivazi S., Slupina M., Fuhl W., Afkari H., Hafez A., Kasneci E.","37019970200;57193797826;56770084800;56422187000;15135814000;56059892600;","Towards automatic skill evaluation in microsurgery",2017,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,"73","76",,5,"10.1145/3030024.3040985","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016579664&doi=10.1145%2f3030024.3040985&partnerID=40&md5=6b0cc67009727922ad493b9948b13352","University of Tübingen Perception Engineering, Sand 14, Tübingen, 72076, Germany; University of Eastern Finland, School of Computing, Joensuu, Finland; Helsinki University Central Hospital, Department of Neurosurgery, Töölö Hospital, Helsinki, Finland","Eivazi, S., University of Tübingen Perception Engineering, Sand 14, Tübingen, 72076, Germany; Slupina, M., University of Tübingen Perception Engineering, Sand 14, Tübingen, 72076, Germany; Fuhl, W., University of Tübingen Perception Engineering, Sand 14, Tübingen, 72076, Germany; Afkari, H., University of Eastern Finland, School of Computing, Joensuu, Finland; Hafez, A., Helsinki University Central Hospital, Department of Neurosurgery, Töölö Hospital, Helsinki, Finland; Kasneci, E., University of Tübingen Perception Engineering, Sand 14, Tübingen, 72076, Germany","In the past decade, eye tracking has emerged as a promising answer to the increasing needs of understanding surgical expertise. The implicit desire is to design an intelligent user interface (IUI) to monitor and assess the competency of surgical trainees. In this paper, for the first time in microsurgery, we explore the potential for a surgical automatic skill assessment through a combination of machine learning techniques, computational modeling, and eye tracking. We present primary findings from a random forest classification method where we achieved about 70% recognition rate for the detection of expert and novice group. This leads us to a conclusion that prediction of the micro-surgeon performance is possible, can be automated, and that the eye movement data carry important information about the skills of micro-surgeons. Copyright held by the owner/author(s).","Eye tracking; Machine learning; Microneurosurgery; Skill assessment","Artificial intelligence; Decision trees; End effectors; Eye movements; Graphical user interfaces; Learning systems; Surgery; Computational model; Eye movement datum; Eye-tracking; Intelligent User Interfaces; Machine learning techniques; Microneurosurgery; Random forest classification; Skill assessment; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85016579664
"Vergani Dambros G., Ungewiss J., Kübler T.C., Kasneci E., Spüler M.","57193795984;56529736700;55701951700;56059892600;54886066400;","Monitoring response quality during campimetry via eye-tracking",2017,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,"61","64",,1,"10.1145/3030024.3038268","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016570258&doi=10.1145%2f3030024.3038268&partnerID=40&md5=563f93677a2c50d7c3bd1a1eb8fd96ae","Institute of Informatics, UFRGS, Germany; Vision Research, Aalen University, Germany; Computer Engineering, University of Tübingen, Germany","Vergani Dambros, G., Institute of Informatics, UFRGS, Germany; Ungewiss, J., Vision Research, Aalen University, Germany; Kübler, T.C., Computer Engineering, University of Tübingen, Germany; Kasneci, E., Computer Engineering, University of Tübingen, Germany; Spüler, M., Computer Engineering, University of Tübingen, Germany","In a variety of use-cases, deriving information on user's fatigue is an important step for content adaptation. In this work, we investigate which eye-tracking related measures can predict the error rate (as a proxy of subject's fatigue) during a visual experiment. Data was collected during a 40 minutes campimetric task, where the user has to detect visual stimuli (i.e., dots) of different contrast. We found that eye-tracking measures can be used to train a machine learning model to predict the error rate of a user with an average correlation of 0.72±0.17. The results show that this method can be used to measure the user's response quality. Copyright is held by the owner/author(s).","Blink rate; Campimetry; Eye-tracking; Fatigue; Pupil diameter; Vigilance","Fatigue of materials; Learning systems; Blink rates; Campimetry; Eye-tracking; Pupil diameter; Vigilance; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85016570258
"Barz M., Poller P., Sonntag D.","57189847803;6507187236;12241487800;","Evaluating remote and head-worn eye trackers in multi-modal speech-based HRI (Demo)",2017,"ACM/IEEE International Conference on Human-Robot Interaction",,,,"39","",,2,"10.1145/3029798.3036665","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016432565&doi=10.1145%2f3029798.3036665&partnerID=40&md5=8b6ef80eb408d264ad3387cf380efad7","German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarbrucken, Germany","Barz, M., German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarbrucken, Germany; Poller, P., German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarbrucken, Germany; Sonntag, D., German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarbrucken, Germany","Gaze is known to be a dominant modality for conveying spatial information, and it has been used for grounding in human-robot dialogues. In this work, we present the prototype of a gaze-supported multi-modal dialogue system that enhances two core tasks in human-robot collaboration: 1) our robot is able to learn new objects and their location from user instructions involving gaze, and 2) it can instruct the user to move objects and passively track this movement by interpreting the user's gaze. We performed a user study to investigate the impact of different eye trackers on user performance. In particular, we compare a head-worn device and an RGB-based remote eye tracker. Our results show that the head-mounted eye tracker outperforms the remote device in terms of task completion time and the required number of utterances due to its higher precision. © 2017 Authors.","eye tracking; human-centred computing; human-robot interaction; machine learning; multi-modal interaction","Eye movements; Human computer interaction; Learning systems; Man machine systems; Robots; Speech processing; Speech recognition; Dialogue systems; Eye-tracking; human-centred computing; Human-robot collaboration; Human-robot dialogue; Multi-Modal Interactions; Spatial informations; Task completion time; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85016432565
"Paletta L., Dini A., Murko C., Yahyanejad S., Schwarz M., Lodron G., Ladstätter S., Paar G., Velik R.","6602696802;57193756837;57193761146;36474258100;56684019900;36168421000;36623838000;7003864369;23669569200;","Towards real-time probabilistic evaluation of situation awareness from human gaze in human-robot interaction",2017,"ACM/IEEE International Conference on Human-Robot Interaction",,,,"247","248",,11,"10.1145/3029798.3038322","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016418667&doi=10.1145%2f3029798.3038322&partnerID=40&md5=13018a7cfc4a485566c84aa69b1a310f","DIGITAL, Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, 8010, Austria; ROBOTICS, Institute for Robotics and Mechatronics, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Klagenfurt, 9020, Austria; Graz University of Technology, Austria","Paletta, L., DIGITAL, Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, 8010, Austria; Dini, A., DIGITAL, Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, 8010, Austria, Graz University of Technology, Austria; Murko, C., DIGITAL, Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, 8010, Austria; Yahyanejad, S., ROBOTICS, Institute for Robotics and Mechatronics, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Klagenfurt, 9020, Austria; Schwarz, M., DIGITAL, Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, 8010, Austria; Lodron, G., DIGITAL, Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, 8010, Austria; Ladstätter, S., DIGITAL, Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, 8010, Austria; Paar, G., DIGITAL, Institute for Information and Communication Technologies, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Graz, 8010, Austria; Velik, R., ROBOTICS, Institute for Robotics and Mechatronics, JOANNEUM RESEARCH Forschungsgesellschaft MbH, Klagenfurt, 9020, Austria","Human attention processes play a major role for optimization in human-robot interaction (HRI). This work describes a novel methodology to measure situation awareness in real-time from gaze interaction with scene objects of interest using eye tracking glasses and 3D gaze analysis. A probabilistic framework of uncertainty considers coping with measurement errors in eye and position tracking. Comprehensive experiments on HRI were conducted with tasks including handover in a lab based prototypical manufacturing environment. The methodology is proven to predict a standard measure of situation awareness (SAGAT) in real-time and will open new opportunities for human factors based performance optimization in HRI applications. © 2017 Authors.","eye tracking; situation awareness; visual attention","Behavioral research; Human computer interaction; Man machine systems; Robots; Tracking (position); Uncertainty analysis; Eye-tracking; Human robot Interaction (HRI); Manufacturing environments; Performance optimizations; Probabilistic evaluation; Probabilistic framework; Situation awareness; Visual Attention; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85016418667
"Barz M., Poller P., Sonntag D.","57189847803;6507187236;12241487800;","Evaluating remote and head-worn eye trackers in multi-modal speech-based hri",2017,"ACM/IEEE International Conference on Human-Robot Interaction",,,,"79","80",,3,"10.1145/3029798.3038367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016409620&doi=10.1145%2f3029798.3038367&partnerID=40&md5=4f476b274a7e573c27f31814a7aab21c","German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarbrücken, Germany","Barz, M., German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarbrücken, Germany; Poller, P., German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarbrücken, Germany; Sonntag, D., German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarbrücken, Germany","Gaze is known to be a dominant modality for conveying spatial information, and it has been used for grounding in human-robot dialogues. In this work, we present the prototype of a gaze-supported multi-modal dialogue system that enhances two core tasks in human-robot collaboration: 1) our robot is able to learn new objects and their location from user instructions involving gaze, and 2) it can instruct the user to move objects and passively track this movement by interpreting the user's gaze. We performed a user study to investigate the impact of different eye trackers on user performance. In particular, we compare a head-worn device and an RGB-based remote eye tracker. Our results show that the head-mounted eye tracker outperforms the remote device in terms of task completion time and the required number of utterances due to its higher precision. © 2017 Authors.","eye tracking; human-centred computing; human-robot interaction; machine learning; multi-modal interaction","Eye movements; Human computer interaction; Learning systems; Man machine systems; Robots; Speech processing; Speech recognition; Dialogue systems; Eye-tracking; human-centred computing; Human-robot collaboration; Human-robot dialogue; Multi-Modal Interactions; Spatial informations; Task completion time; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85016409620
"Zhou T., Bhaskar H., Liu F., Yang J., Cai P.","56367644000;8942447300;57188640717;15039078800;35812886000;","Online learning and joint optimization of combined spatial-temporal models for robust visual tracking",2017,"Neurocomputing","226",,,"221","237",,11,"10.1016/j.neucom.2016.11.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008245239&doi=10.1016%2fj.neucom.2016.11.055&partnerID=40&md5=357f36f16201e2662459fb184faa5822","Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, 200240, China; Department of Electrical and Computer Engineering, Khalifa University of Science, Technology and Research, Abu Dhabi, 127788, United Arab Emirates; Department of Instrument Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China","Zhou, T., Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, 200240, China; Bhaskar, H., Department of Electrical and Computer Engineering, Khalifa University of Science, Technology and Research, Abu Dhabi, 127788, United Arab Emirates; Liu, F., Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, 200240, China; Yang, J., Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, 200240, China; Cai, P., Department of Instrument Science and Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China","Visual tracking is highly challenged by factors such as occlusion, background clutter, an abrupt target motion, illumination variation, and changes in scale and orientation. In this paper, an integrated framework for online learning of a fused temporal appearance and spatial constraint models for robust and accurate visual target tracking is proposed. The temporal appearance model aims to encapsulate historical appearance information of the target in order to cope with variations due to illumination changes and motion dynamics. On the other hand, the spatial constraint model exploits the relationships between the target and its neighbors to handle occlusion and deal with a cluttered background. For the purposes of reducing the computational complexity of the state estimation algorithm and in order to emphasize the importance of the different basis vectors, a K-nearest Local Smooth Algorithm (KLSA) is used to describe the spatial state model. Further, a customized Accelerated Proximal Gradient (APG) method is implemented for iteratively obtaining an optimal solution using KLSA. Finally, the optimal state estimate is obtained by using weighted samples within a particle filtering framework. Experimental results on large-scale benchmark sequences show that the proposed tracker achieves favorable performance compared to state-of-the-art methods. © 2016 Elsevier B.V.","K-nearest local smooth algorithm (KLSA); Spatial constraint model; Spatial-temporal models learning; Temporal appearance model; Visual tracking","Benchmarking; Clutter (information theory); Iterative methods; Optimization; State estimation; Target tracking; Tracking (position); Appearance modeling; Illumination variation; Spatial constraints; Spatial temporal model; State estimation algorithms; State-of-the-art methods; Visual target tracking; Visual Tracking; E-learning; algorithm; Article; clinical effectiveness; controlled study; decision tree; eye tracking; k nearest local smooth algorithm; learning; measurement accuracy; measurement precision; online system; principal component analysis; priority journal; problem solving; process optimization; spatiotemporal analysis; systematic error; validation process",Article,"Final","",Scopus,2-s2.0-85008245239
"Feng P., Xu C., Zhao Z., Liu F., Yuan C., Wang T., Duan K.","57202074446;55878062700;56969972300;57091781700;56834954400;55833111900;57190139449;","Sparse representation combined with context information for visual tracking",2017,"Neurocomputing","225",,,"92","102",,9,"10.1016/j.neucom.2016.11.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006728115&doi=10.1016%2fj.neucom.2016.11.009&partnerID=40&md5=02da895eac93f8cc0a8a576bf66720d7","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing, 210094, China; School Hospital, Huazhong University of Science and Technology, Wuhan, 430074, China","Feng, P., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Xu, C., School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing, 210094, China; Zhao, Z., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Liu, F., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Yuan, C., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Wang, T., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, 430074, China; Duan, K., School Hospital, Huazhong University of Science and Technology, Wuhan, 430074, China","In visual tracking, the main problem is to find candidates that are most likely to be the target in successive frames, so it is important to design a proper mechanism to evaluate this. In this paper, we propose a novel sparse representation based visual tracking algorithm, which well integrates the temporal and spatial context information of tracking objects into a unified framework. Specifically, we compute the similarity between the target and its candidates, which is acquired by fusing three aspects of the target's appearance variation with different weights. For the first part, we apply a patch based sparse representation to measure the similarities between the target in the first frame and candidates in current frame. Since the tracking result in the last frame provides the latest variation information of the target, we employ an image quality assessment method to obtain the similarity scores in the second part, and the spatial context information is also exploited. As the target appearance may suffer from radical changes along the video sequence, tracking that only uses the two parts mentioned above will suffer from serious drifting problems and easily cause incorrect tracking results. In order to ease this problem, we exploit the temporal context information by generating a group of history target templates adaptively according to previous tracking results, computing similarities between each candidate with them and the maximum will be used in the third part. Finally, we combine these parts to calculate the similarity scores and take those candidates with the highest score as the new targets in current frames. The extensive experiments on twelve challenging video sequences show that our algorithm can achieve performance competitive with state-of-the-art trackers. © 2016 Elsevier B.V.","Context information; Sparse representation; Visual tracking","Rapid thermal annealing; Semantics; Tracking (position); Video recording; Context information; Image quality assessment; Similarity scores; Sparse representation; Temporal and spatial; Unified framework; Visual Tracking; Visual tracking algorithm; Target tracking; algorithm; Article; context information; controlled study; eye tracking; image analysis; image quality; information; light intensity; priority journal; qualitative analysis; quantitative analysis; sparse representation; spatiotemporal analysis; statistical parameters",Article,"Final","",Scopus,2-s2.0-85006728115
"Netzel R., Weiskopf D.","55570212900;6603960393;","Hilbert attention maps for visualizing spatiotemporal gaze data",2017,"Proceedings of the 2nd Workshop on Eye Tracking and Visualization, ETVIS 2016",,,"7851160","21","25",,7,"10.1109/ETVIS.2016.7851160","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016038608&doi=10.1109%2fETVIS.2016.7851160&partnerID=40&md5=2125ad648570da2e0ebc2090d20e2b95","University of Stuttgart, VISUS, Germany","Netzel, R., University of Stuttgart, VISUS, Germany; Weiskopf, D., University of Stuttgart, VISUS, Germany","Attention maps-often in the form of heatmaps-Are a common visualization approach to obtaining an overview of the spatial distribution of gaze data from eye tracking experiments. However, attention maps are not designed to let us easily analyze the temporal information of gaze data: They completely ignore temporal information by aggregating over time, or they use animation to build a sequence of attention maps. To overcome this issue, we introduce Hilbert attention maps: A 2D static visualization of the spatiotemporal distribution of gaze points. The visualization is based on the projection of the 2D spatial domain onto a space-filling Hilbert curve that is used as one axis of our new attention map; the other axis represents time. We visualize Hilbert attention maps either as dot displays or heatmaps. This 2D visualization works for data from individual participants or large groups of participants, it supports static and dynamic stimuli alike, and it does not require any preprocessing or definition of areas of interest. We demonstrate how our visualization allows analysts to identify spatiotemporal patterns of visual reading behavior, including attentional synchrony and smooth pursuit. © 2016 IEEE.",,"Eye movements; Spatial distribution; Visualization; 2-D visualizations; Dynamic stimuli; Smooth pursuit; Spatial domains; Spatiotemporal distributions; Spatiotemporal patterns; Static visualizations; Temporal information; Data visualization",Conference Paper,"Final","",Scopus,2-s2.0-85016038608
"Naqshbandi K., Gedeon T., Abdulla U.A.","56026674400;24400830200;56025820200;","Automatic clustering of eye gaze data for machine learning",2017,"2016 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2016 - Conference Proceedings",,,"7844411","1239","1244",,6,"10.1109/SMC.2016.7844411","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015735052&doi=10.1109%2fSMC.2016.7844411&partnerID=40&md5=f55637ae01cde499086b0e2df41a9e4c","Research School of Computer Science, Australian National University, Canberra, Australia; Sch. of Eng. and IT, University of New South Wales, ADFA, Canberra, Australia","Naqshbandi, K., Research School of Computer Science, Australian National University, Canberra, Australia; Gedeon, T., Research School of Computer Science, Australian National University, Canberra, Australia; Abdulla, U.A., Sch. of Eng. and IT, University of New South Wales, ADFA, Canberra, Australia","Eye gaze patterns or scanpaths of subjects looking at art while answering questions related to the art have been used to decode those tasks with the use of certain classifiers and machine learning techniques. Some of these techniques require the artwork to be divided into several Areas or Regions of Interest. In this paper, two ways of clustering the static visual stimuli - k-means and the density based clustering algorithm called OPTICS - were used for this purpose. These algorithms were used to cluster the gaze points before classification. The classification success rates were then compared. While it was observed that both k-means and OPTICS gave better success rates than manual clustering, which is itself higher than chance level, OPTICS consistently gave higher success rates than k-means given the right parameter settings. OPTICS also formed clusters that look more intuitive and consistent with the heat map readings than k-means, which formed clusters that look unintuitive and less consistent with the heat map. © 2016 IEEE.","Density-based clustering; Eye gaze; K-means clustering; OPTICS; Task decoding","Artificial intelligence; Cybernetics; Decoding; Learning systems; Maps; Optics; Automatic clustering; Density-based Clustering; Density-based clustering algorithms; Eye-gaze; K-means clustering; Machine learning techniques; Manual clustering; Regions of interest; Clustering algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85015735052
"Bao H., Lin M., Chen Z.","57225816078;56547745100;57203879679;","Robust visual tracking based on hierarchical appearance model",2017,"Neurocomputing","221",,,"108","122",,9,"10.1016/j.neucom.2016.09.069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997497891&doi=10.1016%2fj.neucom.2016.09.069&partnerID=40&md5=108c110b554f1ae628d895398d7533f5","Department of Automation, University of Science and Technology of China, Hefei, 230027, China; Quanzhou Institute of Equipment Manufacturing, Haixi Institutes, Chinese Academy of Sciences, Jinjiang, 362200, China","Bao, H., Department of Automation, University of Science and Technology of China, Hefei, 230027, China; Lin, M., Quanzhou Institute of Equipment Manufacturing, Haixi Institutes, Chinese Academy of Sciences, Jinjiang, 362200, China; Chen, Z., Department of Automation, University of Science and Technology of China, Hefei, 230027, China","In order to track the target object effectively in the presence of significant appearance variation, e.g., occlusion, scale variation, deformation, fast motion and background clutter, we develop a new approach based on hierarchical appearance model under the Bayesian framework. The proposed approach represents the target at two levels, i.e., the local and the global levels. At the local level, a set of local patches are used to represent the target so as to adapt the changes in appearance. Likelihood defined as the weighted sum of reliability index and stability index is applied to evaluate how likely a patch pertaining to the target. At the global level, the target is represented by using double bounding boxes regarding the foreground and background, respectively. The inner bounding box only contains the target region, and the outer bounding box contains both the target region and the background region surrounding the target. The target model is encoded by using two HSV color histograms with respect to the target and the background, respectively. As this, the drifts can be effectively suppressed in the tracking process. Furthermore, the object position can be estimated by maximizing the likelihood of the target under the Bayesian framework. An experimental study is employed to illustrate the advantages of our proposed approach. The experimental results demonstrate that our method is very effective and performs favorably in comparison to the state-of-the-art trackers in terms of efficiency, accuracy and robustness. © 2016 Elsevier B.V.","Bayesian framework; Hierarchical appearance model; Visual tracking","Computer applications; Neural networks; Appearance modeling; Background clutter; Background region; Bayesian frameworks; Hsv color histograms; nocv1; Reliability Index; Stability indices; Visual Tracking; Tracking (position); accuracy; Article; Bayes theorem; eye tracking; hierarchical appearance model; histogram; priority journal; quantitative analysis; reliability; statistical model",Article,"Final","",Scopus,2-s2.0-84997497891
"Soliman M., Tavakoli H.R., Laaksonen J.","57193344768;55765000935;56253107200;","Towards gaze-based video annotation",2017,"2016 6th International Conference on Image Processing Theory, Tools and Applications, IPTA 2016",,,"7821028","","",,3,"10.1109/IPTA.2016.7821028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013170705&doi=10.1109%2fIPTA.2016.7821028&partnerID=40&md5=176b65c1aa781f4c3559a31a7e19dd00","Aalto University, Finland","Soliman, M., Aalto University, Finland; Tavakoli, H.R., Aalto University, Finland; Laaksonen, J., Aalto University, Finland","This paper presents our efforts towards a framework for video annotation using gaze. In computer vision, video annotation (VA) is an essential step in providing a ground truth for the evaluation of object detection and tracking techniques. VA is a demanding element in the development of video processing algorithms, where each object of interest should be manually labelled. Although the community has handled VA for a long time, the size of new data sets and the complexity of the new tasks pushes us to revisit it. A barrier towards automated video annotation is the recognition of the object of interest and tracking it over image sequences. To tackle this problem, we employ the concept of visual attention for enhancing video annotation. In an image, human attention naturally grasps interesting areas that provide valuable information for extracting the objects of interest, which can be exploited to annotate videos. Under task-based gaze recording, we utilize an observer's gaze to filter seed object detector responses in a video sequence. The filtered boxes are then passed to an appearance-based tracking algorithm. We evaluate the gaze usefulness by comparing the algorithm with gaze and without it. We show that eye gaze is an influential cue for enhancing the automated video annotation, improving the annotation significantly. © 2016 IEEE.","Eye gaze; Object detection and tracking; Video annotation; Video processing; Visual attenstion","Behavioral research; Image processing; Object detection; Object recognition; Tracking (position); Eye-gaze; Object detection and tracking; Video annotations; Video processing; Visual attenstion; Video signal processing",Conference Paper,"Final","",Scopus,2-s2.0-85013170705
"Xia Z., Zhang W., Tan F., Feng X., Hadid A.","36623086300;57218452085;57193343224;16642777700;55925650500;","An accurate eye localization approach for smart embedded system",2017,"2016 6th International Conference on Image Processing Theory, Tools and Applications, IPTA 2016",,,"7821006","","",,7,"10.1109/IPTA.2016.7821006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013150864&doi=10.1109%2fIPTA.2016.7821006&partnerID=40&md5=62a3464b8a00b30fee31845b99407d5e","School of Electronics and Information, Northwestern Polytechnical University, China; Center for Machine Vision Research, University of Oulu, Finland","Xia, Z., School of Electronics and Information, Northwestern Polytechnical University, China; Zhang, W., School of Electronics and Information, Northwestern Polytechnical University, China; Tan, F., School of Electronics and Information, Northwestern Polytechnical University, China; Feng, X., School of Electronics and Information, Northwestern Polytechnical University, China; Hadid, A., School of Electronics and Information, Northwestern Polytechnical University, China, Center for Machine Vision Research, University of Oulu, Finland","Eye localization is a vital procedure in many applications, such as face recognition and gaze tracking, and can further facilitate related procedures. Although many works have been devoted to localizing eyes in frontal facial images, most approaches cannot work effectively and efficiently in smart embedded systems (e.g., the vehicle system). In this paper, we propose an accurate eye localization approach for smart embedded systems. An illumination normalization procedure with the perception based model is utilized to remove the illumination effects of facial images. Then the integral projection method is employed to localize the candidate positions of eyes. The support vector machine (SVM) classifiers are trained with the spacial and intensity information to verify these candidates rapidly with compact 3-dimensional features. Based on the output of SVMs, the two candidates with top scores are determined as the final accurate eye positions. Extensive experiments on the extended Yale B, AR and ORL face datasets demonstrate that the proposed approach achieves good accuracy and fast computation results for localizing eyes. © 2016 IEEE.","3D Feature; Eye Localization; Illumination Normalization; Integral Projection; SVM","Classification (of information); Computation theory; Face recognition; Image processing; Support vector machines; Tracking (position); 3D Feature; Candidate positions; Eye localization; Illumination effect; Illumination normalization; Integral projections; Intensity information; Perception-based; Embedded systems",Conference Paper,"Final","",Scopus,2-s2.0-85013150864
"Huang G.-J., Du X., Zhu Y.-F.","57193422951;15046410300;13408190100;","Learning stereoscopic visual attention model for 3D video",2017,"2015 International Conference on Computer Science and Applications, CSA 2015",,,"7810822","6","9",,1,"10.1109/CSA.2015.17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013827903&doi=10.1109%2fCSA.2015.17&partnerID=40&md5=588bc87791c1d1a33664ada2fe0a5069","Department of Information and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Computer and Information Engineering, Zhejiang Gongshang University, Hangzhou, China","Huang, G.-J., Department of Information and Electronic Engineering, Zhejiang University, Hangzhou, China; Du, X., Department of Information and Electronic Engineering, Zhejiang University, Hangzhou, China; Zhu, Y.-F., College of Computer and Information Engineering, Zhejiang Gongshang University, Hangzhou, China","Various saliency detection model have been proposed for the visual attention prediction in 2D images/videos. The rapid development of stereoscopic display techniques along with the emerging 3D applications, bring the depth information. Depth is an appealing vision feature since it provides instantaneous depth perception for the scene and can affect human's visual attention. In this paper, we propose to build a model of visual attention based on learning method for stereoscopic videos. The proposed method takes the eye tracking data to build up the ground truth saliency map. The 2D features and the depth information are integrated by the learning method. Experimental results demonstrate that our method is effective and reasonable. © 2015 IEEE.","Eye tracking; Stereoscopic videos; SVM; Visual attention","Depth perception; Learning systems; Stereo image processing; Depth information; Eye-tracking; Learning methods; Saliency detection; Stereoscopic display; Stereoscopic video; Visual Attention; Visual attention model; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85013827903
"Schrom-Feiertag H., Settgast V., Seer S.","37017384000;6504006479;16178702300;","Evaluation of indoor guidance systems using eye tracking in an immersive virtual environment",2017,"Spatial Cognition and Computation","17","1-2",,"163","183",,34,"10.1080/13875868.2016.1228654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991017537&doi=10.1080%2f13875868.2016.1228654&partnerID=40&md5=e9f7870b9841d394c4b5100908f17ff2","Mobility Department, Austrian Institute of Technology, Vienna, Austria; Division Visual Computing, Fraunhofer Austria, Graz, Austria","Schrom-Feiertag, H., Mobility Department, Austrian Institute of Technology, Vienna, Austria; Settgast, V., Division Visual Computing, Fraunhofer Austria, Graz, Austria; Seer, S., Mobility Department, Austrian Institute of Technology, Vienna, Austria","In this article, we present a novel method for evaluating guidance systems using an immersive virtual environment in combination with a mobile eye tracking system. Accurate measurements of position, locomotion, viewing frustum, and gaze are captured in the virtual environment. They are applied to the projection of an attention map onto the virtual 3D environment for visualizing the fixation in the environment as well as the amount of time objects were fixated. To demonstrate the method's applicability, we conducted an experiment with 24 participants evaluating a guidance system of a large public infrastructure. The results show that our method allows for the creation of attention maps as well as for the identification of objects of interest based on eye tracking. © 2017 Taylor & Francis.","eye tracking; virtual environments; virtual reality; visual perception; wayfinding","Remote control; Accurate measurement; Eye-tracking; Immersive virtual environments; Mobile eye-tracking; Public infrastructures; Virtual 3d environments; Visual perception; Way-finding; Virtual reality",Article,"Final","",Scopus,2-s2.0-84991017537
"Fraser K.C., Fors K.L., Kokkinakis D., Nordlund A.","55551099200;57205408602;24331949100;22951856800;","An analysis of eye-movements during reading for the detection of mild cognitive impairment",2017,"EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,"1016","1026",,10,"10.18653/v1/d17-1107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067577996&doi=10.18653%2fv1%2fd17-1107&partnerID=40&md5=d49ce3556ce09a500cb52d3cbbdea407","Swedish Language Bank, Department of Swedish, Sweden; Department of Psychiatry and Neurochemistry University of Gothenburg, Sweden","Fraser, K.C., Swedish Language Bank, Department of Swedish, Sweden; Fors, K.L., Swedish Language Bank, Department of Swedish, Sweden; Kokkinakis, D., Swedish Language Bank, Department of Swedish, Sweden; Nordlund, A., Department of Psychiatry and Neurochemistry University of Gothenburg, Sweden","We present a machine learning analysis of eye-tracking data for the detection of mild cognitive impairment, a decline in cognitive abilities that is associated with an increased risk of developing dementia. We compare two experimental configurations (reading aloud versus reading silently), as well as two methods of combining information from the two trials (concatenation and merging). Additionally, we annotate the words being read with information about their frequency and syntactic category, and use these annotations to generate new features. Ultimately, we are able to distinguish between participants with and without cognitive impairment with up to 86% accuracy. © 2017 Association for Computational Linguistics.",,"Eye tracking; Learning algorithms; Natural language processing systems; Risk assessment; Syntactics; Cognitive ability; Cognitive impairment; Mild cognitive impairments; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85067577996
"Suda S., Yamagishi K., Takemura K.","56145672600;57201291604;8575290600;","User calibration-free method using corneal surface image for eye tracking",2017,"VISIGRAPP 2017 - Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","6",,,"67","73",,,"10.5220/0006100100670073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047849222&doi=10.5220%2f0006100100670073&partnerID=40&md5=8dc79cf7d1ee24cd831ea2f66c083320","School of Engineering, Tokai University, Hiratsuka, Japan; Department of Applied Computer Engineering, Tokai University, Hiratsuka, Japan","Suda, S., School of Engineering, Tokai University, Hiratsuka, Japan; Yamagishi, K., Department of Applied Computer Engineering, Tokai University, Hiratsuka, Japan; Takemura, K., School of Engineering, Tokai University, Hiratsuka, Japan, Department of Applied Computer Engineering, Tokai University, Hiratsuka, Japan","Various calibration methods to determine the point-of-regard have been proposed for eye tracking. Although user calibration can be performed for experiments carried out in the laboratory, it is unsuitable when applying an eye-Tracker in user interfaces and in public displays. Therefore, we propose a novel calibration-free approach for users that is based on the use of the corneal surface image. As the environmental information is reflected on the corneal surface, we extracted the unwarped image around the point-of-regard from the cornea. The point-of-regard is estimated on the screen by using the unwarped image, and the regression formula is solved using these points without user calibration. We implemented the framework of the algorithm, and we confirmed the feasibility of the proposed method through experiments. © 2017 by SCITEPRESS - Science and Technology Publications, Lda.","3D Eye Model; Corneal Surface Image; User Calibration-free","3D modeling; Calibration; Computer graphics; Computer vision; User interfaces; 3D eye models; Calibration free; Calibration method; Environmental information; Point of regards; Regression formulas; Surface image; User calibration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85047849222
"Fuhl W., Santini T., Geisler D., Kübler T., Kasneci E.","56770084800;54881866000;57189847283;55701951700;56059892600;","EyeLad: Remote eye tracking image labeling tool: Supportive eye, eyelid and pupil labeling tool for remote eye tracking videos",2017,"VISIGRAPP 2017 - Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","5",,,"405","410",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047847240&partnerID=40&md5=6ac6584f53fdbb1b5311ca1b8c098344","Perception Engineering, University of Tübingen, Tübingen, Germany","Fuhl, W., Perception Engineering, University of Tübingen, Tübingen, Germany; Santini, T., Perception Engineering, University of Tübingen, Tübingen, Germany; Geisler, D., Perception Engineering, University of Tübingen, Tübingen, Germany; Kübler, T., Perception Engineering, University of Tübingen, Tübingen, Germany; Kasneci, E., Perception Engineering, University of Tübingen, Tübingen, Germany","Ground truth data is an important prerequisite for the development and evaluation of many algorithms in the area of computer vision, especially when these are based on convolutional neural networks or other machine learning approaches that unfold their power mostly by supervised learning. This learning relies on ground truth data, which is laborious, tedious, and error prone for humans to generate. In this paper, we contribute a labeling tool (EyeLad) specifically designed for remote eye-tracking data to enable researchers to leverage machine learning based approaches in this field, which is of great interest for the automotive, medical, and human-computer interaction applications. The tool is multi platform and supports a variety of state-of-theart detection and tracking algorithms, including eye detection, pupil detection, and eyelid coarse positioning. Furthermore, the tool provides six types of point-wise tracking to automatically track the labeled points. The software is openly and freely available at: www.ti.uni-tuebingen.de/perception. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","Data labeling; Eye tracking data; Feature tracking; Image processing; Object detection; Remote eye tracking","Computer graphics; Computer vision; Eye protection; Human computer interaction; Image processing; Learning systems; Neural networks; Object detection; Coarse positioning; Convolutional neural network; Data labeling; Detection and tracking algorithms; Feature-tracking; Ground truth data; Machine learning approaches; Pupil detection; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85047847240
"Tokunaga T., Nishikawa H., Iwakura T.","22434161600;37012639700;23389313800;","An eye-tracking study of named entity annotation",2017,"International Conference Recent Advances in Natural Language Processing, RANLP","2017-September",,,"758","764",,5,"10.26615/978-954-452-049-6-097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045764082&doi=10.26615%2f978-954-452-049-6-097&partnerID=40&md5=b439a472e4f23a9af78d56f355708c4c","Tokyo Institute of Technology, Japan; Fujitsu Laboratories Ltd., Japan","Tokunaga, T., Tokyo Institute of Technology, Japan; Nishikawa, H., Tokyo Institute of Technology, Japan; Iwakura, T., Fujitsu Laboratories Ltd., Japan","Utilising effective features in machine learning-based natural language processing (NLP) is crucial in achieving good performance for a given NLP task. The paper describes a pilot study on the analysis of eye-ttacking data during named entity (NE) annotation, aiming at obtaining insights into effective features for the NE recognition task. The eye gaze data were collected from 10 annotators and analysed regarding working time and fixation distribution. The results of the preliminary qualitative analysis showed that human annotators tend to look at broader contexts around the target NE than recent state-of-the-art automatic NE recognition systems and to use predicate argument relations to identify the NE categories. © 2018 Association for Computational Linguistics (ACL). All rights reserved.",,"Deep learning; Learning algorithms; Natural language processing systems; Eye-tracking studies; Fixation distributions; Named entities; Pilot studies; Qualitative analysis; Recent state; Recognition systems; Working time; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85045764082
"Tomi A., Rambli D.R.A.","55638599800;25824683000;","A conceptual design of spatial calibration for optical see-through head mounted display using electroencephalographic signal processing on eye tracking",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10645 LNCS",,,"331","339",,,"10.1007/978-3-319-70010-6_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035098234&doi=10.1007%2f978-3-319-70010-6_31&partnerID=40&md5=43c38d67efdb36faa8ec221fd9be9bef","Department of Computer Information Sciences, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, Perak, Malaysia","Tomi, A., Department of Computer Information Sciences, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, Perak, Malaysia; Rambli, D.R.A., Department of Computer Information Sciences, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, Perak, Malaysia","One of vital issue in Optical See-Through Head Mounted Display (OST HMD) used in Augmented Reality (AR) systems is frequent (re)calibrations. OST HMD calibration that involved user interaction is time consuming. It will distract users from their application, which will reduce AR experience. Additionally, (re)calibration procedure will be prone to user errors. Nowadays, there are several approaches toward interaction-free calibration on OST HMD. In this proposed work, we propose a novel approach that uses EEG signal processing on eye movement into OST HMD calibration. By simultaneously recording eye movements through EEG during a guided eye movement paradigm, a few properties of eye movement artifacts can be useful for eye localization algorithm which can be used in interaction-free calibration for OST HMD. The proposed work is expected to enhance OST HMD calibration focusing on spatial calibration formulation in term reducing 2D projection error. © Springer International Publishing AG 2017.","Electroencephalographic; Eye-tracking; Optical see-through head mounted display; Spatial calibration","Augmented reality; Biomedical signal processing; Calibration; Conceptual design; Electroencephalography; Helmet mounted displays; Optical signal processing; Signal processing; Augmented reality systems; Calibration procedure; EEG signal processing; Electroencephalographic; Electroencephalographic signals; Eye-tracking; Optical see-through head-mounted displays; Spatial calibration; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85035098234
"González-Ortega D., González-Díaz J., Díaz-Pernas F.J., Martínez-Zarzuela M., Antón-Rodríguez M.","13408419500;57196052918;6507870236;23393198300;25122110400;","3D kinect-based gaze region estimation in a driving simulator",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10586 LNCS",,,"789","795",,,"10.1007/978-3-319-67585-5_76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031409301&doi=10.1007%2f978-3-319-67585-5_76&partnerID=40&md5=29ea6fb5852ed9aaa0b66bd52fbd92d6","Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain","González-Ortega, D., Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain; González-Díaz, J., Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain; Díaz-Pernas, F.J., Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain; Martínez-Zarzuela, M., Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain; Antón-Rodríguez, M., Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain","In this paper, we present a 3D Kinect-based gaze region estimation module to add gaze pattern information in a driving simulator. Gaze region is estimated using only face orientation cues, similarly to other previous approaches in the literature. An initial user-based calibration stage is included in our approach. The module is able to detect the region, out of 7 in which the driving scene was divided, that a driver is gazing on route every processed frame. 8 people tested the module, which achieved an accuracy of 88.23%. The information provided by the gaze estimation module enriches the driving simulator data and makes it possible a multimodal driving performance analysis. © 2017, Springer International Publishing AG.","3D computer vision; Confusion matrix; Driving simulator; Face tracking; Gaze estimation; Kinect device","Artificial intelligence; Automobile drivers; Automobile simulators; Simulators; Ubiquitous computing; 3D computer vision; Confusion matrices; Driving simulator; Face Tracking; Gaze estimation; Kinect device; Ambient intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85031409301
"Morozkin P., Swynghedauw M., Trocan M.","55886731400;56919530000;12783726500;","Neural Network Based Eye Tracking",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10449 LNAI",,,"600","609",,2,"10.1007/978-3-319-67077-5_58","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030837025&doi=10.1007%2f978-3-319-67077-5_58&partnerID=40&md5=83f8ea2947e9a86a594ddf72b2135b84","SuriCog, 130 Rue de Lourmel, Paris, 75015, France; Institut Supérieur d’Electronique de Paris, 28 Rue Notre Dame des Champs, Paris, 75006, France","Morozkin, P., SuriCog, 130 Rue de Lourmel, Paris, 75015, France, Institut Supérieur d’Electronique de Paris, 28 Rue Notre Dame des Champs, Paris, 75006, France; Swynghedauw, M., SuriCog, 130 Rue de Lourmel, Paris, 75015, France; Trocan, M., Institut Supérieur d’Electronique de Paris, 28 Rue Notre Dame des Champs, Paris, 75006, France","The EyeDee embedded eye tracking solution developed by SuriCog is the world’s first solution using the eye as a real-time mobile digital cursor, while maintaining full mobility. In order to reduce the time of eye image transmission, image compression techniques can be employed. Being hardware implemented, several standard image coding systems (JPEG and JPEG2000) were evaluated for their potential use in the next generation device of the EyeDee product line. In order to satisfy low-power, low-heat, low-MIPS requirements several non-typical approaches have been considered. One example consists in the complete replacement of currently used eye tracking algorithm based on image processing coupled with geometric eye modeling by a precisely tuned and perfectly trained neural network, which directly transforms wirelessly transmitted floating-point values of decimated eye image (result of the 3D perspective projection of a model of rotating pupil disk) into five floating-point parameters of pupil’s ellipse (result of the eye tracking). Hence implementation of the eye tracking algorithm is reduced to a known challenge of neural network construction and training, preliminary results of which are presented in the paper. © 2017, Springer International Publishing AG.","Eye tracking; Human–machine interaction; Neural networks","Digital arithmetic; Digital image storage; Image coding; Image processing; Mathematical transformations; Neural networks; Three dimensional computer graphics; Tracking (position); Eye-tracking; Floating point parameters; Floating points; Image compression techniques; Network construction; Perspective projections; Standard images; Trained neural networks; Image compression",Conference Paper,"Final","",Scopus,2-s2.0-85030837025
"Razin Y., Feigh K.","57195955923;15831610200;","Learning to predict intent from gaze during robotic hand-eye coordination",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,,,"4596","4602",,11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030476651&partnerID=40&md5=5b83b72adc950f3b3f6ded4f095795c6","School of Aerospace Engineering, Georgia Institute of Technology, 270 Ferst Drive, NW, Atlanta, GA  30332, United States","Razin, Y., School of Aerospace Engineering, Georgia Institute of Technology, 270 Ferst Drive, NW, Atlanta, GA  30332, United States; Feigh, K., School of Aerospace Engineering, Georgia Institute of Technology, 270 Ferst Drive, NW, Atlanta, GA  30332, United States","Effective human-aware robots should anticipate their user's intentions. During hand-eye coordination tasks, gaze often precedes hand motion and can serve as a powerful predictor for intent. However, cooperative tasks where a semi-autonomous robot serves as an extension of the human hand have rarely been studied in the context of hand-eye coordination. We hypothesize that accounting for anticipatory eye movements in addition to the movements of the robot will improve intent estimation. This research compares the application of various machine learning methods to intent prediction from gaze tracking data during robotic hand-eye coordination tasks. We found that with proper feature selection, accuracies exceeding 94% and AUC greater than 91% are achievable with several classification algorithms but that anticipatory gaze data did not improve intent prediction. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Forecasting; Learning systems; Robotic arms; Robotics; Robots; Classification algorithm; Cooperative tasks; Gaze tracking; Hand eye coordination; Human-aware; Machine learning methods; Semi-autonomous robots; User's intentions; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85030476651
"Lejeune L., Christoudias M., Sznitman R.","57195776416;55698981300;36100657400;","Expected Exponential Loss for Gaze-Based Video and Volume Ground Truth Annotation",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10552 LNCS",,,"106","115",,3,"10.1007/978-3-319-67534-3_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029807545&doi=10.1007%2f978-3-319-67534-3_12&partnerID=40&md5=5246472af0f1582b8567019bf65e7de4","University of Bern, Bern, Switzerland; Weather Analytics, Washington, DC, United States","Lejeune, L., University of Bern, Bern, Switzerland; Christoudias, M., Weather Analytics, Washington, DC, United States; Sznitman, R., University of Bern, Bern, Switzerland","Many recent machine learning approaches used in medical imaging are highly reliant on large amounts of image and ground truth data. In the context of object segmentation, pixel-wise annotations are extremely expensive to collect, especially in video and 3D volumes. To reduce this annotation burden, we propose a novel framework to allow annotators to simply observe the object to segment and record where they have looked at with a $200 eye gaze tracker. Our method then estimates pixel-wise probabilities for the presence of the object throughout the sequence from which we train a classifier in semi-supervised setting using a novel Expected Exponential loss function. We show that our framework provides superior performances on a wide range of medical image settings compared to existing strategies and that our method can be combined with current crowd-sourcing paradigms as well. © 2017, Springer International Publishing AG.",,"Data visualization; Learning systems; Medical imaging; Pixels; Stents; Exponential loss function; Eye gaze trackers; Ground truth; Ground truth data; Large amounts; Machine learning approaches; Object segmentation; Semi-supervised; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85029807545
"Wan Z., Xiong C.","57195490237;57211738191;","Estimating 3D gaze point on object using stereo scene cameras",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10462 LNAI",,,"323","329",,,"10.1007/978-3-319-65289-4_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028461642&doi=10.1007%2f978-3-319-65289-4_31&partnerID=40&md5=1e437877023832a9a9f7d19fdf8afcca","State Key Lab of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China","Wan, Z., State Key Lab of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China; Xiong, C., State Key Lab of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China","3D eye gaze estimation in real environment is still challenging. A novel method of scene-based 3D gaze estimation is proposed in this paper. As this model combines two models, the 2D nonlinear polynomial mapping model of traditional regression-based gaze estimation and the 3D visual axis linear ray model of traditional geometry-based gaze estimation, it includes two steps. The first step is to estimate the visual axis from the pupil center in an eye camera image. The second one is to estimate the 3D gaze point which is the intersection between the visual axis and the scene object, which can be obtained by stereo scene cameras. As the 3D gaze points are on the object, rather than outside or inside the object like geometry-based 3D gaze estimation, this method is potential for human robot interaction in real environment. Through a simple test, the accuracy of our 3D gaze estimation system is acceptable. © Springer International Publishing AG 2017.","3D gaze estimation; Eye tracking; Stereo scene cameras","Cameras; Robotics; Robots; Eye-tracking; Gaze estimation; Nonlinear polynomials; Pupil centers; Real environments; Scene object; Simple tests; Stereo scene; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85028461642
"Li Q., Xiong C., Liu K.","57204585324;57211738191;57197928286;","Eye gaze tracking based interaction method of an upper-limb exoskeletal rehabilitation robot",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10462 LNAI",,,"340","349",,3,"10.1007/978-3-319-65289-4_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028461276&doi=10.1007%2f978-3-319-65289-4_33&partnerID=40&md5=60a15ecb6b41da73ff6e787917a512a3","State Key Lab of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China","Li, Q., State Key Lab of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China; Xiong, C., State Key Lab of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China; Liu, K., State Key Lab of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China","Stroke is one of the leading causes of long-term disability today. Rehabilitation robot can benefit the patients to perform intensive and repetitive task-specific rehabilitation training to enhance motor recovery with less effort. Engagement in rehabilitation training, namely active rehabilitation is a key factor for patient to get effective recovery. In this paper, we propose an upper-limb exoskeletal underactuated rehabilitation robot with only 2 actuated degrees of freedom (DOFs) for task-specific rehabilitation training. To ensure the patients’ engagement in performing training, we used an eye gaze tracking based interaction method to guide the end-point of the robot to move on its workspace, a 2D surface, as a result of performing task-specific rehabilitation training. Finally, some validation experiment is conducted on the interaction efficiency. © Springer International Publishing AG 2017.","Eye gaze tracking; Patient engagement in training; Rehabilitation robotics; Upper extremity exoskeleton","Degrees of freedom (mechanics); Exoskeleton (Robotics); Eye movements; Neuromuscular rehabilitation; Robotics; Robots; Tracking (position); Active rehabilitations; Degrees of freedom (DoFs); Eye gaze tracking; Interaction efficiency; Rehabilitation robot; Rehabilitation robotics; Rehabilitation training; Upper extremity; Patient rehabilitation",Conference Paper,"Final","",Scopus,2-s2.0-85028461276
"Fernandes D.L., Siqueira-Batista R., Gomes A.P., Souza C.R., Da Costa I.T., Cardoso F.D.S.L., De Assis J.V., Caetano G.H.L., Cerqueira F.R.","57196659838;7004326329;7202386004;57193637190;56352370300;56814395100;57195348493;57195350812;35075705600;","Investigation of the visual attention role in clinical bioethics decision-making using machine learning algorithms",2017,"Procedia Computer Science","108",,,"1165","1174",,3,"10.1016/j.procs.2017.05.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027312266&doi=10.1016%2fj.procs.2017.05.032&partnerID=40&md5=d660b3841be4cae736d1445b1c93db59","Graduate Program in Computer Science, Universidade Federal de Viçosa, Minas Gerais, Brazil; Department of Production Engineering, Universidade Federal Fluminense, Rio de Janeiro, Brazil; Department of Medicine and Nursing, Universidade Federal de Viçosa, Minas Gerais, Brazil; Department of Physical Education, Universidade Federal de Viçosa, Minas Gerais, Brazil","Fernandes, D.L., Graduate Program in Computer Science, Universidade Federal de Viçosa, Minas Gerais, Brazil; Siqueira-Batista, R., Department of Medicine and Nursing, Universidade Federal de Viçosa, Minas Gerais, Brazil; Gomes, A.P., Department of Medicine and Nursing, Universidade Federal de Viçosa, Minas Gerais, Brazil; Souza, C.R., Department of Medicine and Nursing, Universidade Federal de Viçosa, Minas Gerais, Brazil; Da Costa, I.T., Department of Physical Education, Universidade Federal de Viçosa, Minas Gerais, Brazil; Cardoso, F.D.S.L., Department of Physical Education, Universidade Federal de Viçosa, Minas Gerais, Brazil; De Assis, J.V., Department of Physical Education, Universidade Federal de Viçosa, Minas Gerais, Brazil; Caetano, G.H.L., Department of Physical Education, Universidade Federal de Viçosa, Minas Gerais, Brazil; Cerqueira, F.R., Graduate Program in Computer Science, Universidade Federal de Viçosa, Minas Gerais, Brazil, Department of Production Engineering, Universidade Federal Fluminense, Rio de Janeiro, Brazil","This study proposes the use of a computational approach based on machine learning (ML) algorithms to build predictive models using eye tracking data. Our intention is to provide results that may support the study of medical investigation in the decision-making process in clinical bioethics, particularly in this work, in cases of euthanasia. The data used in the approach were collected from 75 students of the nursing undergraduate course using an eye tracker. The available data were processed through feature selection methods, and were later used to create models capable of predicting the euthanasia decision through ML algorithms. Statistical experiments showed that the predictive model resultant from the multilayer perceptron (MLP) algorithm led to the best performance compared with the other tested algorithms, presenting an accuracy of 90.7% and a mean area under the ROC curve of 0.90. Interesting knowledge (patterns and rules) for the studied bioethical decision-making was extracted using simulations with MLP models and inspecting the obtained decision-tree rules. The good performance shown by the obtained MLP predictive model demonstrates that the proposed investigation approach may be used to test scientific hypotheses related to visual attention and decision-making. © 2017 The Authors. Published by Elsevier B.V.","Decision-making in bioethics; Machine learning in medicine; Mobile eye tracking; Visual attention","Artificial intelligence; Behavioral research; Data mining; Decision trees; Education; Learning algorithms; Learning systems; Students; Area under the ROC curve; Bioethical decision-making; Decision making process; Feature selection methods; Mobile eye-tracking; Multi layer perceptron; Statistical experiments; Visual Attention; Decision making",Conference Paper,"Final","",Scopus,2-s2.0-85027312266
"Lin Y., Xue C., Guo Q., Zhang J., Peng N., Niu Y.","57195064831;14053333400;57195062168;56004265300;57189886548;36975929200;","The study of presentation characteristics of the warning information and its influence on user’s cognitive process based on eye tracking",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10276 LNAI",,,"86","100",,,"10.1007/978-3-319-58475-1_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025125452&doi=10.1007%2f978-3-319-58475-1_7&partnerID=40&md5=8ea426fcad690bfcccf6c2a0216caba8","School of Mechanical Engineering, Southeast University, Nanjing, 211189, China","Lin, Y., School of Mechanical Engineering, Southeast University, Nanjing, 211189, China; Xue, C., School of Mechanical Engineering, Southeast University, Nanjing, 211189, China; Guo, Q., School of Mechanical Engineering, Southeast University, Nanjing, 211189, China; Zhang, J., School of Mechanical Engineering, Southeast University, Nanjing, 211189, China; Peng, N., School of Mechanical Engineering, Southeast University, Nanjing, 211189, China; Niu, Y., School of Mechanical Engineering, Southeast University, Nanjing, 211189, China","This research has adopted eye movement technique to study the influence of warning character on the information processing, which involves in two experiments. With our study, we have made our focus of research on the warning position, warning icon and warning border as a visual stimulus means. In our investigation, we have been keeping on with such commonly-made eye-movement recording parameters, such as the Fixation Count, the First Fixation and Duration by using an Eye Link II eye tracker, which is in a position to reflect the subject’s attention and conversion of the attentions. What is more, we have done a single factor variance analysis (ANOVA) in hoping to work out the experimental data due to the kinds of eye movement parameters, and the following conclusions were drawn: (1) The warning position on the warning interface affects the machining process of the warning. When the warning is embedded in text, the warning is more noticeable and the perceived hazard level is higher. (2) Consistent with the results of the relevant studies on warnings on product labels, there are also icon effects on the warning interface, and the icon can improve the salience of the warning itself and the level of perceived danger. (3) There are also border effects on the warning interface. The appearance of the border makes the warning more significant and the perceived hazard level can be improved. The research results of this paper can also be adopted as the warning message design reference, which has had a great significance in improving the identification of warning message and reducing the rate of visual accidents on interface. © Springer International Publishing AG 2017.","Digital interface; Eye tracking; Warning characteristics; Warning effectiveness","Ergonomics; Eye movements; Hazards; Human computer interaction; Machining; Digital interfaces; Eye-tracking; Focus of researches; Movement techniques; Recording parameters; Warning characteristics; Warning effectiveness; Warning informations; Analysis of variance (ANOVA)",Conference Paper,"Final","",Scopus,2-s2.0-85025125452
"Jiang B., Ma J., Zhou D.","36975790800;57194975374;57192908690;","The application of multi-view and multi-task learning for on-board interaction design based on visual selection",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10290 LNCS",,,"79","93",,,"10.1007/978-3-319-58640-3_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024499111&doi=10.1007%2f978-3-319-58640-3_7&partnerID=40&md5=0e10e1e72209301484dc37bdd27a98aa","School of Design Arts and Media, Nanjing University of Science and Technology, 200, Xiaolingwei Street, Nanjing, Jiangsu  210094, China","Jiang, B., School of Design Arts and Media, Nanjing University of Science and Technology, 200, Xiaolingwei Street, Nanjing, Jiangsu  210094, China; Ma, J., School of Design Arts and Media, Nanjing University of Science and Technology, 200, Xiaolingwei Street, Nanjing, Jiangsu  210094, China; Zhou, D., School of Design Arts and Media, Nanjing University of Science and Technology, 200, Xiaolingwei Street, Nanjing, Jiangsu  210094, China","The core of information visualization and visual selection is the mapping from abstract data to visual structure. The aim of information visualization doesn’t lie in visualization itself, its ultimate aim is to collect information on the basis of visualization so as to offer support to decision making. Under the complex driving environment, Designers have to continue their research during the process of interface design. They can explore the implications and presentation methods of interface interaction inside the car in order to form an on-board interaction design system based on visual selection. This can also realize information sharing between cars and X (people, cars, roads and back-stage) and possess functions like strong sensation for complex environment, intelligent decision and mutual control. At the same time, on-board interaction equipment will have more diversified tasks. For example, the alternation of interaction and decision-making between multiple tasks like reality conformation, cluster display, gesture interaction, speech recognition, body sensation and eye tracking. At present, the new direction for interaction design is the analysis of multitask visual selection so as to realize secure, comfortable, energy-saving and efficient driving and finally the invention of a new generation of on-board interaction design system which can perform on human behalf. Through multi-view and multi-task learning, this paper gave an analysis of on-board interface design and concluded design scheme and suggestion with optimal user experience. By combing reasonable analysis of human intelligence and sensible interface design, this paper can provide new ways of thinking and methods for future on-board interface design. © Springer International Publishing AG 2017.","Information visualization; Interface design; Machine learning; On-board interaction design; User experience","Automobile drivers; Behavioral research; Data visualization; Decision making; Design; Education; Information analysis; Information systems; Learning systems; Speech recognition; User interfaces; Visualization; Complex environments; Driving environment; Information visualization; Intelligent decisions; Interaction design; Interface designs; Interface interaction; User experience; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85024499111
"Dorado A., Hong S., Saavedra G., Martinez-Corral M., Javidi B.","55511457500;56900172400;56210176100;7004753323;7101754586;","Integral display for non-static observers",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10219",,"1021912","","",,,"10.1117/12.2263457","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023633874&doi=10.1117%2f12.2263457&partnerID=40&md5=2ef03930f3da1f922b86149d7c84a594","Department of Optics, University of Valencia, Burjassot, E-46100, Spain; Electrical and Computer Engineering Dept., University of Connecticut, Storrs, CT  06269-2157, United States","Dorado, A., Department of Optics, University of Valencia, Burjassot, E-46100, Spain; Hong, S., Department of Optics, University of Valencia, Burjassot, E-46100, Spain; Saavedra, G., Department of Optics, University of Valencia, Burjassot, E-46100, Spain; Martinez-Corral, M., Department of Optics, University of Valencia, Burjassot, E-46100, Spain; Javidi, B., Electrical and Computer Engineering Dept., University of Connecticut, Storrs, CT  06269-2157, United States","We propose to combine the Kinect and the Integral-Imaging technologies for the implementation of Integral Display. The Kinect device permits the determination, in real time, of (x,y,z) position of the observer relative to the monitor. Due to the active condition of its IR technology, the Kinect provides the observer position even in dark environments. On the other hand, SPOC 2.0 algorithm permits to calculate microimages adapted to the observer 3D position. The smart combination of these two concepts permits the implementation, for the first time we believe, of an Integral Display that provides the observer with color 3D images of real scenes that are viewed with full parallax and which are adapted dynamically to its 3D position. © 2017 SPIE.","3D display; Eye-tracking; Integral Imaging; Kinect","Geometrical optics; Imaging systems; Imaging techniques; Visualization; 3-D displays; 3-D image; 3D positions; Active conditions; Eye-tracking; Full-parallax; Integral imaging; Kinect; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85023633874
"Yoon K.-H., Kim S.-K.","54382599400;56272760600;","Expansion method of the three-dimensional viewing freedom of autostereoscopic 3D display with dynamic merged viewing zone (MVZ) under eye tracking",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10219",,"1021914","","",,2,"10.1117/12.2264905","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023598285&doi=10.1117%2f12.2264905&partnerID=40&md5=a5f818b23e4872eaf81a884f75a54e6b","Center for Imaging Media Research, Korea Institute of Science and Technology, 5, Hwarang-ro 14-gil, Seongbuk-gu, Seoul, 02792, South Korea","Yoon, K.-H., Center for Imaging Media Research, Korea Institute of Science and Technology, 5, Hwarang-ro 14-gil, Seongbuk-gu, Seoul, 02792, South Korea; Kim, S.-K., Center for Imaging Media Research, Korea Institute of Science and Technology, 5, Hwarang-ro 14-gil, Seongbuk-gu, Seoul, 02792, South Korea","We studied expansion method of the three-dimensional viewing freedom of autostereoscopic 3D display with dynamic MVZ under tracking of viewer's eye. The dynamic MVZ technique can provide three dimensional images with minimized crosstalk when observer move at optimal viewing distance (OVD). In order to be extended to movement in the depth direction of the observer of this technology, it is provided a new pixel mapping method of the left eye and the right eye images at the time of the depth direction movement of the observer. When this pixel mapping method is applied to common autostereoscopic 3D display, the image of the 3D display as viewed from the observer position has the nonuniformed brightness distribution of a constant period in the horizontal direction depending on depth direction distance from OVD. It makes it difficult to provide a three-dimensional image of good quality to the observer who deviates from OVD. In this study, it is simulated brightness distribution formed by the proposed pixel mapping when it is moved in the depth direction away OVD and confirmed the characteristics with the captured photos of two cameras on observer position to simulated two eyes of viewer using a developed 3D display system. As a result, we found that observer can perceive 3D images of same quality as OVD position even when he moves away from it in the developed 3D display system. © 2017 SPIE.","Autostereoscopic display; Merged viewing zone (MVZ); Minimized crosstalk; Optimal viewing distance (OVD)","Crosstalk; Eye movements; Imaging systems; Luminance; Mapping; Pixels; Stereo image processing; Three dimensional computer graphics; Visualization; 3D display systems; Auto-stereoscopic display; Autostereoscopic 3D displays; Brightness distribution; Expansion methods; Three dimensional images; Viewing distance; Viewing zone; Display devices",Conference Paper,"Final","",Scopus,2-s2.0-85023598285
"Jensen R.R., Stets J.D., Suurmets S., Clement J., Aanæs H.","35107405700;57194507881;57194499001;54938534100;8846879600;","Wearable gaze trackers: Mapping visual attention in 3D",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10269 LNCS",,,"66","76",,5,"10.1007/978-3-319-59126-1_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020440302&doi=10.1007%2f978-3-319-59126-1_6&partnerID=40&md5=ff009b8af8571caab4cc9de6665cdd3e","Technical University of Denmark, Kongens Lyngby, Denmark; Copenhagen Business School, Frederiksberg, Denmark","Jensen, R.R., Technical University of Denmark, Kongens Lyngby, Denmark; Stets, J.D., Technical University of Denmark, Kongens Lyngby, Denmark; Suurmets, S., Copenhagen Business School, Frederiksberg, Denmark; Clement, J., Copenhagen Business School, Frederiksberg, Denmark; Aanæs, H., Technical University of Denmark, Kongens Lyngby, Denmark","The study of visual attention in humans relates to a wide range of areas such as: psychology, cognition, usability, and marketing. These studies have been limited to fixed setups with respondents sitting in front of a monitor mounted with a gaze tracking device. The introduction of wearable mobile gaze trackers allows respondents to move freely in any real world 3D environment, removing the previous restrictions. In this paper we propose a novel approach for processing visual attention of respondents using mobile wearable gaze trackers in a 3D environment. The pipeline consists of 3 steps: modeling the 3D area-of-interest, positioning the gaze tracker in 3D space, and 3D mapping of visual attention. The approach is general, but as a case study we created 3D heat maps of respondents visiting supermarket shelves as well as finding their in-store movement relative to these shelves. The method allows for analysis across multiple respondents and to distinguish between phases of in-store orientation (far away) and product recognition/selection (up close) based on distance to shelves. © Springer International Publishing AG 2017.",,"3D modeling; Behavioral research; Image analysis; Mapping; Wearable technology; 3-D environments; 3-D mapping; Area of interest; Gaze tracker; Gaze tracking; Heat maps; Real-world; Visual Attention; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85020440302
"Jain S., Kamath S.S.","57208446516;57216704186;","Saliency prediction for visual regions of interest with applications in advertising",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10165 LNCS",,,"48","60",,1,"10.1007/978-3-319-56687-0_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018586343&doi=10.1007%2f978-3-319-56687-0_5&partnerID=40&md5=1ae4b430430f695281f5080414dc016c","National Institute of Technology Karnataka, Surathkal, 575025, India","Jain, S., National Institute of Technology Karnataka, Surathkal, 575025, India; Kamath, S.S., National Institute of Technology Karnataka, Surathkal, 575025, India","Human visual fixations play a vital role in a plethora of genres, ranging from advertising design to human-computer interaction. Considering saliency in images thus brings significant merits to Computer Vision tasks dealing with human perception. Several classification models have been developed to incorporate various feature levels and estimate free eye-gazes. However, for real-time applications (Here, real-time applications refer to those that are time, and often resource-constrained, requiring speedy results. It does not imply on-line data analysis), the deep convolution neural networks are either difficult to deploy, given current hardware limitations or the proposed classifiers cannot effectively combine image semantics with low-level attributes. In this paper, we propose a novel neural network approach to predict human fixations, specifically aimed at advertisements. Such analysis significantly impacts the brand value and assists in audience measurement. A dataset containing 400 print ads across 21 successful brands was used to successfully evaluate the effectiveness of advertisements and their associated fixations, based on the proposed saliency prediction model. © Springer International Publishing AG 2017.","Advertising; Free eye-gaze estimation; Machine Learning; Neural networks; Support Vector Machines; Visual saliency","Deep neural networks; Face recognition; Forecasting; Learning systems; Marketing; Neural networks; Semantics; Support vector machines; Classification models; Convolution neural network; Eye-gaze; Low-level attributes; Novel neural network; Real-time application; Regions of interest; Visual saliency; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85018586343
"Szymański A., Szlufik S., Koziorowski D.M., Przybyszewski A.W.","56304189900;55334567200;7801382272;6603763540;","Building classifiers for parkinson’s disease using new eye tribe tracking method",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10192 LNAI",,,"351","358",,2,"10.1007/978-3-319-54430-4_34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018472828&doi=10.1007%2f978-3-319-54430-4_34&partnerID=40&md5=03e84e70b315c1a9fdafade42552eab5","Polish-Japanese Academy of Information Technology, Koszykowa 86, Warsaw, 02-008, Poland; Faculty of Health Science, Department of Neurology, Medical University of Warsaw, Warsaw, Poland","Szymański, A., Polish-Japanese Academy of Information Technology, Koszykowa 86, Warsaw, 02-008, Poland; Szlufik, S., Faculty of Health Science, Department of Neurology, Medical University of Warsaw, Warsaw, Poland; Koziorowski, D.M., Faculty of Health Science, Department of Neurology, Medical University of Warsaw, Warsaw, Poland; Przybyszewski, A.W., Polish-Japanese Academy of Information Technology, Koszykowa 86, Warsaw, 02-008, Poland","Parkinson Disease (PD) is the second major neurodegenerative disease, which causes severe complications for patients’ daily life. PD remains unspecified in many aspects including best treatment, prediction of its progression and precise diagnosis. In our study we have built machine learning (ML) models, which address some of those issues by helping to improve symptom evaluation precision by using advanced biomarkers such as fast eye movements. We have built and compared model accuracy relaying on data from two systems for recording eye movements: one is saccadometer (Ober Consulting), and another is based on the Eye Tribe (ET1000). We have reached 85% accuracy in prediction of neurologic attributes based on ET and 82% accuracy with saccadometer with help of rough set theory. The purpose of this study was to compare ET with clinically approved eye movement measurements saccadometer of Ober. We have demonstrated in 8 PD patients that both systems gave comparable results based on neurological and eye movement measurements attributes. © Springer International Publishing AG 2017.","Data mining; Eye tracking; Parkinson disease; Rough set theory","Data mining; Database systems; Diagnosis; Learning systems; Neurodegenerative diseases; Rough set theory; Set theory; Daily lives; Eye movement measurement; Eye-tracking; Model accuracy; Parkinson disease; Tracking method; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85018472828
"Saikh T., Das D., Bandyopadhayay S.","55081932600;56965150200;57194040959;","Identifying and pruning features for classifying translated and post-edited gaze durations",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10089 LNAI",,,"119","131",,,"10.1007/978-3-319-58130-9_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018389770&doi=10.1007%2f978-3-319-58130-9_12&partnerID=40&md5=9836e0344b4acedb5f593bf20cb170d1","Department of Computer Science and Engineering, Jadavpur University, Kolkata, India","Saikh, T., Department of Computer Science and Engineering, Jadavpur University, Kolkata, India; Das, D., Department of Computer Science and Engineering, Jadavpur University, Kolkata, India; Bandyopadhayay, S., Department of Computer Science and Engineering, Jadavpur University, Kolkata, India","The present paper reports on various experiments carried out to classify the source and target gaze fixation durations on an eye tracking dataset, namely Translation Process Research (TPR). Different features were extracted from both the source and target parts of the TPR dataset, separately and different models were developed separately by employing such features using a machine learning framework. These models were trained using Support Vector Machine (SVM) and the best accuracy of 49.01% and 59.78% were obtained with respect to cross validation for source and target gaze fixation durations, respectively. The experiments were also carried out on the post edited data set using same experimental set up and the highest accuracy of 71.70% was obtained. Finally, Information Gain based pruning has been performed in order to select the best features that are useful for classifying the gaze durations. © 2017, Springer International Publishing AG.","Eye tracking; Gaze fixation duration; Information gain; Post-editing; Translation","Learning systems; Support vector machines; Target tracking; Translation (languages); Cross validation; Data set; Experimental set up; Eye-tracking; Fixation duration; Information gain; Post-editing; Translation process; Classification (of information)",Conference Paper,"Final","",Scopus,2-s2.0-85018389770
"Coutinho V.D.A., Cintra R.J., Bayer F.M.","57190883380;55930266600;37114122300;","Low-complexity multidimensional DCT approximations for high-order tensor data decorrelation",2017,"IEEE Transactions on Image Processing","26","5",,"2296","2310",,9,"10.1109/TIP.2017.2679442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017454851&doi=10.1109%2fTIP.2017.2679442&partnerID=40&md5=207c264d1bd852de3d16c96a7860340e","Graduate Program in Electrical Engineering, Signal Processing Group, Departamento de Estatística, Universidade Federal de Pernambuco, Recife, 50670-901, Brazil; Signal Processing Group, Departamento de Estatística, Universidade Federal de Pernambuco, Recife, 50670-901, Brazil; Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB  T2N 1N4, Canada; Laboratory of Space Sciences of Santa Maria, Departamento de Estatística, Universidade Federal de Santa Maria, Santa Maria, 97105900, Brazil","Coutinho, V.D.A., Graduate Program in Electrical Engineering, Signal Processing Group, Departamento de Estatística, Universidade Federal de Pernambuco, Recife, 50670-901, Brazil; Cintra, R.J., Signal Processing Group, Departamento de Estatística, Universidade Federal de Pernambuco, Recife, 50670-901, Brazil, Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB  T2N 1N4, Canada; Bayer, F.M., Laboratory of Space Sciences of Santa Maria, Departamento de Estatística, Universidade Federal de Santa Maria, Santa Maria, 97105900, Brazil","In this paper, we introduce low-complexity multidimensional discrete cosine transform (DCT) approximations. 3D DCT approximations are formalized in terms of high-order tensor theory. The formulation is extended to higher dimensions with arbitrary lengths. Several multiplierless 8x8x8 approximate methods are proposed and the computational complexity is discussed for the general multidimensional case. The proposed methods complexity cost was assessed, presenting considerably lower arithmetic operations when compared with the exact 3D DCT. The proposed approximations were embedded into 3D DCT-based video coding scheme and a modified quantization step was introduced. The simulation results showed that the approximate 3D DCT coding methods offer almost identical output visual quality when compared with exact 3D DCT scheme. The proposed 3D approximations were also employed as a tool for visual tracking. The approximate 3D DCT-based proposed system performs similarly to the original exact 3D DCT-based method. In general, the suggested methods showed competitive performance at a considerably lower computational cost. © 2017 IEEE.","3D DCT; 3D video compression; Approximate DCT; DCT approximation; Image compression; Multidimensional DCT; Three dimensional DCT; Visual tracking","Discrete cosine transforms; Image coding; Tensors; Tracking (position); Video signal processing; 3-D videos; 3D DCT; Approximate DCT; DCT approximation; Multidimensional DCT; Visual Tracking; Image compression; arithmetic; article; eye tracking; simulation; videorecording",Article,"Final","",Scopus,2-s2.0-85017454851
"Li X.H., Rötting M., Wang W.S.","57193838842;6506942608;56070077300;","Bayesian network-based identification of driver lane-changing intents using eye tracking and vehicle-based data",2017,"Advanced Vehicle Control AVECÃÂ¢Ã¢âÂ¬Ã¢âÂ¢16 - Proceedings of the 13th International Symposium on Advanced Vehicle Control AVECÃÂ¢Ã¢âÂ¬Ã¢âÂ¢16",,,,"299","304",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017030072&partnerID=40&md5=20bb3506501ec7b5e61adf44b25598d6","Technische Universität Berlin, Germany; Department of Mechanical Engineering, Beijing Institute of Technology, Beijing, China; Department of Mechanical Engineering, University of California BerkeleyCA, United States","Li, X.H., Technische Universität Berlin, Germany; Rötting, M., Technische Universität Berlin, Germany; Wang, W.S., Department of Mechanical Engineering, Beijing Institute of Technology, Beijing, China, Department of Mechanical Engineering, University of California BerkeleyCA, United States","A Bayesian network decision-making method is proposed by combining driver’s eye-tracking data and vehicle-based data together to identify driver lane-changing intents. First, experiments are conducted in a driving simulator with eye-tracker device to obtain the data when a subject driver makes lane-changing maneuvers. Second, collected data are analyzed in machine learning method using Bayesian decision-making approach to predict driver’s lane-changing intents. Last, to show the benefits of our proposed method, comparison experiments are made between the data fusion way and only using eye tracking data or vehicle-based data. The results show that the Bayesian network with data fusion method performs better than using single information to recognize driver’s lane-changing intents. At the same time, thresholds of Lane-changing probability and vehicle-based data as restricting condition choosing work is discussed in order to select the best identification parameter. © 2017 Taylor & Francis Group, London.",,"Advanced vehicle control systems; Bayesian networks; Control system synthesis; Data fusion; Decision making; Learning systems; Vehicles; Bayesian decision makings; Data fusion methods; Decision-making method; Driving simulator; Identification parameters; Lane changing maneuver; Lane changing probability; Machine learning methods; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85017030072
"Hsieh Y.-Y., Liu C.-C., Wang W.-L., Chuang J.-H.","56683898000;57193719622;57193720666;57154887400;","Investigating size personalization for more accurate eye tracking glasses",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10118 LNCS",,,"239","248",,,"10.1007/978-3-319-54526-4_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016106734&doi=10.1007%2f978-3-319-54526-4_18&partnerID=40&md5=7d2526f7c442635e3ba3d88c57c42100","National Chiao Tung University, Hsinchu, Taiwan","Hsieh, Y.-Y., National Chiao Tung University, Hsinchu, Taiwan; Liu, C.-C., National Chiao Tung University, Hsinchu, Taiwan; Wang, W.-L., National Chiao Tung University, Hsinchu, Taiwan; Chuang, J.-H., National Chiao Tung University, Hsinchu, Taiwan","Personalized eyewear frame could improve the accuracy of eye tracking. To obtain the personalized frame size (temple length), we propose a new measuring instrument that consists of (i) the hardware, a 3D printed trial frame which has marks but no scales, and (ii) the software, a vision-based measurement which is view invariant. The vision-based measurement has accuracy and precision that are both 0.02 cm, while the trial frame can achieve a precision of 0.17 cm for secure wearing. Moreover, dispersion up to 2.56 cm is obtained among the personalized frame sizes for just a fairly small group of users, indicating the importance of having such a personalized measurement system. © Springer International Publishing AG 2017.",,"3D printers; Accuracy and precision; Eye-tracking; Frame size; Measuring instruments; Personalizations; Personalized measurements; View invariants; Vision-based measurements; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85016106734
"Kacete A., Séguier R., Collobert M., Royan J.","57190134112;6505976299;6506362460;23101034500;","Unconstrained gaze estimation using random forest regression voting",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10113 LNCS",,,"419","432",,4,"10.1007/978-3-319-54187-7_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016032758&doi=10.1007%2f978-3-319-54187-7_28&partnerID=40&md5=be1345d30c3171f7af986bb93aff4310","Institute of Research and Technology B-com, Cesson-Sévigné, France","Kacete, A., Institute of Research and Technology B-com, Cesson-Sévigné, France; Séguier, R., Institute of Research and Technology B-com, Cesson-Sévigné, France; Collobert, M., Institute of Research and Technology B-com, Cesson-Sévigné, France; Royan, J., Institute of Research and Technology B-com, Cesson-Sévigné, France","In this paper we address the problem of automatic gaze estimation using a depth sensor under unconstrained head pose motion and large user-sensor distances. To achieve robustness, we formulate this problem as a regression problem. To solve the task in hand, we propose to use a regression forest according to their high ability of generalization by handling large training set. We train our trees on an important synthetic training data using a statistical model of the human face with an integrated parametric 3D eyeballs. Unlike previous works relying on learning the mapping function using only RGB cues represented by the eye image appearances, we propose to integrate the depth information around the face to build the input vector. In our experiments, we show that our approach can handle real data scenarios presenting strong head pose changes even though it is trained only on synthetic data, we illustrate also the importance of the depth information on the accuracy of the estimation especially in unconstrained scenarios. © Springer International Publishing AG 2017",,"Computer vision; Decision trees; Regression analysis; Depth information; Gaze estimation; Mapping functions; Regression forests; Regression problem; Statistical modeling; Synthetic data; Synthetic training data; Problem solving",Conference Paper,"Final","",Scopus,2-s2.0-85016032758
"Kacete A., Séguier R., Collobert M., Royan J.","57190134112;6505976299;6506362460;23101034500;","Head pose free 3D gaze estimation using RGB-D camera",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10225",,"102251S","","",,2,"10.1117/12.2266091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015075842&doi=10.1117%2f12.2266091&partnerID=40&md5=e92db534544a4514f98190ded7bb7949","Institute of Research and Technology B-com, France","Kacete, A., Institute of Research and Technology B-com, France; Séguier, R., Institute of Research and Technology B-com, France; Collobert, M., Institute of Research and Technology B-com, France; Royan, J., Institute of Research and Technology B-com, France","In this paper, we propose an approach for 3D gaze estimation under head pose variation using RGB-D camera. Our method uses a 3D eye model to determine the 3D optical axis and infer the 3D visual axis. For this, we estimate robustly user head pose parameters and eye pupil locations with an ensembles of randomized trees trained with an important annotated training sets. After projecting eye pupil locations in the sensor coordinate system using the sensor intrinsic parameters and a one-time simple calibration by gazing a known 3D target under different directions, the 3D eyeball centers are determined for a specific user for both eyes yielding the determination of the visual axis. Experimental results demonstrate that our method shows a good gaze estimation accuracy even if the environment is highly unconstrained namely large user-sensor distances (> 1m50) unlike state-of-the-art methods which deal with relatively small distances (<1m). © 2017 SPIE.","3D eyeball; Gaze estimation; head pose estimation; pupil localization; Random Forest; RGB-D camera","Cameras; Decision trees; Image recognition; Optical data processing; 3D eyeball; Gaze estimation; Head Pose Estimation; Pupil localization; Random forests; Rgb-d cameras; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85015075842
"Wibirama S., Mahesa R.R., Nugroho H.A., Hamamoto K.","26654457700;57202034597;57210591699;7102699225;","Estimating 3D gaze in physical environment: A geometric approach on consumer-level remote eye tracker",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10225",,"102251H","","",,,"10.1117/12.2266109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015050734&doi=10.1117%2f12.2266109&partnerID=40&md5=08325d07569b8d786a49d5deb925bafe","Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Department of Information Media Technology, School of Information and Telecommunication Engineering, Tokai University, Tokyo, 108-8619, Japan","Wibirama, S.; Mahesa, R.R.; Nugroho, H.A., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Hamamoto, K., Department of Information Media Technology, School of Information and Telecommunication Engineering, Tokai University, Tokyo, 108-8619, Japan","Remote eye trackers with consumer price have been used for various applications on flat computer screen. On the other hand, 3D gaze tracking in physical environment has been useful for visualizing gaze behavior, robots controller, and assistive technology. Instead of using affordable remote eye trackers, 3D gaze tracking in physical environment has been performed using corporate-level head mounted eye trackers, limiting its practical usage to niche user. In this research, we propose a novel method to estimate 3D gaze using consumer-level remote eye tracker. We implement geometric approach to obtain 3D point of gaze from binocular lines-of-sight. Experimental results show that the proposed method yielded low errors of 3.47±3.02 cm, 3.02±1.34 cm, and 2.57±1.85 cm in X, Y, and Z dimensions, respectively. The proposed approach may be used as a starting point for designing interaction method in 3D physical environment. © 2017 SPIE.","3D gaze tracking; eye tracking; gaze visualization; geometric estimation","Eye movements; Geometry; Image processing; Tracking (position); Assistive technology; Computer screens; Corporate level; Eye-tracking; Gaze tracking; Geometric approaches; Interaction methods; Physical environments; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85015050734
"Sharma S., Chakravarthy B.K.","57193515233;57091351000;","Exploring visual response on form features of the autorickshaw",2017,"Smart Innovation, Systems and Technologies","65",,,"273","282",,,"10.1007/978-981-10-3518-0_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014496244&doi=10.1007%2f978-981-10-3518-0_24&partnerID=40&md5=f10a0f3b3bb7d2aaf5f198142a4653b2","School of Design, Indian Institute of Technology, Powai, Mumbai, India","Sharma, S., School of Design, Indian Institute of Technology, Powai, Mumbai, India; Chakravarthy, B.K., School of Design, Indian Institute of Technology, Powai, Mumbai, India","Styling plays an important role in automotive appearances to attract visual attention. Features such as the headlights and windscreen become the prime carriers of style that can evoke visual interest. These features involve major to fine refinements in form as well as their compositional arrangements while designing. The feature that evokes high visual interest in isolation, or whether the combinations of features sustain visual interest for a long time, can be valuable knowledge for the designer and stakeholder teams to make visual decisions on form especially during early stages of designing. Present study deals with feature level observations of Autorickshaw concepts as Areas of Interest (AOI) to assess attention distribution towards the form features during concept development stage. The data is obtained as visual response of Product Designers (PD), Automobile Designers (AD), Autorickshaw Drivers (DR), and Passengers (PS) through eye tracking. The investigation analyzes how visual attention can be driven by features in isolation or in combination within larger units for four concept representations of the Autorickshaw. The effect of variations in visual-formal characteristics of the designed form is analyzed conjointly with by assessing the dwell time (DT) on each feature through a demarcation of specific AOIs, as well as a gridded analysis of the spatial distribution of visual attention on specific features. The findings indicate that the concept with curved features that are arranged in proximity, evoke high visual interest; instead of a concept, that supports sharp angular non-proximal features, implying that proximity and curved features can have strong combining effect to evoke and sustain visual interest in Autorickshaw styling. Overall, it was observed that the spatial distribution of attention converges, getting focused on the Cowl and the Headlights during style scrutiny. Increase in proximity of features results in an increased dwell time, thereby causing an active visual scrutiny that sustains visual interest. A combination of strategically placed proximal features, accentuates visual interest and the perception of style in observers. When available at the early stages of designing such inferential visual response feedback as objective knowledge can help the designers to anticipate sustained visual interest in a designed object, and help predict product acceptance, thereby curtailing potential market risks. © Springer Nature Singapore Pte Ltd. 2017.","Visual attention; Visual feature analysis; Visual response","Eye movements; Headlights; Product design; Spatial distribution; Concept development; Feature level; Potential markets; Product designers; Visual Attention; Visual feature; Visual interest; Visual response; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85014496244
"Xu M., Jiang L., Sun X., Ye Z., Wang Z.","55703599800;57188642646;56566919900;57188644828;24170127500;","Learning to Detect Video Saliency with HEVC Features",2017,"IEEE Transactions on Image Processing","26","1","7742914","369","385",,44,"10.1109/TIP.2016.2628583","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013498779&doi=10.1109%2fTIP.2016.2628583&partnerID=40&md5=4c661bdd6b634edfbd058e04d5809dc1","School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; Microsoft Research Asia, Beijing, 100080, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, 430079, China","Xu, M., School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; Jiang, L., School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; Sun, X., Microsoft Research Asia, Beijing, 100080, China; Ye, Z., School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China; Wang, Z., School of Electronic and Information Engineering, Beihang University, Beijing, 100191, China, Collaborative Innovation Center of Geospatial Technology, Wuhan, 430079, China","Saliency detection has been widely studied to predict human fixations, with various applications in computer vision and image processing. For saliency detection, we argue in this paper that the state-of-the-art High Efficiency Video Coding (HEVC) standard can be used to generate the useful features in compressed domain. Therefore, this paper proposes to learn the video saliency model, with regard to HEVC features. First, we establish an eye tracking database for video saliency detection, which can be downloaded from https://github.com/remega/video-database. Through the statistical analysis on our eye tracking database, we find out that human fixations tend to fall into the regions with large-valued HEVC features on splitting depth, bit allocation, and motion vector (MV). In addition, three observations are obtained with the further analysis on our eye tracking database. Accordingly, several features in HEVC domain are proposed on the basis of splitting depth, bit allocation, and MV. Next, a kind of support vector machine is learned to integrate those HEVC features together, for video saliency detection. Since almost all video data are stored in the compressed form, our method is able to avoid both the computational cost on decoding and the storage cost on raw data. More importantly, experimental results show that the proposed method is superior to other state-of-the-art saliency detection methods, either in compressed or uncompressed domain. © 1992-2012 IEEE.","compressed domain; HEVC; machine learning; Saliency detection; SVM","Database systems; Digital storage; Eye movements; Image processing; Learning systems; Object recognition; Support vector machines; Bit allocation; Compressed domain; Computational costs; HEVC; High-efficiency video coding; Saliency detection; State of the art; Video saliencies; Feature extraction",Article,"Final","",Scopus,2-s2.0-85013498779
"Ma B., Jain E., Entezari A.","56441976800;36715118000;8393431700;","3D saliency from eye tracking with tomography",2017,"Mathematics and Visualization",,,,"185","198",,1,"10.1007/978-3-319-47024-5_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012306150&doi=10.1007%2f978-3-319-47024-5_11&partnerID=40&md5=4b3fbe26a129f14db689c03624811a05","University of Florida, Gainesville, FL  32611, United States","Ma, B., University of Florida, Gainesville, FL  32611, United States; Jain, E., University of Florida, Gainesville, FL  32611, United States; Entezari, A., University of Florida, Gainesville, FL  32611, United States","This paper presents a method to build a saliency map in a volumetric dataset using 3D eye tracking. Our approach acquires the saliency information from multiple views of a 3D dataset with an eye tracker and constructs the 3D saliency volume from the gathered 2D saliency information using a tomographic reconstruction algorithm. Our experiments, on a number of datasets, show the effectiveness of our approach in identifying salient 3D features that attract user’s attention. The obtained 3D saliency volume provides importance information and can be used in various applications such as illustrative visualization. © Springer International Publishing AG 2017.",,"Tomography; Visualization; Eye trackers; Eye-tracking; Illustrative visualization; Multiple views; Saliency map; Tomographic reconstruction algorithms; Volumetric dataset; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85012306150
"Wang X., Lindlbauer D., Lessig C., Alexa M.","57190735109;55841358000;23987498200;7003588954;","Accuracy of monocular gaze tracking on 3D geometry",2017,"Mathematics and Visualization",,,,"169","184",,4,"10.1007/978-3-319-47024-5_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012299200&doi=10.1007%2f978-3-319-47024-5_10&partnerID=40&md5=e04bef0cf72301caf8b9482db08933a1","TU Berlin, Berlin, Germany","Wang, X., TU Berlin, Berlin, Germany; Lindlbauer, D., TU Berlin, Berlin, Germany; Lessig, C., TU Berlin, Berlin, Germany; Alexa, M., TU Berlin, Berlin, Germany","Many applications such as data visualization or object recognition benefit from accurate knowledge of where a person is looking at. We present a system for accurately tracking gaze positions on a three dimensional object using a monocular head mounted eye tracker. We accomplish this by (1) using digital manufacturing to create stimuli whose geometry is know to high accuracy, (2) embedding fiducial markers into the manufactured objects to reliably estimate the rigid transformation of the object, and, (3) using a perspective model to relate pupil positions to 3D locations. This combination enables the efficient and accurate computation of gaze position on an object from measured pupil positions. We validate the of our system experimentally, achieving an angular resolution of 0.8° and a 1.5 % depth error using a simple calibration procedure with 11 points. © Springer International Publishing AG 2017.",,"Object recognition; Tracking (position); Visualization; Accurate computations; Angular resolution; Calibration procedure; Digital manufacturing; Fiducial marker; Perspective models; Rigid transformations; Three-dimensional object; Data visualization",Conference Paper,"Final","",Scopus,2-s2.0-85012299200
"Song H., Lee J., Kim T.J., Lee K.H., Kim B., Seo J.","36141903000;55894798200;56554316700;57203464195;23389723000;35303195300;","GazeDx: Interactive Visual Analytics Framework for Comparative Gaze Analysis with Volumetric Medical Images",2017,"IEEE Transactions on Visualization and Computer Graphics","23","1","7539334","311","320",,7,"10.1109/TVCG.2016.2598796","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998814343&doi=10.1109%2fTVCG.2016.2598796&partnerID=40&md5=46d476cb662a1e89d6f13aa6340c6ca4","Seoul National University, Bundang Hospital, South Korea; Soongsil University, South Korea; Samsung Medical Center, South Korea; Hankuk University of Foreign Studies, South Korea","Song, H., Seoul National University, Bundang Hospital, South Korea; Lee, J., Soongsil University, South Korea; Kim, T.J., Samsung Medical Center, South Korea; Lee, K.H., Seoul National University, Bundang Hospital, South Korea; Kim, B., Hankuk University of Foreign Studies, South Korea; Seo, J., Seoul National University, Bundang Hospital, South Korea","We present an interactive visual analytics framework, GazeDx (abbr. of GazeDiagnosis), for the comparative analysis of gaze data from multiple readers examining volumetric images while integrating important contextual information with the gaze data. Gaze pattern comparison is essential to understanding how radiologists examine medical images, and to identifying factors influencing the examination. Most prior work depended upon comparisons with manually juxtaposed static images of gaze tracking results. Comparative gaze analysis with volumetric images is more challenging due to the additional cognitive load on 3D perception. A recent study proposed a visualization design based on direct volume rendering (DVR) for visualizing gaze patterns in volumetric images; however, effective and comprehensive gaze pattern comparison is still challenging due to a lack of interactive visualization tools for comparative gaze analysis. We take the challenge with GazeDx while integrating crucial contextual information such as pupil size and windowing into the analysis process for more in-depth and ecologically valid findings. Among the interactive visualization components in GazeDx, a context-embedded interactive scatterplot is especially designed to help users examine abstract gaze data in diverse contexts by embedding medical imaging representations well known to radiologists in it. We present the results from two case studies with two experienced radiologists, where they compared the gaze patterns of 14 radiologists reading two patients' volumetric CT images. © 2016 IEEE.","context-embedded interactive scatterplot; Eye tracking; gaze pattern comparison; gaze visualization; interactive temporal chart; volumetric medical images","Computerized tomography; Data visualization; Image analysis; Medical imaging; Visualization; Volume rendering; Contextual information; Direct volume rendering; gaze pattern comparison; interactive temporal chart; Interactive visualization tool; Interactive visualizations; Scatter plots; Visualization designs; Eye tracking; abdominal radiography; algorithm; computer interface; eye movement; human; medical informatics; physiology; procedures; radiologist; thorax radiography; three dimensional imaging; x-ray computed tomography; Algorithms; Eye Movements; Humans; Imaging, Three-Dimensional; Medical Informatics; Radiography, Abdominal; Radiography, Thoracic; Radiologists; Tomography, X-Ray Computed; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84998814343
"Bellanca J.L., Orr T.J., Helfrich W., Macdonald B., Navoyski J., Eiter B.","55549245600;7006708119;57196800528;57055114200;57190133266;55957595000;","Assessing Hazard identification in surface stone mines in a virtual environment",2017,"Advances in Intelligent Systems and Computing","481",,,"217","230",,9,"10.1007/978-3-319-41627-4_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986232827&doi=10.1007%2f978-3-319-41627-4_20&partnerID=40&md5=c6a4dda17cf5328a4e2f43775d28d54e","Pittsburgh Mining Research Division, National Institute for Occupational Safety and Health, Centers for Disease Control and Prevention, Pittsburgh, PA, United States","Bellanca, J.L., Pittsburgh Mining Research Division, National Institute for Occupational Safety and Health, Centers for Disease Control and Prevention, Pittsburgh, PA, United States; Orr, T.J., Pittsburgh Mining Research Division, National Institute for Occupational Safety and Health, Centers for Disease Control and Prevention, Pittsburgh, PA, United States; Helfrich, W., Pittsburgh Mining Research Division, National Institute for Occupational Safety and Health, Centers for Disease Control and Prevention, Pittsburgh, PA, United States; Macdonald, B., Pittsburgh Mining Research Division, National Institute for Occupational Safety and Health, Centers for Disease Control and Prevention, Pittsburgh, PA, United States; Navoyski, J., Pittsburgh Mining Research Division, National Institute for Occupational Safety and Health, Centers for Disease Control and Prevention, Pittsburgh, PA, United States; Eiter, B., Pittsburgh Mining Research Division, National Institute for Occupational Safety and Health, Centers for Disease Control and Prevention, Pittsburgh, PA, United States","Mine workers are expected to remain vigilant and successfully identify and mitigate hazards in both routine and non-routine locations. The goal of the current research project is to better understand how workers search and identify hazards. NIOSH researchers developed a data collection setup to measure a subject’s gaze, head position, and reaction time while examining 360° 2D-panoramic images at a surface mine. The data is integrated in semi real-time to determine region of interest (ROI) hit accuracy for hazards within the images. The purpose of this paper is to discuss the development and implementation of the hardware and software. The following aspects of the setup will be explored in the paper: (1) environment selection, (2) image creation, (3) stimulus display, (4) synchronization, (5) gaze mapping, and (6) region of interest (ROI) hit calculation. © Springer International Publishing Switzerland 2017.","Eye tracking; Hazard recognition; Virtual reality","Human reaction time; Image segmentation; Virtual reality; Data collection; Environment selection; Eye-tracking; Hardware and software; Hazard identification; Image creation; Panoramic images; Region of interest; Hazards",Conference Paper,"Final","",Scopus,2-s2.0-84986232827
"Qi F., Zhao D., Liu S., Fan X.","57212485274;7403490261;7409458485;54920111800;","3D visual saliency detection model with generated disparity map",2017,"Multimedia Tools and Applications","76","2",,"3087","3103",,15,"10.1007/s11042-015-3229-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955600590&doi=10.1007%2fs11042-015-3229-6&partnerID=40&md5=d9297e4504c6e687c9eb20a0bf7dc00b","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China","Qi, F., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; Zhao, D., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; Liu, S., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China; Fan, X., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, 150001, China","Due to the remarkable distinction between human monocular vision and binocular vision, stereoscopic visual attention becomes an emerging question in the study of 3D applications. Some of existing 3D visual saliency detection models take advantage of ground-truth disparity map to compute center-surround differences of the depth features with high computational cost. In some 3D applications, the ground-truth disparity map may not be always available. In this paper, an efficient and simple 3D visual saliency detection model is proposed without using ground-truth disparity map. The proposed model is based on a band-pass filtering method which coincides with the visual perceptual process in human visual system. Firstly, the monocular luminance, color and texture features are extracted from the left view’s image; the binocular depth feature is extracted from the two views’ disparity map. Then, all the feature maps are filtered to generate three types of saliency maps, i.e., 2D saliency map, texture saliency map and depth saliency map. Subsequently, the three saliency maps are fused to one 3D saliency map by a linear pooling strategy. Finally, the final 3D visual saliency map is enhanced by the center-bias factor. Experimental results on a public eye tracking database show that the proposed model achieves better detection performance with low computational cost among the existing 3D visual saliency detection models. © 2016, Springer Science+Business Media New York.","3D visual attention; Binocular vision; Depth perception; Saliency fusion","Behavioral research; Binocular vision; Bins; Depth perception; Image processing; Stereo image processing; Visualization; Band pass filtering; Center-surround differences; Color and texture features; Computational costs; Detection performance; Human Visual System; Visual Attention; Visual saliency detections; Image segmentation",Article,"Final","",Scopus,2-s2.0-84955600590
"Guo Z., Zhou Q., Liu Z.","56304391000;9632964600;55553990400;","Appearance-based gaze estimation under slight head motion",2017,"Multimedia Tools and Applications","76","2",,"2203","2222",,11,"10.1007/s11042-015-3182-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953392336&doi=10.1007%2fs11042-015-3182-4&partnerID=40&md5=4f6cc1b24fc3f19e41c8ef3a51d3ba6a","School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China","Guo, Z., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Zhou, Q., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Liu, Z., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China","At present a lot of gaze estimation methods can get accurate result under ideal conditions, but some practical issues are still the biggest challenges affect the accuracy such as head motion and eye blinking. Improving the accuracy of gaze estimation and the tolerance of head motion are common tasks in the field of gaze estimation. Therefore, this paper aims to propose an accurate gaze estimation method without fixed head pose. The core problem is how to build the mapping relationship between image features and gaze position, and how to resist the head motion through the training samples. To this end, at first, a new input feature, which can well reflect the change of eye image features with different gaze positions, is proposed and it is based on appearance feature and distance feature. So the number of training samples in the process of calibration is significantly reduced. Then ℓ1-optimization is used to select an optimal set, which represents the mapping relationship between input feature and gaze position. At last, a linear equation is fitted to correct the initial estimation bias which is brought by head motion. In this paper, the experimental results demonstrate that our system achieves accurate result with one camera and a small number of calibration points. The accuracy of final gaze estimation is improved by 22 % through compensation equation. In addition, our system is robustness to eye blink and distance change. © 2016, Springer Science+Business Media New York.","Blink recognition; Compensation equation; Feature extraction; Gaze estimation","Feature extraction; Mapping; Sampling; Appearance based; Blink recognition; Calibration points; Distance feature; Gaze estimation; Initial estimation; Mapping relationships; Practical issues; Calibration",Article,"Final","",Scopus,2-s2.0-84953392336
"Zhuang B., Wang L., Lu H.","56085035700;57142763100;8218163400;","Visual tracking via shallow and deep collaborative model",2016,"Neurocomputing","218",,,"61","71",,16,"10.1016/j.neucom.2016.08.070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994181938&doi=10.1016%2fj.neucom.2016.08.070&partnerID=40&md5=42bc0bd9d7c1d8ad783c977543bfeab8","School of Information and Communication Engineering, Dalian University of Technology, Dalian, 116023, China","Zhuang, B., School of Information and Communication Engineering, Dalian University of Technology, Dalian, 116023, China; Wang, L., School of Information and Communication Engineering, Dalian University of Technology, Dalian, 116023, China; Lu, H., School of Information and Communication Engineering, Dalian University of Technology, Dalian, 116023, China","In this paper, we propose a robust tracking method based on the collaboration of a generative model and a discriminative classifier, where features are learned by shallow and deep architectures, respectively. For the generative model, we introduce a block-based incremental learning scheme, in which a local binary mask is constructed to deal with occlusion. The similarity degrees between the local patches and their corresponding subspace are integrated to formulate a more accurate global appearance model. In the discriminative model, we exploit the advances of deep learning architectures to learn generic features which are robust to both background clutters and foreground appearance variations. To this end, we first construct a discriminative training set from auxiliary video sequences. A deep classification neural network is then trained offline on this training set. Through online fine-tuning, both the hierarchical feature extractor and the classifier can be adapted to the appearance change of the target for effective online tracking. The collaboration of these two models achieves a good balance in handling occlusion and target appearance change, which are two contradictory challenging factors in visual tracking. Both quantitative and qualitative evaluations against several state-of-the-art algorithms on challenging image sequences demonstrate the accuracy and the robustness of the proposed tracker. © 2016 Elsevier B.V.","Collaborative tracking; Deep learning; Shallow feature learning; Visual tracking","Distributed computer systems; Network architecture; Target tracking; Tracking (position); Collaborative tracking; Deep learning; Discriminative classifiers; Discriminative training; Feature learning; Qualitative evaluations; State-of-the-art algorithms; Visual Tracking; Classification (of information); Article; artificial neural network; classification; collaborative tracking; controlled study; deep learning; discrimination learning; eye tracking; female; human; image analysis; learning algorithm; measurement accuracy; online system; priority journal; qualitative analysis; quantitative analysis; shallow feature learning",Article,"Final","",Scopus,2-s2.0-84994181938
"Jeni L.A., Cohn J.F.","24474645300;55423536000;","Person-Independent 3D Gaze Estimation Using Face Frontalization",2016,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,,"7789594","792","800",,17,"10.1109/CVPRW.2016.104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010223613&doi=10.1109%2fCVPRW.2016.104&partnerID=40&md5=47c9218fb3853c66a2cbf47dbd9c58d9","Carnegie Mellon University, Pittsburgh, PA, United States; University of Pittsburgh, Pittsburgh, PA, United States","Jeni, L.A., Carnegie Mellon University, Pittsburgh, PA, United States; Cohn, J.F., University of Pittsburgh, Pittsburgh, PA, United States","Person-independent and pose-invariant estimation of eye-gaze is important for situation analysis and for automated video annotation. We propose a fast cascade regression based method that first estimates the location of a dense set of markers and their visibility, then reconstructs face shape by fitting a part-based 3D model. Next, the reconstructed 3D shape is used to estimate a canonical view of the eyes for 3D gaze estimation. The model operates in a feature space that naturally encodes local ordinal properties of pixel intensities leading to photometric invariant estimation of gaze. To evaluate the algorithm in comparison with alternative approaches, three publicly-available databases were used, Boston University Head Tracking, Multi-View Gaze and CAVE Gaze datasets. Precision for head pose and gaze averaged 4 degrees or less for pitch, yaw, and roll. The algorithm outperformed alternative methods in both datasets. © 2016 IEEE.",,"Computer vision; Pattern recognition; Automated video; Boston University; Gaze estimation; Person-independent; Photometric invariants; Pixel intensities; Pose invariant; Situation analysis; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85010223613
"Jones A., Nagano K., Busch J., Yu X., Peng H.-Y., Barreto J., Alexander O., Bolas M., Debevec P., Unger J.","55977012100;55825559100;35100013700;35101222400;56323328700;57210102530;35733239900;6603846097;7003927551;16426648500;","Time-Offset Conversations on a Life-Sized Automultiscopic Projector Array",2016,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,,"7789610","927","935",,4,"10.1109/CVPRW.2016.120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010222343&doi=10.1109%2fCVPRW.2016.120&partnerID=40&md5=d2c6b7866643a8bad924e77dceaa5959","USC Institute for Creative Technologies, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Linkoping University, Linkoping, 581 83, Sweden","Jones, A., USC Institute for Creative Technologies, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Nagano, K., USC Institute for Creative Technologies, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Busch, J., USC Institute for Creative Technologies, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Yu, X., USC Institute for Creative Technologies, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Peng, H.-Y., USC Institute for Creative Technologies, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Barreto, J., USC Institute for Creative Technologies, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Alexander, O., USC Institute for Creative Technologies, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Bolas, M., USC Institute for Creative Technologies, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Debevec, P., USC Institute for Creative Technologies, 12015 Waterfront Drive, Playa Vista, CA  90094, United States; Unger, J., Linkoping University, Linkoping, 581 83, Sweden","We present a system for creating and displaying interactive life-sized 3D digital humans based on pre-recorded interviews. We use 30 cameras and an extensive list of questions to record a large set of video responses. Users access videos through a natural conversation interface that mimics face-to-face interaction. Recordings of answers, listening and idle behaviors are linked together to create a persistent visual image of the person throughout the interaction. The interview subjects are rendered using flowed light fields and shown life-size on a special rear-projection screen with an array of 216 video projectors. The display allows multiple users to see different 3D perspectives of the subject in proper relation to their viewpoints, without the need for stereo glasses. The display is effective for interactive conversations since it provides 3D cues such as eye gaze and spatial hand gestures. © 2016 IEEE.",,"Computer vision; Pattern recognition; Conversation interface; Digital humans; Face-to-face interaction; Multiple user; Projector arrays; Time offsets; Video projectors; Video response; Stereo image processing",Conference Paper,"Final","",Scopus,2-s2.0-85010222343
"Tomi A.B., Rambli D.R.A.","55638599800;25824683000;","Automated calibration for optical see-through head mounted display using display screen space based eye tracking",2016,"2016 3rd International Conference on Computer and Information Sciences, ICCOINS 2016 - Proceedings",,,"7783257","448","453",,2,"10.1109/ICCOINS.2016.7783257","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010403575&doi=10.1109%2fICCOINS.2016.7783257&partnerID=40&md5=cb9e9ca9a917e38f9028e36fc5e89cbf","Department of Computer Information Sciences, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, Perak, Malaysia","Tomi, A.B., Department of Computer Information Sciences, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, Perak, Malaysia; Rambli, D.R.A., Department of Computer Information Sciences, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, Perak, Malaysia","A crucial problem in Augmented Reality (AR) applications that used optical see-through head mounted display (OST HMD) is frequent (re)calibrations. It is considered time consuming and distract users from their application if each such frequent (re)calibrations involve user interactions. Furthermore, it will be prone to user-dependent errors into the system setup thus leads to reduce user's acceptance on OST HMD in AR applications. Nowadays, a few studies toward interaction-free calibration implementation on OST HMD have been done. Recent implementation on interaction-free calibration issues are associated with the virtual display calibration which is related to the eye and world camera relation with display screen. Thus, we propose a novel approach that utilizes a display screen space based eye tracking into OST HMD calibration. By simultaneously measuring eye movements and map the tracked eye coordinate into a 2D display screen space, a calibration formulation can be done without involving the eye tracker properties. This work is expected to be established for efficient and high accuracy interaction-free OST HMD calibration which will be beneficial in AR applications that used OST HMD. © 2016 IEEE.","Augmented Reality; Automated Calibration; Optical See-Through Head Mounted Display","Augmented reality; Calibration; Eye movements; Information science; AR application; Automated calibration; Display screen; Eye coordinates; HMD calibrations; Optical see-through head-mounted displays; User interaction; Virtual displays; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85010403575
"Yu P., Zhou J., Wu Y.","57191071774;57191078967;36747752300;","Learning Reconstruction-Based Remote Gaze Estimation",2016,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2016-December",,"7780744","3447","3455",,4,"10.1109/CVPR.2016.375","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986333979&doi=10.1109%2fCVPR.2016.375&partnerID=40&md5=57a50788f13acb32af903dd35d0e3d39","Northwestern University, 2145 Sheridan Road, Evanston, IL  60208, United States","Yu, P., Northwestern University, 2145 Sheridan Road, Evanston, IL  60208, United States; Zhou, J., Northwestern University, 2145 Sheridan Road, Evanston, IL  60208, United States; Wu, Y., Northwestern University, 2145 Sheridan Road, Evanston, IL  60208, United States","It is a challenging problem to accurately estimate gazes from low-resolution eye images that do not provide fine and detailed features for eyes. Existing methods attempt to establish the mapping between the visual appearance space to the gaze space. Different from the direct regression approach, the reconstruction-based approach represents appearance and gaze via local linear reconstruction in their own spaces. A common treatment is to use the same local reconstruction in the two spaces, i.e., the reconstruction weights in the appearance space are transferred to the gaze space for gaze reconstruction. However, this questionable treatment is taken for granted but has never been justified, leading to significant errors in gaze estimation. This paper is focused on the study of this fundamental issue. It shows that the distance metric in the appearance space needs to be adjusted, before the same reconstruction can be used. A novel method is proposed to learn the metric, such that the affinity structure of the appearance space under this new metric is as close as possible to the affinity structure of the gaze space under the normal Euclidean metric. Furthermore, the local affinity structure invariance is utilized to further regularize the solution to the reconstruction weights, so as to obtain a more robust and accurate solution. Effectiveness of the proposed method is validated and demonstrated through extensive experiments on different subjects. © 2016 IEEE.",,"Computer vision; Distance metrics; Euclidean metrics; Eye images; Gaze estimation; Local linear; Low resolution; Remote gaze estimation; Visual appearance; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-84986333979
"Ishizuka A., Yorozu A., Takahashi M.","57193754696;51261534300;7406846479;","Experimental verification for motion control of a powered wheelchair using a gazing feature in an environment",2016,"ACM International Conference Proceeding Series",,,,"147","151",,,"10.1145/3029610.3029614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016401010&doi=10.1145%2f3029610.3029614&partnerID=40&md5=37ba876a10d8c48ae03f24516674adb1","Graduate School of Science and Technology, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, 223-8522, Japan; Keio Advanced Research Centers, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, 223-8522, Japan; Department of System Design Engineering, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, 223-8522, Japan","Ishizuka, A., Graduate School of Science and Technology, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, 223-8522, Japan; Yorozu, A., Keio Advanced Research Centers, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, 223-8522, Japan; Takahashi, M., Department of System Design Engineering, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, 223-8522, Japan","This paper describes the motion control system for a powered wheelchair using a gaze in an unknown environment. Recently, new Human-Computer Interfaces (HCIs) that have replaced joysticks have been developed for a person with a disability of the upper body. In this paper, movement of the eyes is used as an HCI. The wheelchair control system proposed in this study aims to achieve an operation such that a passenger gazes towards the direction he or she wants to move in the unknown environment. The gazing feature of the passenger in the 3D environment is acquired in real time and the wheelchair is subsequently controlled. The features include the entrance of the area of the passage and the gazing feature is acquired by obtaining the features and the gazing point of the passenger. The acquired information about the direction in which the passenger wants to move becomes operation input to the wheelchair. The wheelchair is controlled by obtaining this operation input and the information of the environment. The conventional motion control system can perform safe and smooth movement by avoiding obstacles. The effectiveness of the proposed system is demonstrated through experiments in a real environment with three participants. © 2016 ACM.","Control System; Eye Gaze Tracking; Human-Computer Interface; Powered Wheelchair","Control systems; Eye movements; Human computer interaction; Interfaces (computer); Motion control; Tracking (position); Transportation; User interfaces; Wheelchairs; Avoiding obstacle; Conventional motion control; Experimental verification; Eye gaze tracking; Human computer interfaces; Powered wheel chairs; Unknown environments; Wheelchair control; Computer control systems",Conference Paper,"Final","",Scopus,2-s2.0-85016401010
"Schneider B., Sharma K., Cuendet S., Zufferey G., Dillenbourg P., Pea E.R.","55051404100;55903734200;23967764700;36978947600;8912010400;6602146828;","Using mobile eye-trackers to unpack the perceptual benefits of a tangible user interface for collaborative learning",2016,"ACM Transactions on Computer-Human Interaction","23","6","3012009","","",,26,"10.1145/3012009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006839765&doi=10.1145%2f3012009&partnerID=40&md5=d7587888e42353645946dbc1fb64d464","Graduate School of Education, Standford University, 520 Galvez Mall, Stanford, 94305, United States; EPFL - CHILI, RLC D1 740 (Rolex Learning Center), Lausanne, 1015, Switzerland","Schneider, B., Graduate School of Education, Standford University, 520 Galvez Mall, Stanford, 94305, United States; Sharma, K., EPFL - CHILI, RLC D1 740 (Rolex Learning Center), Lausanne, 1015, Switzerland; Cuendet, S., EPFL - CHILI, RLC D1 740 (Rolex Learning Center), Lausanne, 1015, Switzerland; Zufferey, G., EPFL - CHILI, RLC D1 740 (Rolex Learning Center), Lausanne, 1015, Switzerland; Dillenbourg, P., EPFL - CHILI, RLC D1 740 (Rolex Learning Center), Lausanne, 1015, Switzerland; Pea, E.R., Graduate School of Education, Standford University, 520 Galvez Mall, Stanford, 94305, United States","In this study, we investigated the way users memorize, analyze, collaborate, and learn new concepts on a Tangible User Interface (TUI). Twenty-seven pairs of apprentices in logistics (N = 54) interacted with an interactive simulation of a warehouse. Their task was to discover efficient design principles for building storehouses. In a between-subjects experimental design, half of the participants used 3D physical shelves, whereas the other half used 2D paper shelves. This manipulation allowed us to control for the ""representational effect"" of 3D tangibles: the first group saw the warehouse as a small-scale model with realistic shelves, whereas the second group had access to a more abstract layout with rectangular pieces of paper. Both groups interacted with the system in the same way. We found that participants in the first group (i.e., who used 3D realistic shelves) better memorized a warehouse layout, built a more efficient model, and scored higher on a learning test. Additionally, students wore eye-tracking goggles while completing those tasks; preliminary results suggest that 3D interfaces increased joint visual attention, which was found to be a significant predictor for participants' task performance and learning gains. Implications for designing TUIs in collaborative settings are discussed. © 2016 ACM.","Collaborative learning; Eye-tracking; Tangible interface","Behavioral research; Eye movements; Warehouses; Collaborative learning; Collaborative settings; Efficient designs; Eye-tracking; Interactive simulations; Small-scale modeling; Tangible interfaces; Tangible user interfaces; User interfaces",Article,"Final","",Scopus,2-s2.0-85006839765
"Anderson B.B., Jenkins J.L., Vance A., Kirwan C.B., Eargle D.","36986090000;36713489900;24330327900;6508279742;55634592700;","Your memory is working against you: How eye tracking and memory explain habituation to security warnings",2016,"Decision Support Systems","92",,,"3","13",,24,"10.1016/j.dss.2016.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001060900&doi=10.1016%2fj.dss.2016.09.010&partnerID=40&md5=7a7afa62157b03bbddc476f20de96c73","Information Systems Department, Marriott School of Management, 790 TNRB, Brigham Young University, Provo, UT  84602, United States; Department of Psychology & Neuroscience Center, Brigham Young University, United States; 1001 SWKT, Brigham Young University, Provo, UT  84602, United States; Katz Graduate School of Business, University of Pittsburgh, United States; University of Pittsburgh, Joseph M. Katz Graduate School of Business, Doctoral Office, 282 Mervis Hall, Pittsburgh, PA  15260, United States","Anderson, B.B., Information Systems Department, Marriott School of Management, 790 TNRB, Brigham Young University, Provo, UT  84602, United States; Jenkins, J.L., Information Systems Department, Marriott School of Management, 790 TNRB, Brigham Young University, Provo, UT  84602, United States; Vance, A., Information Systems Department, Marriott School of Management, 790 TNRB, Brigham Young University, Provo, UT  84602, United States; Kirwan, C.B., Department of Psychology & Neuroscience Center, Brigham Young University, United States, 1001 SWKT, Brigham Young University, Provo, UT  84602, United States; Eargle, D., Katz Graduate School of Business, University of Pittsburgh, United States, University of Pittsburgh, Joseph M. Katz Graduate School of Business, Doctoral Office, 282 Mervis Hall, Pittsburgh, PA  15260, United States","Security warnings are critical to the security of end users and their organizations, often representing the final defense against an attack. Because warnings require users to make a contextual judgment, it is critical that they pay close attention to warnings. However, research shows that users routinely disregard them. A major factor contributing to the ineffectiveness of warnings is habituation, the decreased response to a repeated warning. Although previous research has identified the problem of habituation, the phenomenon has only been observed indirectly through behavioral measures. Therefore, it is unclear how habituation develops in the brain in response to security warnings, and how this in turn influences users’ perceptions of these warnings. This paper contributes by using eye tracking to measure the eye movement-based memory (EMM) effect, a neurophysiological manifestation of habituation in which people unconsciously scrutinize previously seen stimuli less than novel stimuli. We show that habituation sets in after only a few exposures to a warning and progresses rapidly with further repetitions. Using guidelines from the warning science literature, we design a polymorphic warning artifact which repeatedly changes its appearance. We demonstrate that our polymorphic warning artifact is substantially more resistant to habituation than conventional security warnings, offering an effective solution for practice. Finally, our results highlight the value of applying neuroscience to the domain of information security behavior. © 2016 Elsevier B.V.","Behavioral information security; Eye tracking; Habituation; NeuroIS; Security warnings","Security of data; Target tracking; Behavioral information security; Eye-tracking; Habituation; NeuroIS; Security warning; Eye movements",Article,"Final","",Scopus,2-s2.0-85001060900
"Milovanović M.B., Antić D.S., Milojković M.T., Nikolić S.S., Perić S.L., Spasić M.D.","56608880400;35608641500;15765451500;55498842900;36195880900;55369047300;","Adaptive PID control based on orthogonal endocrine neural networks",2016,"Neural Networks","84",,,"80","90",,17,"10.1016/j.neunet.2016.08.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991404509&doi=10.1016%2fj.neunet.2016.08.012&partnerID=40&md5=10be157cb791d7631539269c4a07addd","University of Niš, Faculty of Electronic Engineering, Department of Control Systems, Aleksandra Medvedeva 14, Niš, 18000, Serbia","Milovanović, M.B., University of Niš, Faculty of Electronic Engineering, Department of Control Systems, Aleksandra Medvedeva 14, Niš, 18000, Serbia; Antić, D.S., University of Niš, Faculty of Electronic Engineering, Department of Control Systems, Aleksandra Medvedeva 14, Niš, 18000, Serbia; Milojković, M.T., University of Niš, Faculty of Electronic Engineering, Department of Control Systems, Aleksandra Medvedeva 14, Niš, 18000, Serbia; Nikolić, S.S., University of Niš, Faculty of Electronic Engineering, Department of Control Systems, Aleksandra Medvedeva 14, Niš, 18000, Serbia; Perić, S.L., University of Niš, Faculty of Electronic Engineering, Department of Control Systems, Aleksandra Medvedeva 14, Niš, 18000, Serbia; Spasić, M.D., University of Niš, Faculty of Electronic Engineering, Department of Control Systems, Aleksandra Medvedeva 14, Niš, 18000, Serbia","A new intelligent hybrid structure used for online tuning of a PID controller is proposed in this paper. The structure is based on two adaptive neural networks, both with built-in Chebyshev orthogonal polynomials. First substructure network is a regular orthogonal neural network with implemented artificial endocrine factor (OENN), in the form of environmental stimuli, to its weights. It is used for approximation of control signals and for processing system deviation/disturbance signals which are introduced in the form of environmental stimuli. The output values of OENN are used to calculate artificial environmental stimuli (AES), which represent required adaptation measure of a second network—orthogonal endocrine adaptive neuro-fuzzy inference system (OEANFIS). OEANFIS is used to process control, output and error signals of a system and to generate adjustable values of proportional, derivative, and integral parameters, used for online tuning of a PID controller. The developed structure is experimentally tested on a laboratory model of the 3D crane system in terms of analysing tracking performances and deviation signals (error signals) of a payload. OENN-OEANFIS performances are compared with traditional PID and 6 intelligent PID type controllers. Tracking performance comparisons (in transient and steady-state period) showed that the proposed adaptive controller possesses performances within the range of other tested controllers. The main contribution of OENN-OEANFIS structure is significant minimization of deviation signals (17%–79%) compared to other controllers. It is recommended to exploit it when dealing with a highly nonlinear system which operates in the presence of undesirable disturbances. © 2016 Elsevier Ltd","Adaptive PID controller; ANFIS; Chebyshev orthogonal polynomials; Endocrine factor; Neural network","Controllers; Cranes; Electric control equipment; Fuzzy inference; Fuzzy neural networks; Fuzzy systems; Neural networks; Orthogonal functions; Polynomials; Proportional control systems; Three term control systems; Tuning; Adaptive neural networks; Adaptive neuro-fuzzy inference system; Adaptive PID controller; ANFIS; Chebyshev orthogonal polynomials; Endocrine factor; Environmental stimuli; Orthogonal neural networks; Adaptive control systems; adaptation; analytical error; Article; artificial environmental stimuli; artificial neural network; defuzzification; derivatization; electromechanics; eye tracking; integral parameters; learning; learning algorithm; mathematical analysis; mechanics; nonlinear system; online system; orthogonal endocrine adaptive neuro fuzzy inference system; orthogonal eworkndocrine neural network; priority journal; process control; process optimization; Proportional Integral Derivative control; steady state; three dimensional imaging; training; algorithm; computer simulation; endocrine system; environment; signal processing; theoretical model; Algorithms; Computer Simulation; Endocrine System; Environment; Models, Theoretical; Neural Networks (Computer); Signal Processing, Computer-Assisted",Article,"Final","",Scopus,2-s2.0-84991404509
"Rieke N., Tan D.J., Amat di San Filippo C., Tombari F., Alsheakhali M., Belagiannis V., Eslami A., Navab N.","56596764700;55430201900;56019075400;55921536100;56896117800;35483155200;9269217900;7003458998;","Real-time localization of articulated surgical instruments in retinal microsurgery",2016,"Medical Image Analysis","34",,,"82","100",,28,"10.1016/j.media.2016.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969983907&doi=10.1016%2fj.media.2016.05.003&partnerID=40&md5=c4e27a1a72bf4d8340d7547763ed9497","Computer Aided Medical Procedures, Technische Universität München, Germany; Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom; Carl Zeiss Meditec AG, München, Germany; DISI, University of Bologna, Italy","Rieke, N., Computer Aided Medical Procedures, Technische Universität München, Germany; Tan, D.J., Computer Aided Medical Procedures, Technische Universität München, Germany; Amat di San Filippo, C., Computer Aided Medical Procedures, Technische Universität München, Germany, Carl Zeiss Meditec AG, München, Germany; Tombari, F., Computer Aided Medical Procedures, Technische Universität München, Germany, DISI, University of Bologna, Italy; Alsheakhali, M., Computer Aided Medical Procedures, Technische Universität München, Germany; Belagiannis, V., Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom; Eslami, A., Carl Zeiss Meditec AG, München, Germany; Navab, N., Computer Aided Medical Procedures, Technische Universität München, Germany","Real-time visual tracking of a surgical instrument holds great potential for improving the outcome of retinal microsurgery by enabling new possibilities for computer-aided techniques such as augmented reality and automatic assessment of instrument manipulation. Due to high magnification and illumination variations, retinal microsurgery images usually entail a high level of noise and appearance changes. As a result, real-time tracking of the surgical instrument remains challenging in in-vivo sequences. To overcome these problems, we present a method that builds on random forests and addresses the task by modelling the instrument as an articulated object. A multi-template tracker reduces the region of interest to a rectangular area around the instrument tip by relating the movement of the instrument to the induced changes on the image intensities. Within this bounding box, a gradient-based pose estimation infers the location of the instrument parts from image features. In this way, the algorithm does not only provide the location of instrument, but also the positions of the tool tips in real-time. Various experiments on a novel dataset comprising 18 in-vivo retinal microsurgery sequences demonstrate the robustness and generalizability of our method. The comparison on two publicly available datasets indicates that the algorithm can outperform current state-of-the art. © 2016","Pose estimation; Retinal microsurgery; Visual tracking","Aldehydes; Augmented reality; Decision trees; End effectors; Gesture recognition; Image segmentation; Motion estimation; Ophthalmology; Surgical equipment; Tracking (position); Automatic assessment; Computer aided technique; Illumination variation; Pose estimation; Real-time localization; Retinal microsurgery; Surgical instrument; Visual Tracking; Surgery; algorithm; Article; eye tracking; in vivo study; microsurgery; priority journal; random forest; retina surgery; surgical equipment; algorithm; computer assisted surgery; human; microsurgery; procedures; retina; surgery; Algorithms; Humans; Microsurgery; Retina; Surgery, Computer-Assisted; Surgical Instruments",Article,"Final","",Scopus,2-s2.0-84969983907
"Yilmaz C.M., Kose C.","56246395000;6602451231;","Local binary pattern histogram features for on-screen eye-gaze direction estimation and a comparison of appearance based methods",2016,"2016 39th International Conference on Telecommunications and Signal Processing, TSP 2016",,,"7760973","693","696",,3,"10.1109/TSP.2016.7760973","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006699745&doi=10.1109%2fTSP.2016.7760973&partnerID=40&md5=f7c6e65889e0218b3f2ffaefd0c4af81","Department of Computer Engineering, Karadeniz Technical University, Trabzon, Turkey","Yilmaz, C.M., Department of Computer Engineering, Karadeniz Technical University, Trabzon, Turkey; Kose, C., Department of Computer Engineering, Karadeniz Technical University, Trabzon, Turkey","Human Computer Interaction (HCI) has become an important focus of both computer science researches and industrial applications. And, on-screen gaze estimation is one of the hottest topics in this rapidly growing field. Eye-gaze direction estimation is a sub-research area of on-screen gaze estimation and the number of studies that focused on the estimation of on-screen gaze direction is limited. Due to this, various appearance-based video-oculography methods are investigated in this work. Firstly, a new dataset is created via user images taken from daylight censored cameras located at computer screen. Then, Local Binary Pattern Histogram (LBPH), which is used in this work for the first time to obtain on-screen gaze direction information, and Principal Component Analysis (PCA) methods are employed to extract image features. And, parameter optimized Support Vector Machine (SVM), Artificial Neural Networks (ANNs) and k-Nearest Neighbor (k-NN) learning methods are adopted in order to estimate on-screen gaze direction. Finally, these methods' abilities to correctly estimate the on-screen gaze direction are compared using the resulting classification accuracies of applied methods and previous works. The best classification accuracy of 96.67% is obtained when using LBPH and SVM method pair which is better than previous works. The results also show that appearance based methods are pretty applicable for estimating on-screen gaze direction. © 2016 IEEE.","Appearance-based methods; Gaze direction estimation; Gaze estimation; Human computer interaction; Video-oculography","Binary images; Graphic methods; Nearest neighbor search; Neural networks; Principal component analysis; Signal processing; Support vector machines; Video recording; Appearance-based methods; Classification accuracy; Computer science research; Gaze direction; Gaze estimation; Human computer interaction (HCI); Local binary patterns; Video oculography; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85006699745
"Gras G., Yang G.-Z.","56641051600;55539304100;","Intention recognition for gaze controlled robotic minimally invasive laser ablation",2016,"IEEE International Conference on Intelligent Robots and Systems","2016-November",,"7759379","2431","2437",,8,"10.1109/IROS.2016.7759379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006482371&doi=10.1109%2fIROS.2016.7759379&partnerID=40&md5=07a5d53eab11a3bd9a2b727d37825c9e","Hamlyn Centre for Robotic Surgery, Imperial College London, London, SW7 2AZ, United Kingdom","Gras, G., Hamlyn Centre for Robotic Surgery, Imperial College London, London, SW7 2AZ, United Kingdom; Yang, G.-Z., Hamlyn Centre for Robotic Surgery, Imperial College London, London, SW7 2AZ, United Kingdom","Eye tracking technology has shown promising results for allowing hands-free control of robotically-mounted cameras and tools. However existing systems present only limited capabilities in allowing the full range of camera motions in a safe, intuitive manner. This paper introduces a framework for the recognition of surgeon intention, allowing activation and control of the camera through natural gaze behaviour. The system is resistant to noise such as blinking, while allowing the surgeon to look away safely at any time. Furthermore, this paper presents a novel approach to control the translation of the camera along its optical axis using a combination of eye tracking and stereo reconstruction. Combining eye tracking and stereo reconstruction allows the system to determine which point in 3D space the user is fixating, enabling a translation of the camera to achieve the optimal viewing distance. In addition, the eye tracking information is used to perform automatic laser targeting for laser ablation. The desired target point of the laser, mounted on a separate robotic arm, is determined with the eye tracking thus removing the need to manually adjust the laser's target point before starting each new ablation. The calibration methodology used to obtain millimetre precision for the laser targeting without the aid of visual servoing is described. Finally, a user study validating the system is presented, showing clear improvement with median task times under half of those of a manually controlled robotic system. © 2016 IEEE.",,"Ablation; Cameras; Intelligent robots; Laser ablation; Robotic surgery; Robotics; Target tracking; Calibration methodologies; Existing systems; Eye tracking technologies; Gaze behaviours; Intention recognition; Minimally invasive; Stereo reconstruction; Viewing distance; Visual servoing",Conference Paper,"Final","",Scopus,2-s2.0-85006482371
"Liu F., Zhou T., Yang J.","57188640717;56367644000;15039078800;","Geometric affine transformation estimation via correlation filter for visual tracking",2016,"Neurocomputing","214",,,"109","120",,11,"10.1016/j.neucom.2016.05.079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992530863&doi=10.1016%2fj.neucom.2016.05.079&partnerID=40&md5=578620af2e620f1987526cb5b0e12dff","Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China","Liu, F., Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China; Zhou, T., Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China; Yang, J., Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, China","Correlation filter achieves promising performance with high speed in visual tracking. However, conventional correlation filter based trackers cannot tackle affine transformation issues such as scale variation, rotation and skew. To address this problem, in this paper, we propose a part-based representation tracker via kernelized correlation filter (KCF) for visual tracking. A Spatial–Temporal Angle Matrix (STAM), severed as confidence metric, is proposed to select reliable patches from parts via multiple correlation filters. These stable patches are used to estimate a 2D affine transformation matrix of the target in a geometric method. Specially, the whole combination scheme for these stable patches is proposed to exploit sampling space in order to obtain numerous affine matrices and their corresponding candidates. The diversiform candidates would help to seek for the optimal candidate to represent the object's accurate affine transformation in a higher probability. Both qualitative and quantitative evaluations on VOT2014 challenge and Object Tracking Benchmark (OTB) show that the proposed tracking method achieves favorable performance compared with other state-of-the-art methods. © 2016 Elsevier B.V.","Affine transformation estimation; Correlation filter; Object tracking; Part-based","Bandpass filters; Benchmarking; Face recognition; Linear transformations; Tracking (position); Affine transformation estimation; Affine transformations; Correlation filters; Object Tracking; Part based; Part-based representation; Quantitative evaluation; State-of-the-art methods; Mathematical transformations; accuracy; affine transformation estimation; Article; background clutter; comparative study; eye tracking; geometry; illumination variation; image analysis; image deformation; image processing; image quality; kernel correlation filter method; kernel method; low resolution; machine learning; motion blur; object tracking benchmark method; peak to side lobe ratio; priority journal; probability theory; qualitative research; quantitative study; spatial temporal angle matrix; spatiotemporal analysis; velocity; VOT2014 method",Article,"Final","",Scopus,2-s2.0-84992530863
"Li G., Ma B., Huang J., Huang Q., Zhang W.","55714185400;13606466200;56716936400;8435766200;56939381400;","Beyond appearance model: Learning appearance variations for object tracking",2016,"Neurocomputing","214",,,"796","804",,3,"10.1016/j.neucom.2016.06.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992515485&doi=10.1016%2fj.neucom.2016.06.058&partnerID=40&md5=85a84e9b644436aa584c3a748010b2fa","School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, 100190, China; Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences(CAS), Beijing, 100080, China; Key Laboratory of Big Data Mining and Knowledge Management, CAS, China; School of Computer Science and Technology, Harbor Inst. of Tech., China","Li, G., School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, 100190, China, Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences(CAS), Beijing, 100080, China, Key Laboratory of Big Data Mining and Knowledge Management, CAS, China; Ma, B., School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, 100190, China, Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences(CAS), Beijing, 100080, China, Key Laboratory of Big Data Mining and Knowledge Management, CAS, China; Huang, J., School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, 100190, China; Huang, Q., School of Computer and Control Engineering, University of Chinese Academy of Sciences, Beijing, 100190, China, Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences(CAS), Beijing, 100080, China, Key Laboratory of Big Data Mining and Knowledge Management, CAS, China; Zhang, W., School of Computer Science and Technology, Harbor Inst. of Tech., China","In this paper, we present a novel appearance variation prediction model which can be embedded into the existing generative appearance model based tracking framework. Different from the existing works, which online learn appearance model with obtained tracking results, we propose to predict appearance reconstruction error. We notice that although the learned appearance model can precisely describe the target in the previous frames, the tracking result is still not accurate if in the following frame, the patch that is most similar to appearance model is assumed to be the target. We first investigate the above phenomenon by conducting experiments on two public sequences and discover that in most cases the best target is not the one with minimal reconstruction error. Then we design three kinds of features which can encode motion, appearance, appearance reconstruction error information of target's surrounding image patches, and capture potential factors that may cause variations of target's appearance as well as its reconstruction error. Finally, with these features, we learn an effective random forest for predicting reconstruction error of the target during tracking. Experiments on various datasets demonstrate that the proposed method can be combined with many existing trackers and improve their performances significantly. © 2016 Elsevier B.V.","Appearance model; Appearance prediction; Object tracking","Decision trees; Errors; Forecasting; Tracking (position); Appearance modeling; Appearance predictions; Image patches; Object Tracking; Prediction model; Random forests; Reconstruction error; Target tracking; algorithm; appearance model; Article; embedding; eye tracking; generative appearance model; information processing; measurement accuracy; measurement error; motion; normal distribution; object tracking; online system; optic flow; prediction; priority journal; quantitative analysis; random forest",Article,"Final","",Scopus,2-s2.0-84992515485
"Chen W., Zhang K., Liu Q.","57191612207;55475000500;36063739200;","Robust visual tracking via patch based kernel correlation filters with adaptive multiple feature ensemble",2016,"Neurocomputing","214",,,"607","617",,47,"10.1016/j.neucom.2016.06.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992491273&doi=10.1016%2fj.neucom.2016.06.048&partnerID=40&md5=9e363313334b3cfc6cbf0f61492d9680","Jiangsu Key Laboratory of Big Data Analysis Technology (B-DAT), Nanjing University of Information Science and Technology, China","Chen, W., Jiangsu Key Laboratory of Big Data Analysis Technology (B-DAT), Nanjing University of Information Science and Technology, China; Zhang, K., Jiangsu Key Laboratory of Big Data Analysis Technology (B-DAT), Nanjing University of Information Science and Technology, China; Liu, Q., Jiangsu Key Laboratory of Big Data Analysis Technology (B-DAT), Nanjing University of Information Science and Technology, China","Both patch based and correlation filter-based tracking methods have achieved competitive results on accuracy and robustness, but there is still a large room to improve their overall performance if carefully dealing with the challenging factors in visual tracking. In this paper, we present a patch based tracker which adaptively integrates the kernel correlation filters with multiple effective features. To take full advantage of the useful information from different parts of the target, we train each template patch by kernel correlation filtering method, and adaptively set the weight of each patch for each particle in a particle filtering framework. Experiments illustrate that this scheme can effectively handle the occlusion problem. Moreover, the effective features including the HOG features and color name features are effectively integrated to learn the correlations between the target and the background, the candidate patches and template ones, which further boosts the overall performance. Extensive experimental results on the CVPR2013 tracking benchmark demonstrate that the proposed approach performs favorably against some representative state-of-the-art tracking algorithms. © 2016 Elsevier B.V.","Correlation filters; Particle filters; Visual tracking","Bandpass filters; Face recognition; Monte Carlo methods; Signal filtering and prediction; Tracking (position); Correlation filtering; Correlation filters; Multiple features; Occlusion problems; Particle filter; State of the art; Tracking algorithm; Visual Tracking; Information filtering; algorithm; Article; color vision; comparative study; correlational study; eye tracking; histogram of gradient; illumination variation; image analysis; image deformation; image occlusion; image processing; image quality; kernel correlation filter method; kernel method; machine learning; priority journal; qualitative analysis; scale variation",Article,"Final","",Scopus,2-s2.0-84992491273
"Yao R., Xia S., Zhou Y., Niu Q.","23567304500;55650066900;35480110700;22635551200;","Robust lifelong visual tracking using compact binary feature with color attributes",2016,"Neurocomputing","213",,,"172","182",,1,"10.1016/j.neucom.2015.10.149","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028264779&doi=10.1016%2fj.neucom.2015.10.149&partnerID=40&md5=96219487bff691be4bfe6faade2aabaa","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, 221116, China","Yao, R., School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, 221116, China; Xia, S., School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, 221116, China; Zhou, Y., School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, 221116, China; Niu, Q., School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, 221116, China","In this paper, we address the problem of visual tracking where the target object undergo appearance variations due to illumination variation, occlusion, motion, background clutter and deformation. To deal with the significant appearance variations, we introduce color attributes into traditional shape feature at cell level, the new feature representation takes into consideration both its photometric invariance as well as its discriminative power. We construct compact binary code for the shape-color feature to reduce the high dimensions, and update the hash function in an online manner. A discriminative lifelong learning model is built to construct an appearance model that optimally separates the object from its surrounds. The lifelong learner uses the shared latent basis to transfer historical observations to simple classifier while a new frame arrives. Experimental results on tracking benchmark demonstrate that the proposed tracking algorithm outperforms state-of-the-art methods. © 2016 Elsevier B.V.","Color attributes; Compact binary code; Lifelong learning; Visual tracking","Binary codes; Color; Hash functions; Motion tracking; Color attributes; Feature representation; Historical observation; Illumination variation; Life long learning; Photometric invariance; State-of-the-art methods; Visual Tracking; Object tracking; algorithm; Article; benchmarking; classifier; color; compact binary code; computer language; discrimination learning; eye tracking; illumination; intermethod comparison; machine learning; morphology; motion; online system; photometry; priority journal",Article,"Final","",Scopus,2-s2.0-85028264779
"Mahfoud E., Lu A.","55840574400;23492979800;","Gaze-directed immersive visualization of scientific ensembles",2016,"Companion Proceedings of the 2016 ACM International Conference on Interactive Surfaces and Spaces: Nature Meets Interactive Surfaces, ISS 2016",,,,"77","82",,4,"10.1145/3009939.3009952","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009756595&doi=10.1145%2f3009939.3009952&partnerID=40&md5=e01bd1a7b7d2a77a712231bf44a720ac","University of North Carolina at Charlotte, Charlotte, NC  28223, United States","Mahfoud, E., University of North Carolina at Charlotte, Charlotte, NC  28223, United States; Lu, A., University of North Carolina at Charlotte, Charlotte, NC  28223, United States","The latest advances in head-mounted displays (HMDs) for augmented reality (AR) and mixed reality (MR) have produced commercialized devices that are gradually accepted by the public. These HMDs are generally equipped with head tracking, which provides an excellent input to explore immersive visualization and interaction techniques for various AR/MR applications. This paper explores the head tracking function on the latest Microsoft HoloLens - where gaze is defined as the ray starting at the head location and points forward. We present a gaze-directed visualization approach to study ensembles of 2D oil spill simulations in mixed reality. Our approach allows users to place an ensemble as an image stack in a real environment and explore the ensemble with gaze tracking. The prototype system demonstrates the challenges and promising effects of gaze-based interaction in the state-of-the-art mixed reality.","Gaze-directed interaction; Immersive visualization; Scientific ensembles","Augmented reality; Display devices; Helmet mounted displays; Oil spills; Stereo vision; Tracking (position); Virtual reality; Directed interactions; Gaze-based interaction; Head mounted displays; Immersive visualization; Interaction techniques; Real environments; Scientific ensembles; State of the art; Visualization",Conference Paper,"Final","",Scopus,2-s2.0-85009756595
"Greenwald S.W., Loreti L., Funk M., Zilberman R., Maes P.","56946457000;57192158560;55876807100;57192163066;56268463300;","Eye gaze tracking with Google cardboard using Purkinje images",2016,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST","02-04-November-2016",,,"19","22",,10,"10.1145/2993369.2993407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998672571&doi=10.1145%2f2993369.2993407&partnerID=40&md5=7068fc6a1d149be18c8f308d572465b6","MIT Media Lab, Cambridge, MA, United States","Greenwald, S.W., MIT Media Lab, Cambridge, MA, United States; Loreti, L., MIT Media Lab, Cambridge, MA, United States; Funk, M., MIT Media Lab, Cambridge, MA, United States; Zilberman, R., MIT Media Lab, Cambridge, MA, United States; Maes, P., MIT Media Lab, Cambridge, MA, United States","Mobile phone-based Virtual Reality (VR) is rapidly growing as a platform for stereoscopic 3D and non-3D digital content and applications. The ability to track eye gaze in these devices would be a tremendous opportunity on two fronts: firstly, as an interaction technique, where interaction is currently awkward and limited, and secondly, for studying human visual behavior. We propose a method to add eye gaze tracking to these existing devices using their on-board display and camera hardware, with a minor modification to the headset enclosure. We present a proof-of-concept implementation of the technique and show results demonstrating its feasibility. The software we have developed will be made available as open source to benefit the research community.","Eye tracking; Low cost; Purkinje images; Virtual reality","Behavioral research; Display devices; Open source software; Open systems; Stereo image processing; Virtual reality; Digital contents; Eye gaze tracking; Eye-tracking; Interaction techniques; Low costs; Proof of concept; Purkinje; Research communities; Tracking (position)",Conference Paper,"Final","",Scopus,2-s2.0-84998672571
"Kellnhofer P., Didyk P., Ritschel T., Masia B., Myszkowski K., Seidel H.-P.","55250016000;24779555900;24537726600;57189656516;55888212000;7202927645;","Motion parallax in stereo 3D: Model and applications",2016,"ACM Transactions on Graphics","35","6","176","","",,18,"10.1145/2980179.2980230","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043522963&doi=10.1145%2f2980179.2980230&partnerID=40&md5=0ad0721f4eed7321f27fd2d6bc5bb277","MPI Informatik, Germany; Saarland University, MMCI, Germany; University College London, United Kingdom; Universidad de Zaragoza, Spain","Kellnhofer, P., MPI Informatik, Germany; Didyk, P., MPI Informatik, Germany, Saarland University, MMCI, Germany; Ritschel, T., MPI Informatik, Germany, Saarland University, MMCI, Germany, University College London, United Kingdom; Masia, B., MPI Informatik, Germany, Universidad de Zaragoza, Spain; Myszkowski, K., MPI Informatik, Germany; Seidel, H.-P., MPI Informatik, Germany","Binocular disparity is the main depth cue that makes stereoscopic images appear 3D. However, in many scenarios, the range of depth that can be reproduced by this cue is greatly limited and typically fixed due to constraints imposed by displays. For example, due to the low angular resolution of current automultiscopic screens, they can only reproduce a shallow depth range. In this work, we study the motion parallax cue, which is a relatively strong depth cue, and can be freely reproduced even on a 2D screen without any limits. We exploit the fact that in many practical scenarios, motion parallax provides sufficiently strong depth information that the presence of binocular depth cues can be reduced through aggressive disparity compression. To assess the strength of the effect we conduct psychovisual experiments that measure the influence of motion parallax on depth perception and relate it to the depth resulting from binocular disparity. Based on the measurements, we propose a joint disparity-parallax computational model that predicts apparent depth resulting from both cues. We demonstrate how this model can be applied in the context of stereo and multiscopic image processing, and propose new disparity manipulation techniques, which first quantify depth obtained from motion parallax, and then adjust binocular disparity information accordingly. This allows us to manipulate the disparity signal according to the strength of motion parallax to improve the overall depth reproduction. This technique is validated in additional experiments. © 2016 Copyright held by the owner/author(s).","Eye tracking; Gaze tracking; Gaze-contingent display; Remapping; Retargeting; Stereoscopic 3D","Binoculars; Depth perception; Eye tracking; Geometrical optics; Three dimensional computer graphics; Additional experiments; Binocular disparity; Gaze-contingent displays; Manipulation techniques; Psychovisual experiments; Remapping; Retargeting; Stereoscopic 3d; Stereo image processing",Article,"Final","",Scopus,2-s2.0-85043522963
"Hajraoui A., Sabri M.","57193499712;56377504700;","Generic and robust method for head pose estimation",2016,"Indonesian Journal of Electrical Engineering and Computer Science","4","2",,"439","446",,1,"10.11591/ijeecs.v4.i2.pp439-446","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014388861&doi=10.11591%2fijeecs.v4.i2.pp439-446&partnerID=40&md5=23a1dc48d36dd8380e7bbb76e4fb8330","Faculty of Science and Technology, University Sultan Moulay Slimane, Beni Mellal, Morocco","Hajraoui, A., Faculty of Science and Technology, University Sultan Moulay Slimane, Beni Mellal, Morocco; Sabri, M., Faculty of Science and Technology, University Sultan Moulay Slimane, Beni Mellal, Morocco","Head pose estimation has fascinated the research community due to its application in facial motion capture, human-computer interaction and video conferencing. It is a pre-requisite to gaze tracking, face recognition, and facial expression analysis. In this paper, we present a generic and robust method for model-based global 2D head pose estimation from single RGB Image. In our approach we use of the one part the Gabor filters to conceive a robust pose descriptor to illumination and facial expression variations, and that target the pose information. Moreover, we ensure the classification of these descriptors using a SVM classifier. The approach has proved effective view the rate for the correct pose estimations that we got. © 2016 Institute of Advanced Engineering and Science. All rights reserved.","Face detection; Gabor filters; PCA-LDA; Pose estimation; SVM",,Article,"Final","",Scopus,2-s2.0-85014388861
"Liang H., Liang R., Sun G.","56021740200;55622054000;55750131400;","Looking Into Saliency Model via Space-Time Visualization",2016,"IEEE Transactions on Multimedia","18","11","7576650","2271","2281",,7,"10.1109/TMM.2016.2613681","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993993254&doi=10.1109%2fTMM.2016.2613681&partnerID=40&md5=8a21da76d85cde1fc290fc77a0dabc3b","College of Information Engineering, Zhejiang University of Technology, Hangzhou, 310013, China","Liang, H., College of Information Engineering, Zhejiang University of Technology, Hangzhou, 310013, China; Liang, R., College of Information Engineering, Zhejiang University of Technology, Hangzhou, 310013, China; Sun, G., College of Information Engineering, Zhejiang University of Technology, Hangzhou, 310013, China","We introduce a visual analytics method to analyze eye-tracking data and saliency models for dynamic stimuli, such as video or animated graphics. The focus lies on the analysis of the different performance of saliency models in contrast to human observers to identify trends in the general viewing behavior, including time sequences of attentional synchrony and objects with a strong attentional focus. By using a space-time cube visualization in combination with clustering, the dynamic stimuli and associated eye gazes as well as the attention maps from saliency models can be analyzed in a static three-dimensional representation. We propose algorithms to keep the appearance of the computer's attention data in line with the human's eye-tracking data. The analytical process is supported by multiple coordinated views that allow the user to focus on different aspects of spatial and temporal information in eye gaze data and saliency map. By comparing attention data from both human and computer incorporated with the spatiotemporal characteristics, we are able to find the different patterns within human and computer algorithms. We list our key findings to help developing better saliency detection algorithms. © 2016 IEEE.","Saliency model; spatiotemporal analysis; visualization","Flow visualization; Visualization; Analytical process; Dimensional representation; Multiple coordinated views; Saliency detection; Saliency modeling; Spatiotemporal analysis; Spatiotemporal characteristics; Temporal information; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-84993993254
"Fuhl W., Tonsen M., Bulling A., Kasneci E.","56770084800;57189844743;6505807414;56059892600;","Pupil detection for head-mounted eye tracking in the wild: an evaluation of the state of the art",2016,"Machine Vision and Applications","27","8",,"1275","1288",,76,"10.1007/s00138-016-0776-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976276920&doi=10.1007%2fs00138-016-0776-4&partnerID=40&md5=19ab518f1e9d35d0f0246aeef9095668","Perception Engineering Group, University of Tübingen, Tübingen, Germany; Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany","Fuhl, W., Perception Engineering Group, University of Tübingen, Tübingen, Germany; Tonsen, M., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Bulling, A., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Kasneci, E., Perception Engineering Group, University of Tübingen, Tübingen, Germany","Robust and accurate detection of the pupil position is a key building block for head-mounted eye tracking and prerequisite for applications on top, such as gaze-based human–computer interaction or attention analysis. Despite a large body of work, detecting the pupil in images recorded under real-world conditions is challenging given significant variability in the eye appearance (e.g., illumination, reflections, occlusions, etc.), individual differences in eye physiology, as well as other sources of noise, such as contact lenses or make-up. In this paper we review six state-of-the-art pupil detection methods, namely ElSe (Fuhl et al. in Proceedings of the ninth biennial ACM symposium on eye tracking research & applications, ACM. New York, NY, USA, pp 123–130, 2016), ExCuSe (Fuhl et al. in Computer analysis of images and patterns. Springer, New York, pp 39–51, 2015), Pupil Labs (Kassner et al. in Adjunct proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing (UbiComp), pp 1151–1160, 2014. doi:10.1145/2638728.2641695), SET (Javadi et al. in Front Neuroeng 8, 2015), Starburst (Li et al. in Computer vision and pattern recognition-workshops, 2005. IEEE Computer society conference on CVPR workshops. IEEE, pp 79–79, 2005), and Świrski (Świrski et al. in Proceedings of the symposium on eye tracking research and applications (ETRA). ACM, pp 173–176, 2012. doi:10.1145/2168556.2168585). We compare their performance on a large-scale data set consisting of 225,569 annotated eye images taken from four publicly available data sets. Our experimental results show that the algorithm ElSe (Fuhl et al. 2016) outperforms other pupil detection methods by a large margin, offering thus robust and accurate pupil positions on challenging everyday eye images. © 2016, Springer-Verlag Berlin Heidelberg.","Computer vision; Data set; Head-mounted eye tracking; Image processing; Pupil detection","Aluminum; Computer vision; Human computer interaction; Image processing; Ubiquitous computing; Computer analysis; Computer interaction; Data set; Head-mounted eye tracking; Individual Differences; Large scale data sets; Pupil detection; Research and application; Pattern recognition",Article,"Final","",Scopus,2-s2.0-84976276920
"Zhang W., Wang H., Zhao F.","57192120079;56591846700;55087079700;","Improved eye center location system in low-resolution images",2016,"2016 IEEE/CIC International Conference on Communications in China, ICCC 2016",,,"7636835","","",,1,"10.1109/ICCChina.2016.7636835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997771244&doi=10.1109%2fICCChina.2016.7636835&partnerID=40&md5=1be3dd1ea4a6adb7a9d20da32126b52d","School of Information and Telecommunication Institute, Beijing University of Post and Telecommunication, China","Zhang, W., School of Information and Telecommunication Institute, Beijing University of Post and Telecommunication, China; Wang, H., School of Information and Telecommunication Institute, Beijing University of Post and Telecommunication, China; Zhao, F., School of Information and Telecommunication Institute, Beijing University of Post and Telecommunication, China","The position of the eyes contains many valuable information, which is used in a wide range of applications such as face recognition or eye tracking. However, most commercial eye-gaze trackers are often expensive and require the user to be equipped with a head mounted device which is inconvenient and unattractive. In order to locate eye centers solely on appearance, several methods have been proposed in the literature, but these methods often fail to accurately locate the eye centers in low-resolution images taken from a simple webcam. Therefore, we propose an improved system for accurate and robust eye center location by combining variance projection and ellipse fitting. First we utilize variance projection to detect the candidate region of the eyes. In this step, for eyeglass wearers, we proposed a method to remove the disruption of the glasses. Then least squares ellipse fitting is used to locate the accurate eye center. In this paper, we extensively test our system for its accuracy and robustness to changes in illumination, head pose and eye rotation. We demonstrate that our system outperforms conventional ones. © 2016 IEEE.","Ellipse fitting; Eye center location; Integral projection; Variance Projection","Face recognition; Geometry; Ellipse fitting; Eye center locations; Eye gaze trackers; Eye-tracking; Integral projections; Least Square; Low resolution images; Variance projections; Location",Conference Paper,"Final","",Scopus,2-s2.0-84997771244
"Sugano Y., Zhang X., Bulling A.","7005470045;57142162900;6505807414;","AggreGaze: Collective estimation of audience attention on public displays",2016,"UIST 2016 - Proceedings of the 29th Annual Symposium on User Interface Software and Technology",,,,"821","831",,31,"10.1145/2984511.2984536","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995812825&doi=10.1145%2f2984511.2984536&partnerID=40&md5=feaef9c16517b376f62189156febd637","Max Planck Institute for Informatics, Germany","Sugano, Y., Max Planck Institute for Informatics, Germany; Zhang, X., Max Planck Institute for Informatics, Germany; Bulling, A., Max Planck Institute for Informatics, Germany","Gaze is frequently explored in public display research given its importance for monitoring and analysing audience attention. However, current gaze-enabled public display interfaces require either special-purpose eye tracking equipment or explicit personal calibration for each individual user. We present AggreGaze, a novel method for estimating spatio-temporal audience attention on public displays. Our method requires only a single off-the-shelf camera attached to the display, does not require any personal calibration, and provides visual attention estimates across the full display. We achieve this by 1) compensating for errors of state-of-the-art appearance-based gaze estimation methods through on-site training data collection, and by 2) aggregating uncalibrated and thus inaccurate gaze estimates of multiple users into joint attention estimates. We propose different visual stimuli for this compensation: a standard 9-point calibration, moving targets, text and visual stimuli embedded into the display content, as well as normal video content. Based on a two-week deployment in a public space, we demonstrate the effectiveness of our method for estimating attention maps that closely resemble ground-truth audience gaze distributions.","Eye tracking; Gaze estimation; Public displays; Visual attention","Behavioral research; Calibration; Estimation; Appearance based; Eye-tracking; Gaze estimation; Joint attention; On-site training; Public display; State of the art; Visual Attention; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-84995812825
"Lee J., Cheon M., Moon S.-E., Lee J.-S.","57188765223;55938054900;56452144400;36062379400;","Peripersonal space in virtual reality: Navigating 3D space with different perspectives",2016,"UIST 2016 Adjunct - Proceedings of the 29th Annual Symposium on User Interface Software and Technology",,,,"207","208",,5,"10.1145/2984751.2984772","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995663200&doi=10.1145%2f2984751.2984772&partnerID=40&md5=73ff1da6777a34c1dd35b1cb7ad28559","School of Integrated Technology and Yonsei Institute of Convergence Technology, Yonsei University, South Korea","Lee, J., School of Integrated Technology and Yonsei Institute of Convergence Technology, Yonsei University, South Korea; Cheon, M., School of Integrated Technology and Yonsei Institute of Convergence Technology, Yonsei University, South Korea; Moon, S.-E., School of Integrated Technology and Yonsei Institute of Convergence Technology, Yonsei University, South Korea; Lee, J.-S., School of Integrated Technology and Yonsei Institute of Convergence Technology, Yonsei University, South Korea","We introduce the concept of ""peripersonal space"" of an avatar in 3D virtual reality and discuss how it plays an important role on 3D navigation with different perspectives. By analyzing the eye-gaze data of avatar-based navigation with first-person perspective and third-person perspective, we examine the effects of an avatar's peripersonal space on the users' perceptual scopes within 3D virtual environments. We propose that manipulating peripersonal space of an avatar with various perspectives has the immediate effects on the users' scopes of perception as well as the patterns of attentional capture. This study provides a helpful guideline for designing more effective navigation system with an avatar in 3D virtual environment. © 2016 Copyright held by the author/owner(s).","Eye-tracking; Gaze Analysis; Human Perception and Cognition; Navigation; Peripersonal Space; Perspective; Virtual Space","Navigation; Navigation systems; Three dimensional computer graphics; User interfaces; Eye-tracking; Gaze analysis; Human perception; Peripersonal Space; Perspective; Virtual spaces; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-84995663200
"Wang Y., Shen T., Yuan G., Bian J., Fu X.","55211773900;57191893663;36877169400;57200854878;7402204912;","Appearance-based gaze estimation using deep features and random forest regression",2016,"Knowledge-Based Systems","110",,,"293","301",,33,"10.1016/j.knosys.2016.07.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994551714&doi=10.1016%2fj.knosys.2016.07.038&partnerID=40&md5=cc5b8620708ea820bcf350fce6d7eb1e","School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Wang, Y., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China, Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Shen, T., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Yuan, G., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Bian, J., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Conventional appearance-based gaze estimation methods employ local or global features as eye gaze appearance descriptor. But these methods don't work well under natural light with free head movement. To solve this problem, we present an appearance-based gaze estimation method using deep feature representation and feature forest regression. The deep feature is learned through hierarchical extraction of deep Convolutional Neural Network (CNN). And random forest regression with cluster-to-classify node splitting rules is used to take advantage of data distribution in sparse feature space. Experimental results demonstrate that the deep feature has a better performance than local features on calibrated gaze regression. The combination of deep features and random forest regression provides an effective solution for gaze estimation in a natural environment. © 2016","Appearance; CNN; Deep features; Gaze estimation; Random forest","Decision trees; Neural networks; Appearance; Convolutional neural network; Deep features; Feature representation; Gaze estimation; Hierarchical extraction; Natural environments; Random forests; Regression analysis",Article,"Final","",Scopus,2-s2.0-84994551714
"Lallé S., Conati C., Carenini G.","55028192000;6602976668;6604008365;","Prediction of individual learning curves across information visualizations",2016,"User Modelling and User-Adapted Interaction","26","4",,"307","345",,13,"10.1007/s11257-016-9179-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986261703&doi=10.1007%2fs11257-016-9179-5&partnerID=40&md5=844a9123a29b8f095b0f9ae079471bf5","Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver, BC  V6T 1Z4, Canada","Lallé, S., Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver, BC  V6T 1Z4, Canada; Conati, C., Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver, BC  V6T 1Z4, Canada; Carenini, G., Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver, BC  V6T 1Z4, Canada","Confident usage of information visualizations is thought to be influenced by cognitive aspects as well as amount of exposure and training. To support the development of individual competency in visualization processing, it is important to ascertain if we can track users’ progress or difficulties they might have while working with a given visualization. In this paper, we extend previous work on predicting in real time a user’s learning curve—a mathematical model that can represent a user’s skill acquisition ability—when working with a visualization. First, we investigate whether results we previously obtained in predicting users’ learning curves during visualization processing generalize to a different visualization. Second, we study to what extent we can make predictions on a user’s learning curve without information on the visualization being used. Our models leverage various data sources, including a user’s gaze behavior, pupil dilation, and cognitive abilities. We show that these models outperform a baseline that leverages knowledge on user task performance so far. Our best performing model achieves good accuracies in predicting users’ learning curves even after observing users’ performance on a few tasks only. These results represent an important step toward understanding how to support users in learning a new visualization. © 2016, Springer Science+Business Media Dordrecht.","Adaptive visualization; Eye tracking; Information visualization; Learning curve; Machine learning; User modeling","Artificial intelligence; Curve fitting; Forecasting; Information analysis; Information science; Information systems; Learning systems; Personnel training; Adaptive visualization; Eye-tracking; Information visualization; Learning curves; User Modeling; Visualization",Article,"Final","",Scopus,2-s2.0-84986261703
"Xiong C., Huang L., Liu C.","56097664100;56566995800;9637739900;","Remote gaze estimation based on 3D face structure and iris centers under natural light",2016,"Multimedia Tools and Applications","75","19",,"11785","11799",,4,"10.1007/s11042-015-2600-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928330693&doi=10.1007%2fs11042-015-2600-y&partnerID=40&md5=9a7a50ca00e9cae33c371a2e054c4691","Institute of Automation, Chinese Academy of Sciences, No. 95, Zhongguancun East Road, Beijing, Haidian District  100190, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","Xiong, C., Institute of Automation, Chinese Academy of Sciences, No. 95, Zhongguancun East Road, Beijing, Haidian District  100190, China; Huang, L., Institute of Automation, Chinese Academy of Sciences, Beijing, China; Liu, C., Institute of Automation, Chinese Academy of Sciences, Beijing, China","Remote gaze estimation under natural light is still a challenging problem. Appearance based methods are seriously sensitive to illumination variation in the visual spectrum and usually can hardly handle the problem of head movements. And most existing feature-based gaze estimation methods strongly rely on cornea reflections, which are unstable to glasses, head movements and especially useless for natural light condition. In this paper, we propose a novel feature based gaze estimation method without use of cornea reflections. A stereo camera system is built for the proposed method. Firstly, 3D Active Shape Models (ASM) is reconstructed using stereo vision to represent 3D face structure. Then, without use of cornea reflections, a 3D Iris-Eye-Contours based descriptor is proposed to represent human gaze information. Iris centers are used in natural light just like the pupil centers in condition of near-infrared light. What’s more, precise estimation of head poses based on 3D face structure is employed to rectify the 3D iris centers and eye contours for improving the ability of tolerance to head movements. Experiments on several subjects show that the system is accurate and allows natural head movements under natural light. © 2015, Springer Science+Business Media New York.","3D face structure; Gaze estimation; Head pose estimation; Iris center location","Eye movements; Face recognition; Image recognition; Infrared devices; Stereo image processing; Stereo vision; Three dimensional computer graphics; 3d active shape models; 3D faces; Appearance-based methods; Center locations; Gaze estimation; Head Pose Estimation; Illumination variation; Remote gaze estimation; Motion estimation",Article,"Final","",Scopus,2-s2.0-84928330693
"West R., Kajihara M., Parola M., Holloway M., Hays K., Hillard L., Carlew A., Deutsch J., Lane B., John B., Sanandaji A., Grimm C.","7402395594;57189851081;56605992000;56315401900;6701805230;57193122748;56951228700;57189850027;57193122646;57205639875;55847420100;7101846420;","Eliciting tacit expertise in 3D volume segmentation",2016,"ACM International Conference Proceeding Series",,,,"59","66",,2,"10.1145/2968220.2968235","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010950710&doi=10.1145%2f2968220.2968235&partnerID=40&md5=6155265013cfe7c126774de72df21082","Univ. of North Texas, United States; Wash. Univ. in St. Louis, United States; Rochester Inst. of Tech., United States; Oregon State University, United States","West, R., Univ. of North Texas, United States; Kajihara, M., Univ. of North Texas, United States; Parola, M., Univ. of North Texas, United States; Holloway, M., Wash. Univ. in St. Louis, United States; Hays, K., Univ. of North Texas, United States; Hillard, L., Univ. of North Texas, United States; Carlew, A., Univ. of North Texas, United States; Deutsch, J., Univ. of North Texas, United States; Lane, B., Rochester Inst. of Tech., United States; John, B., Univ. of North Texas, United States; Sanandaji, A., Oregon State University, United States; Grimm, C., Oregon State University, United States","The output of 3D volume segmentation is crucial to a wide range of endeavors. Producing accurate segmentations often proves to be both ineficient and challenging, in part due to lack of imaging data quality (contrast and resolution), and because of ambiguity in the data that can only be resolved with higher-level knowledge of the structure and the context wherein it resides. Automatic and semi-Automatic approaches are improving, but in many cases still fail or require substantial manual clean-up or intervention. Expert manual segmentation and review is therefore still the gold standard for many applications. Unfortunately, existing tools (both custom-made and commercial) are often designed based on the underlying algorithm, not the best method for expressing higher-level intention. Our goal is to analyze manual (or semi-Automatic) segmentation to gain a better understanding of both low-level (perceptual tasks and actions) and high-level decision making. This can be used to produce segmentation tools that are more accurate, effcient, and easier to use. Questioning or observation alone is insu ffcient to capture this information, so we utilize a hybrid capture protocol that blends observation, surveys, and eye tracking. We then developed, and validated, data coding schemes capable of discerning low-level actions and overall task structures. © 2016 Copyright.","3D volume segmentation; Conceptual framework","Decision making; Conceptual frameworks; Data coding schemes; Gold standards; Manual segmentation; Segmentation tool; Semi-automatics; Task structure; Volume segmentation; Visual communication",Conference Paper,"Final","",Scopus,2-s2.0-85010950710
"Barz M., Sonntag D.","57189847803;12241487800;","Gaze-guided object classification using deep neural networks for attention-based computing",2016,"UbiComp 2016 Adjunct - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing",,,,"253","256",,15,"10.1145/2968219.2971389","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991069525&doi=10.1145%2f2968219.2971389&partnerID=40&md5=381bb812a59fb68a93bfb4b8d764f6bb","German Research Center for Artificial Intelligence, Stuhlsatzenhausweg 3, Saarbruecken, 66123, Germany","Barz, M., German Research Center for Artificial Intelligence, Stuhlsatzenhausweg 3, Saarbruecken, 66123, Germany; Sonntag, D., German Research Center for Artificial Intelligence, Stuhlsatzenhausweg 3, Saarbruecken, 66123, Germany","Recent advances in eye tracking technologies opened the way to design novel attention-based user interfaces. This is promising for pro-Active and assistive technologies for cyber-physical systems in the domains of, e.g., healthcare and industry 4.0. Prior approaches to recognize a user's attention are usually limited to the raw gaze signal or sensors in instrumented environments. We propose a system that (1) incorporates the gaze signal and the egocentric camera of the eye tracker to identify the objects the user focuses at; (2) employs object classification based on deep learning which we recompiled for our purposes on a GPU-based image classification server; (3) detects whether the user actually draws attention to that object; and (4) combines these modules for constructing episodic memories of egocentric events in real-Time. © 2016 ACM.","Eye Tracking; Gaze-Based Interaction; Object Classification; Visual Attention","Behavioral research; Embedded systems; Stereo vision; Ubiquitous computing; User interfaces; Assistive technology; Cyber physical systems (CPSs); Eye tracking technologies; Eye-tracking; Gaze-based interaction; Instrumented environments; Object classification; Visual Attention; Image classification",Conference Paper,"Final","",Scopus,2-s2.0-84991069525
"Kit D., Sullivan B.","54793175600;55747311600;","Classifying mobile eye tracking data with hidden Markov models",2016,"Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct, MobileHCI 2016",,,,"1037","1040",,4,"10.1145/2957265.2965014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991109090&doi=10.1145%2f2957265.2965014&partnerID=40&md5=7535ca297356f42b803a231f9ac79c10","Department of Computer Science, Bath University, Bath, BA2 7AY, United Kingdom; Department of Experimental Psychology, University of Bristol, Bristol, BS8 1TH, United Kingdom","Kit, D., Department of Computer Science, Bath University, Bath, BA2 7AY, United Kingdom; Sullivan, B., Department of Experimental Psychology, University of Bristol, Bristol, BS8 1TH, United Kingdom","Naturalistic eye movement behavior has been measured in a variety of scenarios [15] and eye movement patterns appear indicative of task demands [16]. However, systematic task classification of eye movement data is a relatively recent development [1,3,7]. Additionally, prior work has focused on classification of eye movements while viewing 2D screen based imagery. In the current study, eye movements from eight participants were recorded with a mobile eye tracker. Participants performed five everyday tasks: Making a sandwich, transcribing a document, walking in an office and a city street, and playing catch with a flying disc [14]. Using only saccadic direction and amplitude time series data, we trained a hidden Markov model for each task and classified unlabeled data by calculating the probability that each model could generate the observed sequence. We present accuracy and time to recognize results, demonstrating better than chance performance.","Machine learning; Mobile eye tracking; Natural tasks; Task classification","Artificial intelligence; Classification (of information); Hidden Markov models; Human computer interaction; Learning systems; Markov processes; Mobile devices; Trellis codes; Eye movement datum; Eye movement patterns; Mobile eye-tracking; Movement behavior; Natural tasks; Task classification; Time-series data; Unlabeled data; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84991109090
"Toivanen M., Häkkinen J., Puolamäki K., Radun J., Lukander K.","11840148200;7007052231;12141481400;12760114200;34976891300;","Inferring user action with mobile gaze tracking",2016,"Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct, MobileHCI 2016",,,,"1026","1028",,1,"10.1145/2957265.2965016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991093290&doi=10.1145%2f2957265.2965016&partnerID=40&md5=2287aca7b8c4191127b85260c9f02af0","Finnish Institute of Occupational Health, PO Box 40, Helsinki, FI-00251, Finland; University of Helsinki, Institute of Behavioural Sciences, Helsinki, Finland","Toivanen, M., Finnish Institute of Occupational Health, PO Box 40, Helsinki, FI-00251, Finland; Häkkinen, J., University of Helsinki, Institute of Behavioural Sciences, Helsinki, Finland; Puolamäki, K., Finnish Institute of Occupational Health, PO Box 40, Helsinki, FI-00251, Finland; Radun, J., University of Helsinki, Institute of Behavioural Sciences, Helsinki, Finland; Lukander, K., Finnish Institute of Occupational Health, PO Box 40, Helsinki, FI-00251, Finland","Gaze tracking in psychological, cognitive, and user interaction studies has recently evolved toward mobile solutions, as they enable direct assessing of users' visual attention in natural environments, and augmented and virtual reality (AR/VR) applications. Productive approaches in analyzing and predicting user actions with gaze data require a multi-disciplinary approach with experts in cognitive and behavioral sciences, machine vision, and machine learning. This workshop brings together a cross-domain group of individuals to (i) discuss and contribute to the problem of using mobile gaze tracking for inferring user action, (ii) advance the sharing of data and analysis algorithms as well as device solutions, and (iii) increase understanding of behavioral aspects of gaze-action sequences in natural environments and AR/VR applications.","Action inference; Augmented reality; Behavioral analysis; Gaze tracking algorithms; Machine learning; Mobile gaze tracking; Natural environments; Virtual reality","Artificial intelligence; Augmented reality; Behavioral research; Computer vision; Inference engines; Learning systems; Mobile devices; Tracking (position); Virtual reality; Action inference; Analysis algorithms; Augmented and virtual realities; Behavioral analysis; Behavioral science; Gaze tracking; Multi-disciplinary approach; Natural environments; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84991093290
"Liu J.-W., Sun W.-P., Xia T.","56473696400;16319648000;7101653310;","Adaptive structured sub-blocks tracking",2016,"Neurocomputing","204",,,"97","105",,1,"10.1016/j.neucom.2015.10.133","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973151760&doi=10.1016%2fj.neucom.2015.10.133&partnerID=40&md5=9273300a6d4d4cc575e041580db4b411","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","Liu, J.-W., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Sun, W.-P., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Xia, T., School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","Visual object tracking algorithms based on middle level appearance have been widely studied for their effective representation to non-rigid appearance variation and partial occlusion. Sub-blocks are often adopted as local feature in mid-level based tracking algorithms. How to select representative sub-blocks to reveal the spatial structure of objects and retain the flexibility to model non-rigid deformation has not been adequately addressed. Exploiting discrimination, uniqueness and historical prediction accuracy of sub-blocks of a target, we propose a local feature selection method which includes rough initial subblock selection and refined subblock-sample particle bi-directional selection under particle filter tracking framework. A quantitative evaluation is conducted on 10 sequences. Experimental results show the robustness of our proposed algorithm in tackling with non-rigid deformation and partial occlusion. © 2016 Elsevier B.V.","Particle filter; Structured sub-blocks; Visual object tracking","Algorithms; Deformation; Monte Carlo methods; Target tracking; Tracking (position); Local feature selections; Non-rigid deformation; Partial occlusions; Particle filter; Prediction accuracy; Quantitative evaluation; Sub-blocks; Visual object tracking; Rigid structures; algorithm; Article; conceptual framework; experimental study; eye tracking; mathematical computing; measurement accuracy; prediction; priority journal; quantitative analysis; spatial discrimination; structure analysis; visual discrimination",Article,"Final","",Scopus,2-s2.0-84973151760
"Lu F., Gao Y., Chen X.","54956194300;35298851400;13410318100;","Estimating 3D gaze directions using unlabeled eye images via synthetic iris appearance fitting",2016,"IEEE Transactions on Multimedia","18","9","7484318","1772","1782",,14,"10.1109/TMM.2016.2576284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983441879&doi=10.1109%2fTMM.2016.2576284&partnerID=40&md5=4d505866d1cf97548588da92a35c6766","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; School of Software, Tsinghua University, Beijing, 100084, China","Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China, International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; Gao, Y., School of Software, Tsinghua University, Beijing, 100084, China; Chen, X., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China","Estimating three-dimensional (3D) human eye gaze by capturing a single eye image without active illumination is challenging. Although the elliptical iris shape provides a useful cue, existing methods face difficulties in ellipse fitting due to unreliable iris contour detection. These methods may fail frequently especially with low resolution eye images. In this paper, we propose a synthetic iris appearance fitting (SIAF) method that is model-driven to compute 3D gaze direction from iris shape. Instead of fitting an ellipse based on exactly detected iris contour, our method first synthesizes a set of physically possible iris appearances and then optimizes inside this synthetic space to find the best solution to explain the captured eye image. In this way, the solution is highly constrained and guaranteed to be physically feasible. In addition, the proposed advanced image analysis techniques also help the SIAF method be robust to the unreliable iris contour detection. Furthermore, with multiple eye images, we propose a SIAF-joint method that can further reduce the gaze error by half, and it also resolves the binary ambiguity which is inevitable in conventional methods based on simple ellipse fitting. © 1999-2012 IEEE.","Gaze estimation; iris fitting; three-dimensional (3D) human gaze; unlabeled eye images","Multimedia systems; Signal processing; Active illumination; Contour detection; Conventional methods; Eye images; Gaze estimation; Image analysis techniques; iris fitting; Threedimensional (3-d); Geometry",Article,"Final","",Scopus,2-s2.0-84983441879
"Stuijfzand B.G., Van Der Schaaf M.F., Kirschner F.C., Ravesloot C.J., Van Der Gijp A., Vincken K.L.","57188825055;8617998100;25638944500;55521289400;56003868500;6701588462;","Medical students' cognitive load in volumetric image interpretation: Insights from human-computer interaction and eye movements",2016,"Computers in Human Behavior","62",,,"394","403",,21,"10.1016/j.chb.2016.04.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963705524&doi=10.1016%2fj.chb.2016.04.015&partnerID=40&md5=e09dcc1b7051f33121cf86201df918f1","Department of Education, Utrecht University, Heidelberglaan 1, Utrecht, 3584 CS, Netherlands; Department of Radiology, University Medical Centre Utrecht, Heidelberglaan 100, Utrecht, 3584 CX, Netherlands; Image Sciences Institute, University Medical Centre Utrecht, Heidelberglaan 100, Utrecht, 3584 CX, Netherlands; School of Experimental Psychology, University of Bristol, 12a Priory Road, Bristol, BS8 1TU, United Kingdom","Stuijfzand, B.G., Department of Education, Utrecht University, Heidelberglaan 1, Utrecht, 3584 CS, Netherlands, School of Experimental Psychology, University of Bristol, 12a Priory Road, Bristol, BS8 1TU, United Kingdom; Van Der Schaaf, M.F., Department of Education, Utrecht University, Heidelberglaan 1, Utrecht, 3584 CS, Netherlands; Kirschner, F.C., Department of Education, Utrecht University, Heidelberglaan 1, Utrecht, 3584 CS, Netherlands; Ravesloot, C.J., Department of Radiology, University Medical Centre Utrecht, Heidelberglaan 100, Utrecht, 3584 CX, Netherlands; Van Der Gijp, A., Department of Radiology, University Medical Centre Utrecht, Heidelberglaan 100, Utrecht, 3584 CX, Netherlands; Vincken, K.L., Image Sciences Institute, University Medical Centre Utrecht, Heidelberglaan 100, Utrecht, 3584 CX, Netherlands","Medical image interpretation is moving from using 2D- to volumetric images, thereby changing the cognitive and perceptual processes involved. This is expected to affect medical students' experienced cognitive load, while learning image interpretation skills. With two studies this explorative research investigated whether measures inherent to image interpretation, i.e. human-computer interaction and eye tracking, relate to cognitive load. Subsequently, it investigated effects of volumetric image interpretation on second-year medical students' cognitive load. Study 1 measured human-computer interactions of participants during two volumetric image interpretation tasks. Using structural equation modelling, the latent variable 'volumetric image information' was identified from the data, which significantly predicted self-reported mental effort as a measure of cognitive load. Study 2 measured participants' eye movements during multiple 2D and volumetric image interpretation tasks. Multilevel analysis showed that time to locate a relevant structure in an image was significantly related to pupil dilation, as a proxy for cognitive load. It is discussed how combining human-computer interaction and eye tracking allows for comprehensive measurement of cognitive load. Combining such measures in a single model would allow for disentangling unique sources of cognitive load, leading to recommendations for implementation of volumetric image interpretation in the medical education curriculum. © 2016 Elsevier Ltd. All rights reserved.","Cognitive load; Eye tracking; Human-computer interaction; Medical education; Volumetric image interpretation","Curricula; Education; Education computing; Eye movements; Image analysis; Medical education; Medical imaging; Students; Cognitive loads; Comprehensive measurement; Education curriculums; Eye-tracking; Image interpretation; Multi-level analysis; Structural equation modelling; Volumetric images; Human computer interaction",Article,"Final","",Scopus,2-s2.0-84963705524
"Ahn S., Kim J., Kim H., Lee S.","57191159065;56822525300;43361429400;55746172800;","Visual attention analysis on stereoscopic images for subjective discomfort evaluation",2016,"Proceedings - IEEE International Conference on Multimedia and Expo","2016-August",,"7552998","","",,2,"10.1109/ICME.2016.7552998","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987617968&doi=10.1109%2fICME.2016.7552998&partnerID=40&md5=fa50b4fdfe3f427ee1a6801c562286a0","Department of Electrical and Electronic Engineering, Yonsei University, Seoul, 120-749, South Korea","Ahn, S., Department of Electrical and Electronic Engineering, Yonsei University, Seoul, 120-749, South Korea; Kim, J., Department of Electrical and Electronic Engineering, Yonsei University, Seoul, 120-749, South Korea; Kim, H., Department of Electrical and Electronic Engineering, Yonsei University, Seoul, 120-749, South Korea; Lee, S., Department of Electrical and Electronic Engineering, Yonsei University, Seoul, 120-749, South Korea","By analyzing the statistical behaviors on human visual attention, we discover a clue that the fixation behaviors are highly correlated with how much the viewers feel visual discomfort on stereoscopic images differently from conventional subjective assessments. In order to quantify the correlation between visual attention and discomfort, we explore a novel methodology termed transition of visual attention (ToVA) according to various disparities, which accounts depth attributes of 3D images by eye-tracker experiments. Moreover, the saliency entropy is defined to quantify the distribution of fixations for 3D images. Then, we measure ToVA in terms of the relative saliency entropy using Kullback-Leibler divergence. In order to evaluate the effectiveness of ToVA, a successful example application is also provided, whereby ToVA is applied to obtaining subjective results of measuring discomfort experienced when viewing 3D displays rather than relying on the conventional subjective test by using scoring system. © 2016 IEEE.","eye-tracker; saliency entropy; Stereoscopic; subjective discomfort assessment; transition of visual attention","Entropy; Eye tracking; Image analysis; Stereo image processing; Three dimensional displays; Eye trackers; Human visual attention; Kullback Leibler divergence; Statistical behavior; Stereoscopic; Subjective assessments; subjective discomfort assessment; Visual Attention; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-84987617968
"Li J., Ngai G., Leong H.V., Chan S.C.F.","56433022500;8915594400;7005127948;7404255433;","Your Eye Tells How Well You Comprehend",2016,"Proceedings - International Computer Software and Applications Conference","2",,"7552263","503","508",,8,"10.1109/COMPSAC.2016.220","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987994831&doi=10.1109%2fCOMPSAC.2016.220&partnerID=40&md5=b261dd6ed680297205c258c9f0543b94","Department of Computing, Hong Kong Polytechnic University, Hong Kong","Li, J., Department of Computing, Hong Kong Polytechnic University, Hong Kong; Ngai, G., Department of Computing, Hong Kong Polytechnic University, Hong Kong; Leong, H.V., Department of Computing, Hong Kong Polytechnic University, Hong Kong; Chan, S.C.F., Department of Computing, Hong Kong Polytechnic University, Hong Kong","Systems that adapt to changes in human needs automatically are useful, built upon advancements in human-computer interaction research. In this paper, we investigate the problem of how well the eye movement of a user when reading an article can predict the level of reading comprehension, which could be exploited in intelligent adaptive e-learning systems. We characterize the eye movement pattern in the form of eye gaze signal. We invite human subjects in reading articles of different difficulty levels being induced to different comprehension levels. Machine-learning techniques are applied to identify useful features to recognize when readers are experiencing difficulties in understanding their reading material. Finally, a detection model that can identify different levels of user comprehension is built. We achieve a performance improvement of over 30% above the baseline, translating over 50% reduction in detection error. © 2016 IEEE.","comprehension detection; eye gaze; reading","Application programs; Artificial intelligence; Computer software; E-learning; Human computer interaction; Learning systems; Adaptive e-learning systems; Detection models; Eye movement patterns; Eye-gaze; Human-computer interaction researches; Machine learning techniques; reading; Reading comprehension; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84987994831
"Wang J., Yu N., Zhu F., Zhuang L.","57192574481;57201634030;50862050900;16178485300;","Multi-level visual tracking with hierarchical tree structural constraint",2016,"Neurocomputing","202",,,"1","11",,2,"10.1016/j.neucom.2016.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979467670&doi=10.1016%2fj.neucom.2016.03.010&partnerID=40&md5=2884d561be3ae4f365f4213488b8f563","CAS Key Laboratory of Electromagnetic Space Information, University of Science and Technology of China, Hefei, China","Wang, J., CAS Key Laboratory of Electromagnetic Space Information, University of Science and Technology of China, Hefei, China; Yu, N., CAS Key Laboratory of Electromagnetic Space Information, University of Science and Technology of China, Hefei, China; Zhu, F., CAS Key Laboratory of Electromagnetic Space Information, University of Science and Technology of China, Hefei, China; Zhuang, L., CAS Key Laboratory of Electromagnetic Space Information, University of Science and Technology of China, Hefei, China","Recently, part-based model has drawn much attention in visual tracking for its promising results in handling occlusion and deformation. However how to divide the target into parts and how to model the relationships between parts are still open problems. In this paper, we propose a robust tracker based on multi-level target representation and hierarchical tree structural constraint. The multi-level target representation models the target at three different levels: the bounding box (top) level, the superpixel (middle) level and the keypoint (bottom) level. The relationships between parts at all levels are modeled by the proposed hierarchical tree which includes intra-layer and inter-layer structural constraints. The positions of all the parts are optimized jointly in a unified objective function taking into account both the appearance similarity and the hierarchical tree structural constraint. The appearance model and the hierarchical tree structure are updated online to adapt to the changes of the target in both appearance and structure. Extensive experiments on various challenging video sequences demonstrate that the proposed method outperforms the state-of-the-art trackers significantly. © 2016 Elsevier B.V.","Hierarchical tree; Multi-level representation; Object tracking; Part-based model; Structural constraint","Tracking (position); Trees (mathematics); Hierarchical tree; Multilevels; Object Tracking; Part-based models; Structural constraints; Forestry; algorithm; Article; eye tracking; geometry; hierarchical tree; image processing; mathematical computing; mathematical model; mathematical phenomena; online system; priority journal; process optimization; structural constraint; videorecording",Article,"Final","",Scopus,2-s2.0-84979467670
"Wang J., Wang Y.","55969143100;7601492538;","Multi-period visual tracking via online DeepBoost learning",2016,"Neurocomputing","200",,,"55","69",,5,"10.1016/j.neucom.2016.03.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964331149&doi=10.1016%2fj.neucom.2016.03.016&partnerID=40&md5=5c6baf9ca45f6c711f8a8aec1c02391b","School of Automation, Huazhong University of Science and Technology, Wuhan, 430074, China; National Key Lab of Science and Technology on Multi-spectral Information Processing, Wuhan, 430074, China","Wang, J., School of Automation, Huazhong University of Science and Technology, Wuhan, 430074, China; Wang, Y., School of Automation, Huazhong University of Science and Technology, Wuhan, 430074, China, National Key Lab of Science and Technology on Multi-spectral Information Processing, Wuhan, 430074, China","In this paper, we propose a novel accurate and robust boosting-style tracking-by-detection method. The proposed algorithm adopts a flexible and capacity-conscious object appearance model, which combines the strengths of both local and global visual representations. We firstly propose a joint local-global visual representation, in which main local and global spatial structure information of the target is flexibly embedded in the candidate classifier set with members from multiple complexity families. In addition, to avoid over-fitting our tracker adopts an effective online DeepBoost learning method (ODB). The key capacity-conscious ability of ODB helps to avoid over-fitting and generate a more adaptive and robust tracker. Furthermore, we propose a multi-period tracking framework (MPTF) to enhance the tracker's recovery ability for tracking failures. The proposed Multi-period DeepBoost-Tracker (MPDBT) can well encode the object spatial structures and excellently handle object appearance variations, and it can also recover from tracking failures with the help of the proposed MPTF. The experimental results demonstrate that our tracker outperforms the state-of-the-art trackers. © 2016 Elsevier B.V.","Joint local-global visual representation; Multi-period tracking framework; Online DeepBoost learning; Tracking-by-detection; Visual tracking","Classification (of information); Tracking (position); Multi-period; Online DeepBoost learning; Tracking by detections; Visual representations; Visual Tracking; E-learning; Article; controlled study; DeepBoost learning algorithm; eye tracking; human; illumination; image analysis; kernel method; learning algorithm; measurement accuracy; measurement precision; online system; priority journal; problem solving; systematic error; visual discrimination",Article,"Final","",Scopus,2-s2.0-84964331149
"Li J., Li S.","57203736355;16202805500;","Two-phase approach - Calibration and iris contour estimation - For gaze tracking of head-mounted eye camera",2016,"Proceedings - International Conference on Image Processing, ICIP","2016-August",,"7532937","3136","3140",,4,"10.1109/ICIP.2016.7532937","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006837027&doi=10.1109%2fICIP.2016.7532937&partnerID=40&md5=7bec9567395310f36155a6e61f21616c","Graduate School of Engineering, Tottori University, Tottori, Japan; Graduate School of Information Sciences, Hiroshima City University, Hiroshima, Japan","Li, J., Graduate School of Engineering, Tottori University, Tottori, Japan; Li, S., Graduate School of Information Sciences, Hiroshima City University, Hiroshima, Japan","The fitting of an ellipse to the iris contour is usually performed using five unknown parameters. In this study, we divide the continuous gaze estimation of a head-mounted eye camera into two phases. One phase, known as the calibration phase, is used to estimate the eyeball center position in relation to the coordinate system of the head-mounted eye camera. The other phase is used to fit the iris contour in 2D images employing only two parameters for gaze estimation. As seen from the experimental results, the proposed method demonstrates both credible eyeball center estimation and an accurate iris contour estimation in comparison with the conventional five unknown parameter approach. © 2016 IEEE.","Eyeball calibration; Gaze estimation; Head-mounted camera; Iris fitting","Calibration; Cameras; Image processing; Observability; Tracking (position); Center estimations; Co-ordinate system; Contour estimation; Gaze estimation; Gaze tracking; Head mounted Camera; Iris fitting; Two parameter; Parameter estimation",Conference Paper,"Final","",Scopus,2-s2.0-85006837027
"Vater S., León F.P.","56771184400;6602431857;","Combining isophote and cascade classifier information for precise pupil localization",2016,"Proceedings - International Conference on Image Processing, ICIP","2016-August",,"7532425","589","593",,3,"10.1109/ICIP.2016.7532425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006817524&doi=10.1109%2fICIP.2016.7532425&partnerID=40&md5=f3c2e1979930bcafbcafdc4e3a9d422e","Institute of Industrial Information Technology (IIIT), Karlsruhe Institute of Technology (KIT), Karlsruhe, 76187, Germany","Vater, S., Institute of Industrial Information Technology (IIIT), Karlsruhe Institute of Technology (KIT), Karlsruhe, 76187, Germany; León, F.P., Institute of Industrial Information Technology (IIIT), Karlsruhe Institute of Technology (KIT), Karlsruhe, 76187, Germany","This paper investigates precise pupil center localization in low-resolution images. Being an essential preprocessing step in many applications such as gaze estimation, face alignment as well as human-computer interaction, robust, precise, and efficient methods are necessary. We present a method for accurate eye center localization operating with images from simple off-the-shelf hardware such as webcams. The proposed method utilizes the isophote representation that allows to find pupil center candidates by introducing a novel voting mechanism for pixel weights. To cope with multiple local maxima resulting from the isophote voting map, we combine this information with quasi-continuous responses of a modified cascade classifier framework utilizing appearance-based features. We conduct experiments on the BioID database and show that the presented method outperforms results of existing methods within an error range of the pupil diameter while running at 10 fps on a standard CPU with 3.3 GHz in a Matlab implementation. © 2016 IEEE.","Eye localization; Feature extraction; Image processing; Machine vision; Object detection","Computer vision; Face recognition; Feature extraction; Human computer interaction; Image processing; Object detection; Cascade classifiers; Eye localization; Low resolution images; Off-the-shelf hardwares; Pre-processing step; Pupil center localizations; Pupil localization; Voting mechanism; Classification (of information)",Conference Paper,"Final","",Scopus,2-s2.0-85006817524
"Li J., Su L., Wu B., Pang J., Wang C., Wu Z., Huang Q.","56018090200;36626387200;55339913000;25823996400;57192573189;57191159554;8435766200;","Webpage saliency prediction with multi-features fusion",2016,"Proceedings - International Conference on Image Processing, ICIP","2016-August",,"7532442","674","678",,9,"10.1109/ICIP.2016.7532442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006815902&doi=10.1109%2fICIP.2016.7532442&partnerID=40&md5=edfb004a953a30ca53d23ce4aa51017a","Beijing University of Posts and Telecommunications, China; Key Lab on Big Data Mining and Knowledge Mgmt., University of Chinese Academy of Sciences, China; Key Lab of Intel. Inf. Proc., Inst. of Computing Teth., Chinese Academy of Sciences, China; Capital Medical University, China; College of Metropolitan Transportation, Beijing University of Technology, China","Li, J., Beijing University of Posts and Telecommunications, China; Su, L., Key Lab on Big Data Mining and Knowledge Mgmt., University of Chinese Academy of Sciences, China, Key Lab of Intel. Inf. Proc., Inst. of Computing Teth., Chinese Academy of Sciences, China; Wu, B., Capital Medical University, China; Pang, J., College of Metropolitan Transportation, Beijing University of Technology, China; Wang, C., Key Lab on Big Data Mining and Knowledge Mgmt., University of Chinese Academy of Sciences, China, Key Lab of Intel. Inf. Proc., Inst. of Computing Teth., Chinese Academy of Sciences, China; Wu, Z., Key Lab on Big Data Mining and Knowledge Mgmt., University of Chinese Academy of Sciences, China, Key Lab of Intel. Inf. Proc., Inst. of Computing Teth., Chinese Academy of Sciences, China; Huang, Q., Key Lab on Big Data Mining and Knowledge Mgmt., University of Chinese Academy of Sciences, China, Key Lab of Intel. Inf. Proc., Inst. of Computing Teth., Chinese Academy of Sciences, China","We proposed a novel model to predict human's visual attention when free-viewing webpages. Compared with natural images, webpages are usually full of salient regions such as logos, text, and faces, while few of them attract human's attention in a short sight. Moreover, webpages perform distinct viewing patterns which are quite different from the natural images. In this paper, we introduced multi-features according to our observation on webpages characters and related eye-tracking data. Further, in order to achieve a flexible adaptation to various types of webpages, we employed a machine-learning framework based on our proposed features. Experimental results demonstrate that our model outperforms other state-of-the-art methods in webpage saliency prediction. © 2016 IEEE.","Multi-features; Saliency; Support vector machine; Webpages viewing","Behavioral research; Forecasting; Learning systems; Support vector machines; Websites; Multi features; Multi-features fusions; Natural images; Saliency; Salient regions; State-of-the-art methods; Visual Attention; Webpages viewing; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85006815902
"Suni S.S., Gopakumar K.","57191610732;57204979170;","A real time decision support system using head nod and shake",2016,"Proceedings of IEEE International Conference on Circuit, Power and Computing Technologies, ICCPCT 2016",,,"7530336","","",,3,"10.1109/ICCPCT.2016.7530336","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992076998&doi=10.1109%2fICCPCT.2016.7530336&partnerID=40&md5=f81d81977e54ebbfc4cfa0298a666758","Department of Electronics and Communication, LBS Centre for Science and Technology, Kerala, India; Department of Electronics and Communication, TKM College of Engineering, Kollam, India","Suni, S.S., Department of Electronics and Communication, LBS Centre for Science and Technology, Kerala, India; Gopakumar, K., Department of Electronics and Communication, TKM College of Engineering, Kollam, India","Human gestures are very prominent means to interface with intelligent systems naturally and nonverbally. This paper presents a robust and real time vision based decision support system that automatically predicts the head nod and shake gestures for decision making. Here we apply the Gentle Adaboost algorithm that detects the faces in the video frames automatically. Real time eye tracking is performed to find the center coordinates of the eyes from the detected face region. In this work, we propose support vector machine classifier, a robust algorithm based on machine learning for predicting head nod and shake gestures. Moreover, this system can function as real time decision support tool in an online environment. The proposed system is implemented and tested for several real time videos. The experimental results show that this system is able to detect head nod and shake gestures with a detection rate of 91.1%. © 2016 IEEE.","decision support system; eye tracking; Face detection; head nod and shake detection; support vector machine","Adaptive boosting; Artificial intelligence; Computer circuits; Decision making; Face recognition; Intelligent systems; Learning systems; Reconfigurable hardware; Support vector machines; Eye-tracking; Gentle AdaBoost algorithm; Online environments; Real time decisions; Real time vision; Real-time decision support systems; Real-time eye tracking; Support vector machine classifiers; Decision support systems",Conference Paper,"Final","",Scopus,2-s2.0-84992076998
"Damen D., Leelasawassuk T., Mayol-Cuevas W.","25654582700;25031966100;6507899137;","You-Do, I-Learn: Egocentric unsupervised discovery of objects and their modes of interaction towards video-based guidance",2016,"Computer Vision and Image Understanding","149",,,"98","112",,24,"10.1016/j.cviu.2016.02.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990985868&doi=10.1016%2fj.cviu.2016.02.016&partnerID=40&md5=b883e2fb104ae20f59d5588a8eb4961c","Department of Computer Science, University of Bristol, Bristol, United Kingdom","Damen, D., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Leelasawassuk, T., Department of Computer Science, University of Bristol, Bristol, United Kingdom; Mayol-Cuevas, W., Department of Computer Science, University of Bristol, Bristol, United Kingdom","This paper presents an unsupervised approach towards automatically extracting video-based guidance on object usage, from egocentric video and wearable gaze tracking, collected from multiple users while performing tasks. The approach (i) discovers task relevant objects, (ii) builds a model for each, (iii) distinguishes different ways in which each discovered object has been used and (iv) discovers the dependencies between object interactions. The work investigates using appearance, position, motion and attention, and presents results using each and a combination of relevant features. Moreover, an online scalable approach is presented and is compared to offline results. The paper proposes a method for selecting a suitable video guide to be displayed to a novice user indicating how to use an object, purely triggered by the user's gaze. The potential assistive mode can also recommend an object to be used next based on the learnt sequence of object interactions. The approach was tested on a variety of daily tasks such as initialising a printer, preparing a coffee and setting up a gym machine. © 2016 Elsevier Inc.","Assistive computing; Object discovery; Object usage; Real-time computer vision; Video guidance","Computer vision; Tracking (position); Assistive; Object discovery; Object usage; Real-time computer vision; Video guidance; Air navigation",Article,"Final","",Scopus,2-s2.0-84990985868
"Stefic D., Patras I.","49562089500;6603429806;","Action recognition using saliency learned from recorded human gaze",2016,"Image and Vision Computing","52",,,"195","205",,7,"10.1016/j.imavis.2016.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978271201&doi=10.1016%2fj.imavis.2016.06.006&partnerID=40&md5=1484f18299898acf9475693926e77a05","School of Electronic Engineering and Computer Science Queen Mary, University of London, London, E1 4NS, United Kingdom","Stefic, D., School of Electronic Engineering and Computer Science Queen Mary, University of London, London, E1 4NS, United Kingdom; Patras, I., School of Electronic Engineering and Computer Science Queen Mary, University of London, London, E1 4NS, United Kingdom","This paper addresses the problem of recognition and localization of actions in image sequences, by utilizing, in the training phase only, gaze tracking data of people watching videos depicting the actions in question. First, we learn discriminative action features at the areas of gaze fixation and train a Convolutional Network that predicts areas of fixation (i.e. salient regions) from raw image data. Second, we propose a Support Vector Machine-based recognition method for joint recognition and localization, in which the bounding box of the action in question is considered as a latent variable. In our formulation the optimization attempts to both minimize the classification cost and maximize the saliency within the bounding box. We show that the results obtained with the optimization where saliency within the bounding box is maximized outperform the results obtained when saliency within the bounding box is not maximized, i.e. when only classification cost is minimized. Furthermore, the results that we obtain outperform the state-of-the-art results on the UCF sports dataset. © 2016 Elsevier B.V.","3D Convolutional Neural Network (3D CNN); Action recognition; Latent variable; Saliency; Support Vector Machine (SVM)","Convolution; Neural networks; Stereo vision; Target tracking; Tracking (position); Action recognition; Convolutional networks; Convolutional neural network; Latent variable; Recognition methods; Saliency; Salient regions; State of the art; Support vector machines",Article,"Final","",Scopus,2-s2.0-84978271201
"Kyritsis M., Gulliver S.R., Feredoes E.","14019633900;6603891003;15753701000;","Environmental factors and features that influence visual search in a 3D WIMP interface",2016,"International Journal of Human Computer Studies","92-93",,,"30","43",,3,"10.1016/j.ijhcs.2016.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966711381&doi=10.1016%2fj.ijhcs.2016.04.009&partnerID=40&md5=c1c38e83cac3f31b1b452a1be01bd412","School of Psychology and Clinical Language Sciences, University of Reading, Whiteknights RoadRG6 6AL, United Kingdom; Henley Business School, Department of Informatics, University of Reading, Whiteknights RoadRG6 6UD, United Kingdom","Kyritsis, M., School of Psychology and Clinical Language Sciences, University of Reading, Whiteknights RoadRG6 6AL, United Kingdom; Gulliver, S.R., Henley Business School, Department of Informatics, University of Reading, Whiteknights RoadRG6 6UD, United Kingdom; Feredoes, E., School of Psychology and Clinical Language Sciences, University of Reading, Whiteknights RoadRG6 6AL, United Kingdom","The challenge of moving past the classic Window Icons Menus Pointer (WIMP) interface, i.e. by turning it '3D', has resulted in much research and development. To evaluate the impact of 3D on the 'finding a target picture in a folder' task, we built a 3D WIMP interface that allowed the systematic manipulation of visual depth, visual aides, semantic category distribution of targets versus non-targets; and the detailed measurement of lower-level stimuli features. Across two separate experiments, one large sample web-based experiment, to understand associations, and one controlled lab environment, using eye tracking to understand user focus, we investigated how visual depth, use of visual aides, use of semantic categories, and lower-level stimuli features (i.e. contrast, colour and luminance) impact how successfully participants are able to search for, and detect, the target image. Moreover in the lab-based experiment, we captured pupillometry measurements to allow consideration of the influence of increasing cognitive load as a result of either an increasing number of items on the screen, or due to the inclusion of visual depth. Our findings showed that increasing the visible layers of depth, and inclusion of converging lines, did not impact target detection times, errors, or failure rates. Low-level features, including colour, luminance, and number of edges, did correlate with differences in target detection times, errors, and failure rates. Our results also revealed that semantic sorting algorithms significantly decreased target detection times. Increased semantic contrasts between a target and its neighbours correlated with an increase in detection errors. Finally, pupillometric data did not provide evidence of any correlation between the number of visible layers of depth and pupil size, however, using structural equation modelling, we demonstrated that cognitive load does influence detection failure rates when there is luminance contrasts between the target and its surrounding neighbours. Results suggest that WIMP interaction designers should consider stimulus-driven factors, which were shown to influence the efficiency with which a target icon can be found in a 3D WIMP interface. © 2016 Elsevier Ltd. All rights reserved.","3D WIMP; Cognitive load; Eye tracking; Perceptual sorting algorithms; Target detection; Visual search","Errors; Failure analysis; Luminance; Semantic Web; Semantics; Sorting; Target tracking; 3D WIMP; Cognitive loads; Eye-tracking; Sorting algorithm; Visual search; Feature extraction",Article,"Final","",Scopus,2-s2.0-84966711381
"Fusaglia M., Hess H., Schwalbe M., Peterhans M., Tinguely P., Weber S., Lu H.","55695505400;56921465600;56922356900;24766431600;56922139200;36062169400;36610445600;","A clinically applicable laser-based image-guided system for laparoscopic liver procedures",2016,"International Journal of Computer Assisted Radiology and Surgery","11","8",,"1499","1513",,5,"10.1007/s11548-015-1309-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945218144&doi=10.1007%2fs11548-015-1309-8&partnerID=40&md5=a2340d4456791cd65ceaa6b290b14c2c","Artorg Center for Biomedical Engineering Research, IGT, University of Bern, Bern, Switzerland; Department of Visceral Surgery, University Hospital of Bern, Bern, Switzerland","Fusaglia, M., Artorg Center for Biomedical Engineering Research, IGT, University of Bern, Bern, Switzerland; Hess, H., Artorg Center for Biomedical Engineering Research, IGT, University of Bern, Bern, Switzerland; Schwalbe, M., Artorg Center for Biomedical Engineering Research, IGT, University of Bern, Bern, Switzerland; Peterhans, M., Artorg Center for Biomedical Engineering Research, IGT, University of Bern, Bern, Switzerland; Tinguely, P., Department of Visceral Surgery, University Hospital of Bern, Bern, Switzerland; Weber, S., Artorg Center for Biomedical Engineering Research, IGT, University of Bern, Bern, Switzerland; Lu, H., Artorg Center for Biomedical Engineering Research, IGT, University of Bern, Bern, Switzerland","Purpose: Laser range scanners (LRS) allow performing a surface scan without physical contact with the organ, yielding higher registration accuracy for image-guided surgery (IGS) systems. However, the use of LRS-based registration in laparoscopic liver surgery is still limited because current solutions are composed of expensive and bulky equipment which can hardly be integrated in a surgical scenario. Methods: In this work, we present a novel LRS-based IGS system for laparoscopic liver procedures. A triangulation process is formulated to compute the 3D coordinates of laser points by using the existing IGS system tracking devices. This allows the use of a compact and cost-effective LRS and therefore facilitates the integration into the laparoscopic setup. The 3D laser points are then reconstructed into a surface to register to the preoperative liver model using a multi-level registration process. Results: Experimental results show that the proposed system provides submillimeter scanning precision and accuracy comparable to those reported in the literature. Further quantitative analysis shows that the proposed system is able to achieve a patient-to-image registration accuracy, described as target registration error, of 3.2±0.57mm. Conclusions: We believe that the presented approach will lead to a faster integration of LRS-based registration techniques in the surgical environment. Further studies will focus on optimizing scanning time and on the respiratory motion compensation. © 2015, CARS.","Image-guided surgery; Laparoscopic liver surgery; Laser range scanner; Surface registration","algorithm; Article; camera; cost effectiveness analysis; eye tracking; feasibility study; image analysis; image guided surgery; laparoscope; laparoscopic surgery; laser; liver surgery; mathematical model; measurement accuracy; measurement precision; priority journal; process optimization; surgery; three dimensional imaging; triangulation; computer assisted surgery; devices; diagnostic imaging; human; imaging phantom; laparoscopy; laser; liver; motion; procedures; Humans; Laparoscopy; Lasers; Liver; Motion; Phantoms, Imaging; Surgery, Computer-Assisted",Article,"Final","",Scopus,2-s2.0-84945218144
"Simon D., Sridharan S., Sah S., Ptucha R., Kanan C., Bailey R.","57193235677;25722300800;56131822400;6505949286;35185157400;16641965200;","Automatic scanpath generation with deep recurrent neural networks",2016,"Proceedings of the ACM Symposium on Applied Perception, SAP 2016",,,,"130","",,2,"10.1145/2931002.2948726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011964878&doi=10.1145%2f2931002.2948726&partnerID=40&md5=0d328159c6104d644fe4eeb6cd6b754e","Rochester Institute of Technology, United States","Simon, D., Rochester Institute of Technology, United States; Sridharan, S., Rochester Institute of Technology, United States; Sah, S., Rochester Institute of Technology, United States; Ptucha, R., Rochester Institute of Technology, United States; Kanan, C., Rochester Institute of Technology, United States; Bailey, R., Rochester Institute of Technology, United States","Many computer vision algorithms are biologically inspired and designed based on the human visual system. Convolutional neural networks (CNNs) are similarly inspired by the primary visual cortex in the human brain. However, the key difference between current visual models and the human visual system is how the visual information is gathered and processed. We make eye movements to collect information from the environment for navigation and task performance. We also make specific eye movements to important regions in the stimulus to perform the task-at-hand quickly and efficiently. Researchers have used expert scanpaths to train novices for improving the accuracy of visual search tasks. One of the limitations of such a system is that we need an expert to examine each visual stimuli beforehand to generate the scanpaths. In order to extend the idea of gaze guidance to a new unseen stimulus, there is a need for a computational model that can automatically generate expert-like scanpaths. We propose a model for automatic scanpath generation using a convolutional neural network (CNN) and long short-term memory (LSTM) modules. Our model uses LSTMs due to the temporal nature of eye movement data (scanpaths) where the system makes fixation predictions based on previous locations examined. © 2016 Copyright held by the owner/author(s).","Deep learning; Eye-tracking; Gaze manipulation","Brain; Convolution; Deep learning; Deep neural networks; Image processing; Long short-term memory; Neural networks; Recurrent neural networks; Biologically inspired; Computational model; Computer vision algorithms; Convolutional neural network; Eye-tracking; Gaze manipulations; Human Visual System; Primary visual cortex; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85011964878
"Kellnhofer P., Didyk P., Myszkowski K., Hefeeda M.M., Seidel H.-P., Matusik W.","55250016000;24779555900;55888212000;20433459500;7202927645;56230515000;","GazeStereo3D: Seamless disparity manipulations",2016,"ACM Transactions on Graphics","35","4","a68","","",,20,"10.1145/2897824.2925866","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979998484&doi=10.1145%2f2897824.2925866&partnerID=40&md5=74657e2f4d709777703f167ee61f5647","MIT, CSAIL, Germany; MPI Informatik, Germany; Saarland University, MMCI, Germany; Qatar Computing Research Institute, Qatar","Kellnhofer, P., MIT, CSAIL, Germany, MPI Informatik, Germany; Didyk, P., MPI Informatik, Germany, Saarland University, MMCI, Germany; Myszkowski, K., MPI Informatik, Germany; Hefeeda, M.M., Qatar Computing Research Institute, Qatar; Seidel, H.-P., MPI Informatik, Germany; Matusik, W., MIT, CSAIL, Germany","Producing a high quality stereoscopic impression on current displays is a challenging task. The content has to be carefully prepared in order to maintain visual comfort, which typically affects the quality of depth reproduction. In this work, we show that this problem can be significantly alleviated when the eye fixation regions can be roughly estimated. We propose a new method for stereoscopic depth adjustment that utilizes eye tracking or other gaze prediction information. The key idea that distinguishes our approach from the previous work is to apply gradual depth adjustments at the eye fixation stage, so that they remain unnoticeable. To this end, we measure the limits imposed on the speed of disparity changes in various depth adjustment scenarios, and formulate a new model that can guide such seamless stereoscopic content processing. Based on this model, we propose a real-time controller that applies local manipulations to stereoscopic content to find the optimum between depth reproduction and visual comfort. We show that the controller is mostly immune to the limitations of low-cost eye tracking solutions. We also demonstrate benefits of our model in off-line applications, such as stereoscopic movie production, where skillful directors can reliably guide and predict viewers' attention or where attended image regions are identified during eye tracking sessions. We validate both our model and the controller in a series of user experiments. They show significant improvements in depth perception without sacrificing the visual quality when our techniques are applied. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Eye tracking; Gaze tracking; Gaze-contingent display; Remapping; Retargeting; Stereoscopic 3D","Controllers; Depth perception; Interactive computer graphics; Tracking (position); Visual communication; Eye-tracking; Gaze tracking; Gaze-contingent displays; Remapping; Retargeting; Stereo image processing",Conference Paper,"Final","",Scopus,2-s2.0-84979998484
"Wang C., Shi F., Xia S., Chai J.","56963622800;56118895700;7202893266;7202678260;","Realtime 3D eye gaze animation using a single RGB camera",2016,"ACM Transactions on Graphics","35","4","a118","","",,27,"10.1145/2897824.2925947","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979992583&doi=10.1145%2f2897824.2925947&partnerID=40&md5=2fd8d3a5a92517b6a64397794a98a0ef","Institute of Computing Technology (Chinese Academy of Sciences), China; University of Chinese Academy of Sciences, China; Texas A and M University, United States","Wang, C., Institute of Computing Technology (Chinese Academy of Sciences), China, University of Chinese Academy of Sciences, China; Shi, F., Texas A and M University, United States; Xia, S., Institute of Computing Technology (Chinese Academy of Sciences), China; Chai, J., Texas A and M University, United States","This paper presents the first realtime 3D eye gaze capture method that simultaneously captures the coordinated movement of 3D eye gaze, head poses and facial expression deformation using a single RGB camera. Our key idea is to complement a realtime 3D facial performance capture system with an efficient 3D eye gaze tracker. We start the process by automatically detecting important 2D facial features for each frame. The detected facial features are then used to reconstruct 3D head poses and large-scale facial deformation using multi-linear expression deformation models. Next, we introduce a novel user-independent classification method for extracting iris and pupil pixels in each frame. We formulate the 3D eye gaze tracker in the Maximum A Posterior (MAP) framework, which sequentially infers the most probable state of 3D eye gaze at each frame. The eye gaze tracker could fail when eye blinking occurs. We further introduce an efficient eye close detector to improve the robustness and accuracy of the eye gaze tracker. We have tested our system on both live video streams and the Internet videos, demonstrating its accuracy and robustness under a variety of uncontrolled lighting conditions and overcoming significant differences of races, genders, shapes, poses and expressions across individuals. © 2016 ACM.","3D eye gaze tracking; Facial animation and control; Facial performance capture","Animation; Cameras; Deformation; Eye movements; Feature extraction; Interactive computer graphics; Three dimensional computer graphics; Tracking (position); Video streaming; Classification methods; Coordinated movement; Eye gaze tracking; Facial animation; Facial deformations; Lighting conditions; Maximum a posteriors; Performance capture; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-84979992583
"Ben taher F., Ben Amor N., Jallouli M.","48460892500;8412251500;56556475800;","An extended Eye Movement Tracker system for an electric wheelchair movement control",2016,"Proceedings of IEEE/ACS International Conference on Computer Systems and Applications, AICCSA","2016-July",,"7507101","","",,1,"10.1109/AICCSA.2015.7507101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980368196&doi=10.1109%2fAICCSA.2015.7507101&partnerID=40&md5=2788ff2ea03ab219aabe22171612da2f","Ecole Nationale d'Ingénieurs de Sfax, Université de Sfax, Sfax, Tunisia","Ben taher, F., Ecole Nationale d'Ingénieurs de Sfax, Université de Sfax, Sfax, Tunisia; Ben Amor, N., Ecole Nationale d'Ingénieurs de Sfax, Université de Sfax, Sfax, Tunisia; Jallouli, M., Ecole Nationale d'Ingénieurs de Sfax, Université de Sfax, Sfax, Tunisia","To compensate the incapability of severely disabled person, intelligent wheelchairs are commonly adopted using several non manual command techniques. Eye tracking is one of such techniques. Although it has been the subject of intense research for many years, Eye tracking has not yet reached the level of perfection to be commercially used in an intelligent wheelchair to control electrical powered wheelchairs (EPW). This paper presents an eye tracking system based on fuzzy logic controller to control an EPW with a simple web cam placed in front of the user. Simulation results indicate that the fuzzy logic controller gives better results compared to other techniques. © 2015 IEEE.","3D simulator; EPW; Eye tracking; fuzzy logic","Computer circuits; Controllers; Disabled persons; Eye movements; Fuzzy logic; Intelligent robots; Wheelchairs; Electric wheelchair; Eye tracking systems; Fuzzy logic controllers; Intelligent wheelchair; Movement control; Powered wheel chairs; Tracker system; WebCams; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-84980368196
"Chen M.-H., Wen J., Zhu Y., Xing H.-Y., Wang Y.","56437797600;7402701826;55568220400;8427343800;56230386800;","Multi-level thresholding for pupil location in eye-gaze tracking systerm",2016,"Proceedings - International Conference on Machine Learning and Cybernetics","2",,"7873017","1009","1014",,3,"10.1109/ICMLC.2016.7873017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021244949&doi=10.1109%2fICMLC.2016.7873017&partnerID=40&md5=75b701c4d987d08b8c83db94aa49b5c8","College of Computer Science, Chongqing University, Chongqing, China; Key Laboratory of Pattern Recognition and Intelligent Information Processing, Institutions of Higher Education of Sichuan Province, Chengdu University, China; Magnetic Resonance Imaging Research Centre, Huaxi Hospital, Sichuan Chengdu, China","Chen, M.-H., College of Computer Science, Chongqing University, Chongqing, China; Wen, J., College of Computer Science, Chongqing University, Chongqing, China, Key Laboratory of Pattern Recognition and Intelligent Information Processing, Institutions of Higher Education of Sichuan Province, Chengdu University, China; Zhu, Y., College of Computer Science, Chongqing University, Chongqing, China; Xing, H.-Y., Magnetic Resonance Imaging Research Centre, Huaxi Hospital, Sichuan Chengdu, China; Wang, Y., College of Computer Science, Chongqing University, Chongqing, China","A new pupil location methodis proposed in eye-gaze tracking system. Firstly, input images are enhanced in order to reduce the influence of illumination. Secondly, multiple candidate thresholds are obtained in terms of the valleys in histogram, and then different segmentation results are available. Further, a fusion method based on the overlaps of segmentation results is proposed which acquired candidate regions and the eye region is obtained according to the entropy information of candidate regions. Thirdly, a simple and effective threshold segmentation method based on eye characteristic is employed and pupil area is obtained. Finally, the center of pupil is acquired by ellipse fitting. Experimental results demonstrated that the proposed methods were effective to improve pupil location accuracy. © 2016 IEEE.","Entropy of information; Eye characteristic; Eye location; Pupil location; Thresholdsegmentation","Entropy; Image enhancement; Location; Machine learning; Ellipse fitting; Eye characteristic; Eye gaze tracking; Eye location; Multilevel thresholding; Pupil locations; Segmentation results; Thresholdsegmentation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85021244949
"Zou Y.-H., Wen J., Xing H.-Y., Zhu Y.","57189499192;7402701826;8427343800;55568220400;","Rapid eye movement tracking method based on FPGA",2016,"Proceedings - International Conference on Machine Learning and Cybernetics","2",,"7873019","1021","1025",,2,"10.1109/ICMLC.2016.7873019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021213921&doi=10.1109%2fICMLC.2016.7873019&partnerID=40&md5=ebf2504fc34facd8c702f17257979771","Chengdu University of Information Technology, Sichuan Chengdu, 610225, China; Magnetic Resonance Imaging Research Centre, Huaxi Hospital, Sichuan Chengdu, 610061, China; College of Computer Science, Chongqing University, Chongqing, 400044, China","Zou, Y.-H., Chengdu University of Information Technology, Sichuan Chengdu, 610225, China; Wen, J., College of Computer Science, Chongqing University, Chongqing, 400044, China; Xing, H.-Y., Magnetic Resonance Imaging Research Centre, Huaxi Hospital, Sichuan Chengdu, 610061, China; Zhu, Y., College of Computer Science, Chongqing University, Chongqing, 400044, China","In this study, an eye-tracking system based on FPGA hardware and the center of gravity algorithm is proposed. It captures video via LUPA300 high-speed CMOS and obtains the motion of eye pupil and bright spot through the continuous video frames by using a FPGA implemented the center of gravity algorithm. To obtain a reliable tracking accuracy, a series of binarization and mathematical morphology operations (such as corrosion and expansion) are adopted as a pre-procession step. The results show that FPGA can implement the center of gravity algorithm to track eye motion at high speed, the binarization and corrosion expansion pretreatment can improve the tracking accuracy effectively. © 2016 IEEE.","Corrosion; Expansion; Eye movement; FPGA; Motion tracking; The center of gravity","Corrosion; Expansion; Eye movements; Field programmable gate arrays (FPGA); Machine learning; Mathematical morphology; Motion analysis; Binarizations; Center of gravity; Corrosion expansion; Eye tracking systems; FPGA hardwares; Motion tracking; Rapid eye movement; Tracking accuracy; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85021213921
"Beyl T., Nicolai P., Comparetti M.D., Raczkowsky J., De Momi E., Wörn H.","55208764300;55208398600;54787388500;7003530989;57203951877;7003467912;","Time-of-flight-assisted Kinect camera-based people detection for intuitive human robot cooperation in the surgical operating room",2016,"International Journal of Computer Assisted Radiology and Surgery","11","7",,"1329","1345",,5,"10.1007/s11548-015-1318-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946943818&doi=10.1007%2fs11548-015-1318-7&partnerID=40&md5=2e1c517b23501b56e83d0740d0cc8e44","Institute for Anthropomatics and Robotics (IAR), Intelligent Process Control and Robotics (IPR), Karlsruhe Institute of Technology, Karlsruhe, Germany; NeuroEngineering and Medical Robotics Laboratory, Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy","Beyl, T., Institute for Anthropomatics and Robotics (IAR), Intelligent Process Control and Robotics (IPR), Karlsruhe Institute of Technology, Karlsruhe, Germany; Nicolai, P., Institute for Anthropomatics and Robotics (IAR), Intelligent Process Control and Robotics (IPR), Karlsruhe Institute of Technology, Karlsruhe, Germany; Comparetti, M.D., NeuroEngineering and Medical Robotics Laboratory, Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy; Raczkowsky, J., Institute for Anthropomatics and Robotics (IAR), Intelligent Process Control and Robotics (IPR), Karlsruhe Institute of Technology, Karlsruhe, Germany; De Momi, E., NeuroEngineering and Medical Robotics Laboratory, Department of Electronics, Information and Bioengineering, Politecnico di Milano, Milan, Italy; Wörn, H., Institute for Anthropomatics and Robotics (IAR), Intelligent Process Control and Robotics (IPR), Karlsruhe Institute of Technology, Karlsruhe, Germany","Background: Scene supervision is a major tool to make medical robots safer and more intuitive. The paper shows an approach to efficiently use 3D cameras within the surgical operating room to enable for safe human robot interaction and action perception. Additionally the presented approach aims to make 3D camera-based scene supervision more reliable and accurate. Methods: A camera system composed of multiple Kinect and time-of-flight cameras has been designed, implemented and calibrated. Calibration and object detection as well as people tracking methods have been designed and evaluated. Results: The camera system shows a good registration accuracy of 0.05 m. The tracking of humans is reliable and accurate and has been evaluated in an experimental setup using operating clothing. The robot detection shows an error of around 0.04 m. Conclusions: The robustness and accuracy of the approach allow for an integration into modern operating room. The data output can be used directly for situation and workflow detection as well as collision avoidance. © 2015, CARS.","3D vision; Digital operating room; Environment supervision; RGB-D cameras; Surgical robotics; ToF cameras","accuracy; Article; calibration; camera; clothing; controlled study; cooperation; endoscope; equipment design; eye tracking; human; intuition; operating room; perception; priority journal; registration; reliability; robotics; social interaction; time of flight assisted Kinect camera; biomechanics; computer; computer interface; operating room; procedures; robotic surgical procedure; robotics; Biomechanical Phenomena; Calibration; Computer Peripherals; Humans; Intuition; Operating Rooms; Robotic Surgical Procedures; Robotics; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84946943818
"Vigier T., Baveye Y., Rousseau J., Le Callet P.","55206851100;55440174200;57189663285;57200770358;","Visual attention as a dimension of QoE: Subtitles in UHD videos",2016,"2016 8th International Conference on Quality of Multimedia Experience, QoMEX 2016",,,"7498924","","",,6,"10.1109/QoMEX.2016.7498924","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979670101&doi=10.1109%2fQoMEX.2016.7498924&partnerID=40&md5=e794deda5cad593fef26d8e0c3556f11","IRCCyN UMR CNRS 6597, Université de Nantes, Nantes, France","Vigier, T., IRCCyN UMR CNRS 6597, Université de Nantes, Nantes, France; Baveye, Y., IRCCyN UMR CNRS 6597, Université de Nantes, Nantes, France; Rousseau, J., IRCCyN UMR CNRS 6597, Université de Nantes, Nantes, France; Le Callet, P., IRCCyN UMR CNRS 6597, Université de Nantes, Nantes, France","With the ever-growing availability of multimedia content produced, broadcast and consumed worldwide, subtitling is becoming an essential service to quickly share understandable content. Simultaneously, the increased resolution of the ultra high definition (UHD) standard comes with wider screens and new viewing conditions. Services as the display of subtitles thus require adaptation to better fit the new induced viewing visual angle. This paper aims at evaluating quality of experience of subtitled movies in UHD to propose guidelines for the appearance of subtitles. From an eye-tracking experiment conducted on 68 observers and 30 video sequences, viewing behavior and visual saliency are analyzed with and without subtitles and for different subtitle styles. Various metrics based on eye-tracking data, such as the Reading Index for Dynamic Texts (RIDT), are computed to objectively measure the ease of reading and subtitle disturbance. The results mainly show that doubling the visual angle of subtitles from HD to UHD guarantees subtitle readability without compromising the enjoyment of the video content. © 2016 IEEE.",,"Behavioral research; Digital television; Multimedia systems; Video recording; Essential services; Multimedia contents; Quality of experience (QoE); Ultra high definition (UHD); Video sequences; Viewing conditions; Visual Attention; Visual saliency; Quality of service",Conference Paper,"Final","",Scopus,2-s2.0-84979670101
"Stavroulia K.-E., Ruiz-Harisiou A., Manouchou E., Georgiou K., Sella F., Lanitis A.","57063423500;57190346238;57190344109;57202523998;57190344166;6603828788;","A 3D virtual environment for training teachers to identify bullying",2016,"Proceedings of the 18th Mediterranean Electrotechnical Conference: Intelligent and Efficient Technologies and Services for the Citizen, MELECON 2016",,,"7495417","","",,13,"10.1109/MELCON.2016.7495417","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979294107&doi=10.1109%2fMELCON.2016.7495417&partnerID=40&md5=eacdd2a766348e79674e96d332efc690","Visual Media Computing Lab, Department of Multimedia and Graphic Arts, Cyprus University of Technology, Limassol, Cyprus","Stavroulia, K.-E., Visual Media Computing Lab, Department of Multimedia and Graphic Arts, Cyprus University of Technology, Limassol, Cyprus; Ruiz-Harisiou, A., Visual Media Computing Lab, Department of Multimedia and Graphic Arts, Cyprus University of Technology, Limassol, Cyprus; Manouchou, E., Visual Media Computing Lab, Department of Multimedia and Graphic Arts, Cyprus University of Technology, Limassol, Cyprus; Georgiou, K., Visual Media Computing Lab, Department of Multimedia and Graphic Arts, Cyprus University of Technology, Limassol, Cyprus; Sella, F., Visual Media Computing Lab, Department of Multimedia and Graphic Arts, Cyprus University of Technology, Limassol, Cyprus; Lanitis, A., Visual Media Computing Lab, Department of Multimedia and Graphic Arts, Cyprus University of Technology, Limassol, Cyprus","Incidents involving bullying in schools present a difficult challenge for educators who need to identify potentially dangerous behaviors in comparison to 'innocent' types of interstudent interactions. In this paper we investigate the use of a dedicated Virtual Reality application as a means of training educators to identify alarming bullying activities. The Virtual Reality environment simulates a typical middle school, where different bullying-related incidents take place. The virtual environment is visualized using a Virtual Reality headset in conjunction with eye tracking techniques that facilitate user interaction. In order to make realistic animations that resemble human motions, state of the art motion tracking equipment was used for generating realistic avatar movements. According to the scenario of the application, the user witness different types of student behavior and he/she has to take decisions on how to control and deal with the situation. The prototype was tested by active teachers who reported that the incidents within the simulation were similar to real life experiences and suggested that the application could be used in teacher training education providing inexperienced teachers feedback on how to recognize and manage bullying. © 2016 IEEE.","bullying; teacher training; virtual learning environments","Computer aided instruction; Education; Personnel training; Teaching; Virtual reality; 3-D virtual environment; bullying; Life experiences; State of the art; Teacher training; Virtual learning environments; Virtual-reality environment; Virtual-reality headsets; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-84979294107
"Akahori W., Morishima S., Hirai T., Kawamura S.","57190443732;7005317462;55822308200;57224657182;","Region-of-interest-based subtitle placement using eye-tracking data of multiple viewers",2016,"TVX 2016 - Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video",,,,"123","128",,7,"10.1145/2932206.2933558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979991973&doi=10.1145%2f2932206.2933558&partnerID=40&md5=5368b98222cbf848954c05f4782b9c50","Waseda University, Tokyo, Japan; Waseda Research Institute for Science and Engineering, Tokyo, Japan; Komazawa University, Tokyo, Japan","Akahori, W., Waseda University, Tokyo, Japan; Morishima, S., Waseda Research Institute for Science and Engineering, Tokyo, Japan; Hirai, T., Komazawa University, Tokyo, Japan; Kawamura, S., Waseda University, Tokyo, Japan","We present a subtitle-placement method that reduces viewer's eye movement without interfering with the target region of interest (ROI) in a video scene. Subtitles help viewers understand foreign-language videos. However, subtitles tend to attract viewers' line of sight, which cause viewers to lose focus on the video content. To address this problem, previous studies have attempted to improve viewer experiences by dynamically shifting subtitle positions. Nevertheless, in their user studies, some participants felt that the visual appearance of such subtitles was unnatural and caused them fatigue. We propose a method that places subtitles below the ROI, which is calculated by eye-tracking data from multiple viewers. Two experiments were conducted to evaluate viewer impression and compare line of sight for videos with subtitles placed by the proposed and previous methods. Copyright is held by the owner/author(s).","Accessibility; Dynamic subtitles; Eye-tracking; Region of interest; User experience","Image segmentation; Interactive television; Accessibility; Eye-tracking; Foreign language; Placement methods; Region of interest; User experience; Video contents; Visual appearance; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84979991973
"Tostado P.M., Abbott W.W., Faisal A.A.","57190131185;55312983800;6602900233;","3D gaze cursor: Continuous calibration and end-point grasp control of robotic actuators",2016,"Proceedings - IEEE International Conference on Robotics and Automation","2016-June",,"7487502","3295","3300",,12,"10.1109/ICRA.2016.7487502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977540191&doi=10.1109%2fICRA.2016.7487502&partnerID=40&md5=3ab359f4d727869f5490c406d66e3cd7","Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Dept. of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; MRC Clinical Sciences Centre, United Kingdom","Tostado, P.M., Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Abbott, W.W., Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Faisal, A.A., Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom, Dept. of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom, MRC Clinical Sciences Centre, United Kingdom","Eye movements are closely related to motor actions, and hence can be used to infer motor intentions. Additionally, eye movements are in some cases the only means of communication and interaction with the environment for paralysed and impaired patients with severe motor deficiencies. Despite this, eye-tracking technology still has a very limited use as a human-robot control interface and its applicability is highly restricted to 2D simple tasks that operate on screen based interfaces and do not suffice for natural physical interaction with the environment. We propose that decoding the gaze position in 3D space rather than in 2D results into a much richer spatial cursor signal that allows users to perform everyday tasks such as grasping and moving objects via gaze-based robotic teleoperation. Eye tracking in 3D calibration is usually slow - we demonstrate here that by using a full 3D trajectory for system calibration generated by a robotic arm rather than a simple grid of discrete points, gaze calibration in the 3 dimensions can be successfully achieved in short time and with high accuracy. We perform the non-linear regression from eye-image to 3D-end point using Gaussian Process regressors, which allows us to handle uncertainty in end-point estimates gracefully. Our telerobotic system uses a multi-joint robot arm with a gripper and is integrated with our in-house GT3D binocular eye tracker. This prototype system has been evaluated and assessed in a test environment with 7 users, yielding gaze-estimation errors of less than 1cm in the horizontal, vertical and depth dimensions, and less than 2cm in the overall 3D Euclidean space. Users reported intuitive, low-cognitive load, control of the system right from their first trial and were straightaway able to simply look at an object and command through a wink to grasp this object with the robot gripper. © 2016 IEEE.",,"Calibration; Eye movements; Grippers; Human robot interaction; Motion planning; Point contacts; Robotic arms; Robots; Uncertainty analysis; Communication and interaction; Continuous calibrations; Eye tracking technologies; Human-robot controls; Non-linear regression; Physical interactions; Robotic teleoperation; Telerobotic systems; Robotics",Conference Paper,"Final","",Scopus,2-s2.0-84977540191
"Park H., Kim D.","55953924600;24597347100;","Gaze classification on a mobile device by using deep belief networks",2016,"Proceedings - 3rd IAPR Asian Conference on Pattern Recognition, ACPR 2015",,,"7486590","685","689",,3,"10.1109/ACPR.2015.7486590","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978906804&doi=10.1109%2fACPR.2015.7486590&partnerID=40&md5=27f66eee8ebbe63a17ec262d6607537c","POSTECH, Pohang, South Korea","Park, H., POSTECH, Pohang, South Korea; Kim, D., POSTECH, Pohang, South Korea","In this paper, we introduce a gaze classification method which classifies the locations of human gaze on a display of a mobile device. For example, when the user see the upper part of a display, our method classifies the gaze as the upper part among the available choices: upper, middle, and lower parts of the display. Our method uses appearance-based gaze estimation and gray-scale images captured from a camera of a mobile device. This method does not require any personal calibration. We train gaze classifiers by using Deep Belief Networks with considering head poses in general environments for a mobile device. The gaze classification method is applied to human-computer interaction and various application programs. © 2015 IEEE.",,"Application programs; Human computer interaction; Mobile devices; Pattern recognition; Appearance based; Classification methods; Deep belief networks; Gaze estimation; Gray-scale images; Head pose; Display devices",Conference Paper,"Final","",Scopus,2-s2.0-84978906804
"Du X., Allan M., Dore A., Ourselin S., Hawkes D., Kelly J.D., Stoyanov D.","55243065000;55626720600;22233346900;6602233595;35464300200;7404901815;57203105770;","Combined 2D and 3D tracking of surgical instruments for minimally invasive and robotic-assisted surgery",2016,"International Journal of Computer Assisted Radiology and Surgery","11","6",,"1109","1119",,34,"10.1007/s11548-016-1393-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962169631&doi=10.1007%2fs11548-016-1393-4&partnerID=40&md5=f3e294878187ea48a2492443c87625b3","Centre for Medical Image Computing, Department of Computer Science, University College London, London, United Kingdom; Centre for Medical Image Computing, Department of Medical Physics, University College London, London, United Kingdom; WIREWAX, London, United Kingdom; Division of Surgery and Interventional Science, University College London, London, United Kingdom","Du, X., Centre for Medical Image Computing, Department of Computer Science, University College London, London, United Kingdom; Allan, M., Centre for Medical Image Computing, Department of Computer Science, University College London, London, United Kingdom; Dore, A., WIREWAX, London, United Kingdom; Ourselin, S., Centre for Medical Image Computing, Department of Medical Physics, University College London, London, United Kingdom; Hawkes, D., Centre for Medical Image Computing, Department of Medical Physics, University College London, London, United Kingdom; Kelly, J.D., Division of Surgery and Interventional Science, University College London, London, United Kingdom; Stoyanov, D., Centre for Medical Image Computing, Department of Computer Science, University College London, London, United Kingdom","Purpose: Computer-assisted interventions for enhanced minimally invasive surgery (MIS) require tracking of the surgical instruments. Instrument tracking is a challenging problem in both conventional and robotic-assisted MIS, but vision-based approaches are a promising solution with minimal hardware integration requirements. However, vision-based methods suffer from drift, and in the case of occlusions, shadows and fast motion, they can be subject to complete tracking failure. Methods: In this paper, we develop a 2D tracker based on a Generalized Hough Transform using SIFT features which can both handle complex environmental changes and recover from tracking failure. We use this to initialize a 3D tracker at each frame which enables us to recover 3D instrument pose over long sequences and even during occlusions. Results: We quantitatively validate our method in 2D and 3D with ex vivo data collected from a DVRK controller as well as providing qualitative validation on robotic-assisted in vivo data. Conclusions: We demonstrate from our extended sequences that our method provides drift-free robust and accurate tracking. Our occlusion-based sequences additionally demonstrate that our method can recover from occlusion-based failure. In both cases, we show an improvement over using 3D tracking alone suggesting that combining 2D and 3D tracking is a promising solution to challenges in surgical instrument tracking. © 2016, The Author(s).","Instrument tracking and detection; Minimally invasive surgery; Robot-assisted surgery; Surgical vision","accuracy; Article; environmental change; ex vivo study; eye tracking; histogram; human; illumination; minimally invasive surgery; priority journal; robotic surgical procedure; surgical equipment; three dimensional imaging; computer assisted surgery; devices; minimally invasive surgery; motion; procedures; robotic surgical procedure; surgical equipment; Humans; Minimally Invasive Surgical Procedures; Motion; Robotic Surgical Procedures; Surgery, Computer-Assisted; Surgical Instruments",Article,"Final","",Scopus,2-s2.0-84962169631
"Funes-Mora K.A., Odobez J.-M.","55336423200;57203103085;","Gaze Estimation in the 3D Space Using RGB-D Sensors: Towards Head-Pose and User Invariance",2016,"International Journal of Computer Vision","118","2",,"194","216",,39,"10.1007/s11263-015-0863-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946944319&doi=10.1007%2fs11263-015-0863-4&partnerID=40&md5=399e81b733b016912bbd5adfd6f179b6","Idiap Research Institute, Martigny, Switzerland; École polytechnique fédéral de, Lausanne (EPFL), Switzerland","Funes-Mora, K.A., Idiap Research Institute, Martigny, Switzerland, École polytechnique fédéral de, Lausanne (EPFL), Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, Switzerland, École polytechnique fédéral de, Lausanne (EPFL), Switzerland","We address the problem of 3D gaze estimation within a 3D environment from remote sensors, which is highly valuable for applications in human–human and human–robot interactions. To the contrary of most previous works, which are limited to screen gazing applications, we propose to leverage the depth data of RGB-D cameras to perform an accurate head pose tracking, acquire head pose invariance through a 3D rectification process that renders head pose dependent eye images into a canonical viewpoint, and computes the line-of-sight in the 3D space. To address the low resolution issue of the eye image resulting from the use of remote sensors, we rely on the appearance based gaze estimation paradigm, which has demonstrated robustness against this factor. In this context, we do a comparative study of recent appearance based strategies within our framework, study the generalization of these methods to unseen individual, and propose a cross-user eye image alignment technique relying on the direct registration of gaze-synchronized eye images. We demonstrate the validity of our approach through extensive gaze estimation experiments on a public dataset as well as a gaze coding task applied to natural job interviews. © 2015, Springer Science+Business Media New York.","Appearance based methods; Gaze estimation; Head-pose invariance; Person invariance; RGB-D cameras","Cameras; Gesture recognition; Human robot interaction; Remote sensing; Appearance-based methods; Comparative studies; Gaze estimation; Head pose; Head-pose tracking; Rectification process; Rgb-d cameras; Robot interactions; Motion estimation",Article,"Final","",Scopus,2-s2.0-84946944319
"Li J., Li S.","57203736355;16202805500;","Gaze Estimation From Color Image Based on the Eye Model With Known Head Pose",2016,"IEEE Transactions on Human-Machine Systems","46","3","7272080","414","423",,25,"10.1109/THMS.2015.2477507","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941921838&doi=10.1109%2fTHMS.2015.2477507&partnerID=40&md5=b07b62c4590e7dfd42145720a8271ae0","Graduate School of Engineering, Tottori University, Tottori, 680-8550, Japan; Graduate School of Information Sciences, Hiroshima City University, Hiroshima, 731-3194, Japan","Li, J., Graduate School of Engineering, Tottori University, Tottori, 680-8550, Japan; Li, S., Graduate School of Information Sciences, Hiroshima City University, Hiroshima, 731-3194, Japan","This paper proposes a novel method of gaze estimation based on an eye model with known head pose. The most crucial factors in the eye-model-based approach to gaze estimation are the 3-D positions of the eyeball and iris centers. In the proposed method, an RGB-D camera, Kinect sensor, is used to obtain the head pose as well as the eye region of the color image. The 3-D position of the eyeball center is determined in the calibration phase by gazing at the center of the color image camera. Then, to estimate the 3-D position of the iris center, the 3-D contour of the iris is projected onto the color image with the known head pose obtained from color and depth cues of an RGB-D camera. Thus, the ellipse of the iris in the image can be described using only two parameters: the yaw and pitch angles of the eyeball in the iris coordinate system, rather than the conventional five parameters of an ellipse. The proposed method can fit an iris that is not complete due to eyelid occlusion. The average errors of vertical and horizontal angles of the gaze estimation for seven subjects are 5.9° and 4.4°, respectively. The processing speed is as high as 330 ms per frame. However, for lower resolution and poor illumination images, as tested on the public database EYEDIAP, the performance of the proposed eye-model-based method is inferior to that of the-state-of-the-art appearance-based method. © 2015 IEEE.","3-D eye model; Color and depth cues; eyeball calibration; gaze estimation; iris fitting","Cameras; Color; Appearance-based methods; Co-ordinate system; Horizontal angles; Illumination images; Lower resolution; Processing speed; Public database; State of the art; Motion estimation",Article,"Final","",Scopus,2-s2.0-84941921838
"Boi P., Fenu G., Spano L.D., Vargiu V.","6602429841;24469552000;24741324500;57189682492;","Reconstructing user's attention on the web through mouse movements and perception-based content identification",2016,"ACM Transactions on Applied Perception","13","3","15","","",,11,"10.1145/2912124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974605256&doi=10.1145%2f2912124&partnerID=40&md5=33d8ed9d0c3dbd1225114730ce515b38","University of Cagliari, Department of Mathematics and Computer Science, Italy; Università di Cagliari, Dipartimento di Matematica e Informatica, Via Ospedale 72, Cagliari, 09124, Italy","Boi, P., University of Cagliari, Department of Mathematics and Computer Science, Italy, Università di Cagliari, Dipartimento di Matematica e Informatica, Via Ospedale 72, Cagliari, 09124, Italy; Fenu, G., University of Cagliari, Department of Mathematics and Computer Science, Italy, Università di Cagliari, Dipartimento di Matematica e Informatica, Via Ospedale 72, Cagliari, 09124, Italy; Spano, L.D., University of Cagliari, Department of Mathematics and Computer Science, Italy, Università di Cagliari, Dipartimento di Matematica e Informatica, Via Ospedale 72, Cagliari, 09124, Italy; Vargiu, V., University of Cagliari, Department of Mathematics and Computer Science, Italy, Università di Cagliari, Dipartimento di Matematica e Informatica, Via Ospedale 72, Cagliari, 09124, Italy","Eye tracking is one of the most exploited techniques in literature for finding usability problems in web-based user interfaces (UIs). However, it is usually employed in a laboratory setting, considering that an eye-tracker is not commonly used in web browsing. In contrast, web application providers usually exploit remote techniques for large-scale user studies (e.g. A/B testing), tracking low-level interactions such as mouse clicks and movements. In this article, we discuss a method for predicting whether the user is looking at the content pointed by the cursor, exploiting the mouse movement data and a segmentation of the contents in a web page. We propose an automatic method for segmenting content groups inside a web page that, applying both image and code analysis techniques, identifies the user-perceived group of contents with a mean pixel-based error around the 20%. In addition, we show through a user study that such segmentation information enhances the precision and the accuracy in predicting the correlation between between the user's gaze and the mouse position at the content level, without relaying on user-specific features. © 2016 ACM.","Content segmentation; Layout analysis; Machine learning; Mouse-eye correlation; User's attention","Learning systems; Mammals; User interfaces; Websites; Automatic method; Content identifications; Content segmentation; Layout analysis; Perception-based; Segmentation informations; Usability problems; User's attention; Eye tracking",Article,"Final","",Scopus,2-s2.0-84974605256
"Yi S., Jiang N., Wang X., Liu W.","55513528500;57206924146;36100811100;8043635200;","Individual adaptive metric learning for visual tracking",2016,"Neurocomputing","191",,,"273","285",,9,"10.1016/j.neucom.2016.01.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012020338&doi=10.1016%2fj.neucom.2016.01.052&partnerID=40&md5=36e7bfe51f545763e52d3bdc00d479cb","College of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, China","Yi, S., College of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, China; Jiang, N., College of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, China; Wang, X., College of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, China; Liu, W., College of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, China","Recent attempts demonstrate that learning an appropriate distance metric in visual tracking applications can improve the tracking performance. However, the existing metric learning methods learn and adjust the distance between all pairwise sample points in an iterative way, which raises the time consumption issue in real-time tracking applications. To address this problem, this paper proposes a novel metric learning method and applies it to visual tracking. The main idea of the proposed method is to adapt the distance from each individual sample point to a few anchor points instead of the distance between all pairs of samples, so as to reduce the number of distances to be adjusted. Based on this idea, we construct a convex matrix function that collapses the sample points to their class centers and maximizes the inter-class distance. Given n training samples in d-dimensional space, the equation can be solved in a closed form with the computational complexity of O(d2n). This is much more computationally efficient than traditional methods with the computational complexity of O(dn2) in each iteration (normally in tracking applications, d≪n). Furthermore, the proposed method can be learned in an online manner which is able to accelerate the learning process and improve the matching accuracy in visual tracking applications. Experiments on UCI datasets demonstrate that the proposed learning method is comparable with the traditional metric learning methods in term of classification accuracy but much more time efficient. The comparison experiments on benchmark video sequences show that the tracking algorithm based on our learning approach can outperform the state-of-the-art tracking algorithms. © 2016 Elsevier B.V.","Classification; Metric learning; Visual tracking","Classification (of information); Computational complexity; Learning systems; Tracking (position); Appropriate distances; Classification accuracy; Computationally efficient; Inter-class distance; Metric learning; Tracking application; Tracking performance; Visual Tracking; Iterative methods; algorithm; Article; classification; eye tracking; image analysis; machine learning; mathematical computing; measurement accuracy; metric learning; online system; priority journal",Article,"Final","",Scopus,2-s2.0-85012020338
"Baltrusaitis T., Robinson P., Morency L.-P.","36696075900;57205369790;6603047400;","OpenFace: An open source facial behavior analysis toolkit",2016,"2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016",,,"7477553","","",,631,"10.1109/WACV.2016.7477553","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977660584&doi=10.1109%2fWACV.2016.7477553&partnerID=40&md5=0be2d7086631b6f33fc21965ec7dba80",,"Baltrusaitis, T.; Robinson, P.; Morency, L.-P.","Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system. © 2016 IEEE.",,"Artificial intelligence; Behavioral research; Face recognition; Image recognition; Learning systems; Open systems; Affective Computing; Behavior analysis; Computer vision algorithms; Facial action unit recognition; Facial landmark detection; Head Pose Estimation; Interactive applications; Real time performance; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84977660584
"Mauderer M., Flatla D.R., Nacenta M.A.","6507047073;36141533300;10040644200;","Gaze-contingent manipulation of color perception",2016,"Conference on Human Factors in Computing Systems - Proceedings",,,,"5191","5202",,5,"10.1145/2858036.2858320","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015108121&doi=10.1145%2f2858036.2858320&partnerID=40&md5=1b5f2c96f1be34eee545e2f7b5491bc8","University of St. Andrews, United Kingdom; University of Dundee, United Kingdom","Mauderer, M., University of St. Andrews, United Kingdom; Flatla, D.R., University of Dundee, United Kingdom; Nacenta, M.A., University of St. Andrews, United Kingdom","Using real time eye tracking, gaze-contingent displays can modify their content to represent depth (e.g., through additional depth cues) or to increase rendering performance (e.g., by omitting peripheral detail). However, there has been no research to date exploring how gaze-contingent displays can be leveraged for manipulating perceived color. To address this, we conducted two experiments (color matching and sorting) that manipulated peripheral background and object colors to influence the user's color perception. Findings from our color matching experiment suggest that we can use gaze-contingent simultaneous contrast to affect color appearance and that existing color appearance models might not fully predict perceived colors with gaze-contingent presentation. Through our color sorting experiment we demonstrate how gaze-contingent adjustments can be used to enhance color discrimination. Gazecontingent color holds the promise of expanding the perceived color gamut of existing display technology and enabling people to discriminate color with greater precision.","Color perception; Eye tracking; Gaze-contingent displays; Local contrast enhancement; Simultaneous contrast","Color; Color vision; Human computer interaction; Human engineering; Color perception; Eye-tracking; Gaze-contingent displays; Local Contrast Enhancement; Simultaneous contrast; Color matching",Conference Paper,"Final","",Scopus,2-s2.0-85015108121
"Kulshreshth A., La Viola J.J., Jr.","55315352400;6602792780;","Dynamic stereoscopic 3D parameter adjustments for enhanced depth discrimination",2016,"Conference on Human Factors in Computing Systems - Proceedings",,,,"177","187",,9,"10.1145/2858036.2858078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015029060&doi=10.1145%2f2858036.2858078&partnerID=40&md5=40619a5460ed107fbf84ddb1e0e950ac","Department of Computer Science, University of Central Florida, Orlando, FL  32816, United States","Kulshreshth, A., Department of Computer Science, University of Central Florida, Orlando, FL  32816, United States; La Viola, J.J., Jr., Department of Computer Science, University of Central Florida, Orlando, FL  32816, United States","Most modern stereoscopic 3D applications use fixed stereoscopic 3D parameters (separation and convergence) to render the scene on a 3D display. But, keeping these parameters fixed during usage does not always provide the best experience since it can reduce the amount of depth perception possible in some applications which have large variability in object distances. We developed two stereoscopic rendering techniques which actively vary the stereo parameters based on the scene content. Our first algorithm calculates a low resolution depth map of the scene and chooses ideal stereo parameters based on that depth map. Our second algorithm uses eye tracking data to get the gaze direction of the user and chooses ideal stereo parameters based on the distance of the gazed object. We evaluated our techniques in an experiment that uses three depth judgment tasks: depth ranking, relative depth judgment and path tracing. Our results indicate that variable stereo parameters provide enhanced depth discrimination compared to static parameters and were preferred by our participants over the traditional fixed parameter approach. We discuss our findings and possible implications on the design of future stereoscopic 3D applications. © 2016 ACM.","Depth judgment; Dynamic stereo parameters; Eye tracking; HCI; Stereoscopic 3D; User experience; User study; Video games; Visual fatigue","Depth perception; Human computer interaction; Human engineering; Parameter estimation; Rendering (computer graphics); Stereo image processing; Depth judgment; Eye-tracking; User experience; User study; Video game; Visual fatigue; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85015029060
"Templier T., Bektas K., Hahnloser R.H.R.","57193570832;54413851200;6701353184;","Eye-trace: Segmentation of volumetric microscopy images with eyegaze",2016,"Conference on Human Factors in Computing Systems - Proceedings",,,,"5812","5823",,2,"10.1145/2858036.2858578","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015015650&doi=10.1145%2f2858036.2858578&partnerID=40&md5=fc80c0131ddca3c5af5fce3d5cdb723f","Institute of Neuroinformatics, University of Zurich, ETH Zurich, Neuroscience Center Zurich, Zurich, Switzerland","Templier, T., Institute of Neuroinformatics, University of Zurich, ETH Zurich, Neuroscience Center Zurich, Zurich, Switzerland; Bektas, K., Institute of Neuroinformatics, University of Zurich, ETH Zurich, Neuroscience Center Zurich, Zurich, Switzerland; Hahnloser, R.H.R., Institute of Neuroinformatics, University of Zurich, ETH Zurich, Neuroscience Center Zurich, Zurich, Switzerland","We introduce an image annotation approach for the analysis of volumetric electron microscopic imagery of brain tissue. The core task is to identify and link tubular objects (neuronal fibers) in images taken from consecutive ultrathin sections of brain tissue. In our approach an individual 'flies' through the 3D data at a high speed and maintains eye gaze focus on a single neuronal fiber, aided by navigation with a handheld gamepad controller. The continuous foveation on a fiber of interest constitutes an intuitive means to define a trace that is seamlessly recorded with a desktop eyetracker and transformed into precise 3D coordinates of the annotated fiber (skeleton tracing). In a participant experiment we validate the approach by demonstrating a tracing accuracy of about the respective radiuses of the traced fibers with browsing speeds of up to 40 brain sections per second. © 2016 ACM.","Annotation; Array tomography; Brain mapping; Connectomics; Eye tracking; Neural circuit reconstruction; Segmentation; User interface design","Air navigation; Brain; Fibers; Human computer interaction; Human engineering; Image segmentation; Tissue; Tissue engineering; User interfaces; Annotation; Connectomics; Eye-tracking; Neural circuit reconstruction; User interface designs; Brain mapping",Conference Paper,"Final","",Scopus,2-s2.0-85015015650
"Bulling A., Kunze K.","6505807414;21743317500;","Eyewear computers for human-computer interaction",2016,"Interactions","23","3",,"70","73",,20,"10.1145/2912886","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969256149&doi=10.1145%2f2912886&partnerID=40&md5=3f9379a4c119c7c55f7b0fc9b7221fb4","Max Planck Institute for Informatics, Germany; Graduate School of Media Design, Keio University, Yokohama, Japan","Bulling, A., Max Planck Institute for Informatics, Germany; Kunze, K., Graduate School of Media Design, Keio University, Yokohama, Japan","Head-worn displays and eye trackers, augmented and virtual reality glasses, egocentric cameras, and other smart eyewear have recently emerged as a research platform in fields such as ubiquitous computing, computer vision, and cognitive and social science. Recent technological advances have made eyewear unobtrusive and lightweight, and more suitable for daily use. Vision-based eye tracking relies on special-purpose eye cameras along with infrared illumination, and provides gaze direction in either 2D-scene-camera or 3D-world coordinates.",,"Cameras; Computer vision; Eye movements; Social sciences computing; Ubiquitous computing; Virtual reality; Augmented and virtual realities; Gaze direction; Head-worn displays; Infrared illumination; Research platforms; Technological advances; Vision based; World coordinates; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84969256149
"Prieto L.P., Sharma K., Dillenbourg P., Jesús M.","36936110000;55903734200;8912010400;51161955600;","Teaching analytics: Towards automatic extraction of orchestration graphs using wearable sensors",2016,"ACM International Conference Proceeding Series","25-29-April-2016",,,"148","157",,55,"10.1145/2883851.2883927","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976521244&doi=10.1145%2f2883851.2883927&partnerID=40&md5=1cc84f3b5a6ea8c4b72c2a7651420b5d","EPFL RLC D1 740, CHILI Lab, Station 20, Lausanne, 1015, Switzerland; EPFL ME A3 30, Rodríguez-Triana REACT Lab, Station 9, Lausanne, 1015, Switzerland","Prieto, L.P., EPFL RLC D1 740, CHILI Lab, Station 20, Lausanne, 1015, Switzerland; Sharma, K., EPFL RLC D1 740, CHILI Lab, Station 20, Lausanne, 1015, Switzerland; Dillenbourg, P., EPFL RLC D1 740, CHILI Lab, Station 20, Lausanne, 1015, Switzerland; Jesús, M., EPFL ME A3 30, Rodríguez-Triana REACT Lab, Station 9, Lausanne, 1015, Switzerland","Teaching analytics' is the application of learning analytics techniques to understand teaching and learning processes, and eventually enable supportive interventions. However, in the case of (often, half-improvised) teaching in face-to-face classrooms, such interventions would require first an understanding of what the teacher actually did, as the starting point for teacher reection and inquiry. Currently, such teacher enactment characterization requires costly manual coding by researchers. This paper presents a case study exploring the potential of machine learning techniques to automatically extract teaching actions during classroom enactment, from five data sources collected using wearable sensors (eye-tracking, EEG, accelerometer, audio and video). Our results highlight the feasibility of this approach, with high levels of accuracy in determining the social plane of interaction (90%, k=0.8). The reliable detection of concrete teaching activity (e.g., explanation vs. questioning) accurately still remains challenging (67%, k=0.56), a fact that will prompt further research on multimodal features and models for teaching activity extraction, as well as the collection of a larger multimodal dataset to improve the accuracy and generalizability of these methods. © 2016 ACM.","Activity detection; Multimodal learning analytics; Teacher reection; Teaching analytics; Wearable sensors","Artificial intelligence; Education; Extraction; Learning systems; Teaching; Wearable technology; Activity detection; Automatic extraction; Machine learning techniques; Multi-modal learning; Teacher reection; Teaching activities; Teaching analytics; Teaching and learning; Wearable sensors",Conference Paper,"Final","",Scopus,2-s2.0-84976521244
"Chun J., Bae B., Jo S.","57217727905;57224716106;55246098400;","BCI based hybrid interface for 3D object control in virtual reality",2016,"4th International Winter Conference on Brain-Computer Interface, BCI 2016",,,"7457461","","",,13,"10.1109/IWW-BCI.2016.7457461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969141971&doi=10.1109%2fIWW-BCI.2016.7457461&partnerID=40&md5=fe00410a1031cc02f9fe7d4bed28c393","School of Computing, Korea Advanced Institute of Science and Technology(KAIST), Daejeon, South Korea","Chun, J., School of Computing, Korea Advanced Institute of Science and Technology(KAIST), Daejeon, South Korea; Bae, B., School of Computing, Korea Advanced Institute of Science and Technology(KAIST), Daejeon, South Korea; Jo, S., School of Computing, Korea Advanced Institute of Science and Technology(KAIST), Daejeon, South Korea","People attempts to apply the virtual reality (VR) technology in various fields recently, however, there are many limitations to apply the VR technology in existing interfaces in various fields such as 3D object control. To solve this problem, we propose a combination of eye-tracking and BCI technique to control 3D objects in a three-dimensional VR as an alternative interface. In our proposed interface, users select a virtual 3D object in VR by eye-gazing which is detect by the eye-tracking module of the system and manipulate the object by concentrating their mind via the BCI module. To evaluate the performance of our system, subjects perform the same experiments using the proposed system comparing to other existing interfaces. The result shows that the proposed interface has similar or better performance than other interfaces. This result suggests that our proposed interface can be used as an alternative interface of VR. © 2016 IEEE.","3D Object Control; Brain-Computer Interface; Electroencephalography (EEG); Eye-tracking; Virtual Reality (VR)","Electroencephalography; Electrophysiology; Interfaces (computer); Virtual reality; 3D object; Eye-gazing; Eye-tracking; Hybrid interface; Other interfaces; Virtual 3D objects; VR technology; Brain computer interface",Conference Paper,"Final","",Scopus,2-s2.0-84969141971
"Adiba A.I., Tanaka N., Miyake J.","36809156700;55725499200;7101748008;","An adjustable gaze tracking system and its application for automatic discrimination of interest objects",2016,"IEEE/ASME Transactions on Mechatronics","21","2","7210210","973","979",,6,"10.1109/TMECH.2015.2470522","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963825629&doi=10.1109%2fTMECH.2015.2470522&partnerID=40&md5=0cad0675cdb8077947920b13951c6876","Department of Mechanical and Bioengineering, Osaka University, Toyonaka, 560-8531, Japan; Laboratory for Integrated Biodevice, Quantitative Biology Center, RIKEN, Saitama, 351-0198, Japan","Adiba, A.I., Department of Mechanical and Bioengineering, Osaka University, Toyonaka, 560-8531, Japan; Tanaka, N., Laboratory for Integrated Biodevice, Quantitative Biology Center, RIKEN, Saitama, 351-0198, Japan; Miyake, J., Department of Mechanical and Bioengineering, Osaka University, Toyonaka, 560-8531, Japan","Gaze tracking (GT) systems have attracted the attention of researchers as well as the commercial sector. Commercial systems are readily available, but they are usually fabricated at the same size. In this study, we propose a three-dimensional (3-D)-printable frame and an open-source system to fabricate a wearable GT system with low-cost configuration and reasonable performance. A 3-D printer achieves repeatable and adjustable design. The estimated price to make the GT hardware is less than €100, and the system's average accuracy is 2.58°. Our GT system has a 24-Hz sampling rate, which can analyze human interest points. Another contribution of this study is developing the automatic discrimination of interest objects using our proposed GT and machine learning. The output of this combination system is a database that consists of labeled objects. The label with the highest frequency indicates the object of the highest interest for a corresponding user. Our combination system has a 7-Hz sampling rate and a 3.8% classification error. © 1996-2012 IEEE.","3D printer; Auto-discrimination; Eye detection; Human-gaze; Visual programming","3D printers; Artificial intelligence; Combinatorial switching; Computer programming; Eye protection; Learning systems; Open source software; Printing machinery; Printing presses; Tracking (position); Auto-discrimination; Classification errors; Combination systems; Eye detection; Gaze tracking system; Human-gaze; Threedimensional (3-d); Visual programming; Open systems",Article,"Final","",Scopus,2-s2.0-84963825629
"Berger M., De Souza A.F., Neto J.D.O., de Aguiar E., Oliveira-Santos T.","55606990000;55425796800;55606823300;15134999700;36814134400;","Visual tracking with VG-RAM Weightless Neural Networks",2016,"Neurocomputing","183",,,"90","105",,6,"10.1016/j.neucom.2015.04.127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954305364&doi=10.1016%2fj.neucom.2015.04.127&partnerID=40&md5=35ae04f706a80a8a5862824e5fae97b5","Departamento de Informática, Universidade Federal do Espírito Santo, Av. Fernando Ferrari 541, Vitória, ES, 29075-910, Brazil; Departamento de Computação e Eletrônica, Universidade Federal do Espírito Santo, Rodovia BR 101 Norte km. 60, São Mateus, ES, 29932-540, Brazil","Berger, M., Departamento de Informática, Universidade Federal do Espírito Santo, Av. Fernando Ferrari 541, Vitória, ES, 29075-910, Brazil; De Souza, A.F., Departamento de Informática, Universidade Federal do Espírito Santo, Av. Fernando Ferrari 541, Vitória, ES, 29075-910, Brazil; Neto, J.D.O., Departamento de Informática, Universidade Federal do Espírito Santo, Av. Fernando Ferrari 541, Vitória, ES, 29075-910, Brazil; de Aguiar, E., Departamento de Computação e Eletrônica, Universidade Federal do Espírito Santo, Rodovia BR 101 Norte km. 60, São Mateus, ES, 29932-540, Brazil; Oliveira-Santos, T., Departamento de Informática, Universidade Federal do Espírito Santo, Av. Fernando Ferrari 541, Vitória, ES, 29075-910, Brazil","We present a biologically inspired long-term object tracking system based on Virtual Generalizing Random Access Memory (VG-RAM) Weightless Neural Networks (WNN). VG-RAM WNN is an effective machine learning technique that offers simple implementation and fast training. Our system models the biological saccadic eye movement, the transformation suffered by the images captured by the eyes from the retina to the Superior Colliculus (SC), and the response of SC neurons to previously seen patterns. We evaluated the performance of our system using a well-known visual tracking database. Our experimental results show that our approach is capable of reliably and efficiently track an object of interest in a video with accuracy equivalent or superior to related work. © 2016 Elsevier B.V.","Saccade; Superior colliculus; VG-RAM; Visual tracking; Weightless neural networks","Artificial intelligence; Eye movements; Learning systems; Neural networks; Tracking (position); Biologically inspired; Machine learning techniques; Object Tracking; Random access memory; Saccadic eye movements; Superior colliculus; Visual Tracking; Weightless neural networks; Random access storage; Article; artificial neural network; computer memory; eye tracking; human; image processing; machine learning; nerve cell; nonhuman; priority journal; retina; saccadic eye movement; superior colliculus; videorecording; virtual generalizing random access memory; weightless neural network",Article,"Final","",Scopus,2-s2.0-84954305364
"Lu F., Chen X.","54956194300;13410318100;","Person-independent eye gaze prediction from eye images using patch-based features",2016,"Neurocomputing","182",,,"10","17",,10,"10.1016/j.neucom.2015.07.125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954287829&doi=10.1016%2fj.neucom.2015.07.125&partnerID=40&md5=e111f71cac2eeb4cea3e25901b4ec0b1","School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China","Lu, F., School of Computer Science and Engineering, Beihang University, Beijing, 100191, China, International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; Chen, X., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China","This paper delivers a preliminary attempt towards person-independent appearance-based gaze estimation. Conventional methods need to assume training and test data collected from the same person, otherwise eye shape difference due to individuality will affect the estimation severely. To solve this problem, the key idea in this paper is to extract from eye images more advanced eye features, which helps learn a person-independent relationship between eye gaze change and eye appearance variation. To this end, we propose employing the advantages of recent sparse auto-encoding techniques. We partition any eye image into small patches which can overlap with each other. With patches from many images, we learn a codebook comprising a set of bases, which can reconstruct any eye image patch with sparse coefficients. By examining these coefficients, we can analyze the eye shape more effectively. Finally, we produce the eye features by pooling the coefficients at different scales, and then combine these subfeatures from different codebooks. Experimental results show that the proposed method achieves good accuracy on a public dataset and it also outperforms conventional methods by a large margin. © 2015 Elsevier B.V.","Eye image; Gaze direction classification; Gaze estimation; Sparse auto-encoder","Large dataset; Signal encoding; Auto encoders; Conventional methods; Encoding techniques; Eye images; Eye-gaze predictions; Gaze direction; Gaze estimation; Person-independent; Image processing; accuracy; Article; classification algorithm; correlation coefficient; data base; gaze; gaze direction algorithm; image analysis; image processing; image reconstruction; mathematical analysis; prediction; priority journal; virtual reality",Article,"Final","",Scopus,2-s2.0-84954287829
"Mansouryar M., Steil J., Sugano Y., Bulling A.","57204207379;57170107900;7005470045;6505807414;","3D gaze estimation from 2D pupil positions on monocular head-mounted eye trackers",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"197","200",,33,"10.1145/2857491.2857530","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975317287&doi=10.1145%2f2857491.2857530&partnerID=40&md5=41bfdfbf2ebc6956f21799fa3e7486cf","Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany","Mansouryar, M., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Steil, J., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Sugano, Y., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Bulling, A., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany","3D gaze information is important for scene-centric attention analysis, but accurate estimation and analysis of 3D gaze in real-world environments remains challenging. We present a novel 3D gaze estimation method for monocular head-mounted eye trackers. In contrast to previous work, our method does not aim to infer 3D eyeball poses, but directly maps 2D pupil positions to 3D gaze directions in scene camera coordinate space. We first provide a detailed discussion of the 3D gaze estimation task and summarize different methods, including our own. We then evaluate the performance of different 3D gaze estimation approaches using both simulated and real data. Through experimental validation, we demonstrate the effectiveness of our method in reducing parallax error, and we identify research challenges for the design of 3D calibration procedures. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D gaze estimation; Head-mounted eye tracking; Parallax error","Geometrical optics; Accurate estimation; Coordinate space; Experimental validations; Gaze estimation; Head-mounted eye tracking; Parallax error; Real world environments; Research challenges; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84975317287
"Wang K., Wang S., Ji Q.","56637259500;57189844395;18935108400;","Deep eye fixation map learning for calibration-free eye gaze tracking",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"47","55",,32,"10.1145/2857491.2857515","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975311986&doi=10.1145%2f2857491.2857515&partnerID=40&md5=02c729d78aa9187ac5f9f589cafd64d6","Rensselaer Polytechnic Institute, United States; University of Illinois at Chicago, United States","Wang, K., Rensselaer Polytechnic Institute, United States; Wang, S., University of Illinois at Chicago, United States; Ji, Q., Rensselaer Polytechnic Institute, United States","The existing eye trackers typically require an explicit personal calibration procedure to estimate subject-dependent eye parameters. Despite efforts in simplifying the calibration process, such a calibration process remains unnatural and bothersome, in particular for users of personal and mobile devices. To alleviate this problem, we introduce a technique that can eliminate explicit personal calibration. Based on combining a new calibration procedure with the eye fixation prediction, the proposed method performs implicit personal calibration without active participation or even knowledge of the user. Specifically, different from traditional deterministic calibration procedure that minimizes the differences between the predicted eye gazes and the actual eye gazes, we introduce a stochastic calibration procedure that minimizes the differences between the probability distribution of the predicted eye gaze and the distribution of the actual eye gaze. Furthermore, instead of using saliency map to approximate eye fixation distribution, we propose to use a regression based deep convolutional neural network (RCNN) that specifically learns image features to predict eye fixation. By combining the distribution based calibration with the deep fixation prediction procedure, personal eye parameters can be estimated without explicit user collaboration. We apply the proposed method to both 2D regression-based and 3D model-based eye gaze tracking methods. Experimental results show that the proposed method outperforms other implicit calibration methods and achieve comparable results to those that use traditional explicit calibration methods. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Calibration-free; Eye gaze; Saliency","Eye movements; Forecasting; Mobile devices; Neural networks; Probability distributions; Stochastic systems; Tracking (position); Calibration free; Calibration method; Calibration procedure; Calibration process; Convolutional neural network; Eye-gaze; Saliency; User collaborations; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84975311986
"Thomaz C.E., Amaral V., Gillies D.F., Rueckert D.","16023624400;57189385860;35464353900;7004895812;","Priori-driven dimensions of face-space: Experiments incorporating eye-tracking information",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"279","282",,,"10.1145/2857491.2857508","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975302948&doi=10.1145%2f2857491.2857508&partnerID=40&md5=3f936c2e3bcf5e3392d63f4c8aae19bf","Department of Electrical Engineering, Centro Universitario da FEI, Sao Paulo, Brazil; Department of Computing, Imperial College London, United Kingdom","Thomaz, C.E., Department of Electrical Engineering, Centro Universitario da FEI, Sao Paulo, Brazil; Amaral, V., Department of Electrical Engineering, Centro Universitario da FEI, Sao Paulo, Brazil; Gillies, D.F., Department of Computing, Imperial College London, United Kingdom; Rueckert, D., Department of Computing, Imperial College London, United Kingdom","Face-space has become established as an effective model for representing the dimensions of variation that occur in collections of human faces. For example, a change of expression from neutral to smiling can be represented by one axis in a face space. Principal components can be used to determine the axes of a face-space, however, standard principal components are based entirely on the data set from which they are computed, and do not express any domain specific information about the application of interest. In this paper, we propose a face-space analysis that combines the variance criterion used in principal components with some prior knowledge about the task-driven experiment. The priors are based on measuring eye movements of participants to frontal 2D faces during separate gender and facial expression categorization tasks. Our findings show that saccades to faces are task-driven, especially from 500 to 1000 milliseconds, and automatic recognition performance does not improve with additional exposure time. © 2016 Copyright held by the owner/author(s).","Eye tracking; Face space; Priori-driven dimensions","Face recognition; Principal component analysis; Automatic recognition; Domain-specific information; Eye-tracking; Face space; Facial Expressions; Principal Components; Prior knowledge; Priori-driven dimensions; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84975302948
"Sakai D., Yamamoto M., Nagamatsu T., Fukumori S.","57127784000;56328923300;23398000100;39862796000;","Enter your PIN code securely! - Utilization of personal difference of angle kappa",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"317","318",,3,"10.1145/2857491.2884059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975302210&doi=10.1145%2f2857491.2884059&partnerID=40&md5=ea2fafd3a8c1799f264e9faf87ad023f","Kwansei Gakuin University, Japan; Kobe University, Japan","Sakai, D., Kwansei Gakuin University, Japan; Yamamoto, M., Kwansei Gakuin University, Japan; Nagamatsu, T., Kobe University, Japan; Fukumori, S., Kwansei Gakuin University, Japan","In previous studies, the angle kappa, the offset between the optical axis and visual axis, was considered to be calibrated when eye trackers based on a 3D model were used. However, we found that the angle kappa could be used as personal information, which is immeasurable from outside the human body. This paper proposes a concept for PIN entry by considering the characteristics of the angle kappa. Thus, we measured the distribution of the angle kappa and developed a prototype of the system. We demonstrated the effectiveness of the method. © 2016 Copyright held by the owner/author(s).","3D-model based eye trackers; Angle Kappa; PIN entry","Eye movements; 3-d modeling; Angle Kappa; Eye trackers; Human bodies; Optical axis; Personal information; PIN entry; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-84975302210
"Borsato F.H., Morimoto C.H.","7801411327;7102275798;","Episcleral surface tracking: Challenges and possibilities for using mice sensors for wearable eye tracking",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"39","46",,4,"10.1145/2857491.2857496","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975299427&doi=10.1145%2f2857491.2857496&partnerID=40&md5=677418a9eb5c053c68c7b26b3de52fbd","Computer Science Department, UTFPR, Brazil; Computer Science Department, University of São Paulo, Brazil","Borsato, F.H., Computer Science Department, UTFPR, Brazil; Morimoto, C.H., Computer Science Department, University of São Paulo, Brazil","Video-based eye trackers (VETs) have become the dominant eye tracking technology due to its reasonable cost, accuracy, and easy of use. VETs require real-time image processing to detect and track eye features such as the center of the pupil and corneal reflection to estimate the point of regard. Despite the continuous evolution of cameras and computers that made head mounted eye trackers easier to use in natural activities, real-time processing of high resolution images in mobile devices remains a challenge. In this paper we investigate the feasibility of a novel eye-tracking technique intended for wearable applications that use mice chips as imaging sensors. Such devices are widely available at very low cost, and provide high speed and accurate 2D tracking data. Though mice chips have been used for many purposes other than a computer's pointing device, to our knowledge this is the first attempt to use it as an eye tracker. To validate the technique, we built an episcleral database with about 100 high resolution episcleral patches from 7 individuals. The episclera is the outer most layer of the sclera, which is the white part of the eye, and consists of dense vascular connective tissue. We have used the patches to determine if the episclera contains enough texture to be reliably tracked. We also present results from a prototype built using an off-the-shelf mouse sensor. Our results show that a mouse-based eye tracker has the potential to be very accurate, precise, and fast (measuring 2.1' of visual angle at 1 KHz speed), with little overhead for the wearable computer. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Eye tracking; Mouse sensor; Sclera database; Translation sensor","Collagen; Eye movements; Image processing; Mammals; Mobile devices; Musculoskeletal system; Wearable technology; Corneal reflection; Eye tracking technologies; Eye-tracking; High resolution image; Mouse sensor; Real-time image processing; Realtime processing; Wearable applications; Wearable sensors",Conference Paper,"Final","",Scopus,2-s2.0-84975299427
"Wood E., Baltrušaitis T., Morency L.-P., Robinson P., Bulling A.","56145872800;36696075900;6603047400;57205369790;6505807414;","Learning an appearance-based gaze estimator from one million synthesised images",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"131","138",,126,"10.1145/2857491.2857492","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975298409&doi=10.1145%2f2857491.2857492&partnerID=40&md5=503376b5766f5a3d6a85fd1319b5c8f5","University of Cambridge, United Kingdom; Carnegie Mellon University, United States; Max Planck Institute for Informatics, Germany","Wood, E., University of Cambridge, United Kingdom; Baltrušaitis, T., Carnegie Mellon University, United States; Morency, L.-P., Carnegie Mellon University, United States; Robinson, P., University of Cambridge, United Kingdom; Bulling, A., Max Planck Institute for Informatics, Germany","Learning-based methods for appearance-based gaze estimation achieve state-of-the-art performance in challenging real-world settings but require large amounts of labelled training data. Learningby-synthesis was proposed as a promising solution to this problem but current methods are limited with respect to speed, appearance variability, and the head pose and gaze angle distribution they can synthesize. We present UnityEyes, a novel method to rapidly synthesize large amounts of variable eye region images as training data. Our method combines a novel generative 3D model of the human eye region with a real-time rendering framework. The model is based on high-resolution 3D face scans and uses real-time approximations for complex eyeball materials and structures as well as anatomically inspired procedural geometry methods for eyelid animation. We show that these synthesized images can be used to estimate gaze in difficult in-the-wild scenarios, even for extreme gaze angles or in cases in which the pupil is fully occluded. We also demonstrate competitive gaze estimation results on a benchmark in-the-wild dataset, despite only using a light-weight nearestneighbor algorithm. We are making our UnityEyes synthesis framework available online for the benefit of the research community. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D morphable model; Appearance-based gaze estimation; Learning-bysynthesis; Real-time rendering","Rendering (computer graphics); Three dimensional computer graphics; 3D Morphable model; Gaze estimation; Learning-based methods; Learning-bysynthesis; Nearest-neighbor algorithms; Real-time rendering; Research communities; State-of-the-art performance; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-84975298409
"Pfeiffer T., Renner P., Pfeiffer-Leßmann N.","14027435500;56145227600;25653279000;","EyeSee3D 2.0: Model-based real-time analysis of mobile eye-tracking in static and dynamic three-dimensional scenes",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"189","196",,25,"10.1145/2857491.2857532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975295527&doi=10.1145%2f2857491.2857532&partnerID=40&md5=8a0238bd0fbbcc9d97c77fa95b843e73","Center of Excellence Cognitive Interaction Technology, Bielefeld University, Bielefeld, Germany","Pfeiffer, T., Center of Excellence Cognitive Interaction Technology, Bielefeld University, Bielefeld, Germany; Renner, P., Center of Excellence Cognitive Interaction Technology, Bielefeld University, Bielefeld, Germany; Pfeiffer-Leßmann, N., Center of Excellence Cognitive Interaction Technology, Bielefeld University, Bielefeld, Germany","With the launch of ultra-portable systems, mobile eye tracking finally has the potential to become mainstream. While eye movements on their own can already be used to identify human activities, such as reading or walking, linking eye movements to objects in the environment provides even deeper insights into human cognitive processing. We present a model-based approach for the identification of fixated objects in three-dimensional environments. For evaluation, we compare the automatic labelling of fixations with those performed by human annotators. In addition to that, we show how the approach can be extended to support moving targets, such as individual limbs or faces of human interaction partners. The approach also scales to studies using multiple mobile eye-tracking systems in parallel. The developed system supports real-time attentive systems that make use of eye tracking as means for indirect or direct humancomputer interaction as well as off-line analysis for basic research purposes and usability studies. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D; Augmented reality; Eye tracking; Gaze analysis; Joint attention; Mobile eye tracking; Social interaction","Augmented reality; Real time systems; Eye-tracking; Gaze analysis; Joint attention; Mobile eye-tracking; Social interactions; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84975295527
"Pieszala J., Diaz G., Pelz J., Speir J., Bailey R.","57079289100;55436582600;7007018556;55156153000;16641965200;","3D Gaze Point Localization and Visualization Using LiDAR-based 3D reconstructions",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"201","204",,6,"10.1145/2857491.2857545","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975291125&doi=10.1145%2f2857491.2857545&partnerID=40&md5=161d5b2d3ba20435a0ad0535ee55a311","Rochester Institute of Technology, United States; West Virginia University, United States","Pieszala, J., Rochester Institute of Technology, United States; Diaz, G., Rochester Institute of Technology, United States; Pelz, J., Rochester Institute of Technology, United States; Speir, J., West Virginia University, United States; Bailey, R., Rochester Institute of Technology, United States","We present a novel pipeline for localizing a free roaming eye tracker within a LiDAR-based 3D reconstructed scene with high levels of accuracy. By utilizing a combination of reconstruction algorithms that leverage the strengths of global versus local capture methods and user-assisted refinement, we reduce drift errors associated with Dense-SLAM techniques. Our framework supports regionof-interest (ROI) annotation and gaze statistics generation and the ability to visualize gaze in 3D from an immersive first person or third person perspective. This approach gives unique insights into viewers' problem solving and search task strategies and has high applicability in complex static environments such as crime scenes. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D point-of-regard; 3D reconstruction; Computer vision; Eye-tracking; Virtual reality","Algorithms; Computer vision; Image reconstruction; Optical radar; Problem solving; Virtual reality; 3D reconstruction; Drift errors; Eye-tracking; Point of regards; Reconstruction algorithms; Region of interest; Search tasks; Static environment; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84975291125
"Kothari R., Binaee K., Matthis J.S., Bailey R., Diaz G.J.","56533410500;48161042400;53164518200;16641965200;55436582600;","Novel apparatus for investigation of eye movements when walking in the presence of 3D projected obstacles",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"261","265",,3,"10.1145/2857491.2857540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975284873&doi=10.1145%2f2857491.2857540&partnerID=40&md5=e539ef4bd7661aeb58b8558b8dfda8dd","Rochester Institute of Technology, United States; University of Texas at Austin, United States","Kothari, R., Rochester Institute of Technology, United States; Binaee, K., Rochester Institute of Technology, United States; Matthis, J.S., University of Texas at Austin, United States; Bailey, R., Rochester Institute of Technology, United States; Diaz, G.J., Rochester Institute of Technology, United States","This manuscript details a novel approach for the study of gaze behavior in a controlled naturalistic walking environment. In a single case study, a motion-tracked participant walked along path obstructed by augmented reality obstacles. Despite measurable effects of obstacle size on the kinematics of walking behavior, the subject's gaze behavior remained unchanged. These findings suggest that gaze behavior is incredibly robust to competing intentional demands in the one subject tested. More importantly, this study validates a novel apparatus as suitable for recording both the gaze direction and gait kinematics of a human walker. Future plans will further explore the intricacies of human behavior through alternative experimental designs. For example, increasing the complexity of either the ground plane (e.g. with additional obstacles) or the distractor task, perhaps can exaggerate the consequences of gaze shifts to and from competing tasks on walking behavior. Similarly, one might emphasize the role of vision in walking by manipulating the visibility of obstacles based upon the gaze location during locomotion, perhaps degrading the observer's ability to use information in the fovea during locomotion. One can similarly manipulate parameters of the ground plane based upon the time evolving kinematics of walking behavior. © 2016 Copyright held by the owner/author(s).","Augmented reality; Eye-tracking; Foot placement; Gait; Kinesiology; Motion capture; Postural control; Walking behavior","Augmented reality; Behavioral research; Kinematics; Eye-tracking; Foot placements; Gait; Kinesiology; Motion capture; Postural control; Walking behavior; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84975284873
"Wang K., Ji Q.","56637259500;18935108400;","Hybrid model and appearance based eye tracking with Kinect",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"331","332",,3,"10.1145/2857491.2888591","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975263425&doi=10.1145%2f2857491.2888591&partnerID=40&md5=8a7b9a0e090db4be36740baa63bec63a","Rensselaer Polytechnic Institute, United States","Wang, K., Rensselaer Polytechnic Institute, United States; Ji, Q., Rensselaer Polytechnic Institute, United States","Existing gaze estimation methods rely mainly on 3D eye model or 2D eye appearance. While both methods have validated their effectiveness in various fields and applications, they are still limited in practice, such as portable and non-intrusive system and robust eye gaze tracking in different environments. To this end, we investigate on combining eye model with eye appearance to perform gaze estimation and eye gaze tracking. Specifically, unlike traditional 3D model based methods which rely on cornea reflections, we plan to retrieve 3D information from depth sensor (Eg, Kinect). Kinect integrates camera sensor and IR illuminations into one single device, thus enable more flexible system settings. We further propose to utilize appearance information to help the basic model based methods. Appearance information can help better detection of gaze related features (Eg, pupil center). Plus, eye model and eye appearance can benefit each other to enable robust and accurate gaze estimation. © 2016 Copyright held by the owner/author(s).","3D eye model; Depth sensor; Eye appearance; Learning","Tracking (position); 3D eye models; Appearance based; Depth sensors; Eye appearance; Eye gaze tracking; Flexible system; Learning; Model-based method; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84975263425
"Topić G., Yamaya A., Aizawa A., Martínez-Gómez P.","35103593500;56906986500;6701312731;37007615900;","FixFix: Fixing the fixations",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"319","320",,3,"10.1145/2857491.2884060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975261057&doi=10.1145%2f2857491.2884060&partnerID=40&md5=f879192f9fc54a82f04df45f08462119",,"Topić, G.; Yamaya, A.; Aizawa, A.; Martínez-Gómez, P.","FixFix is a web-based tool for editing reading gaze fixation datasets. The purpose is to provide gaze researchers focusing on reading an easy-to-use interface that will facilitate manual interpretation, but even more so to create gold standard datasets for machine learning and data mining. It allows the users to identify fixations, then move them either singly or in groups, in order to correct both variable and systematic gaze sampling errors. © 2016 Copyright held by the owner/author(s).","Editor; Eye tracking; Gaze data; Reading","Artificial intelligence; Data mining; Learning systems; Systematic errors; Editor; Eye-tracking; Gaze data; Gold standards; Reading; Sampling errors; Web-based tools; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84975261057
"Pfeiffer T., Memili C.","14027435500;57063541000;","Model-based real-time visualization of realistic three-dimensional heat maps for mobile eye tracking and eye tracking in virtual reality",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"95","102",,12,"10.1145/2857491.2857541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975260964&doi=10.1145%2f2857491.2857541&partnerID=40&md5=1dab5a3d38768965bc25ceded44d6ec7","Center of Excellence Cognitive Interaction Technology, Bielefeld University, Bielefeld, Germany","Pfeiffer, T., Center of Excellence Cognitive Interaction Technology, Bielefeld University, Bielefeld, Germany; Memili, C., Center of Excellence Cognitive Interaction Technology, Bielefeld University, Bielefeld, Germany","Heat maps, or more generally, attention maps or saliency maps are an often used technique to visualize eye-tracking data. With heat maps qualitative information about visual processing can be easily visualized and communicated between experts and laymen. They are thus a versatile tool for many disciplines, in particular for usability engineering, and are often used to get a first overview about recorded eye-tracking data. Today, heat maps are typically generated for 2D stimuli that have been presented on a computer display. In such cases the mapping of overt visual attention on the stimulus is rather straight forward and the process is well understood. However, when turning towards mobile eye tracking and eye tracking in 3D virtual environments, the case is much more complicated. In the first part of the paper, we discuss several challenges that have to be considered in 3D environments, such as changing perspectives, multiple viewers, object occlusions, depth of fixations, or dynamically moving objects. In the second part, we present an approach for the generation of 3D heat maps addressing the above mentioned issues while working in real-time. Our visualizations provide high-quality output for multi-perspective eye-tracking recordings of visual attention in 3D environments. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D; Eye tracking; Heat map; Visualization","Behavioral research; Flow visualization; Three dimensional computer graphics; Usability engineering; Virtual reality; Visual communication; Visualization; 3-D virtual environment; Eye-tracking; Heat maps; Mobile eye-tracking; Multi-perspective; Overt visual attentions; Qualitative information; Real time visualization; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84975260964
"Sanandaji A., Grimm C., West R., Parola M., Kajihara M., Deutsch J., Carlew A., Yates D.","55847420100;7101846420;7402395594;56605992000;57189851081;57189850027;56951228700;57206169435;","Where do experts look while doing 3D image segmentation",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"171","174",,5,"10.1145/2857491.2857538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975256303&doi=10.1145%2f2857491.2857538&partnerID=40&md5=c5fa54a1850986cfeca333fc7ef29eb2","Oregon State University, United States; University of North Texas, United States","Sanandaji, A., Oregon State University, United States; Grimm, C., Oregon State University, United States; West, R., University of North Texas, United States; Parola, M., University of North Texas, United States; Kajihara, M., University of North Texas, United States; Deutsch, J., University of North Texas, United States; Carlew, A., University of North Texas, United States; Yates, D., Oregon State University, United States","3D image segmentation is a fundamental process in many scientific and medical applications. Automatic algorithms do exist, but there are many use cases where these algorithms fail. The gold standard is still manual segmentation or review. Unfortunately, even for an expert this is laborious, time consuming, and prone to errors. Existing 3D segmentation tools do not currently take into account human mental models and low-level perception tasks. Our goal is to improve the quality and efficiency of manual segmentation and review by analyzing how experts perform segmentation. As a pre-Uminary step we conducted a field study with 8 segmentation experts, recording video and eye tracking data. We developed a novel coding scheme to analyze this data and verified that it successfully covers and quantifies the low-level actions, tasks and behaviors of experts during 3D image segmentation. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D image segmentation; Coding scheme; Perception","Algorithms; Image coding; Medical applications; Medical imaging; Sensory perception; 3D image segmentation; 3D segmentation; Automatic algorithms; Coding scheme; Field studies; Gold standards; Low-level perception; Manual segmentation; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-84975256303
"Kübler T.C., Rittig T., Kasneci E., Ungewiss J., Krauss C.","55701951700;57189852645;56059892600;56529736700;57189850616;","Rendering refraction and reflection of eyeglasses for synthetic eye tracker images",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"143","146",,7,"10.1145/2857491.2857494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975250739&doi=10.1145%2f2857491.2857494&partnerID=40&md5=457aa383fcbc571ec9ee18f150338a35","Wilhelm-Schickard-Institute of Computer Science, University of Tübingen, Germany; Study Course Ophthalmic Optics/Audiology, University of Applied Sciences Aalen, Germany","Kübler, T.C., Wilhelm-Schickard-Institute of Computer Science, University of Tübingen, Germany; Rittig, T., Wilhelm-Schickard-Institute of Computer Science, University of Tübingen, Germany; Kasneci, E., Wilhelm-Schickard-Institute of Computer Science, University of Tübingen, Germany; Ungewiss, J., Study Course Ophthalmic Optics/Audiology, University of Applied Sciences Aalen, Germany; Krauss, C., Study Course Ophthalmic Optics/Audiology, University of Applied Sciences Aalen, Germany","While for the evaluation of robustness of eye tracking algorithms the use of real-world data is essential, there are many applications where simulated, synthetic eye images are of advantage. They can generate labelled ground-truth data for appearance based gaze estimation algorithms or enable the development of model based gaze estimation techniques by showing the influence on gaze estimation error of different model factors that can then be simplified or extended. We extend the generation of synthetic eye images by a simulation of refraction and reflection for eyeglasses. On the one hand this allows for the testing of pupil and glint detection algorithms under different illumination and reflection conditions, on the other hand the error of gaze estimation routines can be estimated in conjunction with different eyeglasses. We show how a polynomial function fitting calibration performs equally well with and without eyeglasses, and how a geometrical eye model behaves when exposed to glasses. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Calibration; Eye tracking; Eyeglasses; Pupil detection; Reflection; Refraction; Rendering","Algorithms; Calibration; Eyeglasses; Reflection; Refraction; Appearance based; Detection algorithm; Eye-tracking; Ground truth data; Model-based OPC; Polynomial functions; Pupil detection; Rendering; Rendering (computer graphics)",Conference Paper,"Final","",Scopus,2-s2.0-84975250739
"Kang S.-J., Jeong Y.-W., Yun J.-J., Bae S.","35088306400;57189099018;56508972500;56315952100;","Real-time eye tracking techni que for multiview 3D systems",2016,"2016 IEEE International Conference on Consumer Electronics, ICCE 2016",,,"7430728","553","554",,1,"10.1109/ICCE.2016.7430728","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965169271&doi=10.1109%2fICCE.2016.7430728&partnerID=40&md5=eaa4f694b67ab3140fe90d8ed803a77a","Sogang University, South Korea; Dong-A University, South Korea; Daegu University, South Korea; Yeungnam University, South Korea","Kang, S.-J., Sogang University, South Korea; Jeong, Y.-W., Dong-A University, South Korea; Yun, J.-J., Daegu University, South Korea; Bae, S., Yeungnam University, South Korea","This paper presents the real-time multi-user eye tracking technique for multiview 3D systems. The proposed technique used the Haar feature-based face detection and classification. Then, it calculated the best matching template, and extracts eye positions based on biological proportion. Simulation results showed the proposed method enhanced the average F1 score up to 0.312, compared with conventional methods. © 2016 IEEE.",,"Consumer electronics; Face recognition; 3d systems; Best matching; Conventional methods; Eye position; Eye-tracking; Haar features; Multi-views; Real-time eye tracking; Real time systems",Conference Paper,"Final","",Scopus,2-s2.0-84965169271
"Bigdely-Shamlo N., Makeig S., Robbins K.A.","25639688900;7004563459;7201662444;","Preparing laboratory and real-world EEG data for large-scale analysis: A containerized approach",2016,"Frontiers in Neuroinformatics","10","MAR","7","","",,17,"10.3389/fninf.2016.00007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962319745&doi=10.3389%2ffninf.2016.00007&partnerID=40&md5=b2b1439c3c7b23e5ddb5bd26c39df11a","Qusp Labs, Qusp, San Diego, CA, United States; Swartz Center for Computational Neuroscience, University of California, San Diego, CA, United States; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, United States","Bigdely-Shamlo, N., Qusp Labs, Qusp, San Diego, CA, United States; Makeig, S., Swartz Center for Computational Neuroscience, University of California, San Diego, CA, United States; Robbins, K.A., Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, United States","Large-scale analysis of EEG and other physiological measures promises new insights into brain processes and more accurate and robust brain–computer interface models. However, the absence of standardized vocabularies for annotating events in a machine understandable manner, the welter of collection-specific data organizations, the difficulty in moving data across processing platforms, and the unavailability of agreed-upon standards for preprocessing have prevented large-scale analyses of EEG. Here we describe a “containerized” approach and freely available tools we have developed to facilitate the process of annotating, packaging, and preprocessing EEG data collections to enable data sharing, archiving, large-scale machine learning/data mining and (meta-)analysis. The EEG Study Schema (ESS) comprises three data “Levels,” each with its own XML-document schema and file/folder convention, plus a standardized (PREP) pipeline to move raw (Data Level 1) data to a basic preprocessed state (Data Level 2) suitable for application of a large class of EEG analysis methods. Researchers can ship a study as a single unit and operate on its data using a standardized interface. ESS does not require a central database and provides all the metadata data necessary to execute a wide variety of EEG processing pipelines. The primary focus of ESS is automated in-depth analysis and meta-analysis EEG studies. However, ESS can also encapsulate meta-information for the other modalities such as eye tracking, that are increasingly used in both laboratory and real-world neuroimaging. ESS schema and tools are freely available at www.eegstudy.org and a central catalog of over 850 GB of existing data in ESS format is available at studycatalog.org. These tools and resources are part of a larger effort to enable data sharing at sufficient scale for researchers to engagee in truly large-scale EEG analysis and data mining (BigEEG.org). © 2016 Bigdely-Shamlo, Makeig and Robbins.","BCI; EEG; Large scale analysis; Neuroinformatics","data base; data mining; electroencephalogram; eye tracking; human; neuroimaging; pipeline; publication; scientist",Article,"Final","",Scopus,2-s2.0-84962319745
"Bixler R., D’Mello S.","55789867400;14053463100;","Automatic gaze-based user-independent detection of mind wandering during computerized reading",2016,"User Modeling and User-Adapted Interaction","26","1",,"33","68",,58,"10.1007/s11257-015-9167-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944706244&doi=10.1007%2fs11257-015-9167-1&partnerID=40&md5=1332663c69cc40ae443e60a114d96425","Department of Computer Science, University of Notre Dame, Notre Dame, IN  46556, United States","Bixler, R., Department of Computer Science, University of Notre Dame, Notre Dame, IN  46556, United States; D’Mello, S., Department of Computer Science, University of Notre Dame, Notre Dame, IN  46556, United States","Mind wandering is a ubiquitous phenomenon where attention involuntarily shifts from task-related thoughts to internal task-unrelated thoughts. Mind wandering can have negative effects on performance; hence, intelligent interfaces that detect mind wandering can improve performance by intervening and restoring attention to the current task. We investigated the use of eye gaze and contextual cues to automatically detect mind wandering during reading with a computer interface. Participants were pseudorandomly probed to report mind wandering while an eye tracker recorded their gaze during the reading task. Supervised machine learning techniques detected positive responses to mind wandering probes from eye gaze and context features in a user-independent fashion. Mind wandering was detected with an accuracy of 72 % (expected accuracy by chance was 60 %) when probed at the end of a page and an accuracy of 67 % (chance was 59 %) when probed in the midst of reading a page. Global gaze features (gaze patterns independent of content, such as fixation durations) were more effective than content-specific local gaze features. An analysis of the features revealed diagnostic patterns of eye gaze behavior during mind wandering: (1) certain types of fixations were longer; (2) reading times were longer than expected; (3) more words were skipped; and (4) there was a larger variability in pupil diameter. Finally, the automatically detected mind wandering rate correlated negatively with measures of learning and transfer even after controlling for prior knowledge, thereby providing evidence of predictive validity. Possible improvements to the detector and applications that utilize the detector are discussed. © 2015, Springer Science+Business Media Dordrecht.","Gaze tracking; Mind wandering; User modeling","Artificial intelligence; Supervised learning; Tracking (position); Fixation duration; Gaze tracking; Improve performance; Intelligent interface; Mind wandering; Supervised machine learning; User independents; User Modeling; Learning systems",Article,"Final","",Scopus,2-s2.0-84944706244
"Zhang Y., Zheng X., Hong W., Mou X.","57016903800;57187383000;55144677700;7003389599;","A comparison study of stationary and mobile eye tracking on EXITs design in a wayfinding system",2016,"2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2015",,,"7415350","649","653",,7,"10.1109/APSIPA.2015.7415350","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986184073&doi=10.1109%2fAPSIPA.2015.7415350&partnerID=40&md5=8fc6e35e5b89d0185743fb91295938ed","Xi'An Jiao University, Xi'an, China; Sichuan University, Chengdu, China","Zhang, Y., Xi'An Jiao University, Xi'an, China, Sichuan University, Chengdu, China; Zheng, X., Xi'An Jiao University, Xi'an, China; Hong, W., Xi'An Jiao University, Xi'an, China; Mou, X., Xi'An Jiao University, Xi'an, China","""Wayfinding system"" is an interdisciplinary research between the environment design and human interface of information system. The design of EXIT is to efficiently help people to identify the emergency exit when the accident occurred. In this paper, eye tracking techniques are utilized to aided EXIT design in the wayfinding system. The mobile eye tracking was used to collect the human eye movement data in the building where different EXIT designs were displayed. While the stationary eye tracking techniques was used to collect the eye movement data on the same building's EXIT designs on virtual 3D sketches to get the quantitative data comparing with the mobile eye tracker. Finally, some general conclusions were obtained from the views of visual elements selecting, EXIT appearance design and EXIT's placement in the building, which is very valuable and can be commonly referred in wayfinding system. The research methods in the paper is very original and has not been mentioned in other papers of the related fields before, which is well worthy data and empirical methodologies can be introduced in wayfinding and space decision making research. © 2015 Asia-Pacific Signal and Information Processing Association.","Eye gaze tracking techniques; eye tracking metrics; space decision making; space recognition; wayfinding system","Behavioral research; Decision making; Design; Information science; Tracking (position); Environment design; Eye gaze tracking; Eye movement datum; Eye-tracking; Interdisciplinary research; Mobile eye-tracking; Space recognition; Way-finding systems; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84986184073
"El Hafi L., Takemura K., Takamatsu J., Ogasawara T.","57188836029;8575290600;35243684200;7201579979;","Model-based approach for gaze estimation from corneal imaging using a single camera",2016,"2015 IEEE/SICE International Symposium on System Integration, SII 2015",,,"7404959","88","93",,4,"10.1109/SII.2015.7404959","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963760990&doi=10.1109%2fSII.2015.7404959&partnerID=40&md5=08fc7cfb51ec1caad97813df1290475e","Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Department of Applied Computer Engineering, School of Information Science and Technology, Tokai University, 4-1-1 Kitakaname, Hiratsuka, Kanagawa, 259-1292, Japan","El Hafi, L., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Takemura, K., Department of Applied Computer Engineering, School of Information Science and Technology, Tokai University, 4-1-1 Kitakaname, Hiratsuka, Kanagawa, 259-1292, Japan; Takamatsu, J., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Ogasawara, T., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan","This paper describes a method to estimate the gaze direction using cornea images captured by a single camera. The purpose is to develop wearable devices capable of obtaining natural user responses, such as interests and behaviors, from eye movements and scene images reflected on the cornea. From an image of the eye, an ellipse is fitted on the colored iris area. A 3D eye model is reconstructed from the ellipse and rotated to simulate projections of the iris area for different eye poses. The gaze direction is then evaluated by matching the iris area of the current image with the corresponding projection obtained from the model. We finally conducted an experiment using a head-mounted prototype to demonstrate the potential of such an eye-tracking method solely based on cornea images captured from a single camera. © 2015 IEEE.",,"Cameras; Three dimensional computer graphics; 3D eye models; Current image; Eye tracking methods; Gaze direction; Gaze estimation; Model based approach; Single cameras; Wearable devices; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84963760990
"De Beugher S., Brône G., Goedemé T.","55776533900;6504039118;6506388986;","Semi-automatic hand annotation of egocentric recordings",2016,"Communications in Computer and Information Science","598",,,"338","355",,1,"10.1007/978-3-319-29971-6_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958582362&doi=10.1007%2f978-3-319-29971-6_18&partnerID=40&md5=97ba8613fe24b7a16ebfd5fccc9ab539","EAVISE, ESAT, KU Leuven, Sint-Katelijne-Waver, Belgium; MIDI Research Group, KU Leuven, Leuven, Belgium","De Beugher, S., EAVISE, ESAT, KU Leuven, Sint-Katelijne-Waver, Belgium; Brône, G., MIDI Research Group, KU Leuven, Leuven, Belgium; Goedemé, T., EAVISE, ESAT, KU Leuven, Sint-Katelijne-Waver, Belgium","We present a fast and accurate algorithm for the detection of human hands in real-life 2D image sequences. We focus on a specific application of hand detection, viz. the annotation of egocentric recordings. A well known type of egocentric camera is the mobile eye-tracker, which is often used in research on human-human interaction. Nowadays, this type of data is typically annotated manually for relevant features (e.g. visual fixations of gestures), which is a time-consuming and error-prone task. We present a semi-automatic approach for the detection of human hands in images. Such an approach reduces the amount of manual analysis drastically while guaranteeing high accuracy. In our algorithm we combine several well-known detection techniques together with an advanced elimination scheme to reduce false detections. We validate our approach using a challenging dataset containing over 4300 hand instances. This validation allows us to explore the capabilities and boundaries of our approach. © Springer International Publishing Switzerland 2016.","(Semi-)automatic analysis; Annotation; Ego-centric; Eye-tracking; Hand detection; Human-human interaction","Algorithms; Eye movements; Annotation; Automatic analysis; Ego-centric; Eye-tracking; Hand detection; Human-human interactions; Palmprint recognition",Article,"Final","",Scopus,2-s2.0-84958582362
"Tamura K., Hashimoto K., Aoki Y.","55375922900;55477157400;35498779700;","Head pose-invariant eyelid and iris tracking method",2016,"Electronics and Communications in Japan","99","2",,"19","27",,1,"10.1002/ecj.11776","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954159736&doi=10.1002%2fecj.11776&partnerID=40&md5=d4aaf0714cdf79f3ae1cadf8b7b2795f","Keio University, Japan","Tamura, K., Keio University, Japan; Hashimoto, K., Keio University, Japan; Aoki, Y., Keio University, Japan","These days, there is more demand for a camera-based gaze estimation method for new interfaces and new marketing measurement tools. Considering these applications, the system should track a new user without any operation such as calibration. It should also admit user's natural head pose changes. Previous methods, however, need a calibration procedure before execution and have less accuracy in a head moving situation. In this paper, we propose a method which tracks the user's eyelids and iris automatically and accurately. Our method is a pretreatment of gaze estimation without any calibration and head pose restraint. First of all, we track the facial feature points from an input face image and estimate its head pose, extracting the eye region image. On the eye region image, we track the eyelid shape based on an eyelid shape model generated beforehand from PCA. Finally we track the iris inside the eyelid based on the eyeball model. The eyelid and iris tracking is processed by Particle Filter. An evaluation against a database including head pose changes confirmed that the accuracy of eyelid and iris tracking was improved compared with previous methods. © 2016 Wiley Periodicals, Inc.","3D reconstruction; ASM; eyelid tracking; gaze estimation; interface; iris tracking","Calibration; Face recognition; Interfaces (materials); 3D reconstruction; ASM; Calibration procedure; Facial feature points; Gaze estimation; Iris tracking; Measurement tools; Particle filter; Motion estimation",Article,"Final","",Scopus,2-s2.0-84954159736
"Pan Y., Steed A.","55549956700;18435050200;","Effects of 3D perspective on head gaze estimation with a multiview autostereoscopic display",2016,"International Journal of Human Computer Studies","86",,,"138","148",,8,"10.1016/j.ijhcs.2015.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946781493&doi=10.1016%2fj.ijhcs.2015.10.004&partnerID=40&md5=ca1e453dfa9ed8a6e6be72b82c8a229b","Virtual Environments and Computer Graphics Group, Department of Computer Science, University College, United Kingdom","Pan, Y., Virtual Environments and Computer Graphics Group, Department of Computer Science, University College, United Kingdom; Steed, A., Virtual Environments and Computer Graphics Group, Department of Computer Science, University College, United Kingdom","Head gaze, or the orientation of the head, is a very important attentional cue in face to face conversation. Some subtleties of the gaze can be lost in common teleconferencing systems, because a single perspective warps spatial characteristics. A recent random hole display is a potentially interesting display for group conversation, as it allows multiple stereo viewers in arbitrary locations, without the restriction of conventional autostereoscopic displays on viewing positions. We represented a remote person as an avatar on a random hole display. We evaluated this system by measuring the ability of multiple observers with different horizontal and vertical viewing angles to accurately and simultaneously judge which targets the avatar is gazing at. We compared three perspective conditions: a conventional 2D view, a monoscopic perspective-correct view, and a stereoscopic perspective-correct views. In the latter two conditions, the random hole display shows three and six views simultaneously. Although the random hole display does not provide high quality view, because it has to distribute display pixels among multiple viewers, the different views are easily distinguished. Results suggest the combined presence of perspective-correct and stereoscopic cues significantly improved the effectiveness with which observers were able to assess the avatar's head gaze direction. This motivates the need for stereo in future multiview displays. © 2015 Elsevier Ltd.","Autostereoscopic display; Avatars; Gaze; Random hole display; Telecommunication","Display devices; Stereo image processing; Telecommunication; Auto-stereoscopic display; Avatars; Face-to-face conversation; Gaze; Multi-view displays; Random holes; Spatial characteristics; Teleconferencing systems; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-84946781493
"Moniri M.M., Luxenburger A., Schuffert W., Sonntag D.","35096486100;57194645943;57192162739;12241487800;","Real-time 3D peripheral view analysis",2016,"International Conference on Artificial Reality and Telexistence and Eurographics Symposium on Virtual Environments, ICAT-EGVE 2016",,,,"37","44",,,"10.2312/egve.20161432","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087951947&doi=10.2312%2fegve.20161432&partnerID=40&md5=28753abbe27fbd0977f63ca9a08306af","German Research Center for Artificial Intelligence (DFKI GmbH), Saarbruecken, Germany","Moniri, M.M., German Research Center for Artificial Intelligence (DFKI GmbH), Saarbruecken, Germany; Luxenburger, A., German Research Center for Artificial Intelligence (DFKI GmbH), Saarbruecken, Germany; Schuffert, W., German Research Center for Artificial Intelligence (DFKI GmbH), Saarbruecken, Germany; Sonntag, D., German Research Center for Artificial Intelligence (DFKI GmbH), Saarbruecken, Germany","Human peripheral vision suffers from several limitations that differ among various regions of the visual field. Since these limitations result in natural visual impairments, many interesting intelligent user interfaces based on eye tracking could benefit from peripheral view calculations that aim to compensate for events occurring outside the very center of gaze. We present a general peripheral view calculation model which extends previous work on attention-based user interfaces that use eye gaze. An intuitive, two dimensional visibility measure based on the concept of solid angle is developed for determining to which extent an object of interest observed by a user intersects with each region of the underlying visual field model. The results are weighted considering the visual acuity in each visual field region to determine the total visibility of the object. We exemplify the proposed model in a virtual reality car simulation application incorporating a head-mounted display with integrated eye tracking functionality. In this context, we provide a quantitative evaluation in terms of a runtime analysis of the different steps of our approach. We provide also several example applications including an interactive web application which visualizes the concepts and calculations presented in this paper. © 2016 The Author(s)",,"Helmet mounted displays; User interfaces; Virtual reality; Visibility; Vision; Calculation models; Head mounted displays; Intelligent User Interfaces; Interactive web applications; Peripheral vision; Quantitative evaluation; Run-time analysis; Simulation applications; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85087951947
"Mills C., Bixler R., Wang X., D'Mello S.K.","40661587800;55789867400;57211027176;14053463100;","Automatic gaze-based detection of mind wandering during narrative film comprehension",2016,"Proceedings of the 9th International Conference on Educational Data Mining, EDM 2016",,,,"30","37",,17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072287925&partnerID=40&md5=6449f242a1318645539f4328743048a0","University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States","Mills, C., University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States; Bixler, R., University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States; Wang, X., University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States; D'Mello, S.K., University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States","Mind wandering (MW) reflects a shift in attention from task-related to task-unrelated thoughts. It is negatively related to performance across a range of tasks, suggesting the importance of detecting and responding to MW in real-time. Currently, there is a paucity of research on MW detection in contexts other than reading. We addressed this gap by using eye gaze to automatically detect MW during narrative film comprehension, an activity that is used across a range of learning environments. In the current study, students self-reported MW as they watched a 32.5-minute commercial film. Students’ eye gaze was recorded with an eye tracker. Supervised machine learning models were used to detect MW using global (content-independent), local (content-dependent), and combined global+local features. We achieved a student-independent score (MW F1) of .45, which reflected a 29% improvement over a chance baseline. Models built using local features were more accurate than the global and combined models. An analysis of diagnostic features revealed that MW primarily manifested as a breakdown in attentional synchrony between eye gaze and visually salient areas of the screen. We consider limitations, applications, and refinements of the MW detector. © 2016 International Educational Data Mining Society. All rights reserved.","Eye gaze; Film comprehension; Machine learning; Mind wandering","Computer aided instruction; Data mining; Learning systems; Machine learning; Students; Supervised learning; Combined model; Content dependent; Diagnostic features; Eye-gaze; Learning environments; Local feature; Mind wandering; Supervised machine learning; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85072287925
"Kim Y.-T., Seo J., Seo W., Sung G., Kim Y., Song H., An J., Choi C.-S., Kim S., Kim H., Kim Y., Kim Y., Lee H.-S.","55826365700;55471873500;56366599800;36843451400;57194380014;24176149200;23471722500;7402961271;57020247800;16063915100;57196171178;57191473529;17137243800;","Holographic augmented reality head-up display with eye tracking and steering light source",2016,"23rd International Display Workshops in conjunction with Asia Display, IDW/AD 2016","1",,,"251","254",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050620911&partnerID=40&md5=35787aa11c080380847a082895181196","Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea","Kim, Y.-T., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Seo, J., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Seo, W., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Sung, G., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, Y., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Song, H., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; An, J., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Choi, C.-S., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, S., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, H., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, Y., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, Y., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Lee, H.-S., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea","We realized a holographic head-up display using a steering light source with eye position tracking. It can represent a real augmented reality which perfectly matches virtual graphic images to the real world. Further, for the determination of the position of the light source, 3D calibration method is proposed. © 2016 Society for Information Display. All rights reserved.","Accommodation-vergence conflict; Augmented reality; Head-up display; Holographic","Augmented reality; Holographic displays; Light sources; 3D calibration; Eye position tracking; Graphic images; Head up displays; Holographic; Real-world; Vergences; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85050620911
"Kim Y.-T., Seo J., Seo W., Sung G., Kim Y., Song H., An J., Choi C.-S., Kim S., Kim H., Kim Y., Kim Y., Lee H.-S.","55826365700;55471873500;56366599800;36843451400;57194380014;24176149200;23471722500;7402961271;57020247800;16063915100;57196171178;57191473529;17137243800;","Holographic augmented reality head-up display with eye tracking and steering light source",2016,"23rd International Display Workshops in conjunction with Asia Display, IDW/AD 2016","3",,,"1395","1398",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050533884&partnerID=40&md5=ad5df30eb31c99a7c7a20dd2c409eee7","Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea","Kim, Y.-T., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Seo, J., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Seo, W., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Sung, G., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, Y., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Song, H., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; An, J., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Choi, C.-S., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, S., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, H., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, Y., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Kim, Y., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea; Lee, H.-S., Device and System Research Center, Samsung Advanced Institute of Technology, Samsung Electronics, Suwon, Gyeonggi-do, South Korea","We realized a holographic head-up display using a steering light source with eye position tracking. It can represent a real augmented reality which perfectly matches virtual graphic images to the real world. Further, for the determination of the position of the light source, 3D calibration method is proposed. © 2016 Chinese Laser Press.","Accommodation-vergence conflict; Augmented reality; Head-up display; Holographic","Augmented reality; Holographic displays; Light sources; 3D calibration; Eye position tracking; Graphic images; Head up displays; Holographic; Real-world; Vergences; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85050533884
"Farahi B.","57196422629;","Caress of the gaze: A gaze actuated 3D printed body architecture",2016,"ACADIA 2016: Posthuman Frontiers: Data, Designers, and Cognitive Machines - Proceedings of the 36th Annual Conference of the Association for Computer Aided Design in Architecture",,,,"352","361",,7,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048240101&partnerID=40&md5=0d56d82ac60d1b5aef8948dc92f8fa96","University of Southern California, School of Cinematc Arts, United States","Farahi, B., University of Southern California, School of Cinematc Arts, United States","This paper describes the design process behind Caress of the Gaze, a project that represents a new approach to the design of a gaze-actuated, 3D printed body architecture - as a form of proto-architectural study - providing a framework for an interactive dynamic design. The design process engages with three main issues. Firstly, it aims to look at form or geometry as a means of controlling material behavior by exploring the tectionic propertes of mult-material 3D printing technologies. Secondly, it addresses novel actuation systems by using Shape Memory Alloy (SMA) in order to achieve life-like behavior. Thirdly, it explores the possibility of engaging with interactive systems by investgating how our clothing could interact with other people as a primary interface, using vision-based eye-gaze tracking technologies. In so doing, this paper describes a radically alternative approach not only to the production of garments but also to the ways we interact with the world around us. Therefore, the paper addresses the emerging field of shape-changing 3D printed structures and interactive systems that bridge the worlds of robotcs, architecture, technology, and design. © 2016 CURRAN-CONFERENCE. All rights reserved.",,"3D printers; Bridges; Computer aided design; Memory architecture; Shape memory effect; Architectural studies; Eye gaze tracking; Interactive system; Material behavior; New approaches; Novel actuation; Printed structures; Shape memory alloys(SMA); Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85048240101
"Wood E., Baltrušaitis T., Morency L.-P., Robinson P., Bulling A.","56145872800;36696075900;6603047400;57205369790;6505807414;","A 3D morphable model of the eye region",2016,"European Association for Computer Graphics - 37th Annual Conference, EUROGRAPHICS 2016 - Posters",,,,"35","36",,3,"10.2312/egp.20161054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046731942&doi=10.2312%2fegp.20161054&partnerID=40&md5=377e57dbb3c432cdf7803638013b7646","Computer Lab, University of Cambridge, United Kingdom; Language Technologies Institute, Carnegie Mellon University, United States; Perceptual User Interfaces, Max Planck Institute for Informatics, Germany","Wood, E., Computer Lab, University of Cambridge, United Kingdom; Baltrušaitis, T., Language Technologies Institute, Carnegie Mellon University, United States; Morency, L.-P., Language Technologies Institute, Carnegie Mellon University, United States; Robinson, P., Computer Lab, University of Cambridge, United Kingdom; Bulling, A., Perceptual User Interfaces, Max Planck Institute for Informatics, Germany","We present the first 3D morphable model that includes the eyes, enabling gaze estimation and gaze re-targetting from a single image. Morphable face models are a powerful tool and are used for a range of tasks including avatar animation and facial expression transfer. However, previous work has avoided the eyes, even though they play an important role in human communication. We built a new morphable model of the facial eye-region from high-quality head scan data, and combined this with a parametric eyeball model constructed from anatomical measurements and iris photos. We fit our models to an input RGB image, solving for shape, texture, pose, and scene illumination simultaneously. This provides us with an estimate of where a person is looking in a 3D scene without per-user calibration - a still unsolved problem in computer vision. It also allows us to re-render a person's eyes with different parameters, thus redirecting their perceived attention. © 2016 The Eurographics Association.",,"Computer graphics; Textures; 3D Morphable model; Facial Expressions; Gaze estimation; Human communications; Morphable face model; Morphable model; Unsolved problems; User calibration; 3D modeling",Conference Paper,"Final","",Scopus,2-s2.0-85046731942
"Kim S., Takahashi M., Watanabe K., Kawai T.","37034234500;57200503001;7406697665;7403559760;","The effects of functional binocular disparity on route memory in stereoscopic images",2016,"IS and T International Symposium on Electronic Imaging Science and Technology",,,,"","",,1,"10.2352/ISSN.2470-1173.2016.5.SDA-454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046054524&doi=10.2352%2fISSN.2470-1173.2016.5.SDA-454&partnerID=40&md5=41d3312f019157ba2015e8c9d59bbda7","Faculty of Science and Engineering, Waseda University, 3-4-1 Okubo, Shinjuku, Tokyo, 169-8555, Japan","Kim, S., Faculty of Science and Engineering, Waseda University, 3-4-1 Okubo, Shinjuku, Tokyo, 169-8555, Japan; Takahashi, M., Faculty of Science and Engineering, Waseda University, 3-4-1 Okubo, Shinjuku, Tokyo, 169-8555, Japan; Watanabe, K., Faculty of Science and Engineering, Waseda University, 3-4-1 Okubo, Shinjuku, Tokyo, 169-8555, Japan; Kawai, T., Faculty of Science and Engineering, Waseda University, 3-4-1 Okubo, Shinjuku, Tokyo, 169-8555, Japan","In this study, the effects of functional binocular disparity on route memory were experimentally verified in the context of learning of evacuation routes in disaster prevention and mitigation training. Functional binocular disparity in 3D images using cognitive characteristics such as the perspective of a specific location correlated memory in this paper. Depth maps were manipulated with the objective of assisting memorization and intuitive understanding of evacuation routes. In particular, with respect to deciding the advancing direction of the evacuation route in buildings without explicit signs, for a specific building, depth maps that could work as guide marks for the advancing route direction were manipulated to augment functional binocular parallax. In the experimental stimuli, eight locations within the building were selected to form the evacuation route, and recording was conducted using a 3D camera. The four conditions simulated in the experiment were 3D conditions using 3D images, 2D conditions using only the left image of the 3D images, adding depth map manipulation and functional binocular disparity to 2D, and placing guide marks at locations in directions that are different from the actual advancing direction to create distracted 3D conditions. 32 participants were given the route recognition task two times, once immediately after the interference task and once more after an interval of one week. The results suggest that, the participants who observed the evacuation route images modified into functional binocular disparity, remembered the correct path more easily after an interval of one week and were able to better focus their eye-gaze onto the parallax augmented locations. © 2016 Society for Imaging Science and Technology.",,"Binoculars; Cameras; Disaster prevention; Display devices; Geometrical optics; Location; Stereo image processing; Binocular disparity; Binocular parallax; Cognitive characteristics; Disaster prevention and mitigations; Evacuation routes; Intuitive understanding; Specific location; Stereoscopic image; Bins",Conference Paper,"Final","",Scopus,2-s2.0-85046054524
"Hirasawa Y., Yamamoto S., Domon R., Kintou H., Tsumura N.","57194568707;55475469000;56698359900;57194571570;7005877111;","Viewpoint entropy for material appearance",2016,"Final Program and Proceedings - IS and T/SID Color Imaging Conference","0",,,"152","156",,1,"10.2352/ISSN.2169-2629.2017.32.152","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020925219&doi=10.2352%2fISSN.2169-2629.2017.32.152&partnerID=40&md5=99393bfe12cece6015c5b72e2f74874f","Graduate School of Advanced Integration Science, Chiba University, CHIBA, Japan; Tokyo Metropolitan College of Industrial Technology, Tokyo, Japan; Nikon Corporation, Tokyo, Japan","Hirasawa, Y., Graduate School of Advanced Integration Science, Chiba University, CHIBA, Japan; Yamamoto, S., Tokyo Metropolitan College of Industrial Technology, Tokyo, Japan; Domon, R., Graduate School of Advanced Integration Science, Chiba University, CHIBA, Japan; Kintou, H., Nikon Corporation, Tokyo, Japan; Tsumura, N., Graduate School of Advanced Integration Science, Chiba University, CHIBA, Japan","In this paper, we proposed an evaluation model to quantify the viewing condition that enhances the material appearance of object without dependence on shape of object. The proposed model is based on viewpoint entropy which is used to find appropriate eye position. In order to establish this model, we first clarify the surface that have important information related to material appearance by measuring the gaze point with eye tracking equipment. Next, we added experimental results as a weight coefficient of material appearance into the conventional viewpoint entropy. We verified that our model is applicable to metal and ceramic objects by comparing the results between subjective evaluation and computational evaluation using the proposed model. © Society for Imaging Science and Technology 2016.",,"Application programs; Color; Computational evaluation; Evaluation modeling; Eye position; Eye-tracking; Gaze point; Subjective evaluations; Viewing conditions; Weight coefficients; Entropy",Conference Paper,"Final","",Scopus,2-s2.0-85020925219
"Augereau O., Fujiyoshi H., Kise K.","25421349800;57191502569;16178222100;","Towards an automated estimation of English skill via TOEIC score based on reading analysis",2016,"Proceedings - International Conference on Pattern Recognition","0",,"7899814","1285","1290",,15,"10.1109/ICPR.2016.7899814","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019144816&doi=10.1109%2fICPR.2016.7899814&partnerID=40&md5=e0640764ba00ce747ad9da53565b065c","Osaka Prefecture University, 1-1 Gakuencho, Nakaku, Sakai, Osaka, 599-8531, Japan","Augereau, O., Osaka Prefecture University, 1-1 Gakuencho, Nakaku, Sakai, Osaka, 599-8531, Japan; Fujiyoshi, H., Osaka Prefecture University, 1-1 Gakuencho, Nakaku, Sakai, Osaka, 599-8531, Japan; Kise, K., Osaka Prefecture University, 1-1 Gakuencho, Nakaku, Sakai, Osaka, 599-8531, Japan","Estimating automatically the degree of language skill by analyzing the eye movements is a promising way to help people from all over the world to learn a new language. In this study, we focus on the English skills of non-native speakers. Our aim is to provide an algorithm that can assess accurately and automatically the TOEIC score after reading English texts for few minutes. As a first step towards this direction, we propose an algorithm that can predict accurately this score after reading and answering some questions about the comprehension of few English texts. We use an eye tracker in order to record the eye gaze, i.e. the positions where the reader is looking at. Then we extract several features to characterize the behavior, and consequently the skill of the reader. We also add a feature based on the number of correct answers to the questions. By using a machine learning based on multivariate regression, the score is estimated user independently. A backward stepwise feature selection is used to select the relevant features and to optimize the estimation. As a main result, the TOEIC score is estimated with 21.7 points of mean absolute error for 21 subjects after reading and answering the questions of only 3 documents. © 2016 IEEE.",,"Eye movements; Eye tracking; Learning systems; Regression analysis; Automated estimation; English skills; Eye trackers; Feature-based; Mean absolute error; Multivariate regression; Non-native speakers; Relevant features; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85019144816
"Kang W., Ji Q.","56637259500;18935108400;","Real time eye gaze tracking with Kinect",2016,"Proceedings - International Conference on Pattern Recognition","0",,"7900052","2752","2757",,19,"10.1109/ICPR.2016.7900052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019121557&doi=10.1109%2fICPR.2016.7900052&partnerID=40&md5=195bfa21defe9ff369b751b4b546d4ee","Rensselaer Polytechnic Institute, Troy, NY  12180, United States","Kang, W., Rensselaer Polytechnic Institute, Troy, NY  12180, United States; Ji, Q., Rensselaer Polytechnic Institute, Troy, NY  12180, United States","Traditional gaze tracking systems rely on explicit infrared lights and high resolution cameras to achieve high performance and robustness. These systems, however, require complex setup and thus are restricted in lab research and hard to apply in practice. In this paper, we propose to perform gaze tracking with a consumer level depth sensor (Kinect). Leveraging on Kinect's capability to obtain 3D coordinates, we propose an efficient model-based gaze tracking system. We first build a unified 3D eye model to relate gaze directions and eye features (pupil center, eyeball center, cornea center) through subject-dependent eye parameters. A personal calibration framework is further proposed to estimate the subject-dependent eye parameters. Finally we can perform real time gaze tracking given the 3D coordinates of eye features from Kinect and the subject-dependent eye parameters from personal calibration procedure. Experimental results with 6 subjects prove the effectiveness of the proposed 3D eye model and the personal calibration framework. Furthermore, the gaze tracking system is able to work in real time (20 fps) and with low resolution eye images. © 2016 IEEE.",,"Calibration; Pattern recognition; 3D coordinates; Calibration procedure; Eye gaze tracking; Eye parameters; Gaze tracking system; High resolution camera; Infrared light; Model-based OPC; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85019121557
"Zhou X., Cai H., Shao Z., Yu H., Liu H.","55743240400;56763253600;55921729700;56115992300;54958434200;","3D eye model-based gaze estimation from a depth sensor",2016,"2016 IEEE International Conference on Robotics and Biomimetics, ROBIO 2016",,,"7866350","369","374",,6,"10.1109/ROBIO.2016.7866350","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016724706&doi=10.1109%2fROBIO.2016.7866350&partnerID=40&md5=0d95d92f94cf9eb91bd45969db6eba49","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; City University of Hong Kong, Shenzhen Research Institute, Shenzhen, Hong Kong; School of Computing, University of Portsmouth, Portsmouth, United Kingdom; School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom","Zhou, X., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China, City University of Hong Kong, Shenzhen Research Institute, Shenzhen, Hong Kong; Cai, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom; Shao, Z., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China, City University of Hong Kong, Shenzhen Research Institute, Shenzhen, Hong Kong; Yu, H., School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Liu, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom","In this paper, we address the 3D eye gaze estimation problem using a low-cost, simple-setup, and non-intrusive consumer depth sensor (Kinect sensor). We present an effective and accurate method based on 3D eye model to estimate the point of gaze of a subject with the tolerance of free head movement. To determine the parameters involved in the proposed eye model, we propose i) an improved convolution-based means of gradients iris center localization method to accurately and efficiently locate the iris center in 3D space; ii) a geometric constraints-based method to estimate the eyeball center under the constraints that all the iris center points are distributed on a sphere originated from the eyeball center and the sizes of two eyeballs of a subject are identical; iii) an effective Kappa angle calculation method based on the fact that the visual axes of both eyes intersect at a same point with the screen plane. The final point of gaze is calculated by using the estimated eye model parameters. We experimentally evaluate our gaze estimation method on five subjects. The experimental results show the good performance of the proposed method with an average estimation accuracy of 3.78°, which outperforms several state-of-the-arts. © 2016 IEEE.",,"Biomimetics; Robotics; Center points; Depth sensors; Gaze estimation; Geometric constraint; Kinect sensors; Localization method; Point of gaze; State of the art; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85016724706
"Hutt S., Mills C., White S., Donnelly P.J., D’Mello S.K.","57195276908;40661587800;57209200161;35072417700;57189619319;","The eyes have it: Gaze-based Detection of Mind Wandering during Learning with an Intelligent Tutoring System",2016,"Proceedings of the 9th International Conference on Educational Data Mining, EDM 2016",,,,"86","93",,39,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016461157&partnerID=40&md5=4aa8622d5562da729ee75d195171f0e2","University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States","Hutt, S., University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States; Mills, C., University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States; White, S., University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States; Donnelly, P.J., University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States; D’Mello, S.K., University of Notre Dame, 384 Fitzpatrick Hall, Notre Dame, IN  46556, United States","Mind wandering (MW) is a ubiquitous phenomenon characterized by an unintentional shift in attention from task-related to task-unrelated thoughts. MW is frequent during learning and negatively correlates with learning outcomes. Therefore, the next generation of intelligent learning technologies should benefit from mechanisms that detect and combat MW. As an initial step in this direction, we used eye-gaze and contextual information (e.g., time into session) to build an automated MW detector as students interact with GuruTutor – an intelligent tutoring system (ITS) for biology. Students self-reported MW by responding to pseudorandom thought-probes during the tutoring session while a consumer-grade eye tracker monitored their eye movements. We used supervised machine learning techniques to discriminate between positive and negative responses to the probes in a student-independent fashion. Our best results for detecting MW (F1 of 0.49) were obtained with an evolutionary approach to develop topologies for neural network classifiers. These outperformed standard classifiers (F1 of 0.43 with a Bayes net) and a chance baseline (F1 of 0.19). We discuss our results in the context of integrating MW detection into an attention-aware version of GuruTutor. © 2016 International Educational Data Mining Society. All rights reserved.","Attention-aware learning; Eye-gaze; Intelligent tutoring systems; Mind wandering","Computer aided instruction; Data mining; Education computing; Eye movements; Intelligent vehicle highway systems; Probes; Students; Supervised learning; Attention-aware learning; Contextual information; Evolutionary approach; Eye-gaze; Intelligent tutoring system; Mind wandering; Neural network classifier; Supervised machine learning; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85016461157
"Wang J., Zhang G., Shi J.","55972836500;56991516800;7404496048;","2D gaze estimation based on Pupil-Glint vector using an artificial neural network",2016,"Applied Sciences (Switzerland)","6","6","174","","",,14,"10.3390/app6060174","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010746952&doi=10.3390%2fapp6060174&partnerID=40&md5=a98a31d74af2aa3fc4483ddf6c0fcbcb","School of Mechatronical Engineering, Beijing Institute of Technology, 5 South Zhongguancun Street, Haidian District, Beijing, 100081, China","Wang, J., School of Mechatronical Engineering, Beijing Institute of Technology, 5 South Zhongguancun Street, Haidian District, Beijing, 100081, China; Zhang, G., School of Mechatronical Engineering, Beijing Institute of Technology, 5 South Zhongguancun Street, Haidian District, Beijing, 100081, China; Shi, J., School of Mechatronical Engineering, Beijing Institute of Technology, 5 South Zhongguancun Street, Haidian District, Beijing, 100081, China","Gaze estimation methods play an important role in a gaze tracking system. A novel 2D gaze estimation method based on the pupil-glint vector is proposed in this paper. First, the circular ring rays location (CRRL) method and Gaussian fitting are utilized for pupil and glint detection, respectively. Then the pupil-glint vector is calculated through subtraction of pupil and glint center fitting. Second, a mapping function is established according to the corresponding relationship between pupil-glint vectors and actual gaze calibration points. In order to solve the mapping function, an improved artificial neural network (DLSR-ANN) based on direct least squares regression is proposed. When the mapping function is determined, gaze estimation can be actualized through calculating gaze point coordinates. Finally, error compensation is implemented to further enhance accuracy of gaze estimation. The proposed method can achieve a corresponding accuracy of 1.29°, 0.89°, 0.52°, and 0.39° when a model with four, six, nine, or 16 calibration markers is utilized for calibration, respectively. Considering error compensation, gaze estimation accuracy can reach 0.36°. The experimental results show that gaze estimation accuracy of the proposed method in this paper is better than that of linear regression (direct least squares regression) and nonlinear regression (generic artificial neural network). The proposed method contributes to enhancing the total accuracy of a gaze tracking system.","Direct least squares regression; Gaze estimation; Gaze tracking; Human-computer interaction; Improved artificial neural network; Pupil-glint vector",,Article,"Final","",Scopus,2-s2.0-85010746952
"Seshadri P., Simons R., Bi Y., Hartley J., Bhatia J., Reid T.","56301131200;57192684477;56300824300;56000196200;57192688164;36133661700;","Evaluations that matter: Customer preferences using industry-based evaluations and eye-gaze data",2016,"Proceedings of the ASME Design Engineering Technical Conference","7",,,"","",,3,"10.1115/DETC201660293","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007369676&doi=10.1115%2fDETC201660293&partnerID=40&md5=c957af0041356e78dbd564f07cb873e2","Mechanical Engineering, Purdue University, West Lafayette, IN, United States; General Motors, Warren, MI, United States","Seshadri, P., Mechanical Engineering, Purdue University, West Lafayette, IN, United States; Simons, R., General Motors, Warren, MI, United States; Bi, Y., Mechanical Engineering, Purdue University, West Lafayette, IN, United States; Hartley, J., General Motors, Warren, MI, United States; Bhatia, J., Mechanical Engineering, Purdue University, West Lafayette, IN, United States; Reid, T., Mechanical Engineering, Purdue University, West Lafayette, IN, United States","This study is the first stage of a research program aimed at understanding differences in how people process 2D and 3D automotive stimuli, using psychophysiological tools such as galvanic skin response (GSR), eye tracking, electroencephalography (EEG), and facial expressions coding, along with respondent ratings. The current study uses just one measure, eye tracking, and one stimulus format, 2D realistic renderings of vehicles, to reveal where people expect to find information about brand and other industry-relevant topics, such as sportiness. The eye-gaze data showed differences in the percentage of fixation time that people spent on different views of cars while evaluating the ""Brand"" and the degree to which they looked ""Sporty/Conservative"", ""Calm/Exciting"", and ""Basic/Luxurious"". The results of this work can give designers insights on where they can invest their design efforts when considering brand and styling cues. Copyright © 2016 by ASME.","2D; 3D; Automotive design; Eye-tracking; Product evaluation; Vehicle design","Design; Electroencephalography; Electrophysiology; Face recognition; Target tracking; Automotive designs; Customer preferences; Eye-tracking; Galvanic skin response; Product evaluation; Psychophysiological tools; Realistic rendering; Vehicle design; Product design",Conference Paper,"Final","",Scopus,2-s2.0-85007369676
"Ekong S., Borst C.W., Woodworth J., Chambers T.L.","57192687998;9736479200;57192676048;7202454867;","Teacher-student VR telepresence with networked depth camera mesh and heterogeneous displays",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10073 LNCS",,,"246","258",,7,"10.1007/978-3-319-50832-0_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007330020&doi=10.1007%2f978-3-319-50832-0_24&partnerID=40&md5=b61631d2f93c34950695dc10f5d4f77d","University of Louisiana at Lafayette, Lafayette, United States","Ekong, S., University of Louisiana at Lafayette, Lafayette, United States; Borst, C.W., University of Louisiana at Lafayette, Lafayette, United States; Woodworth, J., University of Louisiana at Lafayette, Lafayette, United States; Chambers, T.L., University of Louisiana at Lafayette, Lafayette, United States","We present a novel interface for a teacher guiding students immersed in virtual environments. Our approach uses heterogeneous displays, with a teacher using a large 2D monitor while multiple students use immersive head-mounted displays. The teacher is sensed by a depth camera (Kinect) to capture depth and color imagery, which are streamed to student stations to inject a realistic 3D mesh of the teacher into the environment. To support communication needed for an educational application, we introduce visual aids to help teachers point and to help them establish correct eye gaze for guiding students. The result allowed an expert guide in one city to guide users located in another city through a shared educational environment. We include substantial technical details on mesh streaming, rendering, and the interface, to help other researchers. © Springer International Publishing AG 2016.",,"Cameras; Helmet mounted displays; Mesh generation; Sensory perception; Teaching; Virtual reality; Visual communication; Color imagery; Depth camera; Educational Applications; Educational environment; Head mounted displays; Technical details; Telepresence; Visual aids; Students",Conference Paper,"Final","",Scopus,2-s2.0-85007330020
"Mishra A., Kanojia D., Bhattacharyya P.","56349872900;56287676100;7101803108;","Predicting readers' sarcasm understandability by modeling gaze behavior",2016,"30th AAAI Conference on Artificial Intelligence, AAAI 2016",,,,"3747","3753",,19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007236805&partnerID=40&md5=daa0189d11947c1c0289911c5d9d42ef","Department of Computer Science and Engineering, Center for Indian Language Technology, Indian Institute of Technology, Bombay, India","Mishra, A., Department of Computer Science and Engineering, Center for Indian Language Technology, Indian Institute of Technology, Bombay, India; Kanojia, D., Department of Computer Science and Engineering, Center for Indian Language Technology, Indian Institute of Technology, Bombay, India; Bhattacharyya, P., Department of Computer Science and Engineering, Center for Indian Language Technology, Indian Institute of Technology, Bombay, India","Sarcasm understandability or the ability to understand textual sarcasm depends upon readers' language proficiency, social knowledge, mental state and attentiveness. We introduce a novel method to predict the sarcasm understandability of a reader. Presence of incongruity in textual sarcasm often elicits distinctive eye-movement behavior by human readers. By recording and analyzing the eye-gaze data, we show that eyemovement patterns vary when sarcasm is understood vis-a-vis when it is not. Motivated by our observations, we propose a system for sarcasm understandability prediction using supervised machine learning. Our system relies on readers' eyemovement parameters and a few textual features, thence, is able to predict sarcasm understandability with an F-score of 93%, which demonstrates its efficacy. The availability of inexpensive embedded-eye-trackers on mobile devices creates avenues for applying such research which benefits web-content creators, review writers and social media analysts alike. © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Behavioral research; Forecasting; Learning systems; Supervised learning; Gaze behavior; Human readers; Language proficiency; Movement behavior; Social Knowledge; Supervised machine learning; Textual features; Understandability; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85007236805
"Guimarães C.P., Balbio V., Cid G.L., Orselli M.I.V., Xavier A.P., Neto A.S., Corrêa S.C.","7004407739;57192435370;55189421300;55320335300;57188679568;57192427068;38360911400;","3D interactive environment applied to fencing training",2016,"icSPORTS 2016 - Proceedings of the 4th International Congress on Sport Sciences Research and Technology Support",,,,"39","43",,1,"10.5220/0006043100390043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006379926&doi=10.5220%2f0006043100390043&partnerID=40&md5=2412e8dd7caa45a88f6254e88025a186","National Institute of Technology, Rio de Janeiro, Brazil; LACEM, Presbyterian University Mackenzie, São Paulo, Brazil; Franciscan University, Rio Grande do Sul, Brazil","Guimarães, C.P., National Institute of Technology, Rio de Janeiro, Brazil; Balbio, V., National Institute of Technology, Rio de Janeiro, Brazil; Cid, G.L., National Institute of Technology, Rio de Janeiro, Brazil; Orselli, M.I.V., Franciscan University, Rio Grande do Sul, Brazil; Xavier, A.P., LACEM, Presbyterian University Mackenzie, São Paulo, Brazil; Neto, A.S., LACEM, Presbyterian University Mackenzie, São Paulo, Brazil; Corrêa, S.C., LACEM, Presbyterian University Mackenzie, São Paulo, Brazil","The purpose of this study was to present a 3D interactive environment - a Digital Platform to help in fencing training. The first fencing motion described and analysed at the 3D platform was lunge in epee fencing. The platform was able to show kinematic variables of upper and lower limbs and the center of mass that characterized a good performance in epee fencing. The platform also incorporates a digital database of eye track motions of the fencers. An OptiTrack motion capture system was used to capture the lunge motion of five skilled amateur fencing athletes in the presence or not of a static opponent and an Eye Track System Tobbi II was used to track the eye movements of the fencers when performing a lunge attack with a target. The 3D platform was developed using Unity3D and can present some interesting results to improve available information to coaches. That highlights the importance of visualization biomechanical results based on coach criteria in a more understandable way to help athletic training. © Copyright 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","3D Digital platform; Biomechanics; Eye tracking; Fencing; Training","Biomechanics; Engineering research; Eye movements; Personnel training; Sports; Athletic trainings; Center of mass; Digital database; Digital platforms; Fencing; Interactive Environments; Kinematic variables; Motion capture system; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85006379926
"Ghali R., Frasson C., Ouellet S.","36602634500;7003506234;57022206300;","Towards real time detection of learners' need of help in serious games",2016,"Proceedings of the 29th International Florida Artificial Intelligence Research Society Conference, FLAIRS 2016",,,,"154","157",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003944847&partnerID=40&md5=9fdb2ddc71f1378ec9657d5eda70f08f","Département d'Informatique et de Recherche Opérationnelle, Université de Montréal, Montréal, H3C 3J7 1, Canada","Ghali, R., Département d'Informatique et de Recherche Opérationnelle, Université de Montréal, Montréal, H3C 3J7 1, Canada; Frasson, C., Département d'Informatique et de Recherche Opérationnelle, Université de Montréal, Montréal, H3C 3J7 1, Canada; Ouellet, S., Département d'Informatique et de Recherche Opérationnelle, Université de Montréal, Montréal, H3C 3J7 1, Canada","Providing an adequate help to a learner remains a challenge. In this paper we aim to find how to provide learners with real time help in an educational game. Detecting that a player is engaged or motivated is a good sign that he is progressing. For these reasons we need to assess learner's states while learning. In this study we gather a variety of data using three types of sensors (electroencephalography, eye tracking and automatic facial expression recognition) to build a reliable user adaptation system. The data result from an interaction of 40 players with LewiSpace game, that we built for experimental purpose to learn construction of Lewis diagrams. We used machine learning algorithms in order to identify the most important features gathered from each sensor. Two models were trained with these data: a generalized model, trained on all data available, and a personalized model, trained only on the current user during an early phase of the game experience. The predictive results showed that personalized model could outperform the generalized model. © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Electroencephalography; Electrophysiology; Face recognition; Learning algorithms; Automatic facial expression recognition; Educational game; Game experience; Generalized models; Important features; Personalized model; Real-time detection; User adaptation; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85003944847
"Muhammad A., Addenan M.F., Latiff M.M., Haris B., Surip S.S., Mohamed A.S.A.","57197347537;57201709304;57192170886;57192171103;57192173209;57190968285;","Interactive sign language interpreter using skeleton tracking",2016,"Journal of Telecommunication, Electronic and Computer Engineering","8","6",,"137","140",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999114737&partnerID=40&md5=75c02ef77d06f94f371fb4d2555cc81f","School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Faculty of Business and Management, Universiti Teknologi MARA, Shah Alam, Selangor, 40450, Malaysia; School of The Arts, Universiti Sains Malaysia, Penang, 11800, Malaysia","Muhammad, A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Addenan, M.F., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Latiff, M.M., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia; Haris, B., Faculty of Business and Management, Universiti Teknologi MARA, Shah Alam, Selangor, 40450, Malaysia; Surip, S.S., School of The Arts, Universiti Sains Malaysia, Penang, 11800, Malaysia; Mohamed, A.S.A., School of Computer Sciences, Universiti Sains Malaysia, Penang, 11800, Malaysia","The aim of this paper is to introduce an interactive communication system that will benefit both people with hearing and verbal difficulties to convey in the form of the sign language naturally. The idea is to provide two ways of communication between two users by converting sign language to voice and text and provides means of returning communication feedback whereby the other party can speak or key-in text and translates it into sign language movement performed by a three-dimensional (3D) model. A Microsoft Kinect device is used to captures the sign movements by optimizing the skeleton tracking algorithm to understand specific hands movements and dictates using the pre-recorded gesture library to digitized voice and using the same apparatus, speech is translated back into sign language. Research leads in helping the disables have been carried out extensively and majority focuses on only using single type of motion sensing technology such as TOBII (eye tracking) and LEAP (leap motion) which are either costly or limited to a small workable space. Microsoft Kinect technology would be a genuinely equipment used to create a cost-effective and capable technology prototype that enables sign-language communication between signer and non-signer, thus, offers translation into Bahasa Malaysia text.","Kinect; Language translator; Motion; Sign language; Skeleton tracking",,Article,"Final","",Scopus,2-s2.0-84999114737
"Liu Y., Lee B.S., Sluzek A., Rajan D., McKeown M.","57192561421;7405441352;6701500691;7005909381;7005375626;","Feasibility analysis of eye typing with a standard webcam",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9914 LNCS",,,"254","268",,2,"10.1007/978-3-319-48881-3_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996922096&doi=10.1007%2f978-3-319-48881-3_18&partnerID=40&md5=0f5062eb19a168820d2a2bb7cba29ef9","Nanyang Institute of Technology in Health and Medicine, Singapore, Singapore; SCSE, Nanyang Technological University, Singapore, Singapore; Khalifa University, Abu Dhabi, United Arab Emirates; University of British Columbia, Vancouver, BC, Canada","Liu, Y., Nanyang Institute of Technology in Health and Medicine, Singapore, Singapore, SCSE, Nanyang Technological University, Singapore, Singapore; Lee, B.S., SCSE, Nanyang Technological University, Singapore, Singapore; Sluzek, A., Khalifa University, Abu Dhabi, United Arab Emirates; Rajan, D., SCSE, Nanyang Technological University, Singapore, Singapore; McKeown, M., University of British Columbia, Vancouver, BC, Canada","With the development of assistive technology, eye typing has become an alternative form of text entry for physically challenged people with severe motor disabilities. However, additional eye-tracking devices need to be used to track eye movements which is inconvenient in some cases. In this paper, we propose an appearance-based method to estimate the person’s gaze point using a webcam, and also investigate some practical issues of the method. The experimental results demonstrate the feasibility of eye typing using the proposed method. © Springer International Publishing Switzerland 2016.","Appearance-based method; Assistive technology; Eye typing; Gaze estimation","Computer vision; Appearance-based methods; Assistive technology; Eye tracking devices; Eye typing; Feasibility analysis; Gaze estimation; Motor disability; Practical issues; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84996922096
"Ghiass R.S., Arandjelovic O.","25654959400;8557887000;","Highly accurate gaze estimation using a consumer RGB-D Sensor",2016,"IJCAI International Joint Conference on Artificial Intelligence","2016-January",,,"3368","3374",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994582884&partnerID=40&md5=10fdb5bd83c2bc33a8ce90cdfdf68468","Université Laval, Québec, QC  G1V 0A6, Canada; University of St.Andrews, St Andrews, KY16 9SX, United Kingdom","Ghiass, R.S., Université Laval, Québec, QC  G1V 0A6, Canada; Arandjelovic, O., University of St.Andrews, St Andrews, KY16 9SX, United Kingdom","Determining the direction in which a person is looking is an important problem in a wide range of HCI applications. In this paper we describe a highly accurate algorithm that performs gaze estimation using an affordable and widely available device such as Kinect. The method we propose starts by performing accurate head pose estimation achieved by fitting a person specific morphable model of the face to depth data. The ordinarily competing requirements of high accuracy and high speed are met concurrently by formulating the fitting objective function as a combination of terms which excel either in accurate or fast fitting, and then by adaptively adjusting their relative contributions throughout fitting. Following pose estimation, pose normalization is done by re-rendering the fitted model as a frontal face. Finally gaze estimates are obtained through regression from the appearance of the eyes in synthetic, normalized images. Using EYEDIAP, the standard public dataset for the evaluation of gaze estimation algorithms from RGB-D data, we demonstrate that our method greatly outperforms the state of the art.",,"Artificial intelligence; Competing requirements; Head Pose Estimation; Normalized image; Objective functions; Pose estimation; Pose normalization; Relative contribution; State of the art; Image recognition",Conference Paper,"Final","",Scopus,2-s2.0-84994582884
"Pinheiro R.B.O., Pradhananga N., Jianu R., Orabi W.","57191868738;35099310500;55921296000;34868684600;","Eye-tracking technology for construction safety: A feasibility study",2016,"ISARC 2016 - 33rd International Symposium on Automation and Robotics in Construction",,,,"282","290",,5,"10.22260/isarc2016/0035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994355335&doi=10.22260%2fisarc2016%2f0035&partnerID=40&md5=84cc8f6ecf4cd4fef8f21bfa0a57d2cc","Centro Universitário Jorge Amado, Paralela Salvador, BA, Brazil; OHL School of Construction, Florida International University, Miami, FL, United States; School of Computing and Information Sciences, Florida International University, Miami, FL, United States","Pinheiro, R.B.O., Centro Universitário Jorge Amado, Paralela Salvador, BA, Brazil; Pradhananga, N., OHL School of Construction, Florida International University, Miami, FL, United States; Jianu, R., School of Computing and Information Sciences, Florida International University, Miami, FL, United States; Orabi, W., OHL School of Construction, Florida International University, Miami, FL, United States","A construction site is a harsh environment demanding entire human senses and attention, even for regular scheduled tasks. Many accidents occur on construction sites because of inability of workers to identify hazards and make timely decisions. To understand why some hazards go unseen, it is crucial to study how workers perceive the site. The objective of this research is to leverage eye-tracking technology to study workers' gazing pattern in a construction environment. A real picture from an active construction site is modified to introduce hazards and a desktop experiment is conducted, in which, subjects are asked to identify the hazards. A different group of subjects are made to make similar observations on a 2D sketch-representation of the same construction scenario. Eye-tracking data gathered from their observations is analyzed to understand when, how, and which hazards do they recognize and the pattern of recognition is studied. The results of this study will enhance our understanding on the visual factors that govern attention and help workers recognize potential hazards in a construction site. The comparison between the observation pattern in real and sketch-representation is done to assess how subjects respond to artificial images compared to real images. This comparison will test the feasibility of using virtual reality for safety training and simulations.","Construction Safety; Eye-tracking; Hazard Recognition; Site Perception; Virtual Reality","Hazards; Pattern recognition; Robotics; Virtual reality; Artificial image; Construction environment; Construction safety; Construction sites; Eye tracking technologies; Feasibility studies; Harsh environment; Potential hazards; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-84994355335
"Smith M.A., Wiese E.","57198459552;57191244776;","Look at me now: Investigating delayed disengagement for ambiguous human-robot stimuli",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9979 LNAI",,,"950","960",,3,"10.1007/978-3-319-47437-3_93","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992521752&doi=10.1007%2f978-3-319-47437-3_93&partnerID=40&md5=9e3476aff31421f715e368cc68df29af","Department of Psychology, George Mason University, Fairfax, VA  22031, United States","Smith, M.A., Department of Psychology, George Mason University, Fairfax, VA  22031, United States; Wiese, E., Department of Psychology, George Mason University, Fairfax, VA  22031, United States","Human-like appearance has been shown to positively affect perception of and attitudes towards robotic agents. In particular, the more human-like robots look, the more participants are willing to ascribe human-like states to them (i.e., having a mind, emotions, agency). The positive effect of human-likeness on agent ratings, however, does not translate to better performance in human-robot interaction (HRI). Performance first increases as human-likeness increases, then drops dramatically as soon as human-likeness reaches around 70 % to finally reach its maximum at 100 % humanness. The goal of the current paper is to investigate whether attentional mechanisms, in particular delayed disengagement, are responsible for the drop in performance for very human-like, but not perfectly human agents. The idea is that robots with a high degree of human-likeness capture attention and thus make it harder to orient attention away from them towards task-relevant stimuli in the periphery resulting in bad performance. To investigate this question, faces of differing degrees of human-likeness (0 %, 30 %, 70 %, 100 %, non-social control) are presented to participants in an eye-tracking experiment and the time it takes participants to orient towards a peripheral stimulus is measured. Results show significant delayed disengagement for all stimuli, but no stronger delayed disengagement for very human-like agents, making delayed disengagement an unlikely source for the negative effect of human-like appearance on performance in HRI. © Springer International Publishing AG 2016.","Appearance; Delayed disengagement; Eye-tracking; Robotics; Social robotics","Drops; Eye tracking; Robotics; Appearance; Attentional mechanism; Delayed disengagement; Human like robots; Human robot Interaction (HRI); Human-like agents; Robotic agents; Social robotics; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-84992521752
"Wood E., Baltrušaitis T., Morency L.-P., Robinson P., Bulling A.","56145872800;36696075900;6603047400;57205369790;6505807414;","A 3D morphable eye region model for gaze estimation",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9905 LNCS",,,"297","313",,43,"10.1007/978-3-319-46448-0_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990046250&doi=10.1007%2f978-3-319-46448-0_18&partnerID=40&md5=90d77d6b0461ba87d04f78b2cfec6f8d","University of Cambridge, Cambridge, United Kingdom; Carnegie Mellon University, Pittsburgh, United States; Max Planck Institute for Informatics, Saarbrücken, Germany","Wood, E., University of Cambridge, Cambridge, United Kingdom; Baltrušaitis, T., Carnegie Mellon University, Pittsburgh, United States; Morency, L.-P., Carnegie Mellon University, Pittsburgh, United States; Robinson, P., University of Cambridge, Cambridge, United Kingdom; Bulling, A., Max Planck Institute for Informatics, Saarbrücken, Germany","Morphable face models are a powerful tool, but have previously failed to model the eye accurately due to complexities in its material and motion. We present a new multi-part model of the eye that includes a morphable model of the facial eye region, as well as an anatomy-based eyeball model. It is the first morphable model that accurately captures eye region shape, since it was built from high-quality head scans. It is also the first to allow independent eyeball movement, since we treat it as a separate part. To showcase our model we present a new method for illumination- and head-pose–invariant gaze estimation from a single RGB image. We fit our model to an image through analysis-bysynthesis, solving for eye region shape, texture, eyeball pose, and illumination simultaneously. The fitted eyeball pose parameters are then used to estimate gaze direction. Through evaluation on two standard datasets we show that our method generalizes to both webcam and high-quality camera images, and outperforms a state-of-the-art CNN method achieving a gaze estimation accuracy of 9.44° in a challenging user-independent scenario. © Springer International Publishing AG 2016.","Analysis-by-synthesis; Gaze estimation; Morphable model","Computer vision; Textures; Analysis by synthesis; Eyeball movements; Gaze estimation; Morphable face model; Morphable model; Pose parameters; State of the art; User independents; Quality control",Conference Paper,"Final","",Scopus,2-s2.0-84990046250
"Qodseya M., Sanzari M., Ntouskos V., Pirri F.","57191408208;57189662937;55248262900;56990245000;","A3D: A device for studying gaze in 3D",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9913 LNCS",,,"572","588",,5,"10.1007/978-3-319-46604-0_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989877508&doi=10.1007%2f978-3-319-46604-0_41&partnerID=40&md5=338702dded73c383b32c9752684f68c2","ALCOR Lab, DIAG, Sapienza University of Rome, Rome, Italy","Qodseya, M., ALCOR Lab, DIAG, Sapienza University of Rome, Rome, Italy; Sanzari, M., ALCOR Lab, DIAG, Sapienza University of Rome, Rome, Italy; Ntouskos, V., ALCOR Lab, DIAG, Sapienza University of Rome, Rome, Italy; Pirri, F., ALCOR Lab, DIAG, Sapienza University of Rome, Rome, Italy","A wearable device for capturing 3D gaze information in indoor and outdoor environments is proposed. The hardware and software architecture of the device provides an estimate in quasi-real-time of 2.5D points of regard (POR) and then lift their estimations to 3D, by projecting them into the 3D reconstructed scene. The estimation procedure does not need any external device, and can be used both indoor and outdoor and with the subject wearing it moving, though some smooth constraint in the motion are required. To ensure a great flexibility with respect to depth a novel calibration method is proposed, which provides eye-scene calibration that explicitly takes into account depth information, in so ensuring a quite accurate estimation of the PORs. The experimental evaluation demonstrates that both 2.5D and 3D POR are accurately estimated. © Springer International Publishing Switzerland 2016.","3D gaze estimation; Point of regard in 3D scene; Wearable device","Calibration; Wearable technology; 3D scenes; Accurate estimation; Estimation procedures; Experimental evaluation; Gaze estimation; Hardware and software architectures; Outdoor environment; Wearable devices; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84989877508
"Mannaru P., Balasingam B., Pattipati K., Sibley C., Coyne J.","57164857800;34968982000;7006476249;36070781100;8905031900;","On the use of hidden Markov models for gaze pattern modeling",2016,"Proceedings of SPIE - The International Society for Optical Engineering","9851",,"98510R","","",,4,"10.1117/12.2224190","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989360890&doi=10.1117%2f12.2224190&partnerID=40&md5=5daf74bfc9c454a82572cd694be82607","Dept. of Electrical and Computer Engineering, University of Connecticut, 371 Faireld Way, Storrs, CT  06269, United States; Warghter Human Systems Integration Lab, Naval Research Laboratory, 4555 Overlook Ave., Washington, DC  20375, United States","Mannaru, P., Dept. of Electrical and Computer Engineering, University of Connecticut, 371 Faireld Way, Storrs, CT  06269, United States; Balasingam, B., Dept. of Electrical and Computer Engineering, University of Connecticut, 371 Faireld Way, Storrs, CT  06269, United States; Pattipati, K., Dept. of Electrical and Computer Engineering, University of Connecticut, 371 Faireld Way, Storrs, CT  06269, United States; Sibley, C., Warghter Human Systems Integration Lab, Naval Research Laboratory, 4555 Overlook Ave., Washington, DC  20375, United States; Coyne, J., Warghter Human Systems Integration Lab, Naval Research Laboratory, 4555 Overlook Ave., Washington, DC  20375, United States","Some of the conventional metrics derived from gaze patterns (on computer screens) to study visual attention, engagement and fatigue are saccade counts, nearest neighbor index (NNI) and duration of dwells/fixations. Each of these metrics has drawbacks in modeling the behavior of gaze patterns; one such drawback comes from the fact that some portions on the screen are not as important as some other portions on the screen. This is addressed by computing the eye gaze metrics corresponding to important areas of interest (AOI) on the screen. There are some challenges in developing accurate AOI based metrics: firstly, the definition of AOI is always fuzzy; secondly, it is possible that the AOI may change adaptively over time. Hence, there is a need to introduce eye-gaze metrics that are aware of the AOI in the field of view; at the same time, the new metrics should be able to automatically select the AOI based on the nature of the gazes. In this paper, we propose a novel way of computing NNI based on continuous hidden Markov models (HMM) that model the gazes as 2D Gaussian observations (x-y coordinates of the gaze) with the mean at the center of the AOI and covariance that is related to the concentration of gazes. The proposed modeling allows us to accurately compute the NNI metric in the presence of multiple, undefined AOI on the screen in the presence of intermittent casual gazing that is modeled as random gazes on the screen. © 2016 SPIE.","cognitive workload; gaze metrics; hidden Markov models; Human computer interaction; nearest neighbor index; NNI.","Behavioral research; Eye movements; Human computer interaction; Markov processes; Trellis codes; X-Y model; Cognitive workloads; Computer screens; Continuous hidden Markov model; Field of views; Gaussians; gaze metrics; Nearest neighbors; Visual Attention; Hidden Markov models",Conference Paper,"Final","",Scopus,2-s2.0-84989360890
"Monfort S.S., Sibley C.M., Coyne J.T.","56107685500;36070781100;8905031900;","Using machine learning and real-time workload assessment in a high-fidelity UAV simulation environment",2016,"Proceedings of SPIE - The International Society for Optical Engineering","9851",,"98510B","","",,7,"10.1117/12.2219703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989344908&doi=10.1117%2f12.2219703&partnerID=40&md5=63710a410ffaaf7fd2267d1b482eb095","George Mason University, 4400 University Drive, Fairfax, VA  22030-4422, United States; United States Naval Research Laboratory, 4555 Overlook Ave SW, Washington, DC  20375-0001, United States","Monfort, S.S., George Mason University, 4400 University Drive, Fairfax, VA  22030-4422, United States; Sibley, C.M., United States Naval Research Laboratory, 4555 Overlook Ave SW, Washington, DC  20375-0001, United States; Coyne, J.T., United States Naval Research Laboratory, 4555 Overlook Ave SW, Washington, DC  20375-0001, United States","Future unmanned vehicle operations will see more responsibilities distributed among fewer pilots. Current systems typically involve a small team of operators maintaining control over a single aerial platform, but this arrangement results in a suboptimal configuration of operator resources to system demands. Rather than devoting the full-time attention of several operators to a single UAV, the goal should be to distribute the attention of several operators across several UAVs as needed. Under a distributed-responsibility system, operator task load would be continuously monitored, with new tasks assigned based on system needs and operator capabilities. The current paper sought to identify a set of metrics that could be used to assess workload unobtrusively and in near real-time to inform a dynamic tasking algorithm. To this end, we put 20 participants through a variable-difficulty multiple UAV management simulation. We identified a subset of candidate metrics from a larger pool of pupillary and behavioral measures. We then used these metrics as features in a machine learning algorithm to predict workload condition every 60 seconds. This procedure produced an overall classification accuracy of 78%. An automated tasker sensitive to fluctuations in operator workload could be used to efficiently delegate tasks for teams of UAV operators. © 2016 SPIE.","eye tracking; machine learning; Mental workload; situation awareness; unmanned systems","Artificial intelligence; Learning systems; Unmanned aerial vehicles (UAV); Behavioral measures; Classification accuracy; Eye-tracking; Management simulations; Mental workload; Simulation environment; Situation awareness; Unmanned system; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-84989344908
"Tősér Z., Rill R.A., Faragó K., Jeni L.A., Lőrincz A.","56298422000;57191287116;56578404500;24474645300;26643373200;","Personalization of gaze direction estimation with deep learning",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9904 LNAI",,,"200","207",,4,"10.1007/978-3-319-46073-4_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988628720&doi=10.1007%2f978-3-319-46073-4_20&partnerID=40&md5=6c954cf47f909c096d818426c02496da","Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Faculty of Mathematics and Informatics, Babes-Bolyai University, Cluj-napoca, Romania","Tősér, Z., Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary; Rill, R.A., Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary, Faculty of Mathematics and Informatics, Babes-Bolyai University, Cluj-napoca, Romania; Faragó, K., Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary; Jeni, L.A., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Lőrincz, A., Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary","There is a growing interest in behavior based biometrics. Although biometric data has considerable variations for an individual and may be faked, yet the combination of such ‘weak experts’ can be rather strong. A remotely detectable component is gaze direction estimation and thus, eye movement patterns. Here, we present a novel personalization method for gaze estimation systems, which does not require a precise calibration setup, can be non-obtrusive, is fast and easy to use. We show that it improves the precision of gaze direction estimation algorithms considerably. The method is convenient; we exploit 3D face model reconstruction for the enrichment of a small number of collected data artificially. © Springer International Publishing AG 2016.",,"Artificial intelligence; Biometrics; Eye movements; 3-D face modeling; Behavior-based; Biometric data; Deep learning; Eye movement patterns; Gaze direction; Gaze estimation; Personalizations; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84988628720
"Abrahamson D., Shayan S., Bakker A., Van Der Schaaf M.F.","14624980700;36988351500;9634128500;8617998100;","Exposing piaget's scheme: Empirical evidence for the ontogenesis of coordination in learning a mathematical concept",2016,"Proceedings of International Conference of the Learning Sciences, ICLS ","1",,,"466","473",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987788589&partnerID=40&md5=bd8e3d083cf99e3e4bffce2902a3ca32","University of California, Berkeley, United States","Abrahamson, D., University of California, Berkeley, United States; Shayan, S., University of California, Berkeley, United States; Bakker, A., University of California, Berkeley, United States; Van Der Schaaf, M.F., University of California, Berkeley, United States","The combination of two methodological resources-natural-user interfaces (NUI) and multimodal learning analytics (MMLA)-is creating opportunities for educational researchers to empirically evaluate seminal models for the hypothetical emergence of concepts from situated sensorimotor activity. 76 participants (9-14 yo) solved tablet-based non-symbolic manipulation tasks designed to foster grounded meanings for the mathematical concept of proportional equivalence. Data gathered in task-based semi-structured clinical interviews included action logging, eye-gaze tracking, and videography. Successful task performance coincided with spontaneous appearance of stable dynamical gaze-path patterns soon followed by multimodal articulation of strategy. Significantly, gaze patterns included uncued non-salient screen locations. We present cumulative results to argue that these 'attentional anchors' mediated participants' problem solving. We interpret the findings as enabling us to revisit, support, refine, and elaborate on central claims of Piaget's theory of genetic epistemology and in particular his insistence on the role of situated motor-action coordination in the process of reflective abstraction. © 2016 ISLS.","Attentional anchor; Coordination; Eye-tracking; Genetic epistemology; NUI; Proportion","Problem solving; Tracking (position); User interfaces; Video recording; Coordination; Eye-tracking; Genetic epistemology; Methodological resources; Natural user interfaces; Proportion; Sensorimotor activities; Symbolic manipulation; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-84987788589
"Rautenbach V., Coetzee S., Hankel M.","55346444900;6601982331;57190565011;","Exploratory user study to evaluate the effect of street name changes on route planning using 2D maps",2016,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","41",,,"433","440",,,"10.5194/isprsarchives-XLI-B2-433-2016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981263352&doi=10.5194%2fisprsarchives-XLI-B2-433-2016&partnerID=40&md5=8206c46fbb8e473aa4c6543c76dc3714","Department of Geography Geoinformatics and Meteorology, Centre for Geoinformation Science, University of Pretoria, Pretoria, South Africa","Rautenbach, V., Department of Geography Geoinformatics and Meteorology, Centre for Geoinformation Science, University of Pretoria, Pretoria, South Africa; Coetzee, S., Department of Geography Geoinformatics and Meteorology, Centre for Geoinformation Science, University of Pretoria, Pretoria, South Africa; Hankel, M., Department of Geography Geoinformatics and Meteorology, Centre for Geoinformation Science, University of Pretoria, Pretoria, South Africa","This paper presents the results of an exploratory user study using 2D maps to observe and analyse the effect of street name changes on prospective route planning. The study is part of a larger research initiative to understand the effect of street name changes on wayfinding. The common perception is that street name changes affect our ability to navigate an environment, but this has not yet been tested with an empirical user study. A combination of a survey, the thinking aloud method and eye tracking was used with a group of 20 participants, mainly geoinformatics students. A within-subject participant assignment was used. Independent variables were the street network (regular and irregular) and orientation cues (street names and landmarks) portrayed on a 2D map. Dependent variables recorded were the performance (were the participant able to plan a route between the origin and destination?); the accuracy (was the shortest path identified?); the time taken to complete a task; and fixation points with eye tracking. Overall, the results of this exploratory study suggest that street name changes impact the prospective route planning performance and process that individuals use with 2D maps. The results contribute to understanding how route planning changes when street names are changed on 2D maps. It also contributes to the design of future user studies. To generalise the findings, the study needs to be repeated with a larger group of participants.","2D maps; Eye tracking; Route planning; Street name changes; User study","Remote sensing; Dependent variables; Exploratory studies; Eye-tracking; Independent variables; Origin and destinations; Research initiatives; Route planning; User study; Surveys",Conference Paper,"Final","",Scopus,2-s2.0-84981263352
"Dolezalova J., Popelka S.","56950542600;55341416700;","Evaluation of the user strategy on 2D and 3D city maps based on novel scanpath comparison method and graph visualization",2016,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","41",,,"637","640",,9,"10.5194/isprsarchives-XLI-B2-637-2016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981234337&doi=10.5194%2fisprsarchives-XLI-B2-637-2016&partnerID=40&md5=2bf22ac6786aa40a8f57a8f520f9feb7","Department of Geoinformatics, Faculty of Science, Palacký University Olomouc, Olomouc, 77146, Czech Republic","Dolezalova, J., Department of Geoinformatics, Faculty of Science, Palacký University Olomouc, Olomouc, 77146, Czech Republic; Popelka, S., Department of Geoinformatics, Faculty of Science, Palacký University Olomouc, Olomouc, 77146, Czech Republic","The paper is dealing with scanpath comparison of eye-tracking data recorded during case study focused on the evaluation of 2D and 3D city maps. The experiment contained screenshots from three map portals. Two types of maps were used - standard map and 3D visualization. Respondents' task was to find particular point symbol on the map as fast as possible. Scanpath comparison is one group of the eye-tracking data analyses methods used for revealing the strategy of the respondents. In cartographic studies, the most commonly used application for scanpath comparison is eyePatterns that output is hierarchical clustering and a tree graph representing the relationships between analysed sequences. During an analysis of the algorithm generating a tree graph, it was found that the outputs do not correspond to the reality. We proceeded to the creation of a new tool called ScanGraph. This tool uses visualization of cliques in simple graphs and is freely available at www.eyetracking.upol.cz/scangraph. Results of the study proved the functionality of the tool and its suitability for analyses of different strategies of map readers. Based on the results of the tool, similar scanpaths were selected, and groups of respondents with similar strategies were identified. With this knowledge, it is possible to analyse the relationship between belonging to the group with similar strategy and data gathered from the questionnaire (age, sex, cartographic knowledge, etc.) or type of stimuli (2D, 3D map).","Cliques; Eye-tracking; Scanpath comparison; Search strategy; Simple graphs; User perception","Remote sensing; Three dimensional computer graphics; Visualization; Cliques; Eye-tracking; Scanpath comparisons; Search strategies; Simple graphs; User perceptions; Trees (mathematics)",Conference Paper,"Final","",Scopus,2-s2.0-84981234337
"Dong W., Liao H.","22233370800;56059959800;","Eye tracking to explore the impacts of photorealistic 3D representations in pedstrian navigation performance",2016,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","41",,,"641","645",,6,"10.5194/isprsarchives-XLI-B2-641-2016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981172834&doi=10.5194%2fisprsarchives-XLI-B2-641-2016&partnerID=40&md5=4b01767c4541f6cda1f5e7a932371390","State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China","Dong, W., State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; Liao, H., State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China","Despite the now-ubiquitous two-dimensional (2D) maps, photorealistic three-dimensional (3D) representations of cities (e.g., Google Earth) have gained much attention by scientists and public users as another option. However, there is no consistent evidence on the influences of 3D photorealism on pedestrian navigation. Whether 3D photorealism can communicate cartographic information for navigation with higher effectiveness and efficiency and lower cognitive workload compared to the traditional symbolic 2D maps remains unknown. This study aims to explore whether the photorealistic 3D representation can facilitate processes of map reading and navigation in digital environments using a lab-based eye tracking approach. Here we show the differences of symbolic 2D maps versus photorealistic 3D representations depending on users' eye-movement and navigation behaviour data. We found that the participants using the 3D representation were less effective, less efficient and were required higher cognitive workload than using the 2D map for map reading. However, participants using the 3D representation performed more efficiently in self-localization and orientation at the complex decision points. The empirical results can be helpful to improve the usability of pedestrian navigation maps in future designs.","3d photorealism; Cognitive workload; Eye tracking; Pedestrian navigation","Air navigation; Maps; Navigation; Remote sensing; Cognitive workloads; Effectiveness and efficiencies; Eye-tracking; Navigation performance; Pedestrian navigation; Photorealism; Three dimensional (3D) representation; Two-dimensional (2D) map; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84981172834
"Märtin C., Rashid S., Herdin C.","57198806649;57190278742;49561299300;","Designing responsive interactive applications by emotion-tracking and pattern-based dynamic user interface adaptation",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9733",,,"28","36",,10,"10.1007/978-3-319-39513-5_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978870312&doi=10.1007%2f978-3-319-39513-5_3&partnerID=40&md5=54bca6293bf86c8aff043b96c5f69ecf","Faculty of Computer Science, Augsburg University of Applied Sciences, An der Hochschule 1, Augsburg, 86161, Germany","Märtin, C., Faculty of Computer Science, Augsburg University of Applied Sciences, An der Hochschule 1, Augsburg, 86161, Germany; Rashid, S., Faculty of Computer Science, Augsburg University of Applied Sciences, An der Hochschule 1, Augsburg, 86161, Germany; Herdin, C., Faculty of Computer Science, Augsburg University of Applied Sciences, An der Hochschule 1, Augsburg, 86161, Germany","Model-based user interface development environments (MB-UIDEs) can be enhanced by pattern-based frameworks to allow for richer design capabilities and more flexible responsive behavior during the runtime of the implemented target application. In this paper an experimental system prototype for integrating facial analysis and eye-tracking into a pattern-based dynamic user interface adaptation process is discussed. The resulting system evaluates the emotional state of the system users to trigger the HCI-pattern-based adaptation of the user interface. By monitoring the emotional states of the users over longer time periods, while the system changes its behavior and its appearance, conclusions about the perceived quality dimensions of the interactive application can be drawn. © Springer International Publishing Switzerland 2016.","Adaptive user interface; Emotion tracking; Eye tracking; Facial analysis; HCI-patterns; MB-UIDE; Model-based development; Pattern-based development; Responsive design","Behavioral research; Human computer interaction; Interactive devices; Interface states; Adaptive user interface; Eye-tracking; Facial analysis; MB-UIDE; Model based development; Pattern-based development; Responsive designs; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-84978870312
"Hong J.-Y., Lee C.-K., Park S.-G., Kim J., Cha K.-H., Kang K.H., Lee B.","55966528800;55558383300;35801276400;57203325066;8609152800;54404863000;8114205400;","See-through multi-view 3D display with parallax barrier",2016,"Proceedings of SPIE - The International Society for Optical Engineering","9770",,"977003","","",,2,"10.1117/12.2211823","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978682994&doi=10.1117%2f12.2211823&partnerID=40&md5=1126e5b708b0d031be0cb315ce451797","School of Electrical Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 151-744, South Korea; Samsung Electronics, DM and C R/D Center, 416, Maetan-3-Dong, Yeongtong-gu, Gyeonggi-do, Suwon-si, 443-742, South Korea","Hong, J.-Y., School of Electrical Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 151-744, South Korea; Lee, C.-K., School of Electrical Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 151-744, South Korea; Park, S.-G., School of Electrical Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 151-744, South Korea; Kim, J., School of Electrical Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 151-744, South Korea; Cha, K.-H., Samsung Electronics, DM and C R/D Center, 416, Maetan-3-Dong, Yeongtong-gu, Gyeonggi-do, Suwon-si, 443-742, South Korea; Kang, K.H., Samsung Electronics, DM and C R/D Center, 416, Maetan-3-Dong, Yeongtong-gu, Gyeonggi-do, Suwon-si, 443-742, South Korea; Lee, B., School of Electrical Engineering, Seoul National University, Gwanak-Gu Gwanakro 1, Seoul, 151-744, South Korea","In this paper, we propose the see-through parallax barrier type multi-view display with transparent liquid crystal display (LCD). The transparency of LCD is realized by detaching the backlight unit. The number of views in the proposed system is minimized to enlarge the aperture size of parallax barrier, which determines the transparency. For compensating the shortness of the number of viewpoints, eye tracking method is applied to provide large number of views and vertical parallax. Through experiments, a prototype of see-through autostereoscopic 3D display with parallax barrier is implemented, and the system parameters of transmittance, crosstalk, and barrier structure perception are analyzed. © 2016 SPIE.","augmented reality; Autostereoscopic display; parallax barrier; see-through","Augmented reality; Display devices; Geometrical optics; Stereo image processing; Transparency; Auto-stereoscopic display; Autostereoscopic 3D displays; Barrier structures; Eye tracking methods; Multi-view displays; Parallax barriers; see-through; Transparent liquids; Liquid crystal displays",Conference Paper,"Final","",Scopus,2-s2.0-84978682994
"Chang Z., Qiu Q., Sapiro G.","57190190445;54956074400;7005450011;","Synthesis-based low-cost gaze analysis",2016,"Communications in Computer and Information Science","618",,,"95","100",,5,"10.1007/978-3-319-40542-1_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978259221&doi=10.1007%2f978-3-319-40542-1_15&partnerID=40&md5=c3d764ad25e022435ffacd839a5ae550","Electrical and Computer Engineering, Duke University, Durham, NC, United States","Chang, Z., Electrical and Computer Engineering, Duke University, Durham, NC, United States; Qiu, Q., Electrical and Computer Engineering, Duke University, Durham, NC, United States; Sapiro, G., Electrical and Computer Engineering, Duke University, Durham, NC, United States","Gaze analysis has gained much popularity over the years due to its relevance in a wide array of applications, including humancomputer interaction, fatigue detection, and clinical mental health diagnosis. However, accurate gaze estimation from low resolution images outside of the lab (in the wild) still proves to be a challenging task. The new Intel low-cost RealSense 3D camera, capable of acquiring submillimeter resolution depth information, is currently available in laptops, and such technology is expected to become ubiquitous in other portable devices. In this paper, we focus on low-cost, scalable and real time analysis of human gaze using this RealSense camera. We exploit the direct measurement of eye surface geometry captured by the RGB-D camera, and perform gaze estimation through novel synthesis-based training and testing. Furthermore, we synthesize different eye movement appearances using a linear approach. From each 3D eye training sample captured by the RealSense camera, we synthesize multiple novel 2D views by varying the view angle to simulate head motions expected at testing. We then learn from the synthesized 2D eye images a gaze regression model using regression forests. At testing, for each captured RGB-D eye image, we first repeat the same synthesis process. For each synthesized image, we estimate the gaze from our gaze regression model, and factor-out the associated camera/head motion. In this way, we obtain multiple gaze estimations for each RGB-D eye image, and the consensus is adopted. We show that this synthesis-based training and testing significantly improves the precision in gaze estimation, opening the door to true low-cost solutions. © Springer International Publishing Switzerland 2016.",,"Abstracting; Cameras; Cost benefit analysis; Costs; Eye movements; Regression analysis; Direct measurement; Low resolution images; Real time analysis; Regression forests; Sub-millimeter resolutions; Surface geometries; Synthesized images; Training and testing; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-84978259221
"Bampatzia S., Antoniou A., Lepouras G.","55992615500;24469699600;6602608918;","Comparing game input modalities: A study for the evaluation of player experience by measuring self reported emotional states and learning outcomes",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9599",,,"218","227",,1,"10.1007/978-3-319-40216-1_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977126083&doi=10.1007%2f978-3-319-40216-1_23&partnerID=40&md5=22872bb36dcd939fc13e7253fed788c7","Human-Computer Interaction and Virtual Reality Lab, Department of Informatics and Telecommunications, University of Peloponnese, Terma Karaiskaki, Tripolis, 22100, Greece","Bampatzia, S., Human-Computer Interaction and Virtual Reality Lab, Department of Informatics and Telecommunications, University of Peloponnese, Terma Karaiskaki, Tripolis, 22100, Greece; Antoniou, A., Human-Computer Interaction and Virtual Reality Lab, Department of Informatics and Telecommunications, University of Peloponnese, Terma Karaiskaki, Tripolis, 22100, Greece; Lepouras, G., Human-Computer Interaction and Virtual Reality Lab, Department of Informatics and Telecommunications, University of Peloponnese, Terma Karaiskaki, Tripolis, 22100, Greece","As new game controllers such as the Microsoft Kinect for Xbox are introduced into the market, new forms of game interaction are introduced such as gestures, voice and eye tracking, which raise some questions regarding the user experience. Is it possible that different input methods provide a more usable game setting and affect the player’s emotions and learning process? In this paper, a 2D game about the history of photography was designed and implemented to test these hypotheses. Two prototypes of this game were created, with the first requiring input only via mouse, while the second requiring input via voice and gestures (Kinect). Two different groups tested these two prototypes. The findings from previous pilot experiments indicated that using Kinect as an input method caused higher valence and dominance levels than the use of mouse and were further validated here. Additionally, the learning outcomes of players were not affected by the input method. © Springer International Publishing Switzerland 2016.",,"Artificial intelligence; Computer science; Computers; Game controller; Input modalities; Learning outcome; Learning process; Microsoft kinect; Pilot experiment; Player experience; User experience; Mammals",Conference Paper,"Final","",Scopus,2-s2.0-84977126083
"Zhang Y., Wilcockson T., Kim K.I., Crawford T., Gellersen H., Sawyer P.","52664598600;56487914700;7409324679;57209863519;6701531333;7101723760;","Monitoring dementia with automatic eye movements analysis",2016,"Smart Innovation, Systems and Technologies","57",,,"299","309",,9,"10.1007/978-3-319-39627-9_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977108755&doi=10.1007%2f978-3-319-39627-9_26&partnerID=40&md5=034237ca8277c22fe941a704bf71f9ea","Lancaster University, Bailrigg, United Kingdom","Zhang, Y., Lancaster University, Bailrigg, United Kingdom; Wilcockson, T., Lancaster University, Bailrigg, United Kingdom; Kim, K.I., Lancaster University, Bailrigg, United Kingdom; Crawford, T., Lancaster University, Bailrigg, United Kingdom; Gellersen, H., Lancaster University, Bailrigg, United Kingdom; Sawyer, P., Lancaster University, Bailrigg, United Kingdom","Eye movement patterns are found to reveal human cognitive and mental states that can not be easily measured by other biological signals. With the rapid development of eye tracking technologies, there are growing interests in analysing gaze data to infer information about people’ cognitive states, tasks and activities performed in naturalistic environments. In this paper, we investigate the link between eye movements and cognitive function. We conducted experiments to record subject’s eye movements during video watching. By using computational methods, we identified eye movement features that are correlated to people’s cognitive health measures obtained through the standard cognitive tests. Our results show that it is possible to infer people’s cognitive function by analysing natural gaze behaviour. This work contributes an initial understanding of monitoring cognitive deterioration and dementia with automatic eye movement analysis. © Springer International Publishing Switzerland 2016.","Cognitive function; Dementia; Eye movements analysis; Health monitoring; Machine learning","Artificial intelligence; Behavioral research; Brain; Learning systems; Neurodegenerative diseases; Target tracking; Biological signals; Cognitive functions; Dementia; Eye movement analysis; Eye movement patterns; Eye tracking technologies; Health measures; Health monitoring; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84977108755
"Sárkány A., Tősér Z., Verő A.L., Lőrincz A., Toyama T., Toosi E.N., Sonntag D.","56177938600;56298422000;56418438300;26643373200;52464667100;57188710625;12241487800;","Maintain and improve mental health by smart virtual reality serious games",2016,"Communications in Computer and Information Science","604",,,"220","229",,7,"10.1007/978-3-319-32270-4_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964068346&doi=10.1007%2f978-3-319-32270-4_22&partnerID=40&md5=de73bf77647306d7dfb097173abdacf7","Faculty of Informatics, Eötvös Loránd University, Pázmány Péter Sétány 1/C, Budapest, 1117, Hungary; German Research Center for Artificial Intelligence, Trippstadter Strasse 122, Kaiserslautern, 67663, Germany","Sárkány, A., Faculty of Informatics, Eötvös Loránd University, Pázmány Péter Sétány 1/C, Budapest, 1117, Hungary; Tősér, Z., Faculty of Informatics, Eötvös Loránd University, Pázmány Péter Sétány 1/C, Budapest, 1117, Hungary; Verő, A.L., Faculty of Informatics, Eötvös Loránd University, Pázmány Péter Sétány 1/C, Budapest, 1117, Hungary; Lőrincz, A., Faculty of Informatics, Eötvös Loránd University, Pázmány Péter Sétány 1/C, Budapest, 1117, Hungary; Toyama, T., German Research Center for Artificial Intelligence, Trippstadter Strasse 122, Kaiserslautern, 67663, Germany; Toosi, E.N., German Research Center for Artificial Intelligence, Trippstadter Strasse 122, Kaiserslautern, 67663, Germany; Sonntag, D., German Research Center for Artificial Intelligence, Trippstadter Strasse 122, Kaiserslautern, 67663, Germany","Serious games for mental health is seen as the groundwork for assistive technology to maintain and improve mental health. We present a technical system layout we partly implemented for demonstration purposes and highlight vision-based perception and manipulation capabilities. These include physical interactions employing artificial general intelligence in virtual reality applications. We employ hand gesture tracking, as well as an Oculus Rift integrated gaze and eye tracking system. The resulting serious games should eventually cover daily life activities, which we additionally monitor. The dynamic and contextual modelling of obstacles are central issues, and capabilities required for serious games include knowledge about the 3D world. Such knowledge include gaze and hand sensors interpretations for multimedia information extraction in causal relationships. Towards this goal, we envision to make use of virtual reality with a physics engine (rigid and soft body dynamics including collision detection) for the observed objects. We also exploit semantic networks to enable the machine to filter information and infer ongoing complex events including hidden BDI (beliefs, desires, intentions) variables. We see this combination of employed technology as the relevant groundwork for reaching human-level general intelligence and to enable real-world applications. Future applications and user groups we target on include dementia patients. © Springer International Publishing Switzerland 2016.",,"Complex networks; Data mining; Health; Semantics; Ubiquitous computing; Virtual reality; Artificial general intelligences; Daily life activities; Eye tracking systems; General Intelligence; Hand gesture tracking; Multimedia information; Physical interactions; Vision-based perception; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84964068346
"Zhang L., Li X., Nie L., Yang Y., Xia Y.","35231925400;55936260100;36439883200;56159216600;35788434700;","Weakly supervised human fixations prediction",2016,"IEEE Transactions on Cybernetics","46","1","7152897","258","269",,30,"10.1109/TCYB.2015.2400821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960360904&doi=10.1109%2fTCYB.2015.2400821&partnerID=40&md5=9e359738c351efd931daf84a653d9807","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, 230009, China; Center for Optical Imagery Analysis and Learning, State Key Laboratory of Transient Optics and Photonics, Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, 710119, China; School of Computing, National University of Singapore, Singapore, 119613, Singapore; Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney, NSW  2007, Australia; College of Computer Science, Zhejiang University, Hangzhou, 310027, China","Zhang, L., School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, 230009, China; Li, X., Center for Optical Imagery Analysis and Learning, State Key Laboratory of Transient Optics and Photonics, Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, 710119, China; Nie, L., School of Computing, National University of Singapore, Singapore, 119613, Singapore; Yang, Y., Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney, NSW  2007, Australia; Xia, Y., College of Computer Science, Zhejiang University, Hangzhou, 310027, China","Automatically predicting human eye fixations is a useful technique that can facilitate many multimedia applications, e.g., image retrieval, action recognition, and photo retargeting. Conventional approaches are frustrated by two drawbacks. First, psychophysical experiments show that an object-level interpretation of scenes influences eye movements significantly. Most of the existing saliency models rely on object detectors, and therefore, only a few prespecified categories can be discovered. Second, the relative displacement of objects influences their saliency remarkably, but current models cannot describe them explicitly. To solve these problems, this paper proposes weakly supervised fixations prediction, which leverages image labels to improve accuracy of human fixations prediction. The proposed model hierarchically discovers objects as well as their spatial configurations. Starting from the raw image pixels, we sample superpixels in an image, thereby seamless object descriptors termed object-level graphlets (oGLs) are generated by random walking on the superpixel mosaic. Then, a manifold embedding algorithm is proposed to encode image labels into oGLs, and the response map of each prespecified object is computed accordingly. On the basis of the object-level response map, we propose spatial-level graphlets (sGLs) to model the relative positions among objects. Afterward, eye tracking data is employed to integrate these sGLs for predicting human eye fixations. Thorough experiment results demonstrate the advantage of the proposed method over the state-of-the-art. © 2015 IEEE.","Attention; Computer vision; Graphlets; Machine learning; Manifold embedding; Weakly supervised","Forecasting; Image retrieval; Pixels; Random processes; Action recognition; Conventional approach; Embedding algorithms; Multimedia applications; Psychophysical experiments; Relative displacement; Relative positions; Spatial configuration; Eye movements; algorithm; eye fixation; human; image processing; physiology; procedures; psychophysics; statistical model; supervised machine learning; Algorithms; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Models, Statistical; Psychophysics; Supervised Machine Learning",Article,"Final","",Scopus,2-s2.0-84960360904
"Ciupe A., Florea C., Orza B., Vlaicu A., Petrovan B.","57105700600;24829248500;24503681900;57202599214;36599072100;","A bag of words model for improving automatic stress classification",2016,"Advances in Intelligent Systems and Computing","427",,,"339","349",,,"10.1007/978-3-319-29504-6_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958280346&doi=10.1007%2f978-3-319-29504-6_33&partnerID=40&md5=a8a9acff9d9242828a664dedc72ac6c7","Multimedia Technologies and Telecommunications Research Centre, Technical University of Cluj-Napoca, C. Daicoviciu 15, Cluj-Napoca, 400020, Romania","Ciupe, A., Multimedia Technologies and Telecommunications Research Centre, Technical University of Cluj-Napoca, C. Daicoviciu 15, Cluj-Napoca, 400020, Romania; Florea, C., Multimedia Technologies and Telecommunications Research Centre, Technical University of Cluj-Napoca, C. Daicoviciu 15, Cluj-Napoca, 400020, Romania; Orza, B., Multimedia Technologies and Telecommunications Research Centre, Technical University of Cluj-Napoca, C. Daicoviciu 15, Cluj-Napoca, 400020, Romania; Vlaicu, A., Multimedia Technologies and Telecommunications Research Centre, Technical University of Cluj-Napoca, C. Daicoviciu 15, Cluj-Napoca, 400020, Romania; Petrovan, B., Multimedia Technologies and Telecommunications Research Centre, Technical University of Cluj-Napoca, C. Daicoviciu 15, Cluj-Napoca, 400020, Romania","Most of the existing stress assessment frameworks rely on physiological signals measurements (EEG, ECG, GSR, ST, etc.), which involve direct physical contact with the patient in a medical setup. Present technologies rely on capturing moods and emotions through remote devices (cameras), further processed by computer vision and machine learning techniques. The proposed work describes a method of automatic stress classification where stress information is modeled based on pupil diameter non-intrusive measurements, recorded by an eye tracking remote system. The signal extracted from the pupil Dataset has been processed using the Bag-Of-Words model, with a SVM classification and results have been compared to similar experiments in order to validate the applicability and consistency of the Bag-Of-Words model on stress assessment and classification. © Springer International Publishing Switzerland 2016.","Automatic stress classification; Bag-Of-Words model; Biomedical time series; Pupil diameter; SVM","Artificial intelligence; Biomedical signal processing; Classification (of information); Computer vision; Learning systems; Bag-of-words models; Direct physical contacts; Machine learning techniques; Non-intrusive measurements; Physiological signals; Pupil diameter; Stress classifications; SVM classification; Information retrieval",Conference Paper,"Final","",Scopus,2-s2.0-84958280346
"Andreu-Perez J., Solnais C., Sriskandarajah K.","55653526300;55653644900;55628010700;","EALab (Eye Activity Lab): a MATLAB Toolbox for Variable Extraction, Multivariate Analysis and Classification of Eye-Movement Data",2016,"Neuroinformatics","14","1",,"51","67",,8,"10.1007/s12021-015-9275-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953835393&doi=10.1007%2fs12021-015-9275-4&partnerID=40&md5=11075a5a2c84e098fe4c4afc9119e93f","Department of Computing, Imperial College London, London, United Kingdom; Department of Marketing and Market Research, University of Granada, Granada, Spain; Department of Surgery and Cancer, St Mary’s Hospital, Imperial College London, London, United Kingdom","Andreu-Perez, J., Department of Computing, Imperial College London, London, United Kingdom; Solnais, C., Department of Marketing and Market Research, University of Granada, Granada, Spain; Sriskandarajah, K., Department of Surgery and Cancer, St Mary’s Hospital, Imperial College London, London, United Kingdom","Recent advances in the reliability of the eye-tracking methodology as well as the increasing availability of affordable non-intrusive technology have opened the door to new research opportunities in a variety of areas and applications. This has raised increasing interest within disciplines such as medicine, business and education for analysing human perceptual and psychological processes based on eye-tracking data. However, most of the currently available software requires programming skills and focuses on the analysis of a limited set of eye-movement measures (e.g., saccades and fixations), thus excluding other measures of interest to the classification of a determined state or condition. This paper describes ‘EALab’, a MATLAB toolbox aimed at easing the extraction, multivariate analysis and classification stages of eye-activity data collected from commercial and independent eye trackers. The processing implemented in this toolbox enables to evaluate variables extracted from a wide range of measures including saccades, fixations, blinks, pupil diameter and glissades. Using EALab does not require any programming and the analysis can be performed through a user-friendly graphical user interface (GUI) consisting of three processing modules: 1) eye-activity measure extraction interface, 2) variable selection and analysis interface, and 3) classification interface. © 2015, Springer Science+Business Media New York.","Classification; Computer software; Eye tracking; Machine learning; Multivariate analysis; Neuroinformatics","algorithm; computer interface; eye movement; human; machine learning; multivariate analysis; oculography; signal processing; software; Algorithms; Eye Movement Measurements; Eye Movements; Humans; Machine Learning; Multivariate Analysis; Signal Processing, Computer-Assisted; Software; User-Computer Interface",Article,"Final","",Scopus,2-s2.0-84953835393
"Ma C., Liu C., Peng F.","57225854779;23389993800;55566182400;","Two dimensional ensemble hashing for visual tracking",2016,"Neurocomputing","171",,,"1387","1400",,3,"10.1016/j.neucom.2015.07.091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944513877&doi=10.1016%2fj.neucom.2015.07.091&partnerID=40&md5=8da502d225f13243d343c847b4c1cbf6","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China","Ma, C., School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China; Liu, C., School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China; Peng, F., School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China","Appearance model in visual tracking is a key component to attain robustness and efficiency. In the last decades, many complex appearance models have been proposed to improve performance of tracking algorithm. However, these models are difficult to maintain accuracy and efficiency simultaneously. In this paper, we observe that data-dependent hashing method could improve processing speed by generating compact representation for the visual object. But applying the method to visual tracking is still a challenging task. To reinforce the performance of hashing technique, a novel hashing method called two dimensional ensemble hashing is proposed. In our tracker, image samples are hashed to binary matrices, and the Hamming distance is used to measure their confidences. Moreover, for adapting situation change, the hash functions are updated by the learning model at each frame. Experimental results not only demonstrate the accuracy and effectiveness of our tracker, but also show that the tracking algorithm outperforms other state-of-the-art trackers. © 2015 Elsevier B.V.","Appearance model; Hashing; Tracking","Hamming distance; Hash functions; Surface discharges; Appearance modeling; Appearance models; Compact representation; Hashing; Hashing techniques; Improve performance; State of the art; Tracking algorithm; Tracking (position); accuracy; algorithm; Article; eye tracking; human; object relation; pattern recognition; principal component analysis; priority journal; qualitative analysis; quantitative analysis; statistical analysis; statistics; task performance",Article,"Final","",Scopus,2-s2.0-84944513877
