DOI,Publication Year,Authors,Article Title,Abstract,"Screening(Include=1,Exclude=0,maybe=2)","Full text check(Include=1,Exclude=0)",
10.32604/cmc.2022.021107,2022,"Kim J.-H., Jeong J.-W.",Multi-view multi-modal head-gaze estimation for advanced indoor user interaction,"Gaze estimation is one of the most promising technologies for supporting indoor monitoring and interaction systems. However, previous gaze estimation techniques generally work only in a controlled laboratory environment because they require a number of high-resolution eye images. This makes them unsuitable for welfare and healthcare facilities with the following challenging characteristics: 1) usersâ€?continuous movements, 2) various lighting conditions, and 3) a limited amount of available data. To address these issues, we introduce a multi-view multi-modal head-gaze estimation system that translates the userâ€™s head orientation into the gaze direction. The proposed system captures the user using multiple cameras with depth and infrared modalities to train more robust gaze estimators under the aforementioned conditions. To this end, we implemented a deep learning pipeline that can handle different types and combinations of data. The proposed system was evaluated using the data collected from 10 volunteer participants to analyze how the use of single/multiple cameras and modalities affect the performance of head-gaze estimators. Through various experiments, we found that 1) an infrared-modality provides more useful features than a depth-modality, 2) multi-view multi-modal approaches provide better accuracy than single-view single-modal approaches, and 3) the proposed estimators achieve a high inference efficiency that can be used in real-time applications. Â© 2022 Tech Science Press. All rights reserved.",2,,
10.1007/s10799-021-00336-6,2021,"Cao Y., Ding Y., Proctor R.W., Duffy V.G., Liu Y., Zhang X.",Detecting usersâ€?usage intentions for websites employing deep learning on eye-tracking data,"We proposed a method employing deep learning (DL) on eye-tracking data and applied this method to detect intentions to use apparel websites that differed in factors of depth, breadth, and location of navigation. Results showed that usersâ€?intentions could be predicted by combining a deep neural network algorithm and metrics recorded from an eye-tracker. Using all of the eye-tracking metric features attained the best accuracy when predicting usage/not-usage intention to websites. In addition, the results suggest that for apparel websites with the same depth, designers can increase usage intention by using a larger number of navigation items and placing the navigation at the top and left of the homepage. The results show that building intelligent usage intention-detection systems is possible for the range of websites we examined and is also computationally practical. Hence, the study motivates future investigations that focus on design of such systems. Â© 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",0,,
10.1038/s41746-021-00470-z,2021,"Meier I.B., Buegler M., Harms R., Seixas A., Ã‡Ã¶ltekin A., Tarnanas I.",Using a Digital Neuro Signature to measure longitudinal individual-level change in Alzheimerâ€™s disease: the Altoida large cohort study,"Conventional neuropsychological assessments for Alzheimerâ€™s disease are burdensome and inaccurate at detecting mild cognitive impairment and predicting Alzheimerâ€™s disease risk. Altoidaâ€™s Digital Neuro Signature (DNS), a longitudinal cognitive test consisting of two active digital biomarker metrics, alleviates these limitations. By comparison to conventional neuropsychological assessments, DNS results in faster evaluations (10 min vs 45â€?20 min), and generates higher test-retest in intraindividual assessment, as well as higher accuracy at detecting abnormal cognition. This study comparatively evaluates the performance of Altoidaâ€™s DNS and conventional neuropsychological assessments in intraindividual assessments of cognition and function by means of two semi-naturalistic observational experiments with 525 participants in laboratory and clinical settings. The results show that DNS is consistently more sensitive than conventional neuropsychological assessments at capturing longitudinal individual-level change, both with respect to intraindividual variability and dispersion (intraindividual variability across multiple tests), across three participant groups: healthy controls, mild cognitive impairment, and Alzheimerâ€™s disease. Dispersion differences between DNS and conventional neuropsychological assessments were more pronounced with more advanced disease stages, and DNS-intraindividual variability was able to predict conversion from mild cognitive impairment to Alzheimerâ€™s disease. These findings are instrumental for patient monitoring and management, remote clinical trial assessment, and timely interventions, and will hopefully contribute to a better understanding of Alzheimerâ€™s disease. Â© 2021, The Author(s).",0,,
10.1016/j.neucom.2021.08.025,2021,"Zhuang J., Dong Y., Bai H.",Ensemble learning with siamese networks for visual tracking,"Ensemble learning (EL) is an effective and commonly used technique to improve visual tasksâ€?accuracy, such as classification and detection. However, EL is rarely used in visual tracking. To fill this knowledge gap, we first have completed some research to investigate why knowledge distillation was ineffective in visual tracking tasks. Comparing the difference between the classification and visual tracking, conclusions are given: (i) Numerous simple negative examples are redundant, while only a few hard negative samples are valid for visual tracking knowledge distillation. (ii) The hint knowledge flows differently between classification and visual tracking. To solve the above problems, we design two new loss functions and integrate them into the proposed Ensemble Learning (EL) framework that can be employed in Siamese architectures such as SiamFC, SiamRPN, SiamFC+, and SiamRPN+. The EL treats two Siamese networks as students and enables them to learn collaboratively. A better solution is yielded by the EL framework than training students individually. Experiments on OTB-2013, OTB-2015, VOT2015, VOT2016, VOT2017, VOT2018, LaSOT and TrackingNet have verified the effectiveness of our proposed technique on boosting the performance for the four Siamese algorithms. The EL-SiamRPN+ achieves leading performance in the challenges. Â© 2021 Elsevier B.V.",2,,
10.1109/TPAMI.2020.2991150,2021,"Ploumpis S., Ververas E., Sullivan E.O., Moschoglou S., Wang H., Pears N., Smith W.A.P., Gecer B., Zafeiriou S.",Towards a Complete 3D Morphable Model of the Human Head,"Three-dimensional morphable models (3DMMs) are powerful statistical tools for representing the 3D shapes and textures of an object class. Here we present the most complete 3DMM of the human head to date that includes face, cranium, ears, eyes, teeth and tongue. To achieve this, we propose two methods for combining existing 3DMMs of different overlapping head parts: (i). use a regressor to complete missing parts of one model using the other, and (ii). use the Gaussian Process framework to blend covariance matrices from multiple models. Thus, we build a new combined face-and-head shape model that blends the variability and facial detail of an existing face model (the LSFM) with the full head modelling capability of an existing head model (the LYHM). Then we construct and fuse a highly-detailed ear model to extend the variation of the ear shape. Eye and eye region models are incorporated into the head model, along with basic models of the teeth, tongue and inner mouth cavity. The new model achieves state-of-the-art performance. We use our model to reconstruct full head representations from single, unconstrained images allowing us to parameterize craniofacial shape and texture, along with the ear shape, eye gaze and eye color. Â© 1979-2012 IEEE.",0,,
10.1016/j.engappai.2021.104471,2021,"Cho D.-Y., Kang M.-K.",Human gaze-aware attentive object detection for ambient intelligence,"Understanding human behavior and the surrounding environment is essential for realizing ambient intelligence (AmI), for which eye gaze and object information are reliable cues. In this study, the authors propose a novel human gaze-aware attentive object detection framework as an elemental technology for AmI. The proposed framework detects usersâ€?attentive objects and shows more precise and robust performance against object-scale variations. A novel Adaptive-3D-Region-of-Interest (Ada-3D-RoI) scheme is designed as a front-end module, and scalable detection network structures are proposed to maximize cost-efficiency. The experiments show that the detection rate is improved up to 97.6% on small objects (14.1% on average), and it is selectively tunable with a tradeoff between accuracy and computational complexity. In addition, the qualitative results demonstrate that the proposed framework detects a user's single object-of-interest only, even when the target object is occluded or extremely small. Complementary matters for follow-up study are presented as suggestions to extend the results of the proposed framework to further practical AmI applications. This study will help develop advanced AmI applications that demand a higher-level understanding of scene context and human behavior such as humanâ€“robot symbiosis, remote-/autonomous control, and augmented/mixed reality. Â© 2021 The Author(s)",0,,
10.3390/app11199068,2021,"Ansari M.F., Kasprowski P., Obetkal M.",Gaze tracking using an unmodified web camera and convolutional neural network,"Gaze estimation plays a significant role in understating human behavior and in humanâ€?computer interaction. Currently, there are many methods accessible for gaze estimation. However, most approaches need additional hardware for data acquisition which adds an extra cost to gaze tracking. The classic gaze tracking approaches usually require systematic prior knowledge or expertise for practical operations. Moreover, they are fundamentally based on the characteristics of the eye region, utilizing infrared light and iris glint to track the gaze point. It requires high-quality images with particular environmental conditions and another light source. Recent studies on appearance-based gaze estimation have demonstrated the capability of neural networks, especially convolutional neural networks (CNN), to decode gaze information present in eye images and achieved significantly simplified gaze estimation. In this paper, a gaze estimation method that utilizes a CNN for gaze estimation that can be applied to various platforms without additional hardware is presented. An easy and fast data collection method is used for collecting face and eyes images from an unmodified desktop camera. The proposed method registered good results; it proves that it is possible to predict the gaze with reasonable accuracy without any additional tools. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",2,,
10.1016/j.aei.2021.101364,2021,"Hsiao S.-W., Peng P.-H., Tsao Y.-C.",A method for the analysis of the interaction between users and objects in 3D navigational space,"Along with the improvement of eye-tracking technology, more and more distinct field of researches have introduced movements of the eye in relation to the head to understand user behavior. Most of current researches focus on the perception process of single 2-dimensional images by fixed eye-tracking devices or the head-mount devices. A method of applying eye-tracking on the analysis of the interaction between users and objects in 3D navigational space is proposed in this article. It aims to understand the visual stimulation of 3D objects and the user's spatial navigational reactions while receiving the stimulation, and proposes the concept of 3D object attention heat map. It also proposes to construct a computational visual attention model for different geometric featured 3D objects by applying the method of feature curves. The VR results of this study also provide future assistance in the incoming immersive world. This study sets to promote eye-tracking from the mainstream of 2D field to 3D spaces and points to a deeper understanding between human and artificial product or natural objects. It would also serve an important role in the field of human-computer interaction, product usability, aids devices for cognition degenerative individuals, and even the field of visual recognition of daily human behavior. Â© 2021 Elsevier Ltd",2,,
10.1016/j.asoc.2021.107752,2021,"Wu Q., Dey N., Shi F., Crespo R.G., Sherratt R.S.",Emotion classification on eye-tracking and electroencephalograph fused signals employing deep gradient neural networks,"Emotion produces complex neural processes and physiological changes under appropriate event stimulation. Physiological signals have the advantage of better reflecting a person's actual emotional state than facial expressions or voice signals. An electroencephalogram (EEG) is a signal obtained by collecting, amplifying, and recording the human brain's weak bioelectric signals on the scalp. The eye-tracking (E.T.) signal records the potential difference between the retina and the cornea and the potential generated by the eye movement muscle. Furthermore, the different modalities of physiological signals will contain various information representations of human emotions. Finding this different modal information is of great help to get higher recognition accuracy. The E.T. and EEG signals are synchronized and fused in this research, and an effective deep learning (DL) method was used to combine different modalities. This article proposes a technique based on a fusion model of the Gaussian mixed model (GMM) with the Butterworth and Chebyshev signal filter. Features extraction on EEG and E.T. are subsequently calculated. Secondly, the self-similarity (SSIM), energy (E), complexity (C), high order crossing (HOC), and power spectral density (PSD) for EGG, and electrooculography power density estimation ((EOG-PDE), center gravity frequency (CGF), frequency variance (F.V.), root mean square frequency (RMSF) for E.T. are selected hereafter; the maxâ€“min method is applied for vector normalization. Finally, a deep gradient neural network (DGNN) for EEG and E.T. multimodal signal classification is proposed. The proposed neural network predicted the emotions under the eight emotions event stimuli experiment with 88.10% accuracy. For the evaluation indices of accuracy (Ac), precision (Pr), recall (Re), F-measurement (Fm), precisionâ€“recall (P.R.) curve, true-positive rate (TPR) of receiver operating characteristic curve (ROC), the area under the curve (AUC), true-accept rate (TAR), and interaction on union (IoU), the proposed method also performs with high efficiency compared with several typical neural networks including the artificial neural network (ANN), SqueezeNet, GoogleNet, ResNet-50, DarkNet-53, ResNet-18, Inception-ResNet, Inception-v3, and ResNet-101. Â© 2021 Elsevier B.V.",0,,
10.1109/LRA.2021.3097274,2021,"Oishi S., Koide K., Yokozuka M., Banno A.",4D Attention: Comprehensive Framework for Spatio-Temporal Gaze Mapping,"This study presents a framework for capturing human attention in the spatio-temporal domain using eye-tracking glasses. Attention mapping is a key technology for human perceptual activity analysis or Human-Robot Interaction (HRI) to support human visual cognition; however, measuring human attention in dynamic environments is challenging owing to the difficulty in localizing the subject and dealing with moving objects. To address this, we present a comprehensive framework, 4D Attention, for unified gaze mapping onto static and dynamic objects. Specifically, we estimate the glasses pose by leveraging a loose coupling of direct visual localization and Inertial Measurement Unit (IMU) values. Further, by installing reconstruction components into our framework, dynamic objects not captured in the 3D environment map are instantiated based on the input images. Finally, a scene rendering component synthesizes a first-person view with identification (ID) textures and performs direct 2D-3D gaze association. Quantitative evaluations showed the effectiveness of our framework. Additionally, we demonstrated the applications of 4D Attention through experiments in real situations.1 Â© 2016 IEEE.",0,,
10.1145/3460418.3479360,2021,"Moon S., Zhang C., Park S., Zhang H., Kim W.-S., Ko J.H.",A Sub-Milliwatt and Sub-Millisecond 3-D Gaze Estimator for Ultra Low-Power AR Applications,"The critical factors of real-Time gaze tracking are high accuracy and user-friendliness such as low latency and no run-Time calibration. Existing gaze estimator hardware designs are based on 2D regression algorithms as 2D methods have a simple computation process. However, they require multiple run-Time calibration steps, and are vulnerable to head motions. On the other hand, the 3D model-based method can maintain better accuracy than the 2D method without run-Time calibration steps, and is robust to head motions. In this paper, we aim to design the first 3D model-based gaze estimator hardware that consumes less than 1mW power and 1ms latency per frame. The simulation results based on the hardware synthesis show that the proposed design requires 172Î¼W and 0.5ms per frame, while maintaining less than 0.9Â° error. Â© 2021 ACM.",2,,
10.1016/j.neucom.2020.07.119,2021,"Geisler D., Duchowski A.T., Kasneci E.",Predicting visual perceivability of scene objects through spatio-temporal modeling of retinal receptive fields,"Retinal processing of a visual scene is an essential step of human visual perception. Although foveal vision is linked to the visual attention, perception is by not means limited to this region. Rather, the retinal field of view ranges from 60Â° nasal to 107Â° temporal, and from 70Â° superior to 80Â° inferior. Whether a scene object is visually perceived depends on both its visual appearance as well as its retinal location. We present a framework to evaluate the visual stimulus of a scene object with regard to different types of retinal receptive fields. Driven by gaze location provided by an eye tracker, the estimated retinal response considers the visual appearance of the object, its eccentricity in the users field of view, and the capabilities at the local retinal region. A desktop experiment shows that, in additional to foveal processing, the estimated retinal response leads to a significant increase in classification accuracy in terms of whether an object is reported as perceived by the user. Â© 2020 Elsevier B.V.",0,,
10.1016/j.neucom.2020.07.121,2021,"Han Y., Han B., Gao X.",Human scanpath estimation based on semantic segmentation guided by common eye fixation behaviors,"To explore the dynamic process of complex human eye movement behavior, we proposed a new model to simulate human scanpath when subjects observed natural images freely. Previous methods almost focused on finding effective and advanced technology, such as machine learning or deep learning, for estimating human scanpath. In contrast, our proposed method devoted to find a new way that could use the intrinsic property of eye-tracking data between different races to guide the design of a deep network. Inspired by that, the model of human scanpath estimation was established, which based on a semantic segmentation module guided by common eye fixation behaviors between people with different cultures. The semantic segmentation module could deal with locating fixations positions and the fixations ranking problem in parallel and generate human scanpath combined with the output of common attention portions (CAP) generator. The common attention portions (CAP) generator was designed to optimize the performance of semantic segmentation module and extract the common eye fixation behaviors between people with different cultures. We evaluated the performance of our model on three public eye-tracking datasets by comparing the result generated from our model with the ground truth of scanpath produced by a new method in this work. The proposed model also achieved the encouraging performance compared with some classic and fashionable models. Â© 2020 Elsevier B.V.",2,,
10.1145/3409118.3475138,2021,"Bickerdt J., Sonnenberg J., Gollnick C., Kasneci E.",Geopositioned 3D areas of interest for gaze analysis,"To understand driver's gaze behavior, the gaze is usually matched to surrounding objects or static areas of interest (AOI) at fixed positions around the car. Full surround object tracking allows for an understanding of the traffic situation. However, because it requires an extensive sensor set and a lot of processing power, it's not yet broadly available in production cars. The use of static AOIs only requires the addition of eye tracking sensors. They are at fixed positions around the car and can't adapt to the environment, therefore their usefulness is limited. We propose geopositioned 3D AOIs. With adaptability and the use of a small sensor set, they combine the strengths of both methods. To test 3D AOIs' capabilities for gaze analysis, a driving simulator study with 74 participants was conducted. We show that 3D AOIs are suitable for driver's gaze analysis and a promising tool for driver intention prediction. Â© 2021 Association for Computing Machinery.",0,,
10.1016/j.neucom.2021.04.075,2021,"Wu J., Jiang J., Qi M., Li X.",Towards accurate estimation for visual object tracking with multi-hierarchy feature aggregation,"Many methods achieve the visual object tracking task with deep learning technologies. As the deep features of different levels contain various semantic information and functions, this paper presents a multi-hierarchy feature aggregation approach to tackle the specific issues in the tracking task, which consists of two aspects. On one hand, this paper integrates the features captured by the offline and online classifiers at the score level, which constructs complementary roles of these classifiers to enhance the stability of classification. Besides, the proposed offline classifier is continuously optimized with different levels of features to reinforce classification constraints. On the other hand, we design a butterfly attention module to promote the capacity of multi-hierarchy feature aggregation in the regression network, which aims to fuse and strengthen the multi-scale features by attending to their spatial information. It can capture more spatial contexts by utilizing the self-attention mechanism during the fusion procedure, and preserve the hierarchy of the features during the strengthening process. Extensive experiments on four public datasets, i.e., VOT2018, OTB100, NFS and LaSOT datasets, demonstrate the effectiveness of the proposed methods. Â© 2021 Elsevier B.V.",0,,
10.4230/LIPIcs.GIScience.2021.II.5,2021,"Alinaghi N., Kattenbeck M., Golab A., Giannopoulos I.",Will you take this turn? Gaze-based turning activity recognition during navigation,"Decision making is an integral part of wayfinding and people progressively use navigation systems to facilitate this task. The primary decision, which is also the main source of navigation error, is about the turning activity, i.e., to decide either to turn left or right or continue straight forward. The fundamental step to deal with this error, before applying any preventive approaches, e.g., providing more information, or any compensatory solutions, e.g., pre-calculating alternative routes, could be to predict and recognize the potential turning activity. This paper aims to address this step by predicting the turning decision of pedestrian wayfinders, before the actual action takes place, using primarily gaze-based features. Applying Machine Learning methods, the results of the presented experiment demonstrate an overall accuracy of 91% within three seconds before arriving at a decision point. Beyond the application perspective, our findings also shed light on the cognitive processes of decision making as reflected by the wayfinder's gaze behaviour: incorporating environmental and user-related factors to the model, results in a noticeable change with respect to the importance of visual search features in turn activity recognition. Â© Negar Alinaghi, Markus Kattenbeck, Antonia Golab, and Ioannis Giannopoulos; licensed under Creative Commons License CC-BY 4.0",0,,
10.3390/app11188607,2021,"Baazeem I., Al-Khalifa H., Al-Salman A.",Cognitively driven arabic text readability assessment using eye-tracking,"Using physiological data helps to identify the cognitive processing in the human brain. One method of obtaining these behavioral signals is by using eye-tracking technology. Previous cognitive psychology literature shows that readable and difficult-to-read texts are associated with certain eye movement patterns, which has recently encouraged researchers to use these patterns for readability assessment tasks. However, although it seems promising, this research direction has not been explored adequately, particularly for Arabic. The Arabic language is defined by its own rules and has its own characteristics and challenges. There is still a clear gap in determining the potential of using eye-tracking measures to improve Arabic text. Motivated by this, we present a pilot study to explore the extent to which eye-tracking measures enhance Arabic text readability. We collected the eye movements of 41 participants while reading Arabic texts to provide real-time processing of the text; these data were further analyzed and used to build several readability prediction models using different regression algorithms. The findings show an improvement in the readability prediction task, which requires further investigation. To the best of our knowledge, this work is the first study to explore the relationship between Arabic readability and eye movement patterns. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.1145/3478088,2021,"Islam M.R., Sakamoto S., Yamada Y., Vargo A.W., Iwata M., Iwamura M., Kise K.",Self-supervised Learning for Reading Activity Classification,"Reading analysis can relay information about user's confidence and habits and can be used to construct useful feedback. A lack of labeled data inhibits the effective application of fully-supervised Deep Learning (DL) for automatic reading analysis. We propose a Self-supervised Learning (SSL) method for reading analysis. Previously, SSL has been effective in physical human activity recognition (HAR) tasks, but it has not been applied to cognitive HAR tasks like reading. We first evaluate the proposed method on a four-class classification task on reading detection using electrooculography datasets, followed by an evaluation of a two-class classification task of confidence estimation on multiple-choice questions using eye-tracking datasets. Fully-supervised DL and support vector machines (SVMs) are used as comparisons for the proposed SSL method. The results show that the proposed SSL method is superior to the fully-supervised DL and SVM for both tasks, especially when training data is scarce. This result indicates the proposed method is the superior choice for reading analysis tasks. These results are important for informing the design of automatic reading analysis platforms. Â© 2021 ACM.",0,,
10.3389/frobt.2021.729832,2021,"Araya R., Sossa-Rivera J.",Automatic Detection of Gaze and Body Orientation in Elementary School Classrooms,"Detecting the direction of the gaze and orientation of the body of both teacher and students is essential to estimate who is paying attention to whom. It also provides vital clues for understanding their unconscious, non-verbal behavior. These are called â€œhonest signalsâ€?since they are unconscious subtle patterns in our interaction with other people that help reveal the focus of our attention. Inside the classroom, they provide important clues about teaching practices and students' responses to different conscious and unconscious teaching strategies. Scanning this non-verbal behavior in the classroom can provide important feedback to the teacher in order for them to improve their teaching practices. This type of analysis usually requires sophisticated eye-tracking equipment, motion sensors, or multiple cameras. However, for this to be a useful tool in the teacher's daily practice, an alternative must be found using only a smartphone. A smartphone is the only instrument that a teacher always has at their disposal and is nowadays considered truly ubiquitous. Our study looks at data from a group of first-grade classrooms. We show how video recordings on a teacher's smartphone can be used in order to estimate the direction of the teacher and studentsâ€?gaze, as well as their body orientation. Using the output from the OpenPose software, we run Machine Learning (ML) algorithms to train an estimator to recognize the direction of the studentsâ€?gaze and body orientation. We found that the level of accuracy achieved is comparable to that of human observers watching frames from the videos. The mean square errors (RMSE) of the predicted pitch and yaw angles for head and body directions are on average 11% lower than the RMSE between human annotators. However, our solution is much faster, avoids the tedium of doing it manually, and makes it possible to design solutions that give the teacher feedback as soon as they finish the class. Â© Copyright Â© 2021 Araya and Sossa-Rivera.",2,,
10.1007/s00521-020-05646-4,2021,"Samsami M.M., Zaheryani S.M.S., Yazdi M.","Astute, fine and fast method of iris segmentation in unlimited circumstances","Currently, Iris detection is considered as a significant module for robust biometric systems and high-speed applications such as eye tracking. Most iris segmentation models are based on machine learning algorithms or geometric methods. In this paper, we use an elliptical Hough transform to firstly detect the shape of the palpebral fissure. Then, a correlation-based circular Hough transform (we named it CCHT) is proposed to extract iris from the surrounding structures. One of the advantages of the proposed method is its ability to determine the closed-eye images, in order to remove these images in the process of eye tracking procedure. Moreover, the algorithm is simple and fast which make it suitable for on-line eye tracking. Experimental results on UBIRIS, which contains some defocused and eyelid-occluded images as non-ideal and noisy frames, indicate that the proposed method is efficient and much faster, in comparison with the previous approaches and encouraging improved accuracy on iris detection. Â© 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",2,,
10.1177/0735633120978617,2021,"PejiÄ‡ M., SaviÄ‡ G., Segedinac M.",Determining Gaze Behavior Patterns in On-Screen Testing,"This study proposes a software system for determining gaze patterns in on-screen testing. The system applies machine learning techniques to eye-movement data obtained from an eye-tracking device to categorize students according to their gaze behavior pattern while solving an on-screen test. These patterns are determined by converting eye movement coordinates into a sequence of regions of interest. The proposed software system extracts features from the sequence and performs clustering that groups students by their gaze pattern. To determine gaze patterns, the system contains components for communicating with an eye-tracking device, collecting and preprocessing studentsâ€?gaze data, and visualizing data using different presentation methods. This study presents a methodology to determine gaze patterns and the implementation details of the proposed software. The research was evaluated by determining the gaze patterns of 51 undergraduate students who took a general knowledge test containing 20 questions. This study aims to provide a software infrastructure that can use studentsâ€?gaze patterns as an additional indicator of their reading behaviors and their processing attention or difficulty, among other factors. Â© The Author(s) 2020.",1,,
10.1007/s10055-020-00478-y,2021,"Chiquet S., Martarelli C.S., Mast F.W.",Eye movements to absent objects during mental imagery and visual memory in immersive virtual reality,"The role of eye movements in mental imagery and visual memory is typically investigated by presenting stimuli or scenes on a two-dimensional (2D) computer screen. When questioned about objects that had previously been presented on-screen, people gaze back to the location of the stimuli, even though those regions are blank during retrieval. It remains unclear whether this behavior is limited to a highly controlled experimental setting using 2D screens or whether it also occurs in a more naturalistic setting. The present study aims to overcome this shortcoming. Three-dimensional (3D) objects were presented along a circular path in an immersive virtual room. During retrieval, participants were given two tasks: to visualize the objects, which they had encoded before, and to evaluate a statement about visual details of the object. We observed longer fixation duration in the area, on which the object was previously displayed, when compared to other possible target locations. However, in 89% of the time, participants fixated none of the predefined areas. On the one hand, this shows that looking at nothing may be overestimated in 2D screen-based paradigm, on the other hand, the looking at nothing effect was still present in the 3D immersive virtual reality setting, and thus it extends external validity of previous findings. Eye movements during retrieval reinstate spatial information of previously inspected stimuli. Â© 2020, The Author(s).",2,,
10.1109/TCDS.2019.2963073,2021,"Wu W., Sun W., Wu Q.M.J., Zhang C., Yang Y., Yu H., Lu B.-L.",Faster Single Model Vigilance Detection Based on Deep Learning,"Various reports have shown that the rate of road traffic accidents has increased due to reduced driver vigilance. Therefore, an accurate estimation of the driver's alertness status plays an important part. To estimate vigilance, we adopt a novel strategy that is a deep autoencoder with subnetwork nodes (DAESN). The proposed network model is designed not only for sparse representation but also for dimension reduction. Some hidden layers are not calculated by randomly acquired, but by replacement technologies. Unlike the traditional electrooculogram (EOG) signals, the forehead EOG (EOGF) signals are collected through forehead electrodes that do not have to surround the eyes, which has a convenient and effective practical application. The root-mean-square error (RMSE) and correlation coefficient (COR) while separately using three EOGF features improved to 0.11/0.79, 0.10/0.83, and 0.11/0.80, respectively. Implemented in an experimental environment, percentage of eye closure over time is calculated in real time through SMI eye-tracking-glasses, up to 120 frames/s. In addition, the time to extract features from the raw signal and display the prediction is only 34 ms, that is the level of the driver's fatigue can be detected quickly. The experimental study shows that the proposed model for vigilance analysis has better robustness and learning capability. Â© 2016 IEEE.",0,,
10.1109/ICCSCE52189.2021.9530980,2021,"Zheng L.J., Mountstephens J., Teo J.",A Comparative Investigation of Eye Fixation-based 4-Class Emotion Recognition in Virtual Reality Using Machine Learning,"Research on emotion recognition that relies purely on eye-tracking data is very limited although the usability of eye-tracking technology has great potential for emotional recognition. This paper proposes a novel approach for 4-class emotion classification using eye-tracking data solely in virtual reality (VR) with machine learning algorithms. We classify emotions into four specific classes using VR stimulus. Eye fixation data was used as the emotional-relevant feature in this investigation. A presentation of 3600 videos, which contains four different sessions, was played in VR to evoke the user's emotions. The eye-tracking data was collected and recorded using an add-on eye-tracker in the VR headset. Three classifiers were used in the experiment, which are k-nearest neighbor (KNN), random forest (RF), and support vector machine (SVM). The findings showed that RF has the best performance among the classifiers, and achieved the highest accuracy of 80.55%. Â© 2021 IEEE.",0,,
10.1109/INISTA52262.2021.9548444,2021,"Beriwal M., Agrawal S.",Techniques for suicidal ideation prediction: A qualitative systematic review,"Suicide is an increasingly present issue in our society whose eradication could be greatly aided by decision support technologies that can objectively identify early markers of suicidal ideation. We present our paper which reviews various existing techniques for suicidal ideation prediction. These techniques are broadly divided into two major categories - 1) Text-based indicators (which employ NLP) and 2) Behavioural indicators (like eye gaze and smile which are classified using ML algorithms). The existing techniques aim at classifying individuals as suicidal or non-suicidal using classifiers such as Random Forest, Logistic Regression, NaÃ¯ve Bayes, K-Nearest Neighbour (KNN), Support Vector Machine (SVM), Multi-Layer Perceptron and XGBoost. Each technique has its advantages and disadvantages. We aim to bring all these diverse techniques to one place and try to show how each one of them is individually contributing to prediction. We also shed light on how combining these techniques and eliminating the disadvantages could lead to better suicide prediction. Â© 2021 IEEE.",0,,
10.1016/j.neucom.2021.03.056,2021,"Tu Z., Zhou A., Gan C., Jiang B., Hussain A., Luo B.",A novel domain activation mapping-guided network (DA-GNT) for visual tracking,"Conventional convolution neural network (CNN)-based visual trackers are easily influenced by too much background information in candidate samples. Further, extreme imbalance of foreground and background samples has a negative impact on training the classifier, whereas features learned from limited data are insufficient to train the classifier. To address these problems, we propose a novel deep neural network for visual tracking, termed the domain activation mapping guided network (DA-GNT). First, we introduce the class activation mapping with weakly supervised localization in multi-domain to identify the most discriminative regions in the bounding box and suppress the background in the positive sample. Next, to further increase the discriminability of deep feature representation, we utilize an ensemble network to achieve a kind of multi-view feature representation and a channel attention mechanism for adaptive feature selection. Finally, we propose a simple but effective data augmentation method to further increase the positive samples for our network training. Extensive experiments on two widely used benchmark datasets demonstrate the effectiveness of the proposed tracking method against many state-of-the-art trackers. The novel DA-GNT is thus posited as a potential benchmark resource for the computer vision and machine learning research community. Â© 2021 Elsevier B.V.",2,,
10.1145/3450508.3464606,2021,"Mania K., McNamara A., Polychronakis A.",Gaze-aware displays and interaction,"Being able to detect and to employ gaze enhances digital displays. Research on gaze-contingent or gaze-aware display devices dates back two decades. This is the time, though, that it could truly be employed for fast, low-latency gaze-based interaction and for optimization of computer graphics rendering such as in foveated rendering. Moreover, Virtual Reality (VR) is becoming ubiquitous. The widespread availability of consumer grade VR Head Mounted Displays (HMDs) transformed VR to a commodity available for everyday use. VR applications are now abundantly designed for recreation, work and communication. However, interacting with VR setups requires new paradigms of User Interfaces (UIs), since traditional 2D UIs are designed to be viewed from a static vantage point only, e.g. the computer screen. Adding to this, traditional input methods such as the keyboard and mouse are hard to manipulate when the user wears a HMD. Recently, companies such as HTC announced embedded eye-tracking in their headsets and therefore, novel, immersive 3D UI paradigms embedded in a VR setup can now be controlled via eye gaze. Gaze-based interaction is intuitive and natural the users. Tasks can be performed directly into the 3D spatial context without having to search for an out-of-view keyboard/mouse. Furthermore, people with physical disabilities, already depending on technology for recreation and basic communication, can now benefit even more from VR. This course presents timely, relevant information on how gaze-contingent displays, in general, including the recent advances of Virtual Reality (VR) eye tracking capabilities can leverage eye-tracking data to optimize the user experience and to alleviate usability issues surrounding intuitive interaction challenges. Research topics to be covered include saliency models, gaze prediction, gaze tracking, gaze direction, foveated rendering, stereo grading and 3D User Interfaces (UIs) based on gaze on any gaze-aware display technology. Â© 2021 Owner/Author.",0,,
10.3389/frobt.2021.709952,2021,"Shi L., Copot C., Vanlanduit S.",Gaze Gesture Recognition by Graph Convolutional Networks,"Gaze gestures are extensively used in the interactions with agents/computers/robots. Either remote eye tracking devices or head-mounted devices (HMDs) have the advantage of hands-free during the interaction. Previous studies have demonstrated the success of applying machine learning techniques for gaze gesture recognition. More recently, graph neural networks (GNNs) have shown great potential applications in several research areas such as image classification, action recognition, and text classification. However, GNNs are less applied in eye tracking researches. In this work, we propose a graph convolutional network (GCN)â€“based model for gaze gesture recognition. We train and evaluate the GCN model on the HideMyGaze! dataset. The results show that the accuracy, precision, and recall of the GCN model are 97.62%, 97.18%, and 98.46%, respectively, which are higher than the other compared conventional machine learning algorithms, the artificial neural network (ANN) and the convolutional neural network (CNN). Â© Copyright Â© 2021 Shi, Copot and Vanlanduit.",0,,
10.1145/3450618.3469175,2021,"Daskalogrigorakis G., McNamara A., Mania K.",Holo-Box: Level-of-Detail Glanceable Interfaces for Augmented Reality,"Glanceable interfaces are Augmented Reality (AR) User Interfaces (UIs) for information retrieval ""at a glance""relying on eye gaze for implicit input. While they provide rapid information retrieval, they often occlude a large part of the real-world. This is compounded as the amount of virtual information increases. Interacting with complex glanceable interfaces often results in unintentional eye gaze interaction and selections due to the Midas Touch problem. In this work, we present Holo-box, an innovative AR UI design that combines 2D compact glanceable interfaces with 3D virtual ""Holo-boxes"". We can utilize the glanceable 2D interface to provide compact information at a glance while using Holo-box for explicit input such as hand tracking activated when necessary, surpassing the Midas Touch problem and resulting in Level-of-Detail(LOD) for AR glanceable UIs. We test our proposed system inside a real-world machine shop to provide on-demand virtual information while minimizing unintentional real-world occlusion. Â© 2021 Owner/Author.",0,,
10.1109/ICESC51422.2021.9532928,2021,"Akshay S., Abhishek M.B., Sudhanshu D., Anuvaishnav C.",Drowsy Driver Detection using Eye-Tracking through Machine Learning,"Eye tracking is one of the most useful but underutilized technologies in today's world. It can be used in a variety of ways now that the technology is available. We propose an implementation that has not been done but should be after studying several ways to process the same. In the field of Advanced Driving Assistance Systems, tracking drivers' eyes is a hot topic (ADAS). According to data from the World Health Organization (WHO), approximately 1-1.25 million people die and 20-50 million people suffer from non-fatal injuries in road accidents each year around the world. And a high majority of these collisions are caused by drowsy driving. Our paper explores a possible implementation that could help detect drowsiness given a subject, a phone camera and a single board computer. We establish a connection between the phone and the system using a network connection that streams the camera feed onto the system, which further performs computations to determine the drowsiness of the driver. Â© 2021 IEEE.",0,,
10.1109/ISET52350.2021.00044,2021,"Jun L., Juan Y., Weiwei Y., Xiaofang K.",Research on the Design of Primary School English Learning Resources Based on Cognitive Model,"Attention is one of the key cognitive factors that affect English learning. This paper analyzes the distribution and transfer of learners' attention in the process of English learning by using eye movements data, and explores the design principles of English learning resources. According to different English learning characteristics, three sets of learning resources are designed. Through the analysis of eye movements data, the design principles of picture learning resources and audio-visual multi-mode learning resources are obtained. Picture learning resources should emphasize the corresponding relationship between pictures and English vocabulary, so as to help learners establish and improve the second language system. The pictures in the learning resources should help learners form the overall cognition of English, rather than establish the corresponding Chinese meaning in English. Audio-visual multi-mode learning resources have a positive effect on the improvement of learners' English learning ability, but they need to be designed according to different English learning features. At the same time, the coordination of visual and auditory stimuli needs to be changed according to learners' learning ability. Â© 2021 IEEE.",0,,
10.1109/TVCG.2020.2975801,2021,"Meng X., Du R., Jaja J.F., Varshney A.",3D-Kernel Foveated Rendering for Light Fields,"Light fields capture both the spatial and angular rays, thus enabling free-viewpoint rendering and custom selection of the focal plane. Scientists can interactively explore pre-recorded microscopic light fields of organs, microbes, and neurons using virtual reality headsets. However, rendering high-resolution light fields at interactive frame rates requires a very high rate of texture sampling, which is challenging as the resolutions of light fields and displays continue to increase. In this article, we present an efficient algorithm to visualize 4D4D light fields with 3D-kernel foveated rendering (3D-KFR). The 3D-KFR scheme coupled with eye-tracking has the potential to accelerate the rendering of 4D4D depth-cued light fields dramatically. We have developed a perceptual model for foveated light fields by extending the KFR for the rendering of 3D3D meshes. On datasets of high-resolution microscopic light fields, we observe 3.47\times -7.28\times3.47Ã—-7.28Ã— speedup in light field rendering with minimal perceptual loss of detail. We envision that 3D-KFR will reconcile the mutually conflicting goals of visual fidelity and rendering speed for interactive visualization of light fields. Â© 1995-2012 IEEE.",0,,
10.3390/app11156797,2021,"Bjelopavlovic M., Weyhrauch M., Erbe C., Burkard F., Petrowski K., Lehmann K.M.",Influencing factors on aesthetics: Highly controlled study based on eye movement and the forensic aspects in computer-based assessment of visual appeal in upper front teeth,"First impressions are formed by the external appearance and, in this respect, essentially by an examination of the face. In the literature, the teeth, especially the maxillary front, are among an eye-catching and sensitive area that plays a significant role in the overall evaluation of appearance. In this study, the first eye fixation of 60 subjects with different levels of dental training (layperson, trained layperson, dental student, and dentist) is recorded using an eye-tracking system, and their subsequent evaluation of the images is recorded. Ten unedited original photographs of different maxillary anterior teeth and ten subsequently edited photographs will be used to evaluate forensic aspects such as the effect of symmetry and color on the overall evaluation. The results will be used to determine which areas of the maxillary anterior are demonstrably viewed and whether knowledge of dental esthetics influences evaluation and viewing. Â© 2021 by the authors.",0,,
10.3390/s21155109,2021,"Garde G., Larumbe-Bergera A., Bossavit B., Porta S., Cabeza R., Villanueva A.","Low-cost eye tracking calibration: A knowledge-based studyâ€?""Subject calibration has been demonstrated to improve the accuracy in high-performance eye trackers. However", the true weight of calibration in off-the-shelf eye tracking solutions is still not addressed. In this work,2, ,
10.1007/s11517-021-02386-y,2021,"Ayyagari S.S.D.P., Jones R.D., Weddell S.J.",Detection of microsleep states from the EEG: a comparison of feature reduction methods,"Microsleeps are brief lapses in consciousness with complete suspension of performance. They are the cause of fatal accidents in many transport sectors requiring sustained attention, especially driving. A microsleep-warning device, using wireless EEG electrodes, could be used to rouse a user from an imminent microsleep. High-dimensional datasets, especially in EEG-based classification, present challenges as there are often a large number of potentially useful features for detecting the phenomenon of interest. Thus, it is often important to reduce the dimension of the original data prior to training the classifier. In this study, linear dimensionality reduction methodsâ€”principal component analysis (PCA) and probabilistic PCA (PPCA)â€”were compared with eight non-linear dimensionality reduction methods (kernel PCA, classical multi-dimensional scaling, isometric mapping, nearest neighbour estimation, stochastic neighbourhood embedding, autoencoder, stochastic proximity embedding, and Laplacian eigenmaps) on previously collected behavioural and EEG data from eight healthy non-sleep-deprived volunteers performing a 1D-visuomotor tracking task for 1 h. The effectiveness of the feature reduction algorithms was evaluated by visual inspection of class separation on 3D scatterplots, by trustworthiness scores, and by microsleep detection performance on a stacked-generalisation-based linear discriminant analysis (LDA) system estimating the microsleep/responsive state at 1 Hz based on the reduced features. On trustworthiness, PPCA outperformed PCA, but PCA outperformed all of the non-linear techniques. The trustworthiness score for each feature reduction method also correlated strongly with microsleep-state detection performance, providing strong validation of the ability of trustworthiness to estimate the relative effectiveness of feature reduction approaches, in terms of predicting performance, and ability to do so independently of the gold standard. [Figure not available: see fulltext.] Â© 2021, International Federation for Medical and Biological Engineering.",0,,
10.3390/s21154965,2021,"Whang A.J.-W., Chen Y.-Y., Tseng W.-C., Tsai C.-H., Chao Y.-P., Yen C.-H., Liu C.-H., Zhang X.",Pupil size prediction techniques based on convolution neural network,"The size of oneâ€™s pupil can indicate oneâ€™s physical condition and mental state. When we search related papers about AI and the pupil, most studies focused on eye-tracking. This paper proposes an algorithm that can calculate pupil size based on a convolution neural network (CNN). Usually, the shape of the pupil is not round, and 50% of pupils can be calculated using ellipses as the best fitting shapes. This paper uses the major and minor axes of an ellipse to represent the size of pupils and uses the two parameters as the output of the network. Regarding the input of the network, the dataset is in video format (continuous frames). Taking each frame from the videos and using these to train the CNN model may cause overfitting since the images are too similar. This study used data augmentation and calculated the structural similarity to ensure that the images had a certain degree of difference to avoid this problem. For optimizing the network structure, this study compared the mean error with changes in the depth of the network and the field of view (FOV) of the convolution filter. The result shows that both deepening the network and widening the FOV of the convolution filter can reduce the mean error. According to the results, the mean error of the pupil length is 5.437% and the pupil area is 10.57%. It can operate in low-cost mobile embedded systems at 35 frames per second, demonstrating that low-cost designs can be used for pupil size prediction. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.1007/s00500-021-05984-y,2021,"Shi L., Wang C.Y., Tian F., Jia H.B.",An integrated neural network model for pupil detection and tracking,"The accurate detection and tracking of pupil is important to many applications such as humanâ€“computer interaction, driverâ€™s fatigue detection and diagnosis of brain diseases. Existing approaches however face challenges in handing low quality of pupil images. In this paper, we propose an integrated pupil tracking framework, namely LVCF, based on deep learning. LVCF consists of the pupil detection model VCF which is an end-to-end network, and the LSTM pupil motion prediction model which applies LSTM to track pupilâ€™s position. The proposed network was trained and evaluated on 10600 images and 75 videos taken from 3 realistic datasets. Within an error threshold of 5 pixels, VCF achieves an accuracy of more than 81%, and LVCF outperforms the state of arts by 9% in terms of percentage of pupils tracked. The project of LCVF is available at https://github.com/UnderTheMangoTree/LVCF. Â© 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",0,,
10.1016/j.compbiomed.2021.104589,2021,"Sharma H., Drukker L., Papageorghiou A.T., Noble J.A.",Machine learning-based analysis of operator pupillary response to assess cognitive workload in clinical ultrasound imaging,"Introduction: Pupillometry, the measurement of eye pupil diameter, is a well-established and objective modality correlated with cognitive workload. In this paper, we analyse the pupillary response of ultrasound imaging operators to assess their cognitive workload, captured while they undertake routine fetal ultrasound examinations. Our experiments and analysis are performed on real-world datasets obtained using remote eye-tracking under natural clinical environmental conditions. Methods: Our analysis pipeline involves careful temporal sequence (time-series) extraction by retrospectively matching the pupil diameter data with tasks captured in the corresponding ultrasound scan video in a multi-modal data acquisition setup. This is followed by the pupil diameter pre-processing and the calculation of pupillary response sequences. Exploratory statistical analysis of the operator pupillary responses and comparisons of the distributions between ultrasonographic tasks (fetal heart versus fetal brain) and operator expertise (newly-qualified versus experienced operators) are performed. Machine learning is explored to automatically classify the temporal sequences into the corresponding ultrasonographic tasks and operator experience using temporal, spectral, and time-frequency features with classical (shallow) models, and convolutional neural networks as deep learning models. Results: Preliminary statistical analysis of the extracted pupillary response shows a significant variation for different ultrasonographic tasks and operator expertise, suggesting different extents of cognitive workload in each case, as measured by pupillometry. The best-performing machine learning models achieve receiver operating characteristic (ROC) area under curve (AUC) values of 0.98 and 0.80, for ultrasonographic task classification and operator experience classification, respectively. Conclusion: We conclude that we can successfully assess cognitive workload from pupil diameter changes measured while ultrasound operators perform routine scans. The machine learning allows the discrimination of the undertaken ultrasonographic tasks and scanning expertise using the pupillary response sequences as an index of the operatorsâ€?cognitive workload. A high cognitive workload can reduce operator efficiency and constrain their decision-making, hence, the ability to objectively assess cognitive workload is a first step towards understanding these effects on operator performance in biomedical applications such as medical imaging. Â© 2021 The Authors",0,,
10.1007/s00530-021-00806-5,2021,"Min-Allah N., Jan F., Alrashed S.",Pupil detection schemes in human eye: a review,"Pupil detection in a human eyeimage or video plays a key role in many applications such as eye-tracking, diabetic retinopathy screening, smart homes, iris recognition, etc. Literature reveals pupil detection faces many complications including light reflections, cataract disease, pupil constriction/dilation moments, contact lenses, eyebrows, eyelashes, hair strips, and closed eye. To cope with these challenges, research community has been struggling to devise resilient pupil localization schemes for the image/video data collected using the near-infrared (NIR) or visible spectrum (VS) illumination. This study presents a critical review of numerous pupil detection schemes taken from standard sources. This review includes pupil localization schemes based on machine learning, histogram/thresholding, Integro-differential operator (IDO), Hough transform and among others. The probable pros and cons of each scheme are highlighted. Finally, this study offers recommendations for designing a robust pupil detection system. As scope of pupil detection is very broader, therefore this review would be a great source of information for the relevant research community. Â© 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",0,,
10.1016/j.neunet.2021.04.004,2021,"Fan N., Li X., Zhou Z., Liu Q., He Z.",Learning dual-margin model for visual tracking,"Existing trackers usually exploit robust features or online updating mechanisms to deal with target variations which is a key challenge in visual tracking. However, the features being robust to variations remain little spatial information, and existing online updating methods are prone to overfitting. In this paper, we propose a dual-margin model for robust and accurate visual tracking. The dual-margin model comprises an intra-object margin between different target appearances and an inter-object margin between the target and the background. The proposed method is able to not only distinguish the target from the background but also perceive the target changes, which tracks target appearance changing and facilitates accurate target state estimation. In addition, to exploit rich off-line video data and learn general rules of target appearance variations, we train the dual-margin model on a large off-line video dataset. We perform tracking under a Siamese framework using the constructed appearance set as templates. The proposed method achieves accurate and robust tracking performance on five public datasets while running in real-time. The favorable performance against the state-of-the-art methods demonstrates the effectiveness of the proposed algorithm. Â© 2021 Elsevier Ltd",0,,
10.1007/s11042-021-10520-z,2021,"Tadeja S.K., Lu Y., Rydlewicz M., Rydlewicz W., Bubas T., Kristensson P.O.",Exploring gestural input for engineering surveys of real-life structures in virtual reality using photogrammetric 3D models,"Photogrammetry is a promising set of methods for generating photorealistic 3D models of physical objects and structures. Such methods may rely solely on camera-captured photographs or include additional sensor data. Digital twins are digital replicas of physical objects and structures. Photogrammetry is an opportune approach for generating 3D models for the purpose of preparing digital twins. At a sufficiently high level of quality, digital twins provide effective archival representations of physical objects and structures and become effective substitutes for engineering inspections and surveying. While photogrammetric techniques are well-established, insights about effective methods for interacting with such models in virtual reality remain underexplored. We report the results of a qualitative engineering case study in which we asked six domain experts to carry out engineering measurement tasks in an immersive environment using bimanual gestural input coupled with gaze-tracking. The qualitative case study revealed that gaze-supported bimanual interaction of photogrammetric 3D models is a promising modality for domain experts. It allows the experts to efficiently manipulate and measure elements of the 3D model. To better allow designers to support this modality, we report design implications distilled from the feedback from the domain experts. Â© 2021, The Author(s).",0,,
10.23919/CCC52363.2021.9549800,2021,"Yang B., Huang J., Sun M., Huo J., Li X., Xiong C.","Head-free, Human Gaze-driven Assistive Robotic System for Reaching and Grasping","Patients with limb dysfunction have limited mobility, which prevents them from performing daily activities. We have developed an assistive robot system with an intuitive head free gaze interface. The system consists of multiple modules, including 3D gaze estimation, head free coordinate transformation, intention recognition, and robot trajectory planning. The robotic assistive system obtains clues from the user's gaze to decode their intentions and implement actions. This allows the user only needs to look at the objects to make the robot system reach, grasp, and bring them to the user. The 3D gaze estimation is evaluated with 5 subjects, showing an overall accuracy of 5.53Â±1.2 cm. The integrated system's experimental results show that the success rate is 96% in the implementation of automatic trajectory planning, and the success rate is 92% in the implementation of fixation-based trajectory planning. Finally, the results and work required to improve the system are discussed. Â© 2021 Technical Committee on Control Theory, Chinese Association of Automation.",2,,
10.1016/j.neucom.2020.06.137,2021,"Wang X., Zhao X., Zhang Y.",Deep-learning-based reading eye-movement analysis for aiding biometric recognition,"Eye-movement recognition is a new type of biometric recognition technology. Without considering the characteristics of the stimuli, the existing eye-movement recognition technology is based on eye-movement trajectory similarity measurements and uses more eye-movement features. Related studies on reading psychology have shown that when reading text, human eye-movements are different between individuals yet stable for a given individual. This paper proposes a type of technology for aiding biometric recognition based on reading eye-movement. By introducing a deep-learning framework, a computational model for reading eye-movement recognition (REMR) was constructed. The model takes the text, fixation, and text-based linguistic feature sequences as inputs and identifies a human subject by measuring the similarity distance between the predicted fixation sequence and the actual one (to be identified). The experimental results show that the fixation sequence similarity recognition algorithm obtained an equal error rate of 19.4% on the test set, and the model obtained an 86.5% Rank-1 recognition rate on the test set. Â© 2020 Elsevier B.V.",0,,
10.1109/HSI52170.2021.9538710,2021,Kocejko T.,Using deep learning to increase accuracy of gaze controlled prosthetic arm,This paper presents how neural networks can be utilized to improve the accuracy of reach and grab functionality of hybrid prosthetic arm with eye tracing interface. The LSTM based Autoencoder was introduced to overcome the problem of lack of accuracy of the gaze tracking modality in this hybrid interface. The gaze based interaction strongly depends on the eye tracking hardware. In this paper it was presented how the overall the accuracy can be slightly improved by software solution. The cloud of points related to possible final positions of the arm was created to train Autoencoder. The trained model was next used to improve the position provided by the eye tracker. Using the LSTM based Autoencoder resulted in nearly 3% improvement of the overall accuracy. Â© 2021 IEEE.,0,,
10.1109/MOCAST52088.2021.9493357,2021,"Kollias K.-F., Syriopoulou-Delli C.K., Sarigiannidis P., Fragulis G.F.",The contribution of Machine Learning and Eye-tracking technology in Autism Spectrum Disorder research: A Review Study,"According to Diagnostic and Statistical Manual of Mental Disorders, Autism spectrum disorder (ASD) is a developmental disorder characterised by reduced social interaction and communication, and by restricted, repetitive, and stereotyped behaviour. An important characteristic of autism, referred in several diagnostic tests, is a deficit in eye gaze. The objective of this study is to review the literature concerning machine learning and eye-tracking in ASD studies conducted since 2015. Our search on PubMed identified 18 studies which used various eye-tracking instruments, applied machine learning in different ways, distributed several tasks and had a wide range of sample sizes, age groups and functional skills of participants. There were also studies that utilised other instruments, such as Electroencephalography (EEG) and movement measures. Taken together, the results of these studies show that the combination of machine learning, and eye-tracking technology can contribute to autism identification characteristics by detecting the visual atypicalities of ASD people. In conclusion, machine learning and eye-tracking ASD studies could be considered a promising tool in autism research and future studies could involve other technological approaches, such as Internet of Things (IoT), as well. Â© 2021 IEEE.",0,,
10.3390/s21144769,2021,"Palmero C., Sharma A., Behrendt K., Krishnakumar K., Komogortsev O.V., Talathi S.S.",Openeds2020 challenge on gaze tracking for vr: Dataset and results,"This paper summarizes the OpenEDS 2020 Challenge dataset, the proposed baselines, and results obtained by the top three winners of each competition: (1) Gaze prediction Challenge, with the goal of predicting the gaze vector 1 to 5 frames into the future based on a sequence of previous eye images, and (2) Sparse Temporal Semantic Segmentation Challenge, with the goal of using temporal information to propagate semantic eye labels to contiguous eye image frames. Both competitions were based on the OpenEDS2020 dataset, a novel dataset of eye-image sequences captured at a frame rate of 100 Hz under controlled illumination, using a virtual-reality head-mounted display with two synchronized eye-facing cameras. The dataset, which we make publicly available for the research community, consists of 87 subjects performing several gaze-elicited tasks, and is divided into 2 subsets, one for each competition task. The proposed baselines, based on deep learning approaches, obtained an average angular error of 5.37 degrees for gaze prediction, and a mean intersection over union score (mIoU) of 84.1% for semantic segmentation. The winning solutions were able to outperform the baselines, obtaining up to 3.17 degrees for the former task and 95.2% mIoU for the latter. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",1,,
10.3390/s21144686,2021,"Yoo S., Jeong S., Jang Y.",Gaze behavior effect on gaze data visualization at different abstraction levels,"Many gaze data visualization techniques intuitively show eye movement together with visual stimuli. The eye tracker records a large number of eye movements within a short period. Therefore, visualizing raw gaze data with the visual stimulus appears complicated and obscured, making it difficult to gain insight through visualization. To avoid the complication, we often employ fixation identification algorithms for more abstract visualizations. In the past, many scientists have focused on gaze data abstraction with the attention map and analyzed detail gaze movement patterns with the scanpath visualization. Abstract eye movement patterns change dramatically depending on fixation identification algorithms in the preprocessing. However, it is difficult to find out how fixation identification algorithms affect gaze movement pattern visualizations. Additionally, scientists often spend much time on adjusting parameters manually in the fixation identification algorithms. In this paper, we propose a gaze behavior-based data processing method for abstract gaze data visualization. The proposed method classifies raw gaze data using machine learning models for image classification, such as CNN, AlexNet, and LeNet. Additionally, we compare the velocity-based identification (I-VT), dispersion-based identification (I-DT), density-based fixation identification, velocity and dispersion-based (I-VDT), and machine learning based and behavior-based modelson various visualizations at each abstraction level, such as attention map, scanpath, and abstract gaze movement visualization. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.1145/3446638,2021,"Spiller M., Liu Y.-H., Hossain M.Z., Gedeon T., Geissler J., NÃ¼rnberger A.",Predicting Visual Search Task Success from Eye Gaze Data as a Basis for User-Adaptive Information Visualization Systems,"Information visualizations are an efficient means to support the users in understanding large amounts of complex, interconnected data; user comprehension, however, depends on individual factors such as their cognitive abilities. The research literature provides evidence that user-adaptive information visualizations positively impact the users' performance in visualization tasks. This study attempts to contribute toward the development of a computational model to predict the users' success in visual search tasks from eye gaze data and thereby drive such user-adaptive systems. State-of-the-art deep learning models for time series classification have been trained on sequential eye gaze data obtained from 40 study participants' interaction with a circular and an organizational graph. The results suggest that such models yield higher accuracy than a baseline classifier and previously used models for this purpose. In particular, a Multivariate Long Short Term Memory Fully Convolutional Network shows encouraging performance for its use in online user-adaptive systems. Given this finding, such a computational model can infer the users' need for support during interaction with a graph and trigger appropriate interventions in user-adaptive information visualization systems. This facilitates the design of such systems since further interaction data like mouse clicks is not required. Â© 2021 Association for Computing Machinery.",0,,
10.3390/app11136157,2021,"SÃ¡iz-Manzanares M.C., PÃ©rez I.R., RodrÃ­guez A.A., Arribas S.R., Almeida L., Martin C.F.",Analysis of the learning process through eye tracking technology and feature selection techniques,"In recent decades, the use of technological resources such as the eye tracking methodology is providing cognitive researchers with important tools to better understand the learning process. However, the interpretation of the metrics requires the use of supervised and unsupervised learning techniques. The main goal of this study was to analyse the results obtained with the eye tracking methodology by applying statistical tests and supervised and unsupervised machine learning tech-niques, and to contrast the effectiveness of each one. The parameters of fixations, saccades, blinks and scan path, and the results in a puzzle task were found. The statistical study concluded that no significant differences were found between participants in solving the crossword puzzle task; significant differences were only detected in the parameters saccade amplitude minimum and saccade velocity minimum. On the other hand, this study, with supervised machine learning techniques, provided possible features for analysis, some of them different from those used in the statistical study. Regarding the clustering techniques, a good fit was found between the algorithms used (k-means ++, fuzzy k-means and DBSCAN). These algorithms provided the learning profile of the participants in three types (students over 50 years old; and students and teachers under 50 years of age). Therefore, the use of both types of data analysis is considered complementary. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.3390/app11135956,2021,"Parra E., Chicchi Giglioli I.A., Philip J., Carrasco-Ribelles L.A., MarÃ­n-Morales J., AlcaÃ±iz Raya M.",Combining virtual reality and organizational neuroscience for leadership assessment,"In this article, we introduce three-dimensional Serious Games (3DSGs) under an evidencecentered design (ECD) framework and use an organizational neuroscience-based eye-tracking measure to capture implicit behavioral signals associated with leadership skills. While ECD is a wellestablished framework used in the design and development of assessments, it has rarely been utilized in organizational research. The study proposes a novel 3DSG combined with organizational neuroscience methods as a promising tool to assess and recognize leadership-related behavioral patterns that manifest during complex and realistic social situations. We offer a research protocol for assessing task-and relationship-oriented leadership skills that uses ECD, eye-tracking measures, and machine learning. Seamlessly embedding biological measures into 3DSGs enables objective assessment methods that are based on machine learning techniques to achieve high ecological validity. We conclude by describing a future research agenda for the combined use of 3DSGs and organizational neuroscience methods for leadership and human resources. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.3390/s21134542,2021,"Kaczorowska M., Karczmarek P., Plechawska-WÃ³jcik M., Tokovarov M.",On the improvement of eye tracking-based cognitive workload estimation using aggregation functions,"Cognitive workload, being a quantitative measure of mental effort, draws significant interest of researchers, as it allows to monitor the state of mental fatigue. Estimation of cognitive workload becomes especially important for job positions requiring outstanding engagement and responsibility, e.g., air-traffic dispatchers, pilots, car or train drivers. Cognitive workload estimation finds its applications also in the field of education material preparation. It allows to monitor the difficulty degree for specific tasks enabling to adjust the level of education materials to typical abilities of students. In this study, we present the results of research conducted with the goal of examining the influence of various fuzzy or non-fuzzy aggregation functions upon the quality of cognitive workload estimation. Various classic machine learning models were successfully applied to the problem. The results of extensive in-depth experiments with over 2000 aggregation operators shows the applicability of the approach based on the aggregation functions. Moreover, the approach based on aggregation process allows for further improvement of classification results. A wide range of aggregation functions is considered and the results suggest that the combination of classical machine learning models and aggregation methods allows to achieve high quality of cognitive workload level recognition preserving low computational cost. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.3390/s21134502,2021,"Golard A., Talathi S.S.",Ultrasound for gaze estimationâ€”a modeling and empirical study,"Most eye tracking methods are light-based. As such, they can suffer from ambient light changes when used outdoors, especially for use cases where eye trackers are embedded in Augmented Reality glasses. It has been recently suggested that ultrasound could provide a low power, fast, light-insensitive alternative to camera-based sensors for eye tracking. Here, we report on our work on modeling ultrasound sensor integration into a glasses form factor AR device to evaluate the feasibility of estimating eye-gaze in various configurations. Next, we designed a benchtop experimental setup to collect empirical data on time of flight and amplitude signals for reflected ultrasound waves for a range of gaze angles of a model eye. We used this data as input for a low-complexity gradient-boosted tree machine learning regression model and demonstrate that we can effectively estimate gaze (gaze RMSE error of 0.965 Â± 0.178 degrees with an adjusted R2 score of 90.2 Â± 4.6). Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.3390/s21134400,2021,"Antonioli L., Pella A., Ricotti R., Rossi M., Fiore M.R., Belotti G., Magro G., Paganelli C., Orlandi E., Ciocca M., Baroni G.",Convolutional neural networks cascade for automatic pupil and iris detection in ocular proton therapy,"Eye tracking techniques based on deep learning are rapidly spreading in a wide variety of application fields. With this study, we want to exploit the potentiality of eye tracking techniques in ocular proton therapy (OPT) applications. We implemented a fully automatic approach based on two-stage convolutional neural networks (CNNs): the first stage roughly identifies the eye position and the second one performs a fine iris and pupil detection. We selected 707 video frames recorded during clinical operations during OPT treatments performed at our institute. 650 frames were used for training and 57 for a blind test. The estimations of iris and pupil were evaluated against the manual labelled contours delineated by a clinical operator. For iris and pupil predictions, Dice coefficient (median = 0.94 and 0.97), Szymkiewiczâ€“Simpson coefficient (median = 0.97 and 0.98), Intersection over Union coefficient (median = 0.88 and 0.94) and Hausdorff distance (median = 11.6 and 5.0 (pixels)) were quantified. Iris and pupil regions were found to be comparable to the manually labelled ground truths. Our proposed framework could provide an automatic approach to quantitatively evaluating pupil and iris misalignments, and it could be used as an additional support tool for clinical activity, without impacting in any way with the consolidated routine. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.1007/s00371-020-01934-1,2021,"Zhang Z., Lian D., Gao S.",RGB-D-based gaze point estimation via multi-column CNNs and facial landmarks global optimization,"In this work, we utilize a multi-column CNNs framework to estimate the gaze point of a person sitting in front of a display from an RGB-D image of the person. Given that gaze points are determined by head poses, eyeball poses, and 3D eye positions, we propose to infer the three components separately and then integrate them for gaze point estimation. The captured depth images, however, usually contain noises and black holes which prevent us from acquiring reliable head pose and 3D eye position estimation. Therefore, we propose to refine the raw depth for 68 facial keypoints by first estimating their relative depths from RGB face images, which along with the captured raw depths are then used to solve the absolute depth for all facial keypoints through global optimization. The refined depths will provide us reliable estimation for both head pose and 3D eye position. Given that existing publicly available RGB-D gaze tracking datasets are small, we also build a new dataset for training and validating our method. To the best of our knowledge, it is the largest RGB-D gaze tracking dataset in terms of the number of participants. Comprehensive experiments demonstrate that our method outperforms existing methods by a large margin on both our dataset and the Eyediap dataset. Â© 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",1,,
10.5194/isprs-archives-XLIII-B3-2021-887-2021,2021,"Oki T., Kizawa S.",Evaluating visual impressions based on gaze analysis and deep learning: A case study of attractiveness evaluation of streets in densely built-up wooden residential area,"This paper examines the possibility of impression evaluation based on gaze analysis of subjects and deep learning, using an example of evaluating street attractiveness in densely built-up wooden residential areas. Firstly, the relationship between the subjects' gazing tendency and their evaluation of street image attractiveness is analysed by measuring the subjects' gaze with an eye tracker. Next, we construct a model that can estimate an attractiveness evaluation result using convolutional neural networks (CNNs), combined with the method of gradient-weighted class activation mapping (Grad-CAM) - these in in visualizing which street components can contribute to evaluating attractiveness. Finally, we discuss the similarity between the subjects' gaze tendencies and activation heatmaps created by Grad-CAM. Â© 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.",0,,
10.1109/CBMI50038.2021.9461895,2021,"Bernard V., Wannous H., Vandeborre J.-P.",Eye-Gaze Estimation using a Deep Capsule-based Regression Network,"Eye-gaze information is used in a variety of user platforms, such as driver monitoring systems and head-mounted interfaces. In order to estimate human eye-gaze, many solutions have been proposed, using different devices and techniques. However, achieving such estimation using only cheap devices like RGB cameras would enable gaze interactions on mobile devices and therefore generalise this kind of interaction. It could also enable behavior studies based on gaze and made on every day devices. We propose in this paper a new method for eye-gaze estimation using a new deep learning architecture based on the Capsule Neural Network. Capsule Networks have shown great results so far on classification tasks, but only a few works use them for regression tasks.By taking advantage of the Capsule Network architecture and its ability to reconstruct images, we are able to recreate simplified eye images and then estimate human gaze from them. Experiments are performed on two representative datasets for the task of eye-gaze estimation. Encouraging results are obtained for both the estimation and the reconstruction. Â© 2021 IEEE.",2,,1
10.1142/S0218126621501267,2021,"Hou Z., Chao X., Liang J., Yang T.",A Multi-Modal Gaze Tracking Algorithm,"A person's emotional information, needs and cognitive processes can be described by eye movement states and concerns, so gaze tracking was first applied in the field of psychology. With the continuous development of information technology, the application range of gaze tracking has expanded from psychology to medical, military, commercial and many other fields. Aiming at the problem of high misjudgment rate and long time-consuming of traditional iris location methods, this paper proposes a gaze tracking method based on human eye geometric characteristics to improve the tracking accuracy in 2D environment. First, the human face is located by face location algorithm and the position of human eye is estimated roughly. Then the iris template is built by iris image, and the iris center location algorithm is used to locate the iris center position. Finally, the eyes corners and iris center points are extracted to locate the eye area accurately and obtain the binocular image. The binocular images are input into the feature extraction network as multi-modal information in parallel, and the convoluted feature channels are reconstructed using the weight redistribution module in the network. Then the reconstructed features are fused in the full connection layer. Finally, the output layer is used to classify the reconstructed features. Experiments were carried out on a self-built screen block dataset. For 12 classified data, the lowest recognition error rate is 5.34%. Â© 2021 World Scientific Publishing Company.",0,,
10.3390/s21124143,2021,"Barz M., Sonntag D.",Automatic visual attention detection for mobile eye tracking using pre-trained computer vision models and human gaze,"Processing visual stimuli in a scene is essential for the human brain to make situation-aware decisions. These stimuli, which are prevalent subjects of diagnostic eye tracking studies, are commonly encoded as rectangular areas of interest (AOIs) per frame. Because it is a tedious manual annotation task, the automatic detection and annotation of visual attention to AOIs can accelerate and objectify eye tracking research, in particular for mobile eye tracking with egocentric video feeds. In this work, we implement two methods to automatically detect visual attention to AOIs using pre-trained deep learning models for image classification and object detection. Furthermore, we develop an evaluation framework based on the VISUS dataset and well-known performance metrics from the field of activity recognition. We systematically evaluate our methods within this framework, discuss potentials and limitations, and propose ways to improve the performance of future automatic visual attention detection methods. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.3390/s21124006,2021,"Hussain R., Chessa M., Solari F.",Mitigating cybersickness in virtual reality systems through foveated depth-of-field blur,"Cybersickness is one of the major roadblocks in the widespread adoption of mixed reality devices. Prolonged exposure to these devices, especially virtual reality devices, can cause users to feel discomfort and nausea, spoiling the immersive experience. Incorporating spatial blur in stereoscopic 3D stimuli has shown to reduce cybersickness. In this paper, we develop a technique to incorporate spatial blur in VR systems inspired by the human physiological system. The technique makes use of concepts from foveated imaging and depth-of-field. The developed technique can be applied to any eye tracker equipped VR system as a post-processing step to provide an artifact-free scene. We verify the usefulness of the proposed system by conducting a user study on cybersickness evaluation. We used a custom-built rollercoaster VR environment developed in Unity and an HTC Vive Pro Eye headset to interact with the user. A Simulator Sickness Questionnaire was used to measure the induced sickness while gaze and heart rate data were recorded for quantitative analysis. The experimental analysis highlighted the aptness of our foveated depth-of-field effect in reducing cybersickness in virtual environments by reducing the sickness scores by approximately 66%. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.1109/CVPRW53098.2021.00351,2021,"Murthy L.R.D., Biswas P.",Appearance-based gaze estimation using attention and difference mechanism,"Appearance-based gaze estimation problem received wide attention over the past few years. Even though model-based approaches existed earlier, availability of large datasets and novel deep learning techniques made appearance-based methods achieve superior accuracy than model-based approaches. In this paper, we proposed two novel techniques to improve gaze estimation accuracy. Our first approach, I2D-Net uses a difference layer to eliminate any common features from left and right eyes of a participant that are not pertinent to gaze estimation task. Our second approach, AGE-Net adapted the idea of attention-mechanism and assigns weights to the features extracted from eye images. I2D-Net performed on par with the existing state-of-the-art approaches while AGE-Net reported state-of-the-art accuracy of 4.09 and 7.44 error on MPI-IGaze and RT-Gene datasets respectively. We performed ablation studies to understand the effectiveness of the proposed approaches followed by analysis of gaze error distribution with respect to various factors of MPIIGaze dataset. Â© 2021 IEEE.",1,,
10.1109/CBMS52027.2021.00016,2021,"Cardoso T.V., Michelassi G.C., Franco F.O., Sumiya F.M., Portolese J., Brentani H., Machado-Lima A., Nunes F.L.S.",Autism spectrum disorder diagnosis based on trajectories of eye tracking data,"The use of Eye Tracking (ET) has been investigated as an auxiliary mechanism to diagnose Autism Spectrum Disorder (ASD). One of the paradigms investigated using ET is Joint Attention (JA), which refers to moments when two individuals are focused on the same object/event so that both are aware that the focus of attention is shared. The computational tools that assist in the diagnosis of ASD have used Image Processing and Machine Learning techniques to process images, videos and ET signals. However, the JA paradigm is still little explored and presents challenges, as it requires analyzing the gaze trajectory and needs innovative approaches. The purpose of this article is to propose a model capable of extracting features from a video used as a stimulus to capture ET signals in order to verify JA and classify individuals as belonging to the ASD or Typical Development (TD) group. The main differential in relation to the approaches in the literature is the definition and implementation of the concept of floating Regions of Interest, which allows monitoring the gaze in relation to an object, considering its semantics, even if the object presents different characteristics throughout the video. A model based on ensembles of Random Forest classifiers was implemented to classify individuals as ASD or TD using the trajectory features extracted from the ET signals. The method reached 0.75 accuracy and 0.82 F1-score, indicating that the proposed approach, based on trajectory and JA, has the potential to be applied to assist in the diagnosis of ASD. Â© 2021 IEEE.",0,,
10.1109/CBMS52027.2021.00062,2021,"Revers M.C., Oliveira J.S., Franco F.O., Portolese J., Cardoso T.V., Silva A.F., Machado-Lima A., Nunes F.L.S., Brentani H.",Classification of autism spectrum disorder severity using eye tracking data based on visual attention model,"Computer-aided diagnosis using eye tracking data is classically based on regions of interest in the image. However, in recent years, the modeling of visual attention by saliency maps has shown better results. Wang et al., considering 3-layered saliency model that incorporated pixel-level, object-level, and semantic-level attributes, showed differences in the performance of eye tracking in autism spectrum disorder (ASD) and better characterized these differences by looking at which attributes were used, providing meaningful clinical results about the disorder. Our hypothesis is that the context interpretation would be worse according to the severity of ASD, consequently, the eye tracking data processed based on visual attention model (VAM) could be used to classify patients with ASD according to gravity. In this context, the present work proposes: 1) based on VAM, using Image Processing and Artificial Intelligence to learn a model for each group (severe and non-severe), from eye tracking data, and 2) a supervised classifier that, based on the models learned, performs the severity diagnosis. The classifier using the saliency maps was able to identify and separate the groups with an average accuracy of 88%. The most important features were the presence of face and skin color, in other words, semantic features. Â© 2021 IEEE.",0,,
10.1109/CBMS52027.2021.00070,2021,"Gatoula P., Dimas G., Iakovidis D.K., Koulaouzidis A.",Enhanced CNN-Based gaze estimation on wireless capsule endoscopy images,"Wireless capsule endoscopy (WCE) is a modality used for the non-invasive examination of the gastrointestinal (GI) tract. Physicians diagnose pathologies in images derived from Capsule Endoscopy (CE) using specific gaze patterns to observe pathologically related visual cues. Lately, deep learning has advanced in the domain of human eye-fixation estimation in natural images. However, the potentials of predicting the eye related patterns, such as eye fixations, in medical images has not been thoroughly investigated. In this work, we propose a CNN auto-encoder model, that is capable of predicting saliency maps estimating the gaze-patterns, in terms of eye-fixations, of physicians in CE images. The proposed model outperforms other approaches for visual saliency estimation based on physicians' eye fixation by providing an AUC-J of 0.726 among CE images depicting various pathological and normal cases. Â© 2021 IEEE.",0,,
10.3390/computers10060081,2021,"Planke L.J., Gardi A., Sabatini R., Kistan T., Ezer N.",Online multimodal inference of mental workload for cognitive human machine systems,"With increasingly higher levels of automation in aerospace decision support systems, it is imperative that the human operator maintains a high level of situational awareness in different operational conditions and a central role in the decision-making process. While current aerospace systems and interfaces are limited in their adaptability, a Cognitive Human Machine System (CHMS) aims to perform dynamic, real-time system adaptation by estimating the cognitive states of the human operator. Nevertheless, to reliably drive system adaptation of current and emerging aerospace systems, there is a need to accurately and repeatably estimate cognitive states, particularly for Mental Workload (MWL), in real-time. As part of this study, two sessions were performed during a Multi-Attribute Task Battery (MATB) scenario, including a session for offline calibration and validation and a session for online validation of eleven multimodal inference models of MWL. The multimodal inference model implemented included an Adaptive Neuro Fuzzy Inference System (ANFIS), which was used in different configurations to fuse data from an Electroencephalogram (EEG) modelâ€™s output, four eye activity features and a control input feature. The results from the online validation of the ANFIS models demonstrated that five of the ANFIS models (containing different feature combinations of eye activity and control input features) all demonstrated good results, while the best performing model (containing all four eye activity features and the control input feature) showed an average Mean Absolute Error (MAE) = 0.67 Â± 0.18 and Correlation Coefficient (CC) = 0.71 Â± 0.15. The remaining six ANFIS models included data from the EEG modelâ€™s output, which had an offset discrepancy. This resulted in an equivalent offset for the online multimodal fusion. Nonetheless, the efficacy of these ANFIS models could be seen with the pairwise correlation with the task level, where one model demonstrated a CC = 0.77 Â± 0.06, which was the highest among all the ANFIS models tested. Hence, this study demonstrates the ability for online multimodal fusion from features extracted from EEG signals, eye activity and control inputs to produce an accurate and repeatable inference of MWL. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",0,,
10.1016/j.eng.2020.08.027,2021,"Gu S., Wang L., He L., He X., Wang J.",Gaze Estimation via a Differential Eyesâ€?Appearances Network with a Reference Grid,"A person's eye gaze can effectively express that person's intentions. Thus, gaze estimation is an important approach in intelligent manufacturing to analyze a person's intentions. Many gaze estimation methods regress the direction of the gaze by analyzing images of the eyes, also known as eye patches. However, it is very difficult to construct a person-independent model that can estimate an accurate gaze direction for every person due to individual differences. In this paper, we hypothesize that the difference in the appearance of each of a person's eyes is related to the difference in the corresponding gaze directions. Based on this hypothesis, a differential eyesâ€?appearances network (DEANet) is trained on public datasets to predict the gaze differences of pairwise eye patches belonging to the same individual. Our proposed DEANet is based on a Siamese neural network (SNNet) framework which has two identical branches. A multi-stream architecture is fed into each branch of the SNNet. Both branches of the DEANet that share the same weights extract the features of the patches; then the features are concatenated to obtain the difference of the gaze directions. Once the differential gaze model is trained, a new person's gaze direction can be estimated when a few calibrated eye patches for that person are provided. Because person-specific calibrated eye patches are involved in the testing stage, the estimation accuracy is improved. Furthermore, the problem of requiring a large amount of data when training a person-specific model is effectively avoided. A reference grid strategy is also proposed in order to select a few references as some of the DEANet's inputs directly based on the estimation values, further thereby improving the estimation accuracy. Experiments on public datasets show that our proposed approach outperforms the state-of-the-art methods. Â© 2021 THE AUTHORS",1,,
10.1007/s40593-021-00244-4,2021,"Feng S., Law N.",Mapping Artificial Intelligence in Education Research: a Networkâ€based Keyword Analysis,"In this study, we review 1830 research articles on artificial intelligence in education (AIED), with the aim of providing a holistic picture of the knowledge evolution in this interdisciplinary research field from 2010 to 2019. A novel three-step approach in the analysis of the keyword co-occurrence networks (KCN) is proposed to identify the knowledge structure, knowledge clusters and trending keywords within AIED over time. The results reveal considerable research diversity in the AIED field, centering around two sustained themes: intelligent tutoring systems (2010-19) and massive open online courses (since 2014). The focal educational concerns reflected in AIED research are: (1) online learning; (2) game-based learning; (3) collaborative learning; (4) assessment; (5) affect; (6) engagement; and (7) learning design. The highly connected keywords relevant to analytic techniques within this field include natural language processing, educational data mining, learning analytics and machine learning. Neural network, deep learning, eye tracking, and personalized learning are trending keywords in this field as they have emerged with key structural roles in the latest two-year period analyzed. This is the first article providing a systematic review of a large body of literature on artificial intelligence in education, and in it we uncover the underlying patterns of knowledge connectivity within the field, as well as provide insight into its future development. The three-step multi-scale (macro, meso, micro) framework proposed in this study can also be applied to map the knowledge development in other scientific research areas. Â© 2021, International Artificial Intelligence in Education Society.",0,,
10.1007/s12193-020-00341-z,2021,"Volonte M., Anaraky R.G., Venkatakrishnan R., Venkatakrishnan R., Knijnenburg B.P., Duchowski A.T., Babu S.V.",Empirical evaluation and pathway modeling of visual attention to virtual humans in an appearance fidelity continuum,"In this contribution we studied how different rendering styles of a virtual human impacted usersâ€?visual attention in an interactive medical training simulator. In a mixed design experiment, 78 participants interacted with a virtual human representing a sample from the non-photorealistic (NPR) to the photorealistic (PR) rendering continuity. We presented five rendering style samples scenarios, namely All Pencil Shaded (APS), Pencil Shaded (PS), All Cartoon Shaded (ACT), Cartoon Shaded (CT), and Human-Like (HL), and compared how visual attention differed between groups of users. For this study, we employed an eye tracking system for collecting and analyzing usersâ€?gaze during interaction with the virtual human in a failure to rescue medical training simulation. Results shows that users spent more total time in the APS and ACT conditions but users visually attended more to virtual humans in the PS, CT and HL appearance conditions. Â© 2020, Springer Nature Switzerland AG.",0,,
10.1109/MIUCC52538.2021.9447658,2021,"Gowroju S., Aarti, Kumar S.",Robust Pupil Segmentation using UNET and Morphological Image Processing,"The current development in image processing towards biometrics systems has opened much research on realtime applications. The deep learning algorithms are added many expectations to the researchers. The main challenges of these applications are vulnerability towards training time, detection accuracy, and accurate segmentation. In addition to this, the visual noise among various biometric systems is the main challenge. In this paper, we deployed the CNN model using modified UNet to perform the segmentation. The proposed method uses noisy images from the MMU (Multi Media University Iris database) dataset. The acquired colored eye images from the dataset exhibit specular reflections, eye gaze, off-angle images with less resolution, and occlusions caused by eyelids and eyelashes. The focus of our work is mainly to perform accurate segmentation in less training time. Compared the existing methods that uses UNet architecture, with the proposed method, we achieved an accuracy of 91.7%. Â© 2021 IEEE.",0,,
10.1145/3450341.3458496,2021,"Solbach M.D., Tsotsos J.K.",Tracking Active Observers in 3D Visuo-Cognitive Tasks,"Most past and present research in computer vision involves passively observed data. Humans, however, are active observers in real life; they explore, search, select what and how to look. In this work, we present a psychophysical experimental setup for active, visual observation in a 3D world dubbed PESAO. The goal was to design PESAO for various active perception tasks with human subjects (active observers) capable of tracking the head and gaze. Â© 2021 Owner/Author.",0,,
10.1145/3450341.3458881,2021,"L. Miller H., Raphael Zurutuza I., Fears N., Polat S., Nielsen R.",Post-processing integration and semi-Automated analysis of eye-Tracking and motion-capture data obtained in immersive virtual reality environments to measure visuomotor integration,"Mobile eye-Tracking and motion-capture techniques yield rich, precisely quantifiable data that can inform our understanding of the relationship between visual and motor processes during task performance. However, these systems are rarely used in combination, in part because of the significant time and human resources required for post-processing and analysis. Recent advances in computer vision have opened the door for more efficient processing and analysis solutions. We developed a post-processing pipeline to integrate mobile eye-Tracking and full-body motion-capture data. These systems were used simultaneously to measure visuomotor integration in an immersive virtual environment. Our approach enables calculation of a 3D gaze vector that can be mapped to the participant's body position and objects in the virtual environment using a uniform coordinate system. This approach is generalizable to other configurations, and enables more efficient analysis of eye, head, and body movements together during visuomotor tasks administered in controlled, repeatable environments. Â© 2021 ACM.",0,,
10.1145/3450341.3460769,2021,"Mulvey F.B., Mikitovic M., Sadowski M., Hou B., Rasamoel N.D., Paulin Hansen J.P., BÃ¦kgaard P.",Gaze Interactive and Attention Aware Low Vision Aids as Future Smart Glasses,"We present a working paper on integrating eye tracking with mixed and augmented reality for the benefit of low vision aids. We outline the current state of the art and relevant research and point to further research and development required in order to adapt to individual user, environment, and current task. We outline key technical challenges and possible solutions including calibration, dealing with variant eye data quality, measuring and adapting image processing to low vision within current technical limitations, and outline an experimental approach to designing data-driven solutions using machine learning and artificial intelligence. Â© 2021 ACM.",2,,
10.1145/3450341.3458489,2021,"Love K., Velisar A., Shanidze N.","Eye, Robot: Calibration Challenges and Potential Solutions for Wearable Eye Tracking in Individuals with Eccentric Fixation","Loss of the central retina, including the fovea, can lead to a loss of visual acuity and oculomotor deficits, and thus have profound effects on day-To-day tasks. Recent advances in head-mounted, 3D eye tracking have allowed researchers to extend studies in this population to a broader set of daily tasks and more naturalistic behaviors and settings. However, decreases in fixational stability, multiple fixational loci and their uncertain role as oculomotor references, as well as eccentric fixation all provide additional challenges for calibration and collection of eye movement data. Here we quantify reductions in calibration accuracy relative to fixation eccentricity, and suggest a robotic calibration and validation tool that will allow for future developments of calibration and tracking algorithms designed with this population in mind. Â© 2021 ACM.",0,,
10.1145/3450341.3458494,2021,"Gibaldi A., Dutell V., Banks M.S.",Solving Parallax Error for 3D Eye Tracking,"Head-mounted eye-Trackers allow for unrestricted behavior in the natural environment, but have calibration issues that compromise accuracy and usability. A well-known problem arises from the fact that gaze measurements suffer from parallax error due to the offset between the scene camera origin and eye position. To compensate for this error two pieces of data are required: The pose of the scene camera in head coordinates, and the three-dimensional coordinates of the fixation point in head coordinates. We implemented a method that allows for effective and accurate eye-Tracking in the three-dimensional environment. Our approach consists of a calibration procedure that allows to contextually calibrate the eye-Tracker and compute the eyes pose in the reference frame of the scene camera, and a custom stereoscopic scene camera that provides the three-dimensional coordinates of the fixation point. The resulting gaze data are free from parallax error, allowing accurate and effective use of the eye-Tracker in the natural environment. Â© 2021 ACM.",2,,
10.1145/3450341.3458880,2021,"Stone S.A., Boser Q.A., Dawson T.R., Vette A.H., Hebert J.S., Pilarski P.M., Chapman C.S.",Sub-centimeter 3D gaze vector accuracy on real-world tasks: An investigation of eye and motion capture calibration routines,"Measuring where people look in real-world tasks has never been easier but analyzing the resulting data remains laborious. One solution integrates head-mounted eye tracking with motion capture but no best practice exists regarding what calibration data to collect. Here, we compared four âˆ? min calibration routines used to train linear regression gaze vector models and examined how the coordinate system, eye data used and location of fixation changed gaze vector accuracy on three trial types: calibration, validation (static fixation to task relevant locations), and task (naturally occurring fixations during object interaction). Impressively, predicted gaze vectors show âˆ? cm of error when looking straight ahead toward objects during natural arms-length interaction. This result was achieved predicting fixations in a Spherical coordinate frame, from the best monocular data, and, surprisingly, depends little on the calibration routine. Â© 2021 ACM.",2,,
10.1145/3448017.3457382,2021,Kurzhals K.,Image-Based Projection Labeling for Mobile Eye Tracking,"The annotation of gaze data concerning investigated areas of interest (AOIs) poses a time-consuming step in the analysis procedure of eye tracking experiments. For data from mobile eye tracking glasses, the annotation effort is further increased because each recording has to be investigated individually. Automated approaches based on supervised machine learning require pre-trained categories which are hard to obtain without human interpretation, i.e., labeling ground truth data. We present an interactive visualization approach that supports efficient annotation of gaze data based on image content participants with eye tracking glasses focused on. Recordings can be segmented individually to reduce the annotation effort. Thumbnails represent segments visually and are projected on a 2D plane for a fast comparison of AOIs. Annotated scanpaths can then be interpreted directly with the timeline visualization. We showcase our approach with three different scenarios. Â© 2021 ACM.",,,
10.1145/3448017.3457377,2021,"Burch M., Wallner G., Broeks N., Piree L., Boonstra N., Vlaswinkel P., Franken S., Van Wijk V.",The Power of Linked Eye Movement Data Visualizations,"In this paper we showcase several eye movement data visualizations and how they can be interactively linked to design a flexible visualization tool for eye movement data. The aim of this project is to create a user-friendly and easy accessible tool to interpret visual attention patterns and to facilitate data analysis for eye movement data. Hence, to increase accessibility and usability we provide a web-based solution. Users can upload their own eye movement data set and inspect it from several perspectives simultaneously. Insights can be shared and collaboratively be discussed with others. The currently available visualization techniques are a 2D density plot, a scanpath representation, a bee swarm, and a scarf plot, all supporting several standard interaction techniques. Moreover, due to the linking feature, users can select data in one visualization, and the same data points will be highlighted in all active visualizations for solving comparison tasks. The tool also provides functions that make it possible to upload both, private or public data sets, and can generate URLs to share the data and settings of customized visualizations. A user study showed that the tool is understandable and that providing linked customizable views is beneficial for analyzing eye movement data. Â© 2021 ACM.",,,
10.1145/3448017.3457384,2021,"Abdrabou Y., Shams A., Mantawy M.O., Ahmad Khan A., Khamis M., Alt F., Abdelrahman Y.",GazeMeter: Exploring the Usage of Gaze Behaviour to Enhance Password Assessments,"We investigate the use of gaze behaviour as a means to assess password strength as perceived by users. We contribute to the effort of making users choose passwords that are robust against guessing-attacks. Our particular idea is to consider also the users' understanding of password strength in security mechanisms. We demonstrate how eye tracking can enable this: by analysing people's gaze behaviour during password creation, its strength can be determined. To demonstrate the feasibility of this approach, we present a proof of concept study (N = 15) in which we asked participants to create weak and strong passwords. Our findings reveal that it is possible to estimate password strength from gaze behaviour with an accuracy of 86% using Machine Learning. Thus, we enable research on novel interfaces that consider users' understanding with the ultimate goal of making users choose stronger passwords. Â© 2021 ACM.",,,
10.1145/3448018.3457996,2021,"Emery K.J., Zannoli M., Warren J., Xiao L., Talathi S.S.","OpenNEEDS: A Dataset of Gaze, Head, Hand, and Scene Signals during Exploration in Open-Ended VR Environments","We present OpenNEEDS, the first large-scale, high frame rate, comprehensive, and open-source dataset of Non-Eye (head, hand, and scene) and Eye (3D gaze vectors) data captured for 44 participants as they freely explored two virtual environments with many potential tasks (i.e., reading, drawing, shooting, object manipulation, etc.). With this dataset, we aim to enable research on the relationship between head, hand, scene, and gaze spatiotemporal statistics and its applications to gaze estimation. To demonstrate the power of OpenNEEDS, we show that gaze estimation models using individual non-eye sensors and an early fusion model combining all non-eye sensors outperform all baseline gaze estimation models considered, suggesting the possibility of considering non-eye sensors in the design of robust eye trackers. We anticipate that this dataset will support research progress in many areas and applications such as gaze estimation and prediction, sensor fusion, human-computer interaction, intent prediction, perceptuo-motor control, and machine learning. Â© 2021 ACM.",,,
10.1145/3448018.3458009,2021,"Chaudhary A.K., Gyawali P.K., Wang L., Pelz J.B.",Semi-Supervised Learning for Eye Image Segmentation,"Recent advances in appearance-based models have shown improved eye tracking performance in difficult scenarios like occlusion due to eyelashes, eyelids or camera placement, and environmental reflections on the cornea and glasses. The key reason for the improvement is the accurate and robust identification of eye parts (pupil, iris, and sclera regions). The improved accuracy often comes at the cost of labeling an enormous dataset, which is complex and time-consuming. This work presents two semi-supervised learning frameworks to identify eye-parts by taking advantage of unlabeled images where labeled datasets are scarce. With these frameworks, leveraging the domain-specific augmentation and novel spatially varying transformations for image segmentation, we show improved performance on various test cases with limited labeled samples. For instance, for a model trained on just 4 and 48 labeled images, these frameworks improved by at least 4.7% and 0.4% respectively, in segmentation performance over the baseline model, which is trained only with the labeled dataset. Â© 2021 ACM.",,,
10.1145/3448018.3457997,2021,"Prinzler M.H.U., SchrÃ¶der C., Al Zaidawi S.M.K., Zachmann G., Maneth S.",Visualizing Prediction Correctness of Eye Tracking Classifiers,"Eye tracking data is often used to train machine learning algorithms for classification tasks. The main indicator of performance for such classifiers is typically their prediction accuracy. However, this number does not reveal any information about the specific intrinsic workings of the classifier. In this paper we introduce novel visualization methods which are able to provide such information. We introduce the Prediction Correctness Value (PCV). It is the difference between the calculated probability for the correct class and the maximum calculated probability for any other class. Based on the PCV we present two visualizations: (1) coloring segments of eye tracking trajectories according to their PCV, thus indicating how beneficial certain parts are towards correct classification, and (2) overlaying similar information for all participants to produce a heatmap that indicates at which places fixations are particularly beneficial towards correct classification. Using these new visualizations we compare the performance of two classifiers (RF and RBFN). Â© 2021 ACM.",,,
10.1145/3448018.3458014,2021,"Kastrati A., Plomecka M.B., Wattenhofer R., Langer N.",Using Deep Learning to Classify Saccade Direction from Brain Activity,"We present first insights into our project that aims to develop an Electroencephalography (EEG) based Eye-Tracker. Our approach is tested and validated on a large dataset of simultaneously recorded EEG and infrared video-based Eye-Tracking, serving as ground truth. We compared several state-of-the-art neural network architectures for time series classification: InceptionTime, EEGNet, and investigated other architectures such as convolutional neural networks (CNN) with Xception modules and Pyramidal CNN. We prepared and tested these architectures with our rich dataset and obtained a remarkable accuracy of the left/right saccades direction classification (94.8 %) for the InceptionTime network, after hyperparameter tuning. Â© 2021 Owner/Author.",,,
10.1145/3448018.3457993,2021,"KÃ¼bler T.C., Fuhl W., Wagner E., Kasneci E.",55 Rides: Attention annotated head and gaze data during naturalistic driving,"Trained eye patterns are essential for safe driving. Whether for exploration of the surrounding traffic or to make sure that a lane is clear through a shoulder check - quick and effective perception is the key to driving safety. Surprisingly though, free and open access data on gaze behavior during driving are yet extremely sparse. The environment inside a vehicle is challenging for eye-tracking technology due to rapidly changing illumination conditions, such as exiting a tunnel to brightest sunlight, proper calibration and safety. So far, available data exhibits environments that likely influence the viewing behavior, sometimes dramatically (e.g., driving simulators without mirrors, limited field of view). We propose crowd-sourced eye-tracking data collected during real-world driving using NIR-cameras and illuminators that were placed within the driver's cabin. We analyze this data using a deep learning appearance-based gaze estimation, with raw videos not being part of the data set due to legal restrictions. Our data set contains four different drivers in their habitual cars and 55 rides of an average of 30 minutes length. At least three human raters rated each ride continuously with regard to driver attention and vigilance level on a ten-point scale. From the recorded videos we extracted drivers' head and eye movements as well as eye opening angle. For this data, we apply a normalization with respect to different placement of the driver monitoring camera and demonstrate a baseline for driver attention monitoring based on eye gaze and head movement features. Â© 2021 ACM.",,,
10.1145/3448018.3459654,2021,"Lu W., He H., Urban A., Griffin J.",What the Eyes Can Tell: Analyzing Visual Attention with an Educational Video Game,"3D video games show potential as educational tools that improve learner engagement. Integrating 3D games into school curricula, however, faces various challenges. One challenge is providing visualizations on learning dashboards for instructors. Such dashboards provide needed information so that instructors may conduct timely and appropriate interventions when students need it. Another challenge is identifying contributive learning predictors for a computational model, which can be the core algorithm used to make games more intelligent for tutoring and assessment purposes. Previous studies have found that students' visual-attention is a vital aspect of engagement during gameplay. However, few studies have examined whether attention visualization patterns can distinguish students from different performance groups. Complicating this research is the relatively nascent investigation into gaze metrics for learning-prediction models. In this exploratory study, we used eye-tracking data from an educational game, Mission HydroSci, to examine visual-attention pattern differences between low and high performers and how their self-reported demographics affect such patterns. Results showed different visual-attention patterns between low and high performers. Additionally, self-reported science, gaming, and navigational expertise levels were significantly correlated to several gaze metric features. Â© 2021 ACM.",,,
10.1145/3448018.3458004,2021,"Fuhl W., Kasneci E.",A Multimodal Eye Movement Dataset and a Multimodal Eye Movement Segmentation Analysis,"We present a new dataset with annotated eye movements. The dataset consists of over 800,000 gaze points recorded during a car ride in the real world and in the simulator. In total, the eye movements of 19 subjects were annotated. In this dataset, there are several data sources including the eyelid closure, the pupil center, the optical vector, and a vector into the pupil center starting from the center of the eye corners. These different data sources are analyzed and evaluated individually as well as in combination with respect to their suitability for eye movement classification. These results will help developers of real-time systems and algorithms to find the best data sources for their application. Also, new algorithms can be trained and evaluated on this data set. Link to code and dataset https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FA%20Multimodal%20Eye%20Movement%20Dataset%20and%20...mode=list Â© 2021 ACM.",,,
10.1016/j.neucom.2021.01.057,2021,"Yan Y., Guo X., Tang J., Li C., Wang X.",Learning spatio-temporal correlation filter for visual tracking,"Correlation filter (CF) trackers have performed impressive performance with high frame rates. However, the limited information in both spatial and temporal domains is only used in the learning of correlation filters, which might limit the tracking performance. To handle this problem, we propose a novel spatio-temporal correlation filter approach, which employs both spatial and temporal cues in the learning, for visual tracking. In particular, we explore the spatial contexts from background whose contents are ambiguous to the target and integrate them into the correlation filter model for more discriminative learning. Moreover, to capture the appearance variations in temporal domain, we also compute a set of target templates and incorporate them into our model. At the same time, the solution of the proposed spatio-temporal correlation filter is closed-form and the tracking efficiency is thus guaranteed. Experimental experiments on benchmark datasets demonstrate the effectiveness of the proposed tracker against several CF ones. Â© 2021 Elsevier B.V.",,,
10.1109/AIIoT52608.2021.9454176,2021,"Tamim H.M., Sultana F., Tasneem N., Marzan Y., Khan M.M.",Class Insight: A Student Monitoring System with Real-time Updates using Face Detection and Eye Tracking,"The student monitoring system represents a detailed description of Class Insight. It explains the purpose and the features of the system. It also interprets the interfaces, the working procedures of the system, the constraints under which it will operate and how the system will react to external stimuli. This is a machine learning-based student monitoring system that allows teachers to submit an assessment to students completely paperless. It provides tools for teachers and students to keep track of their assignments, reading materials and other tasks. The application will keep track of the students' face and eye while reading and will update progresses instantly. As a result, instructors can track real-time updates of the tasks. They will also be notified whether it is the student's face or not and how much time they spent on a single page of the reading materials. This will be generated as a report. Â© 2021 IEEE.",,,
10.1145/3411763.3451679,2021,"Weber T., Winiker C., Hussmann H.",A Closer Look at Machine Learning Code,"Software using Machine Learning algorithms is becoming ever more ubiquitous making it equally important to have good development processes and practices. Whether we can apply insights from software development research remains open though, since it is not yet clear, whether data-driven development has the same requirements as its traditional counterpart. We used eye tracking to investigate whether the code reading behaviour of developers differs between code that uses Machine Learning and code that does not. Our data shows that there are differences in what parts of the code people consider of interest and how they read it. This is a consequence of differences in both syntax and semantics of the code. This reading behaviour already shows that we cannot take existing solutions as universally applicable. In the future, methods that support Machine Learning must iterate on existing knowledge to meet the challenges of data-driven development. Â© 2021 ACM.",,,
10.1145/3411763.3451658,2021,"Heck M., Edinger J., Becker C.",Conditioning Gaze-Contingent Systems for the Real World: Insights from a Field Study in the Fast Food Industry,"Eye tracking can be used to infer what is relevant to a user, and adapt the content and appearance of an application to support the user in their current task. A prerequisite for integrating such adaptive user interfaces into public terminals is robust gaze estimation. Commercial eye trackers are highly accurate, but require prior person-specific calibration and a relatively stable head position. In this paper, we collect data from 26 authentic customers of a fast food restaurant while interacting with a total of 120 products on a self-order terminal. From our observations during the experiment and a qualitative analysis of the collected gaze data, we derive best practice approaches regarding the integration of eye tracking software into self-service systems. We evaluate several implicit calibration strategies that derive the user's true focus of attention either from the context of the user interface, or from their interaction with the system. Our results show that the original gaze estimates can be visibly improved by taking into account both contextual and interaction-based information. Â© 2021 ACM.",,,
10.1145/3411763.3451546,2021,"Sasikumar P., Collins M., Bai H., Billinghurst M.",XRTB: A Cross Reality Teleconference Bridge to incorporate 3D interactivity to 2D Teleconferencing,"We present XRTeleBridge (XRTB), an application that integrates a Mixed Reality (MR) interface into existing teleconferencing solutions like Zoom. Unlike conventional webcam, XRTB provides a window into the virtual world to demonstrate and visualize content. Participants can join via webcam or via head mounted display (HMD) in a Virtual Reality (VR) environment. It enables users to embody 3D avatars with natural gestures and eye gaze. A camera in the virtual environment operates as a video feed to the teleconferencing software. An interface resembling a tablet mirrors the teleconferencing window inside the virtual environment, thus enabling the participant in the VR environment to see the webcam participants in real-time. This allows the presenter to view and interact with other participants seamlessly. To demonstrate the system's functionalities, we created a virtual chemistry lab environment and presented an example lesson using the virtual space and virtual objects and effects. Â© 2021 Owner/Author.",,,
10.1145/3411764.3445627,2021,"W hler L., Zembaty M.",Towards understanding perceptual diferences between genuine and face-swapped videos,"In this paper, we report on perceptual experiments indicating that there are distinct and quantitatively measurable diferences in the way we visually perceive genuine versus face-swapped videos. Recent progress in deep learning has made face-swapping techniques a powerful tool for creative purposes, but also a means for unethical forgeries. Currently, it remains unclear why people are misled, and which indicators they use to recognize potential manipulations. Here, we conduct three perceptual experiments focusing on a wide range of aspects: the conspicuousness of artifacts, the viewing behavior using eye tracking, the recognition accuracy for diferent video lengths, and the assessment of emotions. Our experiments show that responses difer distinctly when watching manipulated as opposed to original faces, from which we derive perceptual cues to recognize face swaps. By investigating physiologically measurable signals, our fndings yield valuable insights that may also be useful for advanced algorithmic detection. Â© 2021 ACM.",,,
10.1145/3411764.3445697,2021,"Sidenmark L., Potts D.",Radi-eye: Hands-free radial interfaces for 3d interaction using gaze-activated head-crossing,"Eye gaze and head movement are attractive for hands-free 3D interaction in head-mounted displays, but existing interfaces aford only limited control. Radi-Eye is a novel pop-up radial interface designed to maximise expressiveness with input from only the eyes and head. Radi-Eye provides widgets for discrete and continuous input and scales to support larger feature sets. Widgets can be selected with Look and Cross, using gaze for pre-selection followed by head-crossing as trigger and for manipulation. The technique leverages natural eye-head coordination where eye and head move at an ofset unless explicitly brought into alignment, enabling interaction without risk of unintended input. We explore Radi-Eye in three augmented and virtual reality applications, and evaluate the efect of radial interface scale and orientation on performance with Look and Cross. The results show that Radi-Eye provides users with fast and accurate input while opening up a new design space for hands-free fuid interaction. Â© 2021 ACM.",,,
10.1145/3411764.3445711,2021,"Ahuja K., Shah D., Pareddy S., Xhakaj F.",Classroom digital twins with instrumentation-free gaze tracking,"Classroom sensing is an important and active area of research with great potential to improve instruction. Complementing professional observers - the current best practice - automated pedagogical professional development systems can attend every class and capture fne-grained details of all occupants. One particularly valuable facet to capture is class gaze behavior. For students, certain gaze patterns have been shown to correlate with interest in the material, while for instructors, student-centered gaze patterns have been shown to increase approachability and immediacy. Unfortunately, prior classroom gaze-sensing systems have limited accuracy and often require specialized external or worn sensors. In this work, we developed a new computer-vision-driven system that powers a 3D digital twin of the classroom and enables whole-class, 6DOF head gaze vector estimation without instrumenting any of the occupants. We describe our open source implementation, and results from both controlled studies and real-world classroom deployments. Â© 2021 ACM.",,,
10.1109/CSCWD49262.2021.9437692,2021,"Li F., Lee C.-H., Feng S., Trappey A., Gilani F.",Prospective on Eye-Tracking-based Studies in Immersive Virtual Reality,"The current virtual reality (VR) techniques develop immersive environments via inducing illusions to our sense. Nowadays, most of VR focuses on inducing visual illusion. Hence, visual is the most important input channel for experiencing and exploring the VR environments. Recently, extensive research efforts have been put on eye-tracking studies. However, the development and growing trends of the VR-based eye-tracking studies are unrevealed due to the lack of a systematic literature review on it. In this study, we reviewed related literature from 2000 to 2019 and summarized them into two main categories, including eye tracking methods and eye-tracking-enabled applications, such as tracking gaze points to manipulate the VR environment, measuring user states, and evaluating the usability of VR based on eye-tracking data. Based on the literature review, we can find that eye-tracking can assist in developing adaptive VR systems and enhance users experience. While comparing with 2D environments, immersive VR environment still requires more deep studies in eye-tracking. Â© 2021 IEEE.",,,
10.3390/app11104399,2021,"Moghaddasi M., MarÃ­n-Morales J., Khatri J., Guixeres J., Chicchi Giglioli I.A., AlcaÃ±iz M.",Recognition of customersâ€?impulsivity from behavioral patterns in virtual reality,"Virtual reality (VR) in retailing (V-commerce) has been proven to enhance the consumer ex-perience. Thus, this technology is beneficial to study behavioral patterns by offering the opportunity to infer customersâ€?personality traits based on their behavior. This study aims to recognize impulsivity using behavioral patterns. For this goal, 60 subjects performed three tasksâ€”one exploration task and two planned tasksâ€”in a virtual market. Four noninvasive signals (eye-tracking, navigation, posture, and interactions), which are available in commercial VR devices, were recorded, and a set of features were extracted and categorized into zonal, general, kinematic, temporal, and spatial types. They were input into a support vector machine classifier to recognize the impulsivity of the subjects based on the I-8 questionnaire, achieving an accuracy of 87%. The results suggest that, while the exploration task can reveal general impulsivity, other subscales such as perseverance and sensation-seeking are more related to planned tasks. The results also show that posture and interaction are the most informative signals. Our findings validate the recognition of customer impulsivity using sensors incorporated into commercial VR devices. Such information can provide a personalized shopping experience in future virtual shops. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.3390/app11104366,2021,"Kang D., Chang H.S.",Low-complexity pupil tracking for sunglasses-wearing faces for glasses-free 3d huds,"This study proposes a pupil-tracking method applicable to drivers both with and without sunglasses on, which has greater compatibility with augmented reality (AR) three-dimensional (3D) head-up displays (HUDs). Performing real-time pupil localization and tracking is complicated by drivers wearing facial accessories such as masks, caps, or sunglasses. The proposed method fulfills two key requirements: low complexity and algorithm performance. Our system assesses both bare and sunglasses-wearing faces by first classifying images according to these modes and then assigning the appropriate eye tracker. For bare faces with unobstructed eyes, we applied our previous regression-algorithm-based method that uses scale-invariant feature transform features. For eyes occluded by sunglasses, we propose an eye position estimation method: our eye tracker uses nonoccluded face area tracking and a supervised regression-based pupil position estimation method to locate pupil centers. Experiments showed that the proposed method achieved high accuracy and speed, with a precision error of <10 mm in <5 ms for bare and sunglasses-wearing faces for both a 2.5 GHz CPU and a commercial 2.0 GHz CPU vehicle-embedded system. Coupled with its performance, the low CPU consumption (10%) demonstrated by the proposed algorithm highlights its promise for implementation in AR 3D HUD systems. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/ICPC52881.2021.00025,2021,"Al Madi N., Peterson C.S., Sharif B., Maletic J.I.",From Novice to Expert: Analysis of Token Level Effects in a Longitudinal Eye Tracking Study,Program comprehension is a vital skill in software development. This work investigates program comprehension by examining the eye movement of novice programmers as they gain programming experience over the duration of a Java course. Their eye movement behavior is compared to the eye movement of expert programmers. Eye movement studies of natural text show that word frequency and length influence eye movement duration and act as indicators of reading skill. The study uses an existing longitudinal eye tracking dataset with 20 novice and experienced readers of source code. The work investigates the acquisition of the effects of token frequency and token length in source code reading as an indication of program reading skill. The results show evidence of the frequency and length effects in reading source code and the acquisition of these effects by novices. These results are then leveraged in a machine learning model demonstrating how eye movement can be used to estimate programming proficiency and classify novices from experts with 72% accuracy. Â© 2021 IEEE.,,,
10.3390/jimaging7050083,2021,"Elbattah M., Loughnane C., GuÃ©rin J.-L., Carette R., Cilia F., Dequen G.",Variational autoencoder for image-based augmentation of eye-tracking data,"Over the past decade, deep learning has achieved unprecedented successes in a diversity of application domains, given large-scale datasets. However, particular domains, such as healthcare, inherently suffer from data paucity and imbalance. Moreover, datasets could be largely inaccessible due to privacy concerns, or lack of data-sharing incentives. Such challenges have attached significance to the application of generative modeling and data augmentation in that domain. In this context, this study explores a machine learning-based approach for generating synthetic eye-tracking data. We explore a novel application of variational autoencoders (VAEs) in this regard. More specifically, a VAE model is trained to generate an image-based representation of the eye-tracking output, so-called scanpaths. Overall, our results validate that the VAE model could generate a plausible output from a limited dataset. Finally, it is empirically demonstrated that such approach could be employed as a mechanism for data augmentation to improve the performance in classification tasks. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/TVCG.2021.3067784,2021,"Angelopoulos A.N., Martel J.N.P., Kohli A.P., Conradt J., Wetzstein G.","Event-Based Near-Eye Gaze Tracking beyond 10,000 Hz","The cameras in modern gaze-tracking systems suffer from fundamental bandwidth and power limitations, constraining data acquisition speed to 300 Hz realistically. This obstructs the use of mobile eye trackers to perform, e.g., low latency predictive rendering, or to study quick and subtle eye motions like microsaccades using head-mounted devices in the wild. Here, we propose a hybrid frame-event-based near-eye gaze tracking system offering update rates beyond 10,000 Hz with an accuracy that matches that of high-end desktop-mounted commercial trackers when evaluated in the same conditions. Our system, previewed in Figure 1, builds on emerging event cameras that simultaneously acquire regularly sampled frames and adaptively sampled events. We develop an online 2D pupil fitting method that updates a parametric model every one or few events. Moreover, we propose a polynomial regressor for estimating the point of gaze from the parametric pupil model in real time. Using the first event-based gaze dataset, we demonstrate that our system achieves accuracies of 0.45Â°-1.75Â° for fields of view from 45Â° to 98Â°. With this technology, we hope to enable a new generation of ultra-low-latency gaze-contingent rendering and display techniques for virtual and augmented reality. Â© 1995-2012 IEEE.",,,
10.1109/TVCG.2021.3067779,2021,"Hu Z., Bulling A., Li S., Wang G.",FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments,"Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction. However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications. We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments. Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities. Based on this analysis, we propose FixationNet - a novel learning-based model to forecast users' eye fixations in the near future in VR. We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93Â° to 2.35Â°) in free-viewing and of 15.1% (from 2.05Â° to 1.74Â°) in task-oriented situations. As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research. Â© 1995-2012 IEEE.",,,
10.1016/j.image.2021.116198,2021,"Liaqat S., Wu C., Duggirala P.R., Cheung S.-C.S., Chuah C.-N., Ozonoff S., Young G.",Predicting ASD diagnosis in children with synthetic and image-based eye gaze data,"As early intervention is highly effective for young children with autism spectrum disorder (ASD), it is imperative to make accurate diagnosis as early as possible. ASD has often been associated with atypical visual attention and eye gaze data can be collected at a very early age. An automatic screening tool based on eye gaze data that could identify ASD risk offers the opportunity for intervention before the full set of symptoms is present. In this paper, we propose two machine learning methods, synthetic saccade approach and image based approach, to automatically classify ASD given children's eye gaze data collected from free-viewing tasks of natural images. The first approach uses a generative model of synthetic saccade patterns to represent the baseline scan-path from a typical non-ASD individual and combines it with the real scan-path as well as other auxiliary data as inputs to a deep learning classifier. The second approach adopts a more holistic image-based approach by feeding the input image and a sequence of fixation maps into a convolutional or recurrent neural network. Using a publicly-accessible collection of children's gaze data, our experiments indicate that the ASD prediction accuracy reaches 67.23% accuracy on the validation dataset and 62.13% accuracy on the test dataset. Â© 2021 Elsevier B.V.",,,
10.1016/j.image.2021.116184,2021,"Mazumdar P., Arru G., Battisti F.",Early detection of children with Autism Spectrum Disorder based on visual exploration of images,"Autism Spectrum Disorder is a developmental disorder characterized by a deficit in social behaviour and specific interactions such as reduced eye contact and body gestures. Recent advancements in software and hardware multimedia technologies provide the tools for early detecting the presence of this disorder. In this paper we present an approach based on the combined use of machine learning and eye tracking information. More specifically, features are extracted from image content and viewing behaviour, such as the presence of objects and fixations towards the centre of a scene. Those features are used to train a machine learning-based classifier. The obtained results show that the considered features allow to identify children affected by autism spectrum disorder and typically developing ones. Â© 2021 Elsevier B.V.",,,
10.1145/3448139.3448156,2021,"Diederich M., Kang J., Kim T., Lindgren R.",Developing an in-application shared view metric to capture collaborative learning in a multi-platform astronomy simulation,"There has been recent interest in the design of collaborative learning activities that are distributed across multiple technology devices for students to engage in scientific inquiry. Emerging research has begun to investigate students' collaborative behaviors across different device types and students' shared attention by tracking eye gaze, body posture, and their interactions with the digital environment. Using a 3D astronomy simulation that leverages a VR headset and tablet computers, this paper builds on the ideas described in eye-gaze studies by developing and implementing a metric of shared viewing across multiple devices. Preliminary findings suggest that a higher level of shared view could be related to increased conceptual discussion, as well as point to an early-stage pattern of behavior of decreased SV to prompt facilitator intervention to refocus collaborative efforts. We hope this metric will be a promising first step in further understanding and assessing the quality of collaboration across multiple device platforms in a single shared space. This paper provides an in depth look at a highly exploratory stage of a broader research trajectory to establish a robust, effective way to track screen views, including providing resources to teachers when students engage in similar learning environments, and providing insight from log data to understand how students effectively collaborate. Â© 2021 ACM.",,,
10.1145/3448139.3448201,2021,"Hassan J., Leong J., Schneider B.",Multimodal data collection made easy: The EZ-MMLA toolkit: A data collection website that provides educators and researchers with easy access to multimodal data streams.,"While Multimodal Learning Analytics (MMLA) is becoming a popular methodology in the LAK community, most educational researchers still rely on traditional instruments for capturing learning processes (e.g., click-stream, log data, self-reports, qualitative observations). MMLA has the potential to complement and enrich traditional measures of learning by providing high frequency data on learners' behavior, cognition and affects. However, there is currently no easy-to-use toolkit for recording multimodal data streams. Existing methodologies rely on the use of physical sensors and custom-written code for accessing sensor data. In this paper, we present the EZ-MMLA toolkit. This toolkit was implemented as a website that provides easy access to the latest machine learning algorithms for collecting a variety of data streams from webcams: attention (eye-tracking), physiological states (heart rate), body posture (skeletal data), hand gestures, emotions (from facial expressions and speech), and lower-level computer vision algorithms (e.g., fiducial / color tracking). This toolkit can run from any browser and does not require special hardware or programming experience. We compare this toolkit with traditional methods and describe a case study where the EZ-MMLA toolkit was used in a classroom context. We conclude by discussing other applications of this toolkit, potential limitations, and future steps. Â© 2021 ACM.",,,
10.1109/I2CT51068.2021.9417950,2021,"Hari Krishna S.M., Pradyumna G., Aishwarya B., Gayathri C.",Development of Personal Identification Number Authorization Algorithm Using Real- Time Eye Tracking Dynamic Keypad Generation,"The digital financial transaction is on a continuous rise and going to be the order of the day. These transactions rely on the entry of the Personal Identification Number (PIN) by the user. The PIN is a common user authentication method for many applications such as ATM's, unlocking personal devices. Cyber-crimes are committed by shoulder surfing or thermal tracking. PIN entry is found vulnerable to password attacks such as shoulder surfing or thermal tracking. Intruders try to gain passwords or personal identification numbers (PIN) by glancing over the user's shoulder and observing the pattern of PIN entry (shoulder surfing). Thermal tracking is another method followed by cyber thieves, using the heat traces to decode the entered PIN by the user. This paper proposes a novel approach to authorization of PIN. To demonstrate the same, a web application is developed with trio-based authentications using machine learning techniques. Initially, the application detects and recognizes the user's face. A dynamic keypad is displayed which prompts the user to provide input PIN via eye blink. User's eye is detected and monitored by the application in order to capture the PIN and verifies the same with the existing PIN in the database. On a successful PIN verification process, the application allows the user to proceed with the transaction. The results show the proposed novel approach is better than existing approaches. Â© 2021 IEEE.",,,
10.1111/cogs.12977,2021,"Annerer-Walcher S., Ceh S.M., Putze F., Kampen M., KÃ¶rner C., Benedek M.",How Reliably Do Eye Parameters Indicate Internal Versus External Attentional Focus?,"Eye behavior is increasingly used as an indicator of internal versus external focus of attention both in research and application. However, available findings are partly inconsistent, which might be attributed to the different nature of the employed types of internal and external cognition tasks. The present study, therefore, investigated how consistently different eye parameters respond to internal versus external attentional focus across three task modalities: numerical, verbal, and visuo-spatial. Three eye parameters robustly differentiated between internal and external attentional focus across all tasks. Blinks, pupil diameter variance, and fixation disparity variance were consistently increased during internally directed attention. We also observed substantial attentional focus effects on other parameters (pupil diameter, fixation disparity, saccades, and microsaccades), but they were moderated by task type. Single-trial analysis of our data using machine learning techniques further confirmed our results: Classifying the focus of attention by means of eye tracking works well across participants, but generalizing across tasks proves to be challenging. Based on the effects of task type on eye parameters, we discuss what eye parameters are best suited as indicators of internal versus external attentional focus in different settings. Â© 2021 The Authors. Cognitive Science published by Wiley Periodicals LLC on behalf of Cognitive Science Society (CSS).",,,
10.1109/LRA.2021.3062003,2021,"Zhang K., Liu H., Fan Z., Chen X., Leng Y., De Silva C.W., Fu C.",Foot Placement Prediction for Assistive Walking by Fusing Sequential 3D Gaze and Environmental Context,"Predicting the locomotion intent of humans is important for controlling assistive robots. Previous studies have investigated assistive walking on structured terrains, but only a few studies have considered rough terrains. Human intent on rough terrains is more difficult to predict because there is a transition at every step. To predict the foot placements of humans on rough terrains, the present paper fuses sequential 3D gaze and the environmental context. The 3D gaze is assumed to be the intersection point of the line of sight as measured by an eye-tracker and the environmental point cloud as measured by an RGBD camera. The sequential 3D gaze and the environmental context are fused based on an RGBD SLAM algorithm. Then the segmented terrain that is closest to the center of sequential 3D gaze is regarded as the most possible foothold area at the next step. Six able-bodied subjects are invited to walk randomly on rough terrains. Their foot placements are labeled and compared with the predicted foot placements. Experimental results show that the proposed method can predict the foot placements of all subjects 0.5 step ahead. With environmental context and user-dependent time window, the distance error of predicting the foot placements can decrease to 0.086 m. Hence, gaze, environmental context, and time window are all important in predicting the human intent when navigating rough terrains. Â© 2016 IEEE.",,,
10.1016/j.cag.2021.01.001,2021,"Pfeuffer K., Abdrabou Y., Esteves A., Rivu R., Abdelrahman Y., Meitner S., Saadi A., Alt F.",ARtention: A design space for gaze-adaptive user interfaces in augmented reality,"Augmented Reality (AR) headsets extended with eye-tracking, a promising input technology for its natural and implicit nature, open a wide range of new interaction capabilities for everyday use. In this paper we present ARtention, a design space for gaze interaction specifically tailored for in-situ AR information interfaces. It highlights three important dimensions to consider in the UI design of such gaze-enabled applications: transitions from reality to the virtual interface, from single- to multi-layer content, and from information consumption to selection tasks. Such transitional aspects bring previously isolated gaze interaction concepts together to form a unified AR space, enabling more advanced application control seamlessly mediated by gaze. We describe these factors in detail. To illustrate how the design space can be used, we present three prototype applications and report informal user feedback obtained from different scenarios: a conversational UI, viewing a 3D visualization, and browsing items for shopping. We conclude with design considerations derived from our development and evaluation of the prototypes. We expect these to be valuable for researchers and designers investigating the use of gaze input in AR systems and applications. Â© 2021 Elsevier Ltd",,,
10.1016/j.media.2020.101946,2021,"Pelanis E., Teatini A., Eigl B., Regensburger A., Alzaga A., Kumar R.P., Rudolph T., Aghayan D.L., Riediger C., KvarnstrÃ¶m N., Elle O.J., Edwin B.",Evaluation of a novel navigation platform for laparoscopic liver surgery with organ deformation compensation using injected fiducials,"In laparoscopic liver resection, surgeons conventionally rely on anatomical landmarks detected through a laparoscope, preoperative volumetric images and laparoscopic ultrasound to compensate for the challenges of minimally invasive access. Image guidance using optical tracking and registration procedures is a promising tool, although often undermined by its inaccuracy. This study evaluates a novel surgical navigation solution that can compensate for liver deformations using an accurate and effective registration method. The proposed solution relies on a robotic C-arm to perform registration to preoperative CT/MRI image data and allows for intraoperative updates during resection using fluoroscopic images. Navigation is offered both as a 3D liver model with real-time instrument visualization, as well as an augmented reality overlay on the laparoscope camera view. Testing was conducted through a pre-clinical trial which included four porcine models. Accuracy of the navigation system was measured through two evaluation methods: liver surface fiducials reprojection and a comparison between planned and navigated resection margins. Target Registration Error with the fiducials evaluation shows that the accuracy in the vicinity of the lesion was 3.78Â±1.89 mm. Resection margin evaluations resulted in an overall median accuracy of 4.44 mm with a maximum error of 9.75 mm over the four subjects. The presented solution is accurate enough to be potentially clinically beneficial for surgical guidance in laparoscopic liver surgery. Â© 2020",,,
10.1109/LRA.2020.3043167,2021,"Kratzer P., Bihlmaier S., Midlagajni N.B., Prakash R., Toussaint M., Mainprice J.",MoGaze: A Dataset of Full-Body Motions that Includes Workspace Geometry and Eye-Gaze,"As robots become more present in open human environments, it will become crucial for robotic systems to understand and predict human motion. Such capabilities depend heavily on the quality and availability of motion capture data. However, existing datasets of full-body motion rarely include 1) long sequences of manipulation tasks, 2) the 3D model of the workspace geometry, and 3) eye-gaze, which are all important when a robot needs to predict the movements of humans in close proximity. Hence, in this letter, we present a novel dataset of full-body motion for everyday manipulation tasks, which includes the above. The motion data was captured using a traditional motion capture system based on reflective markers. We additionally captured eye-gaze using a wearable pupil-tracking device. As we show in experiments, the dataset can be used for the design and evaluation of full-body motion prediction algorithms. Furthermore, our experiments show eye-gaze as a powerful predictor of human intent. The dataset includes 180 min of motion capture data with 1627 pick and place actions being performed. It is available at https://humans-to-robots-motion.github.io/mogaze/ MoGaze, Dataset and is planned to be extended to collaborative tasks with two humans in the near future. Â© 2016 IEEE.",,,
10.1109/THMS.2020.3035176,2021,"Liu J., Chi J., Hu W., Wang Z.",3D Model-Based Gaze Tracking Via Iris Features with a Single Camera and a Single Light Source,"Traditional 3D gaze estimation methods are usually based on the models of pupil refraction and corneal reflection. These methods typically rely on multiple light sources. The 3D gaze can be estimated using single-camera-single-light-source systems only when certain user-dependent eye parameters are available a priori, which is rarely the case. This article proposes a 3D gaze estimation method which works based on iris features using a single camera and a single light source. User-dependent eye parameters involving the iris radius and the cornea radius are user-calibrated. The 3D line-of-sight is estimated from the optical axis and the positional relationship between the optical axis and the visual axis, and then optimized using a binocular stereo vision model. The feasibility and robustness of the proposed method are assessed by simulations and practical experiments. The system configuration required by the method is simpler than that required by the state-of-the-art methods, which shows significant potential value, especially with regard to mobile device applications. Â© 2013 IEEE.",,,
10.1002/dac.4580,2021,"Liu L., Peng N.",Evaluation of user concentration in ubiquitous and cognitive artificial intelligence-assisted English online guiding system integrating face and eye movement detection,"In recent years, artificial intelligence technology has made significant breakthroughs. Big data, cloud computing, speech recognition, deep learning, and so on have become new hot spots after the Internet of things. The rapid development of artificial intelligence technology promotes the innovation of foreign language teaching ideas and learning methods. The new ecology and new paradigm of foreign language guiding must be reconstructed. The biggest difference between the Internet of things and the Internet is that the Internet of things is directly connected with all kinds of sensors. It does not need people to input information through the keyboard but automatically obtain information and carry out automatic processing. The deep integration of smart classroom and English online teaching will help to promote the reform and innovation of English teaching and provide a new way for the development of English online teaching. In this paper, we analyze the fusion of face recognition technology and eye tracking system and analyze the user focus evaluation theory in the Internet of things environment. Based on the above technology, this paper proposes an online English teaching user focus evaluation system in the context of artificial intelligence. Â© 2020 John Wiley & Sons, Ltd.",,,
10.1145/3460238.3460244,2021,"Gapi K.T., Magbitang R.M.G., Villaverde J.F.",Classification of Attentiveness on Virtual Classrooms using Deep Learning for Computer Vision,"Nowadays, virtual classrooms are highly encouraged due to the COVID-19 pandemic. This could be a disadvantage because some students might not really be engaged with this kind of setup. This study presents a system for classifying level of attentiveness on virtual classrooms using deep learning for computer vision. The study confined in the development of the technology for classifying attentiveness itself, the integration of the system to virtual classrooms is not included in the scope. The criteria for the classification include the prediction of droopy corners of mouth facial cue, hanging eyelid facial cue, eye state, and eye gaze. The software of the system used the combinations of Convolutional Neural Network (CNN) models, Dlib, and OpenCV library. After evaluation, the system was able to successfully classify attentiveness of three classes with an overall accuracy of 83.33%. Â© 2021 ACM.",,,
10.1145/3406522.3446013,2021,"Heck M., Edinger J., BÃ¼nemann J., Becker C.",Exploring Gaze-Based Prediction Strategies for Preference Detection in Dynamic Interface Elements,"Digitization is currently infiltrating all daily processes, forcing casual computer users to become acquainted with unfamiliar tools. In order to avoid overstraining these users, simplified interfaces that are reduced to the functionality and content which are relevant to the individual userare imperative. Gaze-contingent systems thus monitor viewing behavior during natural system interactions to predict relevant interface elements. The prediction performance is highly dependent on theunderlying features and algorithm, especially when the interface consist of dynamic elements such as videos. In this paper, we conduct two studies with a total of 233 subjects in which we record theviewers' gaze while watching videos. We then compare the quality of preference predictions for video elements of majority voting to the performance of machine learning. Our results indicate that (1)majority voting can predict preferences with an accuracy of up to 73% (66%) for two (four) elements, (2) machine learning improves the performance to 82% (74%), (3) prediction accuracy depends on the strength of the user's preference for an element, and (4) we can rank preferences for individual elements. Â© 2021 ACM.",,,
10.1109/SoutheastCon45413.2021.9401875,2021,"Pawar P., McManus B., Anthony T., Stavrinos D.",Hazard detection in driving simulation using deep learning,"The advancement in Big Data and Analytics has led to a data driven approach in research and development. Many traffic research studies use experimental simulators for data acquisition. This enables researchers to collect large amounts of data from human test subjects. Some research practices involve manual procedures to process participant data. On a large scale this is very labor intensive, highly time consuming and expensive. In these cases, automated solutions to perform these tasks can be implemented using deep learning techniques. This paper considers alternative approaches to process data from a longitudinal research study that uses a high-fidelity driving simulator with integrated eye tracking technology. In the simulation, specific roadway hazard scenarios are programmed to replicate realistic driving hazards. These hazard elements are then annotated by a researcher for behavioral analysis. This process is typically done manually on a frame-by-frame level with the help of software. The process is then repeated for every participant. It is a long and tedious task requiring long hours and high labor costs to be completed reliably. A deep learning solution is implemented to automate the hazard detection and annotation process. This method is tested across a small subset of participant data to assess the performance of this solution. With this method, the labor intensive process can be mitigated and improve the overall time and labor efficiency and to ensure timely dissemination of results from this research study. Â© 2021 IEEE.",,,
10.1109/SoutheastCon45413.2021.9401905,2021,"Dang T., Bhattacharya S., Crumbley J.",A review study on the use of oculometry in the assessment of driver cognitive states,"Roadway fatalities are increasing with a growing population and need for reliable transportation. These fatalities can be mitigated by incorporating driver state information with current Driver Safety Systems (DSS). There are primarily two driver cognitive states: Focused and Distracted. These states can be predicted using Machine Learning algorithms (ML) such as Support Vector Machines (SVM), Adaptive Boosting (AdaBoost), and Artificial Neural Networks (ANN) using extracted biomedical features like Electroencephalography (EEG), Electromyography (EMG), Heart Rate and Eye Tracking. This literature review summarizes all biomedical signals that are used in the assessment of driver cognitive states. A thorough literature review in this field identifies eye tracking as the most efficient and quick technique of real time driver state identification. Hence, this paper outlines the latest techniques in eye tracking using oculometry. This review paper also highlights unique ocular feature extraction techniques that can be extremely useful for future researches conducted in the field of driver state recognition. Â© 2021 IEEE.",,,
10.1145/3461353.3461362,2021,"Liu S., Zhou X.-D., Jiang X., Wu H., Shi Y.",Face Shows Your Intention: Visual Search Based on Full-face Gaze Estimation with Channel-spatial Attention,"Visual search is the process that humans use visual perception to recognize targets of interest among multitudinous objects, which is a challenging research topic in computer vision. In contrast to previous works that take the overt gaze signal as input to predict the target of visual search with computational models, we proposed a visual search network based on full-face gaze estimation with channel-spatial mechanism, which can directly predict the user's search objects from the full-face images without extra obtaining the prohibitive intermediate gaze data. We seamlessly integrate the gaze information generated by the full-face gaze estimation module and the semantic information of the scene image into the visual search network that can directly infer the user's search intention. We demonstrate the effectiveness of our method for visual search task in real-world settings, and illustrate that directions for future research on full-face based human visual cognition. Â© 2021 Association for Computing Machinery. All rights reserved.",,,
10.3390/s21062234,2021,"Kapp S., Barz M., Mukhametov S., Sonntag D., Kuhn J.",Arett: Augmented reality eye tracking toolkit for head mounted displays,"Currently an increasing number of head mounted displays (HMD) for virtual and augmented reality (VR/AR) are equipped with integrated eye trackers. Use cases of these integrated eye trackers include rendering optimization and gaze-based user interaction. In addition, visual attention in VR and AR is interesting for applied research based on eye tracking in cognitive or educational sciences for example. While some research toolkits for VR already exist, only a few target AR scenarios. In this work, we present an open-source eye tracking toolkit for reliable gaze data acquisition in AR based on Unity 3D and the Microsoft HoloLens 2, as well as an R package for seamless data analysis. Furthermore, we evaluate the spatial accuracy and precision of the integrated eye tracker for fixation targets with different distances and angles to the user (n = 21). On average, we found that gaze estimates are reported with an angular accuracy of 0.83 degrees and a precision of 0.27 degrees while the user is resting, which is on par with state-of-the-art mobile eye trackers. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/VRW52623.2021.00236,2021,Hu Z.,[DC] Eye fixation forecasting in task-oriented virtual reality,"In immersive virtual reality (VR), users' visual attention is crucial for many important applications, including VR content design, gaze-based interaction, and gaze-contingent rendering. Especially, information on users' future eye fixations is key for intelligent user interfaces and has significant relevance for many areas, such as visual attention enhancement, dynamic event triggering, and human-computer interaction. However, previous works typically focused on free-viewing conditions and paid less attention to task-oriented attention. This paper aims at forecasting users' eye fixations in task-oriented virtual reality. To this end, a VR eye tracking dataset that corresponds to different users performing a visual search task in immersive virtual environments is built. A comprehensive analysis of users' eye fixations is performed based on the collected data. The analysis reveals that eye fixations are correlated with users' historical gaze positions, task-related objects, saliency information of the VR content, and head rotation velocities. Based on this analysis, a novel learning-based model is proposed to forecast users' eye fixations in the near future in immersive virtual environments. Â© 2021 IEEE.",,,
10.1109/TNNLS.2020.2984256,2021,"Ge S., Zhang C., Li S., Zeng D., Tao D.",Cascaded Correlation Refinement for Robust Deep Tracking,"Recent deep trackers have shown superior performance in visual tracking. In this article, we propose a cascaded correlation refinement approach to facilitate the robustness of deep tracking. The core idea is to address accurate target localization and reliable model update in a collaborative way. To this end, our approach cascades multiple stages of correlation refinement to progressively refine target localization. Thus, the localized object could be used to learn an accurate on-the-fly model for improving the reliability of model update. Meanwhile, we introduce an explicit measure to identify the tracking failure and then leverage a simple yet effective look-back scheme to adaptively incorporate the initial model and on-the-fly model to update the tracking model. As a result, the tracking model can be used to localize the target more accurately. Extensive experiments on OTB2013, OTB2015, VOT2016, VOT2018, UAV123, and GOT-10k demonstrate that the proposed tracker achieves the best robustness against the state of the arts. Â© 2012 IEEE.",,,
10.3390/s21051879,2021,"Nezami F.N., WÃ¤chter M.A., Maleki N., Spaniol P., KÃ¼hne L.M., Haas A., Pingel J.M., Tiemann L., Nienhaus F., Keller L., KÃ¶nig S.U., KÃ¶nig P., Pipa G.",Westdrive x loopar: An openâ€access virtual reality project in unity for evaluating user interaction methods during takeover requests,"With the further development of highly automated vehicles, drivers will engage in non-related tasks while being driven. Still, drivers have to take over control when requested by the car. Here, the question arises, how potentially distracted drivers get back into the controlâ€loop quickly and safely when the car requests a takeover. To investigate effective humanâ€“machine interactions, a mobile, versatile, and costâ€efficient setup is needed. Here, we describe a virtual reality toolkit for the Unity 3D game engine containing all the necessary code and assets to enable fast adaptations to various humanâ€“machine interaction experiments, including closely monitoring the subject. The presented project contains all the needed functionalities for realistic traffic behavior, cars, pedestrians, and a large, openâ€source, scriptable, and modular VR environment. It covers roughly 25 km2, a package of 125 animated pedestrians, and numerous vehicles, including motorbikes, trucks, and cars. It also contains all the needed nature assets to make it both highly dynamic and realistic. The presented repository contains a C++ library made for LoopAR that enables force feedback for gaming steering wheels as a fully supported component. It also includes all necessary scripts for eyeâ€tracking in the used devices. All the main functions are integrated into the graphical user interface of the UnityÂ® editor or are available as prefab variants to ease the use of the embedded functionalities. This projectâ€™s primary purpose is to serve as an openâ€access, costâ€efficient toolkit that enables interested researchers to conduct realistic virtual reality research studies without costly and immobile simulators. To ensure the accessibility and usability of the mentioned toolkit, we performed a user experience report, also included in this paper. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.3390/s21051753,2021,"Kim H., Kwon S., Lee S.",Nra-netâ€”neg-region attention network for salient object detection with gaze tracking,"In this paper, we propose a detection method for salient objects whose eyes are focused on gaze tracking; this method does not require a device in a single image. A network was constructed using Neg-Region Attention (NRA), which predicts objects with a concentrated line of sight using deep learning techniques. The existing deep learning-based method has an autoencoder structure, which causes feature loss during the encoding process of compressing and extracting features from the image and the decoding process of expanding and restoring. As a result, a feature loss occurs in the area of the object from the detection results, or another area is detected as an object. The proposed method, that is, NRA, can be used for reducing feature loss and emphasizing object areas with encoders. After separating positive and negative regions using the exponential linear unit activation function, converted attention was performed for each region. The attention method provided without using the backbone network emphasized the object area and suppressed the background area. In the experimental results, the proposed method showed higher detection results than the conventional methods. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/TVCG.2019.2947504,2021,"Katzakis N., Chen L., Ariza O., Teather R.J., Steinicke F.",Evaluation of 3D Pointing Accuracy in the Fovea and Periphery in Immersive Head-Mounted Display Environments,"The coupling between perception and action has seldom been explored in sophisticated motor behaviour such as 3D pointing. In this study, we investigated how 3D pointing accuracy, measured by a depth estimation task, could be affected by the target appearing in different visual eccentricities. Specifically, we manipulated the visual eccentricity of the target and its depth in virtual reality. Participants wore a head-mounted-display with an integrated eye-tracker and docked a cursor into a target. We adopted a within-participants factorial design with three variables. The first variable is Eccentricity: the location of the target on one of five horizontal eccentricities (left far periphery, left near periphery, foveal, right near periphery and right far periphery). The second variable is Depth at three levels and the third variable is Feedback Loop with two levels: open/closed. Eccentricity is refactored into Motion Correspondence between the starting location of the cursor and the target location with four levels: periphery to fovea, fovea to periphery, periphery to periphery, fovea to fovea. The results showed that the pointing accuracy is modulated mainly by the target locations rather than the initial locations of the effector (hand). Visible feedback during pointing improved performance. Â© 2020 IEEE.",,,
10.1109/TVCG.2019.2947037,2021,"Dmitriev K., Marino J., Baker K., Kaufman A.E.",Visual Analytics of a Computer-Aided Diagnosis System for Pancreatic Lesions,"Machine learning is a powerful and effective tool for medical image analysis to perform computer-aided diagnosis (CAD). Having great potential in improving the accuracy of a diagnosis, CAD systems are often analyzed in terms of the final accuracy, leading to a limited understanding of the internal decision process, impossibility to gain insights, and ultimately to skepticism from clinicians. We present a visual analytics approach to uncover the decision-making process of a CAD system for classifying pancreatic cystic lesions. This CAD algorithm consists of two distinct components: random forest (RF), which classifies a set of predefined features, including demographic features, and a convolutional neural network (CNN), which analyzes radiological (imaging) features of the lesions. We study the class probabilities generated by the RF and the semantical meaning of the features learned by the CNN. We also use an eye tracker to better understand which radiological features are particularly useful for a radiologist to make a diagnosis and to quantitatively compare with the features that lead the CNN to its final classification decision. Additionally, we evaluate the effects and benefits of supplying the CAD system with a case-based visual aid in a second-reader setting. Â© 2020 IEEE.",,,
10.1016/j.ijhcs.2020.102563,2021,"Pan Y., Mitchell K.",Improving VIP viewer gaze estimation and engagement using adaptive dynamic anamorphosis,"Anamorphosis for 2D displays can provide viewer centric perspective viewing, enabling 3D appearance, eye contact and engagement, by adapting dynamically in real time to a single moving viewer's viewpoint, but at the cost of distorted viewing for other viewers. We present a method for constructing non-linear projections as a combination of anamorphic rendering of selective objects whilst reverting to normal perspective rendering of the rest of the scene. Our study defines a scene consisting of five characters, with one of these characters selectively rendered in anamorphic perspective. We conducted an evaluation experiment and demonstrate that the tracked viewer centric imagery for the selected character results in an improved gaze and engagement estimation. Critically, this is performed without sacrificing the other viewersâ€?viewing experience. In addition, we present findings on the perception of gaze direction for regularly viewed characters located off-center to the origin, where perceived gaze shifts from being aligned to misalignment increasingly as the distance between viewer and character increases. Finally, we discuss different viewpoints and the spatial relationship between objects. Â© 2020",,,
10.3390/s21041466,2021,"Tang C., Qin P., Zhang J.",Robust template adjustment siamese network for object visual tracking,"Most of the existing trackers address the visual tracking problem by extracting an appearance template from the first frame, which is used to localize the target in the current frame. Unfor-tunately, they typically face the model degeneration challenge, which easily results in model drift and target loss. To address this issue, a novel Template Adjustment Siamese Network (TA-Siam) is proposed in this paper. The proposed framework TA-Siam consists of two simple subnetworks: The template adjustment subnetwork for feature extraction and the classification-regression subnetwork for bounding box prediction. The template adjustment module adaptively uses the feature of sub-sequent frames to adjust the current template. It makes the template adapt to the target appearance variation of long-term sequence and effectively overcomes model drift problem of Siamese net-works. In order to reduce classification errors, the rhombus labels are proposed in our TA-Siam. For more efficient learning and faster convergence, our proposed tracker uses a more effective regression loss in the training process. Extensive experiments and comparisons with trackers are con-ducted on the challenging benchmarks including VOT2016, VOT2018, OTB50, OTB100, GOT-10K, and LaSOT. Our TA-Siam achieves state-of-the-art performance at the speed of 45 FPS. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.3390/s21041155,2021,"Yeamkuan S., Chamnongthai K.",3d point-of-intention determination using a multimodal fusion of hand pointing and eye gaze for a 3d display,"This paper proposes a three-dimensional (3D) point-of-intention (POI) determination method using multimodal fusion between hand pointing and eye gaze for a 3D virtual display. In the method, the finger joint forms of the pointing hand sensed by a Leap Motion sensor are first detected as pointing intention candidates. Subsequently, differences with neighboring frames, which should be during hand pointing period, are checked by AND logic with the hand-pointing intention candidates. A crossing point between the eye gaze and hand pointing lines is finally decided by the closest distance concept. In order to evaluate the performance of the proposed method, experiments with ten participants, in which they looked at and pointed at nine test points for approximately five second each, were performed. The experimental results show the proposed method measures 3D POIs at 75 cm, 85 cm, and 95 cm with average distance errors of 4.67%, 5.38%, and 5.71%, respectively. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1016/j.cmpb.2020.105898,2021,"Sevil M., Rashid M., Hajizadeh I., Askari M.R., Hobbs N., Brandt R., Park M., Quinn L., Cinar A.",Discrimination of simultaneous psychological and physical stressors using wristband biosignals,"Background and objective: In this work, we address the problem of detecting and discriminating acute psychological stress (APS) in the presence of concurrent physical activity (PA) using wristband biosignals. We focused on signals available from wearable devices that can be worn in daily life because the ultimate objective of this work is to provide APS and PA information in real-time management of chronic conditions such as diabetes by automated personalized insulin delivery. Monitoring APS noninvasively throughout free-living conditions remains challenging because the responses to APS and PA of many physiological variables measured by wearable devices are similar. Methods: Various classification algorithms are compared to simultaneously detect and discriminate the PA (sedentary state, treadmill running, and stationary bike) and the type of APS (non-stress state, mental stress, and emotional anxiety). The impact of APS inducements is verified with commonly used self-reported questionnaires (The State-Trait Anxiety Inventory (STAI)). To aid the classification algorithms, novel features are generated from the physiological variables reported by a wristband device during 117 hours of experiments involving simultaneous APS inducement and PA. We also translate the APS assessment into a quantitative metric for use in predicting the adverse outcomes. Results: An accurate classification of the concurrent PA and APS states is achieved with an overall classification accuracy of 99% for PA and 92% for APS. The average accuracy of APS detection during sedentary state, treadmill running, and stationary bike is 97.3, 94.1, and 84.5%, respectively. Conclusions: The simultaneous assessment of APS and PA throughout free-living conditions from a convenient wristband device is useful for monitoring the factors contributing to an elevated risk of acute events in people with chronic diseases like cardiovascular complications and diabetes. Â© 2020",,,
10.1109/TMI.2020.3037013,2021,"Fotouhi J., Mehrfard A., Song T., Johnson A., Osgood G., Unberath M., Armand M., Navab N.",Development and Pre-Clinical Analysis of Spatiotemporal-Aware Augmented Reality in Orthopedic Interventions,"Suboptimal interaction with patient data and challenges in mastering 3D anatomy based on ill-posed 2D interventional images are essential concerns in image-guided therapies. Augmented reality (AR) has been introduced in the operating rooms in the last decade; however, in image-guided interventions, it has often only been considered as a visualization device improving traditional workflows. As a consequence, the technology is gaining minimum maturity that it requires to redefine new procedures, user interfaces, and interactions. The main contribution of this paper is to reveal how exemplary workflows are redefined by taking full advantage of head-mounted displays when entirely co-registered with the imaging system at all times. The awareness of the system from the geometric and physical characteristics of X-ray imaging allows the exploration of different human-machine interfaces. Our system achieved an error of 4.76 Â± 2.91mm for placing K-wire in a fracture management procedure, and yielded errors of 1.57 Â± 1.16Â° and 1.46 Â± 1.00Â° in the abduction and anteversion angles, respectively, for total hip arthroplasty (THA). We compared the results with the outcomes from baseline standard operative and non-immersive AR procedures, which had yielded errors of [4.61mm, 4.76Â°, 4.77Â°] and [5.13mm, 1.78Â°, 1.43Â°], respectively, for wire placement, and abduction and anteversion during THA. We hope that our holistic approach towards improving the interface of surgery not only augments the surgeon's capabilities but also augments the surgical team's experience in carrying out an effective intervention with reduced complications and provide novel approaches of documenting procedures for training purposes. Â© 1982-2012 IEEE.",,,
10.1007/s11554-020-00955-2,2021,Ahmed N.Y.,Real-time accurate eye center localization for low-resolution grayscale images,"Eye center localization is considered a crucial step for many humanâ€“computer interaction (HCI) real-time applications. Detecting the center of eye (COE), accurately and in real time, is very challenging due to the wide variation of poses, eye appearance and specular reflection, especially in low-resolution images. In this paper, an accurate real-time detection algorithm of the COE is proposed. The proposed approach depends on the image gradient to detect the COE. The computational complexity is minimized and the accuracy is improved by down sampling the face resolution and applying a rough-to-fine algorithms, to reduce the search area, in accordance with the Eye Region Of Interest (EROI) and the number of COE candidates, tested by the proposed algorithm. Also, the detection algorithm is applied on a limited number of pixels that represent the iris boundary of the COE candidates. The Look Up Tables (LUTs) are implemented to, initially, store the invariant elements of the proposed image gradient-based algorithm, to reduce the detection time. Before applying the proposed COE detection approach, a modified specular reflection method is used to improve the detection accuracy. The performance of the proposed algorithm has been evaluated by applying it to three benchmark databases: the BIOID, GI4E and Talking Face video datasets, at different face resolutions. Experimental results revealed that the accuracy of the proposed algorithm is up to 91.68% and 96.7% for BIOID and GI4E datasets, respectively, while the minimum achieved average detection time is 2.7Â ms. The promising results highlight the potential of the proposed algorithm to be used in some eye gaze-based real-time applications. Comparing the proposed method with the most state-of-the-art approaches showed that the system outperforms most of them and has a comparable performance with the others, in terms of the COE localization accuracy and detection speed. Â© 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",,,
10.23919/Eusipco47968.2020.9287446,2021,"Zontone P., Affanni A., Bernardini R., Del Linz L., Piras A., Rinaldo R.","Emotional response analysis using electrodermal activity, electrocardiogram and eye tracking signals in drivers with various car setups","In the automotive industry, it is important to evaluate different car setups in order to match a professional driver's preference or to match the most acceptable setup for most drivers. Therefore, it is of great significance to devise objective and automatic procedures to assess a driver's response to different car settings. In this work, we analyze different physiological signals in order to evaluate how a particular car setup can be more or less stressful than others. In detail, we record an endosomatic Electrodermal Activity (EDA) signal, called Skin Potential Response (SPR), the Electrocardiogram (ECG) signal, and eye tracking coordinates. We eliminate motion artifacts by processing two SPR signals, one from each hand of the driver. Tests are carried out in a company that designs driving simulators, where the tested individuals had to drive along a straight highway with several lane changes. Three different car setups have been tested (neutral, understeering, and oversteering). We apply a statistical test to the data extracted from the cleaned SPR signal, and we then compare the results with the ones obtained using a Machine Learning algorithm. We show that we are able to discriminate the drivers' response to each setup, and, in particular, that the base car setup generates the least intense emotional response when compared to the understeering and the oversteering car setups. Â© 2021 European Signal Processing Conference, EUSIPCO. All rights reserved.",,,
10.1109/SLT48900.2021.9383590,2021,"Saeki M., Matsuyama Y., Kobashikawa S., Ogawa T., Kobayashi T.",Analysis of Multimodal Features for Speaking Proficiency Scoring in an Interview Dialogue,"This paper analyzes the effectiveness of different modalities in automated speaking proficiency scoring in an online dialogue task of non-native speakers. Conversational competence of a language learner can be assessed through the use of multimodal behaviors such as speech content, prosody, and visual cues. Although lexical and acoustic features have been widely studied, there has been no study on the usage of visual features, such as facial expressions and eye gaze. To build an automated speaking proficiency scoring system using multi-modal features, we first constructed an online video interview dataset of 210 Japanese English-learners with annotations of their speaking proficiency. We then examined two approaches for incorporating visual features and compared the effectiveness of each modality. Results show the end-to-end approach with deep neural networks achieves a higher correlation with human scoring than one with handcrafted features. Modalities are effective in the order of lexical, acoustic, and visual features. Â© 2021 IEEE.",,,
10.3390/app11020851,2021,"Ou W.-L., Kuo T.-L., Chang C.-C., Fan C.-P.",Deep-learning-based pupil center detection and tracking technology for visible-light wearable gaze tracking devices,"In this study, for the application of visible-light wearable eye trackers, a pupil tracking methodology based on deep-learning technology is developed. By applying deep-learning object detection technology based on the You Only Look Once (YOLO) model, the proposed pupil tracking method can effectively estimate and predict the center of the pupil in the visible-light mode. By using the developed YOLOv3-tiny-based model to test the pupil tracking performance, the detection accuracy is as high as 80%, and the recall rate is close to 83%. In addition, the average visible-light pupil tracking errors of the proposed YOLO-based deep-learning design are smaller than 2 pixels for the training mode and 5 pixels for the cross-person test, which are much smaller than those of the previous ellipse fitting design without using deep-learning technology under the same visible-light conditions. After the combination of calibration process, the average gaze tracking errors by the proposed YOLOv3-tiny-based pupil tracking models are smaller than 2.9 and 3.5 degrees at the training and testing modes, respectively, and the proposed visible-light wearable gaze tracking system performs up to 20 frames per second (FPS) on the GPU-based software embedded platform. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.3390/app11020825,2021,"De-Juan-Ripoll C., Llanes-Jurado J., Giglioli I.A.C., MarÃ­n-Morales J., AlcaÃ±iz M.",An immersive virtual reality game for predicting risk taking through the use of implicit measures,"Risk taking (RT) measurement constitutes a challenge for researchers and practitioners and has been addressed from different perspectives. Personality traits and temperamental aspects such as sensation seeking and impulsivity influence the individualâ€™s approach to RT, prompting risk-seeking or risk-aversion behaviors. Virtual reality has emerged as a suitable tool for RT measurement, since it enables the exposure of a person to realistic risks, allowing embodied interactions, the application of stealth assessment techniques and physiological real-time measurement. In this article, we present the assessment on decision making in risk environments (AEMIN) tool, as an enhanced version of the spheres and shield maze task, a previous tool developed by the authors. The main aim of this article is to study whether it is possible is to discriminate participants with high versus low scores in the measures of personality, sensation seeking and impulsivity, through their behaviors and physiological responses during playing AEMIN. Applying machine learning methods to the dataset we explored: (a) if through these data it is possible to discriminate between the two populations in each variable; and (b) which parameters better discriminate between the two populations in each variable. The results support the use of AEMIN as an ecological assessment tool to measure RT, since it brings to light behaviors that allow to classify the subjects into high/low risk-related psychological constructs. Regarding physiological measures, galvanic skin response seems to be less salient in prediction models. Â© 2021 by the authors.",,,
,2021,"Lang M., Schieder C.",Exploring the impact of personality traits and technical affinity on the appearance of technostress,"Information and communication technologies, such as instant messengers, have become an essential part of every person's work and private life. Undesirable side effects, such as technostress accompany this trend. The present study examines the relationship between the Big Five personality traits and the technical affinity to the appearance of technostress and its effect on the general task fulfillment. The experimental design's central component was an online memory game, combined with the NEO Five-Factor Inventory and a technical affinity questionnaire. The experiment with 13 participants was monitored with a gaze tracking device and corresponding software. This study showed that people with certain personality traits perceive technostress at a higher or lower level. Furthermore, technostress lowers task performance. However, this effect can be mitigated by a higher level of technical affinity. This paper is ongoing research. Therefore, future research should consider a higher number of participants and a variation of stressors. Â© 14th IADIS International Conf. Infor. Sys. 2021. All rights reserved.",,,
10.1007/978-3-030-87361-5_40,2021,"Li X., Wang Y.",Low Crosstalk Multi-view 3D Display Based on Parallax Barrier with Dimmed Subpixel,"Multi-view three-dimentional display system stands out for its easy realization and flexible viewing effect among all kinds of autostereoscopic display techniques. However, traditional multi-view display system suffers from severe crosstalk. Here a five-view display system with dimmed subpixel is presented to reduce crosstalk, and at the same time ensures fine viewing experience. The dimmed subpixel is arranged interlocked to make sure that the full color formation is intact and also to balance the distribution of the exit pupil. The crosstalk is reduced from 16% to 12.5%. There are three synthetic modes. Incorporated with the eye tracking device, the main viewer is able to get the best viewing experience by changing the image synthetic mode according to the viewerâ€™s position. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-87358-5_39,2021,"Li R., Ma H., Wang R., Ding J.",Device-Adaptive 2D Gaze Estimation: A Multi-Point Differential Framework,"Eye tracking system on mobile devices is important for many interactive applications. However, since models are usually customized with limited types of devices and new devices have totally different physical parameters, it is hard to generalize over unseen devices. In this paper, we present a device-adaptive 2D gaze estimation algorithm based on differential prediction. We reformulate the gaze estimation as a relative position prediction problem between the input image and calibration images, which skips the estimation for camera parameters and makes models easily generalize over devices. To tackle the new challenge, this work proposes a framework which jointly trains a differential prediction module and an aggregation module for ensembling the predictions from multiple calibration points. Experiments show that the framework outperforms baseline models constantly on open datasets with only 3â€? calibration points. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-87361-5_54,2021,"Cao S., Zhao X., Qin B., Li J., Xiang Z.",A Monocular Reflection-Free Head-Mounted 3D Eye Tracking System,"Head-mounted eye tracking has significant potential for gaze baesd application such as consumer attention monitoring, human-computer interaction, or virtual reality (VR). Existing methods, however, either use pupil center-corneal reflection (PCCR) vectors as gaze directions or require complex hardware setups and use average physiological parameters of the eye to obtain gaze directions. In view of this situation, we propose a novel method which uses only a single camera to obtain gaze direction by fitting a 3D eye model based on the motion trajectory of pupil contour. Then a 3D to 2D mapping model is proposed based on the fitting model, so the complex structure of hardware and the use of average parameters for the eyes are avoided. The experimental results show that the method can improve the gaze accuracy and simplify the hardware structure. Â© 2021, Springer Nature Switzerland AG.",,,
10.1109/ACCESS.2021.3115961,2021,"Li X.-S., Fan Z.-Z., Ren Y.-Y., Zheng X.-L., Yang R.",Classification of Eye Movement and Its Application in Driving Based on a Refined Pre-Processing and Machine Learning Algorithm,"The eyes are the first channel used by humans to obtain various types of visual information from the outside world and, especially when driving, 80-90% of information is received through the eyes. Eye movement behaviors are generally divided into six types, but attention is often paid to fixation, saccade, and smooth pursuit. Due to their importance, it is essential to classify eye movement behaviors accurately. The classification of eye movements should be a complete process, including the three steps of pre-processing, classification, and post-processing. However, it is very uncommon for all of these steps to be included in the eye-tracking literature when eye movement classification is discussed. Therefore, first, this paper proposes a refined eye movement data pre-processing framework and an improved method consisting of three steps is introduced. Second, an eye movement classification algorithm based on an improved decision tree that is independent of the threshold setting and application environment is proposed, and a post-processing consisting of merging adjacent fixations and discarding short fixations is described. Finally, the application of the classified eye movement behavior in the driving field is described, including the estimation of preview time using fixation and the estimation of time-to-collision using smooth pursuit. Two important results are obtained in this paper. One concerns the classification accuracy of eye movement behavior, the F1-scores of fixation, saccade, and smooth pursuit being respectively 92.63%, 93.46%, and 65.29%, which are higher than the scores of other algorithms. The other relates to the application to driving. On the one hand, the preview time calculated by fixation is mostly distributed around 1-6s, which is closer to reality than the traditional setting of 1s. At the same time, the regression relationship between the preview time and the road turning radius is also quantitatively analyzed and their regression function is obtained. On the other hand, the average estimated error of time-to-collision used by smooth pursuit is 7.37%. These results can play an important role in the development of ADAS and the improvement of traffic safety. Â© 2013 IEEE.",,,
10.1016/j.procs.2021.09.184,2021,"Alhanaee K., Alhammadi M., Almenhali N., Shatnawi M.",Face recognition smart attendance system using deep transfer learning,"Face identification has been considered an interesting research domain in the past few years as it plays a major biometric authentication role in several applications including attendance management and access control systems. Attendance management systems are very important to all organization though they are complex and time-consuming for managing regular attendance log. There are many automated human identification techniques such as biometrics, RFID, eye tracking, voice recognition. Face is one of the most broadly used biometrics for human identity authentication. This paper presents a facial recognition attendance system based on deep learning convolutional neural networks. We utilize transfer learning by using three pre-trained convolutional neural networks and trained them on our data. The three networks showed very high performance in terms of high prediction accuracy and reasonable training time. Â© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.",,,
10.1016/j.procs.2021.08.200,2021,"Anisimov V., Chernozatonsky K., Pikunov A., Raykhrud M., Revazov A., Shedenko K., Zhigulskaya D., Zuev S.",OkenReader: ML-based classification of the reading patterns using an Apple iPad,"Digital learning and professional training require processing of large volumes of information, mostly in the text format. The new information environment requires the development of new studying methods and, most importantly, a new tool for assessing its quality in order to adjust the strategy of compiling the educational content. Management and control of perception in reading can be performed with the use of technology, assessing attention, engagement, understanding, cognitive load and tiredness of readers. The article describes an early effort towards creation of a commercial technology based on a machine-learning (ML) algorithm that uses readers' eye-tracking recording as a proxy for their cognitive state. The solution was realized on Ipad Pro with a standard iOS operating system, which provides opportunities for mass adoption in educational and training settings. Â© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.",,,
10.1016/j.procs.2021.09.067,2021,"Jogeshwar A.K., Pelz J.B.",Gazeenviz4D: 4-D gaze-in-environment visualization pipeline,"Eye-tracking data visualization and analysis is often performed in three dimensions (x,y,t). It involves overlaying the gaze point (x,y) on scene-camera images and creating a pipeline to process the gaze-overlaid spatio-temporal data. In this project, we present a pipeline (called GazeEnViz4D) to extract 3D data from raw 2D eye-tracking data, and we have developed a custom ENvironment VIsualiZer (EnViz4DÂ§) to allow researchers to visualize the pipeline-processed data in four dimensions (x,y,z,t) for extensive, interactive analysis. GazeEnViz4D consists of creating a 3D point cloud of the environment, calculating the observer motion, locating the 2D gaze obtained from the eye-tracker in the 3D model, and visualizing the data over time using EnViz4D. EnViz4D allows a researcher to zoom in the environment at any instance, pause or play the 3D data, speed up or slow down, forward or reverse, essentially recreating the data collection episode in four dimensions from an arbitrary modifiable viewpoint. Â© 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.",,,
10.1007/978-3-030-87583-1_13,2021,"Teng C., Sharma H., Drukker L., Papageorghiou A.T., Noble J.A.",Towards Scale and Position Invariant Task Classification Using Normalised Visual Scanpaths in Clinical Fetal Ultrasound,"We present a method for classifying tasks in fetal ultrasound scans using the eye-tracking data of sonographers. The visual attention of a sonographer captured by eye-tracking data over time is defined by a scanpath. In routine fetal ultrasound, the captured standard imaging planes are visually inconsistent due to fetal position, movements, and sonographer scanning experience. To address this challenge, we propose a scale and position invariant task classification method using normalised visual scanpaths. We describe a normalisation method that uses bounding boxes to provide the gaze with a reference to the position and scale of the imaging plane and use the normalised scanpath sequences to train machine learning models for discriminating between ultrasound tasks. We compare the proposed method to existing work considering raw eye-tracking data. The best performing model achieves the F1-score of 84% and outperforms existing models. Â© 2021, Springer Nature Switzerland AG.",,,
10.1080/01691864.2021.1982405,2021,"Wang Y., Duan F., Wang Y.",Multi-point surface FES hand rehabilitation system for stroke patients based on eye movement control,"Recently, the number of stroke patients has increased greatly. Most of them suffer from hand motor impairment, which creates the need for effective rehabilitation systems. Functional electrical stimulation (FES) is a neurorehabilitation method based on brain plasticity, which is widely accepted because it is non-invasive and convenient. However, it is still a challenge to achieve fine finger control by using surface FES. Here, we use multi-point surface FES to achieve individual finger motion. Besides, simple FES does not engage the subjects' attention, and an optimal stimulation position is difficult to identify. Hence, we try to integrate FES with sensory control by presenting a multi-point FES system based on eye movement control. By programming the electrodes display interface and establishing communication with the FES device and the eye tracker, subjects can change stimulation points via their eye movements. Experiments were performed to test the validity and safety of the proposed system. Feedback data was obtained using a 3D motion capture device. The results indicate that achieving individual finger motion is possible and stimulation points can be changed via eye movements. The system is practical and can be used in hand motor training for stroke patients to shorten the rehabilitation period. Â© 2021 Informa UK Limited, trading as Taylor & Francis Group and The Robotics Society of Japan.",,,
10.1007/978-3-030-87196-3_56,2021,"Saab K., Hooper S.M., Sohoni N.S., Parmar J., Pogatchnik B., Wu S., Dunnmon J.A., Zhang H.R., Rubin D., RÃ© C.",Observational Supervision for Medical Image Classification Using Gaze Data,"Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinicianâ€™s workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-84340-3_16,2021,"Koonsanit K., Tsunajima T., Nishiuchi N.",Evaluation of Strong and Weak Signifiers in a Web Interface Using Eye-Tracking Heatmaps and Machine Learning,"The eye-tracking heatmap is a quantitative research tool that shows the userâ€™s gaze points. Most of the eye-tracking heatmap is a 2D visualization comprising different colors. The heatmap colors indicate gaze duration, and the color cellâ€™s position indicates gaze position. The eye-tracking heatmap has often been used to evaluate the usability of web interfaces to understand user behavior. For example, web designers have used heatmaps to obtain actual evidence for how users use their website. Further, the collection of eye-tracking heatmap data during website viewing facilitates measurement of improvements in site usability. However, although the eye-tracking heatmap provides rich information about how users watch, focus, and interact with a site, the high informational requirements substantially increase computational burden. In many cases, the distribution of gaze points in an eye-tracking heatmap may not be easily understood and interpreted. Accordingly, manual evaluation of heatmaps is inefficient. This study aimed to evaluate web usability by focusing on signifiers as an interface element using eye-tracking heatmaps and machine learning algorithms. We also used the dimensionality reduction technique to reduce the complexity of heatmap data. The results showed that the proposed classification model that combined the decision tree and PCA technique provided more than 90% accuracy when compared with the other nine classical machine learning methods. This finding indicated that the machine learning process reached the correct decision about the interfaceâ€™s usability. Â© 2021, Springer Nature Switzerland AG.",,,
10.1109/TCDS.2021.3114162,2021,"Amadori P.V., Fischer T., Wang R., Demiris Y.",Predicting Secondary Task Performance: A Directly Actionable Metric for Cognitive Overload Detection,"In this paper, we address cognitive overload detection from unobtrusive physiological signals for users in dual-tasking scenarios. Anticipating cognitive overload is a pivotal challenge in interactive cognitive systems and could lead to safer shared-control between users and assistance systems. Our framework builds on the assumption that decision mistakes on the cognitive secondary task of dual-tasking users correspond to cognitive overload events, wherein the cognitive resources required to perform the task exceed the ones available to the users. We propose DecNet, an end-to-end sequence-to-sequence deep learning model that infers in real-time the likelihood of user mistakes on the secondary task, i.e., the practical impact of cognitive overload, from eye-gaze and head-pose data. We train and test DecNet on a dataset collected in a simulated driving setup from a cohort of 20 users on two dual-tasking decision-making scenarios, with either visual or auditory decision stimuli. DecNet anticipates cognitive overload events in both scenarios and can perform in time-constrained scenarios, anticipating cognitive overload events up to 2s before they occur. We show that DecNet&#x2019;s performance gap between audio and visual scenarios is consistent with user perceived difficulty. This suggests that single modality stimulation induces higher cognitive load on users, hindering their decision-making abilities. Crown",,,
10.1007/978-3-030-85613-7_29,2021,"Gomez Cubero C., Rehm M.",Intention Recognition in Human Robot Interaction Based on Eye Tracking,"In human robot interaction any input that might help the robot to understand the human behaviour is valuable, and the eyes and their movement undoubtedly hold valuable information. In this paper we propose a novel algorithm for intention recognition using eye tracking in human robot collaboration. We first explore how the Cascade Effect hypothesis and a LSTM-based machine learning model perform to classify intent from gaze. Second, an algorithm is proposed, which can be used in a real time interaction to infer intention from the human user with a small uncertainty. A data collection with 30 participants was conducted in virtual reality to train and test the algorithm. The algorithm allows to detect the user intention upÂ to two seconds before any user action with a success rate of upÂ to 75%. These results open the possibility to study human robot interaction, where the robot can take the initiative based on the intention recognition. Â© 2021, IFIP International Federation for Information Processing.",,,
10.1007/978-3-030-85616-8_8,2021,"Islam M.R., Nawa S., Vargo A., Iwata M., Matsubara M., Morishima A., Kise K.",Quality Assessment of Crowdwork via Eye Gaze: Towards Adaptive Personalized Crowdsourcing,"A significant challenge for creating efficient and fair crowdsourcing platforms is in rapid assessment of the quality of crowdwork. If a crowdworker lacks the skill, motivation, or understanding to provide adequate quality task completion, this reduces the efficacy of a platform. While this would seem like only a problem for task providers, the reality is that the burden of this problem is increasingly leveraged on crowdworkers. For example, task providers may not pay crowdworkers for their work after the evaluation of the task results has been completed. In this paper, we propose methods for quickly evaluating the quality of crowdwork using eye gaze information by estimating the correct answer rate. We find that the method with features generated by self-supervised learningÂ (SSL) provides the most efficient result with a mean absolute error of 0.09. The results exhibit the potential of using eye gaze information to facilitate adaptive personalized crowdsourcing platforms. Â© 2021, IFIP International Federation for Information Processing.",,,
10.1109/ICASSP39728.2021.9414624,2021,"Chang Y., He C., Zhao Y., Luy T., Gu N.",High-Frame-Rate Eye-Tracking Framework for Mobile Devices,"Gaze-on-screen tracking, an appearance-based eye-tracking task, has drawn significant interest in recent years. While learning-based high-precision eye-tracking methods have been designed in the past, the complex pre-training and high computation in neural network-based deep models restrict their applicability in mobile devices. Moreover, as the display frame rate of mobile devices has steadily increased to 120 fps, high-frame-rate eye tracking becomes increasingly challenging. In this work, we tackle the tracking efficiency challenge and introduce GazeHFR, a biologic-inspired eyetracking model specialized for mobile devices, offering both high accuracy and efficiency. Specifically, GazeHFR classifies the eye movement into two distinct phases, i.e., saccade and smooth pursuit, and leverages inter-frame motion information combined with lightweight learning models tailored to each movement phase to deliver high-efficient eye tracking without affecting accuracy. Compared to prior art, Gaze- HFR achieves approximately 7x speedup and 15% accuracy improvement on mobile devices. Â©2021 IEEE.",,,
10.1109/ACCESS.2021.3110644,2021,"Kang D., Ma L.",Real-Time Eye Tracking for Bare and Sunglasses-Wearing Faces for Augmented Reality 3D Head-Up Displays,"Eye pupil tracking is important for augmented reality (AR) three-dimensional (3D) head-up displays (HUDs). Accurate and fast eye tracking is still challenging due to multiple driving conditions with eye occlusions, such as wearing sunglasses. In this paper, we propose a system for commercial use that can handle practical driving conditions. Our system classifies human faces into bare faces and sunglasses faces, which are treated differently. For bare faces, our eye tracker regresses the pupil area in a coarse-to-fine manner based on a revised Supervised Descent Method based eye-nose alignment. For sunglasses faces, because the eyes are occluded, our eye tracker uses whole face alignment with a revised Practical Facial Landmark Detector for pupil center tracking. Furthermore, we propose a structural inference-based re-weight network to predict eye position from non-occluded areas, such as the nose and mouth. The proposed re-weight sub-network revises the importance of different feature map positions and predicts the occluded eye positions by non-occluded parts. The proposed eye tracker is robust via a tracker-checker and a small model size. Experiments show that our method achieves high accuracy and speed, approximately 1.5 and 6.5 mm error for bare and sunglasses faces, respectively, at less than 10 ms on a 2.0GHz CPU. The evaluation dataset was captured indoors and outdoors to reflect multiple sunlight conditions. Our proposed method, combined with AR 3D HUDs, shows promising results for commercialization with low crosstalk 3D images. Â© 2013 IEEE.",,,
10.3390/mti5090050,2021,"Luo W., Cao J., Ishikawa K., Ju D.",A human-computer control system based on intelligent recognition of eye movements and its application in wheelchair driving,"This paper presents a practical human-computer interaction system for wheelchair motion through eye tracking and eye blink detection. In this system, the pupil in the eye image has been extracted after binarization, and the center of the pupil was localized to capture the trajectory of eye movement and determine the direction of eye gaze. Meanwhile, convolutional neural networks for feature extraction and classification of open-eye and closed-eye images have been built, and machine learning was performed by extracting features from multiple individual images of open-eye and closed-eye states for input to the system. As an application of this human-computer interaction control system, experimental validation was carried out on a modified wheelchair and the proposed method proved to be effective and reliable based on the experimental results. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/TVCG.2021.3106492,2021,"Zhou Z., Wang L., Popescu V.",A Partially-Sorted Concentric Layout for Efficient Label Localization in Augmented Reality,"A common approach for Augmented Reality labeling is to display the label text on a flag planted into the real world element at a 3D anchor point. When there are more than just a few labels, the efficiency of the interface decreases as the user has to search for a given label sequentially. The search can be accelerated by sorting the labels alphabetically, but sorting all labels results in long and intersecting leader lines from the anchor points to the labels. This paper proposes a partially-sorted concentric label layout that leverages the search efficiency of sorting while avoiding the label display problems of long or intersecting leader lines. The labels are partitioned into a small number of sorted sequences displayed on circles of increasing radii. Since the labels on a circle are sorted, the user can quickly search each circle. A tight upper bound derived from circular permutation theory limits the number of circles and thereby the complexity of the label layout. For example, 12 labels require at most three circles. When the application allows it, the labels are presorted to further reduce the number of circles in the layout. The layout was tested in a user study where it significantly reduced the label searching time compared to a conventional single-circle layout. IEEE",,,
10.1148/ryai.2020200047,2021,"Stember J.N., Celik H., Gutman D., Swinburne N., Young R., Eskreis-Winkler S., Holodny A., Jambawalikar S., Wood B.J., Chang P.D., Krupinski E., Bagci U.",Integrating eye tracking and speech recognition accurately annotates mr brain images for deep learning: Proof of principle,"Purpose: To generate and assess an algorithm combining eye tracking and speech recognition to extract brain lesion location labels automatically for deep learning (DL). Materials and Methods: In this retrospective study, 700 two-dimensional brain tumor MRI scans from the Brain Tumor Segmentation database were clinically interpreted. For each image, a single radiologist dictated a standard phrase describing the lesion into a microphone, simulating clinical interpretation. Eye-tracking data were recorded simultaneously. Using speech recognition, gaze points corresponding to each lesion were obtained. Lesion locations were used to train a keypoint detection convolutional neural network to find new lesions. A network was trained to localize lesions for an independent test set of 85 images. The statistical measure to evaluate our method was percent accuracy. Results: Eye tracking with speech recognition was 92% accurate in labeling lesion locations from the training dataset, thereby demonstrating that fully simulated interpretation can yield reliable tumor location labels. These labels became those that were used to train the DL network. The detection network trained on these labels predicted lesion location of a separate testing set with 85% accuracy. Conclusion: The DL network was able to locate brain tumors on the basis of training data that were labeled automatically from simulated clinical image interpretation. Â© RSNA, 2020.",,,
10.1111/jcal.12595,2021,"Baceviciute S., Lucas G., Terkildsen T., Makransky G.",Investigating the redundancy principle in immersive virtual reality environments: An eye-tracking and EEG study,"Background: The increased availability of immersive virtual reality (IVR) has led to a surge of immersive technology applications in education. Nevertheless, very little is known about how to effectively design instruction for this new media, so that it would benefit learning and associated cognitive processing. Objectives: This experiment explores if and how traditional instructional design principles from 2D media translate to IVR. Specifically, it focuses on studying the underlying mechanisms of the redundancy-principle, which states that presenting the same information concurrently in two different sensory channels can cause cognitive overload and might impede learning. Methods: A total of 73 participants learned through a specifically-designed educational IVR application in three versions: (1) auditory representation format, (2) written representation format, and (3) a redundancy format (i.e. both written and auditory formats). The study utilized advanced psychophysiological methods of Electroencephalography (EEG) and eye-tracking (ET), learning measures and self-report scales. Results and Conclusions: Results show that participants in the redundancy condition performed equally well on retention and transfer post-tests. Similarly, results from the subjective measures, EEG and ET suggest that redundant content was not found to be more cognitively demanding than written content alone. Implications: Findings suggest that the redundancy effect might not generalize to VR as originally anticipated in 2D media research, providing direct implications to the design of IVR tools for education. Â© 2021 John Wiley & Sons Ltd",,,
10.1007/978-3-030-82427-3_9,2021,"Khellat-Kihel S., Sun Z., Tistarelli M.",An Hybrid Attention-Based System forÂ theÂ Prediction of Facial Attributes,"Recent research on face analysis has demonstrated the richness of information embedded in feature vectors extracted from a deep convolutional neural network. Even though deep learning achieved a very high performance on several challenging visual tasks, such as determining the identity, age, gender and race, it still lacks a well grounded theory which allows to properly understand the processes taking place inside the network layers. Therefore, most of the underlying processes are unknown and not easy to control. On the other hand, the human visual system follows a well understood process in analyzing a scene or an object, such as a face. The direction of the eye gaze is repeatedly directed, through purposively planned saccadic movements, towards salient regions to capture several details. In this paper we propose to capitalize on the knowledge of the saccadic human visual processes to design a system to predict facial attributes embedding a biologically-inspired network architecture, the HMAX. The architecture is tailored to predict attributes with different textural information and conveying different semantic meaning, such as attributes related and unrelated to the subjectâ€™s identity. Salient points on the face are extracted from the outputs of the S2 layer of the HMAX architecture and fed to a local texture characterization module based on LBP (Local Binary Pattern). The resulting feature vector is used to perform a binary classification on a set of pre-defined visual attributes. The devised system allows to distill a very informative, yet robust, representation of the imaged faces, allowing to obtain high performance but with a much simpler architecture as compared to a deep convolutional neural network. Several experiments performed on publicly available, challenging, large datasets demonstrate the validity of the proposed approach. Â© 2021, The Author(s).",,,
10.1080/10447318.2021.1960092,2021,"Souza K.E.S.D., Aviz I.L.D., Mello H.D.D., Figueiredo K., Vellasco M.M.B.R., Costa F.A.R., Seruffo M.C.D.R.","An Evaluation Framework for User Experience Using Eye Tracking, Mouse Tracking, Keyboard Input, and Artificial Intelligence: A Case Study","User eXperience (UX) has been used to achieve improvements in digital information systems based on how people perceive them. In particular, this paper establishes a framework that employs methods for eye and mouse tracking, keyboard input, self-assessment questionnaire and artificial intelligence algorithms to evaluate user experience and categorize users in terms of performance profiles. The results obtained with this framework are artifacts that can be used to support customizations of the User Interface (UI) on the websites. Moreover, the established framework is generic and flexible and can be applied to any information system, such as the case study shown in the website of the Federal Revenue of Brazil (RFB). The main objectives of this paper are as follows: (i) to set out a powerful UX framework based on three tracking techniquesâ€“the AIT2-UX; (ii) to provide the T2-UXT to collect, collate, process and visualize data obtained from usersâ€?interactions (iii) to use and compare machine learning algorithms with the classification of user performance profiles; (iv) to use the artifacts generated by the framework to manually customize the UI with the website. Â© 2021 Taylor & Francis Group, LLC.",,,
10.1109/TPAMI.2021.3100259,2021,"Xu Y., Zhang Z., Gao S.",Spherical DNNs and Their Applications in 360<formula><tex>$^\circ$</tex></formula> Images and Videos,"Spherical images or videos, as typical non-Euclidean data, are usually stored in the form of 2D panoramas obtained through an equirectangular projection, which is neither equal area nor conformal. The distortion caused by the projection limits the performance of vanilla Deep Neural Networks (DNNs) designed for traditional Euclidean data. In this paper, we design a novel Spherical Deep Neural Network (DNN) to deal with the distortion caused by the equirectangular projection. Specifically, we customize a set of components, including a spherical convolution, a spherical pooling, a spherical ConvLSTM cell and a spherical MSE loss, as the replacements of their counterparts in vanilla DNNs for spherical data. The core idea is to change the identical behavior of the conventional operations in vanilla DNNs across different feature patches so that they will be adjusted to the distortion caused by the variance of sampling rate among different feature patches. We demonstrate the effectiveness of our Spherical DNNs for saliency detection and gaze estimation in <formula><tex>$360^\circ$</tex></formula> videos. To facilitate the study of the 360 video saliency detection, we further construct a large-scale <formula><tex>$360^\circ$</tex></formula> video saliency detection dataset. Comprehensive experiments validate the effectiveness of our proposed Spherical DNNs for spherical handwritten digit classification and sport classification, saliency detection and gaze tracking in <formula><tex>$360^\circ$</tex></formula> videos. IEEE",,,
10.1007/978-3-030-80624-8_13,2021,"Joseph A.W., Vaiz J.S., Murugesh R.",Modeling Cognitive Load in Mobile Human Computer Interaction Using Eye Tracking Metrics,"Modeling cognitive load of user interaction based on ocular parameters have become a dominant method for exploring usability evaluation of interfaces for systems and applications. Growing importance of Artificial Intelligence in Human Computer Interaction (HCI) has proposed many approaches to understand usersâ€?need and enhance human centric method for interface design. In particular, machine learning-based cognitive modeling, using eye tracking parameters have received more attention in the context of smart devices and applications. In this context, this paper aims to model the estimated cognitive load values for each user into different levels of cognition like very high, high, moderate, low, very low etc., while performing different tasks on a smart phone. The study focuses on the use behavioural measures, ocular parameters along with eight traditional machine learning classification algorithms like Decision Tree, Linear Discriminant Analysis, Random Forest, Support Vector Machine, NaÃ¯ve Bayes, Neural Network, Fuzzy Rules with Weight Factor and K-Nearest Neighbor to model different levels of estimated cognitive load for each participant. The data set for modeling consisted of 250 records, 11 ocular parameters as prediction variables including age and type of task; and three types of classes (2-class, 3-class, 5-class) for classifying the estimated cognitive load for each participant. We noted that, Age, Fixation Count, Saccade Count, Saccade Rate, Average Pupil Dilation are the most important parameters contributing to modeling the estimated cognitive load levels. Further, we observed that, the Decision Tree algorithm achieved highest accuracy for classifying estimated cognitive load values into 2-class (86.8%), 3-class (74%) and 5-class (62.8%) respectively. Finally, from our study, it may be noted that, machine learning is an effective method for predicting 2-class-based (Low and High) cognitive load levels using ocular parameters. The outcome of the study also provides the fact that ageing affects usersâ€?cognitive workload while performing tasks on smartphone. Â© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,,
10.1007/978-3-030-78092-0_36,2021,"Breen M., McClarty J., Langley C., Farzidayeri J., Trevethan K., Swenson B., Sarkar M., Wade J., Sarkar N.",2D and 3D Visualization of Eye Gaze Patterns in a VR-Based Job Interview Simulator: Application in Educating Employers on the Gaze Patterns of Autistic Candidates,"Employment of autistic individuals is strikingly low in relation to the skill level and capabilities of this population. Roughly 65% of autistic adults are either unemployed or underemployed relative to their abilities but there is increasing recognition that this number could be greatly improved through empowering autistic individuals while simultaneously providing a boost to the economy. Much of this disparity can be attributed in part to the lack of awareness and understanding among employers regarding behavior of autistic individuals during the hiring process. Most notably, the job interviewâ€”where strong eye contact is traditionally expected but can be extremely uncomfortable for autistic individualsâ€”presents an unreasonable initial barrier to employment for many. The current work presents a data visualization dashboard that is populated with quantitative data (including eye tracking data) captured during simulated job interviews using a novel interview simulator called Career Interview Readiness in Virtual Reality (CIRVR). We conducted a brief series of case studies wherein autistic individuals who took part in a CIRVR interview and other key stakeholders provided lived experiences and qualitative insights into the most effective design and application of such data visualization dashboard. We conclude with a discussion of the role of information related to visual attention in job interviews with an emphasis on the importance of descriptive rather than prescriptive interpretation. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-77817-0_17,2021,"Krishnaswamy N., Pustejovsky J.",The Role of Embodiment and Simulation in Evaluating HCI: Experiments and Evaluation,"In this paper series, we argue for the role embodiment plays in the evaluation of systems developed for Human Computer Interaction. We use a simulation platform, VoxWorld, for building Embodied Human Computer Interactions (EHCI). VoxWorld enables multimodal dialogue systems that communicate through language, gesture, action, facial expressions, and gaze tracking, in the context of task-oriented interactions. A multimodal simulation is an embodied 3D virtual realization of both the situational environment and the co-situated agents, as well as the most salient content denoted by communicative acts in a discourse. It is built on the modeling language VoxML, which encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextually salient inferences and decisions in the environment. Through simulation experiments in VoxWorld, we can begin to identify and then evaluate the diverse parameters involved in multimodal communication between agents. In this second part of this paper series, we discuss the consequences of embodiment and common ground, and how they help evaluate parameters of the interaction between humans and agents, and compare and contrast evaluation schemes enabled by different levels of embodied interaction. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-77817-0_21,2021,"Pustejovsky J., Krishnaswamy N.",The Role of Embodiment and Simulation in Evaluating HCI: Theory andÂ Framework,"In this paper, we argue that embodiment can play an important role in the evaluation of systems developed for Human Computer Interaction. To this end, we describe a simulation platform for building Embodied Human Computer Interactions (EHCI). This system, VoxWorld, enables multimodal dialogue systems that communicate through language, gesture, action, facial expressions, and gaze tracking, in the context of task-oriented interactions. A multimodal simulation is an embodied 3D virtual realization of both the situational environment and the co-situated agents, as well as the most salient content denoted by communicative acts in a discourse. It is built on the modeling language VoxML, which encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextually salient inferences and decisions in the environment. Through simulation experiments in VoxWorld, we can begin to identify and then evaluate the diverse parameters involved in multimodal communication between agents. VoxWorld enables an embodied HCI by situating both human and computational agents within the same virtual simulation environment, where they share perceptual and epistemic common ground. In this first part of this paper series, we discuss the consequences of embodiment and common ground, and how they help evaluate parameters of the interaction between humans and agents, and demonstrate different behaviors and types of interactions on different classes of agents. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-80432-9_28,2021,"Savochkina E., Lee L.H., Drukker L., Papageorghiou A.T., Noble J.A.",First Trimester Gaze Pattern Estimation Using Stochastic Augmentation Policy Search for Single Frame Saliency Prediction,"While performing an ultrasound (US) scan, sonographers direct their gaze at regions of interest to verify that the correct plane is acquired and to interpret the acquisition frame. Predicting sonographer gaze on US videos is useful for identification of spatio-temporal patterns that are important for US scanning. This paper investigates utilizing sonographer gaze, in the form of gaze-tracking data, in a multi-modal imaging deep learning framework to assist the analysis of the first trimester fetal ultrasound scan. Specifically, we propose an encoder-decoder convolutional neural network with skip connections to predict the visual gaze for each frame using 115 first trimester ultrasound videos; 29,250 video frames for training, 7,290 for validation and 9,126 for testing. We find that the dataset of our size benefits from automated data augmentation, which in turn, alleviates model overfitting and reduces structural variation imbalance of US anatomical views between the training and test datasets. Specifically, we employ a stochastic augmentation policy search method to improve segmentation performance. Using the learnt policies, our models outperform the baseline: KLD, SIM, NSS and CC (2.16, 0.27, 4.34 and 0.39 versus 3.17, 0.21, 2.92 and 0.28). Â© 2021, Springer Nature Switzerland AG.",,,
10.1109/ACCESS.2021.3091642,2021,"Chakraborty P., Ahmed S., Yousuf M.A., Azad A., Alyami S.A., Moni M.A.",A Human-Robot Interaction System Calculating Visual Focus of Human's Attention Level,"Attention is the mental awareness of human on a particular object or a piece of information. The level of attention indicates how intense the focus is on an object or an instance. In this study, several types of human attention level have been observed. After introducing image segmentation and detection technique for facial features, eyeball movement and gaze estimation were measured. Eye movement were assessed using the video data, and a total of 10197 data instances were manually labelled for the attention level. Then Artificial Neural Network (ANN) and Recurrent Neural Network-Long Short Term Memory (LSTM) based Deep learning (DL) architectures have been proposed for analysing the data. Next, the trained DL model has been implanted into a robotic system that is capable of detecting various features; ultimately leading to the calculation of visual attention for reading, browsing, and writing purposes. This system is capable of checking the attention level of the participants and also can detect if participants are present or not. Based on a certain level of visual focus of attention (VFOA), this system interacts with the person, generates awareness and establishes verbal or visual communication with that person. The proposed ML techniques have achieved almost 99.24% validation accuracy and 99.43% test accuracy. It is also shown in the comparative study that, since the dataset volumes are limited, ANN is more suitable for attention level calculation than RNN-LSTM. We hope that the implemented robotic structure manifests the real-world implication of the proposed method. Â© 2013 IEEE.",,,
10.1007/978-3-030-80285-1_31,2021,"Chen S., Zhao Y., Wu T., Li Y.",Exploring Relationships Between Distractibility and Eye Tracking During Online Learning,"More than half of students think their attention is easily shifted when theyâ€™re learning online. Distractibility, to a certain extent caused by visual stimuli is the main impact to decrease their academic performance. In addition, eye-tracking technology has been widely applied to explore distractibility in many â€œlookâ€?tasks, such as reading, viewing advertisements, and watching online videos as well as measure the efficiency of visual cognition. Therefore, this paper aimed to discuss the relationship between distractibility with eye movement indices and academic performance. Fifty high school students (30 girls) were recruited to complete experiment that was divided into two groups, which are the experimental group with distractions and controls with no one. The result showed that three of traditional eye movement indices were significantly correlated with distractibility (p&amp;lt; 0.05 ). Then we introduced the network accessibility model and the gaze transformation entropy to create two composite indexes according to the complexity and directivity of distractibility characteristics. The result revealed that the two composite indexes are significantly correlated with distractibility (p&amp;lt; 0.05 ). Finally, we constructed the mapping model about eye movement metrics about distractibility and online learning performance with a machine learning algorithm. The result ration was R2= 0.799, and the error was Re&amp;lt; 0.1, which proved the model was feasible and accessible. The research from the perspective of distractibility can provide valuable support for physiological indicators testing tools of academic performance and highlights the applications of eye movement dynamics. Â© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,,
10.2352/ISSN.2470-1173.2021.6.IRIACV-310,2021,"Paletta L., Ganster H., Schneeberger M., Pszeida M., Lodron G., PechstÃ¤dt K., Spitzer M., Reischl C.",Towards large-scale evaluation of mental stress and biomechanical strain in manufacturing environments using 3D-referenced gaze and wearable-based analytics,"In future manufacturing human-machine interaction will evolve towards flexible and smart collaboration. It will meet requirements from the optimization of assembly processes as well as from motivated and skilled human behavior. Recently, human factors engineering has substantially progressed by means of detailed task analysis. However, there is still a lack in precise measuring cognitive and sensorimotor patterns for the analysis of long-term mental and physical strain. This work presents a novel methodology that enables real-time measurement of cognitive load based on executive function analyses as well as biomechanical strain from non-obtrusive wearable sensors. The methodology works on 3D information recovery of the working cell using a precise stereo measurement device. The worker is equipped with eye tracking glasses and a set of wearable accelerometers. Wireless connectivity transmits the sensor-based data to a nearby PC for monitoring. Data analytics then recovers the 3D geometry of gaze and viewing frustum within the working cell and furthermore extracts the worker's task switching rate as well as a skeleton-based approximation of worker's posture associated with an estimation of biomechanical strain of muscles and joints. First results enhanced by AI-based estimators demonstrate a good match with the results of an activity analysis performed by occupational therapists. Â© 2021, Society for Imaging Science and Technology.",,,
10.2352/ISSN.2470-1173.2021.2.SDA-055,2021,"Nourrit V., Poilane R., De Bougrenet J.-L.",Custom on-axis head-mounted eye tracker for 3D active glasses,"Currently, no low cost commercial 3D active glasses with embedded eye tracker are available despite the importance of 3D and eye tracking for numerous applications. In this context, a simple low cost eye tracker for 3D glasses with liquid crystal shutters is presented and tested for orthoptics applications. By using a beam splitter to better align the camera with the line of sight when the subject looks at a target in front of him at far range, the new design allows recording high quality images with limited pupil deformation when compared to other commercial eye trackers where the cameras can be far from this axis (head mounted or fixed). Such a design could be useful for various applications from orthoptics to virtual reality. Â© 2021, Society for Imaging Science and Technology.",,,
10.2352/J.ImagingSci.Technol.2020.64.6.060407,2021,"Lee S., Park J., Nam D.",Crosstalk minimization method for eye-tracking-based 3D display,"In this article, the authors present an image processing method to reduce three-dimensional (3D) crosstalk for eye-tracking-based 3D display. Specifically, they considered 3D pixel crosstalk and offset crosstalk and applied different approaches based on its characteristics. For 3D pixel crosstalk which depends on the viewer's relative location, they proposed output pixel value weighting scheme based on viewer's eye position, and for offset crosstalk they subtracted luminance of crosstalk components according to the measured display crosstalk level in advance. By simulations and experiments using the 3D display prototypes, the authors evaluated the effectiveness of proposed method. Â© 2020 Society for Imaging Science and Technology.",,,
10.1007/978-3-030-77211-6_19,2021,"Gerbasi A., Groznik V., Georgiev D., Sacchi L., Sadikov A.",Detecting Mild Cognitive Impairment Using Smooth Pursuit and a Modified Corsi Task,"Over 50 million people today live with some form of dementia as it is the most common neurodegenerative disease in the world. Mild cognitive impairment (MCI) is a stage before dementia symptoms overtly manifest. An estimated 10â€?5% of patients diagnosed with MCI annually convert to Alzheimerâ€™s dementia. Early detection of MCI is imperative as disease-modifying therapies in development could have the potential to significantly delay disease progression before dementia symptoms develop. There is evidence that observing oculomotor movements during different neuropsychological tasks can serve as a biomarker for MCI. A clinical study with 105 participants was performed at several centres in Ljubljana, Slovenia. All the participants underwent an extensive neurological and psychological evaluation and were, on the basis of this evaluation, divided into two groups: cognitively impaired and healthy controls. At the same time the participants performed several short tasks on the computer screen, including smooth pursuit dot tracking and a modified version of the Corsi block-tapping test. During the tasks, performed using their gaze alone, their eye movements were recorded with an eye-tracker. The eye-tracking data was analysed and a number of features describing the gaze behaviour was proposed. These features were used to construct several machine learning models to predict whether a person exhibits signs of cognitive impairment or not. A model based on random forest classifier achieved the best performance with 80% classification accuracy and an area under the ROC curve of 85%. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-981-16-0980-0_37,2021,"Sakya G., Singh S., Mishra S., Tiwari S.M.",Intelligent Invigilation Using Video Surveillance,"At the examination center, examinees daily face difficulties in finding their seating position that consumes a lot of time. Also in COVID-19 pandemic, we need to maintain social distancing while taking examination offline. So, this paper proposes system that can help examinees in knowing their seat number in their respective examination hall with the help of online Web portal. It can help in eliminating the crowd at the notice board and can also save their time. With the help of an admit card generated to which a barcode be attached, an examinee can scan that code each day and know their seating location as per a daily basis. Another system is also proposed for face recognition that can help the examiner in identifying the identity of candidates that can also alert them if they are not found in the database using deep learning. Within the examination hall, the orientation of the head and movement of the mouth of an examinee provide us the clue of suspicious behaviors. Nowadays, most of the suspicious behaviors monitoring procedures are done manually that involves a lot of invigilators in each examination hall. In the pandemic situation, we proposed an intelligent invigilation using video surveillance that can autonomously detect and track examineeâ€™s eye gaze, head orientation, and mouth movement to robustly detect their cheating activities. Algorithms which are implemented independently include eigen-face, fisherface, and linear binary pattern histograms. With the help of webcam installed at each examineeâ€™s desk, if any suspicious activity is detected, the system will generate an alarm indicating such behaviors. Â© 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",,,
10.1109/ACCESS.2021.3096032,2021,"Gite S., Pradhan B., Alamri A., Kotecha K.",ADMT: Advanced driver's movement tracking system using spatio-temporal interest points and maneuver anticipation using deep neural networks,"Assistive driving is a complex engineering problem and is influenced by several factors such as the sporadic nature of the quality of the environment, the response of the driver, and the standard of the roads on which the vehicle is being driven. The authors track the driver's anticipation based on his head movements using Spatio-Temporal Interest Point (STIP) extraction and enhance the anticipation of action accuracy well before using the RNN-LSTM framework. This research tackles a fundamental problem of lane change assistance by developing a novel model called Advanced Driver's Movement Tracking (ADMT). ADMT uses customized convolution-based deep learning networks by using Recurrent Convolutional Neural Network (RCNN). STIP with eye gaze extraction and RCNN performed in ADMT on brain4cars dataset for driver movement tracking. Its performance is compared with the traditional machine learning and deep learning models, namely Support Vector Machines (SVM), Hidden Markov Model (HMM), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and provided an increment of almost 12% in the prediction accuracy and 44% in the anticipation time. Furthermore, ADMT systems outperformed all of the models in terms of both the accuracy of the system and the previously mentioned time of anticipation that is discussed at length in the paper. Thus it assists the driver with additional anticipation time to access the typical reaction time for better preparedness to respond to undesired future behavior. The driver is then assured of a safe and assisted driving experience with the proposed system. Â© 2013 IEEE.",,,
10.1007/978-3-030-74608-7_72,2021,"Chihara T., Sakamoto J.",Effect of Time Length of Eye Movement Data Analysis on the Accuracy of Mental Workload Estimation During Automobile Driving,"We investigated the appropriate time window duration for calculating eye and head movement parameters in mental workload (MWL) estimation during automobile driving. Participants performed driving tasks on a driving simulator, and eye and head movements were measured by controlling their MWL using the N-back task, which required them to keep answering aloud the N-th previous digit in a sequence of digits. The eye and head movement parameters were calculated by changing a time window from 30Â s to 150Â s in increments of 30Â s. An anomaly detector of MWL was constructed using the one-class support vector machine (OCSVM) with the no N-back task (â€œNoneâ€? data. In each window length condition, we calculated the area under curve (AUC) for the binary classification between None and the highest MWL condition, the percentage of anomaly data, and the distance from the decision boundary. The results showed that a time window of 30Â s had significantly lower AUC compared with other time windows. In addition, the correlation coefficient between the subjective MWL score and the distance of each eye movement parameter data from the decision boundary monotonically increased in the time window 30Â s to 120Â s and decreased at 150Â s. Therefore, we concluded that 60Â s to 120Â s is an appropriate time window duration for MWL evaluation. Â© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,,
10.1007/978-3-030-68790-8_39,2021,"Bruno A., Lancette S., Zhang J., Moore M., Ward V.P., Chang J.",A saliency-based technique for advertisement layout optimisation to predict customersâ€?behaviour,"Customer retail environments represent an exciting and challenging context to develop and put in place cutting-edge computer vision techniques for more engaging customer experiences. Visual attention is one of the aspects that play such a critical role in the analysis of customers behaviour on advertising campaigns continuously displayed in shops and retail environments. In this paper, we approach the optimisation of advertisement layout content, aiming to grab the audienceâ€™s visual attention more effectively. We propose a fully automatic method for the delivery of the most effective layout content configuration using saliency maps out of each possible set of images with a given grid layout. Visual Saliency deals with the identification of the most critical regions out of pictures from a perceptual viewpoint. We want to assess the feasibility of saliency maps as a tool for the optimisation of advertisements considering all possible permutations of images which compose the advertising campaign itself. We start by analysing advertising campaigns consisting of a given spatial layout and a certain number of images. We run a deep learning-based saliency model over all permutations. Noticeable differences among global and local saliency maps occur over different layout content out of the same images. The latter aspect suggests that each image gives its contribution to the global visual saliency because of its content and location within the given layout. On top of this consideration, we employ some advertising images to set up a graphical campaign with a given design. We extract relative variance values out the local saliency maps of all permutations. We hypothesise that the inverse of relative variance can be used as an Effectiveness Score (ES) to catch those layout content permutations showing the more balanced spatial distribution of salient pixel. A group of 20 participants have run some eye-tracking sessions over the same advertising layouts to validate the proposed method. Â© Springer Nature Switzerland AG 2021.",,,
10.1117/12.2600859,2021,"Sinthanayothin C., Bholsithi W., Wongwaen N.","Morph targets for 3D facial animation with webcam using facemesh, Jeeliz-transfer APIs and Three.js","This paper presents a face control system for 3D avatar with webcam using the Facemesh API for face tracking and Jeeliz-transfer API for eye tracking. 3D avatar face animation is developed as a responsive web application. It starts with face detection and tracking through the webcam. Face coordinate data is normalized to a vertical face view where the distance between the eyes and the level of the eyes are the same for each video frame. Then new face coordinates are calculated in both 2D and 3D to study the change of specific coordinates such as mouth shape and face shape. In addition, the coordinates of face structures in the video platform have also been added to the 3D model platform. Specific coordinates are studied to analyze distance changes to be applied in 3D avatar manipulation. The 3D models are designed and created in multiple blend-shapes or basic character facial features. However, due to the limitations of web browser-based 3D morphing, which Three.js is used for morph target displays, it allows only eight combination shapes to be displayed at the same time. Therefore, the required blending geometry must be pre-assembled. Blend-shape factors are based on an analysis of the coordinates of each moving face in order to eliminate the limitations. Our 3D facial animations with a webcam generate results of high quality, real-time and online simulation. Therefore, our work is a fundamental technology that can be applied to animate other 3D characters in blend-shape format. Â© 2021 SPIE.",,,
10.14569/IJACSA.2021.01206104,2021,"Amer S.G., Kamh S.A., Elshahed M.A., Ramadan R.A.",Wheelchair Control System based Eye Gaze,The inability to control the limbs is the main reason that affects the daily activities of the disabled which causes social restrictions and isolation. More studies were performed to help disabilities for easy communication with the outside world and others. Various techniques are designed to help the disabled in carrying out daily activities easily. Among these technologies is the Smart Wheelchair. This research aims to develop a smart eye-controlled wheelchair whose movement depends on eye movement tracking. The proposed Wheelchair is simple in design and easy to use with low cost compared with previous Wheelchairs. The eye movement was detected through a camera fixed on the chair. The userâ€™s gaze direction is obtained from the captured image after some processing and analysis. The order is sent to the Arduino Uno board which controls the wheelchair movement. The Wheelchair performance was checked using different volunteers and its accuracy reached 94.4% with a very short response time compared with the other existing chairs. Â© 2021. All Rights Reserved.,,,
10.3390/info12060226,2021,"Vortmann L.-M., Schwenke L., Putze F.",Using brain activity patterns to differentiate real and virtual attended targets during augmented reality scenarios,"Augmented reality is the fusion of virtual components and our real surroundings. The simultaneous visibility of generated and natural objects often requires users to direct their selective attention to a specific target that is either real or virtual. In this study, we investigated whether this target is real or virtual by using machine learning techniques to classify electroencephalographic (EEG) and eye tracking data collected in augmented reality scenarios. A shallow convolutional neural net classified 3 second EEG data windows from 20 participants in a person-dependent manner with an average accuracy above 70% if the testing data and training data came from different trials. This accuracy could be significantly increased to 77% using a multimodal late fusion approach that included the recorded eye tracking data. Person-independent EEG classification was possible above chance level for 6 out of 20 participants. Thus, the reliability of such a brainâ€“computer interface is high enough for it to be treated as a useful input mechanism for augmented reality applications. Â© 2021 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1007/s13218-021-00727-5,2021,"Pustejovsky J., Krishnaswamy N.",Embodied Human Computer Interaction,"In this paper, we argue that embodiment can play an important role in the design and modeling of systems developed for Human Computer Interaction. To this end, we describe a simulation platform for building Embodied Human Computer Interactions (EHCI). This system, VoxWorld, enables multimodal dialogue systems that communicate through language, gesture, action, facial expressions, and gaze tracking, in the context of task-oriented interactions. A multimodal simulation is an embodied 3D virtual realization of both the situational environment and the co-situated agents, as well as the most salient content denoted by communicative acts in a discourse. It is built on the modeling language VoxML (Pustejovsky and Krishnaswamy in VoxML: a visualization modeling language, proceedings of LREC, 2016), which encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextually salient inferences and decisions in the environment. VoxWorld enables an embodied HCI by situating both human and artificial agents within the same virtual simulation environment, where they share perceptual and epistemic common ground. We discuss the formal and computational underpinnings of embodiment and common ground, how they interact and specify parameters of the interaction between humans and artificial agents, and demonstrate behaviors and types of interactions on different classes of artificial agents. Â© 2021, Gesellschaft fÃ¼r Informatik e.V. and Springer-Verlag GmbH Germany, part of Springer Nature.",,,
10.1017/ATSIP.2021.8,2021,"Imaoka H., Hashimoto H., Takahashi K., Ebihara A.F., Liu J., Hayasaka A., Morishita Y., Sakurai K.",The future of biometrics technology: From face recognition to related applications,"Biometric recognition technologies have become more important in the modern society due to their convenience with the recent informatization and the dissemination of network services. Among such technologies, face recognition is one of the most convenient and practical because it enables authentication from a distance without requiring any authentication operations manually. As far as we know, face recognition is susceptible to the changes in the appearance of faces due to aging, the surrounding lighting, and posture. There were a number of technical challenges that need to be resolved. Recently, remarkable progress has been made thanks to the advent of deep learning methods. In this position paper, we provide an overview of face recognition technology and introduce its related applications, including face presentation attack detection, gaze estimation, person re-identification and image data mining. We also discuss the research challenges that still need to be addressed and resolved. Copyright Â© The Author(s), 2021 published by Cambridge University Press in association with Asia Pacific Signal and Information Processing Association.",,,
10.1117/12.2584091,2021,"Sluka T., Kvasov A., Kubes T., Masson J., Fotinos A., Smolik G., Suruceanu G., Ergunay S., Michoud A., Hirt G., Kabengera P., Comminot J.",Light-field brings Augmented Reality to the personal space,"The state-of-The-Art Virtual and Augmented Reality (VR/AR) hardware fails to deliver satisfying visual experience due to missing or conflicting focus cues. The absence of natural focal depth in digital 3D imagery causes the so-called vergence-Accommodation conflict, focal rivalry, and possibly damage the eye-sight, especially during prolonged viewing of virtual objects within the arm's reach. It remains one of the most challenging and market-blocking problems in the VR/AR arena today. This talk will introduce CREAL's unique near-To-eye light-field projection system that provides high-resolution 3D imagery with fully natural focus cues. The system operates without eye-Tracking or severe penalty on image quality, rendering load, power consumption, data bandwidth, form-factor, production cost, or complexity. Â© COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.",,,
10.1587/transinf.2020EDP7072,2021,"Nagamatsu T., Hiroe M., Arai H.",Extending the measurement angle of a gaze estimation method using an eye model expressed by a revolution about the optical axis of the eye,"SUMMARY An eye model expressed by a revolution about the optical axis of the eye is one of the most accurate models for use in a 3D gaze estimation method. The measurement range of the previous gaze estimation method that uses two cameras based on the eye model is limited by the larger of the two angles between the gaze and the optical axes of two cameras. The previous method cannot calculate the gaze when exceeding a certain limit of the rotation angle of the eye. In this paper, we show the characteristics of reflections on the surface of the eye from two light sources, when the eye rotates. Then, we propose a method that extends the rotation angle of the eye for a 3D gaze estimation based on this model. The proposed method uses reflections that were not used in the previous method. We developed an experimental gaze tracking system for a wide projector screen and experimentally validated the proposed method with 20 participants. The result shows that the proposed method can measure the gaze of more number of people with increased accuracy compared with the previous method. Copyright Â© 2021 The Institute of Electronics, Information and Communication Engineers",,,
10.1007/978-3-030-74605-6_96,2021,"Prasetyo Y.T., Widyaningrum R.","Error Rate as Mediators of the Relationships Among 2D/3D TV Environment, Eye Gaze Accuracy, and Symptoms","3D TV is a new platform to enjoy the stereoscopic environment in the home and eye tracking technology has been extensively utilized to evaluate the 3D TV. This study was mainly intended to explore an additional eye movement parameter that can predict the eye gaze accuracy and symptoms while perceiving the image in the 3D TV. A total of 12 graduate students were asked to perform tapping task in the 2D and 3D TV using within-subject design under 6 different levels of index of difficulty (ID). Structural equation modeling (SEM) was applied to analyze the causal relationship between 2D/3D environment, a new eye movement parameter, eye gaze accuracy, and symptoms. The result showed that error rate was found as a significant mediator of the relationships. In addition, the SEM approach was also found as a new significant and reliable approach in the visual ergonomics particularly for bridging the objective and subjective measures. Finally, the new eye movement parameter can be an important key for predicting eye gaze accuracy in the stereoscopic display. Â© 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,,
10.1109/WACV48630.2021.00009,2021,"Richard A., Lea C., Ma S., Gall J., La Torre F.D., Sheikh Y.",Audio- And gaze-driven facial animation of codec avatars,"Codec Avatars are a recent class of learned, photorealistic face models that accurately represent the geometry and texture of a person in 3D (i.e., for virtual reality), and are almost indistinguishable from video [28]. In this paper we describe the first approach to animate these parametric models in real-time which could be deployed on commodity virtual reality hardware using audio and/or eye tracking. Our goal is to display expressive conversations between individuals that exhibit important social signals such as laughter and excitement solely from la-tent cues in our lossy input signals. To this end we collected over 5 hours of high frame rate 3D face scans across three participants including traditional neutral speech as well as expressive and conversational speech. We investigate a multimodal fusion approach that dynamically identifies which sensor encoding should animate which parts of the face at any time. See the supplemental video which demonstrates our ability to generate full face motion far beyond the typically neutral lip articulations seen in competing work: https://research.fb.com/videos/audio-and-gaze-driven-facial-animation-of-codec-avatars/ Â© 2021 IEEE.",,,
10.1117/12.2576660,2021,"Lee J.-H., Yanusik I., Choi Y., Kang B., Hwang C., Malinovskaya E., Park J., Nam D., Lee C., Kim C., Min T., Hong S.",Optical design of automotive augmented reality 3D head-up display with light-field rendering,"Although head-up displays (HUDs) have already been installed in some commercial vehicles, their application to augmented reality (AR) is limited owing to the resulting narrow field of view (FoV) and fixed virtual-image distance. The matching of depth between AR information and real objects across wide FoVs is a key feature of AR HUDs to provide a safe driving experience. Meanwhile, current approaches based on the integration of two-plane virtual images and computer-generated holography suffer from problems such as partial depth control and high computational complexity, respectively, which makes them unsuitable for application in fast-moving vehicles. To bridge this gap, here, we propose a light-field-based 3D display technology with eye-tracking. We begin by matching the HUD optics with the light-field display view formation. First, we design mirrors to deliver high-quality virtual images with an FoV of 10 Ã— 5Â° for a total eyebox size of 140 Ã— 120 mm and compensate for the curved windshield shape. Next, we define the procedure to translate the driver eye position, obtained via eye-tracking, to the plane of the light-field display views. We further implement a lenticular-lens design and the corresponding sub-pixel-allocation-based rendering, for which we construct a simplified model to substitute for the freeform mirror optics. Finally, we present a prototyped device that affords the desired image quality, 3D image depth up to 100 m, and crosstalk level of <1.5%. Our findings indicate that such 3D HUDs can form the mainstream technology for AR HUDs. Â© 2021 SPIE.",,,
10.1007/978-981-16-0041-8_28,2021,"Arjun S., Saluja K.P., Biswas P.",Analyzing Ocular Parameters for Web Browsing and Graph Visualization,"This paper proposes a set of techniques to investigate eye gaze and fixation patterns while users interact with electronic user interfaces. In particular, two case studies are presentedâ€”one on analyzing eye gaze while interacting with deceptive materials in web pages and another on analyzing graphs in standard computer monitor and virtual reality displays. We analyzed spatial and temporal distributions of eye gaze fixations and sequence of eye gaze movements. We used this information to propose new design guidelines to avoid deceptive materials in web and user-friendly representation of data in 2D graphs. In 2D graph study, we identified that area graph has the lowest number of clusters for user's gaze fixations and lowest average response time. The results of 2D graph study were implemented in virtual and mixed reality environment. Along with this, it was observed that the duration while interacting with deceptive materials in web pages is independent of the number of fixations. Furthermore, web-based data visualization tool for analyzing eye tracking data from single and multiple users was developed. Â© 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",,,
10.1117/12.2576972,2021,"Richter M., Blankenbach K., Reichel S.",New approaches for multi-view displays by circular display,"Multi-view displays reproduce more than one view of an object. Classical 3D displays allow only a single user. Today's multi-view displays are flat and reproduce two or more images. They base on e.g. parallax barrier, lenticular lenses, light field displays and projection with reduced effective resolution. Another method is projection on rotating mirrors. This approach requires highest frame rates (âˆ?,000 Hz), so only prototypes without color and grey are realized so far. Our approach base on a rotating (prototype 60 rps) periscope-like mirror system (with magnification) in the center of a 360Â° circular display. For prototyping, we used large high-resolution flat displays. One simple method is to use a fixed position on a single display for every corresponding view. To avoid motion blur and ghosting one mirror is equipped with a vertical slit screen to block light from neighboring areas. We implemented eye tracking for efficient rendering in real time and reproduce only relevant views according the corresponding angular position of the viewer's eyes. So a standard high resolution display can be used to generate thousands of different perspectives (at full color and 60 Hz). Possible applications of our multi-view display are collaborative work for e.g. several designers, which can see the object from very different locations and interactively improve design, (science) museums and entertainment. Â© 2021 SPIE.",,,
10.1109/TITS.2021.3075350,2021,"Hu T., Jha S., Busso C.",Temporal Head Pose Estimation From Point Cloud in Naturalistic Driving Conditions,"Head pose estimation is an important problem as it facilitates tasks such as gaze estimation and attention modeling. In the automotive context, head pose provides crucial information about the driver's mental state, including drowsiness, distraction and attention. It can also be used for interaction with in-vehicle infotainment systems. While computer vision algorithms using RGB cameras are reliable in controlled environments, head pose estimation is a challenging problem in the car due to sudden illumination changes, occlusions and large head rotations that are common in a vehicle. These issues can be partially alleviated by using depth cameras. Head rotation trajectories are continuous with important temporal dependencies. Our study leverages this observation, proposing a novel temporal deep learning model for head pose estimation from point cloud. The approach extracts discriminative feature representation directly from point cloud data, leveraging the 3D spatial structure of the face. The frame-based representations are then combined with bidirectional long short term memory (BLSTM) layers. We train this model on the newly collected multimodal driver monitoring (MDM) dataset, achieving better results compared to non-temporal algorithms using point cloud data, and state-of-the-art models using RGB images. We further show quantitatively and qualitatively that incorporating temporal information provides large improvements not only in accuracy, but also in the smoothness of the predictions. CCBYNCND",,,
10.1109/TIP.2021.3076272,2021,"Yao S., Han X., Zhang H., Wang X., Cao X.",Learning Deep Lucas-Kanade Siamese Network for Visual Tracking,"In most recent years, Siamese trackers have drawn great attention because of their well-balanced accuracy and efficiency. Although these approaches have achieved great success, the discriminative power of the conventional Siamese trackers is still limited by the insufficient template-candidate representation. Most of the existing approaches take non-aligned features to learn a similarity function for template-candidate matching, while the target object's geometrical transformation is seldom explored. To address this problem, we propose a novel Siamese tracking framework, which enables to dynamically transform the template-candidate features to a more discriminative viewpoint for similarity matching. Specifically, we reformulate the template-candidate matching problem of the conventional Siamese tracker from the perspective of Lucas-Kanade (LK) image alignment approach. A Lucas-Kanade network (LKNet) is proposed and incorporated to the Siamese architecture to learn aligned feature representations in data-driven trainable manner, which is able to enhance the model adaptability in challenging scenarios. Within this framework, we propose two Siamese trackers named LK-Siam and LK-SiamRPN to validate the effectiveness. Extensive experiments conducted on the prevalent datasets show that the proposed method is more competitive over a number of state-of-the-art methods. Â© 1992-2012 IEEE.",,,
10.1007/978-3-030-72073-5_20,2021,"Norouzifard M., Nemati A., Mollaee S., GholamHosseini H., Black J., Thompson B., Turuwhenua J., on behalf of the hPOD Study Team",A Comparison of Approaches for Synchronizing Events in Video Streams Using Audio,"A common scenario found in experimentation is to synchronize events, such as breaks between visual stimulus, with the video record taken of an experiment made of participants as they undertake the task. In our case, we recently synchronized a protocol of stimulus presentations shown on a laptop display, with webcam video made of participantsâ€?(who were two year old children) facial and eye movements as they were shown trials of stimulus containing moving dots (a random dot kinematogram or RDK). The purpose was to assess eye movements in response to these RDK stimulus as a part of a potential neurological assessment for children. The video contained audio signals such as â€œbeepsâ€?and musical interludes that indicated the start and end of trials, thereby providing a convenient opportunity to align these audio events with the timing of known events in the video record. The process of alignment can be performed manually, but this is a tedious and time consuming task when considering, for example, large databases of videos. In this paper, we tested two alternate methods for synchronizing known audio events using: 1) a deep learning based model, and a 2) standard template matching algorithm. These methods were used to synchronize the known protocol of stimulus events in videos by processing the audio contents of the recording. The deep learning approach utilized simple mel-spectrum audio signal feature extraction, whilst we adopted a cross-correlation algorithm that detected an audio template in the time domain. We found that whilst correlation was not effective as a means of beep detection; but our machine learning-based technique was robust with 90% accuracy in the testing dataset and did not the same amount of remediation required of the correlation approach. Â© 2021, Springer Nature Switzerland AG.",,,
,2021,"Wells C., Schnabel M.A., Moleta T., Brown A.",Beauty is in the eye of the beholder: Improving the human-computer interface within vrad by the active and two-way employment of our visual senses,"Whether it is via traditional methods with pen and paper or contemporary techniques such as 3D digital modelling and VR drawing, the eye typically plays a mostly passive or consuming role within the design process. By incorporating eye-tracking deeper within these methods, we can begin to discern this technology's possibilities as a method that encompasses the visual experience as an active input. Our research, however, developed the Eye-Tracking Voxel Environment Sculptor (EVES) that incorporates eye-tracking as there design actor. Through EVES we can extend eye-tracking as an active design medium. The eye-tracking data garnered from the designer within EVES is directly utilised as an input within a modelling environment to manipulate and sculpt voxels. In addition to modelling input, eye-tracking is also explored in its usability in the Virtual Reality User Interface. Eye-tracking is implemented within EVES to this extent to test the limits and possibilities of eye-tracking and the Human-Computer Interface within the realm of Virtual Reality Aided Design. Â© 2021 and published by the Association for Computer-Aided Architectural Design Research in Asia (CAADRIA), Hong Kong.",,,
10.1007/978-981-33-4859-2_16,2021,"Ghadekar P., Korpal P., Chendake P., Bansal R., Pawar A., Bhor S.",Real-Time Hands-Free Mouse Control for Disabled,"In this paper, a humanâ€“computer interface system using eye motion is implemented. In traditional methods, humanâ€“computer interfaces use keyboard, mouse as input devices. A hand-free interface between computer and human is represented in this paper. The system is developed using template matching and is a real time, fast and affordable technique for tracking facial features and eye gestures. The traditional computer screen pointing devices can be replaced by this technology, for the use of disabled people. The paper presents computer mouse cursor movement with human eyes. Wherever the eyesight focuses, accordingly the mouse is controlled. The proposed vision-based virtual interface controls the system by various eye movements such as eye blinking, winking of eye. Â© 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",,,
10.1007/978-981-16-1685-3_26,2021,"Mandal R., Becken S., Connolly R.M., Stantic B.",Residual Attention Network vs Real Attention on Aesthetic Assessment,"Photo aesthetics assessment is a challenging problem. Deep Convolutional Neural Network (CNN)-based algorithms have achieved promising results for aesthetics assessment in recent times. Lately, few efficient and effective attention-based CNN architectures are proposed that improve learning efficiency by adaptively adjusts the weight of each patch during the training process. In this paper, we investigate how real human attention affects instead of CNN-based synthetic attention network architecture in image aesthetic assessment. A dataset consists of a large number of images along with eye-tracking information has been developed using an eye-tracking device (https://www.tobii.com/group/about/this-is-eye-tracking/ ) power by sensor technology for our research, and it will be the first study of its kind in image aesthetic assessment. We adopted a Residual Attention Network and ResNet architectures which achieve state-of-the-art performance image recognition tasks on benchmark datasets. We report our findings on photo aesthetics assessment with two sets of datasets consist of original images and images with masked attention patches, which demonstrates higher accuracy when compared to the state-of-the-art methods. Â© 2021, Springer Nature Singapore Pte Ltd.",,,
10.1007/978-981-33-4676-5_7,2021,"Milanova M., Aldaeif F.",Markerless 3D Virtual Glasses Try-On System,"This paper presents the implementation of a markerless mobile augmented reality application called a virtual eye glasses try-on system. The system first detects and tracks human face and eyes. Then, the system overlays the 3D virtual glasses over the face in real time. This system helps the consumer to select any style of glasses available on the virtual space saving both time and effort when shopping online. A method based on local-invariant descriptors is implemented to extract image feature points for eyes detection and tracking. A new approach for camera pose estimation is proposed to augment real images with virtual graphics. Experiments are conducted using Haar cascade and speeded up robust features (SURF) cascade. The system is optimized and adapted for a mobile architecture. Â© 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",,,
10.1109/IAEAC50856.2021.9390807,2021,"Zhuang Y., Zhang Y., Zhao H.",Appearance-based gaze estimation using separable convolution neural networks,"Gaze estimation is one of the current important research contents of computer vision. For the current situation where the gaze estimation neural network has a large amount of parameters but the accuracy is not greatly improved and the head pose is difficult to handle, this paper proposes a simplified gaze estimation network model SLeNet based on the LeNet neural network. The deep separable convolution in the Xception network is used to reduce the amount of parameters in the convolution part and improve the computational performance of the network model. The method of splicing head posture features is retained, but another branch neural network is designed to learn head posture based on eye image and mouth corner information, and no additional module is required to obtain head posture separately. The improved network model is used to compare experiments with the original network and VGG-16 on the MPIIGaze dataset. The results show that the improved SLeNet network model performs better on the MPIIGaze dataset than LeNet and VGG-16 and has fewer parameters. Â© 2021 IEEE.",,,
10.1109/ACCESS.2021.3074913,2021,"Lin W., Kotakehara Y., Hirota Y., Murakami M., Kakusho K., Yueh H.-P.",Modeling Reading Behaviors: An Automatic Approach to Eye Movement Analytics,"Critical reading plays an important role in science learning, and previous studies have endeavored to objectively and precisely capture readers' cognitive processing in reading scientific texts. Since many factors affect readers' initiation and comprehension of scientific texts, studying the interactions of these factors was technically challenging for earlier studies. Recently, the use of artificial intelligent techniques for analyzing physiological signals has gained significant research attention, but exploitation of the educational data for proactive instructional use is still limited. This study proposed and evaluated an automatic approach incorporating the K-means++ clustering method for eye movement analytics. In this study, 64 undergraduate and graduate students read a multi-page popular science text while their eye movements were recorded. The results of the cluster analysis identified three patterns of reading behavior that were consistent and comparable to those of previous studies using self-reported measures and post-analysis analytics. Findings of the study support the potential and validity of a bottom-up, data-driven approach that can directly examine and analyze reading behaviors without interruption, and the contribution of the study to research and practice is outlined. Â© 2013 IEEE.",,,
10.1007/978-3-030-68787-8_8,2021,"Dondi P., Lombardi L., Malagodi M., Licchelli M.",Stylistic Classification of Historical Violins: A Deep Learning Approach,"Stylistic study of artworks is a well-known problem in the Cultural Heritage field. Traditional artworks, such as statues and paintings, have been extensively studied by art experts, producing standard methodologies to analyze and recognize the style of an artist. In this context, the case of historical violins is peculiar. Even if the main stylistic features of a violin are known, only few experts are capable to attribute a violin to its maker with a high degree of certainty. This paper presents a study about the use of deep learning to discriminate a violin style. Firstly, we collected images of 17thâ€?8th century violins held, or in temporary loan, at â€œMuseo del Violinoâ€?of Cremona (Italy) to be used as reference dataset. Then, we tested the performances of three state-of-the-art CNNs (VGG16, ResNet50 and InceptionV3) on a binary classification (Stradivari vs. NotStradivari). The best performing model was able to achieve 77.27% accuracy and 0.72 F1 score. A promising result, keeping in mind the limited amount of data and the complexity of the task, even for human experts. Finally, we compared the regions of interest identified by the network with the regions of interest identified in a previous eye tracking study conducted on expert luthiers, to highlight similarity and differences between the two behaviors. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-68796-0_28,2021,"Elmadjian C., Gonzales C., Morimoto C.H.",Eye Movement Classification with Temporal Convolutional Networks,"Recently, deep learning approaches have been proposed to detect eye movements such as fixations, saccades, and smooth pursuits from eye tracking data. These are end-to-end methods that have shown to surpass traditional ones, requiring no ad hoc parameters. In this work we propose the use of temporal convolutional networks (TCNs) for automated eye movement classification and investigate the influence of feature space, scale, and context window sizes on the classification results. We evaluated the performance of TCNs against a state-of-the-art 1D-CNN-BLSTM model using GazeCom, a public available dataset. Our results show that TCNs can outperform the 1D-CNN-BLSTM, achieving an F-score of 94.2% for fixations, 89.9% for saccades, and 73.7% for smooth pursuits on sample level, and 89.6%, 94.3%, and 60.2% on event level. We also state the advantages of TCNs over sequential networks for this problem, and how these scores can be further improved by feature space extension. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-68796-0_3,2021,"Muddamsetty S.M., Jahromi M.N.S., Moeslund T.B.",Expert Level Evaluations for Explainable AI (XAI) Methods in the Medical Domain,"The recently emerged field of explainable artificial intelligence (XAI) attempts to shed lights on â€˜black boxâ€?Machine Learning (ML) models in understandable terms for human. As several explanation methods are developed alongside different applications for a black box model, the need for expert-level evaluation in inspecting their effectiveness becomes inevitable. This is significantly important for sensitive domains such as medical applications where evaluation of experts is essential to better understand how accurate the results of complex ML are and debug the models if necessary. The aim of this study is to experimentally show how the expert-level evaluation of XAI methods in a medical application can be utilized and aligned with the actual explanations generated by the clinician. To this end, we collect annotations from expert subjects equipped with an eye-tracker while they classify medical images and devise an approach for comparing the results with those obtained from XAI methods. We demonstrate the effectiveness of our approach in several experiments. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-68796-0_34,2021,"Szalma J., Amora K.K., VidnyÃ¡nszky Z., Weiss B.",Investigating the Effect of Inter-letter Spacing Modulation on Data-Driven Detection of Developmental Dyslexia Based on Eye-Movement Correlates of Reading: A Machine Learning Approach,"Developmental dyslexia is a reading disability estimated to affect between 5 to 10% of the population. However, current screening methods are limited as they tell very little about the oculomotor processes underlying natural reading. Accordingly, investigating the eye-movement correlates of reading in a machine learning framework could potentially enhance the detection of poor readers. Here, the capability of eye-movement measures in classifying dyslexic and control young adults (24 dyslexic, 24 control) was assessed on eye-tracking data acquired during reading of isolated sentences presented at five inter-letter spacing levels. The set of 65 eye-movement features included properties of fixations, saccades and glissades. Classification accuracy and importance of features were assessed for all spacing levels by aggregating the results of five feature selection methods. Highest classification accuracy (73.25%) was achieved for an increased spacing level, while the worst classification performance (63%) was obtained for the minimal spacing condition. However, the classification performance did not differ significantly between these two spacing levels (p = 0.28). The most important features contributing to the best classification performance across the spacing levels were as follows: median of progressive and all saccade amplitudes, median of fixation duration and interquartile range of forward glissade duration. Selection frequency was even for the median of fixation duration, while the median amplitude of all and forward saccades measures exhibited complementary distributions across the spacing levels. The results suggest that although the importance of features may vary with the size of inter-letter spacing, the classification performance remains invariant. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-68796-0_32,2021,"Ricotti R., Pella A., Elisei G., Tagaste B., Bello F., Fontana G., Fiore M.R., Ciocca M., Mastella E., Orlandi E., Baroni G.",Gaze Stability During Ocular Proton Therapy: Quantitative Evaluation Based on Eye Surface Surveillance Videos,"Ocular proton therapy (OPT) is acknowledged as a therapeutic option for the treatment of ocular melanomas. OPT clinical workflow is deeply based on x-ray image guidance procedures, both for treatment planning and patient setup verification purposes. An optimized eye orientation relative to the proton beam axis is determined during treatment planning and it is reproduced during treatment by focusing the patient gaze on a fixation light conveniently positioned in space. Treatment geometry verification is routinely performed through stereoscopic radiographic images while real time patient gaze reproducibility is qualitatively monitored by visual control of eye surface images acquired by dedicated optical cameras. We described an approach to quantitatively evaluate the stability of patientsâ€?gaze direction over an OPT treatment course at the National Centre of Oncological Hadrontherapy (Centro Nazionale di Adroterapia Oncologica, CNAO, Pavia, Italy). Pupil automatic segmentation procedure was implemented on eye surveillance videos of five patients recorded during OPT. Automatic pupil detection performance was benchmarked against manual pupil contours of four different clinical operators. Stability of patientsâ€?gaze direction was quantified. 2D distances were expressed as percentage of the reference pupil radius. Valuable approximation between circular fitting and manual contours was observed. Inter-operator manual contours 2D distances were in median (interquartile range) 3.3% (3.6%) of the of the reference pupil radius. The median (interquartile range) of 2D distances between the automatic segmentations and the manual contours was 5.0% (5.3) of the of the reference pupil radius. Stability of gaze direction varied across patients with median values ranging between 6.6% and 16.5% of reference pupil radius. The measured pupil displacement on the camera field of view were clinically acceptable. Further developments are necessary to reach a real-time clip-less quantification of eye during OPT. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-68796-0_27,2021,"Garde G., Larumbe-Bergera A., Porta S., Cabeza R., Villanueva A.",Synthetic Gaze Data Augmentation for Improved User Calibration,"In this paper, we focus on the calibration possibilitiesÃ³ of a deep learning based gaze estimation process applying transfer learning, comparing its performance when using a general dataset versus when using a gaze specific dataset in the pretrained model. Subject calibration has demonstrated to improve gaze accuracy in high performance eye trackers. Hence, we wonder about the potential of a deep learning gaze estimation model for subject calibration employing fine-tuning procedures. A pretrained Resnet-18 network, which has great performance in many computer vision tasks, is fine-tuned using userâ€™s specific data in a few shot adaptive gaze estimation approach. We study the impact of pretraining a model with a synthetic dataset, U2Eyes, before addressing the gaze estimation calibration in a real dataset, I2Head. The results of the work show that the success of the individual calibration largely depends on the balance between fine-tuning and the standard supervised learning procedures and that using a gaze specific dataset to pretrain the model improves the accuracy when few images are available for calibration. This paper shows that calibration is feasible in low resolution scenarios providing outstanding accuracies below 1.5 âˆ?of error. Â© 2021, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-68796-0_26,2021,"Golard A., Talathi S.S.",Ultrasound for Gaze Estimation,"Most eye tracking methods are light-based. As such they can suffer from ambient light changes when used outdoors. It has been suggested that ultrasound could provide a low power, fast, light-insensitive alternative to camera based sensors for eye tracking. We designed a bench top experimental setup to investigate the utility of ultrasound for eye tracking, and collected time of flight and amplitude data for a range of gaze angles of a model eye. We used this data as input for a machine learning model and demonstrate that we can effectively estimate gaze (gaze RMSE error of 1.021 Â± 0.189 âˆ?with an adjusted R2 score of 89.92 Â± 4.9). Â© 2021, Springer Nature Switzerland AG.",,,
10.1109/TITS.2021.3069776,2021,"Zhou F., Yang X.J., de Winter J.C.F.",Using Eye-Tracking Data to Predict Situation Awareness in Real Time During Takeover Transitions in Conditionally Automated Driving,"Situation awareness (SA) is critical to improving takeover performance during the transition period from automated driving to manual driving. Although many studies measured SA during or after the driving task, few studies have attempted to predict SA in real time in automated driving. In this work, we propose to predict SA during the takeover transition period in conditionally automated driving using eye-tracking and self-reported data. First, a tree ensemble machine learning model, named LightGBM (Light Gradient Boosting Machine), was used to predict SA. Second, in order to understand what factors influenced SA and how, SHAP (SHapley Additive exPlanations) values of individual predictor variables in the LightGBM model were calculated. These SHAP values explained the prediction model by identifying the most important factors and their effects on SA, which further improved the model performance of LightGBM through feature selection. We standardized SA between 0 and 1 by aggregating three performance measures (i.e., placement, distance, and speed estimation of vehicles with regard to the ego-vehicle) of SA in recreating simulated driving scenarios, after 33 participants viewed 32 videos with six lengths between 1 and 20 s. Using only eye-tracking data, our proposed model outperformed other selected machine learning models, having a root-mean-squared error (RMSE) of 0.121, a mean absolute error (MAE) of 0.096, and a 0.719 correlation coefficient between the predicted SA and the ground truth. The code is available at https://github.com/refengchou/Situation-awareness-prediction. Our proposed model provided important implications on how to monitor and predict SA in real time in automated driving using eye-tracking data. IEEE",,,
10.1007/s10055-021-00512-7,2021,"Li X., Shan Y., Chen W., Wu Y., Hansen P., Perrault S.",Predicting user visual attention in virtual reality with a deep learning model,"Recent studies show that userâ€™s visual attention during virtual reality museum navigation can be effectively estimated with deep learning models. However, these models rely on large-scale datasets that usually are of high structure complexity and context specific, which is challenging for nonspecialist researchers and designers. Therefore, we present the deep learning model, ALRF, to generalise on real-time user visual attention prediction in virtual reality context. The model combines two parallel deep learning streams to process the compact dataset of temporalâ€“spatial salient features of userâ€™s eye movements and virtual object coordinates. The prediction accuracy outperformed the state-of-the-art deep learning models by reaching record high 91.03%. Importantly, with quick parametric tuning, the model showed flexible applicability across different environments of the virtual reality museum and outdoor scenes. Implications for how the proposed model may be implemented as a generalising tool for adaptive virtual reality application design and evaluation are discussed. Â© 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",,,
10.1109/TCDS.2021.3066465,2021,"Wang X., Zhang J., Zhang H., Zhao S., Liu H.",Vision-based Gaze Estimation: A Review,"Eye gaze is an important natural behavior in social interaction as it delivers complex exchanges between observer and observed, by building up the geometric constraints and relation of the exchanges. These inter-person exchanges can be modeled based on gaze direction estimated using computer vision. Despite significant progresses in vision-based gaze estimation in last 10 years, it is still nontrivial since the accuracy of gaze estimation is significantly affected by such intrinsic factors as head pose variance, individual bias between optical axis and visual axis, eye blink, occlusion and image blur, degrade gaze features, lead to inaccurate gaze-involved human social interaction analysis. This paper aims to review and discuss existing methods addressing above-mentioned problems, gaze involved applications and datasets against the state-of-the-arts in vision-based gaze estimation. It also points out future research directions and challenges of gaze estimation in terms of meta learning, causal inference, disentangled representation, and social gaze behaviour for unconstrained gaze estimation. IEEE",,,
,2021,"Varley P.A.C., Cristina S., Bonnici A., Camilleri K.P.",As plain as the nose on your face?,"We present an investigation into locating nose tips in 2D images of human faces. Our objective is conferenceroom gaze-tracking, in which a presenter can control a presentation or demonstration by gaze from a distance in the range 2m to 10m. In a first step towards this, we here consider faces in the range 150cm to 300cm. Head pose is the major contributing component of gaze direction, and nose tip position within the image of the face is a strong clue to head pose. To facilitate detection of nose tips, we have implemented a combination of two Haar cascades (one for frontal noses and one for profile noses) with a lower failure rate than existing cascades, and we have examined a number of ""hand-crafted ferns""for their potential to locate the nose tip within the nose-like regions returned by our Haar cascades. Copyright Â© 2021 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",,,
,2021,"Ajenaghughrure I.B., Da Costa Sousa S.C., Lamas D.",Psychophysiological modelling of trust in technology: Comparative analysis of psychophysiological signals,"Measuring users trust with psychophysiological signals during interaction (real-time) with autonomous systems that incorporates artificial intelligence has been widely researched with several psychophysiological signals. However, it is unclear what psychophysiological is most reliable for real-time trust assessment during userâ€™s interaction with an autonomous system. This study investigates what psychophysiological signal is most suitable for assessing trust in real-time. A within-subject four condition experiment was implemented with a virtual reality autonomous vehicle driving game that involved 31 carefully selected participants, while electroencephalogram, electrodermal activity, eletrocardiogram, eye-tracking and facial electromyogram psychophysiological signals were acquired. We applied hybrid feature selection methods on the features extracted from the psychophysiological signals. Using training and testing datasets containing only the resulting features from the feature selection methods, for each individual and multi-modal (combined) psychophysiological signals, we trained and tested six stack ensemble trust classifier models. The results of the modelâ€™s performance indicate that the EEG is most reliable, while the multimodal psychophysiological signals remain promising. Copyright Â© 2021 by SCITEPRESS â€?Science and Technology Publications, Lda. All rights reserved.",,,
10.13053/CYS-25-1-3385,2021,"Romaguera T.V., Romaguera L.V., PiÃ±ol D.C., Seisdedos C.R.V.",Pupil center detection approaches: A comparative analysis,"In the last decade, the development of technologies and tools for eye tracking has been a constantly growing area. Detecting the center of the pupil using image processing techniques has been an essential step in this process. A large number of techniques have been proposed for pupil center detection using both traditional image processing and machine learning-based methods. Despite the large number of methods proposed, no comparative work on their performance was found, using the same images and performance metrics. In this work, we aim at comparing four of the most frequently cited traditional methods for pupil center detection in terms of accuracy, robustness, and computational cost. These methods are based on the circular Hough transform, ellipse fitting, Daugman's integro-differential operator and radial symmetry transform. The comparative analysis was performed with 800 infrared images from the CASIAIrisV3 and CASIA-IrisV4 databases containing various types of disturbances. The best performance was obtained by the method based on the radial symmetry transform with an accuracy and average robustness higher than 94%. The shortest processing time, obtained with the ellipse fitting method, was 0.06 s. Â© 2021 Instituto Politecnico Nacional. All rights reserved.",,,
10.1007/978-981-16-0419-5_4,2021,"Krishnan S., Amudha J., Tejwani S.",Gaze Fusion-Deep Neural Network Model for Glaucoma Detection,"The proposed system, Gaze Fusion - Deep Neural Network Model (GFDM) has utilized transfer learning approach to discriminate subjectâ€™s eye tracking data in the form of fusion map into two classes: glaucoma and normal. We have fed eye tracking data in the form of fusion maps of different participants to Deep Neural Network (DNN) model which is pretrained with ImageNet weights. The experimental results of the GFDM show that fusion map dissimilar to pretrained modelâ€™s dataset can give better understanding of glaucoma. The model also show the part of the screen where participants has the difficulty in viewing. GFDM has compared with traditional machine learning models such as Support Vector Classifier, Decision Tree classifier and ensemble classifier and shown that the proposed model outperforms other classifiers. The model has Area Under ROC Curve (AUC) score 0.75. The average sensitivity of correctly identifying glaucoma patients is 100% with specificity value 83%. Â© 2021, Springer Nature Singapore Pte Ltd.",,,
10.1007/978-3-030-67835-7_11,2021,"Lu Y., Wang Y., Xin Y., Wu D., Lu G.",Unsupervised Gaze: Exploration of Geometric Constraints for 3D Gaze Estimation,"Eye gaze estimation can provide critical evidence for people attention, which has extensive applications on cognitive science and computer vision areas, such as human behavior analysis and fake user identification. Existing typical methods mostly place the eye-tracking sensors directly in front of the eyeballs, which is hard to be utilized in the wild. And recent learning-based methods require prior ground truth annotations of gaze vector for training. In this paper, we propose an unsupervised learning-based method for estimating the eye gaze in 3D space. Building on top of the existing unsupervised approach to regress shape parameters and initialize the depth, we propose to apply geometric spectral photometric consistency constraint and spatial consistency constraints across multiple views in video sequences to refine the initial depth values on the detected iris landmark. We demonstrate that our method is able to learn gaze vector in the wild scenes more robust without ground truth gaze annotations or 3D supervision, and show our system leads to a competitive performance compared with existing supervised methods. Â© 2021, Springer Nature Switzerland AG.",,,
10.1109/TNNLS.2021.3055548,2021,"Zhang L., Zhang X., Xu M., Shao L.",Massive-Scale Aerial Photo Categorization by Cross-Resolution Visual Perception Enhancement,"Categorizing aerial photographs with varied weather/lighting conditions and sophisticated geomorphic factors is a key module in autonomous navigation, environmental evaluation, and so on. Previous image recognizers cannot fulfill this task due to three challenges: 1) localizing visually/semantically salient regions within each aerial photograph in a weakly annotated context due to the unaffordable human resources required for pixel-level annotation; 2) aerial photographs are generally with multiple informative attributes (e.g., clarity and reflectivity), and we have to encode them for better aerial photograph modeling; and 3) designing a cross-domain knowledge transferal module to enhance aerial photograph perception since multiresolution aerial photographs are taken asynchronistically and are mutually complementary. To handle the above problems, we propose to optimize aerial photograph's feature learning by leveraging the low-resolution spatial composition to enhance the deep learning of perceptual features with a high resolution. More specifically, we first extract many BING-based object patches (Cheng et al., 2014) from each aerial photograph. A weakly supervised ranking algorithm selects a few semantically salient ones by seamlessly incorporating multiple aerial photograph attributes. Toward an interpretable aerial photograph recognizer indicative to human visual perception, we construct a gaze shifting path (GSP) by linking the top-ranking object patches and, subsequently, derive the deep GSP feature. Finally, a cross-domain multilabel SVM is formulated to categorize each aerial photograph. It leverages the global feature from low-resolution counterparts to optimize the deep GSP feature from a high-resolution aerial photograph. Comparative results on our compiled million-scale aerial photograph set have demonstrated the competitiveness of our approach. Besides, the eye-tracking experiment has shown that our ranking-based GSPs are over 92&#x0025; consistent with the real human gaze shifting sequences. IEEE",,,
10.1109/TITS.2021.3055120,2021,"Amadori P.V., Fischer T., Demiris Y.",HammerDrive: A Task-Aware Driving Visual Attention Model,"We introduce HammerDrive, a novel architecture for task-aware visual attention prediction in driving. The proposed architecture is learnable from data and can reliably infer the current focus of attention of the driver in real-time, while only requiring limited and easy-to-access telemetry data from the vehicle. We build the proposed architecture on two core concepts: 1) driving can be modeled as a collection of sub-tasks (maneuvers), and 2) each sub-task affects the way a driver allocates visual attention resources, i.e., their eye gaze fixation. HammerDrive comprises two networks: a hierarchical monitoring network of forward-inverse model pairs for sub-task recognition and an ensemble network of task-dependent convolutional neural network modules for visual attention modeling. We assess the ability of HammerDrive to infer driver visual attention on data we collected from 20 experienced drivers in a virtual reality-based driving simulator experiment. We evaluate the accuracy of our monitoring network for sub-task recognition and show that it is an effective and light-weight network for reliable real-time tracking of driving maneuvers with above 90&#x0025; accuracy. Our results show that HammerDrive outperforms a comparable state-of-the-art deep learning model for visual attention prediction on numerous metrics with ~13&#x0025; improvement for both Kullback-Leibler divergence and similarity, and demonstrate that task-awareness is beneficial for driver visual attention prediction. Crown",,,
10.1080/13875868.2021.1885411,2021,"De Cock L., Van de Weghe N., Ooms K., Vanhaeren N., Ridolfi M., De Poorter E., De Maeyer P.",Taking a closer look at indoor route guidance; usability study to compare an adapted and non-adapted mobile prototype,"As indoor wayfinding can be very challenging, adapted systems, which adapt the route instruction type, are being developed to facilitate more supportive indoor route guidance. In this study, such a system has been developed based on the results of an online survey. This adapted system was compared with a non-adapted system by use of eye tracking, position tracking, an orientation test and a questionnaire. The results revealed that using symbols instead of photos reduced the imposed cognitive load, while using 3D-simulations instead of photos improved the environmental awareness. This resulted in less wayfinding errors with the adapted system, compared to the non-adapted system. Therefore, the present study provides additional evidence on the benefits of adapted systems for indoor route guidance. Â© 2021 Taylor & Francis.",,,
10.1093/JCDE/QWAA019,2021,"Li T.-H., Suzuki H., Ohtake Y.",Visualization of user's attention on objects in 3d environment using only eye tracking glasses,"Eye tracking technology is widely applied to detect user's attention in a 2D field, such as web page design, package design, and shooting games. However, because our surroundings primarily consist of 3D objects, applications will be expanded if there is an effective method to obtain and display user's 3D gaze fixation. In this research, a methodology is proposed to demonstrate the user's 3D gaze fixation on a digital model of a scene using only a pair of eye tracking glasses. The eye tracking glasses record user's gaze data and scene video. Thus, using image-based 3D reconstruction, a 3D model of the scene can be reconstructed from the frame images; simultaneously, the transformation matrix of each frame image can be evaluated to find 3D gaze fixation on the 3D model. In addition, a method that demonstrates multiple users' 3D gaze fixation on the same digital model is presented to analyze gaze distinction between different subjects. With this preliminary development, this approach shows potential to be applied to a larger environment and conduct a more reliable investigation. Â© 2020 Society for Computational Design and Engineering. All rights reserved.",,,
10.1109/ACCESS.2021.3058664,2021,"Hijazi H., Couceiro R., Castelhano J., De Carvalho P., Castelo-Branco M., Madeira H.",Intelligent biofeedback augmented content comprehension (tellback),"Assessing comprehension difficulties requires the ability to assess cognitive load. Changes in cognitive load induced by comprehension difficulties could be detected with an adequate time resolution using different biofeedback measures (e.g., changes in the pupil diameter). However, identifying the Spatiooral sources of content comprehension difficulties (i.e., when, and where exactly the difficulty occurs in content regions) with a fine granularity is a big challenge that has not been explicitly addressed in the state-of-the-art. This paper proposes and evaluates an innovative approach named Intelligent BiofeedbackAugmented Content Comprehension (TellBack) to explicitly address this challenge. The goal is to autonomously identify regions of digital content that cause user's comprehension difficulty, opening the possibility to provide real-time comprehension support to users. TellBack is based on assessing the cognitive load associated with content comprehension through non-intrusive cheap biofeedback devices that acquire measures such as pupil response or Heart Rate Variability (HRV). To identify when exactly the difficulty in comprehension occurs, physiological manifestations of the Autonomic Nervous System (ANS) such as the pupil diameter variability and the modulation of HRV are exploited, whereas the fine spatial resolution (i.e., the region of content where the user is looking at) is provided by eye-tracking. The evaluation results of this approach show an accuracy of 83.00% Â± 0.75 in classifying regions of content as difficult or not difficult using Support Vector Machine (SVM), and precision, recall, and micro F1-score of 0.89, 0.79, and 0.83, respectively. Results obtained with 4 other classifiers, namely Random Forest, k-nearest neighbor, Decision Tree, and Gaussian Naive Bayes, showed a slightly lower precision. TellBack outperforms the state-of-the-art in precision & recall by 23% and 17% respectively. Â© 2013 IEEE.",,,
10.1109/ICREST51555.2021.9331152,2021,"Akter T., Ali M.H., Khan M.I., Satu M.S., Moni M.A.",Machine Learning Model to Predict Autism Investigating Eye-Tracking Dataset,"Autism spectrum disorder is a neurodevelopmental disorder that characterizes by reducing concentration on social activities and improving interest in non-social tasks. The aim of this work is to investigate eye gazing images and identify autism applying various machine learning techniques. Therefore, we collected eye-tracking data from the Figshare data repository. But, these scanpath images were almost similar for normal and autistic children. To obtain similar groups, k-means clustering method was used and generated four clusters. Further, several classifiers were applied into primary data and these clusters and evaluated the performance of them using various metrics. After the assessment of overall results, MLP shows the highest 87% accuracy in cluster 1. In addition, it shows the best area under curve, f-measure, g-mean, sensitivity, specificity, fall out and miss rate respectively. This predictive model could notably useful to forecast ASD status at early stages. Â© 2021 IEEE.",,,
10.1145/3428121,2021,"Mathis F., Williamson J.H., Vaniea K., Khamis M.",Fast and secure authentication in virtual reality using coordinated 3D manipulation and pointing,"There is a growing need for usable and secure authentication in immersive virtual reality (VR). Established concepts (e.g., 2D authentication schemes) are vulnerable to observation attacks, and most alternatives are relatively slow. We present RubikAuth, an authentication scheme for VR where users authenticate quickly and secure by selecting digits from a virtual 3D cube that leverages coordinated 3D manipulation and pointing. We report on results from three studies comparing how pointing using eye gaze, head pose, and controller tapping impact RubikAuth's usability, memorability, and observation resistance under three realistic threat models. We found that entering a four-symbol RubikAuth password is fast: 1.69-3.5 s using controller tapping, 2.35-4.68 s using head pose and 2.39 -4.92 s using eye gaze, and highly resilient to observations: 96-99.55% of observation attacks were unsuccessful. RubikAuth also has a large theoretical password space: 45n for an n-symbols password. Our work underlines the importance of considering novel but realistic threat models beyond standard one-time attacks to fully assess the observation-resistance of authentication schemes. We conclude with an in-depth discussion of authentication systems for VR and outline five learned lessons for designing and evaluating authentication schemes. Â© 2021 Copyright held by the owner/author(s).",,,
10.1109/TIP.2021.3050861,2021,"Xu M., Yang L., Tao X., Duan Y., Wang Z.",Saliency Prediction on Omnidirectional Image with Generative Adversarial Imitation Learning,"When watching omnidirectional images (ODIs), subjects can access different viewports by moving their heads. Therefore, it is necessary to predict subjects' head fixations on ODIs. Inspired by generative adversarial imitation learning (GAIL), this paper proposes a novel approach to predict saliency of head fixations on ODIs, named SalGAIL. First, we establish a dataset for attention on ODIs (AOI). In contrast to traditional datasets, our AOI dataset is large-scale, which contains the head fixations of 30 subjects viewing 600 ODIs. Next, we mine our AOI dataset and discover three findings: (1) the consistency of head fixations are consistent among subjects, and it grows alongside the increased subject number; (2) the head fixations exist with a front center bias (FCB); and (3) the magnitude of head movement is similar across the subjects. According to these findings, our SalGAIL approach applies deep reinforcement learning (DRL) to predict the head fixations of one subject, in which GAIL learns the reward of DRL, rather than the traditional human-designed reward. Then, multi-stream DRL is developed to yield the head fixations of different subjects, and the saliency map of an ODI is generated via convoluting predicted head fixations. Finally, experiments validate the effectiveness of our approach in predicting saliency maps of ODIs, significantly better than 11 state-of-The-Art approaches. Our AOI dataset and code of SalGAIL are available online at https://github.com/yanglixiaoshen/SalGAIL. Â© 1992-2012 IEEE.",,,
10.26583/SV.12.5.11,2021,Averbukh V.L.,Evolution of human computer interaction,"The work is devoted to the review of the development the human-computer interaction. In the first sections the history of computing in the ""pre-computer"" era is briefly described and then the early history of modern computing, methods of the first computers controlling and the tasks of programmers at this stage are described. It describes the methods of interaction with the first -generation computers using the remote control elements, punched cards and punched tapes. The section, devoted to the second generation computers, describes the emergence of high-level operating systems and programming languages. At this point, there are such means of interaction with the computer as the displays and, respectively, such programming tools as interactive languages and interactive debuggers. Research is also beginning on principles of human-computer interaction the infancy of the discipline ""computer graphics"", the development of computer graphics packages and the emergence of interactive computer graphics standards are considered. In the section â€œRevolutions in computer scienceâ€?describes the appearance of a large number of the same series computers and the first super-computers in the context of human-computer interaction. Revolutionary changes are considered in computer graphics and emerging of the science discipline â€œcomputer visualizationâ€?with its parts â€œscientific visualizationâ€? â€œsoftware visualizationâ€? â€œinformation visualizationâ€?and also â€œprogramming by demonstrationâ€? The information about the attempt to create a fifth generation computer based on logical programming is given. It is told about the initial period of teaching programming. The creation of computer networks and the emergence of personal computing as well as the creation the tools of modern parallel computing have become the important stages in the development of modern computing. The virtual reality becomes an important computer visualization tool. The modern state of human-computer interfaces is characterized primarily by emerging of natural interfaces which can be attributed Brain-Computer Interface (Neurocomputer interface, Brain-Computer Interfaces), interfaces based on the direct use of nerve impulses, speech recognition, recognition of lip movement, mimic recognition and eye tracking (Eye Gaze or Eye Tracking), haptic interfaces and also interfaces giving tactile feedback (allowing you to feel the touch),motion capture interfaces the entire human body or individual organs (head, entire arm, hands, fingers, legs), motion capture toolkits,in particular, interfaces based on leg movements (foot-operated computer interfaces), sign interfaces, sign languages. We briefly describe the activity approach to the design of interfaces and also some problems concerning the problem of mass interfaces. Finally, we discuss a number of problems arising from the increasing capabilities of modern computers. The work is in the nature of a popular science article and it largely reflects the subjective impressions of the author. Â© 2020 National Research Nuclear University. All rights reserved.",,,
10.1109/TPAMI.2019.2929034,2021,"Yang T., Chan A.B.",Visual Tracking via Dynamic Memory Networks,"Template-matching methods for visual tracking have gained popularity recently due to their good performance and fast speed. However, they lack effective ways to adapt to changes in the target object's appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adapt the template to the target's appearance variations during tracking. The reading and writing process of the external memory is controlled by an LSTM network with the search feature map as input. A spatial attention mechanism is applied to concentrate the LSTM input on the potential target as the location of the target is at first unknown. To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template. In order to alleviate the drift problem, we also design a 'negative' memory unit that stores templates for distractors, which are used to cancel out wrong responses from the object template. To further boost the tracking performance, an auxiliary classification loss is added after the feature extractor part. Unlike tracking-by-detection methods where the object's information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target's appearance changes by updating the external memory. Moreover, the capacity of our model is not determined by the network size as with other trackers - the capacity can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Extensive experiments on the OTB and VOT datasets demonstrate that our trackers perform favorably against state-of-the-art tracking methods while retaining real-time speed. Â© 1979-2012 IEEE.",,,
10.1109/TPAMI.2019.2924417,2021,"Wang W., Shen J., Xie J., Cheng M.-M., Ling H., Borji A.",Revisiting Video Saliency Prediction in the Deep Learning Era,"Predicting where people look in static scenes, a.k.a visual saliency, has received significant research interest recently. However, relatively less effort has been spent in understanding and modeling visual attention over dynamic scenes. This work makes three contributions to video saliency research. First, we introduce a new benchmark, called DHF1K (Dynamic Human Fixation 1K), for predicting fixations during dynamic scene free-viewing, which is a long-time need in this field. DHF1K consists of 1K high-quality elaborately-selected video sequences annotated by 17 observers using an eye tracker device. The videos span a wide range of scenes, motions, object types and backgrounds. Second, we propose a novel video saliency model, called ACLNet (Attentive CNN-LSTM Network), that augments the CNN-LSTM architecture with a supervised attention mechanism to enable fast end-to-end saliency learning. The attention mechanism explicitly encodes static saliency information, thus allowing LSTM to focus on learning a more flexible temporal saliency representation across successive frames. Such a design fully leverages existing large-scale static fixation datasets, avoids overfitting, and significantly improves training efficiency and testing performance. Third, we perform an extensive evaluation of the state-of-the-art saliency models on three datasets : DHF1K, Hollywood-2, and UCF sports. An attribute-based analysis of previous saliency models and cross-dataset generalization are also presented. Experimental results over more than 1.2K testing videos containing 400K frames demonstrate that ACLNet outperforms other contenders and has a fast processing speed (40 fps using a single GPU). Our code and all the results are available at https://github.com/wenguanwang/DHF1K. Â© 1979-2012 IEEE.",,,
10.1109/TVCG.2019.2938165,2021,"Wang Z., Chai J., Xia S.",Realtime and Accurate 3D Eye Gaze Capture with DCNN-Based Iris and Pupil Segmentation,"This paper presents a realtime and accurate method for 3D eye gaze tracking with a monocular RGB camera. Our key idea is to train a deep convolutional neural network(DCNN) that automatically extracts the iris and pupil pixels of each eye from input images. To achieve this goal, we combine the power of Unet [1] and Squeezenet [2] to train an efficient convolutional neural network for pixel classification. In addition, we track the 3D eye gaze state in the Maximum A Posteriori (MAP) framework, which sequentially searches for the most likely state of the 3D eye gaze at each frame. When eye blinking occurs, the eye gaze tracker can obtain an inaccurate result. We further extend the convolutional neural network for eye close detection in order to improve the robustness and accuracy of the eye gaze tracker. Our system runs in realtime on desktop PCs and smart phones. We have evaluated our system on live videos and Internet videos, and our results demonstrate that the system is robust and accurate for various genders, races, lighting conditions, poses, shapes and facial expressions. A comparison against Wang et al. [3] shows that our method advances the state of the art in 3D eye tracking using a single RGB camera. Â© 1995-2012 IEEE.",,,
10.1109/TIP.2020.3038356,2021,"Pu S., Song Y., Ma C., Zhang H., Yang M.-H.",Learning recurrent memory activation networks for visual tracking,"Facilitated by deep neural networks, numerous tracking methods have made significant advances. Existing deep trackers mainly utilize independent frames to model the target appearance, while paying less attention to its temporal coherence. In this paper, we propose a recurrent memory activation network (RMAN) to exploit the untapped temporal coherence of the target appearance for visual tracking. We build the RMAN on top of the long short-term memory network (LSTM) with an additional memory activation layer. Specifically, we first use the LSTM to model the temporal changes of the target appearance. Then we selectively activate the memory blocks via the activation layer to produce a temporally coherent representation. The recurrent memory activation layer enriches the target representations from independent frames and reduces the background interference through temporal consistency. The proposed RMAN is fully differentiable and can be optimized end-to-end. To facilitate network training, we propose a temporal coherence loss together with the original binary classification loss. Extensive experimental results on standard benchmarks demonstrate that our method performs favorably against the state-of-the-art approaches. Â© 1992-2012 IEEE.",,,
10.32604/cmc.2020.013249,2021,"Naqvi R.A., Hussain D., Loh W.-K.",Artificial intelligence-based semantic segmentation of ocular regions for biometrics and healthcare applications,"Multiple ocular region segmentation plays an important role in different applications such as biometrics, liveness detection, healthcare, and gaze estimation. Typically, segmentation techniques focus on a single region of the eye at a time. Despite the number of obvious advantages, very limited research has focused on multiple regions of the eye. Similarly, accurate segmentation of multiple eye regions is necessary in challenging scenarios involving blur, ghost effects low resolution, off-angles, and unusual glints. Currently, the available segmentation methods cannot address these constraints. In this paper, to address the accurate segmentation of multiple eye regions in unconstrainted scenarios, a lightweight outer residual encoder-decoder network suitable for various sensor images is proposed. The proposed method can determine the true boundaries of the eye regions from inferior-quality images using the high-frequency information flow from the outer residual encoder-decoder deep convolutional neural network (called ORED-Net). Moreover, the proposed ORED-Net model does not improve the performance based on the complexity, number of parameters or network depth. The proposed network is considerably lighter than previous state-of-theart models. Comprehensive experiments were performed, and optimal performance was achieved using SBVPI and UBIRIS.v2 datasets containing images of the eye region. The simulation results obtained using the proposed OREDNet, with the mean intersection over union score (mIoU) of 89.25 and 85.12 on the challenging SBVPI and UBIRIS.v2 datasets, respectively. Â© 2020 Tech Science Press. All rights reserved.",,,
10.1016/j.compbiomed.2020.104104,2021,"Liu D., Peng X., Liu X., Li Y., Bao Y., Xu J., Bian X., Xue W., Qian D.",A real-time system using deep learning to detect and track ureteral orifices during urinary endoscopy,"Background and objective: To automatically identify and locate various types and states of the ureteral orifice (UO) in real endoscopy scenarios, we developed and verified a real-time computer-aided UO detection and tracking system using an improved real-time deep convolutional neural network and a robust tracking algorithm. Methods: The single-shot multibox detector (SSD) was refined to perform the detection task. We trained both the SSD and Refined-SSD using 447 resectoscopy images with UO and tested them on 818 ureteroscopy images. We also evaluated the detection performance on endoscopy video frames, which comprised 892 resectoscopy frames and 1366 ureteroscopy frames. UOs could not be identified with certainty because sometimes they appeared on the screen in a closed state of peristaltic contraction. To mitigate this problem and mimic the inspection behavior of urologists, we integrated the SSD and Refined-SSD with five different tracking algorithms. Results: When tested on 818 ureteroscopy images, our proposed UO detection network, Refined-SSD, achieved an accuracy of 0.902. In the video sequence analysis, our detection model yielded test sensitivities of 0.840 and 0.922 on resectoscopy and ureteroscopy video frames, respectively. In addition, by testing Refined-SSD on 1366 ureteroscopy video frames, the sensitivity achieved a value of 0.922, and a lowest false positive per image of 0.049 was obtained. For UO tracking performance, our proposed UO detection and tracking system (Refined-SSD integrated with CSRT) performed the best overall. At an overlap threshold of 0.5, the success rate of our proposed UO detection and tracking system was greater than 0.95 on 17 resectoscopy video clips and achieved nearly 0.95 on 40 ureteroscopy video clips. Conclusions: We developed a deep learning system that could be used for detecting and tracking UOs in endoscopy scenarios in real time. This system can simultaneously maintain high accuracy. This approach has great potential to serve as an excellent learning and feedback system for trainees and new urologists in clinical settings. Â© 2020 Elsevier Ltd",,,
10.1080/21681163.2020.1835554,2021,"FÃ©lix I., Raposo C., Antunes M., Rodrigues P., Barreto J.P.",Towards markerless computer-aided surgery combining deep segmentation and geometric pose estimation: application in total knee arthroplasty,"Total knee arthroplasty (TKA) is a surgical procedure performed in patients suffering from knee arthritis. The correct positioning of the implants is strongly related to multiple surgical variables that have a tremendous impact on the success of the surgery. Computer-based navigation systems have been investigated and developed in order to assist the surgeon in accurately controlling those surgical variables. The existing technologies are very costly, require additional bone incisions for fixing markers to be tracked, and these markers are usually bulky, interfering with the standard surgical flow. This work presents a markerless navigation system that supports the surgeon in accurately performing the TKA procedure. The proposed system uses a mobile RGB-D camera for replacing the existing optical tracking systems and does not require markers to be tracked. We combine an effective deep learning-based approach for accurately segmenting the bone surface with a robust geometry-based algorithm for registering the bones with pre-operative models. The favourable performance of our pipeline is achieved by (1) employing a semi-supervised labelling approach for generating training data from real TKA surgery data, (2) using effective data augmentation techniques for improving the generalisation capability and (3) using appropriate depth data cleaning strategies. The construction of this complete markerless registration prototype that generalises for unseen intra-operative data is non-obvious, and relevant insights and future research directions can be derived. The experimental results show encouraging performance for video-based TKA. Â© 2020 Informa UK Limited, trading as Taylor & Francis Group.",,,
10.1007/s11263-020-01371-6,2021,"Jiang L., Xu M., Wang Z., Sigal L.",DeepVS2.0: A Saliency-Structured Deep Learning Method for Predicting Dynamic Visual Attention,"Deep neural networks (DNNs) have exhibited great success in image saliency prediction. However, few works apply DNNs to predict the saliency of generic videos. In this paper, we propose a novel DNN-based video saliency prediction method, called DeepVS2.0. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which provides sufficient data to train the DNN models for predicting video saliency. Through the statistical analysis of LEDOV, we find that human attention is normally attracted by objects, particularly moving objects or the moving parts of objects. Accordingly, we propose an object-to-motion convolutional neural network (OM-CNN) in DeepVS2.0 to learn spatio-temporal features for predicting the intra-frame saliency via exploring the information of both objectness and object motion. We further find from our database that human attention has a temporal correlation with a smooth saliency transition across video frames. Therefore, a saliency-structured convolutional long short-term memory network (SS-ConvLSTM) is developed in DeepVS2.0 to predict inter-frame saliency, using the extracted features of OM-CNN as the input. Moreover, the center-bias dropout and sparsity-weighted loss are embedded in SS-ConvLSTM, to consider the center-bias and sparsity of human attention maps. Finally, the experimental results show that our DeepVS2.0 method advances the state-of-the-art video saliency prediction. Â© 2020, Springer Science+Business Media, LLC, part of Springer Nature.",,,
10.1007/978-3-030-51041-1_55,2021,"Shotton T., Kim J.H.",Assessing Differences on Eye Fixations by Attention Levels in an Assembly Environment,"The purpose of this study is to see how eye movement can be used to determine how much attention a person is paying to a task in an assembly setting. The study uses the Dikablis Eye Tracking Glasses to analyze differences in length and number of eye fixations. Two groups of participants; the high and low attention level groups were compared to understand differences between workers who give full attention and those who do not. The participants who were assigned in the low attention level group had to memorize some given numbers throughout their assembly task while the high attention level group only had to complete the assembly task. According to our results, three of the six areas of interest locations had significantly different eye fixation lengths and one of the six locations had a significantly different number of eye fixations. The findings show that analyzing eye fixation lengths could be a way to measure a workerâ€™s attention level during an assembly task. Â© 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.",,,
10.1007/978-981-15-5784-2_35,2021,"Zhao Z., Nishi Y., Arima S.",Interaction effects of environment and defect features on human cognitions and skills in visual inspections,"This paper discussed the external environmental conditions and humansâ€?internal cognition relating to the visual inspection process of actual mass productions. In visual inspection, human perception and recognition are indispensable to the detection and pass-fail discrimination of the defects, because only those form the discipline of the inspection. First, the effects of the external environment on the defect detection rate were evaluated based on the experimental results. Both defectsâ€?features and environmental factors such as the display luminance and defectsâ€?contrast and size are significant for the peripheral visual inspection. Some of the main effects reported in previous studies were verified again, and new interaction effects and whole factors became clear by quantitative analysis. The second was experiments and analyses of human perception and cognition of actual visual inspection targets. A wearable eye-tracker was used to observe experts and a beginner. The visual inspections by the experts were highly efficient because of their skilled perception at first glance and discrimination based on various industrial knowledge in addition to the defectâ€™s appearance. The experts could stably detect a tiny and low-contrast defect on product images including much disturbing stimulus, and it is thought that their sensitivity and resolution were improved based on â€œattentionâ€?because they answered with high confidence. On the other hand, under the same situation, the number of the beginnerâ€™s focal points much increases, and processing speed deteriorates remarkably. Finally, some suggestions were summarized based on the results of the first and second topics about the environment of the visual inspection to raise the efficiency of defect detection and pass-fail judgement. Â© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2020.",,,
10.1109/TAFFC.2018.2868651,2021,"Xu M., Chen F., Li L., Shen C., Lv P., Zhou B., Ji R.",Bio-Inspired Deep Attribute Learning towards Facial Aesthetic Prediction,"Computational prediction of facial aesthetics has attracted ever-increasing research focus, which has wide range of prospects in multimedia applications. The key challenge lies in extracting discriminative and perception-Aware features to characterize the facial beautifulness. To this end, the existing schemes simply adopt a direct feature mapping, which relies on handcraft-designed low-level features that cannot reflect human-level aesthetic perception. In this paper, we present a systematic framework towards designing biology-inspired, discriminative representation for facial aesthetic prediction. First, we design a group of biological experiments that adopt eye tracker to identify spatial regions of interest during the facial aesthetic judgments of subjects, which forms a Bio-inspired Facial Aesthetic Ontology (Bio-FAO) and is made public available. Second, we adopt the cutting-edge convolutional neural network to train a set of Bio-inspired Attribute features, termed Bio-AttriBank, which forms a mid-level interpretable representation corresponding to the aforementioned Bio-FAO. For a given image, the facial aesthetic prediction is then formulated as a classification problem over the Bio-AttriBank descriptor responses, which well bridges the affective gap, and provides explainable evidences on why/how a face is beautiful or not. We have carried out extensive experiments on both JAFFE and FaceWarehouse datasets, with comparisons to a set of state-of-The-Art and alternative approaches. Superior performance gains in the experiments have demonstrated the merits of the proposed scheme. Â© 2010-2012 IEEE.",,,
10.1109/ICOT51877.2020.9468806,2020,"Fang W., Zhang K.",Real-time object detection of retail products for eye tracking,"Object detection is one important task in automatically analyzing eye tracking video data. This paper presents a real-time object detection method of retail products based on deep learning for eye tracking system. In the proposed approach, an eye tracking based Convolutional Neural Networks is constructed to obtain the feature of original images. Then, a weighted bounding box selection strategy based on gaze location is used for object detection. Besides, parameters are adjusted according to gaze location of eye tracking. Experimental results show that our method can achieve better accuracy for eye tracking than other existing methods in the detection of retail products. Â© 2020 IEEE.",,,
10.1145/3432223,2020,"Ruensuk M., Cheon E., Hong H., Oakley I.",How do you feel online? Exploiting smartphone sensors to detect transitory emotions during social media use,"Emotions are an intrinsic part of the social media user experience that can evoke negative behaviors such as cyberbullying and trolling. Detecting the emotions of social media users may enable responding to and mitigating these problems. Prior work suggests this may be achievable on smartphones: emotions can be detected via built-in sensors during prolonged input tasks. We extend these ideas to a social media context featuring sparse input interleaved with more passive browsing and media consumption activities. To achieve this, we present two studies. In the first, we elicit participant's emotions using images and videos and capture sensor data from a mobile device, including data from a novel passive sensor: its built-in eye-Tracker. Using this data, we construct machine learning models that predict self-reported binary affect, achieving 93.20% peak accuracy. A follow-up study extends these results to a more ecologically valid scenario in which participants browse their social media feeds the study yields high accuracies for both self-reported binary valence (94.16%) and arousal (92.28%). We present a discussion of the sensors, features and study design choices that contribute to this high performance and that future designers and researchers can use to create effective and accurate smartphone-based affect detection systems. Â© 2020 ACM.",,,
10.1109/BIBM49941.2020.9313278,2020,"Anden R., Linstead E.",Predicting eye movement and fixation patterns on scenic images using Machine Learning for Children with Autism Spectrum Disorder,"This study uses eye-tracking experiment data to predict the fixation points for children with Autism Spectrum Disorder (ASD) and Typically Developing (TD) for 14 ASD and 14 TD subjects for 300 scenic images [1]. Based on explanatory Logistic Regression models, it is evident that fixation patterns for both ASD and TD subjects focus on the center of each scenic image. Using gradient boosting the researchers successfully identify 31.7% and 39.5% of all fixation points in the top decile of predicted fixation points for ASD and TD subjects respectively. Results conclude that TD subjects have less variability in their eye movement and fixation points leading to increased accuracy in predicting where they will look. Â© 2020 IEEE.",,,
10.1109/ICIDM51048.2020.9339629,2020,"Jatmiko A.S., Ginalih C.T., Darmakusuma R.",Evaluation of Emotional Meaning of Eyelid Position on a 3D Animatronic Eyes,"Emotion is a complex thing in humans, there are many ways to express emotions including eye gaze. Ekman, et al. describing abstractions regarding core features on human faces adopted by Onchi E, et al. through single-eyed 2D avatar that is designed that only move the upper and lower eyelids only. Adopting a 2D single-eye avatar, this study evaluated the similarity of human emotions expressed using 3D two-eye animatronic models with stiff eyelids shared by Will Cogley. Simulation in the form of a survey to evaluate the degree of similarity of the seven types of emotions described by Ekman, et al. i.e. neutral, happy, surprised, sad, scared, angry and disgusted, given to 40 participants with a variety of different backgrounds. The result is that the sample of emotions displayed had a significant effect on the participants' perceptions, each sample was also able to display the meaning of emotions well because there was a significant effect on the interaction between the sample and emotions (p = 5.63 Ã— 10-17). Based on these results, the participants had almost similar perceptions of the eyelids with physical embodiment and virtual agents so that these results could be mutually reinforcing. Compared with the results of research using Probo, emotions can be expressed better when facial features such as eyebrows, eyelids and mouth are present. The conclusion of this research is expected to be a step to find out the function of each facial feature that contributes to express emotions. Â© 2020 IEEE.",,,
10.1109/INDICON49873.2020.9342348,2020,"Yarlagadda V., Koolagudi S.G., Kumar M V M., Donepudi S.",Driver Drowsiness Detection Using Facial Parameters and RNNs with LSTM,"The drowsiness is an intermediate state between awake and sleep, in which the observation and analysis of a conductor is very small. The lack of concentration due to the driver fatigue is a major cause that leads to the high number of accidents. In this work, an effort has been put to detect the state of drowsiness using facial parameters obtained using facial points. Moreover, the parameters related to eye and mouth organs have also been extracted. Deep neural networks are outperforming when compared to many state-of-the art algorithms. Hence, recurrent neural networks (RNNs) and long short-term memory (LSTM) units are considered to estimate the drowsiness level of a driver. It is found that they are very appropriate in processing of sequential multimedia data. An accuracy of 97.25% is obtained with the proposed approach. Â© 2020 IEEE.",,,
10.1109/INDICON49873.2020.9342182,2020,"Prava Roy A., Kumar Koley S., Garain U.",Eyes speak out Mind: Deep models for Gaze-based Analysis of Bilingual and Monolingual Reading,"This paper presents an approach that attempts to explore effects of bilingualism in reading by analysis of eye-gaze data. As readers are more comfortable in reading first language (L1) compared to second language (L2), their eyes move differently in these two cases. Being motivated by this hypothesis, a deep learning model is developed to predict whether a reader is reading first or second language by analyzing the reading behavior given by the eye-tracking data. Exposure to different languages simultaneously may influence the proficiency in native language reading and this matter is also investigated by using deep learning model which can differentiate monolingual readers from bilingual readers analyzing their eye movement during reading in their respective native language. Experiments are conducted on GECO Corpus [1] which provides eye-tracking data of 19 readers reading a novel in L1 and L2 along with eye-tracking data of several monolingual readers reading same text in their native language. Experimental results show that the proposed models are quite efficient in capturing the differences in eye gaze pattern for reading L1 and L2 for a reader as well as in identifying monolingual and bilingual readers by processing eye-gaze data. Â© 2020 IEEE.",,,
,2020,"Ahsan Z., Obaidellah U.",Predicting expertise among novice programmers with prior knowledge on programming tasks,"The studies on program comprehension have seen developments over the years from the cognitive science perspective. As eye-tracking technology has proven to analyze visual attention and gaze-performance, it has then been largely used in the program comprehension studies to help understand the underlying cognitive processes among the participants. In this research work, we conducted an experiment using common fundamental programming questions on 66 undergraduate computer science students to study the gaze-behavior among the high and low-performing participants on programming comprehension. We aim to better understand the differences in the time taken by the individuals in terms of their performance with existing prior knowledge and use machine learning to predict their expertise. Findings from this study suggest that mental schemas do play a role as the high performers demonstrated less time taken to attempt the questions than the low performers and machine learning algorithms were able to successfully predict their expertise. The conclusions drawn are supported by eye-tracking metrics across individual- and group-levels. Â© 2020 APSIPA.",,,
10.1109/ICSP48669.2020.9321075,2020,"Melesse D., Khalil M., Kagabo E., Ning T., Huang K.",Appearance-Based Gaze Tracking through Supervised Machine Learning,"Applications that use human gaze have become increasingly more popular in the domain of human-computer interfaces, and advances in eye gaze tracking technology over the past few decades have led to the development of promising gaze estimation techniques. In this paper, a low-cost, in-house video camera-based gaze tracking system was developed, trained and evaluated. Seminal gaze detection methods constrained the application space to indoor conditions, and in most cases techniques required intrusive hardware. More modern gaze detection techniques try to eliminate the use of any additional hardware to reduce monetary cost as well as undue burden to the user, all the while maintaining accuracy of detection. In this work, image acquisition was achieved using a low-cost USB web camera mounted at a fixed position on the viewing screen or laptop. In order to determine the point of gaze, the Viola Jones face detection algorithm is used to extract facial features from the image frame. The gaze is then calculated using image processing techniques to extract gaze features, namely related to the image position of the pupil. Thousands of images are classified and labeled to form an in-house database. A multi-class Support Vector Machine (SVM) was trained and tested on this data set to distinguish point of gaze from input face image. Cross validation was used to train the model. Confusion matrices, accuracy, precision, and recall are used to evaluate the performance of the classification model. Evaluation of the proposed appearance-based technique using two different kernel functions is also assessed in detail. 2020 IEEE.",,,
10.1145/3440054.3440058,2020,"Zhang Y., Yang X., Ma Z.",Driver's Gaze Zone Estimation Method: A Four-channel Convolutional Neural Network Model,"Driver's gaze has become an important indicator to analysis driving state. By estimating the gaze zone of drivers, we can further judge their fatigue state and even predict their driving intention in the next step. In this paper, we propose a four-channel gaze estimation model based on Convolutional Neural Network (CNN), which is used to estimate the gaze zones of the driver. In the proposed method, the images of the right eye, the left eye, the face, and the head are used as the input data of the multi-channel CNN. Then, the features of different channels are fused to estimate the gaze zone. Finally, we compared our method with several existing methods, and the experimental results show that the accuracy of our method is 96%. Â© 2020 ACM.",,,
10.1109/ICIDDT52279.2020.00077,2020,"Li S., Wang J., Meng X., Ji B.",A Contrastive Analysis on Effect Evaluation of 3D-to-2D Rendering Technologies for Animated Characters,"With the rapid development of computer technologies and arts, the 3D-to-2D rendering technologies have been extensively applied to the animation industry. This thesis makes a contrastive analysis on the applications and their effects of two 3D-to-2D rendering technologies in the performance of animated characters. The eye movement data of the subjects were recorded by Tobii Pro XL eye tracker, and the subjective evaluation scores of the two kinds of 3D-to-2D rendering images of animated characters, the characteristics of eye movement data and the relationship between the two were analyzed. The results showed that: there were significant differences in subjective evaluation scores between the two technical schemes; subjective evaluation scores were closely related to the concentration trend of fixation points; the scores of each factor were significantly different; eye movement data such as fixation time and fixation points in the region of interest were significantly correlated with subjective scoring behavior of subjects. These subjective and objective indicators can be integrated to promote effect evaluation more objectively and guide the creation direction of 3D-to-2D rendering technical modeling of animated characters. Â© 2020 IEEE.",,,
10.1109/AIVR50618.2020.00012,2020,"Delvigne V., Wannous H., Vandeborre J.-P., Ris L., Dutoit T.",Attention Estimation in Virtual Reality with EEG based Image Regression,"Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental disorder affecting a certain amount of children and their way of living. A novel method to treat this disorder is to use Brain-Computer Interfaces (BCI) throughout the patient learns to self-regulate his symptoms by herself. In this context, researches have led to tools aiming to estimate the attention toward these interfaces. In parallel, the democratization of virtual reality (VR) headset, and the fact that it produces valid environments for several aspects: safe, flexible and ecologically valid have led to an increase of its use for BCI application. Another point is that Artificial Intelligence (AI) is more and more developed in different domain among which medical application. In this paper, we present an innovative method aiming to estimate attention from the measurement of physiological signals: Electroencephalogram (EEG), gaze direction and head movement. This framework is developed to assess attention in VR environments. We propose a novel approach for feature extraction and a dedicated Machine Learning model. The pilot study has been applied on a set of volunteer and our approach presents a lower error rate in comparison with the state of the art methods. Â© 2020 IEEE.",,,
10.1109/SSCI47803.2020.9308238,2020,"Elbattah M., Guerin J.-L., Carette R., Cilia F., Dequen G.",NLP-Based Approach to Detect Autism Spectrum Disorder in Saccadic Eye Movement,"Autism Spectrum Disorder (ASD) is a lifelong condition generally characterized by social and communication impairments. The early diagnosis of ASD is highly desirable, yet it could be complicated by several factors. Standard tests typically require intensive efforts and experience, which calls for developing assistive tools. In this respect, this study aims to develop a Machine Learning-based approach to assist the diagnosis process. Our approach is based on learning the sequence-based patterns in the saccadic eye movements. The key idea is to represent eye-tracking records as textual strings describing the sequences of fixations and saccades. As such, the study could borrow Natural Language Processing (NLP) methods for transforming the raw eye-tracking data. The NLP-based transformation could yield interesting features for training classification models. The experimental results demonstrated that such representation could be beneficial in this regard. With standard ConvNet models, our approach could realize a promising accuracy of classification (ROC-AUC up to 0.84). Â© 2020 IEEE.",,,
10.1115/1.4048410,2020,"Mehta P., Malviya M., McComb C., Manogharan G., Berdanier C.G.P.",Mining design heuristics for additive manufacturing via eye-tracking methods and hidden Markov modeling,"In this research, we collected eye-tracking data from nine engineering graduate students as they redesigned a traditionally manufactured part for additive manufacturing (AM). Final artifacts were assessed for manufacturability and quality of final design, and design behaviors were captured via the eye-tracking data. Statistical analysis of design behavior duration shows that participants with more than 3 years of industry experience spend significantly less time removing material and revising than those with less experience. Hidden Markov modeling (HMM) analysis of the design behaviors gives insight to the transitions between behaviors through which designers proceed. Findings show that high-performing designers proceeded through four behavioral states, smoothly transitioning between states. In contrast, low-performing designers roughly transitioned between states, with moderate transition probabilities back and forth between multiple states. Copyright Â© 2020 by ASME.",,,
10.1109/JBHI.2020.2984483,2020,"Hernandez-Matas C., Zabulis X., Argyros A.A.",REMPE: Registration of Retinal Images through Eye Modelling and Pose Estimation,"Objective: In-vivo assessment of small vessels can promote accurate diagnosis and monitoring of diseases related to vasculopathy, such as hypertension and diabetes. The eye provides a unique, open, and accessible window for directly imaging small vessels in the retina with non-invasive techniques, such as fundoscopy. In this context, accurate registration of retinal images is of paramount importance in the comparison of vessel measurements from original and follow-up examinations, which is required for monitoring the disease and its treatment. At the same time, retinal registration exhibits a range of challenges due to the curved shape of the retina and the modification of imaged tissue across examinations. Thereby, the objective is to improve the state-of-the-art in the accuracy of retinal image registration. Method: In this work, a registration framework that simultaneously estimates eye pose and shape is proposed. Corresponding points in the retinal images are utilized to solve the registration as a 3D pose estimation. Results: The proposed framework is evaluated quantitatively and shown to outperform state-of-the-art methods in retinal image registration for fundoscopy images. Conclusion: Retinal image registration methods based on eye modelling allow to perform more accurate registration than conventional methods. Significance: This is the first method to perform retinal image registration combined with eye modelling. The method improves the state-of-the-art in accuracy of retinal registration for fundoscopy images, quantitatively evaluated in benchmark datasets annotated with ground truth. The implementation of registration method has been made publicly available. Â© 2013 IEEE.",,,
10.1109/JBHI.2020.2999567,2020,"Chesley B., Barbour D.L.",Visual Field Estimation by Probabilistic Classification,"The gold standard clinical tool for evaluating visual dysfunction in cases of glaucoma and other disorders of vision remains the visual field or threshold perimetry exam. Administration of this exam has evolved over the years into a sophisticated, standardized, automated algorithm that relies heavily on specifics of disease processes particular to common retinal disorders. The purpose of this study is to evaluate the utility of a novel general estimator applied to visual field testing. A multidimensional psychometric function estimation tool was applied to visual field estimation. This tool is built on semiparametric probabilistic classification rather than multiple logistic regression. It combines the flexibility of nonparametric estimators and the efficiency of parametric estimators. Simulated visual fields were generated from human patients with a variety of diagnoses, and the errors between simulated ground truth and estimated visual fields were quantified. Error rates of the estimates were low, typically within 2 dB units of ground truth on average. The greatest threshold errors appeared to be confined to the portions of the threshold function with the highest spatial frequencies. This method can accurately estimate a variety of visual field profiles with continuous threshold estimates, potentially using a relatively small number of stimuli. Â© 2013 IEEE.",,,
10.1049/trit.2020.0068,2020,"Juang L.-H., Wu M.-N., Lin C.-H.",Affective computing study of attention recognition for the 3D guide system,"The eye-tracking has been widely used in multiple discipline studies in recent years. However, most of the studies focused on the analysis for static images or text but less for highly interactive operation application. In addition, the affective computing rose and development in recent years have changed completely the design of thinking pattern for the human-computer interaction. Therefore, this study hopes to integrate the affective computing into the 3D guide system which was developed for the real campus through the eye-tracking technology. The analysing user's gaze position and recognising attention emotion are according to the interest region which is stetted into the environment, and shows the feedback content corresponds to the area and the emotion. Through the most intuitive gaze analysis, the operation burden can be reduced and the user's interactive experience can be improved to achieve intuitive and user-friendly experience. The results can also apply for medical therapy on human attention training. Â© IET 2020. All rights reserved.",,,
10.1109/TVCG.2020.3023567,2020,"Biener V., Schneider D., Gesslein T., Otte A., Kuth B., Kristensson P.O., Ofek E., Pahud M., Grubert J.",Breaking the Screen: Interaction across Touchscreen Boundaries in Virtual Reality for Mobile Knowledge Workers,"Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations, such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices, such as tablets, limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces. In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR; 2) design and evaluate a set of interaction concepts; and 3) build example applications and gather user feedback on those applications. Â© 1995-2012 IEEE.",,,
10.1109/ICARM49381.2020.9195348,2020,"Odekhe R., Cao Q., Jing S.M.",Gaze Teleoperation of a Surgical Robot Endoscope for Minimal Invasive Surgery,This paper presents an innovative approach for controlling an endoscope during robot assisted minimally invasive surgery through the gaze of the surgeon. This system incorporates an eye tracking device which captures the 2D gaze fixation of the surgeon. An event detection algorithm is used in discriminating the fixations and these are sent as UDP to an Arduino microcontroller device where it is used to control two servo motors. The endoscope is attached to the two servos which generates it motion in two different planes which corresponds to the target gaze coordinates of the endoscope fixation point. Experimental results show the effectiveness and robustness of the gaze-based system in intuitively controlling the endoscope. Â© 2020 IEEE.,,,
10.1016/j.neunet.2020.09.011,2020,"Li X., Liu Q., Fan N., Zhou Z., He Z., Jing X.-Y.",Dual-regression model for visual tracking,"Existing regression based tracking methods built on correlation filter model or convolution model do not take both accuracy and robustness into account at the same time. In this paper, we propose a dual-regression framework comprising a discriminative fully convolutional module and a fine-grained correlation filter component for visual tracking. The convolutional module trained in a classification manner with hard negative mining ensures the discriminative ability of the proposed tracker, which facilitates the handling of several challenging problems, such as drastic deformation, distractors, and complicated backgrounds. The correlation filter component built on the shallow features with fine-grained features enables accurate localization. By fusing these two branches in a coarse-to-fine manner, the proposed dual-regression tracking framework achieves a robust and accurate tracking performance. Extensive experiments on the OTB2013, OTB2015, and VOT2015 datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods. Â© 2020 Elsevier Ltd",,,
10.1186/s40537-020-00322-9,2020,"Zheng L.J., Mountstephens J., Teo J.",Four-class emotion classification in virtual reality using pupillometry,"Background: Emotion classification remains a challenging problem in affective computing. The large majority of emotion classification studies rely on electroencephalography (EEG) and/or electrocardiography (ECG) signals and only classifies the emotions into two or three classes. Moreover, the stimuli used in most emotion classification studies utilize either music or visual stimuli that are presented through conventional displays such as computer display screens or television screens. This study reports on a novel approach to recognizing emotions using pupillometry alone in the form of pupil diameter data to classify emotions into four distinct classes according to Russellâ€™s Circumplex Model of Emotions, utilizing emotional stimuli that are presented in a virtual reality (VR) environment. The stimuli used in this experiment are 360Â° videos presented using a VR headset. Using an eye-tracker, pupil diameter is acquired as the sole classification feature. Three classifiers were used for the emotion classification which are Support Vector Machine (SVM), k-Nearest Neighbor (KNN), and Random Forest (RF). Findings: SVM achieved the best performance for the four-class intra-subject classification task at an average of 57.05% accuracy, which is more than twice the accuracy of a random classifier. Although the accuracy can still be significantly improved, this study reports on the first systematic study on the use of eye-tracking data alone without any other supplementary sensor modalities to perform human emotion classification and demonstrates that even with a single feature of pupil diameter alone, emotions could be classified into four distinct classes to a certain level of accuracy. Moreover, the best performance for recognizing a particular class was 70.83%, which was achieved by the KNN classifier for Quadrant 3 emotions. Conclusion: This study presents the first systematic investigation on the use of pupillometry as the sole feature to classify emotions into four distinct classes using VR stimuli. The ability to conduct emotion classification using pupil data alone represents a promising new approach to affective computing as new applications could be developed using readily-available webcams on laptops and other mobile devices that are equipped with cameras without the need for specialized and costly equipment such as EEG and/or ECG as the sensor modality. Â© 2020, The Author(s).",,,
10.1186/s40561-020-00122-x,2020,"Sharma K., Giannakos M., Dillenbourg P.",Eye-tracking and artificial intelligence to enhance motivation and learning,"The interaction with the various learners in a Massive Open Online Course (MOOC) is often complex. Contemporary MOOC learning analytics relate with click-streams, keystrokes and other user-input variables. Such variables however, do not always capture usersâ€?learning and behavior (e.g., passive video watching). In this paper, we present a study with 40 students who watched a MOOC lecture while their eye-movements were being recorded. We then proposed a method to define stimuli-based gaze variables that can be used for any kind of stimulus. The proposed stimuli-based gaze variables indicate studentsâ€?content-coverage (in space and time) and reading processes (area of interest based variables) and attention (i.e., with-me-ness), at the perceptual (following teacherâ€™s deictic acts) and conceptual levels (following teacher discourse). In our experiment, we identified a significant mediation effect of the content coverage, reading patterns and the two levels of with-me-ness on the relation between studentsâ€?motivation and their learning performance. Such variables enable common measurements for the different kind of stimuli present in distinct MOOCs. Our long-term goal is to create student profiles based on their performance and learning strategy using stimuli-based gaze variables and to provide students gaze-aware feedback to improve overall learning process. One key ingredient in the process of achieving a high level of adaptation in providing gaze-aware feedback to the students is to use Artificial Intelligence (AI) algorithms for prediction of student performance from their behaviour. In this contribution, we also present a method combining state-of-the-art AI technique with the eye-tracking data to predict student performance. The results show that the student performance can be predicted with an error of less than 5%. Â© 2020, The Author(s).",,,
10.1145/3384419.3430774,2020,"Lan G., Heit B., Scargill T., Gorlatova M.",GazeGraph: Graph-based few-shot cognitive context sensing from human visual behavior,"In this work, we present GazeGraph, a system that leverages human gazes as the sensing modality for cognitive context sensing. GazeGraph is a generalized framework that is compatible with different eye trackers and supports various gaze-based sensing applications. It ensures high sensing performance in the presence of heterogeneity of human visual behavior, and enables quick system adaptation to unseen sensing scenarios with few-shot instances. To achieve these capabilities, we introduce the spatial-temporal gaze graphs and the deep learning-based representation learning method to extract powerful and generalized features from the eye movements for context sensing. Furthermore, we develop a few-shot gaze graph learning module that adapts the 'learning to learn' concept from meta-learning to enable quick system adaptation in a data-efficient manner. Our evaluation demonstrates that GazeGraph outperforms the existing solutions in recognition accuracy by 45% on average over three datasets. Moreover, in few-shot learning scenarios, GazeGraph outperforms the transfer learning-based approach by 19% to 30%, while reducing the system adaptation time by 80%. Â© 2020 ACM.",,,
10.1145/3406499.3415073,2020,"Bourguet M.-L., Xu M., Zhang S., Urakami J., Venture G.",The Impact of a Social Robot Public Speaker on Audience Attention,"Social robots acting as stand-ins for speakers or teachers would enable them to reach large audiences from anywhere in the world, increasing the options for distant learning. They would need to be endowed with effective public speaking skills though, in order to deliver their message, entertain, and maintain audience attention. In this paper, we report two user studies to understand the impact of a social robot public speaker on its audience and compare it to a skilled human speaker. The first study uses an in-house audience attention monitoring system based on computer vision and machine learning; the second study uses eye tracking technology. For both studies, we programmed the social robot Pepper to deliver a short speech in its robotic voice while mimicking the behaviour of a skilled human speaker (gestures and body movements). We found that, when the audience has a genuine interest in the speech, the robot manages to maintain audience attention level as well as the human speaker but fails to arouse interest as much. When the audience is not particularly interested in the speech, the human speaker is better at maintaining audience attention: the novelty of the robot does not compensate for the lack of interest in the speech content, and the robot's behaviour is found to be distracting. Finally, understanding of the speech content is significantly lower for the robot audience. It could be linked to lower audience attention levels, to the robot's lack of facial expressions and failure to convey enthusiasm, or to a feeling that the robot is not legitimate to speak about the topics touched on in the speeches. Â© 2020 ACM.",,,
10.1109/ICSET51301.2020.9265364,2020,"Abdeltawab A., Ahmad A.",Classification of motor imagery EEG signals using machine learning,"Brain Computer Interface (BCI) is a term that was first introduced by Jacques Vidal in the 1970s when he created a system that can determine the human eye gaze direction, making the system able to determine the direction a person want to go or move something to using scalp-recorded visual evoked potential (VEP) over the visual cortex. Ever since that time, many researchers where captivated by the huge potential and list of possibilities that can be achieved if simply a digital machine can interpret human thoughts. In this work, we explore electroencephalography (EEG) signal classification, specifically for motor imagery (MI) tasks. Classification of MI tasks can be carried out by using machine learning and deep learning models, yet there is a trade between accuracy and computation time that needs to be maintained. The objective is to create a machine learning model that can be optimized for real-time classification while having a relatively acceptable classification accuracy. The proposed model relies on common spatial patter (CSP) for feature extraction as well as linear discriminant analysis (LDA) for classification. With simple pre-processing stage and a proper selection of data for training the model proved to have a balanced accuracy of +80% while maintaining small run-time (milliseconds) that is opted for real-time classifications. Â© 2020 IEEE",,,
10.1145/3424616.3424716,2020,"DueÃ±as K., Naderi E., Kwon J.",Investigating the Influence of 2-D presentation versus 3-D rotation presentation formats on user perception,"When presenting a product through digital media, both designers and retailers aim to communicate with their audience in the most effective way using visual perception. Previous studies have found that a users' perception and behavior are affected by the visual presentation. The purpose of this study is to discover how a users' perception and evaluation of a product change depending on how the product is presented. In this study, we specifically look at the presentation formats of 2 - D orthographic views and 3 - D simulated format (360 rotation). This study employs a between-subject design with surveys and an eye - tracking test to determine if 2 - D presentation or 3 - D presentation resulted in better user evaluation and higher approval to the product design. We found that users have a better understanding of product aesthetics, usefulness, and product ease of use when they were viewing 2 - D orthographic images of products versus when they were viewing the 3 - D 360 rotation presentation format. Future marketing, design, and theoretical implications as well as future research directions are discussed. Â© 2020 Owner/Author.",,,
10.1109/CAC51589.2020.9327066,2020,"Li Y., Niu J., Cai Z.",Evaluating Focused Attention Level Using Mouse Operation Data,"Focused Attention Level (FAL) is important in many tasks such as driving, online learning, and many computer-related operations. Previous approaches to evaluation of FAL usually involve eye-tracking and electroencephalogram which require dedicated devices to track the movements of eyes or record the signals from the brain. In this paper, we propose a novel method of evaluation of FAL only using a user's mouse operation data which are easily available in computers equipped with a mouse device. We perform our study using the Schulte Table which is a popular task designed to improve the focused attention levels of children and students. We extract features from mouse operation data in completing a Schulte Table task and establish a machine learning model to predict the focused attention level of the subject. Experimental results show that the accuracy of our method can reach 80.9%. Â© 2020 IEEE.",,,
10.1109/ISETC50328.2020.9301091,2020,"Bublea A., Caleanu C.D.",Deep Learning based Eye Gaze Tracking for Automotive Applications: An Auto-Keras Approach,"We propose a deep neural network-based gaze sensing method in which the design of the neural architecture is performed automatically, through a network architecture search algorithm called Auto-Keras. First, the neural model is generated using the Columbia Gaze Data Set. Then, the performance of the solution is estimated on an online scenario and proves the generalization ability of our model. In comparison to a geometrical approach, which uses dlib facial landmarks, filtering and morphological operators for gaze estimation, the proposed method provides superior results and certain advantages. Â© 2020 IEEE.",,,
10.1109/FG47880.2020.00125,2020,"Famadas J., Madadi M., Palmero C., Escalera S.",Generative Video Face Reenactment by AUs and Gaze Regularization,"In this work, we propose an encoder-decoder-like architecture to perform face reenactment in image sequences. Our goal is to transfer the training subject identity to a given test subject. We regularize face reenactment by facial action unit intensity and 3D gaze vector regression. This way, we enforce the network to transfer subtle facial expressions and eye dynamics, providing a more lifelike result. The proposed encoder-decoder receives as input the previous sequence frame stacked to the current frame image of facial landmarks. Thus, the generated frames benefit from appearance and geometry, while keeping temporal coherence for the generated sequence. At test stage, a new target subject with the facial performance of the source subject and the appearance of the training subject is reenacted. Principal component analysis is applied to project the test subject geometry to the closest training subject geometry before reenactment. Evaluation of our proposal shows faster convergence, and more accurate and realistic results in comparison to other architectures without action units and gaze regularization. Â© 2020 IEEE.",,,
10.1109/FG47880.2020.00039,2020,"Yvinec E., Dapogny A., Bailly K.",DeeSCo: Deep heterogeneous ensemble with Stochastic Combinatory loss for gaze estimation,"From medical research to gaming applications, gaze estimation is becoming a valuable tool. While there exists a number of hardware-based solutions, recent deep learningbased approaches, coupled with the availability of large-scale databases, have allowed to provide a precise gaze estimate using only consumer sensors. However, there remains a number of questions, regarding the problem formulation, architectural choices and learning paradigms for designing gaze estimation systems in order to bridge the gap between geometry-based systems involving specific hardware and approaches using consumer sensors only. In this paper, we introduce a deep, end-toend trainable ensemble of heatmap-based weak predictors for 2D/3D gaze estimation. We show that, through heterogeneous architectural design of these weak predictors, we can improve the decorrelation between the latter predictors to design more robust deep ensemble models. Furthermore, we propose a stochastic combinatory loss that consists in randomly sampling combinations of weak predictors at train time. This allows to train better individual weak predictors, with lower correlation between them. This, in turns, allows to significantly enhance the performance of the deep ensemble. We show that our Deep heterogeneous ensemble with Stochastic Combinatory loss (DeeSCo) outperforms state-of-the-art approaches for 2D/3D gaze estimation on multiple datasets. Â© 2020 IEEE.",,,
10.1109/ISMAR-Adjunct51615.2020.00071,2020,"Wang J., Liang H.-N., Monteiro D.V., Xu W., Chen H., Chen Q.",Real-Time Detection of Simulator Sickness in Virtual Reality Games Based on Players' Psychophysiological Data during Gameplay,"Virtual Reality (VR) technology has been proliferating in the last decade, especially in the last few years. However, Simulator Sickness (SS) still represents a significant problem for its wider adoption. Currently, the most common way to detect SS is using the Simulator Sickness Questionnaire (SSQ). SSQ is a subjective measurement and is inadequate for real-Time applications such as VR games. This research aims to investigate how to use machine learning techniques to detect SS based on in-game characters' and users' physiological data during gameplay in VR games. To achieve this, we designed an experiment to collect such data with three types of games. We trained a Long Short-Term Memory neural network with the dataset eye-Tracking and character movement data to detect SS in real-Time. Our results indicate that, in VR games, our model is an accurate and efficient way to detect SS in real-Time. Â© 2020 IEEE.",,,
10.1109/ISMAR50242.2020.00033,2020,"Li R., Whitmire E., Stengel M., Boudaoud B., Kautz J., Luebke D., Patel S., Aksit K.",Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors,"Gaze tracking is an essential component of next generation displays for virtual reality and augmented reality applications. Traditional camera-based gaze trackers used in next generation displays are known to be lacking in one or multiple of the following metrics: power consumption, cost, computational complexity, estimation accuracy, latency, and form-factor. We propose the use of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to traditional camera-based gaze tracking approaches while taking all of these metrics into consideration. We begin by developing a rendering-based simulation framework for understanding the relationship between light sources and a virtual model eyeball. Findings from this framework are used for the placement of LEDs and photodiodes. Our first prototype uses a neural network to obtain an average error rate of 2. 67^{\circ} at 400 Hz while demanding only 16 mW. By simplifying the implementation to using only LEDs, duplexed as light transceivers, and more minimal machine learning model, namely a light-weight supervised Gaussian process regression algorithm, we show that our second prototype is capable of an average error rate of 1. 57^{\circ} at 250 Hz using 800 mW. Â© 2020 IEEE.",,,
10.1145/3357459,2020,"Taib R., Berkovsky S., Koprinska I., Wang E., Zeng Y., Li J.",Personality Sensing: Detection of Personality Traits Using Physiological Responses to Image and Video Stimuli,"Personality detection is an important task in psychology, as different personality traits are linked to different behaviours and real-life outcomes. Traditionally it involves filling out lengthy questionnaires, which is time-consuming, and may also be unreliable if respondents do not fully understand the questions or are not willing to honestly answer them. In this article, we propose a framework for objective personality detection that leverages humans' physiological responses to external stimuli. We exemplify and evaluate the framework in a case study, where we expose subjects to affective image and video stimuli, and capture their physiological responses using non-invasive commercial-grade eye-tracking and skin conductivity sensors. These responses are then processed and used to build a machine learning classifier capable of accurately predicting a wide range of personality traits. We investigate and discuss the performance of various machine learning methods, the most and least accurately predicted traits, and also assess the importance of the different stimuli, features, and physiological signals. Our work demonstrates that personality traits can be accurately detected, suggesting the applicability of the proposed framework for robust personality detection and use by psychology practitioners and researchers, as well as designers of personalised interactive systems. Â© 2020 ACM.",,,
10.1145/3385956.3418962,2020,"Pfeuffer K., Mecke L., Delgado Rodriguez S., Hassib M., Maier H., Alt F.",Empirical Evaluation of Gaze-enhanced Menus in Virtual Reality,"Many user interfaces involve attention shifts between primary and secondary tasks, e.g., when changing a mode in a menu, which detracts the user from their main task. In this work, we investigate how eye gaze input affords exploiting the attention shifts to enhance the interaction with handheld menus. We assess three techniques for menu selection: dwell time, gaze button, and cursor. Each represents a different multimodal balance between gaze and manual input. We present a user study that compares the techniques against two manual baselines (dunk brush, pointer) in a compound colour selection and line drawing task. We show that user performance with the gaze techniques is comparable to pointer-based menu selection, with less physical effort. Furthermore, we provide an analysis of the trade-off as each technique strives for a unique balance between temporal, manual, and visual interaction properties. Our research points to new opportunities for integrating multimodal gaze in menus and bimanual interfaces in 3D environments. Â© 2020 ACM.",,,
10.1109/JBHI.2020.3002097,2020,"Kacur J., Polec J., Smolejova E., Heretik A.",An analysis of eye-tracking features and modelling methods for free-viewed standard stimulus: Application for schizophrenia detection,"Currently psychiatry is a medical field lacking an automated diagnostic process. The presence of a mental disorder is established by observing its typical symptoms. Eye-movement specifics have already been established as an 'endophenotype' for schizophrenia, but an automated diagnostic process of eye-movement analysis is still lacking. This article presents several novel approaches for the automatic detection of a schizophrenic disorder based on a free-view image test using a Rorschach inkblot and an eye tracker. Several features that enabled us to analyse the eye-tracker signal as a whole as well as its specific parts were tested. The variety of features spans global (heat maps, gaze plots), sequences of features (means, variances, and spectra), static (x and y signals as 2D images), dynamic (velocities), and model-based (limiting probabilities and transition matrices) categories. For each set of features, a proper modelling and classification method was designed (convolutional, recurrent, fully connected and combined neural networks; Hidden Markov models). By doing so, it was possible to find the importance of each feature and its physical representation using k-fold cross validation and a paired t-test. The dataset was sampled on 22 people with schizophrenia and 22 healthy individuals. The most successful approach was based on heat maps using all data and convolutional networks, reaching a 78.8% accuracy, which is a 10.5% improvement over the reference method. From all tested methods, there are two in an 85% accuracy range and over fifteen others in a 75% accuracy range at a 10% significance level. Â© 2013 IEEE.",,,
10.1109/JBHI.2020.3004686,2020,"Mengoudi K., Ravi D., Yong K.X.X., Primativo S., Pavisic I.M., Brotherhood E., Lu K., Schott J.M., Crutch S.J., Alexander D.C.",Augmenting dementia cognitive assessment with instruction-less eye-tracking tests,"Eye-tracking technology is an innovative tool that holds promise for enhancing dementia screening. In this work, we introduce a novel way of extracting salient features directly from the raw eye-tracking data of a mixed sample of dementia patients during a novel instruction-less cognitive test. Our approach is based on self-supervised representation learning where, by training initially a deep neural network to solve a pretext task using well-defined available labels (e.g. recognising distinct cognitive activities in healthy individuals), the network encodes high-level semantic information which is useful for solving other problems of interest (e.g. dementia classification). Inspired by previous work in explainable AI, we use the Layer-wise Relevance Propagation (LRP) technique to describe our network's decisions in differentiating between the distinct cognitive activities. The extent to which eye-tracking features of dementia patients deviate from healthy behaviour is then explored, followed by a comparison between self-supervised and handcrafted representations on discriminating between participants with and without dementia. Our findings not only reveal novel self-supervised learning features that are more sensitive than handcrafted features in detecting performance differences between participants with and without dementia across a variety of tasks, but also validate that instruction-less eye-tracking tests can detect oculomotor biomarkers of dementia-related cognitive dysfunction. This work highlights the contribution of self-supervised representation learning techniques in biomedical applications where the small number of patients, the non-homogenous presentations of the disease and the complexity of the setting can be a challenge using state-of-the-art feature extraction methods. Â© 2013 IEEE.",,,
10.1109/FG47880.2020.00049,2020,"Koujan M.R., Doukas M.C., Roussos A., Zafeiriou S.",ReenactNet: Real-time Full Head Reenactment,"video-to-video synthesis is a challenging problem aiming at learning a translation function between a sequence of semantic maps and a photo-realistic video depicting the characteristics of a driving video. We propose a head-to-head system of our own implementation capable of fully transferring the human head 3D pose, facial expressions and eye gaze from a source to a target actor, while preserving the identity of the target actor. Our system produces high-fidelity, temporally-smooth and photo-realistic synthetic videos faithfully transferring the human time-varying head attributes from the source to the target actor. Our proposed implementation: 1) works in real time (20 fps), 2) runs on a commodity laptop with a webcam as the only input, 3) is interactive, allowing the participant to drive a target person, e.g. a celebrity, politician, etc, instantly by varying their expressions, head pose, and eye gaze, and visualising the synthesised video concurrently. Â© 2020 IEEE.",,,
10.1017/S0890060420000372,2020,"Batliner M., Hess S., Ehrlich-AdÃ¡m C., Lohmeyer Q., Meboldt M.",Automated areas of interest analysis for usability studies of tangible screen-based user interfaces using mobile eye tracking,"The user's gaze can provide important information for human-machine interaction, but the analysis of manual gaze data is extremely time-consuming, inhibiting wide adoption in usability studies. Existing methods for automated areas of interest (AOI) analysis cannot be applied to tangible products with a screen-based user interface (UI), which have become ubiquitous in everyday life. The objective of this paper is to present and evaluate a method to automatically map the user's gaze to dynamic AOIs on tangible screen-based UIs based on computer vision and deep learning. This paper presents an algorithm for automated Dynamic AOI Mapping (aDAM), which allows the automated mapping of gaze data recorded with mobile eye tracking to the predefined AOIs on tangible screen-based UIs. The evaluation of the algorithm is performed using two medical devices, which represent two extreme examples of tangible screen-based UIs. The different elements of aDAM are examined for accuracy and robustness, as well as the time saved compared to manual mapping. The break-even point for an analyst's effort for aDAM compared to manual analysis is found to be 8.9 min gaze data time. The accuracy and robustness of both the automated gaze mapping and the screen matching indicate that aDAM can be applied to a wide range of products. aDAM allows, for the first time, automated AOI analysis of tangible screen-based UIs with AOIs that dynamically change over time. The algorithm requires some additional initial input for the setup and training, but analyzed gaze data duration and effort is only determined by computation time and does not require any additional manual work thereafter. The efficiency of the approach has the potential for a broader adoption of mobile eye tracking in usability testing for the development of new products and may contribute to a more data-driven usability engineering process in the future. Copyright Â© The Author(s), 2020. Published by Cambridge University Press.",,,
10.1007/s11548-020-02246-4,2020,"Sun Q., Mai Y., Yang R., Ji T., Jiang X., Chen X.",Fast and accurate online calibration of optical see-through head-mounted display for AR-based surgical navigation using Microsoft HoloLens,"Purpose:: The use of an optical see-through head-mounted display (OST-HMD) in augmented reality (AR) has significantly increased in recent years, but the alignment between the virtual scene and physical reality is still a challenge. A fast and accurate calibration method of OST-HMD is important for augmented reality in the medical field. Methods:: We proposed a fast online calibration procedure for OST-HMD with the aid of an optical tracking system. Two 3D datasets are collected in this procedure: the virtual points rendered in front of the observerâ€™s eyes and the corresponding points in optical tracking space. The transformation between these two 3D coordinates is solved to build the connection between virtual and real space. An AR-based surgical navigation system is developed with the help of this procedure, which is used for experiment verification and clinical trial. Results:: Phantom experiment based on the 3D-printed skull is performed, and the average root-mean-square error of control points between rendered object and skull model is 1.30 Â± 0.39 mm, and the time consumption of the calibration procedure can be less than 30Â s. A clinical trial is also conducted to show the feasibility in real surgery theatre. Conclusions:: The proposed calibration method does not rely on the camera of the OST-HMD and is very easy to operate. Phantom experiment and clinical case demonstrated the feasibility of our AR-based surgical navigation system and indicated it has the potential in clinical application. Â© 2020, CARS.",,,
10.1145/3385959.3418444,2020,"Liu C., Orlosky J., Plopski A.",Eye Gaze-based Object Rotation for Head-mounted Displays,"Hands-free manipulation of 3D objects has long been a challenge for augmented and virtual reality (AR/VR). While many methods use eye gaze to assist with hand-based manipulations, interfaces cannot yet provide completely gaze-based 6 degree-of-freedom (DoF) manipulations in an efficient manner. To address this problem, we implemented three methods to handle rotations of virtual objects using gaze, including RotBar: a method that maps line-of-sight eye gaze onto per-axis rotations, RotPlane: a method that makes use of orthogonal planes to achieve per-axis angular rotations, and RotBall: a method that combines a traditional arcball with an external ring to handle user-perspective roll manipulations. We validated the efficiency of each method by conducting a user study involving a series of orientation tasks along different axes with each method. Experimental results showed that users could accomplish single-axis orientation tasks with RotBar and RotPlane significantly faster and more accurate than RotBall. On the other hand for multi-axis orientation tasks, RotBall significantly outperformed RotBar and RotPlane in terms of speed and accuracy. Â© 2020 ACM.",,,
10.1145/3436369.3437438,2020,"Liu S., Liu D., Wu H.",Gaze estimation with multi-scale channel and spatial attention,"Gaze estimation is well established as a significant research topic in computer vision given its importance for different applications. Recent studies demonstrate that other regions of the face beyond the two eyes contain valuable information for gaze estimation. Motivated by these works, we propose a novel and powerful deep convolutional network with multi-scale channel and spatial attention, which only takes the full-face image as input without additional modules to detect the eyes and estimate the head pose, to handle the gaze estimation task. It uses multi-scale channel and spatial information to adaptively select and increase important features and suppress some unnecessary facial regions which may not contribute to estimate gaze. By rigorously evaluating our module, we show that our method significantly outperforms the state-of-the-art for 3D gaze estimation on multiple public datasets. Â© 2020 ACM.",,,
10.1145/3395035.3425256,2020,"Chauhan H., Prasad A., Shukla J.",Engagement analysis of ADHD students using visual cues from eye tracker,"In this paper, we focus on finding the correlation between visual attention and engagement of ADHD students in one-on-one sessions with specialized educators using visual cues and eye-tracking data. Our goal is to investigate the extent to which observations of eye-gaze, posture, emotion and other physiological signals can be used to model the cognitive state of subjects and to explore the integration of multiple sensor modalities to improve the reliability of detection of human displays of awareness and emotion in the context of ADHD affected children. This is a novel problem since no previous studies have aimed to identify markers of attentiveness in the context of students affected with ADHD. The experiment has been designed to collect data in a controlled environment and later on can be used to generate Machine Learning models to assist real-world educators. Additionally, we propose a novel approach for AOI (Area of Interest) detection for eye-tracking analysis in dynamic scenarios using existing deep learning-based saliency prediction and fixation prediction models. We aim to use the processed data to extract the features from a subject's eye-movement patterns and use Machine Learning models to classify the attention levels. Â© 2020 ACM.",,,
10.1145/3395035.3425261,2020,"Al Zaidawi S.M.K., Prinzler M.H.U., SchrÃ¶der C., Zachmann G., Maneth S.",Gender classification of prepubescent children via eye movements with reading stimuli,"We present a new study of gender prediction using eye movements of prepubescent children aged 9 - 10. Despite previous research indicating that gender differences in eye movements are observed only in adults, we are able to predict gender with accuracies of up to 64%. Our method segments gaze point trajectories into saccades and fixations. It then computes a small number of features and classifies saccades and fixations separately using statistical methods. The used dataset contains non-dyslexic and dyslexic children. In mixed groups, the accuracy of our classifiers drops dramatically. To address this challenge, we construct a hierarchical classifier that makes use of dyslexia prediction to improve significantly the accuracy of gender prediction in mixed groups. Â© 2020 ACM.",,,
10.1109/IROS45743.2020.9341755,2020,"Liu M., Fu Li Y., Liu H.",3D gaze estimation for head-mounted devices based on visual saliency,"Compared with the maturity of 2D gaze tracking technology, 3D gaze tracking has gradually become a research hotspot in recent years. The head-mounted gaze tracker has shown great potential for gaze estimation in 3D space due to its appealing flexibility and portability. The general challenge for 3D gaze tracking algorithms is that calibration is necessary before the usage, and calibration targets cannot be easily applied in some situations or might be blocked by moving human and objects. Besides, the accuracy on depth direction has always come to be a crucial problem. Regarding the issues mentioned above, a 3D gaze estimation with auto-calibration method is proposed in this study. We use an RGBD camera as the scene camera to acquire the accurate 3D structure of the environment. The automatic calibration is achieved by uniting gaze vectors with saliency maps of the scene which aligned depth information. Finally, we determine the 3D gaze point through a point cloud generated from the RGBD camera. The experiment result demonstrates that our proposed method achieves 4.34 of average angle error in the field from 0.5m to 3m and the average depth error is 23.22mm, which is sufficient for 3D gaze estimation in the real scene. Â© 2020 IEEE.",,,
10.1109/ISMSIT50672.2020.9254786,2020,"Olawale O.P., Dimililer K.",Individual Eye Gaze Prediction with the Effect of Image Enhancement Using Deep Neural Networks,"The prediction of individual eye gaze is a research topic that has gained the interest of researchers with its wide range of applications because neural networks majorly increase the rate of accuracy of individual gaze. In this research work, MPIIGaze dataset has been employed for the prediction of individual gaze and the direction of individual gaze was grouped into down view, left view, right view and lastly centre view. A CNN model was used to train and validate a random selection of images. Firstly, the ordinary images were trained and validated, after which image enhancement processing technique was applied. With the image brightness enhancement technique, a higher rate of gaze prediction accuracy was achieved. Hence, it can be deduced that image enhancement has proved its purpose by providing image interpretation with better quality. Â© 2020 IEEE.",,,
10.1145/3382507.3418828,2020,"Sims S.D., Conati C.",A Neural Architecture for Detecting User Confusion in Eye-tracking Data,"Encouraged by the success of deep learning in a variety of domains, we investigate the effectiveness of a novel application of such methods for detecting user confusion with eye-tracking data. We introduce an architecture that uses RNN and CNN sub-models in parallel, to take advantage of the temporal and visuospatial aspects of our data. Experiments with a dataset of user interactions with the ValueChart visualization tool show that our model outperforms an existing model based on a Random Forest classifier, resulting in a 22% improvement in combined confused and not confused class accuracies. Â© 2020 ACM.",,,
10.1145/3382507.3418890,2020,"Emerson A., Henderson N., Rowe J., Min W., Lee S., Minogue J., Lester J.",Early Prediction of Visitor Engagement in Science Museums with Multimodal Learning Analytics,"Modeling visitor engagement is a key challenge in informal learning environments, such as museums and science centers. Devising predictive models of visitor engagement that accurately forecast salient features of visitor behavior, such as dwell time, holds significant potential for enabling adaptive learning environments and visitor analytics for museums and science centers. In this paper, we introduce a multimodal early prediction approach to modeling visitor engagement with interactive science museum exhibits. We utilize multimodal sensor data including eye gaze, facial expression, posture, and interaction log data captured during visitor interactions with an interactive museum exhibit for environmental science education, to induce predictive models of visitor dwell time. We investigate machine learning techniques (random forest, support vector machine, Lasso regression, gradient boosting trees, and multi-layer perceptron) to induce multimodal predictive models of visitor engagement with data from 85 museum visitors. Results from a series of ablation experiments suggest that incorporating additional modalities into predictive models of visitor engagement improves model accuracy. In addition, the models show improved predictive performance over time, demonstrating that increasingly accurate predictions of visitor dwell time can be achieved as more evidence becomes available from visitor interactions with interactive science museum exhibits. These findings highlight the efficacy of multimodal data for modeling museum exhibit visitor engagement. Â© 2020 ACM.",,,
10.1145/3382507.3417961,2020,"Yu Z., Huang X., Zhang X., Shen H., Li Q., Deng W., Tang J., Yang Y., Ye J.",A Multi-Modal Approach for Driver Gaze Prediction to Remove Identity Bias,"Driver gaze prediction is an important task in Advanced Driver Assistance System (ADAS). Although the Convolutional Neural Network (CNN) can greatly improve the recognition ability, there are still several unsolved problems due to the challenge of illumination, pose and camera placement. To solve these difficulties, we propose an effective multi-model fusion method for driver gaze estimation. Rich appearance representations, i.e. holistic and eyes regions, and geometric representations, i.e. landmarks and Delaunay angles, are separately learned to predict the gaze, followed by a score-level fusion system. Moreover, pseudo-3D appearance supervision and identity-adaptive geometric normalization are proposed to further enhance the prediction accuracy. Finally, the proposed method achieves state-of-the-art accuracy of 82.5288% on the test data, which ranks 1st at the EmotiW2020 driver gaze prediction sub-challenge. Â© 2020 ACM.",,,
10.1145/3382507.3418865,2020,"Putze F., KÃ¼ster D., Urban T., Zastrow A., Kampen M.",Attention Sensing through Multimodal User Modeling in an Augmented Reality Guessing Game,"We developed an attention-sensitive system that is capable of playing the children's guessing game ""I spy with my litte eye""with a human user. In this game, the user selects an object from a given scene and provides the system with a single-sentence clue about it. For each trial, the system tries to guess the target object. Our approach combines top-down and bottom-up machine learning for object and color detection, automatic speech recognition, natural language processing, a semantic database, eye tracking, and augmented reality. Our evaluation demonstrates performance significantly above chance level, and results for most of the individual machine learning components are encouraging. Participants reported very high levels of satisfaction and curiosity about the system. The collected data shows that our guessing game generates a complex and rich data set. We discuss the capabilities and challenges of our system and its components with respect to multimodal attention sensing. Â© 2020 ACM.",,,
10.1145/3382507.3417967,2020,"Stappen L., Rizos G., Schuller B.",X-AWARE: ConteXt-AWARE Human-Environment Attention Fusion for Driver Gaze Prediction in the Wild,"Reliable systems for automatic estimation of the driver's gaze are crucial for reducing the number of traffic fatalities and for many emerging research areas aimed at developing intelligent vehicle-passenger systems. Gaze estimation is a challenging task, especially in environments with varying illumination and reflection properties. Furthermore, there is wide diversity with respect to the appearance of drivers' faces, both in terms of occlusions (e.g. vision aids) and cultural/ethnic backgrounds. For this reason, analysing the face along with contextual information - for example, the vehicle cabin environment - adds another, less subjective signal towards the design of robust systems for passenger gaze estimation. In this paper, we present an integrated approach to jointly model different features for this task. In particular, to improve the fusion of the visually captured environment with the driver's face, we have developed a contextual attention mechanism, X-AWARE, attached directly to the output convolutional layers of InceptionResNetV2 networks. In order to showcase the effectiveness of our approach, we use the Driver Gaze in the Wild dataset, recently released as part of the Eighth Emotion Recognition in the Wild Challenge (EmotiW) challenge. Our best model outperforms the baseline by an absolute of 15.03% in accuracy on the validation set, and improves the previously best reported result by an absolute of 8.72% on the test set. Â© 2020 ACM.",,,
10.1145/3382507.3418816,2020,"Kangas J., Koskinen O., Raisamo R.",Gaze Tracker Accuracy and Precision Measurements in Virtual Reality Headsets,"To effectively utilize a gaze tracker in user interaction it is important to know the quality of the gaze data that it is measuring. We have developed a method to evaluate the accuracy and precision of gaze trackers in virtual reality headsets. The method consists of two software components. The first component is a simulation software that calibrates the gaze tracker and then performs data collection by providing a gaze target that moves around the headset's field-of-view. The second component makes an off-line analysis of the logged gaze data and provides a number of measurement results of the accuracy and precision. The analysis results consist of the accuracy and precision of the gaze tracker in different directions inside the virtual 3D space. Our method combines the measurements into overall accuracy and precision. Visualizations of the measurements are created to see possible trends over the display area. Results from selected areas in the display are analyzed to find out differences between the areas (for example, the middle/outer edge of the display or the upper/lower part of display). Â© 2020 ACM.",,,
10.1145/3383652.3423910,2020,"Pustejovsky J., Krishnaswamy N.",Embodied Human-Computer Interactions through Situated Grounding,"In this paper, we introduce a simulation platform for modeling and building Embodied Human-Computer Interactions (EHCI). This system, VoxWorld, is a multimodal dialogue system enabling communication through language, gesture, action, facial expressions, and gaze tracking, in the context of task-oriented interactions. A multimodal simulation is an embodied 3D virtual realization of both the situational environment and the co-situated agents, as well as the most salient content denoted by communicative acts in a discourse. It is built on the modeling language VoxML [7], which encodes objects with rich semantic typing and action affordances, and actions themselves as multimodal programs, enabling contextu-ally salient inferences and decisions in the environment. VoxWorld enables an embodied HCI by situating both human and computational agents within the same virtual simulation environment, where they share perceptual and epistemic common ground. Â© 2020 Owner/Author.",,,
10.3389/fnbot.2020.591128,2020,"Bing Z., Lemke C., Morin F.O., Jiang Z., Cheng L., Huang K., Knoll A.",Perception-Action Coupling Target Tracking Control for a Snake Robot via Reinforcement Learning,"Visual-guided locomotion for snake-like robots is a challenging task, since it involves not only the complex body undulation with many joints, but also a joint pipeline that connects the vision and the locomotion. Meanwhile, it is usually difficult to jointly coordinate these two separate sub-tasks as this requires time-consuming and trial-and-error tuning. In this paper, we introduce a novel approach for solving target tracking tasks for a snake-like robot as a whole using a model-free reinforcement learning (RL) algorithm. This RL-based controller directly maps the visual observations to the joint positions of the snake-like robot in an end-to-end fashion instead of dividing the process into a series of sub-tasks. With a novel customized reward function, our RL controller is trained in a dynamically changing track scenario. The controller is evaluated in four different tracking scenarios and the results show excellent adaptive locomotion ability to the unpredictable behavior of the target. Meanwhile, the results also prove that the RL-based controller outperforms the traditional model-based controller in terms of tracking accuracy. Â© Copyright Â© 2020 Bing, Lemke, Morin, Jiang, Cheng, Huang and Knoll.",,,
10.1145/3424978.3425003,2020,"Xia Y., Liang B.",Gaze Estimation Based on Deep Learning Method,"Many mature methods of gaze estimation are available in various scenarios. Relying on additional hardware or platforms with professional equipment to tackle intensive computation tasks is a prominent problem of traditional methods, which usually involves high costs and is relatively tedious. Besides, the implementation of traditional gaze estimation method is typically complex. Traditional gaze estimation approaches require systematic prior knowledge or expertise for practical operations, and the gaze is estimated through the representation of pupil and iris, so high-quality images shot in special environments are required. This paper proposes a data-driven method for gaze estimation. It can be applied to various mobile platforms with deep learning methods instead of additional hardware devices or systematic prior knowledge. When collecting gaze data set, the paper designs a set of automatic and fast data collection mechanism on the mobile platform. Beyond that, the paper proposes an annotation method on collected gaze dataset that improves the predicted accuracy. The results demonstrate that the deep learning method performs well and can satisfy the task need of different applications. Â© 2020 ACM.",,,
10.1145/3423328.3423498,2020,"Abid M., Perreira Da Silva M., Le Callet P.",Perceptual Characterization of 3D Graphical Contents based on Attention Complexity Measures,"This paper provides insights on how to perceptually characterize colored 3D Graphical Contents (3DGC). In this study, pre-defined viewpoints were considered to render static graphical objects. For perceptual characterization, we used visual attention complexity (VAC) measures. Considering a view-based approach to exploit the perceived information, an eye-tracking experiment was conducted using colored graphical objects. Based on the collected gaze data, we revised the VAC measure, suggested in 2D imaging context, and adapted it to 3DGC. We also provided an objective predictor that highly mimics the experimental attentional complexity information. This predictor can be useful in Quality of Experience (QoE) studies: to balance content selection when benchmarking 3DGC processing techniques (e.g., rendering, coding, streaming, etc.) for human panel studies or ad hoc key performance indicator, and also to optimize the user's QoE when rendering such contents. Â© 2020 ACM.",,,
10.1109/GCCE50665.2020.9291784,2020,"Otsu K., Seo M., Kitajima T., Chen Y.-W.",Automatic generation of eye gaze corrected video using recursive conditional generative adversarial networks,"Eye contact plays an important role in conversations. However, it is difficult to maintain eye contact while using current popular video calling systems due to the different positions of the camera and display. To solve this problem, we introduce convolutional long short-term memory, which captures the features of frames up to the previous instant, into a deep learning generation model - conditional generative adversarial networks (GANs), which is recursive GANs - to generate eye gaze corrected video. By extending this network, the generator generates an image that takes the previous frame into account and the discriminator identifies the image by considering the previous frame. Thus, we aimed to achieve a consistent video conversion. Â© 2020 IEEE.",,,
10.1109/GCCE50665.2020.9291871,2020,"Kurono H., Shibuya Y.",Non-contact digital signage allowing users to change contents with their face orientation,"In today's society, digital signage is widely used in various places such as commercial facilities and train stations. Most digital signages use a liquid crystal display or LED panel. One of the advantages of digital signage is the ability to dynamically change the content of the advertisement. However, the method of change is not due to the viewer's intention, but rather to elapsed time to show, and it is merely a sequential display of static advertisements. In this study, we propose and implement a non-contact interactive digital signage system using a web camera that allows users to change the content according to their face orientation. In the implementation, we use OpenCV for showing the content and image processing, and the machine learning library, named dlib, is used for detecting user's face and identifying its orientation. The proposed system changes the layout and enlarges the content in which the user is interested. The user does not need to wear any device and to do any special action. Furthermore, the proposed system offers an intuitive interaction manner to the user. In this paper, some experimental designs are also described. Â© 2020 IEEE.",,,
10.1109/ICMA49215.2020.9233571,2020,"Liu M., Li Y.F., Liu H.",Towards Robust Auto-calibration for Head-mounted Gaze Tracking Systems,"Removing explicit user calibration is indeed an appealing goal for gaze tracking systems. In this paper, a novel auto-calibration method is proposed to achieve the 3D point of regard (PoR) prediction for the head-mounted gaze tracker. Our method chooses an RGBD sensor as the scene camera to capture 3D structures of the environment and treats salient regions as possible 3D calibration targets. In order to improve efficiency, the bag of words (BoW) algorithm is applied to calculate scene images' similarity and eliminate redundant maps. After elimination, the translation relationship between eye cameras and the scene camera can be determined by uniting calibration targets with gaze vectors, and 3D gaze points are obtained by transformed gaze vectors and the point cloud of environment. The experiment results indicate that our method achieves effective performance on 3D gaze estimation for head-mounted gaze trackers, which can promote engineering applications of human-computer interaction technology in many areas. Â© 2020 IEEE.",,,
10.1145/3394171.3413683,2020,"Bermejo C., Chatzopoulos D., Hui P.",EyeShopper: Estimating Shoppers' Gaze using CCTV Cameras,"Recent advances in machine and deep learning allow for enhanced retail analytics by applying object detection techniques. However, existing approaches either require laborious installation processes to function or lack precision when the customers turn their back in the installed cameras. In this paper, we present EyeShopper, an innovative system that tracks the gaze of shoppers when facing away from the camera and provides insights about their behavior in physical stores. EyeShopper is readily deployable in existing surveillance systems and robust against low-resolution video inputs. At the same time, its accuracy is comparable to state-of-the-art gaze estimation frameworks that require high-resolution and continuous video inputs to function. Furthermore, EyeShopper is more robust than state-of-the-art gaze tracking techniques for back head images. Extensive evaluation with different real video datasets and a synthetic dataset we produced shows that EyeShopper estimates with high accuracy the gaze of customers. Â© 2020 ACM.",,,
10.1109/B-HTC50970.2020.9297845,2020,"HemaMalini B.H., Supritha R.C., Venkatesh Prasad N.K., Vandana R., Yadav R.",Eye and Voice Controlled Wheel Chair,"The smart wheelchair developed for people with inabilities dependent on the eye-Tracking and by utilizing the voice assistant module. The smart wheelchair contains two modules, including a module for processing the image, a wheelchair-controlled voice associate module and constrained by appliances. The module for picture processing comprises a camera mounted on the wheelchair that can capture the picture and handling those images. The captured picture moved into a Raspberry Pie micro controller process using Open CV to get pupil movement in the 2D bearing. The movement of pupil is then moved remotely to the module for managing a wheelchair. The eyeball movement is also used as the controller to regulate the operations. Speech recognition technology is a technology that provides a way of interaction for human with the wheel chair. Hence, the problems faced by the people can be easily solved by using this technology for controlling the wheel chair. This can be implemented by using the smart phone or smart assistant enabled device as an interface between the human and the wheel chair. Â© 2020 IEEE.",,,
10.1109/ICITEE49829.2020.9271771,2020,"Alfaroby E. M., Wibirama S., Ardiyanto I.",Accuracy Improvement of Object Selection in Gaze Gesture Application using Deep Learning,"Gaze-based interaction is a crucial research area. Gaze gesture provides faster interaction between a user and a computer application because people naturally look at the object of interest before taking any other actions. Spontaneous gaze-gesture-based application uses gaze-gesture as an input modality without performing any calibration. The conventional eye tracking systems have a problem with low accuracy. In general, data captured by eye tracker contains errors and noise within gaze position signal. The errors and noise affect the performance of object selection in gaze gesture based application that controls digital contents on the display using smooth-pursuit eye movement. The conventional object selection method suffers from low accuracy (<80%). In this paper, we addressed this accuracy problem with a novel approach using deep learning. We exploited deep learning power to recognize the pattern of eye-gaze data. Long Short Term Memory (LSTM) is a deep learning architecture based on recurrent neural network (RNN). We used LSTM to perform object selection task. The dataset consisted of 34 participants taken from previous study of object selection technique of gaze gesture-based application. Our experimental results show that the proposed method achieved 96.17% of accuracy. In future, our result may be used as a guidance for developing gaze gesture application. Â© 2020 IEEE.",,,
10.1109/ICIP40778.2020.9191186,2020,"Sun Y., Prabhushankar M., Alregib G.",Implicit Saliency in Deep Neural Networks,"In this paper, we show that existing recognition and localization deep architectures, that have not been exposed to eye tracking data or any saliency datasets, are capable of predicting the human visual saliency. We term this as implicit saliency in deep neural networks. We calculate this implicit saliency using expectancy-mismatch hypothesis in an unsupervised fashion. Our experiments show that extracting saliency in this fashion provides comparable performance when measured against the state-of-art supervised algorithms. Additionally, the robustness outperforms those algorithms when we add large noise to the input images. Also, we show that semantic features contribute more than low-level features for human visual saliency detection. Based on these properties and performances, our proposed method greatly lowers the threshold for saliency detection in terms of required data and bridges the gap between human visual saliency and model saliency. Â© 2020 IEEE.",,,
10.1109/ICIP40778.2020.9191100,2020,"Mitsuzum Y., Irie G., Kimura A., Nakazawa A.",A Generative Self-Ensemble Approach to Simulated+Unsupervised Learning,"In this paper, we consider Simulated and Unsupervised (S+U) learning which is a problem of learning from labeled synthetic and unlabeled real images. After translating the synthetic images to real ones, existing S+U learning methods use only the labeled synthetic images for training a predictor (e.g., a regression function) and ignore the target real images, which may result in unsatisfactory prediction performance. Our approach utilizes both synthetic and real images to train the predictor. The main idea of ours is to involve a self-ensemble learning framework into S+U learning. More specifically, we require the prediction results for an unlabeled real image to be consistent between 'teacher' and 'student' predictors, even after some perturbations are added to the image. Furthermore, aiming at generating diverse perturbations along the underlying data manifold, we introduce one-to-many image translation between synthetic and real images. Evaluation experiments on an appearance-based gaze estimation task demonstrate that the proposed ideas can improve the prediction accuracy and our full method can outperform existing S+U learning methods. Â© 2020 IEEE.",,,
10.1109/DSAA49011.2020.00079,2020,"Chen Q., Yu X., Liu N., Yuan X., Wang Z.",Personalized course recommendation based on eye-tracking technology and deep learning,"With the rapid development of online courses, the requirements of personalized course recommendation have been increasing. The traditional collaborative filtering algorithm confronts with the challenge of cold start, which is difficult to settle on online course recommendation effectively. In this paper, we propose a novel click through rate (CTR) model for personalized online course recommendation, with discriminative user features, item features and cross features. The feature representation ability of the CTR model is improved and the serious challenge of cold start is alleviated. Furthermore, transfer learning is introduced to deal with the problem of insufficient data in models training. More specially, eye tracking technology is applied to capture the users' cognitive styles, which are visualized with the heat map and fixation point trajectory. Finally, the recommendation interface sent to the learners, according to the user's cognitive style. The experiments show that the novel CTR model improves the performance of the personalized online course recommendation. Â© 2020 IEEE.",,,
10.3390/educsci10100293,2020,"von Reumont F., Budke A.",Strategies for successful learning with geographical comics: An eye-tracking study with young learners,"Many studies report that comics are useful as learning material. However, there is little known about how learning with comics works. Based on previously established theories about multimedia learning, we conducted an eye-tracking experiment to examine learning about geography with a specially designed combination of comic and map which we call geo-comic. In our experiment, we show that our geo-comic fulfills many prerequisites for promoting deep learning. Thus, we establish guidelines for an effective design of geo-comics and recommend deploying comics in combination with maps in geography classes. Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/JBHI.2020.2976150,2020,"Aresta G., Ferreira C., Pedrosa J., Araujo T., Rebelo J., Negrao E., Morgado M., Alves F., Cunha A., Ramos I., Campilho A.",Automatic Lung Nodule Detection Combined with Gaze Information Improves Radiologists' Screening Performance,"Early diagnosis of lung cancer via computed tomography can significantly reduce the morbidity and mortality rates associated with the pathology. However, searching lung nodules is a high complexity task, which affects the success of screening programs. Whilst computer-aided detection systems can be used as second observers, they may bias radiologists and introduce significant time overheads. With this in mind, this study assesses the potential of using gaze information for integrating automatic detection systems in the clinical practice. For that purpose, 4 radiologists were asked to annotate 20 scans from a public dataset while being monitored by an eye tracker device, and an automatic lung nodule detection system was developed. Our results show that radiologists follow a similar search routine and tend to have lower fixation periods in regions where finding errors occur. The overall detection sensitivity of the specialists was 0.67\pm 0.07, whereas the system achieved 0.69. Combining the annotations of one radiologist with the automatic system significantly improves the detection performance to similar levels of two annotators. Filtering automatic detection candidates only for low fixation regions still significantly improves the detection sensitivity without increasing the number of false-positives. Â© 2013 IEEE.",,,
10.1111/cogs.12905,2020,"Southwell R., Gregg J., Bixler R., D'Mello S.K.",What Eye Movements Reveal About Later Comprehension of Long Connected Texts,"We know that reading involves coordination between textual characteristics and visual attention, but research linking eye movements during reading and comprehension assessed after reading is surprisingly limited, especially for reading long connected texts. We tested two competing possibilities: (a) the weak association hypothesis: Links between eye movements and comprehension are weak and short-lived, versus (b) the strong association hypothesis: The two are robustly linked, even after a delay. Using a predictive modeling approach, we trained regression models to predict comprehension scores from global eye movement features, using participant-level cross-validation to ensure that the models generalize across participants. We used data from three studies in which readers (NsÂ =Â 104, 130, 147) answered multiple-choice comprehension questions ~30Â min after reading a 6,500-word text, or after reading up to eight 1,000-word texts. The models generated accurate predictions of participants' text comprehension scores (correlations between observed and predicted comprehension: 0.384, 0.362, 0.372, psÂ <.001), in line with the strong association hypothesis. We found that making more, but shorter fixations, consistently predicted comprehension across all studies. Furthermore, models trained on one study's data could successfully predict comprehension on the others, suggesting generalizability across studies. Collectively, these findings suggest that there is a robust link between eye movements and subsequent comprehension of a long connected text, thereby connecting theories of low-level eye movements with those of higher order text processing during reading. Â© 2020 Cognitive Science Society, Inc",,,
10.1016/j.aei.2020.101167,2020,"Shi Y., Du J., Zhu Q.",The impact of engineering information format on task performance: Gaze scanning pattern analysis,"The emergence of new visualization technologies such as Virtual Reality (VR) and Augmented Reality (AR) had been widely implemented in the Architecture, Engineering, and Construction (AEC) industry. Although cumulative evidence pointed out a positive impact of these visualization technologies on construction task performance, there is still an obvious disagreement on the benefits or implications of these new visualization technologies, due to the lack of understanding of the mechanisms in which the visualization affects cognitive processes related to information processing. To obtain more evidence, this paper presents a human-subject experiment (n = 90) to investigate the impact of information format on the performance of an industrial pipeline maintenance task. The investigation centers around how different engineering information formats affect the attention patterns as a potential explanation for the changes in performance. A between-group experiment design was used where the participants were randomly assigned to one of the three groups (2D group, 3D group, and VR group) depending on what type of information was given to review the pipe operation instruction. After the review session, the participants were asked to perform the operation task in the virtual environment based on their memory. The results showed that the 3D and VR groups outperformed the 2D group in task performance. The analysis of eye-tracking data further indicated that the information format significantly changed the gaze scanning pattern when participants were reviewing the operational instructions. We also found that the task performance was correlated with eye-tracking features including gaze movement and pupil dilation. Our findings provided more evidence about the mechanisms in which new visualization technologies affect the attention patterns, helped resolve the current disagreement within the literature. In addition, a prediction model was proposed to use eye-tracking features to predict construction task performance. Â© 2020 Elsevier Ltd",,,
10.1016/j.aei.2020.101153,2020,"Shi Y., Zhu Y., Mehta R.K., Du J.",A neurophysiological approach to assess training outcome under stress: A virtual reality experiment of industrial shutdown maintenance using Functional Near-Infrared Spectroscopy (fNIRS),"Shutdown maintenance, i.e., turning off a facility for a short period for renewal or replacement operations is a highly stressful task. With the limited time and complex operation procedures, human stress is a leading risk. Especially shutdown maintenance workers often need to go through excessive and stressful on-site trainings to digest complex operation information in limited time. The challenge is that workersâ€?stress status and task performance are hard to predict, as most trainings are only assessed after the shutdown maintenance operation is finished. A proactive assessment or intervention is needed to evaluate workersâ€?stress status and task performance during the training to enable early warning and interventions. This study proposes a neurophysiological approach to assess workersâ€?stress status and task performance under different virtual training scenarios. A Virtual Reality (VR) system integrated with the eye-tracking function was developed to simulate the power plant shutdown maintenance operations of replacing a heat exchanger in both normal and stressful scenarios. Meanwhile, a portable neuroimaging device â€?Functional Near-Infrared Spectroscopy (fNIRS) was also utilized to collect user's brain activities by measuring hemodynamic responses associated with neuron behavior. A humanâ€“subject experiment (n = 16) was conducted to evaluate participantsâ€?neural activity patterns and physiological metrics (gaze movement) related to their stress status and final task performance. Each participant was required to review the operational instructions for a pipe maintenance task for a short period and then perform the task based on their memory in both normal and stressful scenarios. Our experiment results indicated that stressful training had a strong impact on participantsâ€?neural connectivity patterns and final performance, suggesting the use of stressors during training to be an important and useful control factors. We further found significant correlations between gaze movement patterns in review phase and final task performance, and between the neural features and final task performance. In summary, we proposed a variety of supervised machine learning classification models that use the fNIRS data in the review session to estimate individual's task performance. The classification models were validated with the k-fold (k = 10) cross-validation method. The Random Forest classification model achieved the best average classification accuracy (80.38%) in classifying participantsâ€?task performance compared to other classification models. The contribution of our study is to help establish the knowledge and methodological basis for an early warning and estimating system of the final task performance based on the neurophysiological measures during the training for industrial operations. These findings are expected to provide more evidence about an early performance warning and prediction system based on a hybrid neurophysiological measure method, inspiring the design of a cognition-driven personalized training system for industrial workers. Â© 2020 Elsevier Ltd",,,
10.1016/j.cag.2020.06.007,2020,"Shi P., Billeter M., Eisemann E.",SalientGaze: Saliency-based gaze correction in virtual reality,"Eye-tracking with gaze estimation is a key element in many applications, ranging from foveated rendering and user interaction to behavioural analysis and usage metrics. For virtual reality, eye-tracking typically relies on near-eye cameras that are mounted in the VR headset. Such methods usually involve an initial calibration to create a mapping from eye features to a gaze position. However, the accuracy based on the initial calibration degrades when the position of the headset relative to the usersâ€?head changes; this is especially noticeable when users readjust the headset for comfort or even completely remove it for a short while. We show that a correction of such shifts can be achieved via 2D drift vectors in eye space. Our method estimates these drifts by extracting salient cues from the shown virtual environment to determine potential gaze directions. Our solution can compensate for HMD shifts, even those arising from taking off the headset, which enables us to eliminate reinitialization steps. Â© 2020 Elsevier Ltd",,,
10.1016/j.media.2020.101793,2020,"Liu F., Liu D., Tian J., Xie X., Yang X., Wang K.",Cascaded one-shot deformable convolutional neural networks: Developing a deep learning model for respiratory motion estimation in ultrasound sequences,"Improving the quality of image-guided radiation therapy requires the tracking of respiratory motion in ultrasound sequences. However, the low signal-to-noise ratio and the artifacts in ultrasound images make it difficult to track targets accurately and robustly. In this study, we propose a novel deep learning model, called a Cascaded One-shot Deformable Convolutional Neural Network (COSD-CNN), to track landmarks in real time in long ultrasound sequences. Specifically, we design a cascaded Siamese network structure to improve the tracking performance of CNN-based methods. We propose a one-shot deformable convolution module to enhance the robustness of the COSD-CNN to appearance variation in a meta-learning manner. Moreover, we design a simple and efficient unsupervised strategy to facilitate the network's training with a limited number of medical images, in which many corner points are selected from raw ultrasound images to learn network features with high generalizability. The proposed COSD-CNN has been extensively evaluated on the public Challenge on Liver UltraSound Tracking (CLUST) 2D dataset and on our own ultrasound image dataset from the First Affiliated Hospital of Sun Yat-sen University (FSYSU). Experiment results show that the proposed model can track a target through an ultrasound sequence with high accuracy and robustness. Our method achieves new state-of-the-art performance on the CLUST 2D benchmark set, indicating its strong potential for application in clinical practice. Â© 2020",,,
10.1016/j.cmpb.2020.105538,2020,"Jothi Prabha A., Bhargavi R.",Predictive Model for Dyslexia from Fixations and Saccadic Eye Movement Events,"Background: Dyslexia is a disorder characterized by difficulty in reading such as poor speech and sound recognition. They have less capability to relate letters and form words and exhibit poor reading comprehension. Eye-tracking methodologies play a major role in analyzing human cognitive processing. Dyslexia is not a visual impairment disorder but it's a difficulty in phonological processing and word decoding. These difficulties are reflected in their eye movement patterns during reading. Objective: The disruptive eye movement helps us to use eye-tracking methodologies for identifying dyslexics. Methods: In this paper, a small set of eye movement features have been proposed that contribute more to distinguish between dyslexics and non-dyslexics by machine learning models. Features related to eye movement events such as fixations and saccades are detected using statistical measures, dispersion threshold identification (I-DT) and velocity threshold identification (I-VT) algorithms. These features were further analyzed using various machine learning algorithms such as Particle Swarm Optimization (PSO) based SVM Hybrid Kernel (Hybrid SVM â€?PSO), Support Vector Machine (SVM), Random Forest classifier (RF), Logistic Regression (LR) and K-Nearest Neighbor (KNN) for classification of dyslexics and non-dyslexics. Results: The accuracy achieved using the Hybrid SVM â€“PSO model is 95.6 %. The best set of features that gave high accuracy are average no of fixations, average fixation gaze duration, average saccadic movement duration, total number of saccadic movements, and average number of fixations. Conclusion: It is observed that eye movement features detected using velocity-based algorithms performed better than those detected by dispersion-based algorithms and statistical measures. Â© 2020 Elsevier B.V.",,,
10.1145/3440067.3440078,2020,"Xia C., Chen K., Li K., Li H.",Identification of autism spectrum disorder via an eye-tracking based representation learning model,"Autism spectrum disorder (ASD) is a lifelong developmental disorder characterized by repetitive, restricted behavior and deficits in communication and social interactions. Early diagnosis and intervention can significantly reduce the hazards of the disease. However, the lack of effective clinical resources for early diagnosis has been a long-standing problem. In response to this problem, we apply the recent advances in deep neural networks on eye-tracking data in this study to classify children with and without ASD. First, we record the eye movement data of 31 children with ASD and 43 typically developing children on four categories of stimuli to construct an eye-tracking data set for ASD identification. Based on the collected eye movement data, we extract the dynamic saccadic scanpath on each image for all subjects. Then, we utilize the hierarchical features learned from a convolutional neural network and multidimensional visual salient features to encode the scanpaths. Next, we adopt the support vector machine to learn the relationship between encoded pieces of scanpaths and the labels from the two classes via supervised learning. Finally, we derive the scores of each scanpath and make the final judgment for each subject according to the scores on all scanpaths. The experimental results have shown that the proposed model has a maximum classification accuracy of 94.28% in the diagnostic tests. Based on existing research and calculation models, dynamic saccadic scanpaths can provide promising findings and implications for ASD early detection. Furthermore, integrating more information of the scanpaths into the model and developing a more in-depth description of scanpaths can improve the recognition accuracy. We hope our work can contribute to the development of multimodal approaches in the early detection and diagnosis of ASD. Â© 2020 ACM.",,,
10.1109/IJCB48548.2020.9304919,2020,"Boutros F., Damer N., Raja K., Ramachandra R., Kirchbuchner F., Kuijper A.",On benchmarking iris recognition within a head-mounted display for AR/VR applications,"Augmented and virtual reality is being deployed in different fields of applications. Such applications might involve accessing or processing critical and sensitive information, which requires strict and continuous access control. Given that Head-Mounted Displays (HMD) developed for such applications commonly contains internal cameras for gaze tracking purposes, we evaluate the suitability of such setup for verifying the users through iris recognition. In this work, we first evaluate a set of iris recognition algorithms suitable for HMD devices by investigating three well-established handcrafted feature extraction approaches, and to complement it, we also present the analysis using four deep learning models. While taking into consideration the minimalistic hardware requirements of stand-alone HMD, we employ and adapt a recently developed miniature segmentation model (EyeMMS) for segmenting the iris. Further, to account for non-ideal and non-collaborative capture of iris, we define a new iris quality metric that we termed as Iris Mask Ratio (IMR) to quantify the iris recognition performance. Motivated by the performance of iris recognition, we also propose the continuous authentication of users in a non-collaborative capture setting in HMD. Through the experiments on a publicly available OpenEDS dataset, we show that performance with EER = 5% can be achieved using deep learning methods in a general setting, along with high accuracy for continuous user authentication. Â© 2020 IEEE.",,,
10.1109/ICCE-Taiwan49838.2020.9258009,2020,"Lee K.F., Chen Y.L., Yu C.W., Wu C.H.",The Eye Tracking and Gaze Estimation System by Low Cost Wearable Devices,"This study develop a wearable eye tracking and gaze estimation low cost devices. This devices use infrared camera and design by integration of elastic mechanism adaptable. The use of cheap endoscope camera for mobile and 3D print technique for building up the devices, which results in be a low cost solution. This device can effectively extract and estimate pupil ellipse from few camera-captured samples of an eye and compute the corresponding 3D eye model. And this device use multiple point's calibration method to solve the related polynomial formula for future angle-to-gaze mapping. This wearable device is a low-cost which can be used for virtual reality and auxiliary equipment. Â© 2020 IEEE.",,,
10.1109/ICCE-Taiwan49838.2020.9258161,2020,"Yeh S.-C., Lin C.-H., Lin S.-K., Wu E.H., Tsai M.-C.",A Virtual Reality Based System for Drug Addiction and Diagnosis,"By integrating virtual reality (VR) technique with flavor simulator, our research intended to create an immersed virtual environment (VE) that is composed of multiple VR tasks with varied intensities of drug-temptation therefore to gradually induce the carving of addiction patients. Moreover, multi-model sensors, including electrocardiography (ECG), electroencephalography (EEG), galvanic skin response (GSR) and eye tracking, were synchronized with the VE system and utilized to measure the neuro-behavior of the addiction patients before, during and after the exposure to the virtual environment. Further, data of multi-model neuro behaviors and VR task performance were analyzed by statistics and machine learning method in order to evaluate the intensity of craving induced and judge the dependence on the drug so as to make an assessment to the addiction patients. Â© 2020 IEEE.",,,
10.1109/ICCICC50026.2020.9450216,2020,"Musabini A., Chetitah M.",Heatmap-based method for estimating driversâ€?cognitive distraction,"In order to increase road safety, among the visual ? Cognitive distraction: taking the mind off the driving and manual distractions, modern intelligent vehicles need also to detect cognitive distracted driving (i.e., the driverâ€™s mind task. wandering). In this study, the influence of cognitive processes Passive safety systems to combat visual and manual dis-on the driverâ€™s gaze behavior is explored. A novel image-based traction are already widely used in commercial vehicles. representation of the driverâ€™s eye-gaze dispersion is proposed These systems track the driverâ€™s eye-gaze. Once the driver to estimate cognitive distraction. Data are collected on open highway roads, with a tailored protocol to create cognitive looks anywhere other than the road, they are judged to be distraction. The visual difference of created shapes shows that distracted [4]. The downside of this is that if the driver is a driver explores a wider area in neutral driving compared looking at the road but daydreaming (a phenomenon known to distracted driving. Support vector machine (SVM)-based as the mind wandering [5]), they are misjudged as attentive. classifiers are trained, and 85.2% of accuracy is achieved for Cognitive distracted driving is a dangerous situation which a two-class problem, even with a small dataset. Thus, the proposed method has the discriminative power to recognize vehicles should be able to detect to increase road safety. It cognitive distraction using gaze information. Finally, this work has been highlighted as one of the issues to resolve in the details how this image-based representation could be useful for European New Car Assessment Programme (Euro NCAP) other cases of distracted driving detection. 2022 requirements (driver inattentiveness) [6]. Â©2020 IEEE",,,
10.3233/ATDE200082,2020,"Li F., Chang D., Liu Y., Cui J., Feng S., Huang N., Chen C.-H., Sourina O.",Evaluation of humanoid robot design based on global eye-tracking metrics,"The first impression of robot appearance normally affects the interaction with physical robots. Hence, it is critically important to evaluate the humanoid robot appearance design. This study towards evaluating humanoid robot design based on global eye-tracking metrics. Two methods are selected to extract global eye-tracking metrics, including bin-analysis-based entropy and approximate entropy. The data are collected from an eye-tracking experiment, where 20 participants evaluate 12 humanoid robot appearance designs with their eye movements recorded. The humanoid robots are evaluated from five aspects, namely smartness, friendliness, pleasure, arousal, and dominance. The results show that the entropy of fixation duration and velocity, approximate entropy of saccades amplitude are positively associated with the subjective feelings induced by robot appearance. These findings can aid in better understanding the first impression of human-robot interaction and enable the eye-tracking-based evaluation of humanoid robot design. By combining the theory of design and bio-signals analysis, the study contributes to the field of Transdisciplinary Engineering. Â© 2020 The authors and IOS Press.",,,
10.1109/ITSC45102.2020.9294216,2020,"Amadori P.V., Fischer T., Wang R., Demiris Y.",Decision Anticipation for Driving Assistance Systems,"Anticipating the correctness of imminent driver decisions is a crucial challenge in advanced driving assistance systems and has the potential to lead to more reliable and safer human-robot interactions. In this paper, we address the task of decision correctness prediction in a driver-in-the-loop simulated environment using unobtrusive physiological signals, namely, eye gaze and head pose. We introduce a sequence-to-sequence based deep learning model to infer the driver's likelihood of making correct/wrong decisions based on the corresponding cognitive state. We provide extensive experimental studies over multiple baseline classification models on an eye gaze pattern and head pose dataset collected from simulated driving. Our results show strong correlates between the physiological data and decision correctness, and that the proposed sequential model reliably predicts decision correctness from the driver with 80% precision and 72% recall. We also demonstrate that our sequential model performs well in scenarios where early anticipation of correctness is critical, with accurate predictions up to two seconds before a decision is performed. Â© 2020 IEEE.",,,
10.3390/s20185271,2020,"Fan D., Liu Y., Chen X., Meng F., Liu X., Ullah Z., Cheng W., Liu Y., Huang Q.",Eye gaze based 3d triangulation for robotic bionic eyes,"Three-dimensional (3D) triangulation based on active binocular vision has increasing amounts of applications in computer vision and robotics. An active binocular vision system with non-fixed cameras needs to calibrate the stereo extrinsic parameters online to perform 3D triangulation. However, the accuracy of stereo extrinsic parameters and disparity have a significant impact on 3D triangulation precision. We propose a novel eye gaze based 3D triangulation method that does not use stereo extrinsic parameters directly in order to reduce the impact. Instead, we drive both cameras to gaze at a 3D spatial point P at the optical center through visual servoing. Subsequently, we can obtain the 3D coordinates of P through the intersection of the two optical axes of both cameras. We have performed experiments to compare with previous disparity based work, named the integrated two-pose calibration (ITPC) method, using our robotic bionic eyes. The experiments show that our method achieves comparable results with ITPC. Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/ICCNEA50255.2020.00019,2020,"Lin Z., Xue P., Wang C.",Research on data fusion of pilot's los and target information,"Aiming at the problem that the precision of gaze information data extraction is not high in the process of pilot's; flight, this paper puts forward the research on the fusion of the data of the landing point of sight and the target information, and feeds back the information through human-computer interaction to obtain the accurate target information. First, the non wearable eye tracker is used to track the line of sight, estimate the fixation point, obtain the fixation point coordinates, and match with the position coordinates of the instrument panel to get the dial where the fixation point is. Then, data fusion is carried out between the instrument panel where the fixation point is located and the flight parameters to obtain the information displayed on the instrument panel. Finally, the fixation information of the instrument panel is used as the input data to support the identification of the pilot's intention. Secondly, data fusion is carried out between the pilot's attention information and reaction time information, and a large amount of data is used as the input data. Machine learning is adopted Methods after information fusion, the decision- making of pilots was predicted. Â© 2020 IEEE.",,,
10.1109/CcS49175.2020.9231447,2020,"Wibirama S., Sidhawara A.G.P., Lukhayu Pritalia G., Adji T.B.",A Survey of Learning Style Detection Method using Eye-Tracking and Machine Learning in Multimedia Learning,"Current utilization of multimedia learning environment focuses on student-centered approach. This approach is based on a theory stating that learning styles affect individuals in information processing. Based on prior works, there are three main approaches to distinguish learning styles: conventional approach - such as interview and self-reporting, artificial-intelligence-based approach, and sensor-based approach. Unfortunately, there is no comparative analysis that addresses strengths and limitations of these approaches. Thus, there is no information on how and when to use these approaches appropriately. To address this limitation, we present a brief literature review of several studies in distinguishing learning styles, including their strengths and limitations. We also present insights on potential methods of detecting learning styles in multimedia learning based on eye movement data and machine learning algorithms. Our paper is useful as a guideline for developing intelligent e-learning systems based on eye tracking and machine learning. Â© 2020 IEEE.",,,
10.3390/electronics9091408,2020,"Shimauchi T., Sakurai K., Tate L., Tamura H.",Gaze-based vehicle driving evaluation of system with an actual vehicle at an intersection with a traffic light,"Due to the population aging in Japan, more elderly people are retaining their driverâ€™s licenses and the increase in the number of car accidents by elderly drivers is a social problem. To address this problem, an objective data-based method to evaluate whether elderly drivers can continue driving is needed. In this paper, we propose a car driving evaluation system based on gaze as calculated by eye and head angles. We used an eye tracking device (TalkEye Lite) made by the Takei Scientific Instruments Cooperation. For our image processing technique, we propose a gaze fixation condition using deep learning (YOLOv2-tiny). By using an eye tracking device and the proposed gaze fixation condition, we built a system where drivers could be evaluated during actual car operation. We describe our system in this paper. In order to evaluate our proposed method, we conducted experiments from November 2017 to November 2018 where elderly people were evaluated by our system while driving an actual car. The subjects were 22 general drivers (two were 80â€?9 years old, four were 70â€?9 years old, six were 60â€?9 years old, three were 50â€?9 years old, five were 40â€?9 years old and two were 30â€?9 years old). We compared the subjectsâ€?gaze information with the subjective evaluation by a professional driving instructor. As a result, we confirm that the subjectsâ€?gaze information is related to the subjective evaluation by the instructor. Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/TNNLS.2019.2945019,2020,"Hu Y., Wang W., Liu H., Liu L.",Reinforcement Learning Tracking Control for Robotic Manipulator with Kernel-Based Dynamic Model,"Reinforcement learning (RL) is an efficient learning approach to solving control problems for a robot by interacting with the environment to acquire the optimal control policy. However, there are many challenges for RL to execute continuous control tasks. In this article, without the need to know and learn the dynamic model of a robotic manipulator, a kernel-based dynamic model for RL is proposed. In addition, a new tuple is formed through kernel function sampling to describe a robotic RL control problem. In this algorithm, a reward function is defined according to the features of tracking control in order to speed up the learning process, and then an RL tracking controller with a kernel-based transition dynamic model is proposed. Finally, a critic system is presented to evaluate the policy whether it is good or bad to the RL control tasks. The simulation results illustrate that the proposed method can fulfill the robotic tracking tasks effectively and achieve similar and even better tracking performance with much smaller inputs of force/torque compared with other learning algorithms, demonstrating the effectiveness and efficiency of the proposed RL algorithm. Â© 2012 IEEE.",,,
10.1016/j.neunet.2020.06.011,2020,"Javanmardi M., Qi X.",Appearance variation adaptation tracker using adversarial network,"Visual trackers using deep neural networks have demonstrated favorable performance in object tracking. However, training a deep classification network using overlapped initial target regions may lead an overfitted model. To increase the model generalization, we propose an appearance variation adaptation (AVA) tracker that aligns the feature distributions of target regions over time by learning an adaptation mask in an adversarial network. The proposed adversarial network consists of a generator and a discriminator network that compete with each other over optimizing a discriminator loss in a mini-max optimization problem. Specifically, the discriminator network aims to distinguish recent target regions from earlier ones by minimizing the discriminator loss, while the generator network aims to produce an adaptation mask to maximize the discriminator loss. We incorporate a gradient reverse layer in the adversarial network to solve the aforementioned mini-max optimization in an end-to-end manner. We compare the performance of the proposed AVA tracker with the most recent state-of-the-art trackers by doing extensive experiments on OTB50, OTB100, and VOT2016 tracking benchmarks. Among the compared methods, AVA yields the highest area under curve (AUC) score of 0.712 and the highest average precision score of 0.951 on the OTB50 tracking benchmark. It achieves the second best AUC score of 0.688 and the best precision score of 0.924 on the OTB100 tracking benchmark. AVA also achieves the second best expected average overlap (EAO) score of 0.366, the best failure rate of 0.68, and the second best accuracy of 0.53 on the VOT2016 tracking benchmark. Â© 2020 Elsevier Ltd",,,
10.1109/CCECE47787.2020.9255696,2020,"Abdallah A.S., Elliott L.J., Donley D.",Toward Smart Internet of Things (IoT) Devices: Exploring the Regions of Interest for Recognition of Facial Expressions using Eye-gaze Tracking,"A significant portion of the internet of things (IoT) devices will become reliable products in our daily life if and only if they are equipped with strong human computer interaction (HCI) technologies, specifically visual interaction with users through affective computing. One of the major challenges faced in affective computing is recognizing facial expressions and the true emotions behind them. Despite numerous studies performed, current detection systems are ineffective at correctly identifying facial expressions with reliable accuracy, especially in case of negative expressions. Several research projects attempted to extract the recognition process that humans follow to identify facial expressions in order to replicate in smart machines without a significant success. This paper describes our interdisciplinary project whose goal is to extract and define the recognition process that humans follow when identifying the facial expressions of others. We monitor this process by identifying and analyzing the regions of interest participants look at when they are shown static emotions samples under a specific experimental setup. This paper reports the current status of data collection, experimental setup, and initial data visualization. Â© 2020 IEEE.",,,
10.1109/ICIEVicIVPR48672.2020.9306639,2020,"Mancas M., Kong P., Gosselin B.",Visual Attention: Deep Rare Features,"Human visual system is modeled in engineering field providing feature-engineered methods which detect contrasted/surprising/unusual data into images. This data is 'in-teresting' for humans and leads to numerous applications. Deep learning (DNNs) drastically improved the algorithms efficiency on the main benchmark datasets. However, DNN-based models are counter-intuitive: surprising or unusual data is by definition difficult to learn because of its low occurrence probability. In reality, DNNs models mainly learn top-down features such as faces, text, people, or animals which usually attract human attention, but they have low efficiency in extracting surprising or unusual data in the images. In this paper, we propose a model called DeepRare2019 (DR) which uses the power of DNNs feature extraction and the genericity of feature-engineered algorithms. DR 1) does not need any training, 2) it takes less than a second per image on CPU only and 3) our tests on three very different eye-tracking datasets show that DR is generic and is always in the top-3 models on all datasets and metrics while no other model exhibits such a regularity and genericity. DeepRare2019 code can be found at https://gilhub.com/numediart/VisualAttention-RareFamily. Contribution-DeepRare2019 (DR) uses the power of DNNs feature extraction and the genericity of feature-engineered algorithms to provide accurate visual attention prediction in any situation. Â© 2020 IEEE.",,,
10.1145/3388534.3407293,2020,"Li T., Huang Q., Alfaro S., Supikov A., Ratcliff J., Grover G., Azuma R.",Light-Field Displays: A View-Dependent Approach,"Most 3D displays suffer from the vergence-accommodation conflict, which is a significant contributor to eyestrain. Light-field displays avoid this conflict by directly supporting accommodation but they are viewed as requiring too much resolution to be practical, due to the tradeoff between spatial and angular resolution. We demo three light-field display prototypes that show a view-dependent approach which sacrifices viewer independence to achieve acceptable performance with reasonable display resolutions. When combined with a directional backlight and eye tracking system, this approach can provide a 3D volume from which a viewer can see 3D objects with accommodation, without wearing special glasses. Â© 2020 Owner/Author.",,,
10.1016/j.neucom.2019.10.041,2020,"Li D., Wen G., Kuai Y., Zhu L., Porikli F.",Robust visual tracking with channel attention and focal loss,"Recently, the tracking community leads a fashion of end-to-end feature representation learning for visual tracking. Previous works treat all feature channels and training samples equally during training. This ignores channel interdependencies and foregroundâ€“background data imbalance, thus limiting the tracking performance. To tackle these problems, we introduce channel attention and focal loss into the network design to enhance feature representation learning. Specifically, a Squeeze-and-Excitation (SE) block is coupled to each convolutional layer to generate channel attention. Channel attention reflects the channel-wise importance of each feature channel and is used for feature weighting in online tracking. To alleviate the foregroundâ€“background data imbalance, we propose a focal logistic loss by adding a modulating factor to the logistic loss, with two tunable focusing parameters. The focal logistic loss down-weights the loss assigned to easy examples in the background area. Both the SE block and focal logistic loss are computationally lightweight and impose only a slight increase in model complexity. Extensive experiments are performed on three challenging tracking datasets including OTB100, UAV123 and TC128. Experimental results demonstrate that the enhanced tracker achieves significant performance improvement while running at a real-time frame-rate of 66 fps. Â© 2019",,,
10.1109/CoG47356.2020.9231892,2020,"Lee-Cultura S., Sharma K., Papavlasopoulou S., Giannakos M.",Motion-Based Educational Games: Using Multi-Modal Data to Predict Player's Performance,"Multi-Modal Data (MMD) can help educational games researchers understand the synergistic relationship between player's movement and their learning experiences, and consequently uncover insights that may lead to improved design of movement-based game technologies for learning. Predicting player performance fosters opportunities to cultivate heightened educational experiences and outcomes. However, predicting player's performance utilising player-generated MMD during their interactions with educational Motion-Based Touchless Games (MBTG) is challenging. To bridge this gap, we implemented an in-situ study where 26 users, age 11, played 2 maths MBTGs in a single 20-30 minute session. We collected player's MMD (i.e., gaze data from eye-tracking glasses, physiological data from wristbands, and skeleton data from Kinect) produced during game-play. To investigate the potential of MMD for predicting player's academic performance, we used machine learning techniques and MMD derived from player's game-play. This allowed us to identify the MMD features that drive rapid highly accurate predictions of players' academic performance in educational MBTGs. This might allow us to provide real-time proactive feedback to the player to support them through their educational gaming experience. Our analysis compared two data lengths corresponding to half and full duration of the player's question solving time. We showed that all combinations of extracted features associated with gaze, physiological, and skeleton data, predicted student performance more accurately than the majority baseline. Additionally, the most accurate prediction of player's performance derived from the combination of gaze and physiological data for both full and half data lengths. Our findings emphasise the significance of using MMD for real-time performance prediction in educational MBTG and offer implications for practice. Â© 2020 IEEE.",,,
10.1109/RO-MAN47096.2020.9223506,2020,"Minegishi T., Osawa H.",Do You Move Unconsciously? Accuracy of Social Distance and Line of Sight between a Virtual Robot Head and Humans,"In this paper, we examine the effectiveness of the social distance between a virtual agent and users, and the gaze instruction using a display that can be viewed stereoscopically without using any wearable devices. An actual robot cannot always maintain an appropriate interpersonal distance, through nonverbal gestures owing to its limited range of motion. Because large movements may harm humans, the nonverbal gestures of robots are limited in the real world. In this work, 14 participants were asked how far they wanted to move from a robot posing as a museum guide agent. They were also asked to identify the point at which they felt the agent was gazing. There was a significant distance between the initial position and the position to which the participants moved in the first task under two-dimensional (2D) and three-dimensional (3D) scenarios. The participants moved a significant distance in the first task. In the gaze estimation task, however, the error between the 3D and 2D evaluations was significantly lesser, at a point far from the agent. Â© 2020 IEEE.",,,
10.1109/ICSSIT48917.2020.9214290,2020,"Akshay S., Megha Y.J., Shetty C.B.",Machine learning algorithm to identify eye movement metrics using raw eye tracking data,"Eye-tracking studies in software engineering are becoming more prevalent and also in the areas like medical, gaming and commercial fields. Researchers may use the same metrics but it is majorly used to give a different name for same field that cause the difficulties in comparing studies, so in this work, a model is developed to reduce the existing challenges. Many existing algorithms are available to apply on eye tracking data but machine learning is one of the best algorithms, for example random forest is one the machine learning algorithms, which helps to hold the test set. In the eye movement metrics, the dataset will be divided into two sets they are: test set and training set. This paper reports on the eye-tracking metries using raw eye-tracking data. The proposed research work has used random forest, decision tree, KNN and SVM for experimentation in order to understand the dataset. The objective of this study is two-fold. First, the identification of various eye movement metrics events and Second, Apply visualization technique. It can be applied in medical field. Here first we will identify the accuracy, recall, precision and f-measure between KNN classifier and SVM, then identifying the eye movement metrics using machine learning algorithm. We give in this research a brief description of the eye movement metrics and which machine algorithm would give the best result, with its applications. Â© 2020 IEEE.",,,
10.1109/RE48521.2020.00064,2020,Ahrens M.,Towards Automatic Capturing of Traceability Links by Combining Eye Tracking and Interaction Data,"Despite its numerous scientifically cited benefits, traceability is still rarely established in industrial settings. Years of research in the area have brought several different approaches to create traceability links including Information Retrieval (IR) and Machine Learning approaches. However, their accuracy and overall traceability support is not sufficient yet to be properly applied in practice. In my research, I want to investigate the usage of eye tracking and interaction data in the field of traceability. By tracking how software engineers interact with documents, where they focus on and recording gaze links between those documents, an algorithm is designed to obtain trace links between artifacts from these data. Eye tracking and interaction data have the advantage that they can be recorded in an automatic, non-intrusive way without requiring manual effort. They give detailed insight about where people focus on when working on tasks. However, software support of eye tracking is still limited, especially in the context of dynamic content such as switching between, scrolling or editing documents. Therefore, one essential step of my research is to provide software support for recording eye tracking data in dynamic document environments. By combining eye tracking data with additionally recorded metadata such as interactions, this eye tracking framework shall enable the automatic capturing of gaze links and gaze durations during software engineering tasks. The approach of using eye tracking in the context of traceability will be evaluated in several usage scenarios such as requirements coverage assessment. Â© 2020 IEEE.",,,
10.1109/SeGAH49190.2020.9201699,2020,"Delvigne V., Ris L., Dutoit T., Wannous H., Vandeborre J.-P.",VERA: Virtual Environments Recording Attention,"Children with Attention Deficit Hyperactivity Disorder (ADHD), present different symptoms binding for everyday life, e.g. difficulty to be focused, impulsiveness, difficulty to regulate motor functions, etc. The most commonly prescribed treatment is the medication that can present side effects. Another solution is behavioural treatment that does not seem to present better results than medication for a higher cost. A novel method with growing interest is the use of neurofeedback (NF) to teach the patient to self-regulate symptoms by herself, through the visualisation of the brain activity in an understandable form. Moreover, virtual reality (VR) is a supportive environment for NF in the context of ADHD. However, before proceeding the NF, it is important to determine the features of the physiological signals corresponding to the symptoms' appearance. We present here a novel framework based on the joint measurement of electroencephalogram (EEG) and sight direction by equipment that can be embedded in VR headset, the goals being to estimate attentional state. In parallel to the signal acquisition, attentional tasks are performed to label the physiological signals. Features have been extracted from the signals and machine learning (ML) models have been applied to retrieve the attentional state. Encouraging results have been provided from the pilot study with the ability to make the right classification in multiple scenarios. Moreover, a dataset with the labelled physiological signals is under development. It will help to have a better understanding of the mechanism behind ADHD symptoms. Â© 2020 IEEE.",,,
10.1109/CSCloud-EdgeCom49738.2020.00054,2020,"Wang Y., Kotha A., Hong P.-H., Qiu M.",Automated Student Engagement Monitoring and Evaluation during Learning in the Wild,"With the explosive growth of edge computing and massive open online courses (MOOCs), there is an urgent need to enable pervasive learning so that students could study with high efficiency at any comfortable places at their own pace. Although there have been a number of studies and applications for student engagement monitoring and evaluation in the pervasive learning, most of the existing works are either supported by commercial eye tracking devices/software or designed for off-line studies on the basis of questionnaires, self-reports, checklists, quizzes, teacher introspective evaluations, and assignments. In this work, we investigate the feasibility of real-Time student engagement monitoring and evaluation with low-cost off-The-shelf web-cameras in realistic learning scenarios. To recognizing and evaluating student engagement, a new model is developed and trained by a deep learning Convolutional Neural Network (CNN) with an open source dataset. The quantitative experimental results demonstrate that the deep learning CNN and our model work well and efficiently when monitoring student learning and detecting student engagement in real time. Â© 2020 IEEE.",,,
10.1109/MIPR49039.2020.00058,2020,"Tao Y., Coltey E., Wang T., Alonso M., Shyu M.-L., Chen S.-C., Alhaffar H., Elias A., Bogosian B., Vassigh S.",Confidence Estimation Using Machine Learning in Immersive Learning Environments,"As the development of Virtual Reality and Augmented Reality (VR/AR) technology rapidly advances, learning in an artificial immersive environment becomes increasingly feasible. Such emerging technology not only facilitates and promotes an efficient learning process, but also reduces the cost of access to learning materials and environments. Current research mainly focuses on the development of immersive learning environments and the adaptive learning methods based on interactions between trainees and the environment. However, valuable human biometric data available in immersive environments, such as eye gaze and controller pose, have not been explored and utilized to help understand the affective state of the trainees. In this paper, we propose a machine-learning based research framework to estimate trainees' confidence about their decisions in immersive learning environments. Using this framework, we designed an experiment to collect biometric data from a multiple-choice question and answer session in an immersive learning environment. This includes collecting answers from 10 participants on 35 questions and their self-reported confidence in their answers. A Long Short-Term Memory neural network model was used to analyze the data and estimate the confidence with 85.6% accuracy. Â© 2020 IEEE.",,,
10.1109/TITS.2019.2937287,2020,"Deng Q., Wang J., Hillebrand K., Benjamin C.R., Soffker D.",Prediction Performance of Lane Changing Behaviors: A Study of Combining Environmental and Eye-Tracking Data in a Driving Simulator,"Advanced Driver Assistance Systems (ADAS) are systems developed to assist the human driver and therefore to make driving safer and better. Understanding and predicting human driving behavior play an important role in the development of assistance systems. In this contribution, the development of a driver assistance system is based on the prediction of driving behaviors. The driving patterns of three different behaviors are modeled including left/right lane change and lane keeping. A driving simulator is used to simulate a highway scene. The implementation of a prediction system based on different machine learning approaches such as Hidden Markov Model (HMM), Support Vector Machine (SVM), Convolutional neural networks (CNNs), and Random Forest (RF) is accomplished. In addition, eye-tracking information is integrated. The task is to predict behaviors based on the measurement. As test, a 10-fold cross-validation is used based on data sets from driving simulator and applied to compare the performance of different algorithms. In combination with related results in terms of accuracy (ACC), detection rate (DR), and false alarm rate (FAR), the performance and effectiveness of the developed prediction systems are evaluated. The results show that the performance of RF algorithm is the best of all four algorithms compared. Combining environmental and eye-tracking data the RF algorithm achieved the best results. All ACC values are larger than 99 %. Afterwards, two RF-based prediction models with and without eye-tracking data are developed for online test. Finally, some application samples are suggested for driver assistance. The results calculated by the proposed model are shown on a user interface to help the drivers to see when it is suitable to turn left, to turn right, or to keep the direction. Â© 2000-2011 IEEE.",,,
10.1145/3397271.3401449,2020,Baranova-Bolotova V.,Multi-Document Answer Generation for Non-Factoid Questions,"The current research will be devoted to the challenging and under-investigated task of multi-source answer generation for complex non-factoid questions. We will start with experimenting with generative models on one particular type of non-factoid questions-instrumental/procedural questions which often start with ""how-to"". For this, a new dataset, comprised of more than 100,000 QA-pairs which were crawled from a dedicated web-resource where each answer has a set of references to the articles it was written upon, will be used. We will also compare different ways of model evaluation to choose a metric which better correlates with human assessment. To be able to do this, the way people evaluate answers to non-factoid questions and set some formal criteria of what makes a good quality answer is needed to be understood. Eye-tracking and crowdsourcing methods will be employed to study how users interact with answers and evaluate them, and how the answer features correlate with task complexity. We hope that our research will help to redefine the way users interact and work with search engines so as to transform IR finally into the answer retrieval systems that users have always desired. Â© 2020 Owner/Author.",,,
10.1016/j.neucom.2020.03.021,2020,"Zheng Z., Ruan L., Zhu M., Guo X.",Reinforcement learning control for underactuated surface vessel with output error constraints and uncertainties,"This study investigates the trajectory tracking control problem of an underactuated marine vessel in the presence of output constraints, model uncertainties and environmental disturbances. The error transformation technique can ensure that the tracking errors remain within the predefined constraint boundaries. The controller is designed in combination with the critic function and the reinforcement learning (RL) algorithm based on actor-critic neural networks. The RL method is applied to solve model uncertainties and disturbances, and the critic function modifies the control action to supervise the system performance. Based on Lyapunov's direct method, a stability analysis is proposed to prove that the boundedness of system signals and the desired tracking performance can be guaranteed. Finally, the simulation illustrates the effectiveness and feasibility of the proposed controller. Â© 2020 Elsevier B.V.",,,
10.1016/j.neucom.2020.02.095,2020,"Dai M., Xiao G., Cheng S., Wang D., He X.",Structural correlation filters combined with a Gaussian particle filter for hierarchical visual tracking,"Visual tracking is a key problem for many computer vision applications such as human-computer interaction, intelligent medical diagnosis, navigation and traffic control management. Most of the existing tracking methods are mainly based on correlation filters. However, boundary effect, scale estimation and template updating have not been fully resolved. Herein, this paper presents a new hierarchical tracking method combining structural correlation filters with a Gaussian Particle Filter (GPF), named KCF-GPF. Weak KCF classifiers are constructed via a Lukas-Kanade (LK) method and the preliminary target location is presented as a weighted sum of these classifiers. Specially, a facile weight strategy is implemented to estimate the reliability of each weak classifier. On the basis of the preliminary target location, the GPF using features from a Convolutional Neural Network (CNN) is employed to predict the location and scale of a target. Extensive experiments with the OTB-2013 and the OTB-2015 databases demonstrate that the proposed algorithm performs favourably against state-of-the-art trackers. Â© 2020",,,
10.1109/IISA50023.2020.9284374,2020,"Aivaliotis P.-E., Grivokostopoulou F., Perikos I., Daramouskas I., Hatziligeroudis I.",Eye Gaze Analysis of Students in Educational Systems,"Eye gaze provides indicative information about the status and the behavior of a person and can be very assistive in human-computer interaction. Eye-gaze analysis is very helpful in a variety of applications in order to understand the interest of the users, their behavior or even to unveil distractions. However, the accurate eye-gaze estimation is a very challenging process. In this paper, we present an eye gaze estimation work that relies on convolutional neural networks which imitate the LeNet's architecture. They analyze eye gaze and provide a 2D vector that concerns the coordinates of the specific pixel inside the 2D screen's space, in which the user is looking at. Also, a system capable of working under various real-world conditions such as light, angle and distance differentiations was designed and developed. An evaluation study was performed and the results are quite promising pointing out that the system is scalable and accurate in estimating the eye gaze of the users. Â© 2020 IEEE.",,,
10.3390/s20143936,2020,"Dahmani M., Chowdhury M.E.H., Khandakar A., Rahman T., Al-Jayyousi K., Hefny A., Kiranyaz S.",An intelligent and low-cost eye-tracking system for motorized wheelchair control,"In the 34 developed and 156 developing countries, there are ~132 million disabled people who need a wheelchair, constituting 1.86% of the world population. Moreover, there are millions of people suffering from diseases related to motor disabilities, which cause inability to produce controlled movement in any of the limbs or even head. This paper proposes a system to aid people with motor disabilities by restoring their ability to move effectively and effortlessly without having to rely on others utilizing an eye-controlled electric wheelchair. The system input is images of the userâ€™s eye that are processed to estimate the gaze direction and the wheelchair was moved accordingly. To accomplish such a feat, four user-specific methods were developed, implemented, and tested; all of which were based on a benchmark database created by the authors. The first three techniques were automatic, employ correlation, and were variants of template matching, whereas the last one uses convolutional neural networks (CNNs). Different metrics to quantitatively evaluate the performance of each algorithm in terms of accuracy and latency were computed and overall comparison is presented. CNN exhibited the best performance (i.e., 99.3% classification accuracy), and thus it was the model of choice for the gaze estimator, which commands the wheelchair motion. The system was evaluated carefully on eight subjects achieving 99% accuracy in changing illumination conditions outdoor and indoor. This required modifying a motorized wheelchair to adapt it to the predictions output by the gaze estimation algorithm. The wheelchair control can bypass any decision made by the gaze estimator and immediately halt its motion with the help of an array of proximity sensors, if the measured distance goes below a well-defined safety margin. This work not only empowers any immobile wheelchair user, but also provides low-cost tools for the organization assisting wheelchair users. Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/IJCNN48605.2020.9207709,2020,"Dari S., Kadrileev N., Hullermeier E.",A Neural Network-Based Driver Gaze Classification System with Vehicle Signals,"Driver monitoring can play an essential part in avoiding accidents by warning the driver and shifting the driver's attention to the traffic scenery in time during critical situations. This may apply for the different levels of automated driving, for take-over requests as well as for driving in manual mode. A great proxy for this purpose has always been the driver's gazing direction. The aim of this work is to introduce a robust gaze detection system. In this regard, we make several contributions that are novel in the area of gaze detection systems. In particular, we propose a deep learning approach to predict gaze regions, which is based on informative features such as eye landmarks and head pose angles of the driver. Moreover, we introduce different post-processing techniques that improve the accuracy by exploiting temporal information from videos and the availability of other vehicle signals. Last but not least, we confirm our method with a leave-one-driver-out cross-validation. Unlike previous studies, we do not use gazes to predict maneuver changes, but we consider the human-computer-interaction aspect and use vehicle signals to improve the performance of the estimation. The proposed system is able to achieve an accuracy of 92.3% outperforming earlier landmark-based gaze estimators. Â© 2020 IEEE.",,,
10.1109/IJCNN48605.2020.9207189,2020,"Riad Saboundji R., Adrian Rill R.",Predicting Human Errors from Gaze and Cursor Movements,"Intelligent interfaces are increasingly integrated into diverse technological areas. In complex high-risk environments, where humans represent a crucial part of the system and their attention is often divided between simultaneous activities, imminent human errors may have serious consequences. Enhancing interfaces with predictive capabilities promotes the safe and reliable operation of such systems. In this work, we employ a data-driven approach to predict human errors in a special divided attention task involving timing constraints and requiring focused concentration and frequent shifts of attention. We performed a longitudinal study with 10 subjects, and constructed time series from the experimental data using gaze movement and mouse cursor motion features in order to classify successful and failed actions. We evaluate classical machine learning algorithms, compare them with a more traditional temporal modeling approach and a deep learning based LSTM model. Employing a leave-one- subject-out cross-validation procedure we achieve a classification accuracy of up to 86%, with LSTM presenting the highest performance. Furthermore, we investigate the trade-off between evaluation metrics and anticipation window, i.e. the time remaining until the correct action can still be performed. We conclude that prediction is feasible and accuracy and F1-score increases, despite the training dataset becoming greatly imbalanced. Investigating the anticipation window allows to understand how far in advance human errors need to be predicted in order to initiate preventive measures. Our efforts have implications for the design of predictive interfaces involving decision making under time pressure in dynamic divided attention environments. Â© 2020 IEEE.",,,
10.1109/ICCSP48568.2020.9182316,2020,"Vismaya U.K., Saritha E.",A Review on Driver Distraction Detection Methods,"One of the most relevant reason for death is road crashes. Road crashes may occur for many reasons. One of the causes is distracted driving. Distracted driving is the deviation of the driver's consciousness from driving tasks due to some secondary tasks like eating talking etc. It may cause death, injuries and economic losses. The alert given to the driver on time can avoid most of these problems and improve transportation safety. However, the development of such systems can solve many difficulties related to fast and proper recognition of a driver's behavior. Different methods have been used to determine the forms of driver disturbances and to explain the consequences in terms of driving performance and participation in crashes. There are three different types of distractions present, visual distraction, manual distraction, and cognitive distraction. Head, eye movements are one of the strong indicators of driver's distraction. We know remote eye tracking has emerged recently, which gives an opportunity of real-time identification of visual distraction. In this paper discussed visual along with the manual distractions. Â© 2020 IEEE.",,,
10.1109/CBMS49503.2020.00036,2020,"Putra P., Shima K., Shimatani K.",Catchicken: A serious game based on the go/nogo task to estimate inattentiveness and impulsivity symptoms,"We present a Go/NoGo 3D game equipped with an eye tracker that records subjects' responses and his gaze position on the monitor over time. The proposed system consists of two functions: training that allows an instructor to modify the game's parameters and make a customized test; and evaluation in which the instructor can fix the parameters to create a standardized test. During the experiment, subjects were required to respond only to Go character by pressing a spacebar. The experimental results from 59 participants demonstrated that one's response time and its variability correlated with one's gaze behavior. Subjects with higher gaze modulation tended to respond faster and more stable. We also observed that utilizing the proposed system we could monitor the improvements in an Autism Spectrum Disorder child during his rehabilitation: his gaze modulation increased and his response time became more steady. In brief, utilizing the proposed system, we could effectively measure participants' response time variability of NoGo errors and their gaze trajectory area, which previous studies found to have a strong relationship with symptoms of mental disorders. Â© 2020 IEEE.",,,
10.1109/EMBC44109.2020.9176843,2020,"Jiang M., Francis S.M., Tseng A., Srishyla D., Dubois M., Beard K., Conelea C., Zhao Q., Jacob S.",Predicting Core Characteristics of ASD Through Facial Emotion Recognition and Eye Tracking in Youth,"Autism Spectrum Disorder (ASD) is a heterogeneous neurodevelopmental disorder (NDD) with a high rate of comorbidity. The implementation of eye-tracking methodologies has informed behavioral and neurophysiological patterns of visual processing across ASD and comorbid NDDs. In this study, we propose a machine learning method to predict measures of two core ASD characteristics: impaired social interactions and communication, and restricted, repetitive, and stereotyped behaviors and interests. Our method extracts behavioral features from task performance and eye-tracking data collected during a facial emotion recognition paradigm. We achieved high regression accuracy using a Random Forest regressor trained to predict scores on the SRS-2 and RBS-R assessments; this approach may serve as a classifier for ASD diagnosis. Â© 2020 IEEE.",,,
10.3390/s20133785,2020,"Khan W., Hussain A., Kuru K., Al-Askar H.",Pupil localisation and eye centre estimation using machine learning and computer vision,"Various methods have been used to estimate the pupil location within an image or a real-time video frame in many fields. However, these methods lack the performance specifically in low-resolution images and varying background conditions. We propose a coarse-to-fine pupil localisation method using a composite of machine learning and image processing algorithms. First, a pre-trained model is employed for the facial landmark identification to extract the desired eye frames within the input image. Then, we use multi-stage convolution to find the optimal horizontal and vertical coordinates of the pupil within the identified eye frames. For this purpose, we define an adaptive kernel to deal with the varying resolution and size of input images. Furthermore, a dynamic threshold is calculated recursively for reliable identification of the best-matched candidate. We evaluated our method using various statistical and standard metrics along with a standardised distance metric that we introduce for the first time in this study. The proposed method outperforms previous works in terms of accuracy and reliability when benchmarked on multiple standard datasets. The work has diverse artificial intelligence and industrial applications including human computer interfaces, emotion recognition, psychological profiling, healthcare, and automated deception detection. Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1142/S0129065720500252,2020,"De Lope J., GraÃ±a M.",Behavioral Activity Recognition Based on Gaze Ethograms,"Noninvasive behavior observation techniques allow more natural human behavior assessment experiments with higher ecological validity. We propose the use of gaze ethograms in the context of user interaction with a computer display to characterize the user's behavioral activity. A gaze ethogram is a time sequence of the screen regions the user is looking at. It can be used for the behavioral modeling of the user. Given a rough partition of the display space, we are able to extract gaze ethograms that allow discrimination of three common user behavioral activities: reading a text, viewing a video clip, and writing a text. A gaze tracking system is used to build the gaze ethogram. User behavioral activity is modeled by a classifier of gaze ethograms able to recognize the user activity after training. Conventional commercial gaze tracking for research in the neurosciences and psychology science are expensive and intrusive, sometimes impose wearing uncomfortable appliances. For the purposes of our behavioral research, we have developed an open source gaze tracking system that runs on conventional laptop computers using their low quality cameras. Some of the gaze tracking pipeline elements have been borrowed from the open source community. However, we have developed innovative solutions to some of the key issues that arise in the gaze tracker. Specifically, we have proposed texture-based eye features that are quite robust to low quality images. These features are the input for a classifier predicting the screen target area, the user is looking at. We report comparative results of several classifier architectures carried out in order to select the classifier to be used to extract the gaze ethograms for our behavioral research. We perform another classifier selection at the level of ethogram classification. Finally, we report encouraging results of user behavioral activity recognition experiments carried out over an inhouse dataset. Â© 2020 The Author(s).",,,
10.1109/LRA.2020.2998410,2020,"Kim H., Ohmura Y., Kuniyoshi Y.",Using Human Gaze to Improve Robustness against Irrelevant Objects in Robot Manipulation Tasks,"Deep imitation learning enables the learning of complex visuomotor skills from raw pixel inputs. However, this approach suffers from the problem of overfitting to the training images. The neural network can easily be distracted by task-irrelevant objects. In this letter, we use the human gaze measured by a head-mounted eye tracking device to discard task-irrelevant visual distractions. We propose a mixture density network-based behavior cloning method that learns to imitate the human gaze. The model predicts gaze positions from raw pixel images and crops images around the predicted gazes. Only these cropped images are used to compute the output action. This cropping procedure can remove visual distractions because the gaze is rarely fixated on task-irrelevant objects. This robustness against irrelevant objects can improve the manipulation performance of robots in scenarios where task-irrelevant objects are present. We evaluated our model on four manipulation tasks designed to test the robustness of the model to irrelevant objects. The results indicate that the proposed model can predict the locations of task-relevant objects from gaze positions, is robust to task-irrelevant objects, and exhibits impressive manipulation performance especially in multi-object handling. Â© 2016 IEEE.",,,
10.1007/s11548-020-02149-4,2020,"Wang C., Komninos C., Andersen S., Dâ€™Ettorre C., Dwyer G., Maneas E., Edwards P., Desjardins A., Stilli A., Stoyanov D.",Ultrasound 3D reconstruction of malignant masses in robotic-assisted partial nephrectomy using the PAF rail system: a comparison study,"Purpose: In robotic-assisted partial nephrectomy (RAPN), the use of intraoperative ultrasound (IOUS) helps to localise and outline the tumours as well as the blood vessels within the kidney. The aim of this work is to evaluate the use of the pneumatically attachable flexible (PAF) rail system for US 3D reconstruction of malignant masses in RAPN. The PAF rail system is a novel device developed and previously presented by the authors to enable track-guided US scanning. Methods: We present a comparison study between US 3D reconstruction of masses based on: the da Vinci Surgical System kinematics, single- and stereo-camera tracking of visual markers embedded on the probe. An US-realistic kidney phantom embedding a mass is used for testing. A new design for the US probe attachment to enhance the performance of the kinematic approach is presented. A feature extraction algorithm is proposed to detect the margins of the targeted mass in US images. Results: To evaluate the performance of the investigated approaches the resulting 3D reconstructions have been compared to a CT scan of the phantom. The data collected indicates that single camera reconstruction outperformed the other approaches, reconstructing with a sub-millimetre accuracy the targeted mass. Conclusions: This work demonstrates that the PAF rail system provides a reliable platform to enable accurate US 3D reconstruction of masses in RAPN procedures. The proposed system has also the potential to be employed in other surgical procedures such as hepatectomy or laparoscopic liver resection. Â© 2020, The Author(s).",,,
10.1016/j.patrec.2020.04.028,2020,"Fabiano D., Canavan S., Agazzi H., Hinduja S., Goldgof D.",Gaze-based classification of autism spectrum disorder,"People with autism spectrum disorder (ASD) display impairments in social interaction and communication skills, as well as restricted interests and repetitive behaviors, which greatly affect daily life functioning. Current identification of ASD involves a lengthy process that requires an experienced clinician to assess multiple domains of functioning. Considering this, we propose a method for classifying multiple levels of risk of ASD using eye gaze and demographic feature descriptors such as a subject's age and gender. We construct feature descriptors that incorporate the subject's age and gender, as well as features based on eye gaze patterns. We also present an analysis of eye gaze patterns validating the use of the selected hand-crafted features. We assess the efficacy of our descriptors to classify ASD on a National Database for Autism Research dataset, using multiple classifiers including a random forest, C4.5 decision tree, PART, and a deep feedforward neural network. Â© 2020 Elsevier B.V.",,,
10.1109/TNB.2020.2990690,2020,"Zhu J., Wang Z., Gong T., Zeng S., Li X., Hu B., Li J., Sun S., Zhang L.",An Improved Classification Model for Depression Detection Using EEG and Eye Tracking Data,"At present, depression has become a main health burden in the world. However, there are many problems with the diagnosis of depression, such as low patient cooperation, subjective bias and low accuracy. Therefore, reliable and objective evaluation method is needed to achieve effective depression detection. Electroencephalogram (EEG) and eye movements (EMs) data have been widely used for depression detection due to their advantages of easy recording and non-invasion. This research proposes a content based ensemble method (CBEM) to promote the depression detection accuracy, both static and dynamic CBEM were discussed. In the proposed model, EEG or EMs dataset was divided into subsets by the context of the experiments, and then a majority vote strategy was used to determine the subjects' label. The validation of the method is testified on two datasets which included free viewing eye tracking and resting-state EEG, and these two datasets have 36,34 subjects respectively. For these two datasets, CBEM achieves accuracies of 82.5% and 92.65% respectively. The results show that CBEM outperforms traditional classification methods. Our findings provide an effective solution for promoting the accuracy of depression identification, and provide an effective method for identificationof depression, which in the future could be used for the auxiliary diagnosis of depression. Â© 2002-2011 IEEE.",,,
10.1016/j.chb.2020.106329,2020,Couture Bue A.C.,The looking glass selfie: Instagram use frequency predicts visual attention to high-anxiety body regions in young women,"Existing research links social media use with body dissatisfaction and increased appearance comparison, demonstrating that photo-based behaviors such as taking and posting â€˜selfiesâ€?may lead to increased risk of body image disturbance (Cohen, Newton-John, & Slater, 2017). Social media users frequently interact with photos of themselves online while using social media platforms. When looking at a self-photo, individuals can selectively focus attention on body regions that are self-reported as attractive or unattractive; attention to unattractive regions can lead to increased body dissatisfaction (Smeets, Jansen, & Roefs, 2011). Likewise, body dissatisfaction is a predictor of selective attention to self-reported unattractive regions (Glashouwer, Jonker, Thomassen, & de Jong, 2016), creating a reinforcing cycle. The present study used eye-tracking methods to examine how 157 women aged 18â€?5 visually processed a self-photo, measuring attention to self-reported high- and low-anxiety body regions. Even accounting for baseline body dissatisfaction, Instagram but not Facebook use frequency predicted greater visual attention to high-anxiety body regions, with physical appearance comparison and body dissatisfaction serving as serial mediators. Discussion focuses on social comparison theory, suggesting that Instagram (a highly visual platform) encourages upward social comparisons when self-evaluating, prompting attention to body regions perceived as less attractive. Â© 2020 Elsevier Ltd",,,
10.1145/3389189.3389191,2020,"Neumann A., Strenge B., Uhlich J.C., Schlicher K.D., Maier G.W., Schalkwijk L., WaÃŸmuth J., Essig K., Schack T.",AVIKOM: Towards a mobile audiovisual cognitive assistance system for modern manufacturing and logistics,"This paper introduces the novel Augmented Reality (AR) assistance system AVIKOM, a joint endeavour of three research groups together with four small and medium-sized enterprises (SME) as well as network partners and a diaconal institution. In particular, we investigate how AR-enabled assistance systems can be tailored to individual requirements of workers with diverse cognitive and physical capabilities for today's real-world industrial applications in the areas of (manual) assembly, logistics and operation of industrial machinery. We combine best practices from the domains of artificial intelligence, machine learning, user experience engineering, ethics research, and cognitive science with state-of-the-art insights for multi-modal system development to create a cognitive action assistance system that recognizes and adapts to individual users in various situational contexts, such as picking and training. Proven work and organizational psychology methods and worth-related evaluations will accompany the system introduction into working environments. Using user- and worth-centred system design and change management strategies (e.g. information and participation) right from the beginning of such a technological development facilitates proper involvement of future users in the development process. This can lead to better congruence of technology features with workers' requirements and positively shape future users' attitudes towards the system. Â© 2020 ACM.",,,
10.1145/3408127.3408179,2020,"Cha X., Yang X., Zhang Y., Feng Z., Xu T., Fan X.",Eye Tracking in Driving Environment Based on Multichannel Convolutional Neural Network,"Gaze is the most important way for human to obtain information from the outside world, and it is the most direct and significant cue to analysis human behavior and intention. In driving environment, eye tracking is usually applied to model driver's fixations and gaze allocations, which is important in advanced driver assistance system (ADAS). In this paper, we have proposed a new eye tracking method in driving environment, which is based on multichannel convolutional neural network. Firstly, we establish the dataset for driver's eye tracking, which includes the left eye region image, the right eye region image and the face region image. After that, the multi-channel convolutional neural network is training using the dataset. Finally, the driver's gaze zone will be estimated using the pre-trained network. Experimental results show that the accuracy of the proposed method is 94.60% for seven gaze zone estimation, and it can be used in ADAS to analysis the driver's behavior and detect driver distraction. Â© 2020 ACM.",,,
10.1145/3408127.3408198,2020,"Song H., Yang M., Li T., Chen S.",Fixation Points Estimation Based on Binocular Stereo Vision,"This paper proposes a method of fixation points estimation combining the two-dimension mapping model with three-dimension stereo vision. The purpose of our study is to design an easy-to-use non-contact gaze tracking system under natural light. The method of two-dimension mapping is easy to achieve, however, it needs the user to keep the head being fixed. It could achieve higher estimation accuracy though it is still not easy for users to use the algorithm. To solve this problem, we have introduced the binocular cameras to calculate the pose of head and then add the related result into the result of 2D mapping to compensate the movement of head. The average error angles of gaze estimation in the case head fixed and head moving are 5.1Â°and 8.5Â°, respectively. This proposed method is easy to achieve and the experiment device is easy to mounted. Â© 2020 ACM.",,,
10.1145/3386901.3388917,2020,"Wu H., Feng J., Tian X., Sun E., Liu Y., Dong B., Xu F., Zhong S.",EMO: Real-time emotion recognition from single-eye images for resource-constrained eyewear devices,"Real-time user emotion recognition is highly desirable for many applications on eyewear devices like smart glasses. However, it is very challenging to enable this capability on such devices due to tightly constrained image contents (only eye-area images available from the on-device eye-tracking camera) and computing resources of the embedded system. In this paper, we propose and develop a novel system called EMO that can recognize, on top of a resource-limited eyewear device, real-time emotions of the user who wears it. Unlike most existing solutions that require whole-face images to recognize emotions, EMO only utilizes the single-eye-area images captured by the eye-tracking camera of the eyewear. To achieve this, we design a customized deep-learning network to effectively extract emotional features from input single-eye images and a personalized feature classifier to accurately identify a user's emotions. EMO also exploits the temporal locality and feature similarity among consecutive video frames of the eye-tracking camera to further reduce the recognition latency and system resource usage. We implement EMO on two hardware platforms and conduct comprehensive experimental evaluations. Our results demonstrate that EMO can continuously recognize seven-type emotions at 12.8 frames per second with a mean accuracy of 72.2%, significantly outperforming the state-of-the-art approach, and consume much fewer system resources. Â© 2020 ACM.",,,
10.1145/3379157.3391653,2020,"Koorathota S.C., Thakoor K., Adelman P., Mao Y., Liu X., Sajda P.",Sequence Models in Eye Tracking: Predicting Pupil Diameter during Learning,"A deep learning framework for predicting pupil diameter using eye tracking data is described. Using a variety of input, such as fixation positions, durations, saccades and blink-related information, we assessed the performance of a sequence model in predicting future pupil diameter in a student population as they watched educational videos in a controlled setting. Through assessing student performance on a post-viewing test, we report that deep learning sequence models may be useful for separating components of pupil responses that are linked to luminance and accommodation from those that are linked to cognition and arousal. Â© 2020 ACM.",,,
10.1145/3379157.3391996,2020,Jogeshwar A.K.,Analysis and visualization tool for motion and gaze,"Observers' gaze is studied as a marker of attention, and by tracking the eyes, one can obtain gaze data. Attention of an individual performing natural tasks such as making a sandwich, playing squash, or teaching a class can be studied with the help of eye-tracking. Data analysis of real world interaction is challenging and time-consuming as it consists of varying or undefined environments, massive amounts of video data and unrestricted movement. To approach these challenges, my research aims to create an interactive four-dimensional (x,y,z,t) tool for the analysis and visualization of observer motion and gaze data, of one or more observers performing natural day-to-day tasks. Three solutions are necessary to achieve this goal: Simulation of the environment with the ability to vary viewpoint, gaze visualization from two-dimensional scene to three-dimensions, and tracing of the observer(s) motion. The approaches to these challenges are described in the following sections. Â© 2020 Owner/Author.",,,
10.1145/3379157.3391420,2020,"Lohr D.J., Aziz S., Komogortsev O.",Eye Movement Biometrics Using a New Dataset Collected in Virtual Reality,"This paper introduces a novel eye movement dataset collected in virtual reality (VR) that contains both 2D and 3D eye movement data from over 400 subjects. We establish that this dataset is suitable for biometric studies by evaluating it with both statistical and machine learning-based approaches. For comparison, we also include results from an existing, similarly constructed dataset. Â© 2020 Owner/Author.",,,
10.1145/3379157.3391992,2020,Jayawardena G.,RAEMAP: Real-Time Advanced Eye Movements Analysis Pipeline,"Eye-tracking measures enable means to understand the underlying covert processes engaged during inhibitory tasks which rely on attention allocation. We propose Real-Time Advanced Eye Movements Analysis Pipeline (RAEMAP) to utilize eye tracking measures as a valid psychophysiological measure. RAEMAP will include realtime analysis of the traditional positional gaze metrics as well as advanced metrics such as ambient/focal coefficient Îº, gaze transition entropy, and index of pupillary activity (IPA). RAEMAP will also provide visualizations of calculated eye gaze metrics, heatmaps, and dynamic AOI generation in real-time. This paper will outline the proposed architecture of RAEMAP in terms of distributed computing, incorporation of machine learning models, and the evaluation to prove the utility of RAEMAP to diagnose ADHD in real-time. Â© 2020 Owner/Author.",,,
10.1145/3379157.3391654,2020,"Kim J.-H., Jeong J.-W.",Gaze Estimation in the Dark with Generative Adversarial Networks,"In this paper, we propose to utilize generative adversarial networks (GANs) to achieve successful gaze estimation in interactive multimedia environments with low light conditions such as a digital museum or exhibition hall. The proposed approach utilizes a GAN to enhance user images captured under low-light conditions, thereby recovering missing information for gaze estimation. The recovered images are fed into the CNN architecture to estimate the direction of user gaze. The preliminary experimental results on the modified MPIIGaze dataset demonstrated an average performance improvement of 6.6 under various low light conditions, which is a promising step for further research. Â© 2020 ACM.",,,
10.1145/3379157.3391990,2020,"Nair N., Chaudhary A.K., Kothari R.S., Diaz G.J., Pelz J.B., Bailey R.",RIT-Eyes: Realistically rendered eye images for eye-tracking applications,"Convolutional neural network-based solutions for video oculography require large quantities of accurately labeled eye images acquired under a wide range of image quality, surrounding environmental reflections, feature occlusion, and varying gaze orientations. Manually annotating such a dataset is challenging, time-consuming, and error-prone. To alleviate these limitations, this work introduces an improved eye image rendering pipeline designed in Blender. RIT-Eyes provides access to realistic eye imagery with error-free annotations in 2D and 3D which can be used for developing gaze estimation algorithms. Furthermore, RIT-Eyes is capable of generating novel temporal sequences with realistic blinks and mimicking eye and head movements derived from publicly available datasets. Â© 2020 Owner/Author.",,,
10.1145/3379157.3391986,2020,"Hildebrandt M., Langstrand J.-P., Nguyen H.T.",Synopticon: Sensor Fusion for Automated Gaze Analysis,"This demonstration presents Synopticon, an open-source software system for automatic, real-time gaze object detection for mobile eye tracking. The system merges gaze data from eye tracking glasses with position data from a motion capture system and projects the resulting gaze vector onto a 3D model of the environment. Â© 2020 Owner/Author.",,,
10.1109/ICAICA50127.2020.9181854,2020,"Li Y., Zhan Y., Yang Z.",Evaluation of appearance-based eye tracking calibration data selection,"Eye tracking is a valuable topic in computer vision. Appearance-based eye tracking is a promising research direction in recent years. Convolutional neural networks (CNN) had been used in gaze estimation, which cover the significant variability in eye appearance caused by unconstrained head motion. With computation capability of consumer devices rapidly evolving, accurate and efficient appearance-based eye tracking has the potential for multipurpose applications. Person-independent networks have limit in improving gaze estimation accuracy. Person-specific network with calibration is more effective than person-independent approaches. Unlike classical eye tracking methods, appearance-based eye tracking has not a clear way to calibration. Our goal is to analyze the impact of calibration data selection and calibration target distribution on person-specific gaze estimation accuracy. We trained person-independent network and use SVR to calibration. We choose two kind of typical distribution targets to evaluation. Use different distribution targets to calibration achieves different accuracy. Â© 2020 IEEE.",,,
10.1109/ICECCE49384.2020.9179472,2020,"Caporusso N., Zhang K., Carlson G.",Using Eye-tracking to Study the Authenticity of Images Produced by Generative Adversarial Networks,"Nowadays, Machine Learning algorithms, such as Generative Adversarial Networks (GANs), enable generating content, and especially images, featuring people, objects, or landscapes, with unprecedented levels of accuracy and fidelity. As a result, it is becoming challenging for a viewer to distinguish a picture of a fake profile from one that has a real human in it. In this paper, we present the results of an experimental study in which we investigated the perception of images produced by GANs. Specifically, we focused on the individuals' ability to discriminate between fake and real profiles. Furthermore, we utilized eye-tracking technology to identify the presence of patterns in subjects' gaze, which, in turn, can be useful to optimize the output of GANs and, simultaneously, provide insight on the underlying cognitive dynamics. Â© 2020 IEEE.",,,
10.1109/HSI49210.2020.9142647,2020,"Kocejko T., Weglerski R., Zubowicz T., Ruminski J., Wtorek J., Arminski K.",Design aspects of a low-cost prosthetic arm for people with severe movement disabilities,"In this paper the main aspects of mechanical design behind the low-cost prosthetic arm are presented. The fundamentals of a proper design has been defined to obtain functional 3D printed 5 degree of freedom (DOF) prosthesis. The designed prosthetic arm is a part of the hybrid interface with eye tracking movement control. The main focus was to create affordable but usable prosthesis which corresponds in size and weights to the human arm. The iterative process (starting from the final segment of the arm) was used to design fully functioning arm. All the elements were evaluated regarding total weight and the maximum load that can be carried by the arm. The result of this work is a prototype that weighs below 6kg and has a range of motion comparable to the human's arm. Final product is able to freely move an object of a total weight of 1 kg. All the mechanical parts of the designed arm were 3D printed which therefore presented construction can be adopted by people with different disabilities and (when connected to interfaces like EEG, EMG or eye tracking) provide support in everyday life activities. Â© 2020 IEEE.",,,
10.1109/ITOEC49072.2020.9141800,2020,"Hu L., Gao J.",Research on real-time distance measurement of mobile eye tracking system based on neural network,"With the development and application of eye-tracking technology, mobile eye-tracking systems have become more widely used due to their safety and portability. We combine eye-tracking systems with real-time object detection using machine learning. We propose a method of wearing an eye tracker in daily life to obtain the distance between the eye tracking system and the gaze target in real time. During the visual interaction of the eye tracking system, in order to obtain the distance from the eyeball fixation target to the eyeball in real time, the world camera of the mobile eye tracking system pupil labs first collects the position and scale information of the detected target image in real time, and uses camera calibration principle, pinhole camera model and camera distortion model to establish a ranging equation, and then the feasibility of the real-time ranging equation is verified through a specified distance experiment. The total average relative error after de-distortion at the position of 50cm-75cm is reduced to 1.25%, and the highest accuracy-0.9182cm distance measurement can be achieved within the effective distance. Â© 2020 IEEE.",,,
10.1016/j.cag.2020.04.005,2020,"Liu C., Plopski A., Orlosky J.",OrthoGaze: Gaze-based three-dimensional object manipulation using orthogonal planes,"In virtual and augmented reality, gaze-based methods have been explored for decades as effective user interfaces for hands-free interaction. Though several well-known gaze-based methods exist for simple interactions such as selection, no solutions exist for 3D manipulation tasks requiring a higher degree of freedom (DoF). In this paper, we introduce OrthoGaze, a novel user interface that allows users to intuitively manipulate the three-dimensional position of a virtual object using only their eye or head gaze. Our approach makes use of three selectable, orthogonal planes, where each plane not only helps guide the user's gaze in an arbitrary virtual space, but also allows for 2-DoF manipulations of object position. To evaluate our method, we conducted two user studies involving aiming and docking tasks in virtual reality to evaluate the fundamental characteristics of sustained gaze aiming and to determine which type of gaze-based control performs best when combined with OrthoGaze. Results showed that eye gaze was more accurate than head gaze for sustained aiming. Additionally, eye and head gaze-based control for 3D manipulations achieved 78% and 96% performance, respectively, in comparison with a hand-held controller. Subjective results also suggest that gaze-based manipulation can comprehensively cause more fatigue than controller-based. From the experimental results, we expect OrthoGaze to become an effective method for pure hands-free object manipulation in head-mounted displays. Â© 2020 Elsevier Ltd",,,
10.1109/TCYB.2018.2886580,2020,"Deng C., Han Y., Zhao B.",High-Performance Visual Tracking with Extreme Learning Machine Framework,"In real-time applications, a fast and robust visual tracker should generally have the following important properties: 1) feature representation of an object that is not only efficient but also has a good discriminative capability and 2) appearance modeling which can quickly adapt to the variations of foreground and backgrounds. However, most of the existing tracking algorithms cannot achieve satisfactory performance in both of the two aspects. To address this issue, in this paper, we advocate a novel and efficient visual tracker by exploiting the excellent feature learning and classification capabilities of an emerging learning technique, that is, extreme learning machine (ELM). The contributions of the proposed work are as follows: 1) motivated by the simplicity and learning ability of the ELM autoencoder (ELM-AE), an ELM-AE-based feature extraction model is presented, and this model can provide a compact and discriminative representation of the inputs efficiently and 2) due to the fast learning speed of an ELM classifier, an ELM-based appearance model is developed for feature classification, and is able to rapidly distinguish the object of interest from its surroundings. In addition, in order to cope with the visual changes of the target and its backgrounds, the online sequential ELM is used to incrementally update the appearance model. Plenty of experiments on challenging image sequences demonstrate the effectiveness and robustness of the proposed tracker. Â© 2013 IEEE.",,,
10.1016/j.compedu.2020.103858,2020,"Knoop-van Campen C.A.N., Segers E., Verhoeven L.",Effects of audio support on multimedia learning processes and outcomes in students with dyslexia,"Adding audio to written text may cause redundancy effects, but could be beneficial for students with dyslexia for whom it supports their reading. Studying both learning process and learning outcomes in students with and without dyslexia can shed light on this issue and helps to find out whether there are constraints to the redundancy effect as proposed in the Cognitive Theory of Multimedia Learning. We examined to what extent adding -redundant- audio affects multimedia learning in 42 university students with dyslexia and 44 typically developing students. Participants studied two user-paced multimedia lessons (text-picture, text-audio-picture) with retention and transfer post-tests. An SMI RED-500 eye-tracker captured eye-movements during learning. Regarding process measures, students had longer study times, with more focus on pictures, and more transitions between text and pictures in the text-audio-picture condition. Regarding learning outcomes, negative redundancy effects on transfer knowledge (deep learning), but not on (factual) retention knowledge were found across both groups. When relating learning processes to learning outcomes, longer study time predicted higher transfer knowledge in both groups in the text-audio-picture condition, whereas in the text-picture condition, more study time predicted lower transfer knowledge in typically developing students only. To conclude, adding audio seems to have a negative effect on the quality of knowledge and leads to less efficient learning across the two groups. Reading ability does not impact the universality of the redundancy effect, but students with dyslexia should only use audio support when aiming to learn factual knowledge and should be aware that it increases study time. Â© 2020 The Authors",,,
10.1007/s11042-019-08327-0,2020,"Joo H.-J., Jeong H.-Y.",A study on eye-tracking-based Interface for VR/AR education platform,"In recent years, a platform providing a Visual Programming development environment capable of 3D editing and interaction editing in an In-VR environment to quickly prototype VR/AR contents for education of VR and AR for general users and children. In the past, VR contents were mostly viewed by users. However, thanks to the rapid development of recent computing technologies, VR contents interacting with users have emerged as a device capable of tracking user behavior in a small size It was able to appear. In addition, because VR is extended to AR and MR, it can be used in all three virtual environments and requires efficient user interface (UI). In this paper, we propose UI based on eye tracking. Eye-tracking-based UI not only reduces the amount of time the user directly manipulates the controller, but also dramatically lowers the time spent on simple operations, while reducing the need for a dedicated controller by allowing multiple types of controllers to be used in combination. Â© 2019, Springer Science+Business Media, LLC, part of Springer Nature.",,,
10.1145/3396339.3396393,2020,"Mahanama B., Jayawardana Y., Jayarathna S.",Gaze-Net: Appearance-based gaze estimation using capsule networks,"Recent studies on appearance based gaze estimation indicate the ability of Neural Networks to decode gaze information from facial images encompassing pose information. In this paper, we propose Gaze-Net: A capsule network capable of decoding, representing, and estimating gaze information from ocular region images. We evaluate our proposed system using two publicly available datasets, MPIIGaze (200,000+ images in the wild) and Columbia Gaze (5000+ images of users with 21 gaze directions observed at 5 camera angles/positions). Our model achieves a Mean Absolute Error (MAE) of 2.84Â° for Combined angle error estimate within dataset for MPIIGaze dataset. Further, model achieves a MAE of 10.04Â° for across dataset gaze estimation error for Columbia gaze dataset. Through transfer learning, the error is reduced to 5.9Â°. The results show this approach is promising with implications towards using commodity webcams to develop low-cost multi-user gaze tracking systems. Â© 2020 Association for Computing Machinery.",,,
10.1016/j.neucom.2019.04.099,2020,"Zhou X., Lin J., Zhang Z., Shao Z., Chen S., Liu H.",Improved itracker combined with bidirectional long short-term memory for 3D gaze estimation using appearance cues,"Gaze is an important non-verbal cue for speculating human's attention, which has been widely employed in many humanâ€“computer interaction-based applications. In this paper, we propose an improved Itracker to predict the subject's gaze for a single image frame, as well as employ a many-to-one bidirectional Long Short-Term Memory (bi-LSTM) to fit the temporal information between frames to estimate gaze for video sequence. For single image frame gaze estimation, we improve the conventional Itracker by removing the face-grid and reducing one network branch via concatenating the two-eye region images. Experimental results show that our improved Itracker obtains 11.6% significant improvement over the state-of-the-art methods on MPIIGaze dataset and has robust estimation accuracy for different image resolutions under the premise of greatly reducing network complexity. For video sequence gaze estimation, by employing the bi-LSTM to fit the temporal information between frames, experimental results on EyeDiap dataset further demonstrate 3% accuracy improvement. Â© 2019",,,
10.1016/j.neucom.2019.05.106,2020,"Li J., Zhong Y., Han J., Ouyang G., Li X., Liu H.",Classifying ASD children with LSTM based on raw videos,"Autism spectrum disorder (ASD) is a serious neurodevelopmental disorder that impairs a child's ability to communicate and interact with others. Usually, recognizing a child with ASD needs the diagnosis by professional doctors. However, it is not only expensive and time-consuming, but also the results are influenced by subjective factors, such as the experience of a doctor. Recently, some methods which identify ASD based on biomarkers have been developed, but there are rarely works specific to raw video data. This paper is the first attempt to help diagnose the children with ASD in raw video data using a deep learning technique. Firstly, in order to investigate different gaze patterns between ASD children and typically developing (TD) children, we track the eye movement in each video by the tracking-learning-detection method. Secondly, we divide these tracking trajectories into two components: (i) the length; and (ii) the angle. Afterwards, we calculate an accumulative histogram for each component. Finally, we adopt three-layer Long Short-Term Memory (LSTM) network for classification. Experimental results on our extended dataset (Ext-Dataset) containing 272 videos captured from 136 ASD children and 136 TD children show the LSTM network outperforms the traditional machine learning methods, e.g., Support Vector Machine, with the improvement of accuracy by 6.2% (from 86.4% to 92.6%). Especially, for ASD, we obtain the sensitivity (the true positive rate, TPR) of 91.9% and the specificity (the true negative rate, TNR) of 93.4%, which demonstrates the effectiveness of our method. Â© 2019",,,
10.1093/iwc/iwaa018,2020,"Kyritsis M., Gulliver S.R., Feredoes E.",Visual Search Fixation Strategies in a 3D Image Set: An Eye-Tracking Study,"In this study, we explore whether the inclusion of monocular depth within a pseudo-3D picture gallery negatively affects visual search strategy and performance. Experimental design facilitated control of (i) the number of visible depth planes and (ii) the presence of semantic sorting. Our results show that increasing the number of visual depth planes facilitates efficiency in search, which in turn results in a decreased response time to target selection and a reduction in participant average pupil dilation - used for measuring cognitive load. Furthermore, results identified that search strategy is based on sorting, which implies that an appropriate management of semantic associations can increase search efficiency by decreasing the number of potential targets. Â© 2020 The Author(s) 2020. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.",,,
10.1111/cgf.13945,2020,"Wen Q., Bradley D., Beeler T., Park S., Hilliges O., Yong J., Xu F.",Accurate Real-time 3D Gaze Tracking Using a Lightweight Eyeball Calibration,"3D gaze tracking from a single RGB camera is very challenging due to the lack of information in determining the accurate gaze target from a monocular RGB sequence. The eyes tend to occupy only a small portion of the video, and even small errors in estimated eye orientations can lead to very large errors in the triangulated gaze target. We overcome these difficulties with a novel lightweight eyeball calibration scheme that determines the user-specific visual axis, eyeball size and position in the head. Unlike the previous calibration techniques, we do not need the ground truth positions of the gaze points. In the online stage, gaze is tracked by a new gaze fitting algorithm, and refined by a 3D gaze regression method to correct for bias errors. Our regression is pre-trained on several individuals and works well for novel users. After the lightweight one-time user calibration, our method operates in real time. Experiments show that our technique achieves state-of-the-art accuracy in gaze angle estimation, and we demonstrate applications of 3D gaze target tracking and gaze retargeting to an animated 3D character. Â© 2020 The Author(s) Computer Graphics Forum Â© 2020 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.",,,
10.1109/ICICCS48265.2020.9121022,2020,"Manal S., Ov S., Kg S.",Catch Gesture: An Ultimate Action Recognition Technology Using IoT Device,"Nowadays, the internet of things (IoT) which connects the objects with the network is one of the most important technology. Everyone needs to access the IoT devices from anywhere. A challenge in our world is how to provide an efficient and automatic approach to control the IoT devices. In this paper, a system is proposed which provide a smart IoT controlling system, for getting efficient communication between the IoT devices and the users. An IoT device controller named as catch gesture contains three modules 1) object recognition module, 2) eye tracking module, and 3) gesture recognition module. The IoT device is identified by using some deep learning-based methods for getting accurate output and performance. Subsequently, the IoT device control command is generated and transmitted to the IoT devices to control the IoT devices by using hand gesture. The performance evaluation gives the efficiency and usefulness of the proposed system in detecting the IoT devices. Â© 2020 IEEE.",,,
10.1016/j.compbiomed.2020.103722,2020,"Kang J., Han X., Song J., Niu Z., Li X.",The identification of children with autism spectrum disorder by SVM approach on EEG and eye-tracking data,"Objective: To identify autistic children, we used features extracted from two modalities (EEG and eye-tracking) as input to a machine learning approach (SVM). Methods: A total of 97 children aged from 3 to 6 were enrolled in the present study. After resting-state EEG data recording, the children performed eye-tracking tests individually on own-race and other-race stranger faces stimuli. Power spectrum analysis was used for EEG analysis and areas of interest (AOI) were selected for face gaze analysis of eye-tracking data. The minimum redundancy maximum relevance (MRMR) feature selection method combined with SVM classifiers were used for classification of autistic versus typically developing children. Results: Results showed that classification accuracy from combining two types of data reached a maximum of 85.44%, with AUC = 0.93, when 32 features were selected. Limitations: The sample consisted of children aged from 3 to 6, and no younger patients were included. Conclusions: Our machine learning approach, combining EEG and eye-tracking data, may be a useful tool for the identification of children with ASD, and may help for diagnostic processes. Â© 2020 Elsevier Ltd",,,
10.1145/3334480.3382856,2020,"Kim J.-H., Jeong J.-W.",A preliminary study on performance evaluation of multi-view multi-modal gaze estimation under challenging conditions,"In this paper, we address gaze estimation under practical and challenging conditions. Multi-view and multi-modal learning have been considered useful for various complex tasks; however, an in-depth analysis or a large-scale dataset on multi-view, multi-modal gaze estimation under a long-distance setup with a low illumination is still very limited. To address these limitations, first, we construct a dataset of images captured under challenging conditions. And we propose a simple deep learning architecture that can handle multi-view multi-modal data for gaze estimation. Finally, we conduct a performance evaluation of the proposed network with the constructed dataset to understand the effects of multiple views of a user and multi-modality (RGB, depth, and infrared). We report various findings from our preliminary experimental results and expect this would be helpful for gaze estimation studies to deal with challenging conditions. Â© 2020 Owner/Author.",,,
10.1145/3313831.3376438,2020,"Sidenmark L., Clarke C., Zhang X., Phu J., Gellersen H.",Outline Pursuits: Gaze-assisted Selection of Occluded Objects in Virtual Reality,"In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded. Â© 2020 ACM.",,,
10.1145/3313831.3376550,2020,"Bai H., Sasikumar P., Yang J., Billinghurst M.",A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing,"Supporting natural communication cues is critical for people to work together remotely and face-to-face. In this paper we present a Mixed Reality (MR) remote collaboration system that enables a local worker to share a live 3D panorama of his/her surroundings with a remote expert. The remote expert can also share task instructions back to the local worker using visual cues in addition to verbal communication. We conducted a user study to investigate how sharing augmented gaze and gesture cues from the remote expert to the local worker could affect the overall collaboration performance and user experience. We found that by combing gaze and gesture cues, our remote collaboration system could provide a significantly stronger sense of co-presence for both the local and remote users than using the gaze cue alone. The combined cues were also rated significantly higher than the gaze in terms of ease of conveying spatial actions. Â© 2020 ACM.",,,
10.1145/3371300.3383340,2020,"Eraslan S., Yesilada Y., Yaneva V., Harper S.",Autism detection based on eye movement sequences on the web: a scanpath trend analysis approach,"Autism diagnostic procedure is a subjective, challenging and expensive procedure and relies on behavioral, historical and parental report information. In our previous, we proposed a machine learning classifier to be used as a potential screening tool or used in conjunction with other diagnostic methods, thus aiding established diagnostic methods. The classifier uses eye movements of people on web pages but it only considers non-sequential data. It achieves the best accuracy by combining data from several web pages and it has varying levels of accuracy on different web pages. In this present paper, we investigate whether it is possible to detect autism based on eye-movement sequences and achieve stable accuracy across different web pages to be not dependent on specific web pages. We used Scanpath Trend Analysis (STA) which is designed for identifying a trending path of a group of users on a web page based on their eye movements. We first identify trending paths of people with autism and neurotypical people. To detect whether or not a person has autism, we calculate the similarity of his/her path to the trending paths of people with autism and neurotypical people. If the path is more similar to the trending path of neurotypical people, we classify the person as a neurotypical person. Otherwise, we classify her/him as a person with autism. We systematically evaluate our approach with an eye-tracking dataset of 15 verbal and highly-independent people with autism and 15 neurotypical people on six web pages. Our evaluation shows that the STA approach performs better on individual web pages and provides more stable accuracy across different pages. Â© 2020 ACM.",,,
,2020,"Zheng L.J., Mountstephens J., Teo J.",Comparing eye-tracking versus EEG features for four-class emotion classification in VR predictive analytics,"This paper presents a novel emotion recognition approach using electroencephalography (EEG) brainwave signals augmented with eye-tracking data in virtual reality (VR) to classify 4-quadrant circumplex model of emotions. 3600 videos are used as the stimuli to evoke userâ€™s emotions (happy, angry, bored, calm) with a VR headset and a pair of earphones. EEG signals are recorded via a wearable EEG brain-computer interfacing (BCI) device and pupil diameter is collected also from a wearable portable eye-tracker. We extract 5 frequency bands which are Delta, Theta, Alpha, Beta, and Gamma from EEG data as well as obtaining pupil diameter from the eye-tracker as the chosen as the eye-related feature for this investigation. Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel is used as the classifier. The best accuracies based on EEG brainwave signals and pupil diameter are 98.44% and 58.30% respectively. Â© 2020 SERSC.",,,
10.1016/j.neucom.2019.12.022,2020,"Du Y., Yan Y., Chen S., Hua Y.",Object-adaptive LSTM network for real-time visual tracking with adversarial data augmentation,"In recent years, deep learning based visual tracking methods have obtained great success owing to the powerful feature representation ability of Convolutional Neural Networks (CNNs). Among these methods, classification-based tracking methods exhibit excellent performance while their speeds are heavily limited by the expensive computation for massive proposal feature extraction. In contrast, matching-based tracking methods (such as Siamese networks) possess remarkable speed superiority. However, the absence of online updating renders these methods unadaptable to significant object appearance variations. In this paper, we propose a novel real-time visual tracking method, which adopts an object-adaptive LSTM network to effectively capture the video sequential dependencies and adaptively learn the object appearance variations. For high computational efficiency, we also present a fast proposal selection strategy, which utilizes the matching-based tracking method to pre-estimate dense proposals and selects high-quality ones to feed to the LSTM network for classification. This strategy efficiently filters out some irrelevant proposals and avoids the redundant computation for feature extraction, which enables our method to operate faster than conventional classification-based tracking methods. In addition, to handle the problems of sample inadequacy and class imbalance during online tracking, we adopt a data augmentation technique based on the Generative Adversarial Network (GAN) to facilitate the training of the LSTM network. Extensive experiments on four visual tracking benchmarks demonstrate the state-of-the-art performance of our method in terms of both tracking accuracy and speed, which exhibits great potentials of recurrent structures for visual tracking. Â© 2019 Elsevier B.V.",,,
10.1109/AEMCSE50948.2020.00070,2020,"Xu L., Chi J.",3D eye model-based gaze tracking system with a consumer depth camera,"Most existing gaze tracking systems are high-cost, intrusive and difficult to calibrate, and some rely on the infrared illuminant. However, such systems may not work outdoor and meet real-time requirements. This paper proposes a non-intrusive system based on the 3D eyeball model, which does not need the exact infrared illuminant and complicated calibration process and allows the natural movement of the head. In the proposed system, Kinect is used to track the iris center and face model of the person, and the 3D information is easy to obtain. At the same time, point cloud registration algorithm is applied based on feature points in the face model sequence to obtain accurate head pose estimation results. In this paper, a personal calibration process is also proposed to obtain the gaze model parameters for different users, such as the eyeball center and angle kappa. The proposed method has good adaptability to the change of illuminant and head movement. In the actual operating environment, the system speed reaches 30 fps, which can meet the requirements of real-time control. Â© 2020 IEEE.",,,
10.1109/IWBF49977.2020.9107939,2020,"Boutros F., Damer N., Raja K., Ramachandra R., Kirchbuchner F., Kuijper A.",Periocular Biometrics in Head-Mounted Displays: A Sample Selection Approach for Better Recognition,"Virtual and augmented reality technologies are increasingly used in a wide range of applications. Such technologies employ a Head Mounted Display (HMD) that typically includes an eye-facing camera and is used for eye tracking. As some of these applications require accessing or transmitting highly sensitive private information, a trusted verification of the operator's identity is needed. We investigate the use of HMD-setup to perform verification of operator using periocular region captured from inbuilt camera. However, the uncontrolled nature of the periocular capture within the HMD results in images with a high variation in relative eye location and eye-opening due to varied interactions. Therefore, we propose a new normalization scheme to align the ocular images and then, a new reference sample selection protocol to achieve higher verification accuracy. The applicability of our proposed scheme is exemplified using two handcrafted feature extraction methods and two deep-learning strategies. We conclude by stating the feasibility of such a verification approach despite the uncontrolled nature of the captured ocular images, especially when proper alignment and sample selection strategy is employed. Â© 2020 IEEE.",,,
10.1109/ICCP48838.2020.9105134,2020,"Guo Q., Tang H., Schmitz A., Zhang W., Lou Y., Fix A., Lovegrove S., Strasdat H.M.",Raycast calibration for augmented reality HMDS with off-axis reflective combiners,"Augmented reality overlays virtual objects on the real world. To do so, the head mounted display (HMD) needs to be calibrated to establish a mapping between 3D points in the real world with 2D pixels on display panels. This distortion is a high-dimensional function that also depends on pupil position and varifocal settings. We present Raycast calibration, an efficient approach to geometrically calibrate AR displays with off-axis reflective combiners. Our approach requires a small amount of data to estimate a compact, physics-based, and ray-traceable model of the HMD optics. We apply this technique to automatically calibrate an AR prototype with display, SLAM and eye-tracker, without user in the loop. Â© 2020 IEEE.",,,
10.3390/s20071949,2020,"Li X., Younes R., Bairaktarova D., Guo Q.",Predicting spatial visualization problemsâ€?difficulty level from eye-tracking data,"The difficulty level of learning tasks is a concern that often needs to be considered in the teaching process. Teachers usually dynamically adjust the difficulty of exercises according to the prior knowledge and abilities of students to achieve better teaching results. In e-learning, because there is no teacher involvement, it often happens that the difficulty of the tasks is beyond the ability of the students. In attempts to solve this problem, several researchers investigated the problem-solving process by using eye-tracking data. However, although most e-learning exercises use the form of filling in blanks and choosing questions, in previous works, research focused on building cognitive models from eye-tracking data collected from flexible problem forms, which may lead to impractical results. In this paper, we build models to predict the difficulty level of spatial visualization problems from eye-tracking data collected from multiple-choice questions. We use eye-tracking and machine learning to investigate (1) the difference of eye movement among questions from different difficulty levels and (2) the possibility of predicting the difficulty level of problems from eye-tracking data. Our models resulted in an average accuracy of 87.60% on eye-tracking data of questions that the classifier has seen before and an average of 72.87% on questions that the classifier has not yet seen. The results confirmed that eye movement, especially fixation duration, contains essential information on the difficulty of the questions and it is sufficient to build machine-learning-based models to predict difficulty level. Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/MPRV.2020.2967736,2020,"Bace M., Staal S., Bulling A.",How Far Are We from Quantifying Visual Attention in Mobile HCI?,"With an ever-increasing number of mobile devices competing for attention, quantifying when, how often, or for how long users look at their devices has emerged as a key challenge in mobile human-computer interaction. Encouraged by recent advances in automatic eye contact detection using machine learning and device-integrated cameras, we provide a fundamental investigation into the feasibility of quantifying overt visual attention during everyday mobile interactions. In this article, we discuss the main challenges and sources of error associated with sensing visual attention on mobile devices in the wild, including the impact of face and eye visibility, the importance of robust head poses estimation, and the need for accurate gaze estimation. Our analysis informs future research on this emerging topic and underlines the potential of eye contact detection for exciting new applications toward next-generation pervasive attentive user interfaces. Â© 2002-2012 IEEE.",,,
10.3390/s20071917,2020,"Lee K.-F., Chen Y.-L., Yu C.-W., Chin K.-Y., Wu C.-H.",Gaze tracking and point estimation using low-cost head-mounted devices,"In this study, a head-mounted device was developed to track the gaze of the eyes and estimate the gaze point on the user's visual plane. To provide a cost-effective vision tracking solution, this head-mounted device is combined with a sized endoscope camera, infrared light, and mobile phone; the devices are also implemented via 3D printing to reduce costs. Based on the proposed image pre-processing techniques, the system can efficiently extract and estimate the pupil ellipse from the camera module. A 3D eye model was also developed to effectively locate eye gaze points from extracted eye images. In the experimental results, average accuracy, precision, and recall rates of the proposed system can achieve an average of over 97%, which can demonstrate the efficiency of the proposed system. This study can be widely used in the Internet of Things, virtual reality, assistive devices, and human-computer interaction applications. Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1016/j.aei.2020.101061,2020,"Shi Y., Du J., Ragan E.",Review visual attention and spatial memory in building inspection: Toward a cognition-driven information system,"With the increasing complexity of modern buildings, it is becoming more challenging for the professionals in the Architecture, Engineering, and Construction (AEC) industry to effectively digest complex engineering and design information and develop an accurate spatial memory that is critical to their daily tasks. As emerging visualization technologies, such as Virtual Reality, are considered as a promising solution, there is a pressing need to understand the mechanism by which different information visualization methods affect AEC task performance. Cognition literature has discovered a strong relationship between attention and memory development, but little has been done to understand how the visual attention patterns during the design documents review affect the effectiveness of spatial memory in AEC tasks. To fill the knowledge gap, this paper presents a human-subject experiment (n = 63) to test how spatial knowledge is acquired in a building inspection task and how the different visual attention patterns affect the development of spatial memory. Participants were asked to review the design information of a real building on campus. To trigger different attention patterns, they were randomly assigned to one of the three groups based on the forms of information given in the review session, including 2D, 3D, and VR groups. After a brief review session, participants were asked to go to the real building to identify discrepancies (based on memory) that were intentionally inserted by the authors. The inspection performance was used to evaluate the spatial memory development. The results indicate that in general there is a positive relationship between test subjectsâ€?visual attention (fixation time) and spatial memory, but the increasing rate varies across the three groups, suggesting that visual context plays a critical role in the development efficiency of spatial memory. The findings also indicate that the visual attention â€?spatial memory relationship may be mediated by the use of different spatial knowledge acquisition strategies. This study is expected to contribute to the construction information technology literature by setting the cornerstone of a cognition-driven information system that tailors into the spatial cognitive process of AEC professionals. Â© 2020 Elsevier Ltd",,,
10.1109/LRA.2020.2965416,2020,"Rudenko A., Kucner T.P., Swaminathan C.S., Chadalavada R.T., Arras K.O., Lilienthal A.J.",THÃ–R: Human-Robot Navigation Data Collection and Accurate Motion Trajectories Dataset,"Understanding human behavior is key for robots and intelligent systems that share a space with people. Accordingly, research that enables such systems to perceive, track, learn and predict human behavior as well as to plan and interact with humans has received increasing attention over the last years. The availability of large human motion datasets that contain relevant levels of difficulty is fundamental to this research. Existing datasets are often limited in terms of information content, annotation quality or variability of human behavior. In this article, we present THÃ–R, a new dataset with human motion trajectory and eye gaze data collected in an indoor environment with accurate ground truth for position, head orientation, gaze direction, social grouping, obstacles map and goal coordinates. THÃ–R also contains sensor data collected by a 3D lidar and involves a mobile robot navigating the space. We propose a set of metrics to quantitatively analyze motion trajectory datasets such as the average tracking duration, ground truth noise, curvature and speed variation of the trajectories. In comparison to prior art, our dataset has a larger variety in human motion behavior, is less noisy, and contains annotations at higher frequencies. Â© 2016 IEEE.",,,
10.1145/3377325.3377540,2020,"Aydin A.S., Feiz S., Ashok V., Ramakrishnan I.V.",SaIL,"Navigating webpages with screen readers is a challenge even with recent improvements in screen reader technologies and the increased adoption of web standards for accessibility, namely ARIA. ARIA landmarks, an important aspect of ARIA, lets screen reader users access different sections of the webpage quickly, by enabling them to skip over blocks of irrelevant or redundant content. However, these landmarks are sporadically and inconsistently used by web developers, and in many cases, even absent in numerous web pages. Therefore, we propose SaIL, a scalable approach that automatically detects the important sections of a web page, and then injects ARIA landmarks into the corresponding HTML markup to facilitate quick access to these sections. The central concept underlying SaIL is visual saliency, which is determined using a state-of-the-art deep learning model that was trained on gaze-tracking data collected from sighted users in the context of web browsing. We present the findings of a pilot study that demonstrated the potential of SaIL in reducing both the time and effort spent in navigating webpages with screen readers. Â© ACM.",,,
10.1145/3379336.3381506,2020,Murthy L.R.D.,Multimodal interaction for real and virtual environments,"Multimodal interfaces can leverage the information from multiple modalities to provide robust and error-free interaction. Early multimodal interfaces demonstrate the feasibility of building such systems but focused on specific applications. The challenge in building adaptive systems is lack of techniques for input data fusion. In this direction, we have developed a multimodal head and eye gaze interface and evaluated it in two scenarios. In aviation scenario, our interface has reduced the task time and perceived cognitive load significantly from the existing interface. We have also studied the effect of various output conditions on user's performance in a Virtual Reality (VR) task. Further, we are making our proposed interface to include additional modalities and building novel haptic and multimodal output systems for VR. Â© 2020 International Conference on Intelligent User Interfaces, Proceedings IUI. All rights reserved.",,,
10.1145/3343413.3378010,2020,"Davari M., Hienert D., Kern D., Dietze S.",The role of word-eye-fixations for query term prediction,"Throughout the search process, the user's gaze on inspected SERPs and websites can reveal his or her search interests. Gaze behavior can be captured with eye tracking and described with word-eye-fixations. Word-eye-fixations contain the user's accumulated gaze fixation duration on each individual word of a web page. In this work, we analyze the role of word-eye-fixations for predicting query terms. We investigate the relationship between a range of in-session features, in particular, gaze data, with the query terms and train models for predicting query terms. We use a dataset of 50 search sessions obtained through a lab study in the social sciences domain. Using established machine learning models, we can predict query terms with comparably high accuracy, even with only little training data. Feature analysis shows that the categories Fixation, Query Relevance and Session Topic contain the most effective features for our task. Â© 2020 ACM.",,,
10.1093/iwcomp/iwaa013,2020,"Vidyapu S., Vedula V.S., Bhattacharya S.",Weighted Voting-Based Effective Free-Viewing Attention Prediction on Web Image Elements,"Quantifying and predicting the user attention on web image elements finds applications in synthesis and rendering of elements on webpages. However, the majority of the existing approaches either overlook the visual characteristics of these elements or do not incorporate the usersâ€?visual attention. Especially, obtaining a representative quantified attention (for images) from the attention allocation of multiple users is a challenging task. Toward overcoming the challenge for free-viewing attention, this paper introduces four weighted voting strategies to assign effective visual attention (fixation index (FI)) for web image elements. Subsequently, the prominent image visual features in explaining the assigned attention are identified. Further, the association between image visual features and the assigned attention is modeled as a multi-class prediction problem, which is solved through support vector machine-based classification. The analysis of the proposed approach on real-world webpages reveals the following: (i) image elementâ€™s position, size and mid-level color histograms are highly informative for the four weighting schemes; (ii) the presented computational approach outperforms the baseline for four weighted voting schemes with an average accuracy of 85% and micro F1-score of 60%; and (iii) uniform weighting (same weight for all FIs) is adequate for estimating the userâ€™s initial attention while the proportional weighting (weight the FI in proportion to its likelihood of occurrence) extends to the latter attention prediction. Â© The Author(s) 2020. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved.",,,
10.1109/PerComWorkshops48775.2020.9156225,2020,"Yang S., Bailey E., Yang Z., Ostrometzky J., Zussman G., Seskar I., Kostic Z.",COSMOS Smart Intersection: Edge Compute and Communications for Bird's Eye Object Tracking,"Smart-city intersections will play a crucial role in automated traffic management and improvement in pedestrian safety in cities of the future. They will (i) aggregate data from in-vehicle and infrastructure sensors; (ii) process the data by taking advantage of low-latency high-bandwidth communications, edge-cloud computing, and AI-based detection and tracking of objects; and (iii) provide intelligent feedback and input to control systems. The Cloud Enhanced Open Software Defined Mobile Wireless Testbed for City-Scale Deployment (COSMOS) enables research on technologies supporting smart cities. In this paper, we provide results of experiments using bird's eye cameras to detect and track vehicles and pedestrians from the COSMOS pilot site. We assess the capabilities for real-time computation, and detection and tracking accuracy-by evaluating and customizing a selection of video pre-processing and deep-learning algorithms. Distinct issues that are associated with the difference in scale for bird's eye view of pedestrians vs. cars are explored and addressed: the best multiple-object tracking accuracies (MOTA) for cars are around 73.2, and around 2.8 for pedestrians. The real-time goal of 30 frames-per-second-i.e., a total of 33.3 ms of latency for object detection for vehicles will be reachable once the processing time is improved roughly by a factor of three. Â© 2020 IEEE.",,,
10.1109/WACV45572.2020.9093476,2020,"Wang Z., Zhao J., Lu C., Huang H., Yang F., Li L., Guo Y.",Learning to detect head movement in unconstrained remote gaze estimation in the wild,"Unconstrained remote gaze estimation remains challenging mostly due to its vulnerability to the large variability in head-pose. Prior solutions struggle to maintain reliable accuracy in unconstrained remote gaze tracking. Among them, appearance-based solutions demonstrate tremendous potential in improving gaze accuracy. However, existing works still suffer from head movement and are not robust enough to handle real-world scenarios. Especially most of them study gaze estimation under controlled scenarios where the collected datasets often cover limited ranges of both head-pose and gaze which introduces further bias. In this paper, we propose novel end-to-end appearance-based gaze estimation methods that could more robustly incorporate different levels of head-pose representations into gaze estimation. Our method could generalize to real-world scenarios with low image quality, different lightings and scenarios where direct head-pose information is not available. To better demonstrate the advantage of our methods, we further propose a new benchmark dataset with the most rich distribution of head-gaze combination reflecting real-world scenarios. Extensive evaluations on several public datasets and our own dataset demonstrate that our method consistently outperforms the state-of-the-art by a significant margin. Â© 2020 IEEE.",,,
10.1109/VR46266.2020.1580098058128,2020,"Ichii T., Mitake H., Hasegawa S.",TEllipsoid: Ellipsoidal Display for Videoconference System Transmitting Accurate Gaze Direction,"We propose ""TEllipsoid"", an ellipsoidal display for video conference systems that can provide not only accurate eye-gaze transmission but also practicality in conferences, namely the convenience to use and the preservation of the identity of the displayed face.The display comprises an ellipsoidal screen, a small projector, and a convex mirror, where the bottom-installed projector projects the facial image of a remote participant onto the screen via the convex mirror. The facial image is made from photos shot from 360 degrees around the participant. Moreover, the image is modified to improve identity. The gaze representation is implemented by projecting the 3D model of eyeballs onto a virtual ellipsoidal screen.We evaluated the gaze transmissibility of the display in conference situations. As a result of experiments, we concluded that accurate gaze transmission is available in conferences when the angular distance of the adjacent participants is more than 38.5 degrees. Â© 2020 IEEE.",,,
10.1109/WACV45572.2020.9093419,2020,"Chen Z., Shi B.E.",Offset calibration for appearance-based gaze estimation via gaze decomposition,"Appearance-based gaze estimation provides relatively unconstrained gaze tracking. However, subject-independent models achieve limited accuracy partly due to individual variations. To improve estimation, we propose a gaze decomposition method that enables low complexity calibration, i.e., using calibration data collected when subjects view only one or a few gaze targets and the number of images per gaze target is small. Lowering the complexity of calibration makes it more convenient and less timeconsuming for the user, and more widely applicable. Motivated by our finding that the inter-subject squared bias exceeds the intra-subject variance for a subject-independent estimator, we decompose the gaze estimate into the sum of a subject-independent term estimated from the input image by a deep convolutional network and a subject-dependent bias term. During training, both the weights of the deep network and the bias terms are estimated. During testing, if no calibration data is available, we can set the bias term to zero. Otherwise, the bias term can be estimated from images of the subject gazing at known gaze targets. Experimental results on three datasets show that without calibration, our method outperforms state-of-the-art by at least 6.3%. For low complexity calibration sets, our method outperforms other calibration methods. More complex calibration algorithms do not outperform our method until the size of the calibration set is excessively large. Even then, the gains obtained by alternatives are small, e.g., only 0.1Â° lower error for 64 gaze targets. Source code is available at https://github.com/czk32611/Gaze-Decomposition. Â© 2020 IEEE.",,,
10.1109/VR46266.2020.1581300202881,2020,"Rahman Y., Asish S.M., Fisher N.P., Bruce E.C., Kulshreshth A.K., Borst C.W.",Exploring Eye Gaze Visualization Techniques for Identifying Distracted Students in Educational VR,"Virtual Reality (VR) headsets with embedded eye trackers are appearing as consumer devices (e.g. HTC Vive Eye, FOVE). These devices could be used in VR-based education (e.g., a virtual lab, a virtual field trip) in which a live teacher guides a group of students. The eye tracking could enable better insights into students' activities and behavior patterns. For real-time insight, a teacher's VR environment can display student eye gaze. These visualizations would help identify students who are confused/distracted, and the teacher could better guide them to focus on important objects. We present six gaze visualization techniques for a VR-embedded teacher's view, and we present a user study to compare these techniques. The results suggest that a short particle trail representing eye trajectory is promising. In contrast, 3D heatmaps (an adaptation of traditional 2D heatmaps) for visualizing gaze over a short time span are problematic. Â© 2020 IEEE.",,,
10.1109/VRW50115.2020.00196,2020,"Heo H., Lee M., Kim S., Hwang Y.",Gaze+Gesture Interface: Considering Social Acceptability,"In public places like cafes, the usage of smart glasses with interfaces including controller, touchpad, voice, or mid-air gesture not only receives a lot of interest from people around him or her but also cannot protect the privacy of individuals. If smart glasses become more advanced and more popular than regular glasses in the future, socially acceptable user interfaces can be required. In this paper, we propose a user interface on HoloLens by using gaze tracking instead of head tracking for navigation, and unobtrusive gesture based on deep learning instead of mid-air gestures for selection/manipulation, that is more socially acceptable than the existing user interfaces on smart glasses. A study was conducted to investigate social acceptability from the users' perspective, and the results showed the advantages of the proposed method to improve social acceptability. Â© 2020 IEEE.",,,
10.1109/VRW50115.2020.00146,2020,"Onuki Y., Kudo K., Kumazawa I.",Removal of the Infrared Light Reflection of Eyeglass Using Multi-Channel CycleGAN Applied for the Gaze Estimation Images,"In virtual reality (VR) environments, the importance of the eye gaze estimation is rapidly increasing. The geometrical model base method is commonly used by equipping infrared (IR) light sources and cameras inside of the head mount displays (HMDs). Some HMDs enable users to wear with spectacles on, and, in this case, IR light reflections of the eyeglass often causes serious obstruction to detect those of the corneal. In this study, we propose the multi-channel CycleGAN to generate the eye images with no eyeglass from those with eyeglass. Proposed method has 4 channels input, which consists of three normal eye images at different time points and an image in blinking, in order to distinguish stationary and moving reflections in images. Proposal achieved to selectively remove the eyeglass reflections and keep the corneal reflections alive in sequential gaze estimation images. Â© 2020 IEEE.",,,
10.3390/app10051668,2020,"Pavan Kumar B.N., Balasubramanyam A., Patil A.K., Chethana B., Chai Y.H.",GazeGuide: An eye-gaze-guided active immersive UAV camera,"Over the years, gaze input modality has been an easy and demanding human-computer interaction (HCI) method for various applications. The research of gaze-based interactive applications has advanced considerably, as HCIs are no longer constrained to traditional input devices. In this paper, we propose a novel immersive eye-gaze-guided camera (called GazeGuide) that can seamlessly control the movements of a camera mounted on an unmanned aerial vehicle (UAV) from the eye-gaze of a remote user. The video stream captured by the camera is fed into a head-mounted display (HMD) with a binocular eye tracker. The user's eye-gaze is the sole input modality to maneuver the camera. A user study was conducted considering the static and moving targets of interest in a three-dimensional (3D) space to evaluate the proposed framework. GazeGuide was compared with a state-of-the-art input modality remote controller. The qualitative and quantitative results showed that the proposed GazeGuide performed significantly better than the remote controller. Â© 2020 by the authors.",,,
10.1109/JBHI.2019.2933773,2020,"Temel D., Mathew M.J., Alregib G., Khalifa Y.M.",Relative Afferent Pupillary Defect Screening through Transfer Learning,"Abnormalities in pupillary light reflex can indicate optic nerve disorders that may lead to permanent visual loss if not diagnosed in an early stage. In this study, we focus on relative afferent pupillary defect (RAPD), which is based on the difference between the reactions of the eyes when they are exposed to light stimuli. Incumbent RAPD assessment methods are based on subjective practices that can lead to unreliable measurements. To eliminate subjectivity and obtain reliable measurements, we introduced an automated framework to detect RAPD. For validation, we conducted a clinical study with lab-on-a-headset, which can perform automated light reflex test. In addition to benchmarking handcrafted algorithms, we proposed a transfer learning-based approach that transformed a deep learning-based generic object recognition algorithm into a pupil detector. Based on the conducted experiments, proposed algorithm RAPDNet can achieve a sensitivity and a specificity of 90.6% over 64 test cases in a balanced set, which corresponds to an AUC of 0.929 in ROC analysis. According to our benchmark with three handcrafted algorithms and nine performance metrics, RAPDNet outperforms all other algorithms in every performance category. Â© 2013 IEEE.",,,
10.1007/s10055-019-00386-w,2020,"Wibirama S., Santosa P.I., Widyarani P., Brilianto N., Hafidh W.",Physical discomfort and eye movements during arbitrary and optical flow-like motions in stereo 3D contents,"Users of stereo 3D technology commonly report physical discomfort during or after exposure of stereo 3D contents. The discomfort has been associated with sensation of arbitrary and optical flow-like self-motion. However, there is no information on whether arbitrary motion induces stronger physical discomfort compared with optical flow-like motion. To address this research gap, we investigate physical discomfort among players and spectators of stereo 3D contents using eye tracking and Simulator Sickness Questionnaire. Thirty participants (N= 30) acted as players and spectators of a first-person shooter (FPS) and a car racing game. The FPS and the car racing game produce a sensation of arbitrary and optical flow-like self-motion, respectively. Experimental results show that the FPS game induces more severe physical discomfort than its racing counterpart (p< 0.0083 , with a Bonferroni correction to the p value). We also found that severeness of oculomotor symptoms can be predicted using two eye movements metrics: the amount of fixational eye movements and viewing duration at the center of the screen. Our study implies that one should pay particular attention to different types of self-motion in stereo 3D contents regardless of whether the user controls or solely watches the contents. Our study also suggests that physical discomfort can be reduced by decreasing the frequency of fixational eye movements while prolonging the duration of each fixation at the center of screen. Â© 2019, Springer-Verlag London Ltd., part of Springer Nature.",,,
10.1145/3379155.3391317,2020,"Garbin S.J., Shen Y., Schuetz I., Cavin R., Hughes G., Komogortsev O., Talathi S.S.",Dataset for eye tracking on a virtual reality platform,"We present a large scale data set of eye-images captured using a virtual-reality (VR) head mounted display mounted with two synchronized eye-facing cameras at a frame rate of 200 Hz under controlled illumination. This dataset is compiled from video capture of the eye-region collected from 152 individual participants and is divided into four subsets: (i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil and sclera (ii) 252,690 unlabeled eye-images, (iii) 91,200 frames from randomly selected video sequences of 1.5 seconds in duration, and (iv) 143 pairs of left and right point cloud data compiled from corneal topography of eye regions collected from a subset, 143 out of 152, participants in the study. A baseline experiment has been evaluated on the dataset for the task of semantic segmentation of pupil, iris, sclera and background, with the mean intersection-over-union (mIoU) of 98.3 %. We anticipate that this dataset will create opportunities to researchers in the eye tracking community and the broader machine learning and computer vision community to advance the state of eye-tracking for VR applications, which in its turn will have greater implications in Human-Computer Interaction. Â© 2020 ACM.",,,
10.1145/3379155.3391318,2020,"Keyvanara M., Allison R.",Effect of a constant camera rotation on the visibility of transsaccadic camera shifts,"Often in 3D games and virtual reality, changes in fixation occur during locomotion or other simulated head movements. We investigated whether a constant camera rotation in a virtual scene modulates saccadic suppression. The users viewed 3D scenes from the vantage point of a virtual camera which was either stationary or rotated at a constant rate about a vertical axis (camera pan) or horizontal axis (camera tilt). During this motion, observers fixated an object that was suddenly displaced horizontally/vertically in the scene, triggering them to produce a saccade. During the saccade an additional sudden movement was applied to the virtual camera. We estimated discrimination thresholds for these transsaccadic camera shifts using a Bayesian adaptive procedure. With an ongoing camera pan, we found higher thresholds (less noticeability) for additional sudden horizontal camera motion. Likewise, during simulated vertical head movements (i.e. a camera tilt), vertical transsaccadic image displacements were better hidden from the users for both horizontal and vertical saccades. Understanding the effect of continuous movement on the visibility of a sudden transsaccadic change can help optimize the visual performance of gaze-contingent displays and improve user experience. Â© 2020 ACM.",,,
10.1145/3379155.3391314,2020,"Barz M., Stauden S., Sonntag D.",Visual search target inference in natural interaction settings with machine learning,"Visual search is a perceptual task in which humans aim at identifying a search target object such as a traffic sign among other objects. Search target inference subsumes computational methods for predicting this target by tracking and analyzing overt behavioral cues of that person, e.g., the human gaze and fixated visual stimuli. We present a generic approach to inferring search targets in natural scenes by predicting the class of the surrounding image segment. Our method encodes visual search sequences as histograms of fixated segment classes determined by SegNet, a deep learning image segmentation model for natural scenes. We compare our sequence encoding and model training (SVM) to a recent baseline from the literature for predicting the target segment. Also, we use a new search target inference dataset. The results show that, first, our new segmentation-based sequence encoding outperforms the method from the literature, and second, that it enables target inference in natural settings. Â© 2020 ACM.",,,
10.1145/3379155.3391320,2020,"Castner N., KÃ¼ebler T.C., Scheiter K., Richter J., Eder T., HÃ¼ettig F., Keutel C., Kasneci E.",Deep semantic gaze embedding and scanpath comparison for expertise classification during OPT viewing,"Modeling eye movement indicative of expertise behavior is decisive in user evaluation. However, it is indisputable that task semantics affect gaze behavior. We present a novel approach to gaze scanpath comparison that incorporates convolutional neural networks (CNN) to process scene information at the fixation level. Image patches linked to respective fixations are used as input for a CNN and the resulting feature vectors provide the temporal and spatial gaze information necessary for scanpath similarity comparison. We evaluated our proposed approach on gaze data from expert and novice dentists interpreting dental radiographs using a local alignment similarity score. Our approach was capable of distinguishing experts from novices with 93% accuracy while incorporating the image semantics. Moreover, our scanpath comparison using image patch features has the potential to incorporate task semantics from a variety of tasks. Â© 2020 ACM.",,,
10.1145/3379155.3391327,2020,"Eskildsen A.M., Hansen D.W.",Label Likelihood Maximisation: Adapting iris segmentation models using domain adaptation,"We propose to use unlabelled eye image data for domain adaptation of an iris segmentation network. Adaptation allows the model to be less reliant on its initial generality. This is beneficial due to the large variance exhibited by eye image data which makes training of robust models difficult. The method uses a label prior in conjunction with network predictions to produce pseudo-labels. These are used in place of ground-truth data to adapt a base model. A fully connected neural network performs the pixel-wise iris segmentation. The base model is trained on synthetic data and adapted to several existing datasets with real-world eye images. The adapted models improve the average pupil centre detection rates by 24% at a distance of 25 pixels. We argue that the proposed method, and domain adaptation in general, is an interesting direction for increasing robustness of eye feature detectors. Â© 2020 ACM.",,,
10.1145/3379155.3391316,2020,"Venuprasad P., Xu L., Huang E., Gilman A., Leanne Chukoskie L., Cosman P.",Analyzing gaze behavior using object detection and unsupervised clustering,"Gaze behavior is important in early development, and atypical gaze behavior is among the first symptoms of autism. Here we describe a system that quantitatively assesses gaze behavior using eye-tracking glasses. Objects in the subject's field of view are detected using a deep learning model on the video captured by the glasses' world-view camera, and a stationary frame of reference is estimated using the positions of the detected objects. The gaze positions relative to the new frame of reference are subjected to unsupervised clustering to obtain the time sequence of looks. The clustering method increases the accuracy of look detection on test videos compared against a previous algorithm, and is considerably more robust on videos with poor calibration. Â© 2020 ACM.",,,
10.1145/3379155.3391315,2020,"BÃ¢ce M., Becker V., Wang C., Bulling A.",Combining gaze estimation and optical flow for pursuits interaction,"Pursuit eye movements have become widely popular because they enable spontaneous eye-based interaction. However, existing methods to detect smooth pursuits require special-purpose eye trackers. We propose the first method to detect pursuits using a single off-the-shelf RGB camera in unconstrained remote settings. The key novelty of our method is that it combines appearance-based gaze estimation with optical flow in the eye region to jointly analyse eye movement dynamics in a single pipeline. We evaluate the performance and robustness of our method for different numbers of targets and trajectories in a 13-participant user study. We show that our method not only outperforms the current state of the art but also achieves competitive performance to a consumer eye tracker for a small number of targets. As such, our work points towards a new family of methods for pursuit interaction directly applicable to an ever-increasing number of devices readily equipped with cameras. Â© 2020 ACM.",,,
10.1145/3379156.3391379,2020,"Szalma J., Weiss B.",Data-Driven classification of dyslexia using eye-movement correlates of natural reading,"Developmental dyslexia is a reading disability estimated to affect between 5 to 10 percent of the population. Current screening methods are limited as they tell very little about the oculomotor processes underlying natural reading. Investigation of eye-movement correlates of reading using machine learning could enhance detection of dyslexia. Here we used eye-tracking data collected during natural reading of 48 young adults (24 dyslexic, 24 control). We established a set of 67 features containing saccade-, glissade-, fixation-related measures and the reading speed. To detect participants with dyslexic reading patterns, we used a linear support vector machine with 10-fold stratified cross-validation repeated 10 times. For feature selection we used a recursive feature elimination method, and we also considered hyperparameter optimization, both with nested and regular cross-validation. The overall best model achieved a 90.1% classification accuracy, while the best nested model achieved a 75.75% accuracy. Â© 2020 ACM.",,,
10.1145/3379156.3391370,2020,"Katrychuk D., Griffith H., Komogortsev O.",A calibration framework for photosensor-based eye-tracking system,"The majority of eye-tracking systems require user-specific calibration to achieve suitable accuracy. Traditional calibration is performed by presenting targets at fixed locations that form a certain coverage of the device screen. If simple regression methods are used to learn a gaze map from the recorded data, the risk of overfitting is minimal. This is not the case if a gaze map is formed using neural networks, as is often employed in photosensor oculography (PSOG), which raises the question of careful design of calibration procedure. This paper evaluates different calibration data parsing approaches and the collection time-performance trade-off effect of grid density to build a calibration framework for PSOG with the use of video-based simulation framework. Â© 2020 ACM.",,,
10.1145/3379156.3391340,2020,"SchÃ¤fer A., Isomura T., Reis G., Watanabe K., Stricker D.",MutualEyeContact: A conversation analysis tool with focus on eye contact,"Eye contact between individuals is particularly important for understanding human behaviour. To further investigate the importance of eye contact in social interactions, portable eye tracking technology seems to be a natural choice. However, the analysis of available data can become quite complex. Scientists need data that is calculated quickly and accurately. Additionally, the relevant data must be automatically separated to save time. In this work, we propose a tool called MutualEyeContact which excels in those tasks and can help scientists to understand the importance of (mutual) eye contact in social interactions. We combine state-of-the-art eye tracking with face recognition based on machine learning and provide a tool for analysis and visualization of social interaction sessions. This work is a joint collaboration of computer scientists and cognitive scientists. It combines the fields of social and behavioural science with computer vision and deep learning. Â© 2020 ACM.",,,
10.1145/3379156.3391368,2020,"Garde G., Larumbe-Bergera A., Bossavit B., Cabeza R., Porta S., Villanueva A.",Gaze estimation problem tackled through synthetic images,"In this paper, we evaluate a synthetic framework to be used in the field of gaze estimation employing deep learning techniques. The lack of sufficient annotated data could be overcome by the utilization of a synthetic evaluation framework as far as it resembles the behavior of a real scenario. In this work, we use U2Eyes synthetic environment employing I2Head datataset as real benchmark for comparison based on alternative training and testing strategies. The results obtained show comparable average behavior between both frameworks although significantly more robust and stable performance is retrieved by the synthetic images. Additionally, the potential of synthetically pretrained models in order to be applied in user's specific calibration strategies is shown with outstanding performances. Â© 2020 ACM.",,,
10.1145/3379156.3391337,2020,"Tavakoli H.R., Borji A., Kannala J., Rahtu E.",Deep audio-visual saliency: Baseline model and data,"This paper introduces a conceptually simple and effective Deep Audio-Visual Embedding for dynamic saliency prediction dubbed ""DAVE"" in conjunction with our efforts towards building an Audio-Visual Eye-tracking corpus named ""AVE"". Despite existing a strong relation between auditory and visual cues for guiding gaze during perception, video saliency models only consider visual cues and neglect the auditory information that is ubiquitous in dynamic scenes. Here, we propose a baseline deep audio-visual saliency model for multi-modal saliency prediction in the wild. Thus the proposed model is intentionally designed to be simple. A video baseline model is also developed on the same architecture to assess effectiveness of the audio-visual models on a fair basis. We demonstrate that audio-visual saliency model outperforms the video saliency models. The data and code are available at https://hrtavakoli.github.io/AVE/and https://github.com/hrtavakoli/DAVE. Â© 2020 ACM.",,,
10.1145/3379156.3391376,2020,"Palmero Cantarino C., Komogortsev O.V., Talathi S.S.",Benefits of temporal information for appearance-based gaze estimation,"State-of-the-art appearance-based gaze estimation methods, usually based on deep learning techniques, mainly rely on static features. However, temporal trace of eye gaze contains useful information for estimating a given gaze point. For example, approaches leveraging sequential eye gaze information when applied to remote or low-resolution image scenarios with off-the-shelf cameras are showing promising results. The magnitude of contribution from temporal gaze trace is yet unclear for higher resolution/frame rate imaging systems, in which more detailed information about an eye is captured. In this paper, we investigate whether temporal sequences of eye images, captured using a high-resolution, high-frame rate head-mounted virtual reality system, can be leveraged to enhance the accuracy of an end-to-end appearance-based deep-learning model for gaze estimation. Performance is compared against a static-only version of the model. Results demonstrate statistically-significant benefits of temporal information, particularly for the vertical component of gaze. Â© 2020 ACM.",,,
10.1145/3379156.3391831,2020,"Menges R., Kramer S., Hill S., Nisslmueller M., Kumar C., Staab S.",A visualization tool for eye tracking data analysis in the web,"Usability analysis plays a significant role in optimizing Web interaction by understanding the behavior of end users. To support such analysis, we present a tool to visualize gaze and mouse data of Web site interactions. The proposed tool provides not only the traditional visualizations with fixations, scanpath, and heatmap, but allows for more detailed analysis with data clustering, demographic correlation, and advanced visualization like attention flow and 3D-scanpath. To demonstrate the usefulness of the proposed tool, we conducted a remote qualitative study with six analysts, using a dataset of 20 users browsing eleven real-world Web sites. Â© 2020 ACM.",,,
10.1145/3379156.3391829,2020,"Pathmanathan N., Becher M., Rodrigues N., Reina G., Ertl T., Weiskopf D., Sedlmair M.",Eye vs. Head: Comparing gaze methods for interaction in augmented reality,"Visualization in virtual 3D environments can provide a natural way for users to explore data. Often, arm and short head movements are required for interaction in augmented reality, which can be tiring and strenuous though. In an effort toward more user-friendly interaction, we developed a prototype that allows users to manipulate virtual objects using a combination of eye gaze and an external clicker device. Using this prototype, we performed a user study comparing four different input methods of which head gaze plus clicker was preferred by most participants. Â© 2020 ACM.",,,
10.1145/3379156.3391354,2020,"Mokatren M., Kuflik T., Shimshoni I.",EyeLinks: Methods to compute reliable stereo mappings used for eye gaze tracking,"We present methods for extracting corneal images and estimating pupil centers continuously and reliably using head worn glasses that consists of two eye cameras. An existing CNN was modified for detecting pupils in IR and RGB images, and stereo vision together with 2D and 3D models are used. We confirm the feasibility of the proposed methods through user study results, which show that the methods can be used in future real gaze estimation systems. Â© 2020 ACM.",,,
10.1145/3379156.3391361,2020,"Kumar A., Howlader P., Garcia R., Weiskopf D., Mueller K.",Challenges in interpretability of neural networks for eye movement data,"Many applications in eye tracking have been increasingly employing neural networks to solve machine learning tasks. In general, neural networks have achieved impressive results in many problems over the past few years, but they still suffer from the lack of interpretability due to their black-box behavior. While previous research on explainable AI has been able to provide high levels of interpretability for models in image classification and natural language processing tasks, little effort has been put into interpreting and understanding networks trained with eye movement datasets. This paper discusses the importance of developing interpretability methods specifically for these models. We characterize the main problems for interpreting neural networks with this type of data, how they differ from the problems faced in other domains, and why existing techniques are not sufficient to address all of these issues. We present preliminary experiments showing the limitations that current techniques have and how we can improve upon them. Finally, based on the evaluation of our experiments, we suggest future research directions that might lead to more interpretable and explainable neural networks for eye tracking. Â© 2020 ACM.",,,
10.1145/3379156.3391835,2020,"Ã–ney S., Rodrigues N., Becher M., Ertl T., Reina G.",Evaluation of gaze depth estimation from eye tracking in augmented reality,"Gaze tracking in 3D has the potential to improve interaction with objects and visualizations in augmented reality. However, previous research showed that subjective perception of distance varies between real and virtual surroundings. We wanted to determine whether objectively measured 3D gaze depth through eye tracking also exhibits differences between entirely real and augmented environments. To this end, we conducted an experiment (N = 25) in which we used Microsoft HoloLens with a binocular eye tracking add-on from Pupil Labs. Participants performed a task that required them to look at stationary real and virtual objects while wearing a HoloLens device. We were not able to find significant differences in the gaze depth measured by eye tracking. Finally, we discuss our findings and their implications for gaze interaction in immersive analytics, and the quality of the collected gaze data. Â© 2020 ACM.",,,
10.1145/3379156.3391346,2020,"Fuhl W., Gao H., Kasneci E.",Neural networks for optical vector and eye ball parameter estimation,"In this work we evaluate neural networks, support vector machines and decision trees for the regression of the center of the eyeball and the optical vector based on the pupil ellipse. In the evaluation we analyze single ellipses as well as window-based approaches as input. Comparisons are made regarding accuracy and runtime. The evaluation gives an overview of the general expected accuracy with different models and amounts of input ellipses. A simulator was implemented for the generation of the training and evaluation data. For a visual evaluation and to push the state of the art in optical vector estimation, the best model was applied to real data. This real data came from public data sets in which the ellipse is already annotated by an algorithm. The optical vectors on real data and the generator are made publicly available. Link to the generator and models. Â© 2020 ACM.",,,
10.1145/3379156.3391365,2020,"Hausamann P., Sinnott C., MacNeilage P.R.",Positional head-eye tracking outside the lab: An open-source solution,"Simultaneous head and eye tracking has traditionally been confined to a laboratory setting and real-world motion tracking limited to measuring linear acceleration and angular velocity. Recently available mobile devices such as the Pupil Core eye tracker and the Intel RealSense T265 motion tracker promise to deliver accurate measurements outside the lab. Here, the researchers propose a hard-and software framework that combines both devices into a robust, usable, low-cost head and eye tracking system. The developed software is open source and the required hardware modifications can be 3D printed. The researchers demonstrate the system's ability to measure head and eye movements in two tasks: an eyes-fixed head rotation task eliciting the vestibulo-ocular reflex inside the laboratory, and a natural locomotion task where a subject walks around a building outside of the laboratory. The resultant head and eye movements are discussed, as well as future implementations of this system. Â© 2020 ACM.",,,
10.1145/3379156.3391830,2020,"Streichert A., Angerbauer K., Schwarzl M., Sedlmair M.",Comparing input modalities for shape drawing tasks,"With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N=20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods. Â© 2020 ACM.",,,
10.1145/3379156.3391364,2020,"Bozkir E., Ãœnal A.B., AkgÃ¼n M., Kasneci E., Pfeifer N.",Privacy preserving gaze estimation using synthetic images via a randomized encoding based framework,"Eye tracking is handled as one of the key technologies for applications that assess and evaluate human attention, behavior, and biometrics, especially using gaze, pupillary, and blink behaviors. One of the challenges with regard to the social acceptance of eye tracking technology is however the preserving of sensitive and personal information. To tackle this challenge, we employ a privacy-preserving framework based on randomized encoding to train a Support Vector Regression model using synthetic eye images privately to estimate the human gaze. During the computation, none of the parties learn about the data or the result that any other party has. Furthermore, the party that trains the model cannot reconstruct pupil, blinks or visual scanpath. The experimental results show that our privacy-preserving framework is capable of working in real-time, with the same accuracy as compared to non-private version and could be extended to other eye tracking related problems. Â© 2020 ACM.",,,
10.1145/3379156.3391335,2020,"Ahn S., Kelton C., Balasubramanian A., Zelinsky G.",Towards predicting reading comprehension from gaze behavior,"As readers of a language, we all agree to move our eyes in roughly the same way. Yet might there be hidden within this self-similar behavior subtle clues as to how a reader is understanding the material being read? Here we attempt to decode a reader's eye movements to predict their level of text comprehension and related states. Eye movements were recorded from 95 people reading 4 published SAT passages, each followed by corresponding SAT questions and self-evaluation questionnaires. A sequence of 21 fixation-location (x,y), fixation-duration, and pupil-size features were extracted from the reading behavior and input to two deep networks (CNN/RNN), which were used to predict the reader's comprehension level and other comprehension-related variables. The best overall comprehension prediction accuracy was 65% (cf. null accuracy = 54%) obtained by CNN. This prediction generalized well to fixations on new passages (64%) from the same readers, but did not generalize to fixations from new readers (41%), implying substantial individual differences in reading behavior. Our work is the first attempt to predict comprehension from fixations using deep networks, where we hope that our large reading dataset and our protocol for evaluation will benefit the development of new methods for predicting reading comprehension by decoding gaze behavior. Â© 2020 ACM.",,,
10.1109/ICICT48043.2020.9112493,2020,"Taha Ahmed Z.A., Jadhav M.E.",A Review of Early Detection of Autism Based on Eye-Tracking and Sensing Technology,"The current paper is a review of eye-tracking and sensing technologies that detect and monitor Autism Spectrum Disorder (ASD). Nowadays, the biggest challenge is the detection of autism before the age of 36 months. The diagnosis of autism in the early stage of life can help autistic children improve their social communication and quality of life. Therefore, the technology can support psychologists to get the right diagnoses of autism and accordingly the autistic children can get appropriate treatment for their condition. In this review, the focus is on eyetracking and sensing technologies. The autistic children have different attentional biases in social interactions that can be measured by eye-tracking technology. Moreover, the autistic children have some signs that can be easily detected by using the sensing technology such as hand flapping, body rocking and motion trackers. Â© 2020 IEEE.",,,
10.1109/SANER48275.2020.9054848,2020,"Saddler J.A., Peterson C.S., Sama S., Nagaraj S., Baysal O., Guerrouj L., Sharif B.",Studying Developer Reading Behavior on Stack Overflow during API Summarization Tasks,"Stack Overflow is commonly used by software developers to help solve problems they face while working on software tasks such as fixing bugs or building new features. Recent research has explored how the content of Stack Overflow posts affects attraction and how the reputation of users attracts more visitors. However, there is very little evidence on the effect that visual attractors and content quantity have on directing gaze toward parts of a post, and which parts hold the attention of a user longer. Moreover, little is known about how these attractors help developers (students and professionals) answer comprehension questions. This paper presents an eye tracking study on thirty developers constrained to reading only Stack Overflow posts while summarizing four open source methods or classes. Results indicate that on average paragraphs and code snippets were fixated upon most often and longest. When ranking pages by number of appearance of code blocks and paragraphs, we found that while the presence of more code blocks did not affect number of fixations, the presence of increasing numbers of plain text paragraphs significantly drove down the fixations on comments. SO posts that were looked at only by students had longer fixation times on code elements within the first ten fixations. We found that 16 developer summaries contained 5 or more meaningful terms from SO posts they viewed. We discuss how our observations of reading behavior could benefit how users structure their posts. Â© 2020 IEEE.",,,
10.3390/electronics9020266,2020,"Sulikowski P., Zdziebko T.",Deep learning-enhanced framework for performance evaluation of a recommending interface with varied recommendation position and intensity based on eye-tracking equipment data processing,"The increasing amount of marketing content in e-commerce websites results in the limited attention of users. For recommender systems, the way recommended items are presented becomes as important as the underlying algorithms for product selection. In order to improve the effectiveness of content presentation, marketing experts experiment with the layout and other visual aspects of website elements to find the most suitable solution. This study investigates those aspects for a recommending interface. We propose a framework for performance evaluation of a recommending interface, which takes into consideration individual user characteristics and goals. At the heart of the proposed solution is a deep neutral network trained to predict the efficiency a particular recommendation presented in a selected position and with a chosen degree of intensity. The proposed Performance Evaluation of a Recommending Interface (PERI) framework can be used to automate an optimal recommending interface adjustment according to the characteristics of the user and their goals. The experimental results from the study are based on research-grade measurement electronics equipment Gazepoint GP3 eye-tracker data, together with synthetic data that were used to perform pre-assessment training of the neural network. Â© 2020 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/TMI.2019.2927226,2020,"Li L., Xu M., Liu H., Li Y., Wang X., Jiang L., Wang Z., Fan X., Wang N.",A Large-Scale Database and a CNN Model for Attention-Based Glaucoma Detection,"Glaucoma is one of the leading causes of irreversible vision loss. Many approaches have recently been proposed for automatic glaucoma detection based on fundus images. However, none of the existing approaches can efficiently remove high redundancy in fundus images for glaucoma detection, which may reduce the reliability and accuracy of glaucoma detection. To avoid this disadvantage, this paper proposes an attention-based convolutional neural network (CNN) for glaucoma detection, called AG-CNN. Specifically, we first establish a large-scale attention-based glaucoma (LAG) database, which includes 11 760 fundus images labeled as either positive glaucoma (4878) or negative glaucoma (6882). Among the 11 760 fundus images, the attention maps of 5824 images are further obtained from ophthalmologists through a simulated eye-tracking experiment. Then, a new structure of AG-CNN is designed, including an attention prediction subnet, a pathological area localization subnet, and a glaucoma classification subnet. The attention maps are predicted in the attention prediction subnet to highlight the salient regions for glaucoma detection, under a weakly supervised training manner. In contrast to other attention-based CNN methods, the features are also visualized as the localized pathological area, which are further added in our AG-CNN structure to enhance the glaucoma detection performance. Finally, the experiment results from testing over our LAG database and another public glaucoma database show that the proposed AG-CNN approach significantly advances the state-of-the-art in glaucoma detection. Â© 1982-2012 IEEE.",,,
10.1016/j.media.2019.101631,2020,"Dunnhofer M., Antico M., Sasazawa F., Takeda Y., Camps S., Martinel N., Micheloni C., Carneiro G., Fontanarosa D.",Siam-U-Net: encoder-decoder siamese network for knee cartilage tracking in ultrasound images,"The tracking of the knee femoral condyle cartilage during ultrasound-guided minimally invasive procedures is important to avoid damaging this structure during such interventions. In this study, we propose a new deep learning method to track, accurately and efficiently, the femoral condyle cartilage in ultrasound sequences, which were acquired under several clinical conditions, mimicking realistic surgical setups. Our solution, that we name Siam-U-Net, requires minimal user initialization and combines a deep learning segmentation method with a siamese framework for tracking the cartilage in temporal and spatio-temporal sequences of 2D ultrasound images. Through extensive performance validation given by the Dice Similarity Coefficient, we demonstrate that our algorithm is able to track the femoral condyle cartilage with an accuracy which is comparable to experienced surgeons. It is additionally shown that the proposed method outperforms state-of-the-art segmentation models and trackers in the localization of the cartilage. We claim that the proposed solution has the potential for ultrasound guidance in minimally invasive knee procedures. Â© 2019",,,
10.1007/s11548-019-02080-3,2020,"Li G., Patel N.A., Hagemeister J., Yan J., Wu D., Sharma K., Cleary K., Iordachita I.",Body-mounted robotic assistant for MRI-guided low back pain injection,"Purpose: This paper presents the development of a body-mounted robotic assistant for magnetic resonance imaging (MRI)-guided low back pain injection. Our goal was to eliminate the radiation exposure of traditional X-ray guided procedures while enabling the exquisite image quality available under MRI. The robot is designed with a compact and lightweight profile that can be mounted directly on the patientâ€™s lower back via straps, thus minimizing the effect of patient motion by moving along with the patient. The robot was built with MR-conditional materials and actuated with piezoelectric motors so it can operate inside the MRI scanner bore during imaging and therefore streamline the clinical workflow by utilizing intraoperative MR images. Methods: The robot is designed with a four degrees of freedom parallel mechanism, stacking two identical Cartesian stages, to align the needle under intraoperative MRI-guidance. The system targeting accuracy was first evaluated in free space with an optical tracking system, and further assessed with a phantom study under live MRI-guidance. Qualitative imaging quality evaluation was performed on a human volunteer to assess the image quality degradation caused by the robotic assistant. Results: Free space positioning accuracy study demonstrated that the mean error of the tip position to be 0.51 Â± 0.27 mm and needle angle to be 0. 70 âˆ˜Â?0. 38 âˆ? MRI-guided phantom study indicated the mean errors of the target to be 1.70 Â± 0.21 mm, entry point to be 1.53 Â± 0.19 mm, and needle angle to be 0. 66 âˆ˜Â?0. 43 âˆ? Qualitative imaging quality evaluation validated that the image degradation caused by the robotic assistant in the lumbar spine anatomy is negligible. Conclusions: The study demonstrates that the proposed body-mounted robotic system is able to perform MRI-guided low back injection in a phantom study with sufficient accuracy and with minimal visible image degradation that should not affect the procedure. Â© 2019, CARS.",,,
10.2352/ISSN.2470-1173.2020.16.AVM-110,2020,"Ewaisha M., El Shawarby M., Abbas H., Sobh I.",End-to-end multitask learning for driver gaze and head pose estimation,"Modern automobiles accidents occur mostly due to inattentive behavior of drivers, which is why driver's gaze estimation is becoming a critical component in automotive industry. Gaze estimation has introduced many challenges due to the nature of the surrounding environment like changes in illumination, or driver's head motion, partial face occlusion, or wearing eye decorations. Previous work conducted in this field includes explicit extraction of hand-crafted features such as eye corners and pupil center to be used to estimate gaze, or appearance-based methods like Convolutional Neural Networks which implicitly extracts features from an image and directly map it to the corresponding gaze angle. In this work, a multitask Convolutional Neural Network architecture is proposed to predict subject's gaze yaw and pitch angles, along with the head pose as an auxiliary task, making the model robust to head pose variations, without needing any complex preprocessing or hand-crafted feature extraction.Then the network's output is clustered into nine gaze classes relevant in the driving scenario. The model achieves 95.8% accuracy on the test set and 78.2% accuracy in cross-subject testing, proving the model's generalization capability and robustness to head pose variation. Â© 2020, Society for Imaging Science and Technology.",,,
10.2352/ISSN.2470-1173.2020.11.HVEI-129,2020,"Blakey W.A., Katsigiannis S., Hajimirza N., Ramzan N.",Defining gaze tracking metrics by observing a growing divide between 2D and 3D tracking,"This work examines the different terminology used for defining gaze tracking technology and explores the different methodologies used for describing their respective accuracy. Through a comparative study of different gaze tracking technologies, such as infrared and webcam-based, and utilising a variety of accuracy metrics, this work shows how the reported accuracy can be misleading. The lack of intersection points between the gaze vectors of different eyes (also known as convergence points) in definitions has a huge impact on accuracy measures and directly impacts the robustness of any accuracy measuring methodology. Different accuracy metrics and tracking definitions have been collected and tabulated to more formally demonstrate the divide in definitions. Â© 2020, Society for Imaging Science and Technology",,,
10.2352/J.ImagingSci.Technol.2019.63.6.060403,2020,"Tanaka M., Lanaro M.P., Horiuchi T., Rizzi A.",Random spray retinex extensions considering region of interest and eyemovements,"The Random spray Retinex (RSR) algorithm was developed by taking into consideration the mathematical description of Milano-Retinex. The RSR substituted random paths with random sprays. Mimicking some characteristics of the human visual system (HVS), this article proposes two variants of RSR adding a mechanism of region of interest (ROI). In the first proposed model, a cone distribution based on anatomical data is considered as ROI. In the second model, the visual resolution depending on the visual field based on the knowledge of visual information processing is considered as ROI. We have measured actual eye movements using an eye-tracking system. By using the eye-tracking data, we have simulated the HVS using test images. Results show an interesting qualitative computation of the appearance of the processed area around real gaze points. Â© 2019 Society for Imaging Science and Technology.",,,
10.2352/ISSN.2470-1173.2020.9.IQSP-288,2020,"Jogeshwar A.K., Diaz G.J., Farnand S.P., Pelz J.B.",The cone model: Recognizing gaze uncertainty in virtual environments,"Eye tracking is used by psychologists, neurologists, vision researchers, and many others to understand the nuances of the human visual system, and to provide insight into a person's allocation of attention across the visual environment. When tracking the gaze behavior of an observer immersed in a virtual environment displayed on a head-mounted display, estimated gaze direction is encoded as a three-dimensional vector extending from the estimated location of the eyes into the 3D virtual environment. Additional computation is required to detect the target object at which gaze was directed. These methods must be robust to calibration error or eye tracker noise, which may cause the gaze vector to miss the target object and hit an incorrect object at a different distance. Thus, the straightforward solution involving a single vector-to-object collision could be inaccurate in indicating object gaze. More involved metrics that rely upon an estimation of the angular distance from the ray to the center of the object must account for an object's angular size based on distance, or irregularly shaped edges - information that is not made readily available by popular game engines (e.g. Unity Â© 2020 Society for Imaging Science and Technology.",,,
10.1145/3377049.3377108,2020,"Fahim Shahriar A.B.M., Moon M.Z., Mahmud H., Hasan K.",Online product recommendation system by using eye gaze data,"Recommendation system takes information related to the usersâ€?habits or interest or profile to suggest users with more convenient or similar materials that the users might be interested in. In general, these systems mostly rely on explicit feedback techniques (rating, search history etc.) to recommend products. In this case, users need to interact directly with the system. We seek implicit methods (indirect interaction) to relate usersâ€?preferences and recommend desired products automatically on the interface in order to minimize the meddling interaction and workload. In this paper, we present a recommendation system that will use usersâ€?eye gaze data to apprehend their interest to recommend products as an implicit feedback technique. Eye gaze data can provide information about the products that the users are interested in, without any direct interaction with the system. Eye gaze features is very effective as an implicit interaction technique. So, integrating eye gaze features with recommendation system will generate more user-oriented results. This system will collect usersâ€?eye gaze data during an e-commerce website navigation through a web-cam based eye tracker. Other features of a product (ratings, number of orders) were also included in generating the results in order to get more convenient results. Finally, a clustering-based machine learning algorithm was used to group the similar product based on the input data and recommend similar products to the users, implicitly expressed greater interest. In this study, we concluded that users can find their desired products with less physical assertion and more satisfaction. Â© 2020 Association for Computing Machinery.",,,
10.24867/GRID-2020-p38,2020,Franken G.,Packaging design and testing by eye tracking,"The importance of packaging design has been increasing in todayâ€™s competitive world. Approximately 70% of purchasing decisions are made in a store. Over 60% of purchasing decisions are based on packaging; the actual shopping is thus the final chance for the packaging to attract the buyer. Packaging has between 2 and 3 seconds to convince the buyer. In addition to the appearance of the individual packages, the appearance of brand packaging is important. We compared different designs of packages. Finally, we placed individual packages on the shelves in a store and carried out measurement of in-store noting. The measurements were carried out using eye tracking equipment (Tobii X120). For each participant, the observing time and the number of fixations in individual areas of interest were measured; both were then compared with heat maps. In this way, we compared the suitability of the form of individual packages and the salience of the packages on the shelves for potential buyers. Â© 2020 Authors.",,,
10.1109/ICPR48806.2021.9412709,2020,"Han S.Y., Cho N.I.",User-independent gaze estimation by extracting pupil parameter and its mapping to the gaze angle,"Since gaze estimation plays a crucial role in recognizing human intentions, it has been researched for a long time, and its accuracy is ever increasing. However, due to the wide variation in eye shapes and focusing abilities between the individuals, accuracies of most algorithms vary depending on each person in the test group, especially when the initial calibration is not well performed. To alleviate the user-dependency, we attempt to derive features that are general for most people and use them as the input to a deep network instead of using the images as the input. Specifically, we use the pupil shape as the core feature because it is directly related to the 3D eyeball rotation, and thus the gaze direction. While existing deep learning methods learn the gaze point by extracting various features from the image, we focus on the mapping function from the eyeball rotation to the gaze point by using the pupil shape as the input. It is shown that the accuracy of gaze point estimation also becomes robust for the uncalibrated points by following the characteristics of the mapping function. Also, our gaze network learns the gaze difference to facilitate the re-calibration process to fix the calibration-drift problem that typically occurs with glass-type or head-mount devices. Â© 2020 IEEE",,,
10.1109/ICPR48806.2021.9412066,2020,"Chugh S., Brousseau B., Rose J., Eizenman M.",Detection and correspondence matching of corneal reflections for eye tracking using deep learning,"Eye tracking systems that estimate the point-of-gaze are essential in extended reality (XR) systems as they enable new interaction paradigms and technological improvements. It is important for these systems to maintain accuracy when the headset moves relative to the head (known as device slippage) due to head movements or user adjustment. One of the most accurate eye tracking techniques, which is also insensitive to shifts of the system relative to the head, uses two or more infrared (IR) light emitting diodes to illuminate the eye and an IR camera to capture images of the eye. An essential step in estimating the point-of-gaze in these systems is the precise determination of the location of two or more corneal reflections (virtual images of the IR-LEDs that illuminate the eye) in images of the eye. Eye trackers tend to have multiple light sources to ensure at least one pair of reflections for each gaze position. The use of multiple light sources introduces a difficult problem: the need to match the corneal reflections with the corresponding light source over the range of expected eye movements. Corneal reflection detection and matching often fail in XR systems due to the proximity of camera and steep illumination angles of light sources with respect to the eye. The failures are caused by corneal reflections having varying shape and intensity levels or disappearance due to rotation of the eye, or the presence of spurious reflections. We have developed a fully convolutional neural network, based on the UNET architecture, that solves the detection and matching problem in the presence of spurious and missing reflections. Eye images of 25 people were collected in a virtual reality headset using a binocular eye tracking module consisting of five infrared light sources per eye. A set of 4,000 eye images were manually labelled for each of the corneal reflections, and data augmentation was used to generate a dataset of 40,000 images. The network is able to correctly identify and match 91% of corneal reflections present in the test set. This is comparable to a state-of-the-art deep learning system, but our approach requires 33 times less memory and executes 10 times faster. The proposed algorithm, when used in an eye tracker in a VR system, achieved an average mean absolute gaze error of 1Â°. This is a significant improvement over the state-of-the-art learning-based XR eye tracking systems that have reported gaze errors of 2-3Â°. Â© 2020 IEEE",,,
10.1109/ICPR48806.2021.9413229,2020,"Ralekar C., Gandhi T.K., Chaudhury S.",Collaborative human machine attention module for character recognition,"The deep learning models, which include attention mechanisms, are shown to enhance the performance and efficiency of the various computer vision tasks such as pattern recognition, object detection, face recognition, etc. Although the visual attention mechanism is the source of inspiration for these models, recent attention models consider 'attention' as a pure machine vision optimization problem, and visual attention remains the most neglected aspect. Therefore, this paper presents a collaborative human and machine attention module which considers both visual and network's attention. The proposed module is inspired by the dorsal ('where') pathways of visual processing and can be integrated with any convolutional neural network (CNN) model. First, the module computes the spatial attention map from the input feature maps, which is then combined with the visual attention maps. The visual attention maps are created using eye-fixations obtained by performing an eye-tracking experiment with human participants. The visual attention map covers the highly salient and discriminating image regions as humans tend to focus on such regions, whereas the other relevant image regions are processed by spatial attention map. The combination of these two maps results in the finer refinement in feature maps, resulting in improved performance. The comparative analysis reveals that our model not only shows significant improvement over the baseline model but also outperforms the other models. We hope that our findings using a collaborative human-machine attention module will be helpful in other computer vision tasks as well. Â© 2020 IEEE",,,
10.1109/ICPR48806.2021.9412857,2020,"Sean Liu H.Y., Chung J., Eizenman M.",A general end-to-end method for characterizing neuropsychiatric disorders using free-viewing visual scanning tasks,"The growing availability of eye-gaze tracking technology has allowed for its employment in a wide variety of applications, one of which is the objective diagnosis and monitoring of neuropsychiatric disorders from features of attentional bias extracted from visual scanning patterns. Current techniques in this field are largely comprised of non-generalizable methodologies that rely on domain expertise and study-specific assumptions. In this paper, we present a general, data-driven, end-to-end framework that extracts relevant features of attentional bias from visual scanning behaviour and uses these features to classify between subject groups with standard machine learning techniques. The general framework uses visual scanning data from free-viewing tasks. In these tasks, subjects look at sets of slides with several thematic images while their visual scanning patterns (sets of ordered fixations) are monitored by an eye-tracking system. Subjects' fixations are encoded into relative visual attention maps (RVAMs), and two data-driven methods are proposed to segment regions of interests (ROIs) from RVAMs: 1) using group average RVAMs, and 2) using differences of group average RVAMs. Relative fixation times within the segmented ROIs are then used as input features for a vanilla multilayered perceptron to classify between patient groups. The methods were evaluated on data from two studies: an anorexia nervosa (AN)/healthy controls study (AN study) with 37 subjects, and a bipolar disorder (BD)/major depressive disorder (MDD) study (BD-MDD study) with 73 subjects. Using leave-one-subject-out cross validation, the general methods achieved an area under the receiver operating curve (AUROC) score of 0.935 for the AN study and 0.888 for the BD-MDD study, the latter of which exceeds the performance of the state-of-the-art analysis model designed specifically for the BD-MDD study, which had an AUROC of 0.879. The results validate the proposed framework's efficacy as a generalizable, standard baseline for analyzing visual scanning data. Â© 2020 IEEE",,,
10.1109/ICPR48806.2021.9413139,2020,"Li M., Liu B., Hu Y., Wang Y.",Exposing deepfake videos by tracking eye movements,"It has recently become a major threat to the public media that fake videos are rapidly spreading over the Internet. The advent of Deepfake, a deep-learning based toolkit, has facilitated a massive abuse of improper synthesized videos, which may influence the media credibility and human rights. A worldwide alert has been set off that finding ways to detect such fake videos is not only crucial but also urgent. This paper reports a novel approach to expose deepfake videos. We found that most fake videos are markedly different from the real ones in the way the eyes move. We are thus motivated to define four features that could well capture such differences. The features are then fed to SVM for classification. It is shown to be a promising approach that without high dimensional features and complicated neural networks, we are able to achieve competitive results on several public datasets. Moreover, the proposed features could well participate with other existing methods in the confrontation with deepfakes. Â© 2020 IEEE",,,
10.1007/978-981-15-3341-9_11,2020,"Zheng C., Zhou J., Sun J., Zhao L.",Adaptive Person-Specific Appearance-Based Gaze Estimation,"Non-invasive gaze estimation from only eye images captured by camera is a challenging problem due to various eye shapes, eye structures and image qualities. Recently, CNN network has been applied to directly regress eye image to gaze direction and obtains good performance. However, generic approaches are susceptible to bias and variance highly relating to different individuals. In this paper, we study the person-specific bias when applying generic methods on new person. And we introduce a novel appearance-based deep neural network integrating meta-learning to reduce the person-specific bias. Given only a few person-specific calibration images collected in normal calibration process, our model adapts quickly to test person and predicts more accurate gaze directions. Experiments on public MPIIGaze dataset and Eyediap dataset show our approach has achieved competitive accuracy to current state-of-the-art methods and are able to alleviate person-specific bias problem. Â© 2020, Springer Nature Singapore Pte Ltd.",,,
,2020,"Zheng Y., Park S., Zhang X., de Mello S., Hilliges O.",Self-learning transformations for improving gaze and head redirection,"Many computer vision tasks rely on labeled data. Rapid progress in generative modeling has led to the ability to synthesize photorealistic images. However, controlling specific aspects of the generation process such that the data can be used for supervision of downstream tasks remains challenging. In this paper we propose a novel generative model for images of faces, that is capable of producing high-quality images under fine-grained control over eye gaze and head orientation angles. This requires the disentangling of many appearance related factors including gaze and head orientation but also lighting, hue etc. We propose a novel architecture which learns to discover, disentangle and encode these extraneous variations in a self-learned manner. We further show that explicitly disentangling task-irrelevant factors results in more accurate modelling of gaze and head orientation. A novel evaluation scheme shows that our method improves upon the state-of-the-art in redirection accuracy and disentanglement between gaze direction and head orientation changes. Furthermore, we show that in the presence of limited amounts of real-world training data, our method allows for improvements in the downstream task of semi-supervised cross-dataset gaze estimation. Please check our project page at: https://ait.ethz.ch/projects/2020/STED-gaze/ Â© 2020 Neural information processing systems foundation. All rights reserved.",,,
,2020,"Elbattah M., GuÃ©rin J.-L., Carette R., Cilia F., Dequen G.",Generative modeling of synthetic eye-tracking data: NLP-based approach with recurrent neural networks,"This study explores a Machine Learning-based approach for generating synthetic eye-tracking data. In this respect, a novel application of Recurrent Neural Networks is experimented. Our approach is based on learning the sequence patterns of eye-tracking data. The key idea is to represent eye-tracking records as textual strings, which describe the sequences of fixations and saccades. The study therefore could borrow methods from the Natural Language Processing (NLP) domain for transforming the raw eye-tracking data. The NLP-based transformation is utilised to convert the high-dimensional eye-tracking data into an amenable representation for learning. Furthermore, the generative modeling could be implemented as a task of text generation. Our empirical experiments support further exploration and development of such NLP-driven approaches for the purpose of producing synthetic eye-tracking datasets for a variety of potential applications. Copyright Â© 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved",,,
,2020,"Lueckenhoff A., Wessels C., Kyrarini M., Makedon F.",Towards game-based assessment of executive functions in children,"Executive Functions are very important mental skills that help us to coordinate, plan, pay attention, organize, and multitask, among others. Weak executive functions may affect school or work performance. Therefore, there is a need of identifying executive function deficits early during childhood and enable interventions that could improve executive functioning skills. In this work, we present a game-based assessment system of executive functions in children that could be performed at home. The proposed system utilizes machine learning techniques to detect and track head and eye movements from image frames and fuses this data with game performance. A novel variation of the Flanker task has been developed as a game to measure engagement, attention, working memory, and processing speed. In the future, the proposed system will be evaluated in a real-world study on children between 6 and 14 years old. Copyright Â© 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",,,
10.1109/ICPR48806.2021.9412205,2020,"Bao Y., Cheng Y., Liu Y., Lu F.",Adaptive feature fusion network for gaze tracking in mobile tablets,"Recently, many multi-stream gaze estimation methods have been proposed. They estimate gaze from eye and face appearances and achieve reasonable accuracy. However, most of the methods simply concatenate the features extracted from eye and face appearance. The feature fusion process has been ignored. In this paper, we propose a novel Adaptive Feature Fusion Network (AFF-Net), which performs gaze tracking task in mobile tablets. We stack two-eye feature maps and utilize Squeeze-and-Excitation layers to adaptively fuse two-eye features according to their similarity on appearance. Meanwhile, we also propose Adaptive Group Normalization to recalibrate eye features with the guidance of facial feature. Extensive experiments on both GazeCapture and MPIIFaceGaze datasets demonstrate consistently superior performance of the proposed method. Â© 2020 IEEE",,,
10.1109/ACCESS.2020.3023448,2020,"Wan Z., Xiong C., Li Q., Chen W., Wong K.K.L., Wu S.",Accurate regression-based 3d gaze estimation using multiple mapping surfaces,"Accurate 3D gaze estimation using a simple setup remains a challenging issue for head-mounted eye tracking. Current regression-based gaze direction estimation methods implicitly assume that all gaze directions intersect at one point called the eyeball pseudo-center. The effect of this implicit assumption on gaze estimation is unknown. In this paper, we find that this assumption is approximate based on a simulation of all intersections of gaze directions, and it is conditional based on a sensitivity analysis of the assumption in gaze estimation. Hence, we propose a gaze direction estimation method with one mapping surface that satisfies conditions of the assumption by configuring one mapping surface and achieving a high-quality calibration of the eyeball pseudo-center. This method only adds two additional calibration points outside the mapping surface. Furthermore, replacing the eyeball pseudo-center with an additional calibrated surface, we propose a gaze direction estimation method with two mapping surfaces that further improves the accuracy of gaze estimation. This method improves accuracy on the state-of-the-art method by 20 percent (from a mean error of 1.84 degrees to 1.48 degrees) on a public dataset with a usage range of 1 meter and by 17 percent (from a mean error of 2.22 degrees to 1.85 degrees) on a public dataset with a usage range of 2 meters. Â© 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",,,
10.1109/ACCESS.2020.3025195,2020,"Valenzuela A., Arellano C., Tapia J.E.",Towards an efficient segmentation algorithm for near-infrared eyes images,"Semantic segmentation has been widely used for several applications, including the detection of eye structures. This is used in tasks such as eye-tracking and gaze estimation, which are useful techniques for human-computer interfaces, salience detection, and Virtual reality (VR), amongst others. Most of the state of the art techniques achieve high accuracy but with a considerable number of parameters. This article explores alternatives to improve the efficiency of the state of the art method, namely DenseNet Tiramisu, when applied to NIR image segmentation. This task is not trivial; the reduction of block and layers also affects the number of feature maps. The growth rate (k) of the feature maps regulates how much new information each layer contributes to the global state, therefore the trade-off amongst grown rate (k), IOU, and the number of layers needs to be carefully studied. The main goal is to achieve a light-weight and efficient network with fewer parameters than traditional architectures in order to be used for mobile device applications. As a result, a DenseNet with only three blocks and ten layers is proposed (DenseNet10). Experiments show that this network achieved higher IOU rates when comparing with Encoder-Decoder, DensetNet56-67-103, MaskRCNN, and DeeplabV3+ models in the Facebook database. Furthermore, this method reached 8th place in The Facebook semantic segmentation challenge with 0.94293 mean IOU and 202.084 parameters with a final score of 0.97147. This score is only 0,001 lower than the first place in the competition. The sclera was identified as the more challenging structure to be segmented. Â© 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",,,
10.1109/ACCESS.2020.3029300,2020,"Chi J., Wang D., Lu N., Wang Z.",Cornea radius calibration for remote 3D gaze tracking systems,"Cornea radius estimation is a key technique for 3D gaze estimation in the single-camera 3D gaze tracking system. Traditional methods with one-camera-one-light-source systems or one-camera-two- light-source systems cannot achieve 3D gaze estimation. The 3D line-of-sight can be estimated only when the cornea radius is pre-calibrated by the user. A cornea radius calibration method based on the iris radius is proposed in this paper for 3D gaze estimation in remote one-camera-two-light-source systems. We first calibrate the iris radius based on the binocular strategy, estimate the spatial iris center using the calibrated iris radius, and then calibrate the cornea radius by a set of non-linear equations under the constraint of equivalent distances from the cornea center to the iris edge points. The calibrated cornea radius is verified by binocular optimization constraints. Simulations and physical experiments validate the effectiveness of the proposed method. The iris-based cornea radius calibration approach is novel; it can be used to obtain the cornea radius and 3D gaze using remote one-camera-one-light-source or one-camera-multi-light-source systems. Â© 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",,,
10.1007/978-3-030-66415-2_18,2020,"GÃ¼nther U., Harrington K.I.S., Dachselt R., Sbalzarini I.F.",Bionic Tracking: Using Eye Tracking to Track Biological Cells in Virtual Reality,"We present Bionic Tracking, a novel method for solving biological cell tracking problems with eye tracking in virtual reality using commodity hardware. Using gaze data, and especially smooth pursuit eye movements, we are able to track cells in time series of 3D volumetric datasets. The problem of tracking cells is ubiquitous in developmental biology, where large volumetric microscopy datasets are acquired on a daily basis, often comprising hundreds or thousands of time points that span hours or days. The image data, however, is only a means to an end, and scientists are often interested in the reconstruction of cell trajectories and cell lineage trees. Reliably tracking cells in crowded three-dimensional space over many time points remains an open problem, and many current approaches rely on tedious manual annotation or curation. In the Bionic Tracking approach, we substitute the usual 2D point-and-click interface for annotation or curation with eye tracking in a virtual reality headset, where users follow cells with their eyes in 3D space in order to track them. We detail the interaction design of our approach and explain the graph-based algorithm used to connect different time points, also taking occlusion and user distraction into account. We demonstrate Bionic Tracking using examples from two different biological datasets. Finally, we report on a user study with seven cell tracking experts, highlighting the benefits and limitations of Bionic Tracking compared to point-and-click interfaces. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-66415-2_34,2020,"Gudi A., Li X., van Gemert J.",Efficiency in Real-Time Webcam Tracking,"Efficiency and ease of use are essential for practical applications of camera based eye/gaze-tracking. Gaze tracking involves estimating where a person is looking on a screen based on face images from a computer-facing camera. In this paper we investigate two complementary forms of efficiency in gaze tracking: 1. The computational efficiency of the system which is dominated by the inference speed of a CNN predicting gaze-vectors; 2. The usability efficiency which is determined by the tediousness of the mandatory calibration of the gaze-vector to a computer screen. To do so, we evaluate the computational speed/accuracy trade-off for the CNN and the calibration effort/accuracy trade-off for screen calibration. For the CNN, we evaluate the full face, two-eyes, and single eye input. For screen calibration, we measure the number of calibration points needed and evaluate three types of calibration: 1. pure geometry, 2. pure machine learning, and 3. hybrid geometric regression. Results suggest that a single eye input and geometric regression calibration achieve the best trade-off. Â© 2020, Springer Nature Switzerland AG.",,,
10.2352/J.IMAGINGSCI.TECHNOL.2020.64.6.060407,2020,"Lee S., Park J., Nam D.",Crosstalk minimization method for eye-tracking-based 3D display,"In this article, the authors present an image processing method to reduce three-dimensional (3D) crosstalk for eye-tracking-based 3D display. Specifically, they considered 3D pixel crosstalk and offset crosstalk and applied different approaches based on its characteristics. For 3D pixel crosstalk which depends on the viewerâ€™s relative location, they proposed output pixel value weighting scheme based on viewerâ€™s eye position, and for offset crosstalk they subtracted luminance of crosstalk components according to the measured display crosstalk level in advance. By simulations and experiments using the 3D display prototypes, the authors evaluated the effectiveness of proposed method. c 2020 Society for Imaging Science and Technology. c Society for Imaging Science and Technology 2020",,,
10.1109/IV47402.2020.9304770,2020,"Kim H., Martin S., Tawari A., Misu T., Gabbard J.L.",Toward Real-Time Estimation of Driver Situation Awareness: An Eye-tracking Approach based on Moving Objects of Interest,"Eye-tracking techniques have the potential for estimating driver awareness of road hazards. However, traditional eye-movement measures based on static areas of interest may not capture the unique characteristics of driver eyeglance behavior and challenge the real-time application of the technology on the road. This article proposes a novel method to operationalize driver eye-movement data analysis based on moving objects of interest. A human-subject experiment conducted in a driving simulator demonstrated the potential of the proposed method. Correlation and regression analyses between indirect (i.e., eye-tracking) and direct measures of driver awareness identified some promising variables that feature both spatial and temporal aspects of driver eye-glance behavior relative to objects of interest. Results also suggest that eye-glance behavior might be a promising but insufficient predictor of driver awareness. This work is a preliminary step toward real-time, on-road estimation of driver awareness of road hazards. The proposed method could be further combined with computer-vision techniques such as object recognition to fully automate eye-movement data processing as well as machine learning approaches to improve the accuracy of driver awareness estimation. Â© 2020 IEEE.",,,
10.21437/Interspeech.2020-2074,2020,"Youssef H., Laurent P., Magalie O., Thierry C.",Identifying causal relationships between behavior and local brain activity during natural conversation,"Characterizing precisely neurophysiological activity involved in natural conversations remains a major challenge. We explore in this paper the relationship between multimodal conversational behavior and brain activity during natural conversations. This is challenging due to Functional Magnetic Resonance Imaging (fMRI) time resolution and to the diversity of the recorded multimodal signals. We use a unique corpus including localized brain activity and behavior recorded during a fMRI experiment when several participants had natural conversations alternatively with a human and a conversational robot. The corpus includes fMRI responses as well as conversational signals that consist of synchronized raw audio and their transcripts, video and eye-tracking recordings. The proposed approach includes a first step to extract discrete neurophysiological time-series from functionally well defined brain areas, as well as behavioral time-series describing specific behaviors. Then, machine learning models are applied to predict neurophysiological time-series based on the extracted behavioral features. The results show promising prediction scores, and specific causal relationships are found between behaviors and the activity in functional brain areas for both conditions, i.e., human-human and human-robot conversations. Copyright Â© 2020 ISCA",,,
10.1007/978-3-030-58529-7_3,2020,"Lee K.I., Jeon J.H., Song B.C.",Deep Learning-Based Pupil Center Detection for Fast and Accurate Eye Tracking System,"In augmented reality (AR) or virtual reality (VR) systems, eye tracking is a key technology and requires significant accuracy as well as real-time operation. Many techniques for detecting pupil centers with error range of iris radius have been developed, but few techniques have precise performance with error range of pupil radius. In addition, the conventional methods rarely guarantee real-time pupil center detection in a general-purpose computer environment due to high complexity. Thus, we propose more accurate pupil center detection by improving the representation quality of the network in charge of pupil center detection. This is realized by representation learning based on mutual information. Also, the latency of the entire system is greatly reduced by using non-local block and self-attention block with large receptive field, which makes it accomplish real-time operation. The proposed system not only shows real-time performance of 52 FPS in a general-purpose computer environment but also provides state-of-the-art accuracy in terms of fine level index of 96.71%,Â 99.84% and 96.38% for BioID, GI4E and Talking Face Video datasets, respectively. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-61705-9_12,2020,"de Lope J., GraÃ±a M.",Comparison of Labeling Methods for Behavioral Activity Classification Based on Gaze Ethograms,"The paper describes and compares several novel alternatives for labeling gaze ethograms data to estimate the activity that users carry out in front of computers with the use of the onboard camera. Gaze ethograms are basically discrete functions of time, therefore, the problem can be formulated by applying statistical and machine learning inspired methods to reduce the amount of information on a specific activity. To compare the proposed methods we carry out several experiments with experimental subjects in an office-like environment with no special lighting conditions. The result is a set of recommendations that allow to classify the activities with high precision. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-63119-2_16,2020,"Nguyen D.-L., Putro M.D., Jo K.-H.",Human Eye Detector with Light-Weight and Efficient Convolutional Neural Network,"The human eye detection plays an important role in computer vision. Along with face detection, it is widely applied in practical security, surveillance, and warning systems such as eye tracking system, eye disease detection, gaze detection, eye blink, and drowsiness detection system. There have been many studies to detect eyes from applying traditional methods to using modern methods based on machine learning and deep learning. This network is deployed with two main blocks, namely the feature extraction block and the detection block. The feature extraction block starts with the use of the convolution layers, C.ReLU (Concatenated Rectified Linear Unit) module, and max pooling layers alternately, followed by the last six inception modules and four convolution layers. The detection block is constructed by two sibling convolution layers using for classification and regression. The experiment was trained and tested on CEW (Closed Eyes In The Wild), BioID Face and GI4E (Gaze Interaction for Everybody) dataset with the results achieved 96.48%, 99.58%, and 75.52% of AP (Average Precision), respectively. The speed was tested in real-time by 37.65 fps (frames per second) on Intel Core I7-4770 CPU @ 3.40 GHz. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-62807-9_4,2020,"Cenzi E.D., Rudek M.",A Method to Gaze Following Detection by Computer Vision Applied to Production Environments,"The humans have the natural ability of following objects with the head and eyes and identify the relationship between those objects. This daily activity represents a challenge for computer vision systems. The procedure to identify the relationship between human eye gaze and the trackable objects is complex and demands several details. In this current paper we proposed a review of the main gazing following methods, identified the respective performance of them and also proposed an AI based method to estimate the gaze from 2D images based on head pose estimation. The main important details to be recovered from images are scene depth, head position and alignment and ocular rotation. In this approach we perform a track estimation of the gaze direction without the use of the eye position, and also, the face partial occlusion is considered in the analysis. The proposed approach allows low cost in processing with considerable accuracy at low complexity sceneries, because we donâ€™t need to extract the facial features. Gaze tracking is important to evaluate employeesâ€?attention to specific tasks in order to prevent accidents and improve work quality. The presented method improves the current knowing workflow by applying the head pose estimation instead face detection for training and inference. The promisors results are presented and open points are also discussed. Â© 2020, IFIP International Federation for Information Processing.",,,
,2020,"Cheng Y., Huang S., Wang F., Qian C., Lu F.",A Coarse-to-fine adaptive network for appearance-based gaze estimation,"Human gaze is essential for various appealing applications. Aiming at more accurate gaze estimation, a series of recent works propose to utilize face and eye images simultaneously. Nevertheless, face and eye images only serve as independent or parallel feature sources in those works, the intrinsic correlation between their features is overlooked. In this paper we make the following contributions: 1) We propose a coarseto- fine strategy which estimates a basic gaze direction from face image and refines it with corresponding residual predicted from eye images. 2) Guided by the proposed strategy, we design a framework which introduces a bi-gram model to bridge gaze residual and basic gaze direction, and an attention component to adaptively acquire suitable fine-grained feature. 3) Integrating the above innovations, we construct a coarse-to-fine adaptive network named CA-Net and achieve state-of-the-art performances on MPIIGaze and EyeDiap. Â© AAAI 2020 - 34th AAAI Conference on Artificial Intelligence. All Rights Reserved.",,,
10.1007/978-3-030-61255-9_27,2020,"Xi Z., Newton O., McGowin G., Sukthankar G., Fiore S., Oden K.",Predicting Student Flight Performance with Multimodal Features,"This paper investigates the problem of predicting student flight performance in a training simulation from multimodal features, including flight controls, visual attention, and knowledge acquisition tests. This is a key component of developing next generation training simulations that can optimize the usage of student training time. Two types of supervised machine learning classifiers (random forest and support vector machines) were trained to predict the performance of twenty-three students performing simple flight tasks in virtual reality. Our experiments reveal the following: 1) features derived from gaze tracking and knowledge acquisition tests can serve as an adequate substitute for flight control features; 2) data from the initial portion of the flight task is sufficient to predict the final outcome; 3) our classifiers perform slightly better at predicting student failure than success. These results indicate the feasibility of using machine learning for early prediction of student failures during flight training. Â© 2020, Springer Nature Switzerland AG.",,,
10.1016/j.procs.2020.09.041,2020,"Rakhmatulin I., Duchowski A.T.",Deep neural networks for low-cost eye tracking,"The paper presents a detailed analysis of modern techniques that can be used to track gaze with a webcam. We present a practical implementation of the most popular methods for tracking gaze. Various models of deep neural networks that can be involved in the process of online gaze monitoring are reviewed. We introduce a new eye-tracking approach where the effectiveness of using a deep learning method is significantly increased. Implementation is in Python where its application is demonstrated by controlling interaction with the computer. Specifically, a dual coordinate system is given for controlling the computer with the help of a gaze. The first set of coordinates-the position of the face relative to the computer, is implemented by detecting color from the infrared LED via the OpenCV library. The second set of coordinates-giving gaze position-is obtained via the YOLO (v3) package. A method of labeling the eyes is given, in which 3 objects are used to track gaze (to the left, to the right, and in the center). Â© 2020 The Authors. Published by Elsevier B.V.",,,
10.1016/j.procs.2020.09.245,2020,"Prasse P., JÃ¤ger L.A., Makowski S., Feuerpfeil M., Scheffer T.",On the relationship between eye tracking resolution and performance of oculomotoric biometric identification,"Distributional properties of fixations and saccades are known to constitute biometric characteristics. Additionally, high-frequency micro-movements of the eyes have recently been found to constitute biometric characteristics that allow for faster and more robust biometric identification than just macro-movements. Micro-movements of the eyes occur on scales that are very close to the precision of currently available eye trackers. This study therefore characterizes the relationship between the temporal and spatial resolution of eye tracking recordings on one hand and the performance of a biometric identification method that processes micro- and macro-movements via a deep convolutional network. We find that that the deteriorating effects of decreasing both, the temporal and spatial resolution are not cumulative. We observe that on low-resolution data, the network reaches performance levels above chance and outperforms statistical approaches. Â© 2020 The Authors. Published by Elsevier B.V.",,,
10.1007/978-3-030-58610-2_44,2020,"Park S., Aksan E., Zhang X., Hilliges O.",Towards End-to-End Video-Based Eye-Tracking,"Estimating eye-gaze from images alone is a challenging task, in large parts due to un-observable person-specific factors. Achieving high accuracy typically requires labeled data from test users which may not be attainable in real applications. We observe that there exists a strong relationship between what users are looking at and the appearance of the userâ€™s eyes. In response to this understanding, we propose a novel dataset and accompanying method which aims to explicitly learn these semantic and temporal relationships. Our video dataset consists of time-synchronized screen recordings, user-facing camera views, and eye gaze data, which allows for new benchmarks in temporal gaze tracking as well as label-free refinement of gaze. Importantly, we demonstrate that the fusion of information from visual stimuli as well as eye images can lead towards achieving performance similar to literature-reported figures acquired through supervised personalization. Our final method yields significant performance improvements on our proposed EVE dataset, with upÂ to 28 % improvement in Point-of-Gaze estimates (resulting in 2. 49âˆ?in angular error), paving the path towards high-accuracy screen-based eye tracking purely from webcam sensors. The dataset and reference source code are available at https://ait.ethz.ch/projects/2020/EVE. Â© 2020, Springer Nature Switzerland AG.",,,
10.1109/CVPR42600.2020.00734,2020,"Yu Y., Odobez J.-M.",Unsupervised representation learning for gaze estimation,"Although automatic gaze estimation is very important to a large variety of application areas, it is difficult to train accurate and robust gaze models, in great part due to the difficulty in collecting large and diverse data (annotating 3D gaze is expensive and existing datasets use different setups). To address this issue, our main contribution in this paper is to propose an effective approach to learn a low dimensional gaze representation without gaze annotations, which to the best of our best knowledge, is the first work to do so. The main idea is to rely on a gaze redirection network and use the gaze representation difference of the input and target images (of the redirection network) as the redirection variable. A redirection loss in image domain allows the joint training of both the redirection network and the gaze representation network. In addition, we propose a warping field regularization which not only provides an explicit physical meaning to the gaze representations but also avoids redirection distortions. Promising results on few-shot gaze estimation (competitive results can be achieved with as few as â‰?100 calibration samples), cross-dataset gaze estimation, gaze network pretraining, and another task (head pose estimation) demonstrate the validity of our framework. Â© 2020 IEEE",,,
10.1007/978-3-030-59716-0_42,2020,"You F., Khakhar R., Picht T., Dobbelstein D.",VR Simulation of Novel Hands-Free Interaction Concepts for Surgical Robotic Visualization Systems,"In microsurgery, visualization systems such as the traditional surgical microscope are essential, as surgeons rely on the highly magnified stereoscopic view for performing their operative tasks. For well-aligned visual perspectives onto the operating field during surgery, precise adjustments of the positioning of the system are frequently required. This, however, implies that the surgeon has to reach for the device and each time remove their hand(s) from the operating field, i.e. a disruptive event to the operative task at hand. To address this, we propose two novel hands-free interaction concepts based on head-, and gaze-tracking, that should allow surgeons to efficiently control the 6D positioning of a robotic visualization system with little interruptions to the main operative task. The new concepts were purely simulated in a virtual reality (VR) environment using a HTC Vive for a robotic visualization system. The two interaction concepts were evaluated within the virtual reality simulation in a quantitative user study with 11 neurosurgeons at the CharitÃ© hospital and compared to conventional interaction with a surgical microscope. After a brief introduction to the interaction concepts in the virtual reality simulation, neurosurgeons were 29% faster in reaching a set of virtual targets (position and orientation) in simulation as compared to reaching equivalent physical targets on a 3D-printed reference object. Â© 2020, Springer Nature Switzerland AG.",,,
10.1117/12.2555676,2020,"Leong-Hoi A., Murat S., Acker T., Sattler Q., Radoux J.-P.",Touchless interface interaction by hand tracking with a depth camera and a Convolutional Neural Network,"In operating rooms (OR), physicians have to work in strict compliance to asepsis rules to not endanger the health of patients. In the case of laparoscopic mini-invasive surgery, where the field of view of the surgeon is restricted, the use of computers is necessary to provide the missing information. Physicians must therefore interact with computers either by directly manipulating mouse and keyboard, by removing their gloves or by using a protection for the devices, or by voice-commanding an assistant to do so. However, in addition to be time-consuming, it may cause hygiene issues in the first case and a lack of precision in the second. The need to have better way of interactions with computers had led to important researches in that area during the last ten years, especially in Touchless Human Machine Interaction (THMI). Indeed, THMI, including gesture recognition, voice recognition and eye-tracking, has a promising future in the medical field, allowing surgeons to interact by themselves with devices, thereby avoiding error-prone process while complying to asepsis rules. In this context, the â€œIntelligent Touchless Glassless Human-Machine Interfaceâ€?(ITG-HMI) project aims to provide a new tool for viewing and manipulating 3D objects. In this article, we present how this interface was implemented, through the detection and recognition of hand gestures using Deep Learning, the establishment of a graphical interface to display 3D models and the adaptation of gestures recognized in actions to achieve. Â© 2020 SPIE",,,
10.1007/978-3-030-59716-0_51,2020,"Jiao J., Cai Y., Alsharid M., Drukker L., Papageorghiou A.T., Noble J.A.",Self-Supervised Contrastive Video-Speech Representation Learning for Ultrasound,"In medical imaging, manual annotations can be expensive to acquire and sometimes infeasible to access, making conventional deep learning-based models difficult to scale. As a result, it would be beneficial if useful representations could be derived from raw data without the need for manual annotations. In this paper, we propose to address the problem of self-supervised representation learning with multi-modal ultrasound video-speech raw data. For this case, we assume that there is a high correlation between the ultrasound video and the corresponding narrative speech audio of the sonographer. In order to learn meaningful representations, the model needs to identify such correlation and at the same time understand the underlying anatomical features. We designed a framework to model the correspondence between video and audio without any kind of human annotations. Within this framework, we introduce cross-modal contrastive learning and an affinity-aware self-paced learning scheme to enhance correlation modelling. Experimental evaluations on multi-modal fetal ultrasound video and audio show that the proposed approach is able to learn strong representations and transfers well to downstream tasks of standard plane detection and eye-gaze prediction. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-58820-5_58,2020,"Freitas L.F.S., Ancioto A.S.R., de FÃ¡tima Rodrigues GuimarÃ£es R., Martins V.F., Dias D.R.C., de Paiva GuimarÃ£es M.",A Virtual Reality Simulator to Assist in Memory Management Lectures,"Virtual reality technology can assist the teaching-learning process via concrete concepts that follow a sequence of steps to accomplish a task in a three-dimensional space, as proven in the literature (e.g., operating a vehicle or disaster relief simulation). However, it is not clear whether virtual reality can also enhance situations involving concepts that are not inherently related to a three-dimensional space, such as the execution of a computer algorithm. This paper presents an immersive and interactive virtual reality simulator that can aid in the teaching of the memory management functionality of operating systems, including single, fixed and dynamic contiguous techniques, and the non-contiguous technique of paging. Learners are immersed inside a computer motherboard to learn memory management functionality of operating systems. They use a head-mounted display such as 3D virtual reality headsets and can interact with the environment using eye-gaze tracking and a joystick. We also present a case study in which 80 students were divided into two groups to evaluate the simulator. Our results indicate that using our simulator is a more effective approach for teaching memory management concepts than expositive classes. The functionality and usability tests results highlighted the positive aspects of the simulator and improvements that could be made. Â© 2020, Springer Nature Switzerland AG.",,,
10.1117/12.2568883,2020,"Jin Y., Chen M., Bell T.G., Wan Z., Bovik A.",Study of 2D foveated video quality in virtual reality,"In Virtual Reality (VR), the necessity of immersive videos leads to greater challenges in compression and communication owing to the much higher spatial resolution, rapid, and the often real-time changes in viewing direction. Foveation in displays exploits the space-variant density of the retinal photoreceptors, which decreases exponentially with increasing eccentricity, to reduce the amount of data from the visual periphery. Foveated compression is gaining relevance and popularity for Virtual Reality. Likewise, being able to predict the quality of displayed foveated and compressed content has become more important. Towards advancing the development of objective quality assessment algorithms for foveated and compressed measurements of VR video contents, we built a new VR database of foveated/compressed videos, and conducted a human study of perceptual quality on it. A foveated video player having low motion-to-photon latency (50ms) was designed to meet the requirements of smooth playback, while an eye tracker was deployed to provide gaze direction in real time. We generated 180 distorted videos from 10 pristine 8K videos (30fps) having varying levels and combinations of foveation and compression distortions. These contents were viewed and quality-rated by 36 subjects in a controlled VR setting. Both the subject ratings and the eye tracking data are being made available along with the rest of the database. Â© 2020 SPIE",,,
10.20965/JACIII.2020.P0676,2020,"Kanda D., Kawai S., Nobuhara H.",Visualization method corresponding to regression problems and its application to deep learning-based gaze estimation model,"The human gaze contains substantial personal information and can be extensively employed in several applications if its relevant factors can be accurately measured. Further, several fields could be substantially innovated if the gaze could be analyzed using popular and familiar smart devices. Deep learning-based methods are robust, making them crucial for gaze estimation on smart devices. However, because internal functions in deep learning are black boxes, deep learning systems often make estimations for unclear reasons. In this paper, we propose a visualization method corresponding to a regression problem to solve the black box problem of the deep learning-based gaze estimation model. The proposed visualization method can clarify which region of an image contributes to deep learning-based gaze estimation. We visualized the gaze estimation model proposed by a research group at the Massachusetts Institute of Technology. The accuracy of the estimation was low, even when the facial features important for gaze estimation were recognized correctly. The effectiveness of the proposed method was further determined through quantitative evaluation using the area over the MoRF perturbation curve (AOPC). Â© 2020 Fuji Technology Press. All rights reserved.",,,
10.1145/3405962.3405966,2020,"Fu B., Steichen B., McBride A.",Tumbling to Succeed: A Predictive Analysis of User Success in Interactive Ontology Visualization,"Ontology visualization is an important component in the support of human-ontology interaction, as it amplifies cognition and offloads cognitive efforts to the human perceptual system. While a significant amount of research efforts has focused on designing and developing various visual layouts and improve performance of large-scale visualizations, the differences in user preferences and cognitive abilities have been largely overlooked. This provides an opportunity to investigate ways to potentially provide more personalized visual support in human-ontology interaction. To this end, this paper demonstrates successful predictions on an individual user's likelihood to succeed in a given task, based on this person's gaze data collected during interaction. Specifically, we show several statistically significant predictions against a baseline classifier when inferring users' success before a given task is actually completed. Moreover, we present results showing that accurate predictions of user success can be achieved early on during user interaction, such as after just a few minutes in some cases. These findings suggest there are ample opportunities throughout various stages of human-ontology interaction where the underlying visual system may adapt in real time to the user's visual needs to provide the most appropriate visualization with the overall goal of possibly increasing user success in a given task. Â© 2020 ACM.",,,
10.1007/978-3-030-58465-8_5,2020,"George C., Buschek D., Ngao A., Khamis M.",Gazeroomlock: Using gaze and head-pose to improve the usability and observation resistance of 3d passwords in virtual reality,"Authentication has become an important component of Immersive Virtual Reality (IVR) applications, such as virtual shopping stores, social networks, and games. Recent work showed that compared to traditional graphical and alphanumeric passwords, a more promising form of passwords for IVR is 3D passwords. This work evaluates four multimodal techniques for entering 3D passwords in IVR that consist of multiple virtual objects selected in succession. Namely, we compare eye gaze and head pose for pointing, and dwell time and tactile input for selection. A comparison of a) usability in terms of entry time, error rate, and memorability, and b) resistance to real world and offline observations, reveals that: multimodal authentication in IVR by pointing at targets using gaze, and selecting them using a handheld controller significantly improves usability and security compared to the other methods and to prior work. We discuss how the choice of pointing and selection methods impacts the usability and security of 3D passwords in IVR. Â© Springer Nature Switzerland AG 2020.",,,
10.1109/ACCESS.2020.3017680,2020,"Mao Y., He Y., Liu L., Chen X.",Disease Classification Based on Synthesis of Multiple Long Short-Term Memory Classifiers Corresponding to Eye Movement Features,"Medical research confirms that eye movement abnormalities are related to a variety of psychological activities, mental disorders and physical diseases. However, as the specific manifestations of various diseases in terms of eye movement disorders remain unclear, the accurate diagnosis of diseases according to eye movement is difficult. In this paper, a deep neural network (DNN) method is employed to establish a disease discrimination model according to eye movement. First, multiple eye-tracking experiments are designed to obtain eye images. Second, pupil characteristics, including position and size, are extracted, and the feature vectors of eye movement are obtained from the normalized pupil information. Based on a long short-term memory (LSTM) network, a classifier that corresponds to each feature, which is referred to as a weak classifier, is built. The experimental samples are preclassified, and the classification ability of each weak classifier for different diseases is also calculated. Last, a strong classifier is achieved for disease discrimination by synthesizing all the weak classifiers and their classification abilities. By classification testing for three categories of healthy controls, brain injury patients and vertigo patients, the experimental results demonstrated the efficiency of this method. With the deep learning method, more medical information can be excavated from eye movement to improve the values in disease diagnosis. Â© 2013 IEEE.",,,
,2020,"KÃ¼chemann S., Klein P., Becker S., Kumari N., Kuhn J.",Classification of students' conceptual understanding in STEM education using their visual attention distributions: A comparison of three machine-learning approaches,"Line-Graphs play a central role in STEM education, for instance, for the instruction of mathematical concepts or for analyzing measurement data. Consequently, they have been studied intensively in the past years. However, despite this wide and frequent use, little is known about students' visual strategy when solving line-graph problems. In this work, we study two example line-graph problems addressing the slope and the area concept, and apply three supervised machine-learning approaches to classify the students performance using visual attention distributions measured via remote eye tracking. The results show the dominance of a large-margin classifier at small training data sets above random decision forests and a feed-forward artificial neural network. However, we observe a sensitivity of the large-margin classifier towards the discriminatory power of used features which provides a guide for a selection of machine learning algorithms for the optimization of adaptive learning environments. Copyright Â© 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved",,,
,2020,"Lediaeva I., LaViola J.J., Jr.",Evaluation of Body-referenced graphical menus in virtual environments,"Graphical menus have been extensively used in desktop applications and widely adopted and integrated into virtual environments (VEs). However, while desktop menus are well evaluated and established, adopted 2D menus in VEs are still lacking a thorough evaluation. In this paper, we present the results of a comprehensive study on body-referenced graphical menus in a virtual environment. We compare menu placements (spatial, arm, hand, and waist) in conjunction with various shapes (linear and radial) and selection techniques (ray-casting with a controller device, head, and eye gaze). We examine task completion time, error rates, number of target re-entries, and user preference for each condition and provide design recommendations for spatial, arm, hand, and waist graphical menus. Our results indicate that the spatial, hand, and waist menus are significantly faster than the arm menus, and the eye gaze selection technique is more prone to errors and has a significantly higher number of target re-entries than the other selection techniques. Additionally, we found that a significantly higher number of participants ranked the spatial graphical menus as their favorite menu placement and the arm menu as their least favorite one. Â© 2020 Canadian Information Processing Society. All rights reserved.",,,
10.24132/CSRN.2020.3001.18,2020,"Garro V., Sundstedt V.",Pose and visual attention: Exploring the effects of 3D shape near-isometric deformations on gaze,"Recent research in 3D shape analysis focuses on the study of visual attention on rendered 3D shapes investigating the impact of different factors such as material, illumination, and camera movements. In this paper, we analyze how the pose of a deformable shape affects visual attention. We describe an eye-tracking experiment that studied the influence of different poses of non-rigid 3D shapes on visual attention. The subjects free-viewed a set of 3D shapes rendered in different poses and from different camera views. The fixation maps obtained by the aggregated gaze data were projected onto the 3D shapes and compared at vertex level. The results indicate an impact of the pose for some of the tested shapes and also that view variation influences visual attention. The qualitative analysis of the 3D fixation maps shows high visual focus on the facial regions regardless of the pose, coherent with previous works. The visual attention variation between poses appears to correspond to geometric salient features and semantically salient parts linked to the action represented by the pose. Â© 2020, Vaclav Skala Union Agency. All rights reserved.",,,
10.1109/ACCESS.2020.3013540,2020,"Akinyelu A.A., Blignaut P.",Convolutional Neural Network-Based Methods for Eye Gaze Estimation: A Survey,"Eye tracking is becoming a very important tool across many domains, including human-computer-interaction, psychology, computer vision, and medical diagnosis. Different methods have been used to tackle eye tracking, however, some of them are inaccurate under real-world conditions, while some require explicit user calibration which can be burdensome. Some of these methods suffer from poor image quality and variable light conditions. The recent success and prevalence of deep learning have greatly improved the performance of eye-tracking. The availability of large-scale datasets has further improved the performance of deep learning-based methods. This article presents a survey of the current state-of-the-art on deep learning-based gaze estimation techniques, with a focus on Convolutional Neural Networks (CNN). This article also provides a survey on other machine learning-based gaze estimation techniques. This study aims to empower the research community with valuable and useful insights that can enhance the design and development of improved and efficient deep learning-based eye-tracking models. This study also provides information on various pre-trained models, network architectures, and open-source datasets that are useful for training deep learning models. Â© 2013 IEEE.",,,
10.1080/21681163.2020.1799867,2020,"Ben Slama A., Sahli H., Mouelhi A., Marrakchi J., Sayadi M., Trabelsi H.",DBN-DNN: discrimination and classification of VNG sequence using deep neural network framework in the EMD domain,"The Vestibulo-ocular response VOR is characterized by a smooth pursuit eye movements in one direction, calledÂ slow phase of ocular nystagmus, interrupted by resetting saccades fast phase of nystagmus in the other direction.Â Recording of ocular nystagmus during vestibular tests does not quantify the true response of the vestibulo-ocular reflex (VOR). In order to extract the real VOR, our study is focused on nystagmus analysis using videonystagmography (VNG) technique based on measuring amplitude vibration of eyeball movement. The effectiveness of this attendance is severely topic to the attention and the experience of ENT doctors. In this case, automatic methods of image analysis offer the possibility of obtaining a homogeneous, objective and above all fast diagnosis of vestibular disorder. Â In this paper, a fully automatic system based on nystagmus parameter analysis using a pupil detection algorithm is proposed. After a segmentation stage, a deep neural Network based classification method is applied on 90 eye movement recordings from videonystagmography (VNG) containing two types of peripheral vestibular disorders and normal patients. Experimental results obtained after several simulation, show the efficiency of the proposed methodology when compared with other classification methods. Â© 2020 Informa UK Limited, trading as Taylor & Francis Group.",,,
10.1007/978-3-030-50729-9_9,2020,"Khatri J., Moghaddasi M., Llanes-Jurado J., Spinella L., MarÃ­n-Morales J., Guixeres J., AlcaÃ±iz M.",Optimizing virtual reality eye tracking fixation algorithm thresholds based on shopper behavior and age,"Eye tracking (ET) is becoming a popular tool to study the consumer behavior. One of the significant problems that arise with ET integrated into 3D virtual reality is defining fixations and saccades, which are essential part of feature extraction in ET analysis and have a critical impact on higher level analysis. In this study, the ET data from 60 subjects, were recorded. To define the fixations, Dispersion Threshold Identification algorithm was used which requires to define several thresholds. Since there are multiple thresholds and extracted features, a Multi-Objective Reinforcement Learning (MORL) algorithm was implemented to solve this problem. The objective of the study was to optimize these thresholds in order to improve accuracy of classification of the age based on different visual patterns undertaken by the subject during shopping in a virtual store. Regarding the nature of the classification, the objective for this optimization problem was to maximize the differences between the averages of each feature in different classes and minimize the variances of the same feature within each class. For the current study, thresholds optimization has shown an improvement in results for the accuracies of classification between age groups after applying the MORL algorithm. In conclusion, the results suggest that the optimization of thresholds is an important factor to improve feature extraction methods and in turn improve the overall results of an ET study involving consumer behavior inside virtual reality. This method can be used to optimize thresholds in similar studies to provide improved accuracy of classification results. Â© Springer Nature Switzerland AG 2020.",,,
,2020,"van der Graaf J., Molenaar I., Lim L., Fan Y., Engelmann K., GaÅ¡eviÄ‡ D., Bannert M.",Facilitating self-regulated learning with personalized scaffolds on studentâ€™s own regulation activities,"The focus of education is increasingly set on studentsâ€?ability to regulate their own learning within technology-enhanced learning environments. Scaffolds have been used to foster self-regulated learning, but scaffolds often are standardized and do not do not adapt to the individual learning process. Learning analytics and machine learning offer an approach to better understand SRL-processes during learning. Yet, current approaches lack validity or require extensive analysis after the learning process. The FLORA project aims to investigate how to advance support given to students by i) improving unobtrusive data collection and machine learning techniques to gain better measurement and understanding of SRL-processes and ii) using these new insights to facilitate studentâ€™s SRL by providing personalized scaffolds. We will reach this goal by investigating and improving trace data in exploratory studies (exploratory study1 and study 2) and using the insight gained from these studies to develop and test personalized scaffolds based on individual learning processes in laboratory (experimental study 3 and study 4) and a subsequent field study (field study 5). At the moment study 2 is ongoing. The setup consists of a learning environment presented on a computer with a screen-based eye-tracker. Other data sources are log files and audio of studentsâ€?think aloud. The analysis will focus on detecting sequences that are indicative of micro-level self-regulated learning processes and aligning them between the different data sources. Copyright Â© 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International 1 (CC BY 4.0).",,,
10.1007/978-3-030-50506-6_12,2020,"Hasegawa S., Hirako A., Zheng X., Karimah S.N., Ota K., Unoki T.",Learnerâ€™s mental state estimation with pc built-in camera,"The purpose of this research is to estimate learnersâ€?mental states such as difficulty, interest, fatigue, and concentration that change with the time series between learners and their learning tasks. Nowadays, we have many opportunities to learn specific topics in the individual learning process, such as active learning and self-directed learning. In such situations, it is challenging to grasp learnersâ€?progress and engagement in their learning process. Several studies have estimated learnersâ€?engagement from facial images/videos in the learning process. However, there is no extensive benchmark dataset except for the video watching process. Therefore, we gathered learnersâ€?videos with facial expression and retrospective self-report from 19 participants through the CAB test process using a PC built-in camera. In this research, we applied an existing face image recognition library Face++ to extract the data such as estimated emotion, eye gaze, face orientation, face position (percentage on the screen) by each frame of the videos. Then, we built a couple of machine learning models, including deep learning methods, to estimate their mental states from the facial expressions and compared them with the average accuracy of prediction. The results demonstrated the potential of the proposed method to the estimation and provided the improvement plan from the accuracy point of view. Â© Springer Nature Switzerland AG 2020.",,,
10.1007/978-3-030-49065-2_35,2020,"Monaro M., Capuozzo P., Ragucci F., Maffei A., Curci A., Scarpazza C., Angrilli A., Sartori G.",Using blink rate to detect deception: A study to validate an automatic blink detector and a new dataset of videos from liars and truth-tellers,"Eye-blink is a sensitive index of cognitive load and some studies have reported that it can be a useful cue for detecting deception. However, it is difficult to apply in the real forensic scenario as very complex techniques to record eye blinking are usually needed (e.g., electrooculography, eye tracker technology). In this paper, we propose a new approach to automatically detect eye blinking based on a computer vision algorithm, which does not require any expensive technology to record data. Results demonstrated that the automatic blink detector reached an accuracy similar to the electrooculogram in detecting the blink rate. Moreover, the automatic blink detector was applied to 68 videos of people who were lying or telling the truth about a past holiday, testing the difference between the two groups in terms of blink rate and response timing. Training machine learning classification models on these features, an accuracy up to 70% in identifying liars and truth-tellers was obtained. Â© Springer Nature Switzerland AG 2020.",,,
10.1007/978-3-030-49282-3_10,2020,"Larradet F., Barresi G., Mattos L.S.",Affective communication enhancement system for locked-in syndrome patients,"Patients with Locked-In Syndrome such as people with Amyotrophic Lateral Sclerosis (ALS) rely on technology for basic communication. However, available Augmentative and Alternative Communication (AAC) tools such as gaze-controlled keyboards have limited abilities. In particular, they do not allow for expression of emotions in addition to words. In this paper we propose a novel gaze-based speaking tool that enable locked-in syndrome patients to express emotions as well as sentences. It also features patient-controlled emotionally modulated speech synthesis. Additionally, an emotional 3D avatar can be controlled by the patient to represent emotional facial-expressions. The systems were tested with 36 people without disabilities separated into an affective group - full control of emotional voice, avatar facial expressions and laugh - and a control group - no emotional tools. The study proved the systemâ€™s capacity to enhance communication for both the patient and the interlocutor. The emotions embedded in the synthesized voices were found recognizable at 80% on the first trial and 90% on the second trial. The conversation was perceived as more natural when using the affective tool. The subjects felt it was easier to express and identify emotions using this system. The emotional voice and the emotional avatar were found to help the conversation. This highlights the needs for more affective-driven communicative solutions for locked-in patients. Â© Springer Nature Switzerland AG 2020.",,,
10.1007/978-3-030-49907-5_22,2020,"BabicsnÃ©-HorvÃ¡th M., Hercegfi K.",The difficulties in usability testing of 3-dimensional software applying eye-tracking methodology â€?Presented via two case studies of evaluation of digital human modelling software,"Eye-tracking based usability testing methods today are very accepted by researchers. These methods are ones of the most commons in human-computer interaction. There are various types of applications of these methods in software or web usability area, however, there is a difficulty during usability tests with the 3D environment. The problem is occurred when the participant wants to rotate, zoom or move the 3D space. In these cases, the gaze plots, the heatmaps, or the statistics of Area of Interests (AOI) cannot be used regarding the 3D workspace. The data on the menu bar is interpretable, however, on the 3D environment hardly or not at all. In our research, we tested ViveLab and Jack Digital Human Modelling (DHM) software knowing the mentioned problem. Our goal was dual. Firstly, with this usability tests, we wanted to detect the issues in the software. Secondly, we tested the utility of a new methodology which was included the tests. At one point of the usability test, the participants was asked not to move the 3D space, while they had to perform the given tasks. Several methods were used to locate the usability problems of the software. During the tests, we applied eye-tracking method, and after that, each participant was interviewed. Based on the experiences of this research, we can advise future researchers testing similar products. This methodology is useful, and applicable in other related usability tests, and its visualisation techniques for one or more participants are interpretable. Â© Springer Nature Switzerland AG 2020.",,,
10.1007/978-3-030-49062-1_18,2020,"Wang C., Biancardi B., Mancini M., Cafaro A., Pelachaud C., Pun T., Chanel G.",Impression Detection and Management Using an Embodied Conversational Agent,"Embodied Conversational Agents (ECAs) are a promising medium for human-computer interaction, since they are capable of engaging users in real-time face-to-face interaction [1, 2]. Usersâ€?formed impressions of an ECA (e.g. favour or dislike) could be reflected behaviourally [3, 4]. These impressions may affect the interaction and could even remain afterwards [5, 7]. Thus, when we build an ECA to impress users, it is important to detect how users feel about the ECA. The impression the ECA leaves can then be adjusted by controlling its non-verbal behaviour [7]. Motivated by the role of ECAs in interpersonal interaction and the state-of-the-art on affect recognition, we investigated three research questions: 1) which modality (facial expressions, eye movements, and physiological signals) reveals most of the formed impressions; 2) whether an ECA could leave a better impression by maximizing the impression it produces; 3) whether there are differences in impression formation during human-human vs. human-agent interaction. Our results firstly showed the interest to use different modalities to detect impressions. An ANOVA test indicated that facial expressions performance outperforms the physiological modality performance (M = 1.27, p = 0.02). Secondly, our results presented the possibility of creating an adaptive ECA. Compared with the randomly selected ECA behaviour, participantsâ€?ratings tended to be higher in the conditions where the ECA adapted its behaviour based on the detected impressions. Thirdly, we found similar behaviour during human-human vs. human-agent interaction. People treated an ECA similarly to a human by spending more time observing the face area when forming an impression. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-51828-8_88,2020,"Suk H.J., Kim S.J.",The effect of ocular dominance on decision making in a virtual environment,"Human laterality is the preference of an individual to use one side of their body rather than the other. Ocular dominance, also known as eye dominance, is one type of laterality in that eye dominance affects human visual perception. This study explores how ocular human laterality is associated with decision making in determining one choice from the left or right and top or bottom. Two simple tasks are developed running in a virtual environment in which a user wears a head-mounted display and observes two 3D ball shape objects that one moves to the left and the other moves to the right side of their eyes. The user performs the same task as the direction of the ball movement is changed to up and down. An eye-tracking device is used to measure the usersâ€?decision on the two tasks by tracking down their eye movements. A user study conducted with a total of 20 college students, ages ranged from 18 to 30 years old, revealed that there was a significant relationship between eye dominance and their decision making. Â© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2020.",,,
10.1007/978-3-030-50732-9_19,2020,"Almenara C.A., AimÃ© A., MaÃ¯ano C.",Effect of Online Weight Loss Advertising in Young Women with Body Dissatisfaction: An Experimental Protocol Using Eye-Tracking and Facial Electromyography,"The weight loss industry is projected to reach USD$ 278.95 billion worldwide by 2023. Weight loss companies devote a large part of their budget for advertising their products. Unfortunately, as revealed by the Federal Trade Commission (FTC), there are many deceptive ads. The effect of weight loss advertising on consumerâ€™s diet and eating behavior is so large that it has been proposed a causal relationship between advertising and diet. Adolescents, women with appearance concerns, and obese people, are the most vulnerable consumers for this kind of advertising. Within the Internet, most weight loss products are advertised under algorithmic rules. This algorithmic regulation refers to the online advertising being established by a series of rules (i.e., algorithms). These algorithms collect information about our online identity and behavior (e.g., sociodemographic characteristics, online searches we do, online content we download, â€œlikedâ€?content, etc.), to personalize the content displayed while we browse the Internet. Because of it, this algorithmic regulation has been described as a â€œfilter bubbleâ€? because most content we see on the Internet is reflecting our idiosyncratic interests, desires, and needs. Following this paradigm, this study presents a research protocol to experimentally examine the effect of online weight loss advertising in the attention (using eye-tracking) and physiological response (using facial electromyography) of women with different levels of body dissatisfaction. The protocol describes the methodology for: participantsâ€?recruitment; collecting weight loss ads; and the experimental study, which includes the stimuli (ads) and the responses (eye fixations and facial muscles activity). Â© 2020, Springer Nature Switzerland AG.",,,
10.1109/ACCESS.2020.3006700,2020,"Bruno A., Gugliuzza F., Pirrone R., Ardizzone E.",A Multi-Scale Colour and Keypoint Density-Based Approach for Visual Saliency Detection,"In the first seconds of observation of an image, several visual attention processes are involved in the identification of the visual targets that pop-out from the scene to our eyes. Saliency is the quality that makes certain regions of an image stand out from the visual field and grab our attention. Saliency detection models, inspired by visual cortex mechanisms, employ both colour and luminance features. Furthermore, both locations of pixels and presence of objects influence the Visual Attention processes. In this paper, we propose a new saliency method based on the combination of the distribution of interest points in the image with multiscale analysis, a centre bias module and a machine learning approach. We use perceptually uniform colour spaces to study how colour impacts on the extraction of saliency. To investigate eye-movements and assess the performances of saliency methods over object-based images, we conduct experimental sessions on our dataset ETTO (Eye Tracking Through Objects). Experiments show our approach to be accurate in the detection of saliency concerning state-of-the-art methods and accessible eye-movement datasets. The performances over object-based images are excellent and consistent on generic pictures. Besides, our work reveals interesting findings on some relationships between saliency and perceptually uniform colour spaces. Â© 2013 IEEE.",,,
10.1016/j.promfg.2020.02.211,2020,"Christofi M., Katsaros M., Kotsopoulos S.D.",Form Follows Brain Function: A Computational Mapping Approach,"The strong association between computation and the built environment gives birth to new research approaches to urban design. Although these approaches enable the implementation of urban systems based on various collections of datasets, they usually neglect human experience. To address this problem, our research identifies and quantifies spatial information and patterns of urban organization based on experience, by gathering two parallel types of user data: sensory and recollection. The presented experiment, executed in Copenhagen, Denmark with 23 participants, involves a walking task, and a recollection task. The walking task is 1 km walk from the Copenhagen Court House building to the Royal Danish Theatre through StrÃ¸get Street, while wearing eye-tracking spectacles. The recollection task includes the underlining - on a 3D photorealistic digital model - of the elements that the participants can recall from their walk. We monitor the eye movement of the participants to locally characterize the urban path by computing four spatial attributes: a) The fixations on elements (counted through iMotions Software), b) The distance from the viewer (taken by GPS points through FUMapp software), c) The position of the stimuli related to the eye-level (calculated in FUMapp), and d) The degree of spatial transformations on street intersections (through spatial variables in FUMapp). Then we compare the eye-tracking and the recollection data and we evaluate their heat maps. The originality of this research is twofold: First, for the first time a substantial amount of quantifiable optical data related to urban walking is recorded; Second, for the first time this experiment is performed on the public street. The proposed method could offer a quantifiable basis for predicting eye fixation locations, determining human centric guidelines for urban design. Additional modes of body sensors could yield even more all-encompassing body-metrics related to the urban walking experience. Â© 2020 The Authors.",,,
10.1080/0144929X.2020.1784282,2020,"Hsieh A.-Y., Lo S.-K., Chiu Y.-P., Lie T.",Do not allow pop-up ads to appear too early internet usersâ€?browsing behaviour to pop-up ads,"This study examines the timing of pop-up advertising appearance and its effect on perceived intrusiveness, advertising irritation and advertising avoidance. Experiment was designed to build a virtual Internet environment (including the main content on the webpage and a pop-up ad) and to manipulate the timing of the pop-up advertising appearance. Participants were invited to participate in two experiments, and then assigned to a specific target browsing task; their advertising browsing activities during the task were measured. In order to measure their cognitive advertising avoidance, an eye-tracking device was utilised to gain objective and accurate psychological information. Results showed that earlier pop-up advertising appearances are associated with a lower consumer fixation count and fixation length; in contrast, pop-up advertising that appears later is associated with a higher fixation count and fixation length. This study attempts to gain more objective and accurate psychological data by using an eye-tracking device to collect information about eye movements associated with the appearance of pop-up advertising to better analyse consumer behaviours towards them. These results offer insights to Internet advertisers and Internet platform companies on how to provide more efficient Internet advertising. Â© 2020, Â© 2020 Informa UK Limited, trading as Taylor & Francis Group.",,,
10.1109/ACCESS.2020.2999633,2020,"Liu M., Li Y., Liu H.",3D Gaze Estimation for Head-Mounted Eye Tracking System with Auto-Calibration Method,"The general challenges of 3D gaze estimation for head-mounted eye tracking systems are inflexible marker-based calibration procedure and significant errors of depth estimation. In this paper, we propose a 3D gaze estimation with an auto-calibration method. To acquire the accurate 3D structure of the environment, an RGBD camera is applied as the scene camera of our system. By adopting the saliency detection method, saliency maps can be acquired through scene images, and 3D salient pixels in the scene are considered potential 3D calibration targets. The 3D eye model is built on the basis of eye images to determine gaze vectors. By combining 3D salient pixels and gaze vectors, the auto-calibration can be achieved with our calibration method. Finally, the 3D gaze point is obtained through the calibrated gaze vectors, and the point cloud is generated from the RGBD camera. The experimental result shows that the proposed system can achieve an average accuracy of 3.7Â° in the range of 1 m to 4 m indoors and 4.0Â° outdoors. The proposed system also presents a great improvement in depth measurement, which is sufficient for tracking users' visual attention in real scenes. Â© 2013 IEEE.",,,
10.1117/12.2566952,2020,"Oue S., Yamada R., Akamatsu S.",Human performance of face recognition inferred from characteristics of observing eye movement patterns learned by hidden Markov model,"We investigated the relationship between the face recognition performance of individuals and their eye movement characteristics that were measured while each subject observed the faces that were displayed on a screen. We formulated the statistical nature of their eye movements from a machine-learning perspective by applying a hidden Markov model (HMM). We used a set of computer-generated faces that included both the images of actual faces and synthetic images obtained by slightly transforming the impressions of the original faces. With these visual stimuli, we conducted a simple face recognition experiment, and subjects judged whether they had seen the faces before. We obtained a quantitative hit rate score for each stimulus and subject. We also tracked their eye movements and recorded as temporal chains their eye fixation points using an eye-tracking system. For each class of face stimulus and subject, we estimated the HMM parameters from the training samples of the eye movement. For the given eye movement data as test samples, we conducted a classification test among the pre-defined classes based on the differences of the log-likelihood values obtained from each HMM. Better discrimination of the subjects by the HMM-based classification of the eye movement data corresponded to lower face recognition scores by the subjects, suggesting that individually consistent eye movement patterns may lower the face recognition performance by humans. Â© 2020 SPIE.",,,
10.1287/ISRE.2019.0907,2020,"Pfeiffer J., Pfeiffer T., MeiÃŸner M., WeiÃŸ E.",Eye-tracking-based classification of information search behavior using machine learning: Evidence from experiments in physical shops and virtual reality shopping environments,"Classifying information search behavior helps tailor recommender systems to individual customers' shopping motives. But how can we identify these motives without requiring users to exert too much effort? Our research goal is to demonstrate that eye tracking can be used at the point of sale to do so. We focus on two frequently investigated shopping motives: Goal-directed and exploratory search. To train and test a prediction model, we conducted two eye-tracking experiments in front of supermarket shelves. The first experiment was carried out in immersive virtual reality; the second, in physical reality-in other words, as a field study in a real supermarket. We conducted a virtual reality study, because recently launched virtual shopping environments suggest that there is great interest in using this technology as a retail channel. Our empirical results show that support vector machines allow the correct classification of search motives with 80% accuracy in virtual reality and 85% accuracy in physical reality. Our findings also imply that eye movements allow shopping motives to be identified relatively early in the search process: Our models achieve 70% prediction accuracy after only 15 seconds in virtual reality and 75% in physical reality. Applying an ensemble method increases the prediction accuracy substantially, to about 90%. Consequently, the approach that we propose could be used for the satisfiable classification of consumers in practice. Furthermore, both environments' best predictor variables overlap substantially. This finding provides evidence that in virtual reality, information search behavior might be similar to the one used in physical reality. Finally, we also discuss managerial implications for retailers and companies that are planning to use our technology to personalize a consumer assistance system. Â© 2020 The Author(s).",,,
10.1007/978-3-030-47679-3_21,2020,"Kaczorowska M., Wawrzyk M., Plechawska-WÃ³jcik M.",Binary Classification of Cognitive Workload Levels with Oculography Features,"Assessment of cognitive workload level is important to understand human mental fatigue, especially in the case of performing intellectual tasks. The paper presents a case study on binary classification of cognitive workload levels. The dataset was received from two versions of the digit symbol substitution test (DSST), conducted on 26 healthy volunteers. A screen-based eye tracker was applied during an examination gathering oculographic data. DSST test results such as total number of matches and error ratio were also applied. Classification was performed with several different machine learning models. The best accuracy (97%) was achieved with linear SVM classifier. TheÂ final dataset for classification was based on nine features selected with theÂ Fisher score feature selection method. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-981-15-4301-2_6,2020,"Saravanakumar D., Ramasubba Reddy M.",A Brain Computer Interface Based Visual Keyboard System Using SSVEP and Electrooculogram,"This study aims to design a steady-state visual evoked potential (SSVEP) based, on-screen keyboard/speller system along with the integration of electrooculogram (EOG). The characters/targets were designed using the pattern reversal square checkerboard flickering visual stimuli. In this study, twenty-three characters were randomly selected and their corresponding visual stimuli were designed using five frequencies (6, 6.667, 7.5, 8.57 and 10Â Hz). The keyboard layout was divided into nine regions and each region was identified by using the subjectâ€™s eye gaze information with the help of EOG data. The information from the EOG was used to locate the area on the visual keyboard/display, where the subject is looking. The region identification helps to use the same frequency valued visual stimuli more than once on the keyboard layout. In this proposed study, more targets were designed using less number of visual stimulus frequencies by integrating EOG with the SSVEP keyboard system. The multi-threshold algorithm and extended multivariate synchronization index (EMSI) method were used for eye gaze detection and SSVEP frequency recognition respectively. Ten healthy subjects were recruited for validating the proposed visual keyboard system. Â© Springer Nature Singapore Pte Ltd. 2020.",,,
10.1109/ACCESS.2020.2990685,2020,"Zhou X., Jiang J., Liu Q., Fang J., Chen S., Cai H.",Learning a 3D Gaze Estimator with Adaptive Weighted Strategy,"As a method of predicting the target's attention distribution, gaze estimation plays an important role in human-computer interaction. In this paper, we learn a 3D gaze estimator with adaptive weighted strategy to get the mapping from the complete images to the gaze vector. We select the both eyes, the complete face and their fusion features as the input of the regression model of gaze estimator. Considering that the different areas of the face have different contributions on the results of gaze estimation under free head movement, we design a new learning strategy for the regression net. To improve the efficiency of the regression model to a great extent, we propose a weighted network that can adjust the learning strategy of the regression net adaptively. Experimental results conducted on the MPIIGaze and EyeDiap datasets demonstrate that our method can achieve superior performance compared with other state-of-the-art 3D gaze estimation methods. Â© 2013 IEEE.",,,
10.1007/978-3-030-45002-1_37,2020,"Pappas I.O., Sharma K., Mikalef P., Giannakos M.N.",How Quickly Can We Predict Usersâ€?Ratings on Aesthetic Evaluations of Websites? Employing Machine Learning on Eye-Tracking Data,"This study examines how quickly we can predict usersâ€?ratings on visual aesthetics in terms of simplicity, diversity, colorfulness, craftsmanship. To predict usersâ€?ratings, first we capture gaze behavior while looking at high, neutral, and low visually appealing websites, followed by a survey regarding user perceptions on visual aesthetics towards the same websites. We conduct an experiment with 23 experienced users in online shopping, capture gaze behavior and through employing machine learning we examine how fast we can accurately predict their ratings. The findings show that after 25Â s we can predict ratings with an error rate ranging from 9% to 11% depending on which facet of visual aesthetic is examined. Furthermore, within the first 15Â s we can have a good and sufficient prediction for simplicity and colorfulness, with error rates 11% and 12% respectively. For diversity and craftsmanship, 20Â s are needed to get a good and sufficient prediction similar to the one from 25Â s. The findings indicate that we need more than 10Â s of viewing time to be able to accurately capture perceptions on visual aesthetics. The study contributes by offering new ways for designing systems that will take into account usersâ€?gaze behavior in an unobtrusive manner and will be able inform researchers and designers about their perceptions of visual aesthetics. Â© 2020, IFIP International Federation for Information Processing.",,,
10.1007/978-981-15-3863-6_52,2020,"Xu B., Li X., Wang Y.",2D-3D Autostereoscopic Switchable Display Based on Multi-distance Dynamic Directional Backlight,"A 2D-3D autostereoscopic switchable display based on multi-distance dynamic directional backlight is introduced in this paper. The working principle and design process of the system are described in detail as well. Our prototype consists of a multi-distance dynamic directional backlight, LCD panel, and eye tracking system. The multi-distance dynamic directional backlight includes an LED array and corresponding driving circuit. The LEDs in the array are controlled in synchronization with the vertical synchronization signal of the LCD panel. The prototype can measure stereoacuity of the viewerâ€™s eyes in different viewing distance with full resolution, low crosstalk, and 2D-3D compatibility. Combined with mechanical structure and eye tracking technique, the system can measure stereoacuity at 0.4, 1.0, and 5.0Â m. Also, a multi-distance autostereoscopic display prototype is fabricated and demonstrated experimentally. It shows that the viewer can perceive high-quality 3D images in different distance. Without the auxiliary glasses, the crosstalk is about 5%. Our prototype is also compatible with active shutter glasses. In conjunction with active shutter glasses, the crosstalk is about 1%, just next to that of commercial display (View Sonic VX2268WM). Â© Springer Nature Singapore Pte Ltd. 2020.",,,
10.1007/978-981-15-3863-6_47,2020,"Li X.-L., Xu B., Wu Q.-Q., Wang Y.-Q.",Autostereoscopic 3D Display System Based on Lenticular Lens and Quantum-Dot Film,"An autostereoscopic 3D display system based on the lenticular lens and Quantum-Dot (QD) film is presented in this paper. Wide color gamut, multi-view, and high brightness are achieved in this system. In addition, it can be easily realized on flat-panel display, which is the mainstream of display nowadays. Multiple viewing positions are available and with the assistance of eye tracking device, one pair of parallax images can be projected to the viewerâ€™s left and right eyes, respectively. In order to value the performance of the backlight system, a prototype (10.1 inches) is fabricated and demonstrated experimentally. The structure of the system and working principle are explained in detail. The error of the exit pupils is analyzed. Compared with the traditional display platform, the color gamut of this system benefited from the use of QD material can be extended to 78%. The crosstalk of our prototype is about 7.7% and motion resolution is 30Â mm. Â© Springer Nature Singapore Pte Ltd. 2020.",,,
10.1007/978-3-030-46147-8_18,2020,"JÃ¤ger L.A., Makowski S., Prasse P., Liehr S., Seidler M., Scheffer T.",Deep Eyedentification: Biometric Identification Using Micro-movements of the Eye,"We study involuntary micro-movements of the eye for biometric identification. While prior studies extract lower-frequency macro-movements from the output of video-based eye-tracking systems and engineer explicit features of these macro-movements, we develop a deep convolutional architecture that processes the raw eye-tracking signal. Compared to prior work, the network attains a lower error rate by one order of magnitude and is faster by two orders of magnitude: it identifies users accurately within seconds. Â© Springer Nature Switzerland AG 2020.",,,
10.1007/978-3-030-46147-8_20,2020,"Abdelwahab A., Landwehr N.",Quantile Layers: Statistical Aggregation in Deep Neural Networks for Eye Movement Biometrics,"Human eye gaze patterns are highly individually characteristic. Gaze patterns observed during the routine access of a user to a device or document can therefore be used to identify subjects unobtrusively, that is, without the need to perform an explicit verification such as entering a password. Existing approaches to biometric identification from gaze patterns segment raw gaze data into short, local patterns called saccades and fixations. Subjects are then identified by characterizing the distribution of these patterns or deriving hand-crafted features for them. In this paper, we follow a different approach by training deep neural networks directly on the raw gaze data. As the distribution of short, local patterns has been shown to be particularly informative for distinguishing subjects, we introduce a parameterized and end-to-end learnable statistical aggregation layer called the quantile layer that enables the network to explicitly fit the distribution of filter activations in preceding layers. We empirically show that deep neural networks with quantile layers outperform existing probabilistic and feature-based methods for identifying subjects based on eye movements by a large margin. Â© Springer Nature Switzerland AG 2020.",,,
10.1109/ACCESS.2020.2986815,2020,"Ali A., Kim Y.-G.",Deep Fusion for 3D Gaze Estimation from Natural Face Images Using Multi-Stream CNNs,"Over the last few decades, eye gaze estimation techniques have been thoroughly investigated by many researchers. However, predicting a 3D gaze from a 2D natural image remains challenging because it has to deal with several issues such as diverse head positions, face shape transformation, illumination variations, and subject individuality. Many previous studies employ convolutional neural networks (CNNs) for this task, and yet the accuracy needs improvement for its practical use. In this paper, we propose a 3D gaze estimation framework based on the data science perspective: First, a novel neural network architecture is designed to exploit every possible visual attribute such as the states of both eyes and the head position, including several augmentations; secondly, the data fusion method is utilized by incorporating multiple gaze datasets. Extensive experiments were carried out using two standard eye gaze datasets, including comparative analysis. The experimental results suggest that our method outperforms state-of-the-art with 2.8 degrees for MPIIGaze and 3.05 degrees for EYEDIAP dataset, respectively, indicating that it has a potential for real applications. Â© 2013 IEEE.",,,
10.1109/ACCESS.2020.2986810,2020,"Gjoreski M., Gams M.Å½., LuÅ¡trek M., Genc P., Garbas J.-U., Hassan T.",Machine Learning and End-to-End Deep Learning for Monitoring Driver Distractions from Physiological and Visual Signals,"It is only a matter of time until autonomous vehicles become ubiquitous; however, human driving supervision will remain a necessity for decades. To assess the driver's ability to take control over the vehicle in critical scenarios, driver distractions can be monitored using wearable sensors or sensors that are embedded in the vehicle, such as video cameras. The types of driving distractions that can be sensed with various sensors is an open research question that this study attempts to answer. This study compared data from physiological sensors (palm electrodermal activity (pEDA), heart rate and breathing rate) and visual sensors (eye tracking, pupil diameter, nasal EDA (nEDA), emotional activation and facial action units (AUs)) for the detection of four types of distractions. The dataset was collected in a previous driving simulation study. The statistical tests showed that the most informative feature/modality for detecting driver distraction depends on the type of distraction, with emotional activation and AUs being the most promising. The experimental comparison of seven classical machine learning (ML) and seven end-to-end deep learning (DL) methods, which were evaluated on a separate test set of 10 subjects, showed that when classifying windows into distracted or not distracted, the highest F1-score of 79% was realized by the extreme gradient boosting (XGB) classifier using 60-second windows of AUs as input. When classifying complete driving sessions, XGB's F1-score was 94%. The best-performing DL model was a spectro-temporal ResNet, which realized an F1-score of 75% when classifying segments and an F1-score of 87% when classifying complete driving sessions. Finally, this study identified and discussed problems, such as label jitter, scenario overfitting and unsatisfactory generalization performance, that may adversely affect related ML approaches. Â© 2013 IEEE.",,,
10.1109/ACCESS.2020.2985095,2020,"Han S.Y., Kwon H.J., Kim Y., Cho N.I.",Noise-Robust Pupil Center Detection through CNN-Based Segmentation with Shape-Prior Loss,"Detecting the pupil center plays a key role in human-computer interaction, especially for gaze tracking. The conventional deep learning-based method for this problem is to train a convolutional neural network (CNN), which takes the eye image as the input and gives the pupil center as a regression result. In this paper, we propose an indirect use of the CNN for the task, which first segments the pupil region by a CNN as a classification problem, and then finds the center of the segmented region. This is based on the observation that CNN works more robustly for the pupil segmentation than for the pupil center-point regression when the inputs are noisy IR images. Specifically, we use the UNet model for the segmentation of pupil regions in IR images and then find the pupil center as the center of mass of the segment. In designing the loss function for the segmentation, we propose a new loss term that encodes the convex shape-prior for enhancing the robustness to noise. Precisely, we penalize not only the deviation of each predicted pixel from the ground truth label but also the non-convex shape of pupils caused by the noise and reflection. For the training, we make a new dataset of 111,581 images with hand-labeled pupil regions from 29 IR eye video sequences. We also label commonly used datasets (ExCuSe and ElSe dataset) that are considered real-world noisy ones to validate our method. Experiments show that the proposed method performs better than the conventional methods that directly find the pupil center as a regression result. Â© 2013 IEEE.",,,
,2020,"Selim M., Firintepe A., Pagani A., Stricker D.",Autopose: Large-scale automotive driver head pose and gaze dataset with deep head orientation baseline,"In computer vision research, public datasets are crucial to objectively assess new algorithms. By the wide use of deep learning methods to solve computer vision problems, large-scale datasets are indispensable for proper network training. Various driver-centered analysis depend on accurate head pose and gaze estimation. In this paper, we present a new large-scale dataset, AutoPOSE. The dataset provides âˆ?1.1M IR images taken from the dashboard view, and âˆ?315K from Kinect v2 (RGB, IR, Depth) taken from center mirror view. AutoPOSEâ€™s ground truth -head orientation and position- was acquired with a sub-millimeter accurate motion capturing system. Moreover, we present a head orientation estimation baseline with a state-of-the-art method on our AutoPOSE dataset. We provide the dataset as a downloadable package from a public website. Copyright Â© 2020 by SCITEPRESS â€?Science and Technology Publications, Lda. All rights reserved.",,,
,2020,"Dhingra N., Hirt C., Angst M., Kunz A.",Eye gaze tracking for detecting non-verbal communication in meeting environments,"Non-verbal communication in a team meeting is important to understand the essence of the conversation. Among other gestures, eye gaze shows the focus of interest on a common workspace and can also be used for an interpersonal synchronisation. If this non-verbal information is missing and or cannot be perceived by blind and visually impaired people (BVIP), they would lack important information to get fully immersed in the meeting and may feel alienated in the course of the discussion. Thus, this paper proposes an automatic system to track where a sighted person is gazing at. We use the open source software 'OpenFace' and develop it as an eye tracker by using a support vector regressor to make it work similarly to commercially available expensive eye trackers. We calibrate OpenFace using a desktop screen with a 2Ã—3 box matrix and conduct a user study with 28 users on a big screen (161.7 cm Ã— 99.8 cm Ã— 11.5 cm) with a 1Ã—5 box matrix. In this user study, we compare the results of our developed algorithm for OpenFace to an SMI RED 250 eye tracker. The results showed that our work achieved an overall relative accuracy of 58.54%. Copyright Â© 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",,,
10.1109/TIP.2020.2982828,2020,"Cheng Y., Zhang X., Lu F., Sato Y.",Gaze Estimation by Exploring Two-Eye Asymmetry,"Eye gaze estimation is increasingly demanded by recent intelligent systems to facilitate a range of interactive applications. Unfortunately, learning the highly complicated regression from a single eye image to the gaze direction is not trivial. Thus, the problem is yet to be solved efficiently. Inspired by the two-eye asymmetry as two eyes of the same person may appear uneven, we propose the face-based asymmetric regression-evaluation network (FARE-Net) to optimize the gaze estimation results by considering the difference between left and right eyes. The proposed method includes one face-based asymmetric regression network (FAR-Net) and one evaluation network (E-Net). The FAR-Net predicts 3D gaze directions for both eyes and is trained with the asymmetric mechanism, which asymmetrically weights and sums the loss generated by two-eye gaze directions. With the asymmetric mechanism, the FAR-Net utilizes the eyes that can achieve high performance to optimize network. The E-Net learns the reliabilities of two eyes to balance the learning of the asymmetric mechanism and symmetric mechanism. Our FARE-Net achieves leading performances on MPIIGaze, EyeDiap and RT-Gene datasets. Additionally, we investigate the effectiveness of FARE-Net by analyzing the distribution of errors and ablation study. Â© 1992-2012 IEEE.",,,
10.1117/12.2552997,2020,"Lee B., Yoo D., Jeong J., Bang K., Moon S.",Ultra-high-definition holography for near-eye display,"Holographic near-eye displays (NEDs) have large potential for augmented reality (AR) devices as they modulate the wavefront of light. They can provide observers with comfortable three-dimensional (3D) views with focus-cues, and little optical aberrations since the unwanted phase delay added by optical systems can be compensated by wavefront modulation. With the advent of ultra-high-definition (UHD) spatial light modulator (SLM), a degree of freedom in designing holographic NEDs has been further expanded. Here, we introduce several holographic NEDs using UHD SLM. The holographic NED using an HOE is introduced for the optical see-Through display. Besides, the holographic NED with enlarged eye-box using point-source array and eye-Tracking method will be presented. Finally, the holographic NED of which optical aberration is compensated by Zernike's polynomial adaptation will be introduced. Â© COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.",,,
10.1117/12.2553259,2020,"Jia Z., Chung H., Daiker J., Sedighi S., Morley N., Dominguez D., Grata J., Welch H.",Eyeball camera based calibration and performance verification for spatial computing systems,"Spatial computing enables overlay of the digital world over the real world in a spatially interactive manner by merging digital light-fields, perception systems, and computing. The digital content presented by the spatial computing needs to work tandemly with real-world surroundings, and more importantly the human eye-brain system, which is the ultimate judge for system success. As a result, to develop a spatial computing system, it would be essential to have a proxy for the human eye-brain to calibrate and verify the performance of the spatial computing system. This paper proposes a novel camera design for such purpose which mimics human ocular anatomy and physiology in the following aspects: geometry, optical performance and ocular motor control. Specifically, the proposed camera not only adopts the same corneal and pupil geometry from human eye, also the iris and pupil can be configured with multiple texture, color and diameter options. Furthermore, the resolution of eyeball camera is designed to match the acuity of typical 20/20 human vision, and focus can be dynamically adjusted from 0 to 3 diopters. Lastly, a pair of eyeball cameras are mounted independently on two hexapods to simulate the eye gaze and vergence. With the help of the eyeball cameras, both perceived virtual and real world can be calibrated and evaluated in a deterministic and quantifiable eye conditions like pupil location and gaze. Principally, the proposed eyeball camera serves as a bridge which combines all the data from spatial computing like eye tracking, 3D geometry of the digital world, display color accuracy/uniformity, and display optical quality (sharpness, contrast, etc) for a holistic view, which helps to effectively blend the virtual and real worlds together seamlessly. Â© 2020 SPIE.",,,
10.1117/12.2548336,2020,AlÃ£o N.,Qualitative and quantitative visual information detected by portable eye-tracking technology,"This paper presents some results of a larger study about vision geometry, carried out between 2014 and 2016 at the Lisbon School of Architecture, considering eye-tracking technology as an effective way of studying human vision. The methodology relied on portable eye-tracking equipment to analyze 3D immersive visual perception. 30 observers conducted 120 analyses of four different three-dimensional architectural spaces. The 120 samples allowed the analysis of 60,000 video frames to understand different kinds of elements that can be used to describe and study visual information. Quantitative and qualitative results are presented in the form of graphics to better understand eye movements in the perception of three-dimensional visual spaces, with a particular focus on macro-saccadic movements. The purpose of this work is to examine how the technology works, the possibilities it offers, and its limitations. The paper presents results in a way that seems trustworthy to work with at present, in order to obtain insights of scientific value that can point towards future possible resolutions of current methodological and technological issues. Â© 2020 SPIE.",,,
10.1117/12.2545394,2020,"Nakamura H., Kitada T., Hamagishi G., Yoshimoto K., Kusafuka K., Takahashi H.",Control method of active parallax barrier and binocular image for glasses-free stereoscopic display according to viewing position,"In 3D displays based on parallax barriers, active parallax barriers that can change the barrier pattern according to the viewing position have been proposed to expand the viewing area. However, the production cost increases because the active barrier requires a special LC panel. Therefore, to lower the cost, we propose a glasses-free stereoscopic display using an active parallax barrier of an LC panel with the same specifications as the image LC panel. The ideal image pattern cannot be formed because the minimum control unit of the LC panel is equal to one subpixel. However, by using an image cycle pitch method (ICPM) that periodically increases the horizontal pitch of one pair of binocular images by one subpixel, we can realize the ideal relationship between the average pitch of the binocular image and the barrier pitch. In our previous research, we could make the average pitch match to the ideal pitch on the discrete optimum viewing distances (OVDs), but we could not follow the viewing position between the discrete OVDs. In this paper, we propose the ICPM that can keep the stereoscopic vision even at any viewing position. In order to verify the effectiveness of the proposed method, we made the prototype displays and evaluated them. As a result, we showed the crosstalk ratio could be suppressed low over a very wide range by using the proposed method. Also, we confirmed we could obtain the stereoscopic vision at the viewing distance from 300 mm to 729 mm by a subjective evaluation. Â© COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.",,,
10.1109/ACCESS.2020.2980901,2020,"Panetta K., Wan Q., Rajeev S., Kaszowska A., Gardony A.L., Naranjo K., Taylor H.A., Agaian S.",ISeeColor: Method for Advanced Visual Analytics of Eye Tracking Data,"Recent advances in head-mounted eye-tracking technology have allowed researchers to monitor eye movements during locomotion in real-world environments, increasing the ecological validity of research on human gaze behavior. While collecting eye-tracking data is becoming more accessible, visual analytics of eye-tracking data remains difficult and time-consuming. As such, there is a significant need for developing efficient visualization and analysis tools for large-scale eye-tracking data. This work develops a first-of-its-kind eye-tracking data visualization and analysis system that allows for automatic recognition of independent objects within field-of-vision, using deep-learning-based semantic segmentation. This system recolors the fixated objects-of-interest by integrating gaze fixation information with semantic maps. The system effectively allows researchers to automatically infer what objects users view and for how long in dynamic contexts. The contributions are 1) a data visualization and analysis system that uses deep-learning technology along with eye-tracking data to automatically recognize objects-of-interest from head-mounted eye-tracking video recordings, and 2) a graphical user interface that presents objects-of-interest annotation along with eye-tracking data information. The architecture is tested with an outdoor case study of users walking around the Tufts University campus as part of a navigation study, which was administered by a team of research psychologists. Â© 2013 IEEE.",,,
10.2197/ipsjtbio.13.7,2020,"Emoto J., Hirata Y.",Lightweight convolutional neural network for image processing method for gaze estimation and eye movement event detection,"Advancements in technology have recently made it possible to obtain various types of biometric information from humans, enabling studies on estimation of human conditions in medicine, automobile safety, marketing, and other areas. These studies have particularly pointed to eye movement as an effective indicator of human conditions, and research on its applications is actively being pursued. The devices now widely used for measuring eye movements are based on the video-oculography (VOG) method, wherein the direction of gaze is estimated by processing eye images obtained through a camera. Applying convolutional neural networks (ConvNet) to the processing of eye images has been shown to enable accurate and robust gaze estimation. Conventional image processing, however, is premised on execution using a personal computer, making it difficult to carry out real-time gaze estimation using ConvNet, which involves the use of a large number of parameters, in a small arithmetic unit. Also, detecting eye movement events, such as blinking and saccadic movements, from the inferred gaze direction sequence for particular purposes requires the use of a separate algorithm. We therefore propose a new eye image processing method that batch-processes gaze estimation and event detection from end to end using an independently designed lightweight ConvNet. This paper discusses the structure of the proposed lightweight ConvNet, the methods for learning and evaluation used, and the proposed method's ability to simultaneously detect gaze direction and event occurrence using a smaller memory and at lower computational complexity than conventional methods. Â© 2020 Information Processing Society of Japan. All rights reserved.",,,
10.1007/978-3-030-42058-1_43,2020,"Åšledzianowski A., Szymanski A., Drabik A., Szlufik S., Koziorowski D.M., Przybyszewski A.W.",Combining Results of Different Oculometric Tests Improved Prediction of Parkinsonâ€™s Disease Development,"In this text we compare the measurement results of reflexive saccades and antisaccades of patients with Parkinsonâ€™s Disease (PD), trying to determine the best settings to predict the Unified Parkinsonâ€™s Disease Rating Scale (UPDRS) results. After Alzheimerâ€™s disease, PD statistically is the second one and until today, no effective therapy has been found. Luckily, PD develops very slowly and early detection can be very important in slowing its progression. In this experiment we examined the reflective saccades (RS) and antisaccades (AS) of 11 PD patients who performed eye-tracking tests in controlled conditions. We correlated neurological measurements of patientâ€™s abilities described by the Unified Parkinsonâ€™s Disease Rating Scale (UPDRS) scale with parameters of RS and AS. We used tools implemented in the Scikit-Learn for data preprocessing and predictions of the UPDRS scoring groups [1]. By experimenting with different datasets we achieved best results by combining means of RS and AS parameters into computed attributes. We also showed, that the accuracy of the prediction increases with the number of such derived attributes. We achieved 89% accuracy of predictions and showed that computed attributes have 50% higher results in the feature importance scoring than source parameters. The eye-tracking tests described in this text are relatively easy to carry out and could support the PD diagnosis. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-42058-1_45,2020,"Chudzik A., SzymaÅ„ski A., Nowacki J.P., Przybyszewski A.W.",Eye-Tracking and Machine Learning Significance in Parkinsonâ€™s Disease Symptoms Prediction,"Parkinsonâ€™s disease (PD) is a progressive, neurodegenerative disorder characterized by resting tremor, rigidity, bradykinesia, and postural instability. The standard measure of the PD progression is Unified Parkinsonâ€™s Disease Rating (UPDRS). Our goal was to predict patientsâ€?UPDRS development based on the various groups of patients in the different stages of the disease. We used standard neurological and neuropsychological tests, aligned with eye movements on a dedicated computer system. For predictions, we have applied various machine learning models with different parameters embedded in our dedicated data science framework written in Python and based on the Scikit Learn and Pandas libraries. Models proposed by us reached 75% and 70% of accuracy while predicting subclasses of UPDRS for patients in advanced stages of the disease who respond to treatment, with a global 57% accuracy score for all classes. We have demonstrated that it is possible to use eye movements as a biomarker for the assessment of symptom progression in PD. Â© 2020, Springer Nature Switzerland AG.",,,
10.1109/ACCESS.2020.2977729,2020,"Wang W., Chen X., Zheng S., Li H.",Fast Head Pose Estimation via Rotation-Adaptive Facial Landmark Detection for Video Edge Computation,"The human head pose estimation is an important and challenging problem, which provides the estimation of the head posture in 3D space from 2D image. It is a crucial technique for face recognition, gaze estimation, facial attribute recognition, etc. However, fast head pose estimation executing on the terminal for video edge computation has many challenges due to the computational complexity of the existing algorithms. In this paper, we propose a fast head pose estimation method based on a novel Rotation-Adaptive facial landmark detection powered by Local Binary Feature (RALBF). The landmark detection method is structured through fusing the prior of the rotation information provided by the Progressive Calibration Networks (PCN) face detector to a Local Binary Feature (LBF) based landmark detection method, which improves the robustness against head pose variations and simultaneously keep the computing efficiency. RALBF is trained and tested on 300W dataset and AFLW2000 dataset, it is verified by the accuracy evaluation that RALBF performs better than LBF. To improve the speed of head pose estimation, the 68, 51 and 10 landmarks distribution schemes are explored and compared on speed and accuracy. In the 10 landmarks scheme, the head pose estimation running once only takes 8.3ms on Intel i7-6700HQ CPU and takes 21.8ms on HiSilicon SoC Hi3519AV100, and the average error of Euler angle is 5.9973Â° when the face yaw angle is between Â±35Â° on AFLW2000 3D dataset. Experiments demonstrate our approach performing well on real scenes. Â© 2013 IEEE.",,,
10.1007/978-3-030-39431-8_14,2020,"Albalawi T., Ghazinour K., Melton A.",Quantifying the effect of cognitive bias on security decision-making for authentication methods,"The main challenge that can impact the effectiveness of authentication mechanisms is human error (unintentional threats). Irrational judgment associated with human error is often linked to a unique attribute called cognitive bias (CB). CB is a tendency to think irrationally in certain situations and make irrational judgment. The appearance of CB in human decisions is considered one of the implications of system usability. In the security filed, usability is recognized as one of the main issues that affect an individualâ€™s security decisions. Clearly, security decision-making is a result of three overlapping factors: security, usability and CB. In this paper, we quantify security decision making by providing a holistic view on how these factors affect the security decision. For this purpose, an experiment was conducted involving 54 participants who performed multiple security tasks related to authentication. An eye-tracking machine was used to record cognitive measurements that were used for decision analysis. Multi Criteria Decision Analysis (MCDA) approach was then used to evaluate the participantsâ€?decisions. The result showed that participants security decisions are varied depends on the authentication method. For instance, picture type was the authentication method least influenced by CB. Low system usability is one of the major causes of CB in decisions. This was not the case for the picture password method. The different levels of usability associated with the picture method resulted in low impact of CB on participantsâ€?security decision. This finding point to investigating how picture-based authentication methods are capable of handling the issue of the CB. Â© Springer Nature Switzerland AG 2020.",,,
10.1007/978-3-030-39431-8_20,2020,"Yan X., Wang Z., Sun M.",Eye fixation assisted detection of video salient objects,"With the increasing maturity of image saliency detection, more and more people are focusing their research on video saliency detection. Currently, video saliency detection can be divided into two forms, eye fixation detection and salient objects detection. In this article, we focus on exploring the relationship between them. Firstly, we propose a network called fixation assisted video salient object detection network (FAVSODNet), which uses the eye gaze information in videos to assist in detecting video salient objects. A fixation assisted module (FAM) is designed to connect FP task and SOD task deeply. Under the guidance of the eye fixation information, multiple salient objects in complex scene can be detected more correctly. Moreover, when the scene suddenly changes or a new person appears, it can better to detect the correct salient objects with the aid of fixation maps. In addition, we adopt an extended multi-scale feature extraction module (EMFEM) to extract rich object features. Thus, the neural network can aware the objects with variable scales in videos more comprehensively. Finally, the experimental results show that our method advances the state-of-art in video salient object detection. Â© Springer Nature Switzerland AG 2020.",,,
10.1007/978-3-030-37218-7_36,2020,"Tamuly S., Jyotsna C., Amudha J.",Deep learning model for image classification,"Starting from images captured on mobile phones, advertisements popping up on our internet browser to e-retail sites displaying flashy dresses for sale, every day we are dealing with a large quantity of visual information and sometimes, finding a particular image or visual content might become a very tedious task to deal with. By classifying the images into different logical categories, our quest to find an appropriate image becomes much easier. Image classification is generally done with the help of computer vision, eye tracking and ways as such. What we intend to implement in classifying images is the use of deep learning for classifying images into pleasant and unpleasant categories. We proposed the use of deep learning in image classification because deep learning can give us a deeper understanding as to how a subject reacts to a certain visual stimuli when exposed to it. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-37218-7_98,2020,"Anusree K., Amudha J.",Eye movement event detection with deep neural networks,"This paper presents a comparison of event detection task in eye movement with the exact events recorded from eye tracking device. The primary goal of this research work is to build a general approach for eye-movement based event detection, which will work with all eye tracking data collected using different eye tracking devices. It utilizes an end to end method based on deep learning, which can efficiently utilize eye tracking raw particulars that is further grouped into Saccades, post-saccadic oscillations and Fixations. The drawback of deep learning method is that it requires a lot of preprocessing data. At first, we have to build up a strategy to enlarge handcoded information, with the goal that we can unequivocally augment the informational index utilized for preparing, limiting the run through time on coding by a human. Utilizing this all-encompassing hand-coded information, we instruct neural networks model to process eye-development fixation grouping from eye-movement information in the absence of any previously defined extraction or post-preparing steps. Â© 2020, Springer Nature Switzerland AG.",,,
10.1109/ACCESS.2019.2962974,2020,"Vo M.T., Nguyen T., Le T.",Robust Head Pose Estimation Using Extreme Gradient Boosting Machine on Stacked Autoencoders Neural Network,"Head pose estimation is an important sign in helping robots and other intelligence machines understand human. It plays a vital role in designing human computer interaction systems because many applications rely on precise results of head pose angles such as human behavior analysis, gaze estimation, 3D head reconstruction etc. This study presents a robust approach for estimating the head pose angles in a single image. More specifically, the proposed system first encodes the global features extracted from Histogram of Oriented Gradients in a multi stacked autoencoders neural network. Based on the hidden nodes in deep layers, Autoencoder has been proposed for feature reduction while maintaining the key information of data. A scalable gradient boosting machine is then employed to train and classify the embedded features. Experiences have evaluated on the Pointing 04 dataset and show that the proposed approach outperforms the state-of-the-art methods with the low head pose angle errors in pitch and yaw as 6.16Â° and 7.17Â°, respectively. Â© 2013 IEEE.",,,
10.1111/tops.12476,2020,"Laubrock J., Dunst A.",Computational Approaches to Comics Analysis,"Comics are complex documents whose reception engages cognitive processes such as scene perception, language processing, and narrative understanding. Possibly because of their complexity, they have rarely been studied in cognitive science. Modeling the stimulus ideally requires a formal description, which can be provided by feature descriptors from computer vision and computational linguistics. With a focus on document analysis, here we review work on the computational modeling of comics. We argue that the development of modern feature descriptors based on deep learning techniques has made sufficient progress to allow the investigation of complex material such as comics for reception studies, including experimentation and computational modeling of cognitive processes. Â© 2019 Cognitive Science Society, Inc",,,
10.1109/TMI.2019.2924254,2020,"Van Oosterom M.N., Van Der Poel H.G., Van Leeuwen F.W.B., Meershoek P., Welling M.M., Pinto F., Matthies P., Simon H., Wendler T., Navab N., Van De Velde C.J.H.",Extending the Hybrid Surgical Guidance Concept with Freehand Fluorescence Tomography,"Within image-guided surgery, 'hybrid' guidance technologies have been used to integrate the complementary features of radioactive guidance and fluorescence guidance. Here, we explore how the generation of a novel freehand fluorescence (fhFluo) imaging approach complements freehand SPECT (fhSPECT) in a hybrid setup. Near-infrared optical tracking was used to register the position and the orientation of a hybrid opto-nuclear detection probe while recording its readings. Dedicated look-up table models were used for 3D reconstruction. In phantom and excised tissue settings (i.e., flat-surface human skin explants), fhSPECT and fhFluo were investigated for image resolution and in-tissue signal penetration. Finally, the combined potential of these freehand technologies was evaluated on prostate and lymph node specimens of prostate cancer patients receiving prostatectomy and sentinel lymph node dissection (tracers: indocyanine green (ICG) +99m Tc-nanocolloid or ICG-99mTc-nanocolloid). After hardware and software integration, the hybrid setup created 3D nuclear and fluorescence tomography scans. The imaging resolution of fhFluo (1 mm) was superior to that of fhSPECT (6 mm). Fluorescence modalities were confined to a maximum depth of 0.5 cm, while nuclear modalities were usable at all evaluated depths (&lt;2 cm). Both fhSPECT and fhFluo enabled augmented-and virtual-reality navigation toward segmented image hotspots, including relative hotspot quantification with an accuracy of 3.9% and 4.1%. Imaging in surgical specimens confirmed these trends (fhSPECT: in-depth detectability, low resolution, and fhFluo: superior resolution, superficial detectability). Overall, when radioactive and fluorescent tracer signatures are used, fhFluo has complementary value to fhSPECT. Combined the freehand technologies render a unique hybrid imaging and navigation modality. Â© 1982-2012 IEEE.",,,
10.1007/s11042-019-08160-5,2020,"Xia Y., Lou J., Dong J., Qi L., Li G., Yu H.",Hybrid regression and isophote curvature for accurate eye center localization,"The eye center localization is a crucial requirement for various human-computer interaction applications such as eye gaze estimation and eye tracking. However, although significant progress has been made in the field of eye center localization in recent years, it is still very challenging for tasks under the significant variability situations caused by different illumination, shape, color and viewing angles. In this paper, we propose a hybrid regression and isophote curvature for accurate eye center localization under low resolution. The proposed method first applies the regression method, which is called Supervised Descent Method (SDM), to obtain the rough location of eye region and eye centers. SDM is robust against the appearance variations in the eye region. To make the center points more accurate, isophote curvature method is employed on the obtained eye region to obtain several candidate points of eye center. Finally, the proposed method selects several estimated eye center locations from the isophote curvature method and SDM as our candidates and a SDM-based means of gradient method further refine the candidate points. Therefore, we combine regression and isophote curvature method to achieve robustness and accuracy. In the experiment, we have extensively evaluated the proposed method on the two public databases which are very challenging and realistic for eye center localization and compared our method with existing state-of-the-art methods. The results of the experiment confirm that the proposed method outperforms the state-of-the-art methods with a significant improvement in accuracy and robustness and has less computational complexity. Â© 2019, The Author(s).",,,
10.1007/978-3-030-31254-1_6,2020,"Piotrowski P., Nowosielski A.",Gaze-Based Interaction for VR Environments,"In this paper we propose a steering mechanism for VR headset utilizing eye tracking. Based on the fovea region traced by the eyetracker assembled into VR headset the visible 3D ray is generated towards the focal point of sight. The user can freely look around the virtual scene and is able to interact with objects indicated by the eyes. The paper gives an overview of the proposed interaction system and addresses the effectiveness and precision issues of such interaction modality. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-29513-4_86,2020,"Li Q., Hou W.",Using expertsâ€?perceptual skill for dermatological image segmentation,"There is a growing reliance on imaging equipment in medical domain, hence medical expertsâ€?specialized visual perceptual capability becomes the key of their superior performance. In this paper, we propose a principled generative model to detect and segment out dermatological lesions by exploiting the expertsâ€?perceptual expertise represented by their patterned eye movement behaviors during examining and diagnosing dermatological images. The image superpixelsâ€?diagnostic significance levels are inferred based on the correlations between their appearances and the spatial structures of the expertsâ€?signature eye movement patterns. In this process, the global relationships between the superpixels are also manifested by the spans of the signature eye movement patterns. Our model takes into account these dependencies between expertsâ€?perceptual skill and image properties to generate a holistic understanding of cluttered dermatological images. A Gibbs sampler is derived to use the generative modelâ€™s structure to estimate the diagnostic significance and lesion spatial distributions from superpixel-based representation of dermatological images and expertsâ€?signature eye movement patterns. We demonstrate the effectiveness of our approach on a set of dermatological images on which dermatologistsâ€?eye movements are recorded. It suggests that the integration of expertsâ€?perceptual skill and dermatological images is able to greatly improve medical image understanding and retrieval. Â© Springer Nature Switzerland AG 2020.",,,
10.1007/978-3-030-25128-4_15,2020,"Yuan Y., Wang Q.",Feature Extraction for Eye Movement Video Data,"Random objects in videos are common stimuli in eye tracker based studies and their locations and time of appearance need to be detected in related research such as depression detection. In this paper, we propose a new method to extract features in eye movement video data captured by the SMI eye tracker. Firstly, we provide a feature extraction method by using the circle Hough transform and the Douglasâ€“Peucker algorithm to extract the feature for each frame of the eye movement video data, and verify its validity in eye movement video data processing. Secondly, because the storage time of the eye tracker is more accurate than the on-screen time of the exported video, we choose to extract the storage time of the eye tracker to improve the quality of feature extraction. Finally, we add batch processing function to improve the efficiency of the experiment. Experimental results show that the method can extract the eye movement features in the eye movement video accurately and effectively. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-20476-1_40,2020,Guilei S.,Research on Location of Emergency Sign Based on Virtual Reality and Eye Tracking Technology,"In order to analyze the attention to exit sign during the emergency state, virtual reality technology (VR) was used to simulate escape scene and 3D Max was used to design the scene. To obtain the number of gaze points and the gaze duration of subjects, eye tracker was utilized to get data and spss 21.0 was taken advantage of to analyze the data. Results show that the position of emergency exit sign has significant influence on the recognition. And the exit sign of the height of 1.0 m on the front of the observer&#x2019;s line of sight is the most beneficial to discovery and identification. Moreover, the height and the position of exit sign have no significant influence on reading time. &#x00A9; 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-20135-7_35,2020,"Brandenburger J., Constapel M., HellbrÃ¼ck H., Janneck M.","Analysis of Types, Positioning and Appearance of Visualizations in Online Teaching Environments to Improve Learning Experiences","In this paper we investigate different visualizations of learnersâ€?data related to collaborative online learning in terms of suitability and attractiveness to students. Furthermore, we analyze whether positioning and color appearance of these data visualizations might have an effect on learnersâ€?behavior. To that end, we conducted an online study (n = 120) as well as an eye tracking study (n = 20) to compare different types of visualizations. Results show that students prefer classical data visualizations like bar charts. Visualizations placed in the sidebar of a two column web interface get less attention than visualizations in the header of the main content area. Color schemes do not seem to influence the perception of visualizations. We discuss possible explanations and implications for designing data visualizations in learning environments. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-19501-4_21,2020,"Gomolka Z., Twarog B., Zeslawska E., Kordos D.",Registration and Analysis of a Pilotâ€™s Attention Using a Mobile Eyetracking System,"This paper presents the next stage of research into the registration and analysis of a pilotâ€™s attention during the take-off and landing procedures. This is a continuation of the research, which the authors conducted using the static SensoMotoric Instruments (SMI) Red500 eyetracker on a group of pilots applying for a pilotâ€™s license. Because static eyetracking introduces a series of restrictions on the properties of the observed scene, a system using Tobii Glasses Pro mobile eye tracker was proposed. Using the mobile system and a training simulator, a measurement stand was prepared for the flight of the aircraft configured as follows: Seneca II airplane with a three-point hidden chassis, location: EPRZ airport - RzeszÃ³w Jasionka. An aviation task was prepared, which was carried out by participants in the experiment. A Tobii project was developed to define several Area of Interests (AOI) for key instruments in the cockpit of the used aircraft. It was observed that for the implementation of an exemplary task, there is no possibility of smoothly modifying coordinates of areas of interest of AOI in subsequent frames of the recorded video stream. Authors designed in the Matlab environment, the Smart Trainer application for a smooth analysis of attention using the characteristic points tracking mechanism. A fuzzy AOI contour is proposed using a modified form of the Butterworth 2D filter for individual instruments, which allows more effective registration of fixations. During the measurement it is possible to observe fixation histograms for a defined set of instruments. Â© 2020, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-14118-9_20,2020,"Bakry A., Al-Khatib R., Negm R., Sabra E., Maher M., Mohamed Z., Shawky D., Badawi A.",Using Eye Movement to Assess Auditory Attention,"Eye movement has been found to be one of the factors that highly affect attention. In this paper, a study for detecting the influence of eye movement on attention is presented. Forty-three participants attended four sessions introducing different auditory stimuli while wearing a 14-channel wireless headset that collects their EEG signals. The participants were asked to fix their eyes in two of the sessions and they were allowed to move them freely in the other two. Their attention during the sessions was estimated using questionnaires that assess the information they were able to gain after the sessions. Different classifiers were trained to predict the attention scores when subjects were freely moving their eyes or fixing them. Among the trained classifiers, K nearest-neighbors classifiers yielded the best classification accuracy, which varied with the addition of the eye-movement features from about 72% to 87%. Thus, the obtained results show that there is an effect of eye movement on the gained attention. Hence, it is possible to detect attention of subjects using their eye movement patterns. Â© 2020, Springer Nature Switzerland AG.",,,
10.1109/AIVR46125.2019.00020,2019,"Vielhaben J., Camalan H., Samek W., Wenzel M.",Viewport forecasting in 360Â° virtual reality videos with machine learning,"Objective. Virtual reality (VR) cloud gaming and 360Â° video streaming are on the rise. With a VR headset, viewers can individually choose the perspective they see on the head-mounted display by turning their head, which creates the illusion of being in a virtual room. In this experimental study, we applied machine learning methods to anticipate future head rotations (a) from preceding head and eye motions, and (b) from the statistics of other spherical video viewers. Approach. Ten study participants watched each 3 1/3 hours of spherical video clips, while head and eye gaze motions were tracked, using a VR headset with a built-in eye tracker. Machine learning models were trained on the recorded head and gaze trajectories to predict (a) changes of head orientation and (b) the viewport from population statistics. Results. We assembled a dataset of head and gaze trajectories of spherical video viewers with great stimulus variability. We extracted statistical features from these time series and showed that a Support Vector Machine can classify the range of future head movements with a time horizon of up to one second with good accuracy. Even population statistics among only ten subjects show prediction success above chance level. %Both approaches resulted in a considerable amount of prediction success using head movements, but using gaze movement did not contribute to prediction performance in a meaningful way. Even basic machine learning models can successfully predict head movement and aspects thereof, while being naive to visual content. Significance. Viewport forecasting opens up various avenues to optimize VR rendering and transmission. While the viewer can see only a section of the surrounding 360Â° sphere, the entire panorama has typically to be rendered and/or broadcast. The reason is rooted in the transmission delay, which has to be taken into account in order to avoid simulator sickness due to motion-To-photon latencies. Knowing in advance, where the viewer is going to look at may help to make cloud rendering and video streaming of VR content more efficient and, ultimately, the VR experience more appealing. Â© 2019 IEEE.",,,
10.1142/S1793351X1940018X,2019,"Fu B., Steichen B., Zhang W.",Towards Adaptive Ontology Visualization - Predicting User Success from Behavioral Data,"Ontology visualization plays an important role in human data interaction by offering clarity and insight for complex structured datasets. Recent usability studies of ontology visualization techniques have added to our understanding of desired features when assisting users in the interactive process. However, user behavioral data such as eye gaze and event logs have largely been used as indirect evidence to explain why a user may have carried out certain tasks in a controlled environment, as opposed to direct input that informs the underlying visualization system. Although findings from usability studies have contributed to the refinement of ontology visualizations as a whole, the visualization techniques themselves remain a one-size-fits-all approach, where all users are presented with the same visualizations and interactive features. By contrast, this paper investigates the feasibility of using behavioral data, such as user gaze and event logs, as real-time indicators of how appropriate or effective a given visualization may be for a specific user at a moment in time, which in turn may be used to inform the adaptation of the visualization to the user on the fly. To this end, we apply established predictive modeling techniques in Machine Learning to predict user success using gaze data and event logs. We present a detailed analysis from a controlled experiment and demonstrate such predictions are not only feasible, but can also be significantly better than a baseline classifier during visualization usage. These predictions can then be used to drive the adaptations of visual systems in providing ad hoc visualizations on a per user basis, which in turn may increase individual user success and performance. Furthermore, we demonstrate the prediction performance using several different feature sets, and report on the results generated from several notable classifiers, where a decision tree-based learning model using a boosting algorithm produced the best overall results. Â© 2019 World Scientific Publishing Company.",,,
10.3390/electronics8121487,2019,"Ullah A., Wang J., Shahid Anwar M., Ahmad U., Saeed U., Fei Z.",Facial expression recognition of nonlinear facial variations using deep locality de-expression residue learning in the wild,"Automatic facial expression recognition is an emerging field. Moreover, the interest has been increased with the transition from laboratory-controlled conditions to in the wild scenarios. Most of the research has been done over nonoccluded faces under the constrained environment, while automatic facial expression is less understood/implemented for partial occlusion in the real world conditions. Apart from that, our research aims to tackle the issues of overfitting (caused by the shortage of adequate training data) and to alleviate the expression-unrelated/intraclass/nonlinear facial variations, such as head pose estimation, eye gaze estimation, intensity and microexpressions. In our research, we control the magnitude of each Action Unit (AU) and combine several of the Action Unit combinations to leverage learning from the generative and discriminative representations for automatic FER. We have also addressed the problem of diversification of expressions from lab controlled to real-world scenarios from our cross-database study and proposed a model for enhancement of the discriminative power of deep features while increasing the interclass scatters, by preserving the locality closeness. Furthermore, facial expression consists of an expressive component as well as neutral component, so we proposed a generative model which is capable of generating neutral expression from an input image using cGAN. The expressive component is filtered and passed to the intermediate layers and the process is called De-expression Residue Learning. The residue in the intermediate/middle layers is very important for learning through expressive components. Finally, we validate the effectiveness of our method (DLP-DeRL) through qualitative and quantitative experimental results using four databases. Our method is more accurate and robust, and outperforms all the existing methods (hand crafted features and deep learning) while dealing the images in the wild. Â© 2019, MDPI AG. All rights reserved.",,,
10.1016/j.cmpb.2019.105053,2019,"Ferreira D.S., Ramalho G.L.B., Torres D., Tobias A.H.G., Rezende M.T., Medeiros F.N.S., Bianchi A.G.C., Carneiro C.M., Ushizima D.M.",Saliency-driven system models for cell analysis with deep learning,"Background and objectives: Saliency refers to the visual perception quality that makes objects in a scene to stand out from others and attract attention. While computational saliency models can simulate the expert's visual attention, there is little evidence about how these models perform when used to predict the cytopathologist's eye fixations. Saliency models may be the key to instrumenting fast object detection on large Pap smear slides under real noisy conditions, artifacts, and cell occlusions. This paper describes how our computational schemes retrieve regions of interest (ROI) of clinical relevance using visual attention models. We also compare the performance of different computed saliency models as part of cell screening tasks, aiming to design a computer-aided diagnosis systems that supports cytopathologists. Method: We record eye fixation maps from cytopathologists at work, and compare with 13 different saliency prediction algorithms, including deep learning. We develop cell-specific convolutional neural networks (CNN) to investigate the impact of bottom-up and top-down factors on saliency prediction from real routine exams. By combining the eye tracking data from pathologists with computed saliency models, we assess algorithms reliability in identifying clinically relevant cells. Results:The proposed cell-specific CNN model outperforms all other saliency prediction methods, particularly regarding the number of false positives. Our algorithm also detects the most clinically relevant cells, which are among the three top salient regions, with accuracy above 98% for all diseases, except carcinoma (87%). Bottom-up methods performed satisfactorily, with saliency maps that enabled ROI detection above 75% for carcinoma and 86% for other pathologies. Conclusions:ROIs extraction using our saliency prediction methods enabled ranking the most relevant clinical areas within the image, a viable data reduction strategy to guide automatic analyses of Pap smear slides. Top-down factors for saliency prediction on cell images increases the accuracy of the estimated maps while bottom-up algorithms proved to be useful for predicting the cytopathologist's eye fixations depending on parameters, such as the number of false positive and negative. Our contributions are: comparison among 13 state-of-the-art saliency models to cytopathologistsâ€?visual attention and deliver a method that the associate the most conspicuous regions to clinically relevant cells. Â© 2019",,,
10.1016/j.displa.2019.08.002,2019,"Lin C.J., Prasetyo Y.T., Widyaningrum R.",Eye movement measures for predicting eye gaze accuracy and symptoms in 2D and 3D displays,"The current study applied Structural Equation Modeling (SEM) to analyze the interrelationship among index of difficulty (ID), environment, saccade duration (SD), revisited fixation duration (RFD), number of fixation (NF), pupil size (PS), eye gaze accuracy (AC), and symptoms simultaneously. SD, RFD, NF, PS, and AC were measured by utilizing the Tobii eye tracker system. Twelve participants were recruited to perform multi-directional tapping task using within-subject design with two different environments (2D screen display and 3D stereoscopic display) and six different levels of ID. SEM showed that ID had significant direct effects on SD and RFD while environment was found had significant direct effects on SD, RFD, PS, AC, and symptoms. Among selected eye movement measures, NF was found to be the best predictor of AC and PS was found to be the best predictor of symptoms. In addition, RFD was also found to be a good predictor of symptoms. Our results found that higher AC was achieved by projecting the image in the 2D screen display with higher ID and it resulted in higher SD and higher NF. Regarding the symptoms, our results found that lower symptoms were achieved by projecting the image in the 2D screen display with lower ID and it resulted in lower PS, and lower RFD. Practitioner Summary: The SEM could provide valuable theoretical foundations to identify the interrelationship among eye movement measures, AC, and symptoms particularly for VR researchers and interface developers. Â© 2019 Elsevier B.V.",,,
10.1109/TCDS.2018.2877128,2019,Vasilyev A.,Optimal Control of Eye Movements during Visual Search,"In this paper, we study the problem of an optimal oculomotor control during the execution of visual search tasks. We introduce a computational model of human eye movements, which takes into account various constraints of the human visual and oculomotor systems. In the model, the choice of the subsequent fixation location is posed as a problem of a stochastic optimal control, which relies on reinforcement learning methods. We show that if biological constraints are taken into account, the trajectories simulated under a learned policy share both basic statistical properties and a scaling behavior with human eye movements. We validated our model simulations with human psychophysical eye-tracking experiments. Â© 2016 IEEE.",,,
10.1145/3359997.3365738,2019,"Zhou Y., Feng T., Shuai S., Li X., Sun L., Duh H.B.L.",An eye-tracking dataset for visual attention modelling in a virtual museum context,"Predicting the user's visual attention enables a virtual reality (VR) environment to provide a context-aware and interactive user experience. Researchers have attempted to understand visual attention using eye-tracking data in a 2D plane. In this poster, we propose the first 3D eye-tracking dataset for visual attention modelling in the context of a virtual museum. It comprises about 7 million records and may facilitate visual attention modelling in a 3D VR space. Â© 2019 Association for Computing Machinery.",,,
10.1080/21681163.2018.1542346,2019,"Matos A.C., Azevedo Terroso T., Corte-Real L., Carvalho P.",Stereo vision system for human motion analysis in a rehabilitation context,"The present demographic trends point to an increase in aged population and chronic diseases which symptoms can be alleviated through rehabilitation. The applicability of passive 3D reconstruction for motion tracking in a rehabilitation context was explored using a stereo camera. The camera was used to acquire depth and color information from which the 3D position of predefined joints was recovered based on: kinematic relationships, anthropometrically feasible lengths and temporal consistency. Finally, a set of quantitative measures were extracted to evaluate the performed rehabilitation exercises. Validation study using data provided by a marker based as ground-truth revealed that our proposal achieved errors within the range of state-of-the-art active markerless systems and visual evaluations done by physical therapists. The obtained results are promising and demonstrate that the developed methodology allows the analysis of human motion for a rehabilitation purpose. Â© 2018, Â© 2018 Informa UK Limited, trading as Taylor & Francis Group.",,,
10.1109/IICSPI48186.2019.9096019,2019,"Xu D., Tan J.",Design of Close Scleral Vascular Imaging System Used for Gazing Tracking,"Scleral blood vessels are very stable biological features. Lightweight head-mounted scleral vascular imaging system can be widely used in personal identification, gaze tracking and many other fields. However, it is difficult to acquire clear images of scleral blood vessels at a small distance with traditional optical imaging systems. We proposed a new scleral vascular imaging system based on micro-lens array, which can capture the light field of scleral blood vessels near eyes with sub-image array. The system has a simple and integrative structure. And it can easily reconstruct the 3D position and structure of scleral blood vessels from multiple sub-aperture images. Â© 2019 IEEE.",,,
10.1109/SITIS.2019.00017,2019,"Valenzuela A., Arellano C., Tapia J.",An efficient dense network for semantic segmentation of eyes images captured with virtual reality lens,"Eye-tracking and Gaze estimation are difficult tasks that may be used for several applications including human-computer interfaces, salience detection and Virtual reality amongst others. This paper presents a segmentation algorithm based on deep learning that efficiently discriminates pupils, iris, and sclera from the background in images captured using a Virtual Reality lens. A light network called DensetNet10 trained from scratch is proposed. It contains fewer parameters than traditional architectures based on DenseNet which allows it to be used in mobile device applications. Experiments show that this network achieved higher IOU rates when comparing with DensetNet56-67-103 and DeeplabV3+ models in the Facebook database. Furthermore, this method reached 8th place in The Facebook semantic segmentation challenge with 0.94293 mean IOU and 202.084 parameters with a final score of 0.97147. Â© 2019 IEEE.",,,
10.1109/HST47167.2019.9032906,2019,"Chopade P., Edwards D., Khan S.M., Andrade A., Pu S.",CPSX: Using AI-Machine Learning for Mapping Human-Human Interaction and Measurement of CPS Teamwork Skills,"The objective of this work is to present a machine learning (ML)-based framework to identify evidence about collaborative problem solving (CPS) cognitive (teamwork) and social-emotional learning (SEL) skills from the dyadic (human-human-HH) interactions. This work extends our previous work (Chopade et al. IEEE HST 2018, LAK2019) [1], [2]. Explicitly, we are interested in how teamwork skills and team dynamics are demonstrated as verbal and nonverbal behaviors, and how these behaviors can be captured and analyzed via passive data collection. For this work we use a two-player cooperative CPS game, Crisis in Space (CIS) from LRNG (Previously GlassLab Inc). During the summer of 2018, we implemented this CIS game for interns as a group study. A total of 34 participants played the game and provided study and survey data. During the study, we collected participants' game play data, such as audio, video and eye tracking data streams. This research involves analyzing CIS multimodal game data, and developing skill models, and machine learning techniques for CPS skills measurement. In this paper, we present our ML framework for the analysis of audio data along with preliminary results from a pilot study. The analysis of audio data uses natural language processing (NLP) techniques, such as bag-of-words and sentence embedding. Our preliminary results show that various NLP features can be used to describe successful and unsuccessful CPS performances. The ML based framework supports the development of evidence centered design for teamwork skills-mapping and aims to help teams operate effectively in a complex situation. Potential applications of this work include support for the Department of Homeland Security (DHS), and the US Army for the development of learner and team centric training, cohort, and team behavioral skill-mapping. Â© 2019 IEEE.",,,
10.1109/IROS40897.2019.8968536,2019,"Su D., Li Y.F., Chen H.",Region-wise Polynomial Regression for 3D Mobile Gaze Estimation,"In the context of mobile gaze tracking techniques, a 3D gaze point can be calculated as the middle point between two 3D visual axes. To infer gaze directions and eyeball positions, a nonlinear optimization problem is typically formulated to minimize the angular disparities between the training gaze directions and prediction ones. Nonetheless, the experimental results reported by some previous works show that this kind of approaches are very likely to yield large prediction errors hence considered less useful for human-machine interactions. In this study, we aim to address this widespread issue in three aspects. At first, instead of using a global regression model, a simple local polynomial model is proposed to back-project a pupil center onto its corresponding visual axis. Based on the Leave-One-Out cross-validation criterion, the partition structure is automatically learned in the process of resolving a homography-like relationship. Secondly, a good starting point for nonlinear-optimization is obtained by the image eyeball center, which can be estimated by systematic parallax errors. Meanwhile, it is necessary to add the suitable constraints for 3D eye positions. Otherwise, the optimization may end up with trivial solutions, i.e., faraway eye positions. Thirdly, we explore a strategy for designing the spatial distribution of calibration points in a principled manner. The experiment results demonstrate that an encouraging gaze estimation accuracy can be achieved by our proposed framework for both the normal vision and eyewear users. Â© 2019 IEEE.",,,
10.1109/GlobalSIP45357.2019.8969158,2019,"Parikh D., Lu Y., Xin Y., Wu D., Pelz J., Lu G.",Where am i looking: Localizing gaze in reconstructed 3D space,"We propose a method to estimate the 3D gaze of the observer onto the scene using a portable eye tracker with a monocular camera. We reconstruct the 3D scene using Structure from Motion (SfM) and use camera pose and 3D reconstruction information of the scene to localize the position and pose of the observer in the 3D reconstructed space. Along with the position, we can obtain the 2D gaze of the observer using an eye-tracker. Each person may have a different perspective of the same 3D object in the scene, observing it from different positions. We use this information to fuse these multiple perspectives in 3D space to get a better understanding of how differently each observer perceives the same scene, compared to others. In the entire system, we developed a convo-lutional neural network to detect and track eye movement and a camera re-localization method to localize the camera in 3D environment. Based on our novel eye tracking and camera re-localization methods, we can accurately localize the gaze in the 3D reconstructed environment. Â© 2019 IEEE.",,,
10.1109/ICDMW.2019.00057,2019,Ishibashi K.,Application of deep learning to pre-processing of cousumer's eye tracking data in supermarket,"The purpose of this study is to automate pre-processing of eye tracking data. In an investigation with eye tracking in a field such as supermarket, pre-processing of data has enormous cost. This is because it is very difficult to map consumer's gaze points to certain snapshot due to her/his movement. This study uses deep learning for automating pre-processing of eye tracking data. General object recognition using deep learning can classify an image into various classes. The proposed method attempts to classify fixated object of consumer into three classes, product, promotion and etc., based on the result of general object recognition. This paper discusses the applicability of proposed method through cross validation using eye tracking data preprocessed manually. Â© 2019 IEEE.",,,
10.1007/s10846-018-0970-x,2019,"Lwin K.N., Myint M., Mukada N., Yamada D., Matsuno T., Saitou K., Godou W., Sakamoto T., Minami M.",Sea Docking by Dual-eye Pose Estimation with Optimized Genetic Algorithm Parameters,"Three-dimensional (3D) estimation of position and orientation (pose) using dynamic (successive) images input at video rates needs to be performed rapidly when the estimated pose is used for real-time feedback control. Single-camera 3D pose estimation has been studied thoroughly, but the estimated position accuracy in the camera depth of field has proven insufficient. Thus, docking systems for underwater vehicles with single-eye cameras have not reached practical application. The authors have proposed a new 3D pose estimation method with dual cameras that exploits the parallactic nature of stereoscopic vision to enable reliable 3D pose estimation in real time. We call this method the â€œreal-time multi-step genetic algorithm (RM-GA).â€?However, optimization of the pose tracking performance has been left unchallenged despite the fact that improved tracking performance in the time domain would help improve performance and stability of the closed-loop feedback system, such as visual servoing of an underwater vehicle. This study focused on improving the dynamic performance of dual-eye real-time pose tracking by tuning RM-GA parameters and confirming optimization of the dynamical performance to estimate a target markerâ€™s pose in real time. Then, the effectiveness and practicality of the real-time 3D pose estimation system was confirmed by conducting a sea docking experiment using the optimum RM-GA parameters in an actual marine environment with turbidity. Â© 2019, Springer Nature B.V.",,,
10.1109/TPAMI.2018.2864965,2019,"Li C., Lin L., Zuo W., Tang J., Yang M.-H.",Visual Tracking via Dynamic Graph Learning,"Existing visual tracking methods usually localize a target object with a bounding box, in which the performance of the foreground object trackers or detectors is often affected by the inclusion of background clutter. To handle this problem, we learn a patch-based graph representation for visual tracking. The tracked object is modeled by with a graph by taking a set of non-overlapping image patches as nodes, in which the weight of each node indicates how likely it belongs to the foreground and edges are weighted for indicating the appearance compatibility of two neighboring nodes. This graph is dynamically learned and applied in object tracking and model updating. During the tracking process, the proposed algorithm performs three main steps in each frame. First, the graph is initialized by assigning binary weights of some image patches to indicate the object and background patches according to the predicted bounding box. Second, the graph is optimized to refine the patch weights by using a novel alternating direction method of multipliers. Third, the object feature representation is updated by imposing the weights of patches on the extracted image features. The object location is predicted by maximizing the classification score in the structured support vector machine. Extensive experiments show that the proposed tracking algorithm performs well against the state-of-the-art methods on large-scale benchmark datasets. Â© 1979-2012 IEEE.",,,
10.1145/3332165.3347933,2019,"Gebhardt C., Hecox B., Van Opheusden B., Wigdor D., Hillis J., Hilliges O., Benko H.",Learning cooperative personalized policies from gaze data,"An ideal Mixed Reality (MR) system would only present virtual information (e.g., a label) when it is useful to the person. However, deciding when a label is useful is challenging: it depends on a variety of factors, including the current task, previous knowledge, context, etc. In this paper, we propose a Reinforcement Learning (RL) method to learn when to show or hide an object's label given eye movement data. We demonstrate the capabilities of this approach by showing that an intelligent agent can learn cooperative policies that better support users in a visual search task than manually designed heuristics. Furthermore, we show the applicability of our approach to more realistic environments and use cases (e.g., grocery shopping). By posing MR object labeling as a model-free RL problem, we can learn policies implicitly by observing users' behavior without requiring a visual search model or data annotation. Copyright Â© 2019 Association of Computing Machinery.",,,
10.1145/3332165.3347921,2019,"Sidenmark L., Gellersen H.",Eye & Head: Synergetic eye and head movement for gaze pointing and selection,"Eye gaze involves the coordination of eye and head movement to acquire gaze targets, but existing approaches to gaze pointing are based on eye-tracking in abstraction from head motion. We propose to leverage the synergetic movement of eye and head, and identify design principles for Eye & Head gaze interaction. We introduce three novel techniques that build on the distinction of head-supported versus eyes-only gaze, to enable dynamic coupling of gaze and pointer, hover interaction, visual exploration around pre-selections, and iterative and fast con-frmation of targets. We demonstrate Eye & Head interaction on applications in virtual reality, and evaluate our techniques against baselines in pointing and confrmation studies. Our results show that Eye & Head techniques enable novel gaze behaviours that provide users with more control and fexibility in fast gaze pointing and selection. Copyright Â© 2019 Association of Computing Machinery.",,,
10.1145/3347319.3356838,2019,"Zhu Y., Sun W., Yuan T.T., Li J.",Gaze detection and prediction using data from infrared cameras,"Knowing the point of gaze on a screen can benefit a variety of applications and improve user experiences. Some electronic devices with infrared cameras can generate 3D point cloud for user identification. We propose a paradigm to use 3D point cloud and eye images for gaze detection and prediction. Our method fuses 3D point cloud with eye images by image registration methods. We develop a cost function to detect saggital plane from point cloud data, and reconstruct a symmetric face by saggital plane. Symmetric face data increase the accuracy of gaze detection. We use long-short term memory models to track head and eye movement, and predict next point of gaze. Our method utilizes the existing hardware setup and provides options to improve user experiences. Â© 2019 Association for Computing Machinery.",,,
10.1145/3343031.3350964,2019,"Chaabouni S., Precioso F.",Impact of saliency and gaze features on visual control: Gaze-saliency interest estimator,"Predicting user intent from gaze presents a challenging question for developing real-time interactive systems like interactive search engine, implicit annotations of large datasets or intelligent robot behavior. Indeed, solutions to annotate easily large sets of images while reducing the burden of annotators is a key aspect for current machine learning techniques. We propose in this paper to design an estimator of the user interest for a given visual content based on eye-tracker feature analysis. We revise existing gaze-based interest estimator, and analyze the impact of the intrinsic saliency of the content displayed for interest estimation. We first explore low-level saliency prediction and propose a new gaze and saliency interest estimator. Experimental results show the advantage of our method for the annotation task in a weakly supervised context. In particular, we extend previous evaluation criteria on new experimental protocol displaying four images by frame as a first step towards ""Google Image search-like"" interfaces. Our Gaze and Saliency Interest Estimator (GSIE) reaches an overall accuracy of 83% in average of user interest prediction. If we consider the accuracy reached in a limited time, the GSIE is 70% in average within about 500ms and 80% in average within 1000ms. This result confirms our GSIE as an efficient real-time visual control solution. Â© 2019 Association for Computing Machinery.",,,
10.1007/s10278-018-00174-z,2019,"Mall S., Brennan P.C., Mello-Thoms C.",Can a Machine Learn from Radiologistsâ€?Visual Search Behaviour and Their Interpretation of Mammogramsâ€”a Deep-Learning Study,"Visual search behaviour and the interpretation of mammograms have been studied for errors in breast cancer detection. We aim to ascertain whether machine-learning models can learn about radiologistsâ€?attentional level and the interpretation of mammograms. We seek to determine whether these models are practical and feasible for use in training and teaching programmes. Eight radiologists of varying experience levels in reading mammograms reviewed 120 two-view digital mammography cases (59 cancers). Their search behaviour and decisions were captured using a head-mounted eye-tracking device and software allowing them to record their decisions. This information from radiologists was used to build an ensembled machine-learning model using top-down hierarchical deep convolution neural network. Separately, a model to determine type of missed cancer (search, perception or decision-making) was also built. Analysis and comparison of variants of these models using different convolution networks with and without transfer learning were also performed. Our ensembled deep-learning network architecture can be trained to learn about radiologistsâ€?attentional level and decisions. High accuracy (95%, p value â‰?0 [better than dumb/random model]) and high agreement between true and predicted values (kappa = 0.83) in such modelling can be achieved. Transfer learning techniques improve by ' 10% with the performance of this model. We also show that spatial convolution neural networks are insufficient in determining the type of missed cancers. Ensembled hierarchical deep convolution machine-learning models are plausible in modelling radiologistsâ€?attentional level and their interpretation of mammograms. However, deep convolution networks fail to characterise the type of false-negative decisions. Â© 2019, Society for Imaging Informatics in Medicine.",,,
10.1007/s10278-018-0169-5,2019,"Lucas A., Wang K., Santillan C., Hsiao A., Sirlin C.B., Murphy P.M.",Image Annotation by Eye Tracking: Accuracy and Precision of Centerlines of Obstructed Small-Bowel Segments Placed Using Eye Trackers,"Small-bowel obstruction (SBO) is a common and important disease, for which machine learning tools have yet to be developed. Image annotation is a critical first step for development of such tools. This study assesses whether image annotation by eye tracking is sufficiently accurate and precise to serve as a first step in the development of machine learning tools for detection of SBO on CT. Seven subjects diagnosed with SBO by CT were included in the study. For each subject, an obstructed segment of bowel was chosen. Three observers annotated the centerline of the segment by manual fiducial placement and by visual fiducial placement using a Tobii 4c eye tracker. Each annotation was repeated three times. The distance between centerlines was calculated after alignment using dynamic time warping (DTW) and statistically compared to clinical thresholds for diagnosis of SBO. Intra-observer DTW distance between manual and visual centerlines was calculated as a measure of accuracy. These distances were 1.1 Â± 0.2, 1.3 Â± 0.4, and 1.8 Â± 0.2 cm for the three observers and were less than 1.5 cm for two of three observers (P < 0.01). Intra- and inter-observer DTW distances between centerlines placed with each method were calculated as measures of precision. These distances were 0.6 Â± 0.1 and 0.8 Â± 0.2 cm for manual centerlines, 1.1 Â± 0.4 and 1.9 Â± 0.6 cm for visual centerlines, and were less than 3.0 cm in all cases (P < 0.01). Results suggest that eye trackingâ€“based annotation is sufficiently accurate and precise for small-bowel centerline annotation for use in machine learningâ€“based applications. Â© 2019, Society for Imaging Informatics in Medicine.",,,
10.1145/3340555.3356088,2019,Vortmann L.-M.,Attention-driven interaction systems for augmented reality,"Augmented reality (AR) glasses enable the embedding of visual content in a real-world surroundings. In this PhD project, I will implement user interfaces which adapt to the cognitive state of the user, for example by avoiding distractions or re-directing the user's attention towards missed information. For this purpose, sensory data from the user is captured (Brain activity via EEG of fNIRS, eye tracking, physiological measurements) and modeled with machine learning techniques. The focus of the cognitive state estimation is centered around attention related aspects. The main task is to build models for an estimation of a person's attentional state from the combination and classification of multimodal data streams and context information, as well as their evaluation. Furthermore, the goal is to develop prototypical user interfaces for AR glasses and to test their usability in different scenarios. Â© 2019 Copyright held by the owner/author(s).",,,
10.1145/3340555.3353735,2019,"Appel T., Sevcenko N., Wortha F., Tsarava K., Moeller K., Ninaus M., Gerjets P., Kasneci E.",Predicting cognitive load in an emergency simulation based on behavioral and physiological measures,"The reliable estimation of cognitive load is an integral step towards real-time adaptivity of learning or gaming environments. We introduce a novel and robust machine learning method for cognitive load assessment based on behavioral and physiological measures in a combined within- and crossparticipant approach. 47 participants completed different scenarios of a commercially available emergency personnel simulation game realizing several levels of difficulty based on cognitive load. Using interaction metrics, pupil dilation, eye-fixation behavior, and heart rate data, we trained individual, participant-specific forests of extremely randomized trees differentiating between low and high cognitive load. We achieved an average classification accuracy of 72%. We then apply these participant-specific classifiers in a novel way, using similarity between participants, normalization, and relative importance of individual features to successfully achieve the same level of classification accuracy in cross-participant classification. These results indicate that a combination of behavioral and physiological indicators allows for reliable prediction of cognitive load in an emergency simulation game, opening up new avenues for adaptivity and interaction. Â© 2019 Copyright held by the owner/author(s).",,,
10.1145/3351529.3360655,2019,"Krishna V., Ding Y., Xu A., HÃ¶llerer T.",Multimodal Biometric Authentication for VR/AR using EEG and Eye Tracking,"Electroencephalogram (EEG) signals can enable an additional non-intrusive input modality especially when paired with a wearable headset (i.e. AR/VR). A great challenge in using EEG data for Brain-Computer Interface (BCI) algorithms is its poor generalization performance across users. Taking advantage of these inter-user differences, we investigate the potential in using this technology for user authentication â€?similar to facial recognition in smartphones. Additionally, we evaluate this in combination with eye tracking data which is also readily available in such headsets. We develop a biometric authentication systems for each of these systems and for their fusion. We formulate a novel evaluation paradigm using publicly available EEG motor imagery and eye tracking data and demonstrate strong feasibility towards using EEG and eye tracking for authentication. Â© 2019 Copyright held by the owner/author(s).",,,
10.1109/CogInfoCom47531.2019.9089993,2019,"Babicsne-Horvath M., Hercegfi K.",Early Results of a Usability Evaluation of Two Digital Human Model-based Ergonomic Software Applying Eye-Tracking Methodology Comparison of the usability of ViveLab and Jack software,"Analysis software for ergonomics are more and more wide spread among researchers. There are various software for ergonomics available on the market, and it would be important to know which one to choose in various research tasks. Although data on the capabilities of the software can be found relatively easily, a comparison regarding their ease of use and the quality of their user interface cannot be found in the literature. In this article, the usability of two software were compared. A cloud based Hungarian software, ViveLab and the well-known Jack software was chosen. In addition to the traditional software usability testing technique based on screen and event recording and user camera, eye-Tracking methodology was also applied. The goal of this research is to find out which software is more usable in different situations. The results were divisive and in some cases astonishing. We are not able to give a definite answer which software is easier to use. There were several cases when ViveLab was easier and almost as much cases when Jack. There are areas for improvement in both software. Our results can help researchers to choose software for their specific tasks. Furthermore, the results can give additional information on which user interface elements and concepts are worth to improve by developers of the ergonomic software, and, in some cases, may inspire developers of other 3D-based software in general. Â© 2019 IEEE.",,,
10.1109/CogInfoCom47531.2019.9089922,2019,"Ujbanyi T., Stankov G., Nagy B.",Eye tracking based usability evaluation of the MaxWhere virtual space in a search task,"The generations of the past decades are addicted to the information and the internet. The ICT devices determine their everyday lives. Several studies suggest that such tools are needed to teach these generations effectively, to make the learning process easier and quicker. The corresponding facts must be chosen, filtered and utilized from a constantly growing set of information by the students. They had to think and then Figure out the answer from these information. The ability of algorithmic thinking, the development of analytical thinking, and the promotion of thinking in the system play a key role. The involvement of 3D virtual spaces in education is investigated by more and more studies nowadays. In this article, through an example of a search task, an eye-Tracking system is used to find out how much a 3D virtual space is more effective than a conventional LMS system. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00156,2019,"Liu Z., Chen Z., Bai J., Li S., Lian S.",Facial pose estimation by deep learning from label distributions,"Facial pose estimation has gained a lot of attentions in many practical applications, such as human-robot interaction, gaze estimation and driver monitoring. Meanwhile, end-to-end deep learning-based facial pose estimation is becoming more and more popular. However, facial pose estimation suffers from a key challenge: the lack of sufficient training data for many poses, especially for large poses. Inspired by the observation that the faces under close poses look similar, we reformulate the facial pose estimation as a label distribution learning problem, considering each face image as an example associated with a Gaussian label distribution rather than a single label, and construct a convolutional neural network which is trained with a multi-loss function on AFLW dataset and 300W-LP dataset to predict the facial poses directly from color image. Extensive experiments are conducted on several popular benchmarks, including AFLW2000, BIWI, AFLW and AFW, where our approach shows a significant advantage over other state-of-the-art methods. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00146,2019,"He J., Pham K., Valliappan N., Xu P., Roberts C., Lagun D., Navalpakkam V.",On-device few-shot personalization for real-time gaze estimation,"Building fast and accurate gaze estimation models without additional specialized hardware is a hard problem. In this paper, we present on-device few-shot personalization methods for 2D gaze estimation. The proposed supervised method achieves better accuracy using as few as 2-5 calibration points per user compared to prior methods that require more than 13 calibration points. In addition, we propose an unsupervised personalization method which uses only unlabeled facial images to improve gaze estimation accuracy. Our best personalized model achieves 24-26% better accuracy (measured by mean error) on phones compared to the state-of-the-art using <=5 calibration points per user. It is also computationally efficient, requiring 20x fewer FLOPS when compared to prior methods. This unlocks a variety of important real world applications such as using gaze for accessibility, gaming and human-computer interaction while running entirely on-device in real-time. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00450,2019,"Griffith H., Katrychuk D., Komogortsev O.",Assessment of shift-invariant CNN gaze mappings for PS-OG eye movement sensors,"Photosensor oculography (PS-OG) eye movement sensors offer desirable performance characteristics for integration within wireless head mounted devices (HMDs), including low power consumption and high sampling rates. To address the known performance degradation of these sensors due to HMD shifts, various machine learning techniques have been proposed for mapping sensor outputs to gaze location. This paper advances the understanding of a recently introduced convolutional neural network designed to provide shift invariant gaze mapping within a specified range of sensor translations. Performance is assessed for shift training examples which better reflect the distribution of values that would be generated through manual repositioning of the HMD during a dedicated collection of training data. The network is shown to exhibit comparable accuracy for this realistic shift distribution versus a previously considered rectangular grid, thereby enhancing the feasibility of in-field set-up. In addition, this work further demonstrates the practical viability of the proposed initialization process by demonstrating robust mapping performance versus training data scale. The ability to maintain reasonable accuracy for shifts extending beyond those introduced during training is also demonstrated. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00147,2019,"Cortacero K., Fischer T., Demiris Y.",RT-BENE: A dataset and baselines for real-time blink estimation in natural environments,"In recent years gaze estimation methods have made substantial progress, driven by the numerous application areas including human-robot interaction, visual attention estimation and foveated rendering for virtual reality headsets. However, many gaze estimation methods typically assume that the subject's eyes are open; for closed eyes, these methods provide irregular gaze estimates. Here, we address this assumption by first introducing a new open-sourced dataset with annotations of the eye-openness of more than 200,000 eye images, including more than 10,000 images where the eyes are closed. We further present baseline methods that allow for blink detection using convolutional neural networks. In extensive experiments, we show that the proposed baselines perform favourably in terms of precision and recall. We further incorporate our proposed RT-BENE baselines in the recently presented RT-GENE gaze estimation framework where it provides a real-time inference of the openness of the eyes. We argue that our work will benefit both gaze estimation and blink estimation methods, and we take steps towards unifying these methods. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00452,2019,"Boutros F., Damer N., Kirchbuchner F., Kuijper A.",Eye-MMS: Miniature multi-scale segmentation network of key eye-regions in embedded applications,"Segmentation of the iris or sclera is an essential processing block in ocular biometric systems. However, human-computer interaction, as in VR/AR applications, requires multiple region segmentation to enable smoother interaction and eye-tracking. Such application does not only demand highly accurate and generalizable segmentation, it requires such segmentation model to be appropriate for the limited computational power of embedded systems. This puts strict limits on the size of the deployed deep learning models. This work presents a miniature multi-scale segmentation network consisting of inter-connected convolutional modules. We present a baseline multi-scale segmentation network and modify it to reduce its parameters by more than 80 times, while reducing its accuracy by less than 3%, resulting in our Eye-MMS model containing only 80k parameters. This work is developed on the OpenEDS database and is conducted in preparation for the OpenEDS Semantic Segmentation Challenge. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00453,2019,"Perry J., Fernandez A.",MinENet: A dilated CNN for semantic segmentation of eye features,"Fast and accurate eye tracking is a critical task for a range of research in virtual and augmented reality, attention tracking, mobile applications, and medical analysis. While deep neural network models excel at image analysis tasks, existing approaches to segmentation often consider only one class, emphasize classification over segmentation, or come with prohibitively high resource costs. In this work, we propose MinENet, a minimized efficient neural network architecture designed for fast multi-class semantic segmentation. We demonstrate performance of MinENet on the OpenEDS Semantic Segmentation Challenge dataset, against a baseline model as well as standard state-of-the-art neural network architectures - a convolutional neural network (CNN) and a dilated CNN. Our encoder-decoder architecture improves accuracy of multi-class segmentation of eye features in this large-scale high-resolution dataset, while also providing a design that is demonstrably lightweight and efficient. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00451,2019,"Porta S., Bossavit B., Cabeza R., Larumbe-Bergera A., Garde G., Villanueva A.",U2Eyes: A binocular dataset for eye tracking and gaze estimation,"Theory shows that huge amount of labelled data are needed in order to achieve reliable classification/regression methods when using deep/machine learning techniques. However, in the eye tracking field, manual annotation is not a feasible option due to the wide variability to be covered. Hence, techniques devoted to synthesizing images show up as an opportunity to provide vast amounts of annotated data. Considering that the well-known UnityEyes tool provides a framework to generate single eye images and taking into account that both eyes information can contribute to improve gaze estimation accuracy we present U2Eyes dataset, that is publicly available. It comprehends about 6 million of synthetic images containing binocular data. Furthermore, the physiology of the eye model employed is improved, simplified dynamics of binocular vision are incorporated and more detailed 2D and 3D labelled data are provided. Additionally, an example of application of the dataset is shown as work in progress. Employing U2Eyes as training framework Supervised Descent Method (SDM) is used for eyelids segmentation. The model obtained as result of the training process is then applied on real images from GI4E dataset showing promising results. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00455,2019,"Wu Z., Rajendran S., Van As T., Badrinarayanan V., Rabinovich A.",EyeNet: A multi-task deep network for off-axis eye gaze estimation,"Eye gaze estimation is a crucial component in Virtual and Mixed Reality. In head-mounted VR/MR devices the eyes are imaged off-axis to avoid blocking the user's gaze, this view-point makes drawing eye related inferences very challenging. In this work, we present EyeNet, the first single deep neural network which solves multiple heterogeneous tasks related to eye gaze estimation for an off-axis camera setting. The tasks include eye segmentation, IR LED glints detection, pupil and cornea center estimation. We benchmark all tasks on MagicEyes, a large and new dataset of 587 subjects with varying morphology, gender, skin-color, make-up and imaging conditions. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00545,2019,"Maekawa Y., Akai N., Hirayama T., Morales L.Y., Deguchi D., Kawanishi Y., Ide I., Murase H.",An analysis of how driver experience affects eye-gaze behavior for robotic wheelchair operation,"Drivers obtain information on surrounding environment using their eyesights. Experienced eye-gaze behavior is needed when driving at places where multiple risks exist to prepare for and avoid them. In this work, we analyze the change in eye-gaze behavior in such situations while a driver gains experience on the operation of a robotic wheelchair. Accurate distance information in the traffic environment is important to analyze the eye-gaze behavior. However, almost all previous works analyze eye-gaze behavior in a 2D environment, so they could not obtain accurate distance information. For this reason, we analyze eye-gaze behavior in 3D space. Concretely, we developed a novel eye-gaze behavior analysis platform based on a robotic wheelchair and estimated the driver's attention in 3D space. We try to analyze the eye-gaze behavior considering a useful field-of-view in 3D space based on the distance information instead of only the fixation point to investigate the objects that a driver implicitly pays attention to and from where s/he focuses on them. Results show that novice drivers pay attention to a single risk at a time. In contrast, they pay more attention to multiple risks simultaneously as they gain experience. Additionally, we discuss what features are effective to model the eye-gaze behavior based on the results. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00145,2019,"Linden E., Sjostrand J., Proutiere A.",Learning to personalize in appearance-based gaze tracking,"Personal variations severely limit the performance of appearance-based gaze tracking. Adapting to these variations using standard neural network model-adaption methods is difficult. The problems range from overfitting, due to small amounts of training data, to underfitting, due to restrictive model architectures. We tackle these problems by introducing SPatial Adaptive GaZe Estimator (SPAZE ). By modeling personal variations as a low-dimensional latent parameter space, SPAZE provides just enough adaptability to capture the range of personal variations without being prone to overfitting. Calibrating SPAZE for a new person reduces to solving a small and simple optimization problem. SPAZE achieves an error of 2.70 degrees on the MPIIGaze dataset, improving on the state-of-the-art by 14 %. We contribute to gaze tracking research by empirically showing that personal variations are well-modeled as a 3-dimensional latent parameter space for each eye. We show that this low-dimensionality is expected by examining model-based approaches to gaze tracking. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00148,2019,"Chang Z., DI Martino J.M., Qiu Q., Espinosa S., Sapiro G.",Salgaze: Personalizing gaze estimation using visual saliency,"Traditional gaze estimation methods typically require explicit user calibration to achieve high accuracy. This process is cumbersome and recalibration is often required when there are changes in factors such as illumination and pose. To address this challenge, we introduce SalGaze, a framework that utilizes saliency information in the visual content to transparently adapt the gaze estimation algorithm to the user without explicit user calibration. We design an algorithm to transform a saliency map into a differentiable loss map that can be used for the optimization of CNN-based models. SalGaze is also able to greatly augment standard point calibration data with implicit video saliency calibration data using a unified framework. We show accuracy improvements over 24% using our technique on existing methods. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00144,2019,"Guo T., Liu Y., Zhang H., Liu X., Kwak Y., Yoo B.I., Han J.-J., Choi C.",A generalized and robust method towards practical gaze estimation on smart phone,"Gaze estimation for ordinary smart phone, e.g. estimating where the user is looking at on the phone screen, can be applied in various applications. However, the widely used appearance-based CNN methods still have two issues for practical adoption. First, due to the limited dataset, gaze estimation is very likely to suffer from over-fitting, leading to poor accuracy at run time. Second, the current methods are usually not robust, i.e. their prediction results having notable jitters even when the user is performing gaze fixation, which degrades user experience greatly. For the first issue, we propose a new tolerant and talented (TAT) training scheme, which is an iterative random knowledge distillation framework enhanced with cosine similarity pruning and aligned orthogonal initialization. The knowledge distillation is a tolerant teaching process providing diverse and informative supervision. The enhanced pruning and initialization is a talented learning process prompting the network to escape from the local minima and re-born from a better start. For the second issue, we define a new metric to measure the robustness of gaze estimator, and propose an adversarial training based Disturbance with Ordinal loss (DwO) method to improve it. The experimental results show that our TAT method achieves state-of-the-art performance on GazeCapture dataset, and that our DwO method improves the robustness while keeping comparable accuracy. Â© 2019 IEEE.",,,
10.1109/ICCVW.2019.00139,2019,"Chen Z., Deng D., Pi J., Shi B.E.",Unsupervised outlier detection in appearance-based gaze estimation,"Appearance-based gaze estimation maps RGB images to estimates of gaze directions. One problem in gaze estimation is that there always exist low-quality samples (outliers) in which the eyes are barely visible. These low-quality samples are mainly caused by blinks, occlusions (e.g. by eye glasses), blur (e.g. due to motion) and failures of the eye landmark detection. Training on these outliers degrades the performance of gaze estimators, since they have no or limited information about gaze directions. It is also risky to give estimates based on these images in real-world applications, as these estimates may be unreliable. To solve this problem, we propose an algorithm that detects outliers without supervision. Based on the input images with only gaze labels, the proposed algorithm learns to predict a gaze estimates and an additional confidence score, which alleviates the impact of outliers during learning. We evaluated this algorithm on the MPIIGaze dataset and on an internal dataset. In cross-subject evaluation, our experimental results show that the proposed algorithm results in a better gaze estimator (8% improvement). The proposed algorithm is also able to reliably detect outliers during testing, with a precision of 0.71 when the recall is 0.63. Â© 2019 IEEE.",,,
10.1109/ICCV.2019.00701,2019,"Kellnhofer P., Recasens A., Stent S., Matusik W., Torralba A.",Gaze360: Physically unconstrained gaze estimation in the wild,"Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale remote gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models will be made available at http://gaze360.csail.mit.edu. Â© 2019 IEEE.",,,
10.1109/ICCV.2019.00703,2019,"He Z., Spurr A., Zhang X., Hilliges O.",Photo-realistic monocular gaze redirection using generative adversarial networks,"Gaze redirection is the task of changing the gaze to a desired direction for a given monocular eye patch image. Many applications such as videoconferencing, films, games, and generation of training data for gaze estimation require redirecting the gaze, without distorting the appearance of the area surrounding the eye and while producing photo-realistic images. Existing methods lack the ability to generate perceptually plausible images. In this work, we present a novel method to alleviate this problem by leveraging generative adversarial training to synthesize an eye image conditioned on a target gaze direction. Our method ensures perceptual similarity and consistency of synthesized images to the real images. Furthermore, a gaze estimation loss is used to control the gaze direction accurately. To attain high-quality images, we incorporate perceptual and cycle consistency losses into our architecture. In extensive evaluations we show that the proposed method outperforms state-of-the-art approaches in terms of both image quality and redirection precision. Finally, we show that generated images can bring significant improvement for the gaze estimation task if used to augment real training data. Â© 2019 IEEE.",,,
10.1109/ICCV.2019.00888,2019,"Berga D., Vidal X.R.F., Otazu X., Pardo X.M.",SID4VAM: A benchmark dataset with synthetic images for visual attention modeling,"A benchmark of saliency models performance with a synthetic image dataset is provided. Model performance is evaluated through saliency metrics as well as the influence of model inspiration and consistency with human psychophysics. SID4VAM is composed of 230 synthetic images, with known salient regions. Images were generated with 15 distinct types of low-level features (e.g. orientation, brightness, color, size...) with a target-distractor pop-out type of synthetic patterns. We have used Free-Viewing and Visual Search task instructions and 7 feature contrasts for each feature category. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation. This study proposes a new way to evaluate saliency models in the forthcoming literature, accounting for synthetic images with uniquely low-level feature contexts, distinct from previous eye tracking image datasets. Â© 2019 IEEE.",,,
10.23919/ICCAS47443.2019.8971669,2019,"Honda T., Matsunaga N., Okajima H.",Hybrid Steering Model depending on Driver's Gazing Point to detect inattentive driving using Machine Learning,"The modeling of driving behaviors is important to analyze and design comfortable functions for the driving systems. The estimation method of the non-linear steering model using the eye tracking information was studied using the heuristic search algorithm. However, the model was limited to gaze and the inattentive driving was not modeled. It is considered that the steering model is consists of a hybrid steering model that switches the controllers according to the eye tracking information is classified in the effective/peripheral viewing field. In this paper, an estimation method of the hybrid system focusing on the effective visual field during driving is proposed. The hybrid model is constructed by steering model depending on the gazing distance and simple on-off controller. This estimation algorithm consists of 2-steps; clustering which classifies data by k-means method and the estimation of the parameters by Particle Swarm Optimization. The experiment with the long driving course consisting of five-curves and straight lines is demonstrated by HONDA driving simulator. Â© 2019 Institute of Control, Robotics and Systems - ICROS.",,,
10.1109/CONIITI48476.2019.8960864,2019,"Carrillo C.-A., Bolivar H., Chaves M.L.",Methodology of Neuromarketing in Websites Analysis Approach,"According to Salesforce of 6,421 customers who bought Web pages during 2016 at the end of 2017, only 2,917 remained, corresponding to a 54% decrease. When tracking 850 clients to know the causes of dissatisfaction with the product of Web pages, it was found that 35.92% of the clients surveyed indicated that the design of their website was not appropriate for their business, this fact allowed to identify a set of possible elements or factors that cause the failure of the web pages, as: not identify potential clients, not identify the final objective, web pages without Consistency, think that appearance is the only thing that matters and optimize before getting traffic. Based on the techniques of Neuromarketing and Eye Tracking a methodology was developed to obtain an indicator that allows to measure the level of empathy that a user presents with a website, studying the moments in which the neuronal stimuli when interacting with the different elements of the site The web and the visual route present a wave behavior similar to those of the emotions, in order to solve the problems that exist in the world of web design on what factors are determinant in the presentation, usability and design. Â© 2019 IEEE.",,,
10.1109/BIBE.2019.00145,2019,"Asvestopoulou T., Manousaki V., Psistakis A., Nikolli E., Andreadakis V., Aslanides I.M., Pantazis Y., Smyrnakis I., Papadopouli M.",Towards a robust and accurate screening tool for dyslexia with data augmentation using GANs,"Eye movements during text reading can provide insights about reading disorders. We developed the DysLexML, a screening tool for developmental dyslexia, based on various ML algorithms that analyze gaze points recorded via eye-tracking during silent reading of children. We comparatively evaluated its performance using measurements collected from two systematic field studies with 221 participants in total. This work presents DysLexML and its performance. It identifies the features with prominent predictive power and performs dimensionality reduction. Specifically, it achieves its best performance using linear SVM, with an accuracy of 97% and 84% respectively, using a small feature set. We show that DysLexML is also robust in the presence of noise. These encouraging results set the basis for developing screening tools in less controlled, larger-scale environments, with inexpensive eye-trackers, potentially reaching a larger population for early intervention. Unlike other related studies, DysLexML achieves the aforementioned performance by employing only a small number of selected features, that have been identified with prominent predictive power. Finally, we developed a new data augmentation/substitution technique based on GANs for generating synthetic data similar to the original distributions. Â© 2019 IEEE.",,,
10.1109/BIBE.2019.00071,2019,"Dimas G., Iakovidis D., Koulaouzidis A.",MedGaze: Gaze Estimation on WCE Images Based on a CNN Autoencoder,"The interpretation of medical images depends on physicians' experience. Over time, physicians develop their ability to examine the images, and this is usually reflected on gaze patterns they follow to observe visual cues, which lead them to diagnostic decisions. In the context of gaze prediction, graph and machine learning methods have been proposed for the visual saliency estimation on generic images. In this work we preset a novel and robust gaze estimation methodology based on physicians' eye fixations, using convolutional neural networks combined with regularization methods, on medical images taken during Wireless Capsule Endoscopy (WCE). Furthermore, we present a novel dataset of physicians' eye fixation patterns which was used for the training of the neural network model. The model was able to achieve 68.5% Judd's Area Under the receiver operating Characteristic (AUC-J). Â© 2019 IEEE.",,,
10.1109/ISMAR.2019.00-10,2019,"Merenda C., Suga C., Gabbard J.L., Misu T.",Effects of 'real-world' visual fidelity on ar interface assessment: A case study using ar head-up display graphics in driving,"Recent AR research efforts have explored the use of virtual envi-ronments to test augmented reality (AR) user interfaces. However, it is yet to be seen what effects the visual fidelity of such virtual environments may have on AR interface assessment, and specifical-ly to what degree assessment results observed in a virtual world would apply to the real world. Automotive AR head-up (HUD) interfaces provide a meaningful application area to examine this problem, especially given that immersive, 3D-graphics-based driving simulators are established tools to examine in-vehicle interfaces safely before testing in real vehicles. In this work, we present an argument that adequately assessing AR interfaces requires a suite of different measures, and that such measures should be considered when debating the appropriateness of virtual environments for AR interface assessment. We present a case study that examines how an AR interface presented via HUD effects driver performance and behavior in different virtual and real environments. Twelve partici-pants completed the study measuring driver task performance, eye gaze behavior and situational awareness during AR guided navigation in low-and high-fidelity virtual simulation, and an on-road environment. Our results suggest that the visual fidelity of the envi-ronmental in which an AR interface is assessed, could impact some measures of effectiveness. Discussion is guided by a proposed initial assessment classification for AR user interfaces that may serve to guide future discussions on AR interface evaluation, as well as the suitability of virtual environments for AR assessment. Â© 2019 IEEE.",,,
10.1109/ISMAR.2019.00031,2019,"Souchet A., Philippe S., Ober F., Leveque A., Leroy L.",Investigating cyclical stereoscopy effects over visual discomfort and fatigue in virtual reality while learning,"Purpose: It is hypothesized that cyclical stereoscopy (displaying stereoscopy or 2D cyclically) has effect over visual fatigue, learning curves and quality of experience, and that those effects are different from regular stereoscopy. Materials and Methods: 59 participants played a serious game simulating a job interview with a Samsung Gear VR Head Mounted Display (HMD). Participants were randomly assigned to 3 groups: HMD with regular stereoscopy (S3D) and HMD with cyclical stereoscopy (cycles of 1 or 3 minutes). Participants played the game thrice (third try on a PC one month later). Visual discomfort, Flow, Presence, were measured with questionnaires. Visual Fatigue was assessed pre-and post-exposure with optometric measures. Learning traces were obtained in-game. Results: Visual discomfort and flow are lower with cyclical-S3D than S3D but not Presence. Cyclical stereoscopy every 1 minute is more tiring than stereoscopy. Cyclical stereoscopy every 3 minutes tends to be more tiring than stereoscopy. Cyclical stereoscopy groups improved during Short-Term Learning. None of the statistical tests showed a difference between groups in either Short-Term Learning or Long-Term Learning curves. Conclusion: cyclical stereoscopy displayed cyclically had a positive impact on Visual Comfort and Flow, but not Presence. It affects oculomotor functions in a HMD while learning with a serious game with low disparities and easy visual tasks. Other visual tasks should be tested, and eye-tracking should be considered to assess visual fatigue during exposure. Results in ecological conditions seem to support models suggesting that activating cyclically stereopsis in a HMD is more tiring than maintaining it. Â© 2019 IEEE.",,,
10.1109/CW.2019.00044,2019,"Liu Y., Li F., Tang L.H., Lan Z., Cui J., Sourina O., Chen C.-H.",Detection of humanoid robot design preferences using EEG and eye tracker,"Currently, many modern humanoid robots have little appeal due to their simple designs and bland appearances. To provide recommendations for designers and improve the designs of humanoid robots, a study of human's perception on humanoid robot designs is conducted using Electroencephalogram (EEG), eye tracking information and questionnaires. We proposed and carried out an experiment with 20 subjects to collect the EEG and eye tracking data to study their reaction to different robot designs and the corresponding preference towards these designs. This study can possibly give us some insights on how people react to the aesthetic designs of different humanoid robot models and the important traits in a humanoid robot design, such as the perceived smartness and friendliness of the robots. Another point of interest is to investigate the most prominent feature of the robot, such as the head, facial features and the chest. The result shows that the head and facial features are the focus. It is also discovered that more attention is paid to the robots that appear to be more appealing. Lastly, it is affirmed that the first impressions of the robots generally do not change over time, which may imply that a good humanoid robot design impress the observers at first sight. Â© 2019 IEEE.",,,
10.1109/BRACIS.2019.00074,2019,"Klein Salvalaio B., De Oliveira Ramos G.",Self-adaptive appearance-based eye-tracking with online transfer learning,"Eye-tracking plays a role in human-computer interactions and has proven useful in a wide variety of domains. We consider appearance-based eye-tracking, where one tracks eye movements based solely on conventional images (rather than on sophisticated additional hardware). Recent advances made in Deep Learning and, in particular, convolutional neural networks have allowed appearance-based eye-tracking to achieve better results than ever. However, current literature still lacks methods that generalize to different combinations of user, environment and device. In this work, we introduce Online Deep Appearance-Based Eye-Tracking (ODABE), which overcomes such a limitation by considering online transfer learning, thus enabling eye-tracking models to self-adapt to different context very rapidly. Our results show that ODABE improves upon previous research when context changes, decreasing the prediction error by 50.95% on average, on tested cases. Â© 2019 IEEE.",,,
10.1109/ICSEngT.2019.8906419,2019,"Heng S.G., Samad R., Mustafa M., Abdullah N.R.H., Pebrianti D.",Analysis of performance between kinect V1 and kinect V2 for various facial part movements,"The aim of this study is to determine the suitable version of Kinect motion sensor for developing facial therapy or exercise throughout the analysis of face tracking performance. A face tracking system is developed in both version of Kinect cameras by referring to the respective version of Kinect SDK. The created face tracking algorithms are then modified to display the detected facial points which are 121 points and 1347 points in total for Kinect v1 and Kinect v2 respectively. A total number of 18 desired facial feature point at similar landmarks will be extracted in the format of 3D coordinates for both Kinect cameras. To investigate the changes in the movement of facial feature points, the points will be paired up for distance ratio calculation between different frames of face image. The action unit of facial points for both Kinect cameras are different and there are some improvements in Kinect v2: asymmetrical facial points, high definition face detection and eye tracking as added action units. However, it shows poor detection in outer eyebrow part compared to Kinect v1. In overall, the Kinect v2 has better performance than Kinect v1 as it provides faster response speed and more detailed facial points movement detection in real-time operation for rehabilitation purposes. Â© 2019 IEEE.",,,
10.3390/s19194332,2019,"Opromolla R., Inchingolo G., Fasano G.",Airborne visual detection and tracking of cooperative UAVs exploiting deep learning,"The performance achievable by using Unmanned Aerial Vehicles (UAVs) for a large variety of civil and military applications, as well as the extent of applicable mission scenarios, can significantly benefit from the exploitation of formations of vehicles able to fly in a coordinated manner (swarms). In this respect, visual cameras represent a key instrument to enable coordination by giving each UAV the capability to visually monitor the other members of the formation. Hence, a related technological challenge is the development of robust solutions to detect and track cooperative targets through a sequence of frames. In this framework, this paper proposes an innovative approach to carry out this task based on deep learning. Specifically, the You Only Look Once (YOLO) object detection system is integrated within an original processing architecture in which the machine-vision algorithms are aided by navigation hints available thanks to the cooperative nature of the formation. An experimental flight test campaign, involving formations of two multirotor UAVs, is conducted to collect a database of images suitable to assess the performance of the proposed approach. Results demonstrate high-level accuracy, and robustness against challenging conditions in terms of illumination, background and target-range variability. Â© 2019 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1016/j.media.2019.07.002,2019,"Du X., Allan M., Bodenstedt S., Maier-Hein L., Speidel S., Dore A., Stoyanov D.",Patch-based adaptive weighting with segmentation and scale (PAWSS) for visual tracking in surgical video,"Vision-based tracking in an important component for building computer assisted interventions in minimally invasive surgery as it facilitates estimation of motion for instruments and anatomical targets. Tracking-by-detection algorithms are widely used for visual tracking, where the problem is treated as a classification task and a tracking target appearance model is updated over time using online learning. In challenging conditions, like surgical scenes, where tracking targets deform and vary in scale, the update step is prone to include background information in model appearance or to lack the ability to estimate change of scale, which degrades the performance of classifier. In this paper, we propose a Patch-based Adaptive Weighting with Segmentation and Scale (PAWSS) tracking framework that tackles both scale and background problems. A simple but effective colour-based segmentation model is used to suppress background information and multi-scale samples are extracted to enrich the training pool, which allows the tracker to handle both incremental and abrupt scale variations between frames. Experimentally, we evaluate our approach on Online Tracking Benchmark (OTB) dataset and Visual Object Tracking (VOT) challenge datasets, showing that our approach outperforms recent state-of-the-art trackers, and it especially improves successful rate score on OTB dataset, while on VOT datasets, PAWSS ranks among the top trackers while operating at real-time frame rates. Focusing on the application of PAWSS to surgical scenes, we evaluate on MICCAI 2015 challenge instrument tracking challenge and in vivo datasets, showing that our approach performs the best among all submitted methods and also has promising performance on in vivo surgical instrument tracking. Â© 2019 The Authors",,,
10.1016/j.aei.2019.100940,2019,"Li F., Lee C.-H., Chen C.-H., Khoo L.P.",Hybrid data-driven vigilance model in traffic control center using eye-tracking data and context data,"Vigilance decrement of traffic controllers would greatly threaten public safety. Hence, extensive studies have been conducted to establish the physiological data-based vigilance model for objectively monitoring or detecting vigilance decrement. Nevertheless, most of them using intrusive devices to collect physiological data and failed to consider context information. Consequently, these models can be used in a laboratory environment while cannot adapt to dynamic working conditions of traffic controllers. The goal of this research is to develop an adaptive vigilance model for monitoring vigilance objectively and non-intrusively. In recent years, with advanced information and communication technology, a massive amount of data can be collected from connected daily use items. Hence, we proposed a hybrid data-driven approach based on connected objects for establishing vigilance model in the traffic control center and provide an elaborated case study to illustrate the method. Specifically, eye movements are selected as the primary inputs of the proposed vigilance model; Bagged trees technique is adapted to generate the vigilance model. The results of case study indicated that (1) eye metrics would be correlated with the vigilance performance subjected to the mental fatigue levels, (2) the bagged trees with the fusion features as inputs achieved a relatively stable performance under the condition of data loss, (3) the proposed method could achieve better performance than the other classic machine learning methods. Â© 2019 Elsevier Ltd",,,
10.1016/j.ijinfomgt.2019.02.003,2019,"Giannakos M.N., Sharma K., Pappas I.O., Kostakos V., Velloso E.",Multimodal data as a means to understand the learning experience,"Most work in the design of learning technology uses click-streams as their primary data source for modelling & predicting learning behaviour. In this paper we set out to quantify what, if any, advantages do physiological sensing techniques provide for the design of learning technologies. We conducted a lab study with 251 game sessions and 17 users focusing on skill development (i.e., user's ability to master complex tasks). We collected click-stream data, as well as eye-tracking, electroencephalography (EEG), video, and wristband data during the experiment. Our analysis shows that traditional click-stream models achieve 39% error rate in predicting learning performance (and 18% when we perform feature selection), while for fused multimodal the error drops up to 6%. Our work highlights the limitations of standalone click-stream models, and quantifies the expected benefits of using a variety of multimodal data coming from physiological sensing. Our findings help shape the future of learning technology research by pointing out the substantial benefits of physiological sensing. Â© 2019 Elsevier Ltd",,,
10.1145/3364335.3364360,2019,"Widyantara P.B., Puspasari M.A.",Breakpoint of attention media evaluation as countermeasure for computer vision syndrome,"Computer Vision Syndrome (CVS) is a complex problem in the eyes and vision that is related to computer use. Approximately 60% reduction in the frequency of blinking during computer use increases the risk of dry eyes and other symptoms related to CVS. There is a need from ergonomics interventions as preventive strategy to CVS risk; one of them is breakpoint of attention media that can trigger the appearance of regular blinks. This study aimed to evaluate the breakpoint of attention media in visual form (which is represented by blink-blink application) and audio (which is represented by metronome application) as CVS countermeasures. Data retrieval was conducted using an eye-tracker for 30 participants. The measurement variables used were the overall blink rate, the frequency of blink duration > 500 ms (indicated as microsleep propensity), and ocular symptoms questionnaire. The results of this study indicate that the blink-blink application is statistically better than metronome in increasing the overall increasing blink frequency, reducing microsleep propensity and from subjective score of ocular symptoms. Â© 2019 Association for Computing Machinery.",,,
10.1145/3349263.3349597,2019,"Hildebrandt M., Langstrand J.-P., Nguyen H.T.",Synopticon: A real-time data fusion platform for behavioral research,"Synopticon is a collection of tools for managing complex, multi-sensory data streams when conducting behavioral research in simulators or on the road. Synopticonâ€™s functionality includes automatic gaze object detection, multi-camera synchronization, camera-sensor synchronization (e.g. physiological sensors), camera-simulator synchronization, and support for computer vision and machine learning. Â© 2019 Copyright is held by the owner/author(s).",,,
10.1145/3343036.3343121,2019,"Keyvanara M., Allison R.",Transsaccadic awareness of scene transformations in a 3D virtual environment,"In gaze-contingent displays, the viewer's eye movement data are processed in real-time to adjust the graphical content. To provide a high-quality user experience, these graphical updates must occur with minimum delay. Such updates can be used to introduce imperceptible changes in virtual camera pose in applications such as networked gaming, collaborative virtual reality and redirected walking. For such applications, perceptual saccadic suppression can help to hide the graphical artifacts. We investigated whether the visibility of these updates depends on the type of image transformation. Users viewed 3D scenes in which the displacement of a target object triggered them to generate a vertical or horizontal saccade, during which a translation or rotation was applied to the virtual camera used to render the scene. After each trial, users indicated the direction of the scene change in a forced-choice task. Results show that type and size of the image transformation affected change detectability. During horizontal or vertical saccades, rotations along the roll axis were the most detectable, while horizontal and vertical translations were least noticed. We confirm that large 3D adjustments to the scene viewpoint can be introduced unobtrusively and with low latency during saccades, but the allowable extent of the correction varies with the transformation applied. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3343036.3343118,2019,"Volonte M., Anaraky R.G., Knijnenburg B.P., Duchowski A.T., Babu S.V.",Empirical evaluation of the interplay of emotion and visual attention in human-virtual human interaction,"We examined the effect of rendering style and the interplay between attention and emotion in users during interaction with a virtual patient in a medical training simulator. The virtual simulation was rendered representing a sample from the photo-realistic to the non-photorealistic continuum, namely Near-Realistic, Cartoon or Pencil-Shader. In a mixed design study, we collected 45 participants' emotional responses and gaze behavior using surveys and an eye tracker while interacting with a virtual patient who was medically deteriorating over time. We used a cross-lagged panel analysis of attention and emotion to understand their reciprocal relationship over time. We also performed a mediation analysis to compare the extent to which the virtual agent's appearance and his affective behavior impacted users' emotional and attentional responses. Results showed the interplay between participants' visual attention and emotion over time and also showed that attention was a stronger variable than emotion during the interaction with the virtual human. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3341162.3343811,2019,"Landsmann M., Augereau O., Kise K.",Classification of reading and not reading behavior based on eye movement analysis,"Nowadays, many researchers analyze reading behavior with eye trackers. Various traits of reading like engagement, or text difficulty have been observed in laboratory settings. But, their automatic application for daily life is usually prevented by one question: when is somebody reading? We have developed a tool to classify short sequences of fixations from eye gaze data into reading and not reading. Our specific use case is the Vocabulometer, a website for learning English by reading texts. We used supervised learning on data from nonnative English speakers to train decision trees for the classification. With features based on vertical eye movement, we achieved 93.1% of correct classifications. Â© 2019 Copyright held by the owner/author(s).",,,
10.1145/3341162.3349308,2019,Khan A.A.,Gaze assisted voice note taking system,[æ— å¯ç”¨æ‘˜è¦],,,
10.1145/3341162.3348385,2019,"Breitenfellner M., Jungwirth F., Ferscha A.",Towards 3D smooth pursuit interaction,"In this position paper, we encourage the use of novel 3D gaze tracking possibilities in the field of gaze-based interaction. Smooth pursuit offers great benefits over other gaze interaction approaches, like the ability to work with uncalibrated eye trackers, but also has disadvantages like the produced visual clutter in more complex user interfaces. We examine the basic concept of smooth pursuits, its hardware and algorithmic requirements and how this can be applied to real world problems. Then we evaluate how the recent change in availability of 3D eye tracking hardware can be used to approach the challenges of 2D smooth pursuit interaction. We take a look at different research opportunities, show concrete ideas and discuss why they are relevant for future research. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1109/BTAS46853.2019.9185980,2019,"Trokielewicz M., Czajka A., Maciejewicz P.",Perception of Image Features in Post-Mortem Iris Recognition: Humans vs Machines,"Post-mortem iris recognition can offer an additional forensic method of personal identification. However, in contrary to already well-established human examination of fingerprints, making iris recognition human-interpretable is harder, and therefore it has never been applied in forensic proceedings. There is no strong consensus among biometric experts which iris features, especially those in iris images acquired post-mortem, are the most important for human experts solving an iris recognition task. This paper explores two ways of broadening this knowledge: (a) with an eye tracker, the salient features used by humans comparing iris images on a screen are extracted, and (b) class-activation maps produced by the convolutional neural network solving the iris recognition task are analyzed. Both humans and deep learning-based solutions were examined with the same set of iris image pairs. This made it possible to compare the attention maps and conclude that (a) deep learning-based method can offer human-interpretable decisions backed by visual explanations pointing a human examiner to salient regions, and (b) in many cases humans and a machine used different features, what means that a deep learning-based method can offer a complementary support to human experts. This paper offers the first known to us human-interpretable comparison of machine-based and human-based post-mortem iris recognition, and the trained models annotating salient iris image regions. Â© 2019 IEEE.",,,
10.1109/CCEM48484.2019.000-1,2019,"Changwani A., Sarode T.",Low-Cost Eye Tracking for Foveated Rendering Using Machine Learning,"This paper outlines a $50 head-mounted real-time eye tracker to track the user's eye, and uses foveated rendering reduce bandwidth costs and improve immersion. This is accomplished by using two cameras for detecting the iris and locating the beacons around the screen which tracks head movement in three dimensions. A neural net is trained on this data which then predicts where the user is looking based on the inputs from the two cameras. Foveated rendering is performed by only rendering the area of the screen currently being focused and blurring the area that falls under peripheral vision. Maturing this technology could be integrated into virtual reality headsets and for other immersive media experiences while dramatically decreasing bandwidth costs and increasing the overall functionality and capabilities of the devices. This would allow for a more robust as well as wireless virtual reality experience, which would mitigate the largest drawback to mass adoption: clunky wires and lack of a truly realistic experience. Â© 2019 IEEE.",,,
10.1109/ACIIW.2019.8925291,2019,"Dipaola S., Yalcin O.N.",A multi-layer artificial intelligence and sensing based affective conversational embodied agent,"Building natural and conversational virtual humans is a task of formidable complexity. We believe that, especially when building agents that affectively interact with biological humans in real-time, a cognitive science-based, multilayered sensing and artificial intelligence (AI) systems approach is needed. For this demo, we show a working version (through human interaction with it) our modular system of natural, conversation 3D virtual human using AI or sensing layers. These including sensing the human user via facial emotion recognition, voice stress, semantic meaning of the words, eye gaze, heart rate, and galvanic skin response. These inputs are combined with AI sensing and recognition of the environment using deep learning natural language captioning or dense captioning. These are all processed by our AI avatar system allowing for an affective and empathetic conversation using an NLP topic-based dialogue capable of using facial expressions, gestures, breath, eye gaze and voice language-based two-way back and forth conversations with a sensed human. Our lab has been building these systems in stages over the years. Â© 2019 IEEE.",,,
10.1109/ICIP.2019.8803479,2019,"Alexiou E., Xu P., Ebrahimi T.",Towards Modelling of Visual Saliency in Point Clouds for Immersive Applications,"Modelling human visual attention is of great importance in the field of computer vision and has been widely explored for 3D imaging. Yet, in the absence of ground truth data, it is unclear whether such predictions are in alignment with the actual human viewing behavior in virtual reality environments. In this study, we work towards solving this problem by conducting an eye-tracking experiment in an immersive 3D scene that offers 6 degrees of freedom. A wide range of static point cloud models is inspected by human subjects, while their gaze is captured in real-time. The visual attention information is used to extract fixation density maps, that can be further exploited for saliency modelling. To obtain high quality fixation points, we devise a scheme that utilizes every recorded gaze measurement from the two eye-cameras of our set-up. The obtained fixation density maps together with the recorded gaze and head trajectories are made publicly available, to enrich visual saliency datasets for 3D models. Â© 2019 IEEE.",,,
10.1109/ICIP.2019.8803590,2019,"Alkabbany I., Ali A., Farag A., Bennett I., Ghanoum M., Farag A.",Measuring Student Engagement Level Using Facial Information,"In this paper, we propose a novel framework that measures the engagement level of students either in a class environment or in an e-learning environment. The proposed framework captures the user's video and tracks their faces' through the video's frames. Different features are extracted from the user's face e.g., facial fiducial points, head pose, eye gaze, learned features, etc. These features are then used to detect the Facial Action Coding System (FACS), which decomposes facial expressions in terms of the fundamental actions of individual muscles or groups of muscles (i.e., action units). The decoded action units (AU's) are then used to measures the student's willingness to participate in the learning process (i.e., behavioral engagement) and his/her emotional attitude towards learning (i.e., emotional engagement). This framework will allow the lecturer to receive a real-time feedback from facial features, gaze, and other body kinesics. The framework is robust and can be utilized in numerous applications including but not limited to the monitoring the progress of students with various degrees of learning disabilities, and the analysis of nerve palsy and its effects on facial expression and social interactions. Â© 2019 IEEE.",,,
10.23919/EUSIPCO.2019.8902786,2019,"Cristina S., Camilleri K.P.",Gaze tracking by joint head and eye pose estimation under free head movement,"Recent trends in the field of eye-gaze tracking have been shifting towards the estimation of gaze direction in everyday life settings, hence calling for methods that alleviate the constraints typically associated with existing methods, which limit their applicability in less controlled conditions. In this paper, we propose a method for eye-gaze estimation as a function of both eye and head pose components, without requiring prolonged user-cooperation prior to gaze estimation. Our method exploits the trajectories of salient feature trackers spread randomly over the face region for the estimation of the head rotation angles, which are subsequently used to drive a spherical eye-in-head rotation model that compensates for the changes in eye region appearance under head rotation. We investigate the validity of the proposed method on a publicly available data set. Â© 2019 IEEE",,,
10.1109/ETFA.2019.8868961,2019,"Porta M., Barboni A.",Strengthening Security in Industrial Settings: A Study on Gaze-Based Biometrics through Free Observation of Static Images,"As security becomes crucial in an increasing number of industrial contexts, the need arises for new ways to check or authenticate the identity of people. In this paper, we present a method that exploits gaze data to implement a soft biometric technique. Specifically, the user's gaze behavior is inspected during the unconstrained observation of different kinds of static images. The obtained results, achieved using a machine learning approach, are generally satisfying, although more experiments will be necessary to fully confirm the viability of the proposed method. Â© 2019 IEEE.",,,
10.1109/JAS.2019.1911684,2019,"Xia Y., Yu H., Wang F.-Y.",Accurate and robust eye center localization via fully convolutional networks,"Eye center localization is one of the most crucial and basic requirements for some human-computer interaction applications such as eye gaze estimation and eye tracking. There is a large body of works on this topic in recent years, but the accuracy still needs to be improved due to challenges in appearance such as the high variability of shapes, lighting conditions, viewing angles and possible occlusions. To address these problems and limitations, we propose a novel approach in this paper for the eye center localization with a fully convolutional network (FCN), which is an end-to-end and pixels-to-pixels network and can locate the eye center accurately. The key idea is to apply the FCN from the object semantic segmentation task to the eye center localization task since the problem of eye center localization can be regarded as a special semantic segmentation problem. We adapt contemporary FCN into a shallow structure with a large kernel convolutional block and transfer their performance from semantic segmentation to the eye center localization task by fine-tuning. Extensive experiments show that the proposed method outperforms the state-of-the-art methods in both accuracy and reliability of eye center localization. The proposed method has achieved a large performance improvement on the most challenging database and it thus provides a promising solution to some challenging applications. Â© 2014 Chinese Association of Automation.",,,
10.1109/MCE.2019.2923042,2019,"Corcoran P., Lemley J., Costache C., Varkarakis V.",Deep Learning for Consumer Devices and Services 2-AI Gets Embedded at the Edge,"The recent explosive growth of deep learning is enabling a new generation of intelligent consumer devices. Specialized deep learning inference now provides data analysis capabilities that once required an active cloud connection, while reducing latency and enhancing data privacy. This paper addresses current progress in Edge artificial intelligence (AI) technology in several consumer contexts including privacy, biometrics, eye gaze, driver monitoring systems, and more. New developments and challenges in edge hardware and emerging opportunities are identified. Our previous article, Deep learning for consumer devices and services, introduced many of the basics of deep learning and AI. In this paper, we explore the current paradigm shift of AI from the data center into CE devices-Edge-AI. Â© 2012 IEEE.",,,
10.1007/s13748-019-00177-z,2019,"Gite S., Agrawal H., Kotecha K.",Early anticipation of driverâ€™s maneuver in semiautonomous vehicles using deep learning,"Making machines to anticipate human action is a complex research problem. Some of the recent research studies on computer vision and assistive driving have reported that the anticipation of driverâ€™s action few seconds in advance is a challenging problem. These studies are based on the driverâ€™s head movement tracking, eye gaze tracking, and spatiotemporal interest points. The study is aimed to address an important question of how to anticipate a driverâ€™s action while driving and improve the anticipation time. The goal of this study is to review the existing deep learning framework for assistive driving. This paper differs from the existing solutions in two ways. First, it proposes a simplified framework using the driverâ€™s inside video data and develops a driverâ€™s movement tracking (DMT) algorithm. Majority of the existing state of the art is based on inside and outside features of the vehicles. Second, the proposed work tends to improve the image pattern recognition by introducing a fusion of spatiotemporal data points (STIPs) for movement tracking along with eye cuboids and then action anticipation by using deep learning. The proposed DMT algorithm tracks the driverâ€™s movement using STIPs from the input video. Also, a fast eye gaze algorithm tracks eye movements. The features extracted from STIP and eye gaze are fused and analyzed by a deep recurrent neural network to improve the prediction time, thereby giving a few extra seconds to anticipate the driverâ€™s correct action. The performance of the DMT algorithm is compared with the previous algorithms and found that DMT offers 30% improvement with regards to anticipating driverâ€™s action over two recently proposed deep learning algorithms. Â© 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",,,
10.1016/j.compmedimag.2019.05.007,2019,"Alkhatib M., Hafiane A., Vieyres P., Delbos A.",Deep visual nerve tracking in ultrasound images,"Ultrasound-guided regional anesthesia (UGRA) becomes a standard procedure in surgical operations and pain management, offers the advantages of nerve localization, and provides region of interest anatomical structure visualization. Nerve tracking presents a crucial step for practicing UGRA and it is useful and important to develop a tool to facilitate this step. However, nerve tracking is a very challenging task that anesthetists can encounter due to the noise, artifacts, and nerve structure variability. Deep-learning has shown outstanding performances in computer vision task including tracking. Many deep-learning trackers have been proposed, where their performance depends on the application. While no deep-learning study exists for tracking the nerves in ultrasound images, this paper explores thirteen most recent deep-learning trackers for nerve tracking and presents a comparative study for the best deep-learning trackers on different types of nerves in ultrasound images. We evaluate the performance of the trackers in terms of accuracy, consistency, time complexity, and handling different nerve situations, such as disappearance and losing shape information. Through the experimentation, certain conclusions were noted on deep learning trackers performance. Overall, deep-learning trackers provide good performance and show a comparative performance for tracking different kinds of nerves in ultrasound images. Â© 2019 Elsevier Ltd",,,
10.5194/isprs-archives-XLII-2-W15-177-2019,2019,"BocheÅ„ska A., Markiewicz J., ÅapiÅ„ski S.",THE COMBINATION of the IMAGE and RANGE-BASED 3D ACQUISITION in ARCHAEOLOGICAL and ARCHITECTURAL RESEARCH in the ROYAL CASTLE in WARSAW,"The paper presents archaeological and architectural research in the Royal Castle in Warsaw where a combination of image- and range-based 3D acquisition was applied. The area examined included excavations situated inside the Tower and near its outer western wall. The work was carried out at various periods and in different weather conditions. As part of the measurements, laser scanning was performed (with a Z+F 5006h scanner) and a series of close-range images were taken. It was important to integrate the data acquired to create a comprehensive documentation of archaeological excavations. When data was acquired from TLS together with photogrammetric data (in different measurement periods), the points' displacements were controlled and analysed. The process of orienting and processing the terrestrial images included photographs taken during the inventory of the tower (Canon 5D Mark II) and photographs provided by the Castle's employees (Canon PowerShot G5 X). Agisoft PhotoScan software was used to orient and process the terrestrial images, and LupoScan for the TLS data. In order to integrate the TLS data and the clouds of points from the photographs from the various stages, they were processed into a raster form; our own software (based on the OpenCV library and the Structure-from-Motion method) and LupoScan software were used to interconnect the multi-temporal and multi-sensor data sets. As a result of processing photographs and TLS data, point clouds in an external reference system were obtained. This data was then used to study the thickness of the walls of the Justice Court Tower, to analyse the course of the retaining wall, and to generate the orthoimages necessary for chronological analysis. Â© 2019 International Society for Photogrammetry and Remote Sensing. All rights reserved.",,,
10.1007/s10278-019-00220-4,2019,"Stember J.N., Celik H., Krupinski E., Chang P.D., Mutasa S., Wood B.J., Lignelli A., Moonis G., Schwartz L.H., Jambawalikar S., Bagci U.",Eye Tracking for Deep Learning Segmentation Using Convolutional Neural Networks,"Deep learning with convolutional neural networks (CNNs) has experienced tremendous growth in multiple healthcare applications and has been shown to have high accuracy in semantic segmentation of medical (e.g., radiology and pathology) images. However, a key barrier in the required training of CNNs is obtaining large-scale and precisely annotated imaging data. We sought to address the lack of annotated data with eye tracking technology. As a proof of principle, our hypothesis was that segmentation masks generated with the help of eye tracking (ET) would be very similar to those rendered by hand annotation (HA). Additionally, our goal was to show that a CNN trained on ET masks would be equivalent to one trained on HA masks, the latter being the current standard approach. Step 1: Screen captures of 19 publicly available radiologic images of assorted structures within various modalities were analyzed. ET and HA masks for all regions of interest (ROIs) were generated from these image datasets. Step 2: Utilizing a similar approach, ET and HA masks for 356 publicly available T1-weighted postcontrast meningioma images were generated. Three hundred six of these image + mask pairs were used to train a CNN with U-net-based architecture. The remaining 50 images were used as the independent test set. Step 1: ET and HA masks for the nonneurological images had an average Dice similarity coefficient (DSC) of 0.86 between each other. Step 2: Meningioma ET and HA masks had an average DSC of 0.85 between each other. After separate training using both approaches, the ET approach performed virtually identically to HA on the test set of 50 images. The former had an area under the curve (AUC) of 0.88, while the latter had AUC of 0.87. ET and HA predictions had trimmed mean DSCs compared to the original HA maps of 0.73 and 0.74, respectively. These trimmed DSCs between ET and HA were found to be statistically equivalent with a p value of 0.015. We have demonstrated that ET can create segmentation masks suitable for deep learning semantic segmentation. Future work will integrate ET to produce masks in a faster, more natural manner that distracts less from typical radiology clinical workflow. Â© 2019, The Author(s).",,,
10.1109/iCCECE46942.2019.8941966,2019,"Fasanmade A., Aliyu S., He Y., Al-Bayatti A.H., Sharif M.S., Alfakeeh A.S.",Context-aware driver distraction severity classification using LSTM network,"Advanced Driving Assistance Systems (ADAS) has been a critical component in vehicles and vital to the safety of vehicle drivers and public road transportation systems. In this paper, we present a deep learning technique that classifies drivers' distraction behaviour using three contextual awareness parameters: speed, manoeuver and event type. Using a video coding taxonomy, we study drivers' distractions based on events information from Regions of Interest (RoI) such as hand gestures, facial orientation and eye gaze estimation. Furthermore, a novel probabilistic (Bayesian) model based on the Long short-term memory (LSTM) network is developed for classifying driver's distraction severity. This paper also proposes the use of frame-based contextual data from the multi-view TeleFOT naturalistic driving study (NDS) data monitoring to classify the severity of driver distractions. Our proposed methodology entails recurrent deep neural network layers trained to predict driver distraction severity from time series data. Â© 2019 IEEE.",,,
10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00064,2019,"MacHado E., Carrillo I., Collado M., Chen L.",Visual attention-based object detection in cluttered environments,"The study of human visual attention is considered a hot topic in the field of activity recognition, experimental psychology research and human computer interaction. The importance of detecting user objects of interest in real time is critical to provide accurate cues about the user intentions.However, current methods for visual attention extraction and object detection suffer from low performance when moving to ongoing condition. Inherent complexity of cluttered environmentsis considered the major barrier to achieve good performances. To address this challenge, we present a novel method that includes head-worn eye tracker and egocentric video. Our method exploits sliding window-based time series approach in conjunction with aHeuristic probabilistic function to analyse user fixations around potential object of interest in an egocentric video. We evaluate the proposed method using a new dataset annotated with user gaze data and object within a frame image. Our experimental results show that our approach can outperforms several state-of-the-art commonality visual attention-based object detection methods. Â© 2019 IEEE.",,,
10.1109/CITS.2019.8862107,2019,"Alarifi J., Fry J., Dancey D., Yap M.H.",Understanding face age estimation: Humans and machine,"Face age estimation is an important part of many disciplines, including dermatology, cosmetology, and computer vision. Traditional age estimation studies focus on certain parts of the face to analyse its surface topology. With the advances of deep learning, many Convolutional Neural Networks (CNNs) now outperform traditional methods in the age estimation task. However, it is still not clear what type of features these networks learn when estimating age. This study aims to investigate which facial features are important, for humans and for CNNs, on the age estimation of women in five age groups. We then compare the heat-maps from the human eye gaze with those of the CNN. We consider two main research questions: (1) Which facial regions do humans look at when performing age estimation? (2) Do humans and machine focus on the same facial regions when estimating the age? We answered these questions by conducting two experiments. In the first experiment, we used an eye-tracking software to detect where on the face the gaze of human participants focused the most when they were asked to assign the person in the image to an age class. In the second experiment, we used transfer learning on the network pre-trained on ImageNet, then fine-tuned the network on a benchmark face age dataset to classify the same images shown to the participants. The heat-maps of the VGG16 network were then visualised using Gradient-based Class Activation Map (Grad-CAM). The results showed how our model was almost as accurate as humans in predicting the age of a person from a single image of their face (CNN: 60%; humans: 61%). The results also showed that people mainly look at the eyes and nose when predicting a person's age, while the features learned by the CNN included the eyes, the mouth, and the skin surface. Â© 2019 IEEE.",,,
10.1007/s10916-019-1410-6,2019,"Illavarason P., Arokia Renjit J., Mohan Kumar P.",Medical Diagnosis of Cerebral Palsy Rehabilitation Using Eye Images in Machine Learning Techniques,"Cerebral Palsy (CP) is a non progressive neurological disorders commonly associated with a spectrum of developmental disabilities such as strabismus (misalignment of eye). The Eye image are captured through camera, this make the quick diagnosis and examination the periodical assessment for CP kids. By capturing the Eye Movement of 40 children with CP (aged 3â€?1Â years) with relatively mild motor-impairment and also we have analyzed the performance of CP children periodically. Nowadays, Bio-Medical image processing and Machine learning Classification algorithm used for detection and diagnosis the certain diseases and plays the important tool to decrease the risk of any diseases. This work presents a computational methodology to automatically diagnose the Improvement of CP children and performance can be evaluated. The alternate medical evaluation techniques have shown their potential for the treatment and diagnosis of disease like strabismus and nystagmus for CP kids. The proposed method is used to measure and quantify the performance improvement by classify the abnormal eye condition of CP kids and these results attained by machine learning method. The results show the best classification accuracy of 94.17% calculated from Neural Network Classifier. Specificity Rate were absorbed as 0.9800 and Sensitivity Rate were absorbed as 0.9165 respectively. The proposed method for non-invasive and automatic detection of abnormalities in CP kids and evaluates the performance improvement more accurately. Â© 2019, Springer Science+Business Media, LLC, part of Springer Nature.",,,
10.1109/TIP.2019.2904434,2019,"Gao J., Zhang T., Xu C.",SMART: Joint Sampling and Regression for Visual Tracking,"Most existing trackers are either sampling-based or regression-based methods. Sampling-based methods estimate the target state by sampling many target candidates. Although these methods achieve significant performance, they often suffer from a high computational burden. Regression-based methods often learn a computationally efficient regression function to directly predict the geometric distortion between frames. However, most of these methods require large-scale external training videos and are still not very impressive in terms of accuracy. To make both types of methods enhance and complement each other, in this paper, we propose a joint sampling and regression scheme for visual tracking, which leverages the region proposal network by a novel design. Specifically, our method can jointly exploit discriminative target proposal generation and structural target regression to predict target location in a simple feedforward propagation. We evaluate the proposed method on five challenging benchmarks, and extensive experimental results demonstrate that our method performs favorably compared with state-of-the-art trackers with respect to both accuracy and speed. Â© 1992-2012 IEEE.",,,
10.1007/s11548-019-01988-0,2019,"Chaplin V., Phipps M.A., Jonathan S.V., Grissom W.A., Yang P.F., Chen L.M., Caskey C.F.",On the accuracy of optically tracked transducers for image-guided transcranial ultrasound,"Purpose: Transcranial focused ultrasound (FUS) is increasingly being explored to modulate neuronal activity. To target neuromodulation, researchers often localize the FUS beam onto the brain region(s) of interest using spatially tracked tools overlaid on pre-acquired images. Here, we quantify the accuracy of optically tracked image-guided FUS with magnetic resonance imaging (MRI) thermometry, evaluate sources of error and demonstrate feasibility of these procedures to target the macaque somatosensory region. Methods: We developed an optically tracked FUS system capable of projecting the transducer focus onto a pre-acquired MRI volume. To measure the target registration error (TRE), we aimed the transducer focus at a desired target in a phantom under image guidance, heated the target while imaging with MR thermometry and then calculated the TRE as the difference between the targeted and heated locations. Multiple targets were measured using either an unbiased or bias-corrected calibration. We then targeted the macaque S1 brain region, where displacement induced by the acoustic radiation force was measured using MR acoustic radiation force imaging (MR-ARFI). Results: All calibration methods enabled registration with TRE on the order of 3Â mm. Unbiased calibration resulted in an average TRE of 3.26Â mm (minâ€“max: 2.80â€?.53Â mm), which was not significantly changed by prospective bias correction (TRE of 3.05Â mm; 2.06â€?.81Â mm, p = 0.55). Restricting motion between the transducer and target and increasing the distance between tracked markers reduced the TRE to 2.43Â mm (minâ€“max: 0.79â€?.88Â mm). MR-ARFI images showed qualitatively similar shape and extent as projected beam profiles in a living non-human primate. Conclusions: Our study describes methods for image guidance of FUS neuromodulation and quantifies errors associated with this method in a large animal. The workflow is efficient enough for in vivo use, and we demonstrate transcranial MR-ARFI in vivo in macaques for the first time. Â© 2019, CARS.",,,
10.1016/j.dsp.2018.12.008,2019,"Zhou J., Wang L., Yin H., Bovik A.C.",Eye movements and visual discomfort when viewing stereoscopic 3D content,"The visual brain fuses the left and right images projected onto the two eyes from a stereoscopic 3D (S3D) display, perceives parallax, and rebuilds a sense of depth. In this process, the eyes adjust vergence and accommodation to adapt to the depths and parallax of the points they gazed at. Conflicts between accommodation and vergence when viewing S3D content potentially lead to visual discomfort. A variety of approaches have been taken towards understanding the perceptual bases of discomfort felt when viewing S3D, including extreme disparities or disparity gradients, negative disparities, dichoptic presentations, and so on. However less effort has been applied towards understanding the role of eye movements as they relate to visual discomfort when viewing S3D. To study eye movements in the context of S3D viewing discomfort, a Shifted-S3D-Image-Database (SSID) is constructed using 11 original nature scene S3D images and their 6 shifted versions. We conducted eye-tracking experiments on humans viewing S3D images in SSID while simultaneously collecting their judgments of experienced visual discomfort. From the collected eye-tracking data, regions of interest (ROIs) were extracted by kernel density estimation using the fixation data, and an empirical formula fitted between the disparities of salient objects marked by the ROIs and the mean opinion scores (MOS). Finally, eye-tracking data was used to analyze the eye movement characteristics related to S3D image quality. Fifteen eye movement features were extracted, and a visual discomfort predication model learned using a support vector regressor (SVR). By analyzing the correlations between features and MOS, we conclude that angular disparity features have a strong correlation with human judgments of discomfort. Â© 2018 Elsevier Inc.",,,
10.1145/3348488.3348494,2019,"Susan S., Agarwal A., Gulati C., Singh S., Chauhan V.",Human attention span modeling using 2D visualization plots for gaze progression and gaze sustenance,"This paper presents a novel perspective on human attention span modeling based on gaze estimation from head pose data extracted from videos. This is achieved by devising specialized 2D visualization plots that capture gaze progression and gaze sustenance over time. In doing so, a low-resolution analysis is assumed, as is the case with most crowd surveillance videos wherein the retinal analysis and iris pattern extraction of individual subjects is made impossible. The information is useful for studies involving the random gaze behavior pattern of humans in a crowded place, or in a controlled environment in seminars or office meetings. The extraction of useful information regarding the attention span of the individual from the spatial and temporal analysis of gaze points is the subject of study in this paper. Different solutions ranging from plotting temporal gaze plots to sustained attention span graphs are investigated, and the results are compared with the existing techniques of attention span modeling and visualization. Â© 2019 Association for Computing Machinery.",,,
10.1016/j.neucom.2019.04.024,2019,"Xin M., Zheng J., Li B., Niu G., Zhang M.",Real-time object tracking via self-adaptive appearance modeling,"One of the main factors that limit the accuracy and robustness of visual tracking algorithms is the lack of suitable appearance models. The robustness and effectiveness of object appearance models is severely affected by the changing object appearances during the tracking process and the interference of other similar objects around the truth object. In this paper, a self-adaptive appearance model pool based on multi-sample is constructed to improve the robustness of the object appearance models. In order to deal with variable object states, the initial sample given in the first frame and the samples generated in the subsequent tracking process are combined into a sample set to represent various appearances of the object. In addition, a dynamic selection strategy is explored to update and maintain the sample components that are derived from varieties of sources. In order to distinguish the tracking object from other similar candidate objects, multi-feature response fusion strategy is proposed, which can effectively improve the expression ability of the appearance model. Extensive experiments on the popular benchmark datasets demonstrate that the proposed tracking approach performs favorably against several other state-of-the-art tracking algorithms. Â© 2019",,,
10.1109/IWOBI47054.2019.9114393,2019,"Larrazabal A.J., Cena C.E.G., Martinez C.E.",Eye corners tracking for head movement estimation,"Recently, video-oculographic gaze tracking has begun to be used in the diagnosis of a wide variety of neurological diseases, such as Parkinson and Alzheimer. For this application, the so-called feature-based methods are used, more precisely, 2D regression-based methods. They use geometrically derived eye features from high-resolution eye images captured by zooming into the user's eyes. The main weakness of these methods is that the head of the user must remain motionless to avoid estimation errors. In some patients, some involuntary movements cannot be avoided and it is necessary to measure them. In this paper, we tackle the measurement of head position as a way to improve the gaze tracking on these precision demanding medical applications. As a first stage, we propose to obtain the eye corners coordinates as a reference point, since they are the most stable points in front of the eyeball and eyelids movements. The problem was handled as a regression problem using a coarse-to-fine cascaded convolutional neural network in order to accurately regress the coordinates of the eye corner. Particularly, with the aim of achieving high precision we cascade two levels of convolutional networks. Finally, we added temporal information to increase accuracy and decrease computation time. The accuracy of the estimation was calculated from the mean square error between the predictions and the ground truth. Subjective performance was also evaluated through video inspection. In both cases, satisfactory results were obtained. Â© 2019 IEEE.",,,
10.1109/IIAI-AAI.2019.00041,2019,"Wang C.-C., Wang S.-C., Chu C.-P.",Combining Virtual Reality Advertising and Eye Tracking to Understand Visual Attention: A Pilot Study,"This paper aims to design a virtual reality eye tracker for the analysis of dynamic behavior of viewers when watching a virtual reality (VR) commercial advertising. Seven experimental participants were recruited for the pilot study. Experiment contents included a one-minute VR commercial advertising designed with Unity 3D development tool and through integration into eye tracking device of a head-mounted display (HMD), eye movements of participants were recorded. The finding results of the paper showed that: 1) when watching the VR commercial advertising, participants did not put the first sequence of gaze in each dynamic regions of interest (ROIs) in the commercial object; 2) in terms for concentration span sequence of gaze in each dynamic ROIs, if participant gazed the commercial object, more concerns were put and there was longer time for gazing; on the contrary, there was no eye movement information about gazing at the commercial object; 3) when watching the VR commercial advertising, the commercial object did not receive longer total gazing time and numbers of relative concentration gazing of participants spending on all dynamic ROIs. According to experimental results, we concluded that the VR 360-degree VR film affects the expected outcome in the operation of eye tracking technology. Â© 2019 IEEE.",,,
10.1109/ECTI-CON47248.2019.8955244,2019,"Pichitwong W., Chamnongthai K.",Obscured 3D point-of-gaze estimation by multipoint cloud data,"Point cloud sensor is currently used to sense point cloud data information which is 3D position on the surface of target object. Point cloud data information can be input to improve for 3D point of gaze estimation (3D POG). Presently, there are limitation on creating point cloud data information on target object since point cloud data cannot be found if any obstacle in front of the sensor, there are shadow projection. This paper proposes method of multipoint cloud data to create point cloud data on the surface of target object and obscured point cloud data in the shadow projection. Eye tracker sensor provides 3D eyes position data and 2D POG on screen data which each origin represents center of eye tracker and center of screen respectively. These mentioned data are integrated by model fitting to draw a straight line, originating from the center point between left pupil and right pupil, which passes through the2D POG on virtual screen and ends when the line meets the closest point on the target object. In performance evaluation of proposed method, firstly the obscured point cloud data are successfully defined. Secondly, experiment by 4 participants by watching 9 units of testing objects at 2 seconds in free move provide the result of 3D POG estimation at average distance errors by 1.09 cm. Â© 2019 IEEE.",,,
10.1109/ICCCNT45670.2019.8944564,2019,"Tamuly S., Jyotsna C., Amudha J.",Tracking Eye Movements to Predict the Valence of A Scene,"Studying human bio signals such as eye movements and tracking them can help in identifying and classifying the emotional essence of a scene. The existing methods employed to evaluate the reaction of the eyes based on exposure to a scene or image often use a classifier to extract features from eye movements. These extracted features are then evaluated to determine the valence of a scene. On the contrary, as much as eye movement has proved to be a reliable source in scene or image detection, factors such as how each feature affects the outcome of the prediction have not been explored. For the determination of the emotional category of images using eye movements, images are categorized into pleasant, neutral and unpleasant images and then these images are shown to the test subjects to record their response. Features of eye movement like fixation count, fixation frequency, saccade count, and saccade frequency among others, along with a machine learning approach was used for scene classification. Â© 2019 IEEE.",,,
10.1109/EMBC.2019.8856904,2019,"Elbattah M., Carette R., Dequen G., Guerin J.-L., Cilia F.",Learning Clusters in Autism Spectrum Disorder: Image-Based Clustering of Eye-Tracking Scanpaths with Deep Autoencoder,"Autism spectrum disorder (ASD) is a lifelong condition characterized by social and communication impairments. This study attempts to apply unsupervised Machine Learning to discover clusters in ASD. The key idea is to learn clusters based on the visual representation of eye-tracking scanpaths. The clustering model was trained using compressed representations learned by a deep autoencoder. Our experimental results demonstrate a promising tendency of clustering structure. Further, the clusters are explored to provide interesting insights into the characteristics of the gaze behavior involved in autism. Â© 2019 IEEE.",,,
10.1109/EMBC.2019.8857005,2019,"Jiang M., Francis S.M., Srishyla D., Conelea C., Zhao Q., Jacob S.",Classifying Individuals with ASD Through Facial Emotion Recognition and Eye-Tracking,"Individuals with Autism Spectrum Disorder (ASD) have been shown to have atypical scanning patterns during face and emotion perception. While previous studies characterized ASD using eye-tracking data, this study examined whether the use of eye movements combined with task performance in facial emotion recognition could be helpful to identify individuals with ASD. We tested 23 subjects with ASD and 35 controls using a Dynamic Affect Recognition Evaluation (DARE) task that requires an individual to recognize one of six emotions (i.e., anger, disgust, fear, happiness, sadness, and surprise) while observing a slowly transitioning face video. We observed differences in response time and eye movements, but not in the recognition accuracy. Based on these observations, we proposed a machine learning method to distinguish between individuals with ASD and typically developing (TD) controls. The proposed method classifies eye fixations based on a comprehensive set of features that integrate task performance, gaze information, and face features extracted using a deep neural network. It achieved an 86% classification accuracy that is comparable with the standardized diagnostic scales, with advantages of efficiency and objectiveness. Feature visualization and interpretations were further carried out to reveal distinguishing features between the two subject groups and to understand the social and attentional deficits in ASD. Â© 2019 IEEE.",,,
10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00126,2019,"Su H., Hou Z., Huan J., Yan K., Ding H.",A multi-modal gaze tracking algorithm,"Gaze tracking is an assistant system of human-computer interaction. Aiming at the problem of high misjudgment rate and long time-consuming of traditional iris location methods, this paper proposes a gaze tracking method based on human eye geometric characteristics to improve the tracking accuracy in 2D environment. Firstly, the human face is located by face location algorithm and the position of human eye is estimated roughly. Then the iris template is built by iris image, and the iris center location algorithm is used to locate the iris center position. Finally, the eyes corners and iris center points are extracted to locate the eye area accurately and obtain the binocular image. The binocular images are input into the feature extraction network as multi-modal information in parallel, and the convoluted feature channels are reconstructed using the weight redistribution module in the network. Then the reconstructed features are fused in the full connection layer. Finally, the output layer is used to classify the reconstructed features. Experiments were carried out on a self-built screen block data set. For 12 classified data, the lowest recognition error rate is 5.34%. Â© 2019 IEEE.",,,
10.1109/IJCNN.2019.8851961,2019,"Dubey N., Ghosh S., Dhall A.",Unsupervised Learning of Eye Gaze Representation from the Web,"Automatic eye gaze estimation has interested researchers for a while now. In this paper, we propose an unsupervised learning based method for estimating the eye gaze region. To train the proposed network ""Ize-Net"" in self-supervised manner, we collect a large 'in the wild' dataset containing 1,54,251 images from the web. For the images in the database, we divide the gaze into three regions based on an automatic technique based on pupil-centers localization and then use a feature-based technique to determine the gaze region. The performance is evaluated on the Tablet Gaze and CAVE datasets by fine-tuning results of Ize-Net for the task of eye gaze estimation. The feature representation learned is also used to train traditional machine learning algorithms for eye gaze estimation. The results demonstrate that the proposed method learns a rich data representation, which can be efficiently finetuned for any eye gaze estimation dataset. Â© 2019 IEEE.",,,
10.1109/COMPSAC.2019.00095,2019,"Wang J., Fu E.Y., Ngai G., Leong H.V.",Investigating differences in gaze and typing behavior across age groups and writing genres,"Typing is one of the most common activities that are undertaken on a computer. It would therefore be interesting to investigate whether it is possible to deduce characteristics of the user, such as their age or the type of the document that they are writing, just simply from typing dynamics. In this paper, we study the coordination between eye gaze and typing dynamics, or the gaze-typing behavior, of subjects who are producing original text. We focus upon the differences between different age groups (children vs elderly seniors) and different genres of writing (reminiscent, logical and creative). Using machine-learning, we achieve an accuracy of 93.5% for age detection and 61.1% for the article-category detection, using a leave-one-subject-out cross-validation evaluation, which is 44% and 28% higher than baselines. Â© 2019 IEEE.",,,
10.1109/EST.2019.8806201,2019,"Ali A., Alsufyani N., Hoque S., Deravi F.",Gaze-based presentation attack detection for users wearing tinted glasses,"Biometric authentication is vulnerable to presentation (spoofing) attacks. It is important to address the security vulnerability of spoofing attacks where an attacker uses an artefact presented at the sensor to subvert the system. Gaze-tracking has been proposed for such attack detection. In this paper, we explore the sensitivity of a gaze-based approach to spoofing detection in the presence of eye-glasses that may impact detection performance. In particular, we investigate the use of partially tinted glasses such as may be used in hazardous environments or outdoors in mobile application scenarios The attack scenarios considered in this work include the use of projected photos, 2D and 3D masks. A gaze-based spoofing detection system has been extensively evaluated using data captured from volunteers performing genuine attempts (with and without wearing such tinted glasses) as well as spoofing attempts using various artefacts. The results of the evaluations indicate that the presence of tinted glasses has a small impact on the accuracy of attack detection, thereby making the use of such gaze-based features possible for a wider range of applications. Â© 2019 IEEE.",,,
10.1109/ICMEW.2019.00095,2019,"Xu W., Cheung S.-C.",Fully automatic photorealistic facial expression and eye gaze transfer with a single image,"Facial expression transfer has many applications in animation, special effects, and social media. Due to the variability in pose and environmental condition, generating a photo-realistic facial expression with a single image under uncon-trolled environment is a challenging problem. In this paper, we present a novel approach to simultaneously transfer both the facial expression and the eye gaze of an actor from one image to another. Based on the estimated facial geometry, the key idea of our work is to use mesh deformation to manipulate the expression of the actor in the target image. We start the process by reconstructing 3D facial geometry from un-constrained 2D images using a two-stage optimization strategy. A non-rigid registration is then used to find correspon-dences and alignment between reconstructed models, while eye gaze transfer is accomplished by piecewise affine warping. Finally we re-render the manipulated image with the use of Poisson image blending. We demonstrate the effectiveness of our method on images with various lighting conditions, head poses, and facial expressions. Â© 2019 IEEE.",,,
10.1109/ICME.2019.00151,2019,"Zhou X., Lin J., Jiang J., Chen S.",Learning a 3D gaze estimator with improved itracker combined with bidirectional LSTM,"Free-head 3D gaze estimation which outputs gaze vector in 3D space has wide application in human-computer interaction. In this paper, we propose a novel 3D gaze estimator by improving the Itracker and employing a many-to-one bidirectional LSTM (bi-LSTM). First, we improve the conventional Itracker by removing the face-grid and reducing one network branch via concatenating the two-eye region images to predict the subject's gaze of a single frame. Then, we employ the bi-LSTM to fit the temporal information between frames to estimate gaze vector for video sequence. Experimental results show that our improved Itracker obtains 11.6% significant improvement over the state-of-the-art methods on MPIIGaze dataset (single image frame) and has robust estimation accuracy for different image resolutions. Moreover, experimental results on EyeDiap dataset (video sequence) further bring 3% accuracy improvement by employing the bi-LSTM. Â© 2019 IEEE.",,,
10.1109/ICME.2019.00052,2019,"Li Z., Zhang X.",Collaborative deep reinforcement learning for image cropping,"An automatic photo composition method based on collaborative deep reinforcement learning(called CDRL-RC) is proposed in this paper. Our method models photo composition as a markov decision-making process by reinforcement learning and generates cropping result through a series of moving and zooming actions. Emotional attention information is added to the composition task, which was trained by eye-tracking datasets to consider the relationship and importance between objects. In order to sufficiently use the emotional attention map and original image for image cropping, they are processed as inputs to two collaborative agents. For the collaborative composition of two agents, we design an information interaction module, which allows inter-agents to exchange information and give advice to each other, and finally predict the action together. In addition, we add attention weight to the traditional IoU to efficiently evaluate the cropping quantity in the reward function. Experiment results show that our CDRL-RC model achieved the state-of-the-art photo composition performance on a variety of datasets. Â© 2019 IEEE.",,,
10.1145/3308532.3329461,2019,"Volonte M., Duchowski A.T., Babu S.V.",Effects of a virtual human appearance fidelity continuum on visual attention in virtual reality,"In this contribution we studied how different rendering styles of a virtual human impacted users' visual attention in an interactive medical training simulator. In a mixed design experiment, 78 participants interacted with a virtual human representing a sample from the non-photorealistic (NPR) to the photorealistic (PR) rendering continuity. We presented five rendering style samples scenarios, namely low fidelity all Pencil Shaded (APS), Low to Mid Fidelity Pencil Shader on virtual patient (VP) only (PS), Mid Fidelity All Cartoon Shaded (ACT), Mid to High Fidelity Cartoon Shader on VP only (CT), and relatively High Fidelity Human Like (HL) appearance, and compared how visual attention differed between groups of users. For this study, we employed an eye tracking system for collecting and analyzing users' gaze during interaction with the virtual human in a failure to rescue medical training simulation. Results suggests that users may spend more time in the simulations on the non-realistic fidelity continuum that necessarily do not involve interaction with the virtual human. However, users preferred visually attending to virtual humans in the middle and high fidelity visual appearance conditions when engaging virtual humans in simulated social face-To-face dialogue as compared to the other conditions. Â© 2019 Copyright held by the owner/author(s).",,,
10.1109/TIP.2019.2897966,2019,"Xia C., Han J., Qi F., Shi G.",Predicting Human Saccadic Scanpaths Based on Iterative Representation Learning,"Visual attention is a dynamic process of scene exploration and information acquisition. However, existing research on attention modeling has concentrated on estimating static salient locations. In contrast, dynamic attributes presented by saccade have not been well explored in previous attention models. In this paper, we address the problem of saccadic scanpath prediction by introducing an iterative representation learning framework. Within the framework, saccade can be interpreted as an iterative process of predicting one fixation according to the current representation and updating the representation based on the gaze shift. In the predicting phase, we propose a Bayesian definition of saccade to combine the influence of perceptual residual and spatial location on the selection of fixations. In implementation, we compute the representation error of an autoencoder-based network to measure perceptual residuals of each area. Simultaneously, we integrate saccade amplitude and center-weighted mechanism to model the influence of spatial location. Based on estimating the influence of two parts, the final fixation is defined as the point with the largest posterior probability of gaze shift. In the updating phase, we update the representation pattern for the subsequent calculation by retraining the network with samples extracted around the current fixation. In the experiments, the proposed model can replicate the fundamental properties of psychophysics in visual search. In addition, it can achieve superior performance on several benchmark eye-tracking data sets. Â© 1992-2012 IEEE.",,,
10.1145/3314111.3319813,2019,"Goltz J., Grossberg M., Etemadpour R.",Exploring simple neural network architectures for eye movement classification,"Analysis of eye-gaze is a critical tool for studying human-computer interaction and visualization. Yet eye tracking systems only report eye-gaze on the scene by producing large volumes of coordinate time series data. To be able to use this data, we must first extract salient events such as eye fixations, saccades, and post-saccadic oscillations (PSO). Manually extracting these events is time-consuming, labor-intensive and subject to variability. In this paper, we present and evaluate simple and fast automatic solutions for eye-gaze analysis based on supervised learning. Similar to some recent studies, we developed different simple neural networks demonstrating that feature learning produces superior results in identifying events from sequences of gaze coordinates. We do not apply any ad-hoc post-processing, thus creating a fully automated end-to-end algorithms that perform as good as current state-of-the-art architectures. Once trained they are fast enough to be run in a near real time setting. Â© 2019 Association for Computing Machinery.",,,
10.1145/3314111.3319914,2019,"Eivazi S., Santini T., Keshavarzi A., KÃ¼bler T., Mazzei A.",Improving real-time CNN-based pupil detection through domain-specific data augmentation,"Deep learning is a promising technique for real-world pupil detection. However, the small amount of available accurately-annotated data poses a challenge when training such networks. Here, we utilize non-challenging eye videos where algorithmic approaches perform virtually without errors to automatically generate a foundational data set containing subpixel pupil annotations. Then, we propose multiple domain-specific data augmentation methods to create unique training sets containing controlled distributions of pupil-detection challenges. The feasibility, convenience, and advantage of this approach is demonstrated by training a CNN with these datasets. The resulting network outperformed current methods in multiple publicly-available, realistic, and challenging datasets, despite being trained solely with the augmented eye images. This network also exhibited better generalization w.r.t. the latest state-of-the-art CNN: Whereas on datasets similar to training data, the nets displayed similar performance, on datasets unseen to both networks, ours outperformed the state-of-the-art by â‰?7% in terms of detection rate. Â© 2019 Association for Computing Machinery.",,,
10.1145/3314111.3319843,2019,"Venuprasad P., Dobhal T., Paul A., Nguyen T.N.M., Gilman A., Cosman P., Chukoskie L.",Characterizing joint attention behavior during real world interactions using automated object and gaze detection,"Joint attention is an essential part of the development process of children, and impairments in joint attention are considered as one of the first symptoms of autism. In this paper, we develop a novel technique to characterize joint attention in real time, by studying the interaction of two human subjects with each other and with multiple objects present in the room. This is done by capturing the subjectsâ€?gaze through eye-tracking glasses and detecting their looks on predefined indicator objects. A deep learning network is trained and deployed to detect the objects in the field of vision of the subject by processing the video feed of the world view camera mounted on the eye-tracking glasses. The looking patterns of the subjects are determined and a real-time audio response is provided when a joint attention is detected, i.e., when their looks coincide. Our findings suggest a trade-off between the accuracy measure (Look Positive Predictive Value) and the latency of joint look detection for various system parameters. For more accurate joint look detection, the system has higher latency, and for faster detection, the detection accuracy goes down. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3314111.3319819,2019,"Dierkes K., Kassner M., Bulling A.",A fast approach to refraction-aware eye-model fitting and gaze prediction,"By temporally integrating information about pupil contours extracted from eye images, model-based methods for glint-free gaze estimation can mitigate pupil detection noise. However, current approaches require time-consuming iterative solving of a nonlinear minimization problem to estimate key parameters, such as eyeball position. Based on the method presented by [Swirski and Dodgson 2013], we propose a novel approach to glint-free 3D eye-model fitting and gaze prediction using a single near-eye camera. By recasting model optimization as a least-squares intersection of lines, we make it amenable to a fast non-iterative solution. We further present a method for estimating deterministic refraction-correction functions from synthetic eye images and validate them on both synthetic and real eye images. We demonstrate the robustness of our method in the presence of pupil detection noise and show the benefit of temporal integration of pupil contour information on eyeball position and gaze estimation accuracy. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3314111.3319817,2019,"Utsu T., Takemura K.",Remote corneal imaging by integrating a 3D face model and an eyeball model,"In corneal imaging methods, it is essential to use a 3D eyeball model for generating an undistorted image. Thus, the relationship between the eye and eye camera is fixed by using a head-mounted device. Remote corneal imaging has several potential applications such as surveillance systems and driver monitoring. Therefore, we integrated a 3D eyeball model with a 3D face model to facilitate remote corneal imaging. We conducted evaluation experiments and confirmed the feasibility of remote corneal imaging. We showed that the center of the eyeball can be estimated based on face tracking, and thus, corneal imaging can function as continuous remote eye tracking. Â© 2019 Association for Computing Machinery.",,,
10.1145/3317959.3321492,2019,"Wang H., Shi B.E.",Gaze awareness improves collaboration efficiency in a collaborative assembly task,"In building human robot interaction systems, it would be helpful to understand how humans collaborate, and in particular, how humans use othersâ€?gaze behavior to estimate their intent. Here we studied the use of gaze in a collaborative assembly task, where a human user assembled an object with the assistance of a human helper. We found that the being aware of the partnerâ€™s gaze significantly improved collaboration efficiency. Task completion times were much shorter when gaze communication was available, than when it was blocked. In addition, we found that the userâ€™s gaze was more likely to lie on the object of interest in the gaze-aware case than the gaze-blocked case. In the context of human-robot collaboration systems, our results suggest that gaze data in the period surrounding verbal requests will be more informative and can be used to predict the target object. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3314111.3319840,2019,"Aronson R.M., Admoni H.",Semantic gaze labeling for human-robot shared manipulation,"Human-robot collaboration systems benefit from recognizing peopleâ€™s intentions. This capability is especially useful for collaborative manipulation applications, in which users operate robot arms to manipulate objects. For collaborative manipulation, systems can determine usersâ€?intentions by tracking eye gaze and identifying gaze fixations on particular objects in the scene (i.e., semantic gaze labeling). Translating 2D fixation locations (from eye trackers) into 3D fixation locations (in the real world) is a technical challenge. One approach is to assign each fixation to the object closest to it. However, calibration drift, head motion, and the extra dimension required for real-world interactions make this position matching approach inaccurate. In this work, we introduce velocity features that compare the relative motion between subsequent gaze fixations and a finite set of known points and assign fixation position to one of those known points. We validate our approach on synthetic data to demonstrate that classifying using velocity features is more robust than a position matching approach. In addition, we show that a classifier using velocity features improves semantic labeling on a real-world dataset of human-robot assistive manipulation interactions. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3314111.3319822,2019,"Mardanbegi D., Clarke C., Gellersen H.",Monocular gaze depth estimation using the vestibulo-ocular reflex,"Gaze depth estimation presents a challenge for eye tracking in 3D. This work investigates a novel approach to the problem based on eye movement mediated by the vestibulo-ocular reflex (VOR). VOR stabilises gaze on a target during head movement, with eye movement in the opposite direction, and the VOR gain increases the closer the fixated target is to the viewer. We present a theoretical analysis of the relationship between VOR gain and depth which we investigate with empirical data collected in a user study (N=10). We show that VOR gain can be captured using pupil centres, and propose and evaluate a practical method for gaze depth estimation based on a generic function of VOR gain and two-point depth calibration. The results show that VOR gain is comparable with vergence in capturing depth while only requiring one eye, and provide insight into open challenges in harnessing VOR gain as a robust measure. Â© 2019 Association for Computing Machinery.",,,
10.1145/3314111.3322874,2019,"Yang Z., Bailey R.",Towards a Data-driven Framework for Realistic Self-Organized Virtual Humans: Coordinated Head and Eye movements,"Driven by significant investments from the gaming, film, advertising, and customer service industries among others, efforts across many different fields are converging to create realistic representations of humans that look like (computer graphics), sound like (natural language generation), move like (motion capture), and reason like (artificial intelligence) real humans. The ultimate goal of this work is to push the boundaries even further by exploring the development of realistic self-organized virtual humans that are capable of demonstrating coordinated behaviors across different modalities. Eye movements, for example, may be accompanied by changes in facial expression, head orientation, posture, gait properties, or speech. Traditionally however, these modalities are captured and modeled separately and this disconnect contributes to the well-known uncanny valley phenomenon. We focus initially on facial modalities, in particular, coordinated eye and head movements (and eventually facial expressions), but our proposed data-driven framework will be able to accommodate other modalities as well. Â© 2019 Copyright held by the owner/author(s).",,,
10.1145/3314111.3319821,2019,"Katrychuk D., Griffith H.K., Komogortsev O.V.",Power-efficient and shift-robust eye-tracking sensor for portable VR headsets,"Photosensor oculography (PSOG) is a promising solution for reducing the computational requirements of eye tracking sensors in wireless virtual and augmented reality platforms. This paper proposes a novel machine learning-based solution for addressing the known performance degradation of PSOG devices in the presence of sensor shifts. Namely, we introduce a convolutional neural network model capable of providing shift-robust end-to-end gaze estimates from the PSOG array output. Moreover, we propose a transfer-learning strategy for reducing model training time. Using a simulated workflow with improved realism, we show that the proposed convolutional model offers improved accuracy over a previously considered multilayer perceptron approach. In addition, we demonstrate that the transfer of initialization weights from pre-trained models can substantially reduce training time for new users. In the end, we provide the discussion regarding the design trade-offs between accuracy, training time, and power consumption among the considered models. Â© 2019 Association for Computing Machinery.",,,
10.1145/3314111.3319820,2019,"Hassoumi A., Peysakhovich V., Hurter C.",Eyeflow: Pursuit interactions using an unmodified camera,"We investigate the smooth pursuit eye movement based interaction using an unmodiied of-the-shelf RGB camera. In each pair of sequential video frames, we compute the indicative direction of the eye movement by analyzing low vectors obtained using the Lucas-Kanade optical low algorithm. We discuss how carefully selected low vectors could replace the traditional pupil centers detection in smooth pursuit interaction. We examine implications of unused features in the eye camera imaging frame as potential elements for detecting gaze gestures. This simple approach is easy to implement and abstains from many of the complexities of pupil based approaches. In particular, EyeFlow does not call for either a 3D pupil model or 2D pupil detection to track the pupil center location. We compare this method to state-of-the-art approaches and ind that this can enable pursuit interactions with standard cameras. Results from the evaluation with 12 users data yield an accuracy that compares to previous studies. In addition, the beneit of this work is that the approach does not necessitate highly matured computer vision algorithms and expensive IR-pass cameras. Â© 2019 Association for Computing Machinery.",,,
10.1145/3314111.3319844,2019,"Siegfried R., Yu Y., Odobez J.-M.",A deep learning approach for robust head pose independent eye movements recognition from videos,"Recognizing eye movements is important for gaze behavior understanding like in human communication analysis (human-human or robot interactions) or for diagnosis (medical, reading impairments). In this paper, we address this task using remote RGB-D sensors to analyze people behaving in natural conditions. This is very challenging given that such sensors have a normal sampling rate of 30 Hz and provide low-resolution eye images (typically 36x60 pixels), and natural scenarios introduce many variabilities in illumination, shadows, head pose, and dynamics. Hence gaze signals one can extract in these conditions have lower precision compared to dedicated IR eye trackers, rendering previous methods less appropriate for the task. To tackle these challenges, we propose a deep learning method that directly processes the eye image video streams to classify them into fixation, saccade, and blink classes, and allows to distinguish irrelevant noise (illumination, low-resolution artifact, inaccurate eye alignment, difficult eye shapes) from true eye motion signals. Experiments on natural 4-party interactions demonstrate the benefit of our approach compared to previous methods, including deep learning models applied to gaze outputs. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3314111.3319829,2019,"Agtzidis I., Dorr M.",Getting (more) real: Bringing eye movement classification to HMD experiments with equirectangular stimuli,"The classification of eye movements is a very important part of eye tracking research and has been studied since its early days. Over recent years, we have experienced an increasing shift towards more immersive experimental scenarios with the use of eye-tracking enabled glasses and head-mounted displays. In these new scenarios, however, most of the existing eye movement classification algorithms cannot be applied robustly anymore because they were developed with monitor-based experiments using regular 2D images and videos in mind. In this paper, we describe two approaches that reduce artifacts of eye movement classification for 360Â° videos shown in head-mounted displays. For the first approach, we discuss how decision criteria have to change in the space of 360Â° videos, and use these criteria to modify five popular algorithms from the literature. The modified algorithms are publicly available at https://web.gin.g-node.org/ioannis.agtzidis/360_em_algorithms. For cases where an existing algorithm cannot be modified, e.g. because it is closed-source, we present a second approach that maps the data instead of the algorithm to the 360Â° space. An empirical evaluation of both approaches shows that they significantly reduce the artifacts of the initial algorithm, especially in the areas further from the horizontal midline. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3314111.3322503,2019,"Rostaminia S., Mayberry A., Ganesan D., Marlin B., Gummeson J.",ILID: Eyewear solution for low-power fatigue and drowsiness monitoring,"The ability to monitor eye closures and blink patterns has long been known to enable accurate assessment of fatigue and drowsiness in individuals. Many measures of the eye are known to be correlated with fatigue including coarse-grained measures like the rate of blinks as well as fine-grained measures like the duration of blinks and the extent of eye closures. Despite a plethora of research validating these measures, we lack wearable devices that can continually and reliably monitor them in the natural environment. In this work, we present a low-power system, iLid, that can continually sense fine-grained measures such as blink duration and Percentage of Eye Closures (PERCLOS) at high frame rates of 100fps. We present a complete solution including design of the sensing, signal processing, and machine learning pipeline and implementation on a prototype computational eyeglass platform. Â© 2019 Copyright held by the owner/author(s).",,,
10.1145/3314111.3323074,2019,"Fuhl W., Bozkir E., Hosp B., Castner N., Geisler D., Santini T.C., Kasneci E.",Encodji: Encoding gaze data into emoji space for an amusing scanpath classification approach,"To this day, a variety of information has been obtained from human eye movements, which holds an imense potential to understand and classify cognitive processes and states â€?e.g., through scanpath classification. In this work, we explore the task of scanpath classification through a combination of unsupervised feature learning and convolutional neural networks. As an amusement factor, we use an Emoji space representation as feature space. This representation is achieved by training generative adversarial networks (GANs) for unpaired scanpath-to-Emoji translation with a cyclic loss. The resulting Emojis are then used to train a convolutional neural network for stimulus prediciton, showing an accuracy improvement of more than five percentual points compared to the same network trained using solely the scanpath data. As a side effect, we also obtain novel unique Emojis representing each unique scanpath. Our goal is to demonstrate the applicability and potential of unsupervised feature learning to scanpath classification in a humorous and entertaining way. Â© 2019 Association for Computing Machinery.",,,
10.1145/3314111.3319827,2019,"Le Louedec J., Guntz T., Crowley J.L., Vaufreydaz D.",Deep learning investigation for chess player attention prediction using eye-tracking and game data,"This article reports on an investigation of the use of convolutional neural networks to predict the visual attention of chess players. The visual attention model described in this article has been created to generate saliency maps that capture hierarchical and spatial features of chessboard, in order to predict the probability fixation for individual pixels Using a skip-layer architecture of an autoencoder, with a unified decoder, we are able to use multiscale features to predict saliency of part of the board at different scales, showing multiple relations between pieces. We have used scan path and fixation data from players engaged in solving chess problems, to compute 6600 saliency maps associated to the corresponding chess piece configurations. This corpus is completed with synthetically generated data from actual games gathered from an online chess platform. Experiments realized using both scan-paths from chess players and the CAT2000 saliency dataset of natural images, highlights several results. Deep features, pretrained on natural images, were found to be helpful in training visual attention prediction for chess. The proposed neural network architecture is able to generate meaningful saliency maps on unseen chess configurations with good scores on standard metrics. This work provides a baseline for future work on visual attention prediction in similar contexts. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3314111.3319830,2019,"Larumbe-Bergera A., Porta S., Cabeza R., Villanueva A.",SetA: Semiautomatic tool for annotation of eye tracking images,"Availability of large scale tagged datasets is a must in the field of deep learning applied to the eye tracking challenge. In this paper, the potential of Supervised-Descent-Method (SDM) as a semiautomatic labelling tool for eye tracking images is shown. The objective of the paper is to evidence how the human effort needed for manually labelling large eye tracking datasets can be radically reduced by the use of cascaded regressors. Different applications are provided in the fields of high and low resolution systems. An iris/pupil center labelling is shown as example for low resolution images while a pupil contour points detection is demonstrated in high resolution. In both cases manual annotation requirements are drastically reduced. Â© 2019 Copyright held by the owner/author(s).",,,
10.1145/3317959.3321490,2019,"Soret R., Charras P., Hurter C., Peysakhovich V.",Attentional orienting in virtual reality using endogenous and exogenous cues in auditory and visual modalities,"The virtual reality (VR) has nowadays numerous applications in training, education, and rehabilitation. To efficiently present the immersive 3D stimuli, we need to understand how spatial attention is oriented in VR. The efficiency of different cues can be compared using the Posner paradigm. In this study, we designed an ecological environment where participants were presented with a modified version of the Posner cueing paradigm. Twenty subjects equipped with an eye-tracking system and VR HMD performed a sandwich preparation task. They were asked to assemble the ingredients which could be either endogenously and exogenously cued in both auditory and visual modalities. The results showed that all valid cues made participants react faster. While directional arrow (visual endogenous) and 3D sound (auditory exogenous) oriented attention globally to the entire cued hemifield, the vocal instruction (auditory endogenous) and object highlighting (visual exogenous) allowed more local orientation, in a specific region of space. No differences in gaze shift initiation nor time to fixate the target were found suggesting the covert orienting. Â© 2019 Association for Computing Machinery.",,,
10.1145/3314111.3319832,2019,"Gunawardena N., Matscheko M., Anzengruber B., Ferscha A., Schobesberger M., Shamiyeh A., Klugsberger B., Solleder P.",Assessing surgeonsâ€?skill level in laparoscopic cholecystectomy using eye metrics,"Laparoscopic surgery has revolutionised state of the art in surgical health care. However, its complexity puts a significant burden on the surgeonâ€™s cognitive resources resulting in major biliary injuries. With the increasing number of laparoscopic surgeries, it is crucial to identify surgeonsâ€?cognitive loads (CL) and levels of focus in real time to give them unobtrusive feedback when detecting the suboptimal level of attention. Assuming that the experts appear to be more focused on attention, we investigate how the skill level of surgeons during live surgery is reflected through eye metrics. Forty-two laparoscopic surgeries have been conducted with four surgeons who have different expertise levels. Concerning eye metrics, we have used six metrics which belong to fixation and pupillary based metrics. With the use of mean, standard deviation and ANOVA test we have proven three reliable metrics which we can use to differentiate the skill level during live surgeries. In future studies, these three metrics will be used to classify the surgeonsâ€?cognitive load and level of focus during the live surgery using machine learning techniques. Â© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3314111.3322504,2019,"Paletta L., Dini A., Murko C., Yahyanejad S., AugsdÃ¶rfer U.",Estimation of situation awareness score and performance using eye and head gaze for human-robot collaboration,"Human attention processes play a major role in the optimization of human-robot collaboration (HRC) [Huang et al. 2015]. We describe a novel methodology to measure and predict situation awareness from eye and head gaze features in real-time. The awareness about scene objects of interest was described by 3D gaze analysis using data from eye tracking glasses and a precise optical tracking system. A probabilistic framework of uncertainty considers coping with measurement errors in eye and position estimation. Comprehensive experiments on HRC were conducted with typical tasks including handover in a lab based prototypical manufacturing environment. The gaze features highly correlate with scores of standardized questionnaires of situation awareness (SART [Taylor 1990], SAGAT [Endsley 2000]) and predict performance in the HRC task. This will open new opportunities for human factors based optimization in HRC applications. Â© 2019 Copyright is held by the owner/author(s).",,,
10.3390/mti3020040,2019,"Hayek U.W., MÃ¼ller K., GÃ¶bel F., Kiefer P., Spielhofer R., GrÃªt-Regamey A.",3D point clouds and eye tracking for investigating the perception and acceptance of power lines in different landscapes,"The perception of the visual landscape impact is a significant factor explaining the publicâ€™s acceptance of energy infrastructure developments. Yet, there is lack of knowledge how people perceive and accept power lines in certain landscape types and in combination with wind turbines, a required setting to achieve goals of the energy turnaround. The goal of this work was to demonstrate how 3D point cloud visualizations could be used for an eye tracking study to systematically investigate the perception of landscape scenarios with power lines. 3D visualizations of near-natural and urban landscapes were prepared based on data from airborne and terrestrial laser scanning. These scenes were altered with varying amounts of the respective infrastructure, and they provided the stimuli in a laboratory experiment with 49 participants. Eye tracking and questionnaires served for measuring the participantsâ€?responses. The results show that the point cloud-based simulations offered suitable stimuli for the eye tracking study. Particularly for the analysis of guided perceptions, the approach fostered an understanding of disturbing landscape elements. A comparative in situ eye tracking study is recommended to further evaluate the quality of the point cloud simulations, whether they produce similar responses as in the real world. Â© 2019 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1109/CVPR.2019.01221,2019,"Yu Y., Liu G., Odobez J.-M.",Improving few-shot user-specific gaze adaptation via gaze redirection synthesis,"As an indicator of human attention gaze is a subtle behavioral cue which can be exploited in many applications. However, inferring 3D gaze direction is challenging even for deep neural networks given the lack of large amount of data (groundtruthing gaze is expensive and existing datasets use different setups) and the inherent presence of gaze biases due to person-specific difference. In this work, we address the problem of person-specific gaze model adaptation from only a few reference training samples. The main and novel idea is to improve gaze adaptation by generating additional training samples through the synthesis of gaze-redirected eye images from existing reference samples. In doing so, our contributions are threefold:(i) we design our gaze redirection framework from synthetic data, allowing us to benefit from aligned training sample pairs to predict accurate inverse mapping fields; (ii) we proposed a self-supervised approach for domain adaptation; (iii) we exploit the gaze redirection to improve the performance of person-specific gaze estimation. Extensive experiments on two public datasets demonstrate the validity of our gaze retargeting and gaze estimation framework. Â© 2019 IEEE.",,,
10.1109/CVPR.2019.01006,2019,"Wang K., Su H., Ji Q.",Neuro-inspired eye tracking with eye movement dynamics,"Generalizing eye tracking to new subjects/environments remains challenging for existing appearance-based methods. To address this issue, we propose to leverage on eye movement dynamics inspired by neurological studies. Studies show that there exist several common eye movement types, independent of viewing contents and subjects, such as fixation, saccade, and smooth pursuits. Incorporating generic eye movement dynamics can therefore improve the generalization capabilities. In particular, we propose a novel Dynamic Gaze Transition Network (DGTN) to capture the underlying eye movement dynamics and serve as the topdown gaze prior. Combined with the bottom-up gaze measurements from the deep convolutional neural network, our method achieves better performance for both within-dataset and cross-dataset evaluations compared to state-of-the-art. In addition, a new DynamicGaze dataset is also constructed to study eye movement dynamics and eye gaze estimation. Â© 2019 IEEE.",,,
10.1109/CVPR.2019.01218,2019,"Wang K., Zhao R., Su H., Ji Q.",Generalizing eye tracking with bayesian adversarial learning,"Existing appearance-based gaze estimation approaches with CNN have poor generalization performance. By systematically studying this issue, we identify three major factors: 1) appearance variations; 2) head pose variations and 3) over-fitting issue with point estimation. To improve the generalization performance, we propose to incorporate adversarial learning and Bayesian inference into a unified framework. In particular, we first add an adversarial component into traditional CNN-based gaze estimator so that we can learn features that are gaze-responsive but can generalize to appearance and pose variations. Next, we extend the point-estimation based deterministic model to a Bayesian framework so that gaze estimation can be performed using all parameters instead of only one set of parameters. Besides improved performance on several benchmark datasets, the proposed method also enables online adaptation of the model to new subjects/environments, demonstrating the potential usage for practical real-time eye tracking applications. Â© 2019 IEEE.",,,
10.1109/IACS.2019.8809129,2019,"Al-Btoush A.I., Abbadi M.A., Hassanat A.B., Tarawneh A.S., Hasanat A., Prasath V.B.S.",New Features for Eye-Tracking Systems: Preliminary Results,"Due to their large number of applications, eye-tracking systems have gain attention recently. In this work, we propose 4 new features to support the most used feature by these systems, which is the location (x, y). These features are based on the white areas in the four corners of the sclera; the ratio of the whites area (after segmentation) to the corners area is used as a feature coming from each corner. In order to evaluate the new features, we designed a simple eye-tracking system using a simple webcam, where the users faces and eyes are detected, which allows for extracting the traditional and the new features. The system was evaluated using 10 subjects, who looked at 5 objects on the screen. The experimental results using some machine learning algorithms show that the new features are user dependent, and therefore, they cannot be used (in their current format) for a multiuser eye-tracking system. However, the new features might be used to support the traditional features for a better single-user eye-tracking system, where the accuracy results were in the range of 0.90 to 0.98. Â© 2019 IEEE.",,,
10.1109/DESSERT.2019.8770025,2019,"Rupanagudi S.R., Bhat V.G., Gurikar S.K., Pranava Koundinya S., Sumedh Kumar M.S., Shreyas R., Shilpa S., Suman N.M., Bademi R.R., Koppisetti M., Satyananda V.",A Video Processing Based Eye Gaze Recognition Algorithm for Wheelchair Control,"Over the past few years, a lot of research has been carried out in eye gaze recognition and its applications. From controlling wheelchairs to selecting options on a screen, utilizing the gaze of an individual has become a long-sought way for performing these tasks and in turn making the life of several differently abled people easy. In this paper a novel methodology to perform iris segmentation and gaze recognition has been introduced and described. The method elaborated utilizes a segmentation algorithm which can successfully extract the iris under varying lighting conditions with the help of machine learning. All experiments were conducted using the MATLAB R2013a software and a speed improvement of almost 3.433 times was achieved as opposed to other popular methods of iris extraction. In terms of accuracy, the algorithm proved to be 86% accurate and was also adopted to control an actual wheelchair. Â© 2019 IEEE.",,,
10.1007/s10916-019-1257-x,2019,"Wu H., Lin Q., Yang R., Zhou Y., Zheng L., Huang Y., Wang Z., Lao Y., Huang J.",An Accurate Recognition of Infrared Retro-Reflective Markers in Surgical Navigation,"Marker-based optical tracking systems (OTS) are widely used in clinical image-guided therapy. However, the emergence of ghost markers, which is caused by the mistaken recognition of markers and the incorrect correspondences between marker projections, may lead to tracking failures for these systems. Therefore, this paper proposes a strategy to prevent the emergence of ghost markers by identifying markers based on the features of their projections, finding the correspondences between marker projections based on the geometric information provided by markers, and fast-tracking markers in a 2D image between frames based on the sizes of their projections. Apart from validating its high robustness, the experimental results show that the proposed strategy can accurately recognize markers, correctly identify their correspondences, and meet the requirements of real-time tracking. Â© 2019, Springer Science+Business Media, LLC, part of Springer Nature.",,,
10.1109/TCYB.2018.2820731,2019,"Sun X., Zhang L., Wang Z., Chang J., Yao Y., Li P., Zimmermann R.",Scene Categorization Using Deeply Learned Gaze Shifting Kernel,"Accurately recognizing sophisticated sceneries from a rich variety of semantic categories is an indispensable component in many intelligent systems, e.g., scene parsing, video surveillance, and autonomous driving. Recently, there have emerged a large quantity of deep architectures for scene categorization, wherein promising performance has been achieved. However, these models cannot explicitly encode human visual perception toward different sceneries, i.e., the sequence of humans sequentially allocates their gazes. To solve this problem, we propose deep gaze shifting kernel to distinguish sceneries from different categories. Specifically, we first project regions from each scenery into the so-called perceptual space, which is established by combining color, texture, and semantic features. Then, a novel non-negative matrix factorization algorithm is developed which decomposes the regions' feature matrix into the product of the basis matrix and the sparse codes. The sparse codes indicate the saliency level of different regions. In this way, the gaze shifting path from each scenery is derived and an aggregation-based convolutional neural network is designed accordingly to learn its deep representation. Finally, the deep representations of gaze shifting paths from all the scene images are incorporated into an image kernel, which is further fed into a kernel SVM for scene categorization. Comprehensive experiments on six scenery data sets have demonstrated the superiority of our method over a series of shallow/deep recognition models. Besides, eye tracking experiments have shown that our predicted gaze shifting paths are 94.6% consistent with the real human gaze allocations. Â© 2013 IEEE.",,,
10.1016/j.patrec.2019.03.013,2019,"Wang Y., Huang R., Guo L.",Eye gaze pattern analysis for fatigue detection based on GP-BCNN with ESM,"This paper presents a robust fatigue detection system based on binocular consistency, which integrates artificial modulation into deep learning to guide the learning process and removes the extreme cases of dynamic objects through screening mechanism. Specifically, we first build a dual-stream bidirectional convolutional neural network (BCNN) for eye gaze pattern detection, which uses binocular consistency for information interaction. Then we incorporate vectorized local integral projection features which named projection vectors and Gabor filters into BCNN to construct GP-BCNN that not only enhances the resistance of deep learned features to the orientation and scale changes, but strengthens the learning of texture information. Finally, an eye screening mechanism (ESM) based on pupil distance is proposed to eliminate the detected errors caused by the occluded eyes when the lateral face is detected. Demonstrated by introducing binocular consistency and artificial modulation to convolutional neural network (CNN), GP-BCNN improves the widely used CNNs architectures and yields a 2.9% promotion in the average accuracy rate compared with the results obtained by CNN alone. Our approach obtains the state-of-the-art results in fatigue detection and has the generalization potential in general image recognition tasks. Â© 2019 Elsevier B.V.",,,
10.1364/DH.2019.W2A.6,2019,"Lee S., Kang B., Nam D.",Position prediction for eye-tracking based 3D display,The accurate eye position prediction method is presented for eye-tracking based glasses free 3D display to reduced system latency effect on the 3D image quality. Proposed method is experimentally validated using 3D display prototype. Â© OSA 2019 Â© 2019 The Author(s),,,
10.1145/3290605.3300780,2019,"Kim J., Stengel M., Majercik A., De Mello S., Dunn D., Laine S., McGuire M., Luebke D.","NVGaze: An anatomically-informed dataset for low-latency, near-eye gaze estimation","Quality, diversity, and size of training data are critical factors for learning-based gaze estimators. We create two datasets satisfying these criteria for near-eye gaze estimation under infrared illumination: a synthetic dataset using anatomically-informed eye and face models with variations in face shape, gaze direction, pupil and iris, skin tone, and external conditions (2M images at 1280x960), and a real-world dataset collected with 35 subjects (2.5M images at 640x480). Using these datasets we train neural networks performing with sub-millisecond latency. Our gaze estimation network achieves 2.06(Â±0.44)â—?of accuracy across a wide 30â—?Ã— 40â—?field of view on real subjects excluded from training and 0.5â—?best-case accuracy (across the same FOV) when explicitly trained for one real subject. We also train a pupil localization network which achieves higher robustness than previous methods. Â© 2019 Association for Computing Machinery.",,,
10.1145/3290605.3300855,2019,"Hirzle T., Gugenheimer J., Geiselhart F., Bulling A., Rukzio E.",A design space for gaze interaction on head-mounted displays,"Augmented and virtual reality (AR/VR) has entered the mass market and, with it, will soon eye tracking as a core technology for next generation head-mounted displays (HMDs). In contrast to existing gaze interfaces, the 3D nature of AR and VR requires estimating a userâ€™s gaze in 3D. While first applications, such as foveated rendering, hint at the compelling potential of combining HMDs and gaze, a systematic analysis is missing. To fill this gap, we present the first design space for gaze interaction on HMDs. Our design space covers human depth perception and technical requirements in two dimensions aiming to identify challenges and opportunities for interaction design. As such, our design space provides a comprehensive overview and serves as an important guideline for researchers and practitioners working on gaze interaction on HMDs. We further demonstrate how our design space is used in practice by presenting two interactive applications: EyeHealth and XRay-Vision. Â© 2019 Copyright held by the owner/author(s).",,,
10.1145/3290605.3300842,2019,"Mardanbegi D., Langlotz T., Gellersen H.",Resolving target ambiguity in 3D gaze interaction through VOR depth estimation,"Target disambiguation is a common problem in gaze interfaces, as eye tracking has accuracy and precision limitations. In 3D environments this is compounded by objects overlapping in the field of view, as a result of their positioning at different depth with partial occlusion. We introduce VOR depth estimation, a method based on the vestibulo-ocular reflex of the eyes in compensation of head movement, and explore its application to resolve target ambiguity. The method estimates gaze depth by comparing the rotations of the eye and the head when the users look at a target and deliberately rotate their head. We show that VOR eye movement presents an alternative to vergence for gaze depth estimation, that is feasible also with monocular tracking. In an evaluation of its use for target disambiguation, our method outperforms vergence for targets presented at greater depth. Â© 2019 Association for Computing Machinery.",,,
10.1145/3290605.3300646,2019,"Zhang X., Sugano Y., Bulling A.",Evaluation of appearance-based methods and implications for gaze-based applications,"Appearance-based gaze estimation methods that only require an off-the-shelf camera have significantly improved but they are still not yet widely used in the human-computer interaction (HCI) community. This is partly because it remains unclear how they perform compared to model-based approaches as well as dominant, special-purpose eye tracking equipment. To address this limitation, we evaluate the performance of state-of-the-art appearance-based gaze estimation for interaction scenarios with and without personal calibration, indoors and outdoors, for different sensing distances, as well as for users with and without glasses. We discuss the obtained findings and their implications for the most important gaze-based applications, namely explicit eye input, attentive user interfaces, gaze-based user modelling, and passive eye monitoring. To democratise the use of appearance-based gaze estimation and interaction in HCI, we finally present OpenGaze (www.opengaze.org), the first software toolkit for appearance-based gaze estimation and interaction. Â© 2019 Copyright held by the owner/author(s).",,,
10.1145/3290605.3300839,2019,"Wang X., Ley A., Koch S., Lindlbauer D., Hays J., Holmqvist K., Alexa M.",The mental image revealed by gaze tracking,"Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the userâ€™s eyes to be tracked, but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely recall an image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that these results generalize to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario. Â© 2019 Association for Computing Machinery.",,,
10.1145/3290607.3312942,2019,"Hiroe M., Nagamatsu T., Mitsunaga S.",Implicit user calibration for model-based gaze-tracking system using face detection around optical axis of eye,"In recent studies of gaze tracking system using 3D model-based methods, the optical axis of the eye is estimated without user calibration. The remaining problem for achieving implicit user calibration is to estimate the difference between the optical axis and visual axis of the eye (angle Îš). In this paper, we propose an implicit user calibration method using face detection around the optical axis of the eye. We assume that the peak of the average of face region images indicates the visual axis of the eye in the eye coordinate system. The angle Îš is estimated as the difference between the optical axis of the eye and the peak of the average of face region images. We developed a prototype system with two cameras and two IR-LEDs. The experimental results showed that the proposed method can estimate the angle Îš more accurately than the method that uses Itti's saliency map instead of face detection. Â© 2019 Copyright held by the owner/author(s).",,,
10.1109/ICCE-TW46550.2019.8991784,2019,"Lee K.-F., Chen Y.-L., Yu C.-W., Wu C.-H., Hsiao C.-Y.",Low-cost Wearable Eye Gaze Detection and Tracking System,"This study use headset is adaptable by integration of the elastic mechanism design. This proposed system can effectively extract and estimate pupil ellipse from few camera-captured samples of an eye, and compute the corresponding 3D eye model. Then match the later pupil ellipse to give the possible visual angle. We use multiple points calibration method to solve the related polynomial formula for future angle-to-gaze mapping. The proposed eye tracking algorithms can provide a low-complexity solution and provide high accuracy precision and speed. Â© 2019 IEEE.",,,
10.1109/ICRA.2019.8793804,2019,"Shafti A., Orlov P., Faisal A.A.","Gaze-based, context-aware robotic system for assisted reaching and grasping","Assistive robotic systems endeavour to support those with movement disabilities, enabling them to move again and regain functionality. Main issue with these systems is the complexity of their low-level control, and how to translate this to simpler, higher level commands that are easy and intuitive for a human user to interact with. We have created a multi-modal system, consisting of different sensing, decision making and actuating modalities, leading to intuitive, human-in-the-loop assistive robotics. The system takes its cue from the user's gaze, to decode their intentions and implement low-level motion actions to achieve high-level tasks. This results in the user simply having to look at the objects of interest, for the robotic system to assist them in reaching for those objects, grasping them, and using them to interact with other objects. We present our method for 3D gaze estimation, and grammars-based implementation of sequences of action with the robotic system. The 3D gaze estimation is evaluated with 8 subjects, showing an overall accuracy of 4.68pm 0.14cm. The full system is tested with 5 subjects, showing successful implementation of 100% of reach to gaze point actions and full implementation of pick and place tasks in 96%, and pick and pour tasks in 76% of cases. Finally we present a discussion on our results and what future work is needed to improve the system. Â© 2019 IEEE.",,,
10.1109/IMBIOC.2019.8777841,2019,"Li B., Zhang Y., Zheng X., Huang X., Zhang S., He J.",A Smart Eye Tracking System for Virtual Reality,"Virtual reality (VR) technology provides specific three-dimensional (3D) scenes for users, which could be benefit for the applications in the field of medical diagnosis, psychological analysis, cognitive researches and entertainments. The current VR device provides the virtual 3D images, but cannot synchronously detect the user's eye movements which could be significant for deriving the users' gaze points and interest regions in varied applications. In this paper, we proposed a smart eye tracking system for VR device. Firstly, a smart eye movement detection kit is mounted inside the VR device to capture the human eye movement images with the rate of 30 Hz. Based on the proposed eye detection kit, a gaze detection algorithm is then established for VR devices. Next, a gaze estimation model is proposed for human gaze estimation. The proposed smart eye tracking system can detect the user's eye movements in real time under VR stimulation. Moreover, it also provide a new human-VR interaction mode. Â© 2019 IEEE.",,,
10.1109/ICASSP.2019.8683794,2019,"Jha S., Busso C.",Estimation of Gaze Region Using Two Dimensional Probabilistic Maps Constructed Using Convolutional Neural Networks,"Predicting the gaze of a user can have important applications in human computer interactions (HCI). They find applications in areas such as social interaction, driver distraction, human robot interaction and education. Appearance based models for gaze estimation have significantly improved due to recent advances in convolutional neural network (CNN). This paper proposes a method to predict the gaze of a user with deep models purely based on CNNs. A key novelty of the proposed model is that it produces a probabilistic map describing the gaze distribution (as opposed to predicting a single gaze direction). This approach is achieved by converting the regression problem into a classification problem, predicting the probability at the output instead of a single direction. The framework relies in a sequence of downsampling followed by upsampling to obtain the probabilistic gaze map. We observe that our proposed approach works better than a regression model in terms of prediction accuracy. The average mean squared error between the predicted gaze and the true gaze is observed to be 6.89 in a model trained and tested on the MSP-Gaze database, without any calibration or adaptation to the target user. Â© 2019 IEEE.",,,
10.1016/j.ijhcs.2018.12.010,2019,"Causse M., Lancelot F., Maillant J., Behrend J., Cousy M., Schneider N.",Encoding decisions and expertise in the operator's eyes: Using eye-tracking as input for system adaptation,"We investigated the possibility of developing a decision support system (DSS) that integrates eye-fixation measurements to better adapt its suggestions. Indeed, eye fixation give insight into human decision-making: Individuals tend to pay more attention to key information in line with their upcoming selection. Thus, eye-fixation measures can help the DSS to better capture the context that determines user decisions. Twenty-two participants performed a simplified Air Traffic Control (ATC) simulation in which they had to decide to accept or to modify route suggestions according to specific parameter values displayed on the screen. Decisions and fixation times on each parameter were recorded. The user fixation times were used by an algorithm to estimate the utility of each parameter for its decision. Immediately after this training phase, the algorithm generated new route suggestions under two conditions: 1) Taking into account the participant's decisions, 2) Taking into account the participant's decisions plus their visual behavior using the measurements of dwell times on displayed parameters. Results showed that system suggestions were more accurate than the base system when taking into account the participant's decisions, and even more accurate using their dwell times. Capturing the crucial information for the decision using the eye tracker accelerated the DSS learning phase, and thus helped to further enhance the accuracy of consecutive suggestions. Moreover, exploratory eye-tracking analysis reflected two different stages of the decision-making process, with longer dwell times on relevant parameters (i.e. involved in a rule) during the entire decision time course, and frequency of fixations on these relevant parameters that increased, especially during the last fixations prior to the decision. Consequently, future DSS integrating eye-tracking data should pay specific care to the final fixations prior to the decision. In general, our results emphasize the potential interest of eye-tracking to enhance and accelerate system adaptation to user preference, knowledge, and expertise. Â© 2018",,,
10.1016/j.neucom.2019.01.102,2019,"Jiang B., Zhang Y., Tang J., Luo B., Li C.",Robust visual tracking via Laplacian Regularized Random Walk Ranking,"Visual tracking is a fundamental and important problem in computer vision and pattern recognition. Existing visual tracking methods usually localize the visual object with a bounding box. Recently, learning the patch-based weighted features has been demonstrated to be an effective way to mitigate the background effects in the target bounding box descriptions, and can thus improve tracking performance significantly. In this paper, we propose a simple yet effective approach, called Laplacian Regularized Random Walk Ranking (LRWR), to learn more robust patch-based weighted features of the target object for visual tracking. The main advantages of our LRWR model over existing methods are: (1) it integrates both local spatial and global appearance cues simultaneously, and thus leads to a more robust solution for patch weight computation; (2) it has a simple closed-form solution, which makes our tracker efficiently. The learned features are incorporated into the structured SVM to perform object tracking. Experiments show that our approach performs favorably against the state-of-the-art trackers on two standard benchmark datasets. Â© 2019 Elsevier B.V.",,,
10.1109/MIPR.2019.00090,2019,"O'Reilly J., Khan A.S., Li Z., Cai J., Hu X., Chen M., Tong Y.",A Novel Remote Eye Gaze Tracking System Using Line Illumination Sources,"This paper proposes a novel system to estimate the 3D point of gaze using observations of the pupil center and corneal reflections (glints). A mathematical model is developed with two solutions to estimate the corneal center efficiently using lines of LED lights. Differing from existing 3D approaches requiring associating light sources with glints, the model automatically associates glints and LED lines and can handle missing glints well. The new model also enables a user-friendly calibration process, allowing natural head movement. Experiments demonstrate that the proposed system can achieve accurate gaze estimation with natural head movement. The performance is impressive when using the natural calibration, requiring less user cooperation. Â© 2019 IEEE.",,,
10.1145/3299815.3314424,2019,"Veliyath N., De P., Allen A.A., Hodges C.B., Mitra A.",Modeling studentsâ€?attention in the classroom using eyetrackers,"The process of learning is not merely determined by what the instructor teaches, but also by how the student receives that information. An attentive student will naturally be more open to obtaining knowledge than a bored or frustrated student. In recent years, tools such as skin temperature measurements and body posture calculations have been developed for the purpose of determining a studentâ€™s affect, or emotional state of mind. However, measuring eye-gaze data is particularly noteworthy in that it can collect measurements non-intrusively, while also being relatively simple to set up and use. This paper details how data obtained from an eye-tracker can indeed be used to predict a studentâ€™s attention as a measure of affect over the course of a class. From this research, an accuracy of 77% was achieved using the Extreme Gradient Boosting technique of machine learning. The outcome indicates that eye-gaze can be indeed used as a basis for constructing a predictive model. Â© 2019 Association for Computing Machinery.",,,
10.1080/01691864.2019.1600426,2019,"Song S., Yamada S.",Designing LED lights for a robot to communicate gaze,"Eye gaze is considered to be a particularly important non-verbal communication cue. Gaze research is also becoming a hot topic in humanâ€“robot interaction (HRI). However, research on social eye gaze for HRI focuses mainly on human-like robots. There remains a lack of methods for functional robots, which are constrained in appearance, to show gaze-like behavior. In this work, we investigate how we can implement gaze behavior in functional robots to assist humans in reading their intent. We explore design implications based on LED lights as we consider LEDs to be easily installed in most robots while not introducing features that are too human-like (to prevent users from having high expectations towards the robots). In this paper, we first developed a design interface that allows designers to freely test different parameter settings for an LED-based gaze display for a Roomba robot. We summarized design principles for well simulating LED-based gazes. Our suggested design is further evaluated by a large group of participants with regard to their perception and interpretation of the robot's behaviors. On the basis of the findings, we finally offer a set of design implications that can be beneficial to HRI and HCI researchers. Â© 2019, Â© 2019 Informa UK Limited, trading as Taylor & Francis Group and The Robotics Society of Japan.",,,
10.1007/s00138-018-00997-4,2019,"Liu Y., Lee B.-S., Rajan D., Sluzek A., McKeown M.J.",CamType: assistive text entry using gaze with an off-the-shelf webcam,"As modern assistive technology advances, eye-based text entry systems have been developed to help a subset of physically challenged people to improve their communication ability. However, speed of text entry in early eye-typing system tends to be relatively slow due to dwell time. Recently, dwell-free methods have been proposed which outperform the dwell-based systems in terms of speed and resilience, but the extra eye-tracking device is still an indispensable equipment. In this article, we propose a prototype of eye-typing system using an off-the-shelf webcam without the extra eye tracker, in which the appearance-based method is proposed to estimate peopleâ€™s gaze coordinates on the screen based on the frontal face images captured by the webcam. We also investigate some critical issues of the appearance-based method, which helps to improve the estimation accuracy and reduce computing complexity in practice. The performance evaluation shows that eye typing with webcam using the proposed method is comparable to the eye tracker under a small degree of head movement. Â© 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",,,
10.1080/13658816.2018.1482554,2019,"Liao H., Dong W., Huang H., Gartner G., Liu H.",Inferring user tasks in pedestrian navigation from eye movement data in real-world environments,"Eye movement data convey a wealth of information that can be used to probe human behaviour and cognitive processes. To date, eye tracking studies have mainly focused on laboratory-based evaluations of cartographic interfaces; in contrast, little attention has been paid to eye movement data mining for real-world applications. In this study, we propose using machine-learning methods to infer user tasks from eye movement data in real-world pedestrian navigation scenarios. We conducted a real-world pedestrian navigation experiment in which we recorded eye movement data from 38 participants. We trained and cross-validated a random forest classifier for classifying five common navigation tasks using five types of eye movement features. The results show that the classifier can achieve an overall accuracy of 67%. We found that statistical eye movement features and saccade encoding features are more useful than the other investigated types of features for distinguishing user tasks. We also identified that the choice of classifier, the time window size and the eye movement features considered are all important factors that influence task inference performance. Results of the research open doors to some potential real-world innovative applications, such as navigation systems that can provide task-related information depending on the task a user is performing. Â© 2018, Â© 2018 Informa UK Limited, trading as Taylor & Francis Group.",,,
10.35940/ijitee.F1208.0486S419,2019,"Modak M., Ghotane K., Siddhanth V., Kelkar N., Iyer A., Prachi G.",Detection of dyslexia using eye tracking measures,"Dyslexia is one of the most common and hidden learning disabilities found in people, especially in the young age. It particularly affects reading, where the impaired reader takes a longer time to read and grasp the concept than the non-impaired reader. This further leads to academic failures. So studies to detect such issues have been conducted considering various factors like the reading times, fixation times, number of saccades(sudden movements in the eye), of both the impaired and non-impaired subjects, and give the best possible results. Thus, we plan to use the same eye tracking technique supported with machine learning models to detect and classify the individuals with and without dyslexia. The factors considered during the study are font-size, typeface, frequency of words(fixation times of non-impaired readers are more if frequency of encountered words is less) and age(people with learning disorders tend to enhance their reading skills with age), etc. Â© BEIESP.",,,
10.1109/ECAI.2018.8678951,2019,"Rehman H.U., Naeem M., Khan M., Sikander G., Anwar S.",Eye Tracking based Real- Time Non-Interfering Driver Fatigue Detection System,"Driver fatigue, cause fatal road accidents, and is a major socio-economic concern. The human body exhibits signs of fatigue, which can provide us with the information needed for fatigue detection. This study presents to develop an eye tracking based real time noninterfering driver fatigue detection system. In the proposed method Haar-like features based cascade classifiers are utilized to detect the face and eyes of the driver, skin colour based segmentation is used to calculate eye openness and Support Vector Machine (SVM) to deduce the eyes state as fatigued or non-fatigued. Â© 2018 IEEE.",,,
10.1109/LRA.2019.2895419,2019,"Yuan L., Reardon C., Warnell G., Loianno G.",Human gaze-driven spatial tasking of an autonomous MAV,"In this letter, we address the problem of providing human-assisted quadrotor navigation using a set of eye tracking glasses. The advent of these devices (i.e., eye tracking glasses, virtual reality tools, etc.) provides the opportunity to create new, noninvasive forms of interaction between humans and robots. We show how a set of glasses equipped with gaze tracker, a camera, and an inertial measurement unit (IMU) can be used to estimate the relative position of the human with respect to a quadrotor, and decouple the gaze direction from the head orientation, which allows the human to spatially task (i.e., send new 3-D navigation waypoints to) the robot in an uninstrumented environment. We decouple the gaze direction from head motion by tracking the human's head orientation using a combination of camera and IMU data. In order to detect the flying robot, we train and use a deep neural network. We experimentally evaluate the proposed approach, and show that our pipeline has the potential to enable gaze-driven autonomy for spatial tasking. The proposed approach can be employed in multiple scenarios including inspection and first response, as well as by people with disabilities that affect their mobility. Â© 2016 IEEE.",,,
10.1016/j.compbiomed.2019.02.003,2019,"Cercenelli L., Tiberi G., Bortolani B., Giannaccare G., Fresina M., Campos E., Marcelli E.",Gaze Trajectory Index (GTI): A novel metric to quantify saccade trajectory deviation using eye tracking,"Background: Many different indexes have been proposed to quantify saccade curvature based on geometric properties of the saccade trajectory projected on the 2D plane. We introduce the Gaze Trajectory Index (GTI), a novel metric to quantify saccade trajectory deviation based on calculation of the rotational eye movements performed in 3D space while following a 2D saccade trajectory recorded with eye tracking (ET). Methods: We provided a description of GTI calculation. In 13 subjects with normal binocular vision we assessed GTI in single-target tests, then we evaluated GTI against previously proposed metrics (Maximum Deviation,MD; Area Curvature,AC; Quadratic Curvature,QC; Initial Direction,ID) using a distractor paradigm that elicited two types of saccade deviations, i.e.â€œinner-curvedâ€?and â€œouter-curvedâ€?saccades. Results: In single-target tests GTI showed that saccade curvature was significantly higher for oblique than for vertical saccades (0.86Â°Â±0.32 vs 0.55Â°Â±0.60,p < 0.05) and higher for vertical than for horizontal saccades (0.55Â°Â±0.60 vs 0.23Â°Â±0.17,p < 0.05), in accordance with previous studies. In distractor-based tests, for inner-curved saccades, GTI strongly correlated with MD (r = 0.965,p < 0.01), AC (r = 0.940,p < 0.01), QC (r = 0.866,p < 0.01), and Principal Component Analysis (PCA) confirmed that all these metrics reflect the same underlying phenomenon. For outer-curved trajectories, GTI showed poor correlation with MD and AC (r = 0.291 and 0.416,p < 0.01), however PCA included the three metrics in the same first component group. For outer-curved trajectories, GTI was the only metric showing strong correlation (r = 0.950,p < 0.05) with the overshoot degree of the trajectory. Conclusion: The novel GTI seems to have adjunctive potential, particularly for outer-curved trajectories, in the estimation of the absolute amount of saccade trajectory deviation. Â© 2019 Elsevier Ltd",,,
10.1145/3361570.3361577,2019,"Salminen J., Kwak H., Jung S.-G., Nagpal M., An J., Jansen B.J.",Confusion prediction from eye-tracking data: Experiments with machine learning,"Predicting user confusion can help improve information presentation on websites, mobile apps, and virtual reality interfaces. One promising information source for such prediction is eye-tracking data about gaze movements on the screen. Coupled with think-aloud records, we explore if user's confusion is correlated with primarily fixation-level features. We find that random forest achieves an accuracy of more than 70% when prediction user confusion using only fixation features. In addition, adding user-level features (age and gender) improves the accuracy to more than 90%. We also find that balancing the classes before training improves performance. We test two balancing algorithms, Synthetic Minority Over Sampling Technique (SMOTE) and Adaptive Synthetic Sampling (ADASYN) finding that SMOTE provides a higher performance increase. Overall, this research contains implications for researchers interested in inferring users' cognitive states from eye-tracking data. Â© 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1016/j.neucom.2019.01.023,2019,"Chen W., Liao T., Li Z., Lin H., Xue H., Zhang L., Guo J., Cao Z.",Using FTOC to track shuttlecock for the badminton robot,"Sport video analysis is gaining popularity recently owing to its importance in understanding sports and improving the performance of athletes. In this paper we focus on shuttlecock tracking algorithm. Particularly, a novel fast tracking based on object center (i.e., FTOC) method by fusing heterogeneous cues and AdaBoost algorithm are proposed to improve the tracking performance for a robot. Experimental results show that the proposed FTOC tracking method performs favorably against many other popular tracking approaches, such as TLD, MIL, KCF, DCF_CA, SMAF_CA, KCC, DSN, COKCF, etc., in term of speed, accuracy, and robustness, especially in challenging scenarios such as scale variations and background clutter. We further demonstrate the feasibility of the FTOC algorithm in a real-time ZED binocular camera based 3D shuttlecock tracking system for a robot. Â© 2019 Elsevier B.V.",,,
10.1109/ICAIIC.2019.8669057,2019,"Jigang L., Francis B.S.L., Rajan D.",Free-Head Appearance-Based Eye Gaze Estimation on Mobile Devices,"Eye gaze tracking plays an important role in human-computer interaction applications. In recent years, many research have been performed to explore gaze estimation methods to handle free-head movement, most of which focused on gaze direction estimation. Gaze point estimation on the screen is another important application. In this paper, we proposed a two-step training network, called GazeEstimator, to improve the estimation accuracy of gaze location on mobile devices. The first step is to train an eye landmarks localization network on 300W-LP dataset [1], and the second step is to train a gaze estimation network on GazeCapture dataset [2]. Some processing operations are performed between the two networks for data cleaning. The first network is able to localize eye precisely on the image, while the gaze estimation network use only eye images and eye grids as inputs, and it is robust to facial expressions and occlusion.Compared with state-of-The-Art gaze estimation method, iTracker, our proposed deep network achieves higher accuracy and is able to estimate gaze location even in the condition that the full face cannot be detected. Â© 2019 IEEE.",,,
10.1109/ICOSC.2019.8665670,2019,"Fu B., Steichen B.",Using Behavior Data to Predict User Success in Ontology Class Mapping - An Application of Machine Learning in Interaction Analysis,"Ontology visualization has played an important role in human data interaction by offering clarity and insight for complex structured datasets. Recent usability evaluations of ontology visualization techniques have added to our understanding of desired features when assisting users in the interactive process. However, user behavior data such as eye gaze and event logs have largely been used as indirect evidence to explain why a user may have carried out certain tasks in a controlled environment as opposed to direct input that informs the underlying visualization system. Although findings from usability studies have contributed to the refinement of ontology visualizations as a whole, the visualization techniques themselves remain a one-size-fits-all approach where all users are presented with the same visualizations and interactive features. By contrast, this paper investigates how user behavior data may offer real time indications as to how appropriate or effective a given visualization may be for a specific user at a moment in time, which in turn may inform the adaptation of the given visualization to the user on the fly. To this end, we apply established predictive modeling techniques in Machine Learning to predict user success using gaze data and event logs. We present a detailed analysis and demonstrate such predictions can be significantly better than a baseline classifier during visualization usage. These predictions can then be used to drive the adaptations of visual systems in providing ad hoc visualizations on a per user basis, which in turn may increase individual user success and performance. Â© 2019 IEEE.",,,
10.1145/3311823.3311865,2019,"Fujii K., Rekimoto J.",Subme an interactive subtitle system with english skill estimation using eye tracking,"Owing to the improvement in accuracy of eye tracking devices, eye gaze movements occurring while conducting tasks are now a part of physical activities that can be monitored just like other life-logging data. Analyzing eye gaze movement data to predict reading comprehension has been widely explored and researchers have proven the potential of utilizing computers to estimate the skills and expertise level of users in various categories, including language skills. However, though many researchers have worked specifically on written texts to improve the reading skills of users, little research has been conducted to analyze eye gaze movements in correlation to watching movies, a medium which is known to be a popular and successful method of studying English as it includes reading, listening, and even speaking, the later of which is attributed to language shadowing. In this research, we focus on movies with subtitles due to the fact that they are very useful in order to grasp what is occurring on screen, and therefore, overall understanding of the content. We realized that the viewers' eye gaze movements are distinct depending on their English level. After retrieving the viewers' eye gaze movement data, we implemented a machine learning algorithm to detect their English levels and created a smart subtitle system called SubMe. The goal of this research is to estimate English levels through tracking eye movement. This was conducted by allowing the users to view a movie with subtitles. Our aim is create a system that can give the user certain feedback that can help improve their English studying methods. Â©2019 Association for Computing Machinery.",,,
10.1080/21681163.2018.1463175,2019,"Elbalaoui A., Fakir M.",Exudates detection in fundus images using mean-shift segmentation and adaptive thresholding,"Diabetic retinopathy (DR) affects changes to retinal blood vessels that can cause them to bleed or leak fluid and distorting vision. An early detection of exudates is a prerequisite for detecting and grading severe retinal lesions, like DR. This paper presents an automated method for detection of the exudates in digital fundus images. Our approach can be divided into four steps: shifting colour correction, Optic disc (OD) elimination, exudates segmentation and separation of exudatesÂ from background. In order to correct non-uniform illumination, we adopted the grey world method. Then, we must extract the OD prior to the process because it appears with similar colour, intensity and contrast to exudates. Next, to segment the exudates, we applied the mean-shift method. Finally, we used the maximum entropy thresholding to separate the exudates from background. The proposed method is tested on DIARETDB0 and DIARETDB1. Comparing to other recent methods available in the literature, our proposed approach obtains better exudate detection results in terms of sensitivity, specificity and accuracy. Â© 2018, Â© 2018 Informa UK Limited, trading as Taylor & Francis Group.",,,
10.1109/IESPC.2019.8902367,2019,"Das P.J., Talukdar A.K., Sarma K.K.",A Framework for Human Behaviour Detection Using Combined Analysis of Facial Expression and Eye Gaze,"Facial appearances like a happy, sad, disgust, surprise, angry, fear and Eye Gaze Estimations are the fastest means of communication while conveying any type of information. Expressions not only expose the sensitivity or feelings of any person but can also be used to judge his/her mental states. Therefore, it has become a wide interest area of research due to its applications to the fields like Human Computer Interface (HCI), Man Machine Interface (MMI) etc. In order to get more effective human behavior identification, we have presented a combination of facial expression recognition and eye gaze estimation technique where we have to recognize the real time expressions by using Convolutional Neural Network (CNN) model with transfer learning method. After that, we determine the eye gaze by using Viola Jones, Circular Hough Transform (CHT) and a geometric method. With the combination of both processes, we able to predict the human behavior like drivers concentration levels. In this way experiments are carried out on MUG database for our own trained CNN model based on VGG16 (Visual Geometry Group) pre trained model and gives better performance with accuracy of 93% for training and 94% for overall process. Â© 2019 IEEE.",,,
10.1109/VR.2019.8797816,2019,"Alghofaili R., Solah M.S., Huang H.",Optimizing visual element placement via visual attention analysis,"Eye-tracking enables researchers to conduct complex analysis on human behavior. With the recent introduction of eye-tracking into consumer-grade virtual reality headsets, the barrier of entry to visual attention analysis in virtual environments has been lowered significantly. Whether for arranging artwork in a virtual museum, posting banners for virtual events or placing advertisements in virtual worlds, analyzing visual attention patterns provides a powerful means for guiding visual element placement. In this work, we propose a novel data-driven optimization approach for automatically analyzing visual attention and placing visual elements in 3D virtual environments. Using an eye-tracking virtual reality headset, we collect eye-tracking data which we use to train a regression model for predicting gaze duration. We then use the predicted gaze duration output of our regressors to optimize the placement of visual elements with respect to certain visual attention and design goals. Through experiments in several virtual environments, we demonstrate the effectiveness of our optimization approach for predicting gaze duration and for placing visual elements in different practical scenarios. Our approach is implemented as a useful plug-in that level designers can use to automatically populate visual elements in 3D virtual environments. Â© 2019 IEEE.",,,
10.1109/VR.2019.8798327,2019,"Yoshimura A., Khokhar A., Borst C.W.",Eye-gaze-triggered visual cues to restore attention in educational VR,"In educational virtual reality, it is important to deal with problems of student inattention to presented content. We are developing attention-restoring visual cues for display when gaze tracking detects that student focus shifts away from critical objects. These cues include novel aspects and variations of standard cues that performed well in prior work on visual guidance. For the longer term, we propose experiments to compare various cues and their parameters to assess effectiveness and tradeoffs, and to assess the impact of eye tracking. Eye tracking is used to both detect inattention and to control the appearance and location of cues. Â© 2019 IEEE.",,,
10.1109/VR.2019.8797988,2019,"Mardanbegi D., Mayer B., Pfeuffer K., Jalaliniya S., Gellersen H., Perzl A.",EyeSeeThrough: Unifying tool selection and application in virtual environments,"In 2D interfaces, actions are often represented by fixed tools arranged in menus, palettes, or dedicated parts of a screen, whereas 3D interfaces afford their arrangement at different depths relative to the user and the user can move them relative to each other. In this paper, we introduce EyeSeeThrough as a novel interaction technique that utilizes eye-tracking in VR. The user can apply an action to an intended object by visually aligning the object with the tool at the line-of-sight, and then issue a confirmation command. The underlying idea is to merge the two-step process of 1) selection of a mode in a menu and 2) applying it to a target, into one unified interaction. We present a user study where we compare the method to the baseline two-step selection. The results of our user study showed that our technique outperforms the two step selection in terms of speed and comfort. We further developed a prototype of a virtual living room to demonstrate the practicality of the proposed technique. Â© 2019 IEEE.",,,
10.1109/PERCOMW.2019.8730846,2019,"Wong E.T., Yean S., Hu Q., Lee B.S., Liu J., Deepu R.",Gaze Estimation Using Residual Neural Network,"Eye gaze tracking has become an prominent research topic in human-computer interaction and computer vision. It is due to its application in numerous fields, such as the market research, medical, neuroscience and psychology. Eye gaze tracking is implemented by estimating gaze (gaze estimation) for each individual frame in offline or real-time video captured. Therefore, in order to produce the secure the accurate tracking, especially in the emerging use in medical and community, innovation on the gaze estimation posts a challenge in research field. In this paper, we explored the use of the deep learning model, Residual Neural Network (ResNet-18), to predict the eye gaze on mobile device. The model is trained using the large-scale eye tracking public dataset called GazeCapture. We aim to innovate by incorporating methods/techniques of removing the blinking data, applying image histogram normalisation, head pose, and face grid features. As a result, we achieved 3.05cm average error, which is better performance than iTracker (4.11cm average error), the recent gaze tracking deep-learning model using AlexNet architecture. Upon observation, adaptive normalisation of the images was found to produce better results compared to histogram normalisation. Additionally, we found that head pose information was useful contribution to the proposed deep-learning network, while face grid information does not help to reduce test error. Â© 2019 IEEE.",,,
10.1109/IC3D.2018.8657902,2019,"Seychell D., Debono C.J.",Ranking regions of visual saliency in RGB-D content,"Effective immersion takes place when the user can relate to the 3D environment presented and interact with key objects. Efficiently predicting which objects in a scene are in the user's attention, without using additional hardware, such as eye tracking solutions, provides an opportunity for creating more immersive scenes in real time and at lower costs. This is nonetheless algorithmically challenging. In this paper, we are proposing a technique that efficiently and effectively identifies the most salient objects in a scene. We show how it accurately matches user selection within 0.04s and is over 95% faster than other saliency algorithms while also providing a ranking of the most salient segments in a scene. Â© 2018 IEEE.",,,
10.1111/cogs.12716,2019,"KrÃ³l M., KrÃ³l M.","Learning From Peersâ€?Eye Movements in the Absence of Expert Guidance: A ProofÂ of Concept Using Laboratory Stock Trading, Eye Tracking, and Machine Learning","Existing research shows that people can improve their decision skills by learning what experts paid attention to when faced with the same problem. However, in domains like financial education, effective instruction requires frequent, personalized feedback given at the point of decision, which makes it time-consuming for experts to provide and thus, prohibitively costly. We address this by demonstrating an automated feedback mechanism that allows amateur decision-makers to learn what information to attend to from one another, rather than from an expert. In the first experiment, eye movements of NÂ =Â 100 subjects were recorded while they repeatedly performed a standard behavioral finance investment task. Consistent with previous studies, we found that a significant proportion of subjects were affected by decision bias. In the second experiment, a different group of NÂ =Â 100 subjects faced the same task but, after each choice, they received individual, machine learning-generated feedback on whether their pre-decision eye movements resembled those made by Experiment 1 subjects prior to good decisions. As a result, Experiment 2 subjects learned to analyze information similarly to their successful peers, which in turn reduced their decision bias. Furthermore, subjects with low Cognitive Reflection Test scores gained more from the proposed form of process feedback than from standard behavioral feedback based on decision outcomes. Â© 2019 Cognitive Science Society, Inc.",,,
10.1016/j.cviu.2019.01.002,2019,"Ariz M., Villanueva A., Cabeza R.",Robust and accurate 2D-tracking-based 3D positioning method: Application to head pose estimation,"Head pose estimation (HPE) is currently a growing research field, mainly because of the proliferation of humanâ€“computer interfaces (HCI) in the last decade. It offers a wide variety of applications, including human behavior analysis, driver assistance systems or gaze estimation systems. This article aims to contribute to the development of robust and accurate HPE methods based on 2D tracking of the face, enhancing performance of both 2D point tracking and 3D pose estimation. We start with a baseline method for pose estimation based on POSIT algorithm. A novel weighted variant of POSIT is then proposed, together with a methodology to estimate weights for the 2Dâ€?D point correspondences. Further, outlier detection and correction methods are also proposed in order to enhance both point tracking and pose estimation. With the aim of achieving a wider impact, the problem is addressed using a global approach: all the methods proposed are generalizable to any kind of object for which an approximate 3D model is available. These methods have been evaluated for the specific task of HPE using two different head pose video databases; a recently published one that reflects the expected performance of the system in current technological conditions, and an older one that allows an extensive comparison with state-of-the-art HPE methods. Results show that the proposed enhancements improve the accuracy of both 2D facial point tracking and 3D HPE, with respect to the implemented baseline method, by over 15% in normal tracking conditions and over 30% in noisy tracking conditions. Moreover, the proposed HPE system outperforms the state of the art on the two databases. Â© 2019 The Authors",,,
10.1007/s11042-018-6371-0,2019,"Gautam G., Mukhopadhyay S.",An adaptive localization of pupil degraded by eyelash occlusion and poor contrast,"The inner boundary of iris represents the pupilâ€™s edge. Hence, to work an Iris Recognition System (IRS) and the gaze tracking system expeditiously it is important to locate it as precisely as possible in a significant amout of time. In the presence of non-ideal constraints e.g. non-uniform illumination, poor contrast, eyelashes, hairs, glasses, off-angle orientation, these systems may not work well. In this paper we present an adaptive pupil localization method based on the roundness criteria. First, it applies a gray level inversion to suppress the reflections, then it performs Gray level co-occurrence matrix (GLCM) based contrast estimation. If this estimated contrast is lower than a certain threshold, the input image is made to undergo gamma correction to adjust the contrast. Subsequently, anisotropic diffusion filtering followed by log transformation is applied, which suppresses the effect of eyelash occlusion, limits the creation of small regions and highlight the dark pixels. Afterwards, a clean binary image with few regions is acquired using adaptive thresholding and some morphological operations. Finally, the roundness metric is computed for each of these regions and the region with largest roundness metric, also being greater than a prescribed threshold, declared as pupil. Experiments were carried out on few well known databases, NICE1, CASIA V3 lamp, MMU, WVU and IITD. The results are grounded upon subjective and objective evaluation; which in turn, indicate that our method outperforms a state-of-the-art approach and a deep learning approach in terms of localization capability in some unconstrained scenarios and shorter processing time. After assessing the performance of the proposed algorithm, it is manifested that it ensures a fast and robust localization of pupil in the presence of corneal reflection, poor contrast, glasses and eyelash occlusion. Â© 2018, Springer Science+Business Media, LLC, part of Springer Nature.",,,
10.1109/ISSPIT.2018.8642701,2019,"Hosseinkhani J., Joslin C.",Investigating into Saliency Priority of Bottom-up Attributes in 2D Videos Without Cognitive Bias,"Saliency in an image or video is a region of interest that stands out relative to its neighbors and consequently attracts more human attention. A key factor in designing an algorithm to measure the importance and distinctiveness (i.e. saliency) of different regions of a frame is to understand how different visual cues affect the human perceptual and visual system. To this end, we investigated bottom-up features including color, texture, and motion in 2D video sequences for both one-by-one and combined scenarios to provide a ranking system stating the most dominant circumstances for each feature individually and in combination with other features as well. In this work, we mostly considered the feature combination scenarios under conditions in which we had no cognitive bias. Human cognition refers to a systematic pattern of perceptual and rational judgements and decision-making actions. Since computers do not typically have this ability, we tried to minimize this bias in the design of our experiment. First, we modelled our test data as 2D images and videos in a virtual environment to avoid any cognitive bias. Then, we performed an experiment using human subjects to determine which colors, textures, motion directions, and motion speeds attract human attention more. The proposed ranking system of salient visual attention stimuli was achieved using an eye-tracking procedure. This work provides a benchmark to specify the most salient stimulus with comprehensive information for both static and dynamic scenes. The main goal of this work is to create the ability of assigning a ranking of saliency for the entirety of an image/video frame rather than simply extracting a salient object/area which is widely performed in the state-of-the-art. Â© 2018 IEEE.",,,
10.1109/CogInfoCom.2018.8639898,2019,"TÃ¶rÃ¶k Z.G., TÃ¶rÃ¶k A.",Looking at the map - Or navigating in a virtual city: Interaction of visuospatial display and spatial strategies in VR,"To study geo-visualization processes a Cognitive Cartography Lab was established at EÃ¶tvÃ¶s University, and the ""Virtual Tourist"" experiment was designed for the better understanding of actual map use during navigation. In this paper we present some preliminary results of the experiment. We explored the use of a static, north-oriented city map during navigation in an interactive, 3D town. Participants explored the virtual environment or followed verbal instructions before they completed spatial tasks. Their spatial behavior, verbal reactions were recorded, and also eye tracking data from 64 participants was collected. The experiment was designed by a multidisciplinary research group, including students of EÃ¶tvÃ¶s LorÃ¡nd University. Â© 2018 IEEE.",,,
10.1109/IISA.2018.8633693,2019,"Micelli L., Acosta D., Uribe-Quevedo A., Lamberti F., Kapralos B.","Extending upper limb user interactions in AR, VR and MR headsets employing a custom-made wearable device","Upper limb interactions play an important role in virtual, augmented, and mixed reality scenarios. Numerous sensors including optical, magnetic, mechanical, and myography, have been employed to provide more natural interactions in comparison to game controllers. Recently, virtual, augmented, and mixed reality headsets have started embedding hand tracking sensors to simplify the hardware requirements, thus easing operation and setup. The tracking integration is being referred to as inside/out tracking, whereby hand and eye tracking interactions without external sensors or controllers provide interactive freedom. However, the motion capture area of the inside/out sensors is limited to the field of view and technical features of the cameras, and restricted to a few hand tracking gestures. In this paper, we introduce a custom upper limb motion tracking device that extends the user's interaction range while employing a virtual, augmented, or mixed reality headset. Our 3D motion tracking system is a compact wireless wearable prototype that uses inertial measurement units providing orientation and position data employed for upper limb user interaction outside the field of view of the inside/out sensors. Â© 2018 IEEE",,,
10.1109/SSCI.2018.8628799,2019,"GÃ¼ndÃ¼z A., Najjar T.",Analysis Eye Movements during Reading by Machine Learning Algorithms: A Review Paper,"With today's eye-tracking technologies, it is possible to analyze cognitive processes such as reading. There are many studies on that topic. We have studied some of these studies that analyze eye movements recording during reading by various machine learning algorithms. Â© 2018 IEEE.",,,
10.1145/3310986.3311001,2019,"Van Huynh T., Yang H.-J., Lee G.-S., Kim S.-H., Na I.-S.",Emotion recognition by integrating eye movement analysis and facial expression model,"This paper presents an emotion recognition method which combines knowledge from the face and eye movements to improve the system accuracy. Our method has three fundamental stages to recognize the emotion. Firstly, we use a deep learning model to obtain the probability of a sample belonging to each emotion. Then, the eye movement features are extracted from an open-source framework which implements algorithms that demonstrated state-of-the-art results in this task. A new set of 51 features have been used to obtain related information about each emotion for the corresponding sample. Finally, the emotion for a sample is recognized based on the combination of the knowledge from the two previous stages. Experiment on the validation set of Acted Facial Expressions in the Wild (AFEW) dataset shows that the eye movements can make 2.87%improvement in the accuracy for the face model. Â© 2019 Association for Computing Machinery.",,,
10.1109/BigData.2018.8622501,2019,"Thapaliya S., Jayarathna S., Jaime M.",Evaluating the EEG and Eye Movements for Autism Spectrum Disorder,"Autism Spectrum Disorder is a developmental disorder that often impairs a child's normal development of the brain. Early Diagnosis is crucial in the long term treatment of ASD, but this is challenging due to the lack of a proper objective measures. Subjective measures often take more time, resources, and have false positives or false negatives. There is a need for efficient objective measures that can help in diagnosing this disease early as possible with less effort. This paper presents EEG and Eye movement data for the diagnosis of ASD using machine learning algorithms. There are number of studies on classification of ASD using EEG or Eye tracking data. However, all of them simply use either Eye movements or EEG data for the classification. In our study we combine Eye movements and EEG data to develop an efficient methodology for diagnosis. This paper presents several models based on EEG, and eye movements for the diagnosis of ASD. Â© 2018 IEEE.",,,
10.1109/RCAR.2018.8621810,2019,"Li P., Hou X., Wei L., Song G., Duan X.",Efficient and low-cost deep-learning based gaze estimator for surgical robot control,"Surgical robots are playing more and more important role in modern operating room. However, operations by using surgical robot are not easy to handle by doctors. Vision based human-computer interaction (HCI) is a way to ease the difficulty to control surgical robots. While the problem of this method is that eyes tracking devices are expensive. In this paper, a low cost and robust deep-learning based on gaze estimator is proposed to control surgical robots. By this method, doctors can easily control the robot by specifying the starting point and ending point of the surgical robot using eye gazing. Surgical robots can also be controlled to move in 9 directions using controllers' eyes gazing information. A Densely Connected convolutional Neural Networks (Dense CNN) model for 9-direction/36-direction gaze estimation is built. The Dense CNN architecture has much more less trainable parameters compared to traditional CNN network architecture (AlexNet like/VGG like) which is more feasible to deploy on the Field-Programmable Gate Array (FPGA) and other hardware with limited memories. Â© 2018 IEEE.",,,
10.1109/SMC.2018.00109,2019,"Ai G., Hagio M., Ichiki M., Wagatsuma H.",Simultaneous Analysis of EEGs and Movements in Interative Hand Shaking Required Skills to Synchronize Cooperatively in Game,"Data analysis in the simultaneous recording of behavioral and brain activities such as electroencephalography (EEG) is highly important not only for the fundamental analysis of the functional correspondence between brain signals and an actual behavior, but also patients who need rehabilitations due to a brain damage. In the case of the refractory epilepsy, patients are treated with direct electrical cortical stimulation to map higher-motor cortices for an estimation of the deficit level in function of skilled motion control (praxis) after the removal of target brain regions. In the present study, we focused the possibility of the simultaneous analysis of EEGs and hand shaking movements as a natural behavior in the interactive game known as Rock-Paper-Scissors, which requires a smooth skilled synchronous movement with counterpart each other for making the match in the final moment to judge which one is winner, and hypothesized the existence of a synchronous component in EEGs between counterparts with respect to frequency ranges around Mu rhythm, 7.5-12.5 (primarily 9-11). We applied the 3D motion capture system and EEG scalp recording system with noise removal methods into the experimental analysis. Â© 2018 IEEE.",,,
10.1109/DICTA.2018.8615801,2019,"Sun W., You S., Walker J., Li K., Barnes N.",Structural Edge Detection: A Dataset and Benchmark,"Edge detection is a fundamental problem in computer vision community. In this paper, we propose a novel concept for edge detection called Structural Edge. The Structural edges include occluding contours of objects as well as orientation discontinuities in surfaces that define the 3D structure of objects and their environments. This contrasts the semantic edge which is only the boundary between semantic areas. While existing edge detection methods focus on either semantic boundaries or low-level gradients, we focus on the structural edge. To achieve that, in this paper, we propose the structural edge dataset along with a benchmark. The structural edge dataset contains 600 images of natural indoor and outdoor scenes. The structural edges are labeled manually and validated by eye-tracking data from 10 participants with overall 20 trials. Later, we use the dataset to benchmark the existing edge detection methods. We benchmark both the learning based and non-learning based methods and draw the conclusion that existing methods cannot fully solve the structural edge detection. We encourage new research to exploit the proposed task. Â© 2018 IEEE.",,,
10.1109/SMC.2018.00412,2019,"Obo T., Adachi K.",Multi-modal Sensing System for Unilateral Spatial Neglect in Computational System Rehabilitation,"This paper presents a multi-modal sensing system for USN assessment with eye tracker, 3D image sensor and tablet PC. First, we introduce the multi-modal sensing system based on the concept of computational system rehabilitation. Next, we propose a computational approach to extract behavioral and perceptional features of USN patients from the heterogeneous data. Furthermore, we show an experimental example to discuss the effectiveness and applicability to the feature extraction. Â© 2018 IEEE.",,,
10.1109/SMC.2018.00513,2019,"Zhou H., Wei L., Cao R., Hanoun S., Bhatti A., Tai Y., Nahavandi S.",The Study of Using Eye Movements to Control the Laparoscope under a Haptically-Enabled Laparoscopic Surgery Simulation Environment,"The purpose of this study is to investigate the possibility to use eye movements to control the laparoscope during a laparoscopic surgery. Laparoscopic surgery usually needs at least two doctors, a surgeon and a laparoscope assistant. The view of the operating surgeon is provided by the laparoscope assistant. As misunderstandings or conflicts of cooperation may happen, an ideal way is that the surgeon has a full control of all the instruments including the surgical tools and laparoscope. To achieve it, an eye based interaction method is introduced in this paper that allows surgeons to control the view by themselves. With recent developments in the eye tracker platforms and associated eye tracking technologies, many non-contact eye tracking systems are available. It can record where a person is looking at any time and a sequence of eye movements. This information can be used to know where is the attention and interest of the person on a display. As such, surgeon's attention can be captured and then be followed by moving the laparoscope to the region of interest. To have a safe and efficient evaluation on the usability, a virtual reality based laparoscopic surgery simulation is built. It is based on Unity with two haptic devices simulating the surgical tools, a 3D mouse providing 6 degrees-of-freedom control of the camera and an eye tracker capturing eyes' positions on a display. Experiments on moving a camera left, right, up, down, in, out and to specified locations using eyes are conducted, and moreover the performances of the proposed eye based self-control and the 3D mouse based other-control are compared. The results are promising where the proposed pointing method leads to 43.6% faster completion of the tasks against the traditional other-control method using the 3D mouse. Â© 2018 IEEE.",,,
10.1109/ICMLA.2018.00085,2019,"Yin Y., Juan C., Chakraborty J., McGuire M.P.",Classification of Eye Tracking Data Using a Convolutional Neural Network,"Historically, eye tracking analysis has been a useful approach to identify areas of interest (AOIs) where users have specific regions of the user interface (UI) in which they are interested. Many algorithms have been proposed to analyze eye tracking data in order to make user interfaces more effective. The objective of this study is to use convolutional neural networks (CNNs) to classify eye tracking data. First, a CNN was used to classify two different web interfaces for browsing news data. Then in a second experiment, a CNN was used to classify the nationalities of users. In addition, techniques of data-preprocessing and feature-engineering were applied. The algorithm used in this research is convolutional neural network (CNN), which is famous in deep learning field. Keras framework running on top of TensorFlow was used to define and train our CNN model. The purpose of this research is to explore how feature-engineering can affect evaluation metrics about our model. The results of the study show a number of interesting patterns and generally that deep learning shows promise in the analysis of eye tracking data. Â© 2018 IEEE.",,,
10.1109/ICMLA.2018.00063,2019,"Barbosa Monforte P.H., Matos Araujo G., Azevedo De Lima A.",Evaluation of a New Kernel-Based Classifier in Eye Pupil Detection,"Accurate pupil location is paramount to applications such as gaze estimation, assistive technologies and several man-machine interfaces as the ones found in smartphones and VR applications. We introduce a new classifier stemmed from the Inner Product Detector and investigate its features on the challenging task of pupil localization. IPD (Inner Product Detector) is a classifier with high potential in facial landmarks detection. It is robust to variations in the desired pattern while maintaining good generalization and computational efficiency. However, one possible limitation is its linear behavior, which could be overcome by aggregating non-linear techniques, such as kernel methods. Although kernel classifiers have been exhaustively studied in the past two decades, it was not analyzed or applied with IPD, yet. The proposed KIPD achieves in the worst case an accuracy of 97.41% on the BioID dataset and 93.71% in LFPW dataset both at 10% of the interocular distance. In this paper the KIPD is compared to the state of the art methods, including the ones using deep learning, being competitive in terms of accuracy as well as computational complexity. Â© 2018 IEEE.",,,
10.1109/CBS.2018.8612271,2019,"Ni H., Wang J., Wang L., Yan N.",Track Your Emotional Perception of 3-D Virtual Talking Head in Human-computer Interaction,"To investigate how emotions are identified from the avatar character in the natural scene during human-computer interaction. In current paper, a novel 3-D virtual talking head system with dynamic emotional facial expression for applying to human-robot communication was developed. Furthermore, eye tracking experiment and subjective evaluation experiment were utilized to explore the emotional perception of the 3-D virtual talking head. The results showed that there was no significant difference of observation mode between audio-visual animation of 3-D virtual talking head videos (AV3D) and audio-visual human face videos (AVHF). Besides, the recognition accuracy of HF was higher than 3D and almost all the accuracy of emotions had been improved when adding audio to videos. Finally, the results demonstrated that happiness was identified the best whether watching 3-D virtual talking head videos (3D) or human face videos (HF). These results implied that the 3-D talking head has potentially been as a suitable natural communication form in human-computer interaction. Â© 2018 IEEE.",,,
10.2352/ISSN.2470-1173.2019.2.ERVR-175,2019,"Tong H., Wan Q., Kaszowska A., Panetta K., Taylor H.A., Agaian S.",ARFurniture: Augmented reality interior decoration style colorization,"Augmented Reality (AR) can seamlessly create an illusion of virtual elements blended into the real world scene, which is one of the most fascinating human-machine interaction technologies. AR has been utilized in a variety of real-life applications including immersive collaborative gaming, fashion appreciation, interior design, and assistive devices for individuals with vision impairments. This paper contributes a real-time AR application, ARFurniture, which will allow the users to envision furniture-of-interests in different colors and different styles, all from their smart devices. The core software architecture consists of deep-learning based semantic segmentation and fast-speed color transformation. Our software architecture allows the user prompt the system to colorize the style of the furniture-of-interest within the scene on their mobile devices, and has been successfully deployed on mobile devices. In addition, using eye gaze as a pointing indicator, a head-mounted user-centric augmented reality based indoor decoration style colorization concept is discussed. Related algorithms, system design, and simulation results for ARFurniture are presented. Furthermore, a no-reference image quality measure, Naturalness Image Quality Evaluator (NIQE), was utilized to evaluate the immersiveness and naturalness of ARFuniture. The results demonstrate that ARFurniture has game-changing value to enhance user experience in indoor decoration. Â© 2019, Society for Imaging Science and Technology.",,,
10.1145/3309772.3309777,2019,"Cazzato D., Castro S.M., Agamennoni O., FernÃ¡ndez G., Voos H.",A non-invasive tool for attention-deficit disorder analysis based on gaze tracks.,"Attention deficit hyperactivity disorder (ADHD) is a neurodevelop-mental disability characterized by difficulties in keeping concentration, excessive activity and difficulties controlling behaviour not appropriate to the personâ€™s age. It is estimated to affect between 4-9% of youths and 2-5% of adults. Assistive technologies can help people with ADHD to reach goals, stay organized and even fight the urge to succumb to forms of distraction. This work introduces a tool designed for people with ADHD aimed at detecting and training their ability to follow a target in a screen. The tool is based on noninvasive monocular gaze estimation technique without constraints in terms of user dependent calibration or appearance. The system has been employed and validated in a human-computer interaction (HCI) scenario with the aim of evaluating the user visual exploration. Results show that the tool can be used in complex tasks like monitoring a user progress comparing performance after different sessions. Â© 2019 Association for Computing Machinery.",,,
10.1109/ITCGI.2018.8602697,2019,"MacHado M., Aresta G., LeitÃ£o P., Carvalho A.S., Rodrigues M., Ramos I., Cunha A., Campilho A.",Radiologists' Gaze Characterization during Lung Nodule Search in Thoracic CT,"Lung cancer diagnosis is made by radiologists through nodule search in chest Computed Tomography (CT) scans. This task is known to be difficult and prone to errors that can lead to late diagnosis. Although Computer-Aided Diagnostic (CAD) systems are promising tools to be used in clinical practice, experienced radiologists continue to perform better diagnosis than CADs. This paper proposes a methodology for characterizing the radiologist's gaze during nodules search in chest CT scans. The main goals are to identify regions that attract the radiologists' attention, which can then be used for improving a lung CAD system, and to create a tool to assist radiologists during the search task. For that purpose, the methodology processes the radiologists' gaze and their mouse coordinates during the nodule search. The resulting data is then processed to obtain a 3D gaze path from which relevant attention studies can be derived. To better convey the found information, a reference model of the lung that eases the communication of the location of relevant anatomical/pathological findings is also proposed. The methodology is tested on a set of 24 real-practice gazes, recorded via an Eye tracker, from 3 radiologists. Â© 2018 IEEE.",,,
10.1007/978-3-030-36808-1_73,2019,"Gan L., Liu W., Luo Y., Wu X., Lu B.-L.",A cross-culture study on multimodal emotion recognition using deep learning,"In this paper, we aim to investigate the similarities and differences of multimodal signals between Chinese and French on three emotions recognition task using deep learning. We use videos including positive, neutral and negative emotions as stimuli material. Both Chinese and French subjects wear electrode caps and eye tracking glass while doing experiments to collect electroencephalography (EEG) and eye movement data. To deal with the problem of lacking data for training deep neural networks, conditional Wasserstein generative adversarial network is adopted to generate EEG and eye movement data. The EEG and eye movement features are fused by using Deep Canonical Correlation Analysis to analyze the relationship between EEG and eye movement data. Our experimental results show that French has higher classification accuracy on beta frequency band while Chinese performs better on gamma frequency band. In addition, EEG signals and eye movement data of French participants have complementary characteristics in discriminating positive and negative emotions. Â© Springer Nature Switzerland AG 2019.",,,
10.4108/eai.29-6-2019.2282839,2019,"Luo M., Liu X., Wang W., Huang W.",Improved capsule network for gaze estimation in wireless sensor networks,"In this study, aiming at the problem of gaze estimation in the wireless sensor network in the car, we use image-based method to estimate gaze based on the single camera sensor. We use the deep learning model and propose the improved model from three aspects based on the original capsule network. The first is to increase the convolution layer, the second is to increase the capsule layer, and the third is to widen the capsule layer in the network. Through many contrast experiments, it is proved that the appropriate use of the first or second improved method can achieve performance over other comparison models, and the prediction results of gaze estimation are almost no different from the real gaze direction. Â© 2019 EAI.",,,
,2019,"Huang K., Bryant T., Schneider B.","Identifying collaborative learning states using unsupervised machine learning on eye-tracking, physiological and motion sensor data","With the advent of new data collection techniques, there has been a growing interest in studying co-located groups of students using Multimodal Learning Analytics [3] to automatically identify collaborative learning states. In this paper, we analyze a multimodal dataset (N=84) made of eye-tracking, physiological and motion sensing data. We leverage unsupervised machine learning algorithms to find (un)productive collaborative states. We found a three-states solution where different states (and transitions between them) were significantly correlated with task performance, collaboration quality and learning gains. We interpret these findings in light of collaborative learning theories and discuss their implications for studying groups of students using MMLA. Â© EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining. All rights reserved.",,,
,2019,"Angert T., Schneider B.",Augmenting transcripts with natural language processing and multimodal data,"In this paper we explore preliminary applications of augmenting transcripts with multimodal data. In a previous study, pairs of participants (dyads) learned how to program a robot to navigate a maze using a block-based programming language. As the dyads completed the task, their transcripts and various multimodal data were captured, creating a synced dataset of speech, 3D motion capture points, electrodermal activity, heart rate, and eye tracking. In this paper, we describe a simple visualization method to more easily analyze conversation during collaboration: a ""Convergence chart"" which visualizes changes in biometrics between speakers throughout a conversation. These visualizations allow researchers to see high level trends in transcripts by using a combination of Natural Language Processing (NLP) and physiological data to generate new insights on how collaboration changes over time. We conclude with how to use these visualizations to create new metrics to understand group dynamics. Â© EDM 2019 - Proceedings of the 12th International Conference on Educational Data Mining. All rights reserved.",,,
10.1109/ACCESS.2019.2929195,2019,"Pichitwong W., Chamnongthai K.",An eye-tracker-based 3D point-of-gaze estimation method using head movement,"Eye trackers are currently used to sense the positions of both the centers of the pupils and the point-of-gaze (POG) position on a screen, in keeping with the original objective for which they were designed; however, it remains difficult to measure the positions of three-dimensional (3D) POGs. This paper proposes a method for 3D gaze estimation by using head movement, pupil position data, and POGs on a screen. The method assumes that a person, usually unintentionally, moves his or her head a short distance such that multiple straight lines can be drawn from the center point between the two pupils to the POG. When the person is continuously focusing on a given 3D POG while moving, these lines represent the lines of sight that intersect at a 3D POG . That 3D POG can, therefore, be found from the intersection of several lines of sight formed by head movements. To evaluate the performance of the proposed method, experimental equipment was constructed, and experiments with five male and five female participants were performed in which the participants looked at nine test points in a 3D space for approximately 20 s each. The experimental results reveal that the proposed method can measure 3D POGs with average distance errors of 13.36 cm, 7.58 cm, 5.72 cm, 3.97 cm, and 3.52 cm for head movement distances of 1 cm, 2 cm, 3 cm, 4 cm, and 5 cm, respectively. Â© 2019 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",,,
,2019,"Zhang Z., Bambach S., Yu C., Crandall D.J.",From coarse attention to fine-grained gaze: A two-stage 3D fully convolutional network for predicting eye gaze in first person video,"While predicting where people will look when viewing static scenes has been well-studied, a more challenging problem is to predict gaze within the first-person, ego-centric field of view as people go about daily life. This problem is difficult because where a person looks depends not just on their visual surroundings, but also on the task they have in mind, their own internal state, their past gaze patterns and actions, and non-visual cues (e.g., sounds) that might attract their attention. Using data from head-mounted cameras and eye trackers that record people's egocentric fields of view and gaze, we propose and learn a two-stage 3D fully convolutional network to predict gaze in each egocentric frame. The model estimates a coarse attention region in the first stage, combining it with spatial and temporal features to predict a more precise gaze point in the second stage. We evaluate on a public dataset in which adults carry out specific tasks as well as on a new challenging dataset in which parents and toddlers freely interact with toys and each other, and demonstrate that our model outperforms state-of-the-art baselines. Â© 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.",,,
,2019,"Palmero C., Selva J., Bagheri M.A., Escalera S.",Recurrent CNN for 3D gaze estimation using appearance and shape cues,"Gaze behavior is an important non-verbal cue in social signal processing and human-computer interaction. In this paper, we tackle the problem of person- and head pose-independent 3D gaze estimation from remote cameras, using a multi-modal recurrent convolutional neural network (CNN). We propose to combine face, eyes region, and face landmarks as individual streams in a CNN to estimate gaze in still images. Then, we exploit the dynamic nature of gaze by feeding the learned features of all the frames in a sequence to a many-to-one recurrent module that predicts the 3D gaze vector of the last frame. Our multi-modal static solution is evaluated on a wide range of head poses and gaze directions, achieving a significant improvement of 14.6% over the state of the art on EYEDIAP dataset, further improved by 4% when the temporal modality is included. Â© 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.",,,
,2019,"Lian D., Zhang Z., Luo W., Hu L., Wu M., Li Z., Yu J., Gao S.",RGBD based gaze estimation via multi-task CNN,"This paper tackles RGBD based gaze estimation with Convolutional Neural Networks (CNNs). Specifically, we propose to decompose gaze point estimation into eyeball pose, head pose, and 3D eye position estimation. Compared with RGB image-based gaze tracking, having depth modality helps to facilitate head pose estimation and 3D eye position estimation. The captured depth image, however, usually contains noise and black holes which noticeably hamper gaze tracking. Thus we propose a CNN-based multi-task learning framework to simultaneously refine depth images and predict gaze points. We utilize a generator network for depth image generation with a Generative Neural Network (GAN), where the generator network is partially shared by both the gaze tracking network and GAN-based depth synthesizing. By optimizing the whole network simultaneously, depth image synthesis improves gaze point estimation and vice versa. Since the only existing RGBD dataset (EYEDIAP) is too small, we build a large-scale RGBD gaze tracking dataset for performance evaluation. As far as we know, it is the largest RGBD gaze dataset in terms of the number of participants. Comprehensive experiments demonstrate that our method outperforms existing methods by a large margin on both our dataset and the EYEDIAP dataset. Â© 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org).",,,
,2019,"Wang Z., Li P., Zhang L., Shao L.",Community-aware Photo Quality Evaluation by Deeply Encoding Human Perception,"Computational photo quality evaluation is a useful technique in many tasks of computer vision and graphics, <formula><tex>$e.g.$</tex></formula>, photo retaregeting, 3D rendering, and fashion recommendation. Conventional photo quality models are designed by characterizing pictures from all communities (<formula><tex>$e.g.$</tex></formula>, &#x201C;architecture&#x201D; and &#x201C;colorful&#x201D;) indiscriminately, wherein community-specific features are not encoded explicitly. In this work, we develop a new community-aware photo quality evaluation framework. It uncovers the latent community-specific topics by a regularized latent topic model (LTM), and captures human visual quality perception by exploring multiple attributes. More specifically, given massive-scale online photos from multiple communities, a novel ranking algorithm is proposed to measure the visual/semantic attractiveness of regions inside each photo. Meanwhile, three attributes: photo quality scores, weak semantic tags, and inter-region correlations, are seamlessly and collaboratively incorporated during ranking. Subsequently, we construct gaze shifting path (GSP) for each photo by sequentially linking the top-ranking regions from each photo, and an aggregation-based deep CNN calculates the deep representation for each GSP. Based on this, an LTM is proposed to model the GSP distribution from multiple communities in the latent space. To mitigate the overfitting problem caused by communities with very few photos, a regularizer is added into our LTM. Finally, given a test photo, we obtain its deep GSP representation and its quality score is determined by the posterior probability of the regularized LTM. Comprehensive comparative studies on four image sets have shown the competitiveness of our method. Besides, eye tracking experiments demonstrated that our ranking-based GSPs are highly consistent with real human gaze movements. IEEE",,,
,2019,"Baharom N.H., Aid S.R., Amin M.K.M., Wibirama S., Mikami O.",Exploring the eye tracking data of human behaviour on consumer merchandise product,"This article presents an exploration of the human eye tracking data towards consumer products. The study aim to investigate the data attributes of the cognitive processes and focused on the visual attention of the participants when choosing a shampoo brand which is commonly available in Malaysia. However, eye tracking datasets has a wealth of data on the eyes visual attention, fixation, saccade and scan path gaze. Therefore, this paper aims to solve this problem to minimize the datasets by using clustering machine learning approach. This is to observe the relation of these data attributes and possibly predict the possible solution contributing to cognitive processing. Tobii TX300 Eye-tracker was used in this experiment and the eyes tracking data were gathered particularly related to the eyes fixation and saccades by using the Tobii I-VT filter. Sixty subjects participated in this study. K-means clustering was used as statistical analysis to cluster the huge datasets from the eye tracking data. The relationship of the consumer cognitive processes with visual attention was understood when most of the participants chose the most popular shampoo brand such as Head & Shoulder. Further visual analysis on the data attributes results showed that K-means clustering has the potential to cluster and minimize the huge datasets and predicts consumer preferences. Â© 2018 Penerbit Universiti Teknikal Malaysia Melaka.",,,
10.5565/rev/elcvia.1193,2019,Berga D.,Understanding eye movements: Psychophysics and a model of primary visual cortex,"Humans move their eyes in order to learn visual representations of the world. These eye movements depend on distinct factors, either by the scene that we perceive or by our own decisions. To select what is relevant to attend is part of our survival mechanisms and the way we build reality, as we constantly react both consciously and unconsciously to all the stimuli that is projected into our eyes. In this thesis [1] we try to explain (1) how we move our eyes, (2) how to build machines that understand visual information and deploy eye movements, and (3) how to make these machines understand tasks in order to decide for eye movements. (1)We provided the analysis of eye movement behavior elicited by low-level feature distinctiveness with a dataset of 230 synthetically-generated image patterns [2]. A total of 15 types of stimuli has been generated (e.g. orientation, brightness, color, size, etc.), with 7 feature contrasts for each feature category. Eyetracking data was collected from 34 participants during the viewing of the dataset, using Free-Viewing and Visual Search task instructions. Results showed that saliency is predominantly and distinctively influenced by: 1. feature type, 2. feature contrast, 3. temporality of fixations, 4. task difficulty and 5. center bias. From such dataset (SID4VAM), we have computed a benchmark of saliency models by testing performance using psychophysical patterns [3]. Model performance has been evaluated considering model inspiration and consistency with human psychophysics. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation. (2) Computations in the primary visual cortex (area V1 or striate cortex) have long been hypothesized to be responsible, among several visual processing mechanisms, of bottom-up visual attention (also named saliency). In order to validate this hypothesis, images from eye tracking datasets have been processed with a biologically plausible model of V1 (named Neurodynamic Saliency Wavelet Model or NSWAM)[4]. Following Li's neurodynamic model, we define V1's lateral connections with a network of firing rate neurons, sensitive to visual features such as brightness, color, orientation and scale. Early subcortical processes (i.e. retinal and thalamic) are functionally simulated. The resulting saliency maps are generated from the model output, representing the neuronal activity of V1 projections towards brain areas involved in eye movement control. We want to pinpoint that our unified computational architecture is able to reproduce several visual processes (i.e. brightness, chromatic induction and visual discomfort) without applying any type of training or optimization and keeping the same parametrization. The model has been extended (NSWAMCM)[ 5] with an implementation of the cortical magnification function to define the retinotopical projections towards V1, processing neuronal activity for each distinct view during scene observation. Novel computational definitions of top-down inhibition (in terms of inhibition of return and selection mechanisms), are also proposed to predict attention in Free-Viewing and Visual Search conditions. Results show that our model outpeforms other biologically-inpired models of saliency prediction as well as to predict visual saccade sequences, specifically for nature and synthetic images. We also show how temporal and spatial characteristics of inhibition of return can improve prediction of saccades, as well as how distinct search strategies (in terms of feature-selective or category-specific inhibition) predict attention at distinct image contexts. (3) Although previous scanpath models have been able to efficiently predict saccades during Free- Viewing, it is well known that stimulus and task instructions can strongly affect eye movement patterns. In particular, task priming has been shown to be crucial to the deployment of eye movements, involving interactions between brain areas related to goal-directed behavior, working and long-term memory in combination with stimulus-driven eye movement neuronal correlates. In our latest study [6] we proposed an extension of the Selective Tuning Attentive Reference Fixation Controller Model based on task demands (STAR-FCT), describing novel computational definitions of Long-Term Memory, Visual Task Executive and Task Working Memory. With these modules we are able to use textual instructions in order to guide the model to attend to specific categories of objects and/or places in the scene. We have designed our memory model by processing a visual hierarchy of low- and high-level features. The relationship between the executive task instructions and the memory representations has been specified using a tree of semantic similarities between the learned features and the object category labels. Results reveal that by using this model, the resulting object localization maps and predicted saccades have a higher probability to fall inside the salient regions depending on the distinct task instructions compared to saliency. Â© 2019 Universitat Autonoma de Barcelona.",,,
10.1109/ACCESS.2019.2949150,2019,"Li W., Dong Q., Jia H., Zhao S., Wang Y., Xie L., Pan Q., Duan F., Liu T.",Training a camera to perform long-distance eye tracking by another eye-tracker,"Appearance-based gaze estimation techniques have been greatly advanced in these years. However, using a single camera for appearance-based gaze estimation has been limited to short distance in previous studies. In addition, labeling of training samples has been a time-consuming and unfriendly step in previous appearance-based gaze estimation studies. To bridge these significant gaps, this paper presents a new long-distance gaze estimation paradigm: Train a camera to perform eye tracking by another eye tracker, named Learning-based Single Camera eye tracker (LSC eye-tracker). In the training stage, the LSC eye-tracker simultaneously acquired gaze data by a commercial trainer eye tracker and face appearance images by a long-distance trainee camera, based on which deep convolutional neural network (CNN) models are utilized to learn the mapping from appearance images to gazes. In the application stage, the LSC eye-tracker works alone to predict gazes based on the acquired appearance images by the single camera and the trained CNN models. Our experimental results show that the LSC eye-tracker enables both population-based eye tracking and personalized eye tracking with promising accuracy and performance. Â© 2013 IEEE.",,,
10.1117/12.2537753,2019,"Zhdanov A., Zhdanov D., Potemin I., Bogdanov N., Bykovskii S.",Possibility of vergence disagreement reducing on the base of approximate restoration of the depth map,"The article describes the approach that allows to reconstruct the image formed by the video see-through mixed reality system corresponding to the convergence of the device user eyes. Convergence is defined by the user eye pupils position acquired from the mixed reality device eye tracking system. The image reconstruction method is based on the use of an extended (2.5-dimensional) representation of the image obtained, for example, using a 3D scanner that builds a depth map of the scene. In the proposed solution, lens optical systems that form images of the real world on LCD screens and eyepieces that project these images into the user eyes do not change their characteristics and position. The image is reconstructed by projecting the points of the original image to the image points corresponding to the required convergence by the method of ""refocusing"" at a distance for each point. The advantages and disadvantages of this method are shown. An approach is proposed that reduces visual perception discomfort caused by an ambiguous distance to the image point, for example, in the case of mirror or transparent objects. Virtual prototyping of the mixed reality system showed the benefits of the proposed approach to reduce the visual perception discomfort caused by the mismatch between the convergence of human eyes and the images formed by the lenses of the mixed reality system. Â© COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.",,,
10.1109/ACCESS.2019.2953796,2019,"Xu B., Li X., Wang Y.",Wide color gamut autostereoscopic 2d-3d switchable display based on dynamic subpixel arrangement,"This paper is a report on the mathematical analysis and working principle of a wide color gamut autostereoscopic 2D-3D switchable display based on the dynamic subpixel arrangement method. The display prototype discussed in the paper has three major distinctions. First, the use of dynamic subpixel arrangement method and eye-tracking system has improved its performances in the resolution, crosstalk and 2D/3D switching statistics. Second, a design of Quantum-Dot-Polymer (QDP) film and optical layer combined backlight has enhanced its viewing angle and color gamut. Third, the application of parallel computing to the dynamic subpixel arrangement method has improved its real-time performance. Base on observation from finished fabrication and experiment, this prototype has already demonstrated noticeable enhancement in terms of color gamut expansion-reaching 77.98% according to ITU-R Recommendation BT.2020 (Rec.2020), and crosstalk reduction-with the minimum crosstalk rate at nearly 6%. Close comparison with two other commercial 3D displays (BENQ XL2707-B and View Sonic VX2268WM) are also presented in the paper for sufficiency. Â© 2013 IEEE.",,,
10.1109/ACCESS.2019.2946479,2019,"Sun Z., Wang X., Zhang Q., Jiang J.",Real-Time Video Saliency Prediction Via 3D Residual Convolutional Neural Network,"Attention is a fundamental attribute of human visual system that plays important roles in many visual perception tasks. The key issue of video saliency lies in how to efficiently exploit the temporal information. Instead of singling out the temporal saliency maps, we propose a real-time end-to-end video saliency prediction model via 3D residual convolutional neural network (3D-ResNet), which incorporates the prediction of spatial and temporal saliency maps into one single process. In particular, a multi-scale feature representation scheme is employed to further boost the model performance. Besides, a frame skipping strategy is proposed for speeding up the saliency map inference process. Moreover, a new challenging eye tracking database with 220 video clips is established to facilitate the research of video saliency prediction. Extensive experimental results show our model outperforms the state-of-the-art methods over the eye fixation datasets in terms of both prediction accuracy and inference speed. Â© 2013 IEEE.",,,
10.2352/J.ImagingSci.Technol.2019.63.6.060403,2019,"Tanaka M., Lanaro M.P., Horiuchi T., Rizzi A.",Random spray retinex extensions considering region of interest and eyemovements,"The Random spray Retinex (RSR) algorithm was developed by taking into consideration the mathematical description of Milano-Retinex. The RSR substituted random paths with random sprays. Mimicking some characteristics of the human visual system (HVS), this article proposes two variants of RSR adding a mechanism of region of interest (ROI). In the first proposed model, a cone distribution based on anatomical data is considered as ROI. In the second model, the visual resolution depending on the visual field based on the knowledge of visual information processing is considered as ROI. We have measured actual eye movements using an eye-tracking system. By using the eye-tracking data, we have simulated the HVS using test images. Results show an interesting qualitative computation of the appearance of the processed area around real gaze points. Â© 2019 Society for Imaging Science and Technology.",,,
10.3745/JIPS.01.0044,2019,"Song H., Lee K., Moon N.",User modeling using user preference and user life pattern based on personal bio data and sns data,"The purpose of this study was to collect and analyze personal bio data and social network services (SNS) data, derive user preference and user life pattern, and propose intuitive and precise user modeling. This study not only tried to conduct eye tracking experiments using various smart devices to be the ground of the recommendation system considering the attribute of smart devices, but also derived classification preference by analyzing eye tracking data of collected bio data and SNS data. In addition, this study intended to combine and analyze preference of the common classification of the two types of data, derive final preference by each smart device, and based on user life pattern extracted from final preference and collected bio data (amount of activity, sleep), draw the similarity between users using Pearson correlation coefficient. Through derivation of preference considering the attribute of smart devices, it could be found that users would be influenced by smart devices. With user modeling using user behavior pattern, eye tracking, and user preference, this study tried to contribute to the research on the recommendation system that should precisely reflect user tendency. Â© 2019 KIPS.",,,
10.1109/ACCESS.2019.2956953,2019,"Chen H.-H., Hwang B.-J., Hsu W.-H., Kao C.-W., Chen W.-T.",Focus on Area Tracking Based on Deep Learning for Multiple Users,"Most eye-tracking experiments are limited to single subjects because gaze points are difficult to track when multiple users are involved and environmental factors might cause interference. To overcome this problem, this paper proposes a method for gaze tracking that can be applied for multiple users simultaneously. Four models, including FASEM, FAEM and FAFRCM in the signal-user environment, as well as FAEM and FAMAM in the multiple-user environment, are proposed, and we collected raw data of gazing behaviors to train the models. Through a modified VGG19 architecture and adjusting the Number of Convolutional Layers (NoCL), we obtained and compared the accuracy of various models to determine the most suitable architecture. Since data for multiple-users is not easy to obtain, in this paper, we trained the model first with single users, then extended it to multiple users with transfer learning. Finally, we propose an adaptive method to integrate the benefits of FAEM and FAMAM. Â© 2013 IEEE.",,,
10.1007/978-3-030-35343-8_11,2019,"Wang C.-C., Hung J.C., Wang S.-C., Huang Y.-M.",Visual Attention Analysis During Program Debugging Using Virtual Reality Eye Tracker,"The immersion of virtual reality (VR) has transcended the existing experience of multimedia teaching. This paper aims to design a virtual reality eye tracker device to analyze the cognitive process of program debugging by adopting virtual reality technology to build a 3D code rendering system and, at the same time, using eye tracking technology to study visual attention as well as to analyze and compare the differences in internal behavioral cognition in terms of program debugging. This paper has 32 students as participants who have studied C++ programming language courses for more than one year in the department of computer science. With Unity 3D development tool, the experiment creates a virtual classroom scene and C++ programming language code. The participantsâ€?eye movements are recorded by an eye tracker device integrated in a Head-Mounted Display (HMD). The eye movement defines the regions of interest (ROIs) according to the division of the programâ€™s function, and the difference in visual attention between various ROIs in the code is discussed when the participant performs the program debugging task. The finding results are expected to improve the dilemma of the existing programming teaching, so that the instructors can provide appropriate teaching aids for students to achieve the purpose of programming teaching and improving the studentsâ€?programming competence. Â© Springer Nature Switzerland AG 2019.",,,
10.1007/978-3-030-33723-0_5,2019,"Ebrahimpour M.K., Falandays J.B., Spevack S., Noelle D.C.",Do Humans Look Where Deep Convolutional Neural Networks â€œAttendâ€?,"Deep Convolutional Neural Networks (CNNs) have recently begun to exhibit human level performance on some visual perception tasks. Performance remains relatively poor, however, on some vision tasks, such as object detection: specifying the location and object class for all objects in a still image. We hypothesized that this gap in performance may be largely due to the fact that humans exhibit selective attention, while most object detection CNNs have no corresponding mechanism. In examining this question, we investigated some well-known attention mechanisms in the deep learning literature, identifying their weaknesses and leading us to propose a novel attention algorithm called the Densely Connected Attention Model. We then measured human spatial attention, in the form of eye tracking data, during the performance of an analogous object detection task. By comparing the learned representations produced by various CNN architectures with that exhibited by human viewers, we identified some relative strengths and weaknesses of the examined computational attention mechanisms. Some CNNs produced attentional patterns somewhat similar to those of humans. Others focused processing on objects in the foreground. Still other CNN attentional mechanisms produced usefully interpretable internal representations. The resulting comparisons provide insights into the relationship between CNN attention algorithms and the human visual system. Â© 2019, Springer Nature Switzerland AG.",,,
10.1109/JSTSP.2019.2956408,2019,"Chen M., Jin Y., Goodall T., Yu X., Bovik A.C.",Study of 3D Virtual Reality Picture Quality,"Virtual Reality (VR) and its applications have attracted significant and increasing attention. However, the requirements of much larger file sizes, different storage formats, and immersive viewing conditions pose significant challenges to the goals of acquiring, transmitting, compressing and displaying high quality VR content. Towards meeting these challenges, it is important to be able to understand the distortions that arise and that can affect the perceived quality of displayed VR content. It is also important to develop ways to automatically predict VR picture quality. Meeting these challenges requires basic tools in the form of large, representative subjective VR quality databases on which VR quality models can be developed and which can be used to benchmark VR quality prediction algorithms. Towards making progress in this direction, here we present the results of an immersive 3D subjective image quality assessment study. In the study, 450 distorted images obtained from 15 pristine 3D VR images modified by 6 types of distortion of varying severities were evaluated by 42 subjects in a controlled VR setting. Both the subject ratings as well as eye tracking data were recorded and made available as part of the new database, in hopes that the relationships between gaze direction and perceived quality might be better understood. We also evaluated several public available IQA models on the new database, and also report a statistical evaluation of the performances of the compared IQA models. IEEE",,,
10.1007/978-3-030-32251-9_43,2019,"Patra A., Cai Y., Chatelain P., Sharma H., Drukker L., Papageorghiou A.T., Noble J.A.",Efficient ultrasound image analysis models with sonographer gaze assisted distillation,"Recent automated medical image analysis methods have attained state-of-the-art performance but have relied on memory and compute-intensive deep learning models. Reducing model size without significant loss in performance metrics is crucial for time and memory-efficient automated image-based decision-making. Traditional deep learning based image analysis only uses expert knowledge in the form of manual annotations. Recently, there has been interest in introducing other forms of expert knowledge into deep learning architecture design. This is the approach considered in the paper where we propose to combine ultrasound video with point-of-gaze tracked for expert sonographers as they scan to train memory-efficient ultrasound image analysis models. Specifically we develop teacher-student knowledge transfer models for the exemplar task of frame classification for the fetal abdomen, head, and femur. The best performing memory-efficient models attain performance within 5% of conventional models that are 1000Ã— larger in size. Â© Springer Nature Switzerland AG 2019.",,,
10.1007/978-3-030-32254-0_19,2019,"Kogkas A., Ezzat A., Thakkar R., Darzi A., Mylonas G.","Free-View, 3D Gaze-Guided Robotic Scrub Nurse","We introduce a novel 3D gaze-guided robotic scrub nurse (RN) and test the platform in simulated surgery to determine usability and acceptability with clinical teams. Surgeons and trained scrub nurses performed an ex vivo task on pig colon. Surgeons used gaze via wearable eye-tracking glasses to select surgical instruments on a screen, in turn initiating RN to deliver the instrument. Comparison was done between human- and robot-assisted tasks (HT vs RT). Real-time gaze-screen interaction was based on a framework developed with synergy of conventional wearable eye-tracking, motion capture system and RGB-D cameras. NASA-TLX and Van der Laanâ€™s technology acceptance questionnaires were collected and analyzed. 10 teams of surgical trainees (ST) and scrub nurses (HN) participated. Overall, NASA-TLX feedback was positive. ST and HN revealed no statistically significant difference in overall task load. Task performance feedback was unaffected. Frustration was reported by ST. Overall, Van der Laanâ€™s scores showed positive usefulness and satisfaction scores following RN use. There was no significant difference in task interruptions across HT vs RT. Similarly, no statistical difference was found in duration to task completion in both groups. Quantitative and qualitative feedback was positive. The source of frustration has been understood. Importantly, there was no significant difference in task workflow or operative time, with overall perceptions towards task performance remaining unchanged in HT vs RT. Â© 2019, Springer Nature Switzerland AG.",,,
,2019,"Lu C., Uchiyama H., Thomas D., Shimada A., Taniguchi R.-I.",Multi-pedestrian tracking system based on asynchronized IMUs,"We propose a multi-pedestrian tracking system based on MEMS based IMUs as a novel tool for human behavior analysis. With asynchronized multiple IMUs, our system can track IMU-attached pedestrians in synchronization at a high frame rate in the large environment, compared with vision based approaches. The output data is similar to standard PDR systems as follows: the time-series position, velocity, and heading of the pedestrians in the 3D space. To realize our system, we propose a simple but effective calibration technique for synchronizing the timelines of the asynchronized IMUs. With our system, users can analyze the detailed motion behaviors of the people who participate in a group work or a collective activity, quantitatively. By combining with other sensors such as an eye tracker, our system can further provide more comprehensive data in the experiments. Â© 2019 CEUR Workshop Proceedings. All rights reserved.",,,
,2019,"Lyudvichenko V.A., Vatolin D.S.",Predicting video saliency using crowdsourced mouse-tracking data,"This paper presents a new way of getting high-quality saliency maps for video, using a cheaper alternative to eye-tracking data. We designed a mouse-contingent video viewing system which simulates the viewersâ€?peripheral vision based on the position of the mouse cursor. The system enables the use of mouse-tracking data recorded from an ordinary computer mouse as an alternative to real gaze fixations recorded by a more expensive eye-tracker. We developed a crowdsourcing system that enables the collection of such mouse-tracking data at large scale. Using the collected mouse-tracking data we showed that it can serve as an approximation of eye-tracking data. Moreover, trying to increase the efficiency of collected mouse-tracking data we proposed a novel deep neural network algorithm that improves the quality of mouse-tracking saliency maps. Copyright Â© 2019 for this paper by its authors.",,,
10.1007/978-981-13-9917-6_30,2019,"Shen R., Weng D., Guo J., Fang H., Jiang H.",Effects of Dynamic Disparity on Visual Fatigue Caused by Watching 2D Videos in HMDs,"As working at a video display terminal (VDT) for a long time can induce visual fatigue, this paper proposed a method to use dynamic disparity on the situation of video watching in head-mounted displays (HMD), based on the accommodative training. And an experiment was designed to evaluate whether it can alleviate visual fatigue. Subjective and objective methods were combined in the experiment under different disparity conditions to evaluate the visual fatigue of the subjects. The objective assessment was the blink frequency of the subjects, achieved by the eye tracker. The subjective assessment was questionnaire. However, we came to the conclusion that dynamic disparity caused by the movement of left and right eye images in the HMD canâ€™t effectively alleviate visual fatigue. According to the change of the average eye blink frequency ratio of the subjects during the experiment, the change of the visual fatigue over time was analyzed. Â© 2019, Springer Nature Singapore Pte Ltd.",,,
10.31799/1684-8853-2019-3-10-36,2019,"Favorskaya M.N., Jain L.C.",Saliency detection in deep learning era: Trends of development,"Introduction: Saliency detection is a fundamental task of computer vision. Its ultimate aim is to localize the objects of interest that grab human visual attention with respect to the rest of the image. A great variety of saliency models based on different approaches was developed since 1990s. In recent years, the saliency detection has become one of actively studied topic in the theory of Convolutional Neural Network (CNN). Many original decisions using CNNs were proposed for salient object detection and, even, event detection. Purpose: A detailed survey of saliency detection methods in deep learning era allows to understand the current possibilities of CNN approach for visual analysis conducted by the human eyes' tracking and digital image processing. Results: A survey reflects the recent advances in saliency detection using CNNs. Different models available in literature, such as static and dynamic 2D CNNs for salient object detection and 3D CNNs for salient event detection are discussed in the chronological order. It is worth noting that automatic salient event detection in durable videos became possible using the recently appeared 3D CNN combining with 2D CNN for salient audio detection. Also in this article, we have presented a short description of public image and video datasets with annotated salient objects or events, as well as the often used metrics for the results' evaluation. Practical relevance: This survey is considered as a contribution in the study of rapidly developed deep learning methods with respect to the saliency detection in the images and videos. Â© 2019 Saint Petersburg State University of Aerospace Instrumentation. All rights reserved.",,,
10.33965/ihci2019_201906c049,2019,"Ahn Y.-K., Park Y.-C.",Natural user interface-based car infotainment control system,"This article proposes a natural user interface (NUI) system for the control of infotainment content in a smart car that recognizes a user's emotions and gaze with a 2D camera, an eye camera. The configuration of the system used to recognize a user's emotions and gaze is introduced. The methods for recognizing a user's emotions and gaze are described. An experiment was performed to evaluate the performance of emotions and gaze recognition, and to demonstrate the user performance of the system proposed in this article. Â© Copyright 2019 IADIS Press. All rights reserved.",,,
10.33965/g2019_201906c053,2019,"Wang H., Chen W.-W., Sun C.-T.",How gaming experience influences new game learning,"A large number of new video games are specially designed to meet the needs of players with different levels of gaming experience. To provide ideal learning environments, designers must understand differences in how experienced/less experienced players learn new games. Using a sample of players with different experience levels, our goal is to understand learning processes for a new real-time strategy game. Data from observations, post-game interviews, and eye movement recordings indicate that the majority of study participants relied on a trial-and-error approach, with more experienced gamers using a structured mental model involving feedback and expectations about making progress. Specifically, experienced gamers in the sample tended to use a top-down learning style emphasizing connections between goals and available actions, and to focus on the functions of game objects. In comparison, players with little or no gaming experience were more likely to focus on appearance and textual descriptions. Our findings suggest that in-game information interface designers need to consider player experience level in order to facilitate learning. Our results raise questions about whether gaming experience affects student classroom learning styles, as well as expectations regarding teachers and curriculums. Â© Copyright 2019 IADIS Press. All rights reserved.",,,
10.1007/978-3-030-30490-4_31,2019,"Shan H., Liu Y., Stefanov T.",Ensemble of Convolutional Neural Networks for P300 Speller in Brain Computer Interface,"A Brain Computer Interface (BCI) speller allows human-beings to directly spell characters using eye-gazes, thereby building communication between the human brain and a computer. Convolutional Neural Networks (CNNs) have shown better ability than traditional machine learning methods to increase the character spelling accuracy for the BCI speller. Unfortunately, current CNNs can not learn well the features related to the target signal of the BCI speller. This issue limits these CNNs from further character spelling accuracy improvements. To address this issue, we propose a network, which combines our proposed two CNNs, with an existing CNN. These three CNNs of our network extract different features related to the target BCI signal. Our network uses the ensemble of the features extracted by these CNNs for BCI character spelling. Experimental results on three benchmark datasets show that our network outperforms other methods in most cases, with a significant spelling accuracy improvement upÂ to 38.72%. In addition, the communication speed of the P300 speller based on our network is upÂ to 2.56 times faster than the communication speed of the P300 speller based on other methods. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-29888-3_27,2019,"Fuhl W., Rosenstiel W., Kasneci E.","500,000 Images Closer to Eyelid and Pupil Segmentation","Human gaze behavior is not the only important aspect about eye tracking. The eyelids reveal additional important information; such as fatigue as well as the pupil size holds indications of the workload. The current state-of-the-art datasets focus on challenges in pupil center detection, whereas other aspects, such as the lid closure and pupil size, are neglected. Therefore, we propose a fully convolutional neural network for pupil and eyelid segmentation as well as eyelid landmark and pupil ellipsis regression. The network is jointly trained using the Log loss for segmentation and L1 loss for landmark and ellipsis regression. The application of the proposed network is the offline processing and creation of datasets. Which can be used to train resource-saving and real-time machine learning algorithms such as random forests. In addition, we will provide the worlds largest eye images dataset with more than 500,000 images DOWNLOAD. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-30033-3_16,2019,"MartÃ­nez ZÃ¡rate J., Mateus Santiago S.",Sentiment Analysis Through Machine Learning for the Support on Decision-Making in Job Interviews,"In this paper, we propose a sentiment analysis model using machine learning for the support on decision-making in the process of job interviews. To do this, a characterization of the analysis of sentiments, job interviews and machine learning algorithms is first performed. Then, supervised machine learning with artificial neural networks is implemented in a prototype, due to the non-linear behavior described in the variables taken in the study and applying the Eye tracking technique. Finally, tests are carried out with people, in which, by asking questions of these, the involuntary movements of the pupil of the eye are analyzed, through the processing of a volume of data and the results of the ocular patterns are interpreted. Correlated with the questions of the test and with it, a final judgment is presented for the support of the decision making. Â© 2019, Springer Nature Switzerland AG.",,,
,2019,"Ye X., KÃ¶nig M.",Applying eye tracking in virtual construction environments to improve cognitive data collection and human-computer interaction of site hazard identification,"In the Architecture, Engineering and Construction (AEC) field, eye tracking technology is being applied more frequently in cognitive research such as hazard identification. These studies typically use eye tracking in a diagnostic way and pay less attention to the application of virtual environment. However, in virtual environment, eye tracking not only can enhance the study of the cognitive process but also improves the human-computer interaction. Therefore, this paper elaborates on how we use eye tracking devices to track 3D objects in virtual environments diagnostically and interactively. First, we analyze the existing research gaps of using eye tracking in the construction industry. Then, we follow 3D object identification, diagnostic mode and interactive mode to develop a methodology by HTC VIVE device with Pupil Labs HTC Vive Binocular Add-on based on the research gaps. Finally, an example experiment is provided to demonstrate studying hazard identification using eye tracking in construction safety. For analyzing the eye movement data from the participants, we offer the number of confirmations, the scan path and the 3D heatmap of objects in both static and dynamic construction site scenes. This paper provides an approach of applying eye tracking to gather more data in virtual environment for the future cognitive studies and explores the possibility to improve human-computer interaction using eye tracking in the construction industry. Â© 2019 International Association for Automation and Robotics in Construction I.A.A.R.C. All rights reserved.",,,
10.1007/978-3-030-27529-7_53,2019,"Jiang J., Zhou X., Chan S., Chen S.",Appearance-based gaze tracking: A brief review,"Human gaze tracking plays an important role in the field of Human-Computer Interaction. This paper presents a brief review on appearance-based gaze tracking. Based on the appearance of human eyes, input features can be classified into three categories according to the different ways of extracting human eyes features, namely, complete human eye image, pixel-based feature and 3D reconstruction image. The estimation process from human eye feature to fixation point mainly uses different mapping functions. In this paper, common mapping functions and related algorithms are described in detail: k-nearest neighbor (KNN), random forest (RF) regression, gaussian process (GP) regression, support vector machines (SVM) and artificial neural networks (ANN).Â This paper evaluates the performance of these gaze tracking algorithms using different mapping functions. Based on the results of the evaluation, potential challenges are summarized and the future directions of gaze estimation are prospected. Â© Springer Nature Switzerland AG 2019.",,,
10.18848/2327-0144/CGP/V26I01/19-34,2019,"Feidakis M., Rangoussi M., Kasnesis P., Patrikakis C., Kogias D.G., Charitopoulos A.",Affective assessment in distance learning: A semi-explicit approach,"Modern e-learning and distance learning systems suffer severe lack of affect-aware interaction: the typical system is irresponsive to the affective state of the user, while even an inadequate human tutor would respond to it and even adapt his/her instruction accordingly. The main goal of this paper is to describe a scenario that deploys state-of-theart technologies to ""sense"" or ""gauge"" the affective state of a remote class of learners while they participate in a distance learning course, either synchronous or asynchronous, and provide feedback to all stakeholders (individual learner, peers, class tutor) through intuitive, easy-to-grasp visualisations. Both semi-automated, smart (selfreporting/ explicit) solutions through gestures and fully automated (user-transparent/implicit) solutions are sought through fusion of a number of ""experts"" (monitored features of the learner) that feed a decision-making algorithm after suitable processing. Â© Common Ground Research Networks, Michalis Feidakis, Maria Rangoussi, Panagiotis Kasnesis, Charalampos Patrikakis, Dimitrios G. Kogias, Angelos Charitopoulos.",,,
10.14569/ijacsa.2019.0100710,2019,"Alghamdi N., Alhalabi W.",Fixation detection with ray-casting in immersive virtual reality,This paper demonstrates the application of a proposed eye fixation detection algorithm to eye movement recorded during eye gaze input within immersive Virtual Reality and compares it with the standard frame-by-frame analysis for validation. Pearson correlations and a sample paired t-test indicated strong correlations between the two analysis methods in terms of fixation duration. The results showed that the principle of eye movement event detection in 2D can be applied successfully in a 3D environment and ensures efficient detection when combined with ray-casting and event time. Â© 2018 The Science and Information (SAI) Organization Limited.,,,
10.14569/ijacsa.2019.0100775,2019,"Bamidele A.A., Kamardin K., Aziz N.S.N.A., Sam S.M., Ahmed I.S., Azizan A., Bani N.A., Kaidi H.M.",Non-intrusive driver drowsiness detection based on face and eye tracking,"The rate of annual road accidents attributed to drowsy driving are significantly high. Due to this, researchers have proposed several methods aimed at detecting drivers' drowsiness. These methods include subjective, physiological, behavioral, vehicle-based, and hybrid methods. However, recent reports on road safety are still indicating drowsy driving as a major cause of road accidents. This is plausible because the current driver drowsiness detection (DDD) solutions are either intrusive or expensive, thus hindering their ubiquitous nature. This research serves to bridge this gap by providing a test-bed for achieving a non-intrusive and low-cost DDD solution. A behavioral DDD solution is proposed based on tracking the face and eye state of the driver. The aim is to make this research an inception to DDD pervasiveness. To achieve this, National Tsing Hua University (NTHU) Computer Vision Lab's driver drowsiness detection video dataset was utilized. Several video and image processing operations were performed on the videos so as to detect the drivers' eye state. From the eye states, three important drowsiness features were extracted: percentage of eyelid closure (PERCLOS), blink frequency (BF), and Maximum Closure Duration (MCD) of the eyes. These features were then fed as inputs into several machine learning models for drowsiness classification. Models from the K-nearest Neighbors (KNN), Support Vector Machine (SVM), Logistic Regression, and Artificial Neural Networks (ANN) machine learning algorithms were experimented. These models were evaluated by calculating their accuracy, sensitivity, specificity, miss rate, and false alarm rate values. Although these five metrics were evaluated, the focus was more on getting optimal accuracies and miss rates. The result shows that the best models were a KNN model when k = 31 and an ANN model that used an Adadelta optimizer with 3 hidden layer network of 3, 27, and 9 neurons respective. The KNN model obtained an accuracy of 72.25% with a miss rate of 16.67%, while the ANN model obtained 71.61% and 14.44% accuracy and miss rate respectively. Â© 2018 The Science and Information (SAI) Organization Limited.",,,
10.1007/978-3-030-21607-8_29,2019,"Xu Q., Ragan E.D.",Effects of Character Guide in Immersive Virtual Reality Stories,"Bringing cinematic experiences from traditional film screens into Virtual Reality (VR) has become increasingly popular in recent years. However, striking a balance between storytelling and user interaction can cause a big challenge for filmmakers. In this paper, we present a media review on the common strategies that constructed the existing framework of computer generated cinematic VR by evaluating over 80 real-time rendered interactive experiences across different media. We summarized the most-used methods, which creators applied to maintain a relative control when presenting a narrative experience in VR, that were associated with story-progression strategies and attention guidance techniques. We then approach the problem of guiding the audience through major events of a story in VR by using a virtual character as a travel guide providing assistance in directing viewers attention to the target. To assess the effectiveness of this technique, we performed a controlled experiment applying the method in three VR videos. The experiment compared three variations of the character guide: (1) no guide, (2) a guide with a matching art style to the video, and (3) a guide with a non-matching art style. The experiment results provided insights for future directors and designers into how to draw viewers attention to a target point within a narrative VE, such as what could be improved and what should be avoided. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-21607-8_5,2019,"Hansberger J.T., Peng C., Blakely V., Meacham S., Cao L., Diliberti N.",A Multimodal Interface for Virtual Information Environments,"Continuing advances in multimodal technology, machine learning, and virtual reality are providing the means to explore and develop multimodal interfaces that are faster, more accurate, and more meaningful in the interactions they support. This paper describes an ongoing effort to develop an interface using input from voice, hand gestures, and eye gaze to interact with information in a virtual environment. A definition for a virtual environment tailored for the presentation and manipulation of information is introduced along with a new metaphor for multimodal interactions within a virtual environment. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-23528-4_32,2019,"Kato K., Prima O.D.A., Ito H.",3D Eye Tracking for Visual Imagery Measurements,"Experiences of visual imagery, the ability to see in the mindâ€™s eye, occur in various situations. The Vividness of Visual Imagery Questionnaire (VVIQ) has been widely used to subjectively measure the vividness of visualizers based on its score. For objective measurements, studies show that functional Magnetic Resonance Imaging (fMRI) can be used to measure individual variabilities of the vividness of visual imagery. However, questions are remained on how the visualizers see the images spatially. This study proposes a method to measure the spatial distribution of gaze in 3-dimensional space of an object seen by a visualizer using a glass-typed 3D eye tracker. The eye tracker estimated gaze in 3D based on vergence eye movements. Thus, if the visualizer reports good visual imagery of a given image, the eye tracker will be able to estimate the location of the object in 3-dimensional space. The eye tracker is equipped with polarizing lenses to enable the visualizer to see the given stimuli in both virtual and real worlds. Here, a 3D television (3DTV) is used to present the stimuli virtually. Ten introductory students completed the VVIQ and divided into two groups: High and low vividness visualizers, based on total scores of the VVIQ. Experiment results show that 3D gaze fixations of subjects who reported good visual imagery were relatively distributed around the location of the given stimuli. Â© Springer Nature Switzerland AG 2019.",,,
10.1007/978-3-030-22643-5_21,2019,"Hirata Y., Soma H., Takimoto M., Kambayashi Y.",Virtual Space Pointing Based on Vergence,"Recent virtual reality (VR) headsets make users perceive the three-dimensional (3D) virtual space through their parallax. The 3D space has been used for only passive use such as representing something put into intensive reality. We propose a new manner for pointing objects at specific locations in 3D space through vergence. In order to achieve this new manner, we have paid a close attention to the directions of eyes through parallax. Using the pointing manner, we cannot only drag icons or windows to any locations in 3D space, but also intuitively perform most desktop operations such as pilling up a pop-up menu and pushing a button. Because it is not easy for most people to control their vergence angle as they want, in our pointing manner, we present a feedback system of degree of vergence through an indicator appearing around a pointing cursor. In order to show the effectiveness of our proposal, we have implemented the pointing manner in a VR headset with an eye-tracker, and we have conducted numerical experiments. The experimental results show that our vergence based operations are feasible. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-22643-5_14,2019,"Tangnimitchok S., O-larnnithipong N., Ratchatanantakit N., Barreto A.",Affective Monitor: A Process of Data Collection and Data Preprocessing for Building a Model to Classify the Affective State of a Computer User,"This paper outlines the first phase of our implementation of a system for non-intrusive estimation of a computer userâ€™s affective state based on the Circumplex Model of Affect [1], from monitoring the userâ€™s pupil diameter and facial expression [2]. The details of the original design plan for this system have been described previously [2]. The outline describes each part of data collecting process including: Obtaining 3D facial coordinates by Kinect, recording the pupil diameter signal, embedding the facial expression to Facial Animation Parameter indices, and the description of how the experiment will be setup. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-22341-0_35,2019,"Renker J., Kreutzfeldt M., Rinkenauer G.",Eye Blinks Describing the State of the Learner Under Uncertainty,"Adaptive systems are able to support the human-machine interaction in a great manner. However, the question arises which parameter are useful to gain insights into the user and can be easily implemented in the adaptive system. Eye blinks are frequent and most of the time automatic actions that reflect attentional and cognitive processes. They have not gained much attention in the context of adaptive systems until now. Thus, the current experiment investigated the number of blinks as an indicator of the state of the user while interacting with a technical system. Participants had to perform a dynamic visual spatial search task while their eye blinks were tracked. The task is to predict the appearance of target objects and thereby to learn a probability concept in order to improve the prediction. Results showed that eye blinks could distinguish between good and poor learner and increased parallel to the increasing task performance. Further, eye blinks reflected the information processing during the performance of a trial and the completion of the task. Thus, eye blinks might inform about the needs of the user with regard to the amount and detailedness of new information as well as additional help. However, the individual variability necessitates a separate baseline to be determined for each user. Further research is needed to foster the results also in more applied settings. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-23207-8_37,2019,"Min W., Park K., Wiggins J., Mott B., Wiebe E., Boyer K.E., Lester J.",Predicting dialogue breakdown in conversational pedagogical agents with multimodal LSTMs,"Recent years have seen a growing interest in conversational pedagogical agents. However, creating robust dialogue managers for conversational pedagogical agents poses significant challenges. Agentsâ€?misunderstandings and inappropriate responses may cause breakdowns in conversational flow, lead to breaches of trust in agent-student relationships, and negatively impact student learning. Dialogue breakdown detection (DBD) is the task of predicting whether an agentâ€™s utterance will cause a breakdown in an ongoing conversation. A robust DBD framework can support enhanced user experiences by choosing more appropriate responses, while also offering a method to conduct error analyses and improve dialogue managers. This paper presents a multimodal deep learning-based DBD framework to predict breakdowns in student-agent conversations. We investigate this framework with dialogues between middle school students and a conversational pedagogical agent in a game-based learning environment. Results from a study with 92 middle school students demonstrate that multimodal long short-term memory network (LSTM)-based dialogue breakdown detectors incorporating eye gaze features achieve high predictive accuracies and recall rates, suggesting that multimodal detectors can play an important role in designing conversational pedagogical agents that effectively engage students in dialogue. Â© Springer Nature Switzerland AG 2019.",,,
10.5220/0007582705910596,2019,"Yamagishi K., Takemura K.",A hybrid method for remote eye tracking using RGB-IR Camera,"Methods for eye tracking using images can be divided largely into two categories: Methods using a near-infrared image and methods using a visible image. These images have been used independently in conventional eye-tracking methods; however, each category of methods have different advantageous features. Therefore, we propose using these images simultaneously to compensate for the weak points in each technique, and an RGB-IR camera, which can capture visible and near-infrared images simultaneously, is employed. Pupil detection can yield better results than iris detection because the eyelid often occludes the iris. On the other hand, the iris area can be used for model fitting because the iris size is constant. The model fitting can be automated at initialization; thus, the relationship between the 3D eyeball model and eye camera is solved. Additionally, the positions of the eye and gaze vectors are estimated continuously using these images for tracking. We conducted several experiments for evaluating the proposed method and confirmed its feasibility. Copyright Â© 2019 by SCITEPRESS â€?Science and Technology Publications, Lda. All rights reserved",,,
,2019,"Zdebskyi P., Vysotska V., Peleshchak R., Peleshchak I., Demchuk A., Krylyshyn M.",An application development for recognizing of view in order to control the mouse pointer,"The purpose of this article is to develop an application for recognizing the user's point of view in order to control the mouse pointer. In the course of the task, an analysis of the subject area and technologies for implementation of the application was conducted. The algorithms of machine learning for solving the problem were considered. The input data of system are 50 coordinates placed on the face, which include the contours of the face, eyebrows, eyes, nose and coordinates of pupils. Finding the required coordinates occur without use of special devices, but only with webcam used for recognition. The application is implemented in the form of two modules, one of which is responsible for training system for the recognition of the view; the other one is responsible for controlling the mouse cursor with a view. The product can be operated on any operational system: the main requirement is the presence of an interpreter for the Python programming language, which can be downloaded for free from the official site. Â© 2019 CEUR-WS. All rights reserved.",,,
10.1080/03772063.2019.1622461,2019,"Jothi Prabha A., Bhargavi R.",Prediction of Dyslexia from Eye Movements Using Machine Learning,"Dyslexia is a reading disability and a language disorder where the individual exhibits difficulty in reading, writing, speaking, and trouble in spelling words. Early prediction of dyslexia can help dyslexics to get early support or intervention through remedial teaching. There is no remarkable computational model for the prediction of dyslexia in the literature. Existing methods to diagnose dyslexia include oral and written assessments, analysis and interpretation of Magnetic Resonance Imaging (MRI), functional MRI (fMRI), and Electroencephalogram (EEG). These methods require every instance to be interpreted by the domain expert in all stages whereas rigorously trained and tested computational models need subject expert intervention only at the end. In this paper, a prediction model has been proposed that uses statistical methods to differentiate dyslexics from non-dyslexics using their eye movement. The eye movements are tracked with an eye tracker. Eye movement has many features like fixations, saccades, transients, and distortions. From the raw data of eye tracker, high-level features are extracted using Principal Component Analysis. This paper proposes a Particle Swarm Optimization (PSO)-based Hybrid Kernel SVM-PSO for the prediction of dyslexia in individuals. The proposed model gives better predictive accuracy of 95% compared to a Linear SVM model. The proposed model is validated on 187 subjects by tracking their eye movements while reading. It is observed that eye movement data along with machine learning can be used for building models of high predictive accuracy. The proposed model can be used as a screening tool for the diagnosis of dyslexia in schools. Â© 2019, Â© 2019 IETE.",,,
10.1007/978-3-030-20521-8_27,2019,"Findling R.D., Nguyen L.N., Sigg S.",Closed-Eye Gaze Gestures: Detection and Recognition of Closed-Eye Movements with Cameras in Smart Glasses,"Gaze gestures bear potential for user input with mobile devices, especially smart glasses, due to being always available and hands-free. So far, gaze gesture recognition approaches have utilized open-eye movements only and disregarded closed-eye movements. This paper is a first investigation of the feasibility of detecting and recognizing closed-eye gaze gestures from close-up optical sources, e.g. eye-facing cameras embedded in smart glasses. We propose four different closed-eye gaze gesture protocols, which extend the alphabet of existing open-eye gaze gesture approaches. We further propose a methodology for detecting and extracting the corresponding closed-eye movements with full optical flow, time series processing, and machine learning. In the evaluation of the four protocols we find closed-eye gaze gestures to be detected 82.8%â€?1.6% of the time, and extracted gestures to be recognized correctly with an accuracy of 92.9%â€?9.2%. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-22244-4_26,2019,"Abdessalem H.B., Chaouachi M., Boukadida M., Frasson C.",Toward real-time system adaptation using excitement detection from eye tracking,"Usersâ€?performance is known to be impacted by their emotional states. To better understand this relationship, different situations could be simulated during which the usersâ€?emotional reactions are analyzed through sensors like eye tracking and EEG. In addition, virtual reality environments provide an immersive simulation context that induces high intensity emotions such as excitement. Extracting excitement from EEG provides more precise measures then other methods, however it is not always possible to use EEG headset in virtual reality environment. In this paper we present an alternative approach to the use of EEG for excitement detection using only eye movements. Results showed that there is a correlation between eye movements and excitement index extracted from EEG. Five machine learning algorithms were used in order to predict excitement trend exclusively from eye tracking. Results revealed that we can detect the offline excitements trend directly from eye movements with a precision of 92% using deep neural network. Â© Springer Nature Switzerland AG 2019.",,,
10.1007/978-3-030-20876-9_20,2019,"Chen Z., Shi B.E.",Appearance-Based Gaze Estimation Using Dilated-Convolutions,"Appearance-based gaze estimation has attracted more and more attention because of its wide range of applications. The use of deep convolutional neural networks has improved the accuracy significantly. In order to improve the estimation accuracy further, we focus on extracting better features from eye images. Relatively large changes in gaze angles may result in relatively small changes in eye appearance. We argue that current architectures for gaze estimation may not be able to capture such small changes, as they apply multiple pooling layers or other downsampling layers so that the spatial resolution of the high-level layers is reduced significantly. To evaluate whether the use of features extracted at high resolution can benefit gaze estimation, we adopt dilated-convolutions to extract high-level features without reducing spatial resolution. In cross-subject experiments on the Columbia Gaze dataset for eye contact detection and the MPIIGaze dataset for 3D gaze vector regression, the resulting Dilated-Nets achieve significant (upÃ‚Â to 20.8%) gains when compared to similar networks without dilated-convolutions. Our proposed Dilated-Net achieves state-of-the-art results on both the Columbia Gaze and the MPIIGaze datasets. Â© 2019, Springer Nature Switzerland AG.",,,
10.1117/12.2508357,2019,"Tsujino T., Nakamura H., Fujishima T., Hamagishi G., Yoshimoto K., Takahashi H., Matsumoto T., Kusafuka K.",3D display with active parallax barrier using the monochromatic LC panel of specifications same as the image display panel,"In the stereoscopic 3D display using the parallax barrier, the active barrier method can expand the viewing area if the barrier pattern optimizes corresponding to the position of the observer by eye tracking. However, requiring not only the LC display panel but also the specially shaped LC panel for the active barrier, this system should be very expensive. We propose using the active barrier which is a monochromatic panel of the same pixel shape as the image display panel. In the proposed method, it is easy to manufacture panels for the active barrier, and the 3D display provide the wide viewing area and high quality 3D images for observers. When the active barrier is a monochromatic panel having the same pixel shape as the image display panel, basically the barrier pitch cannot realize the ideal value. Thus, the observer cannot observe the stereoscopic image in the full screen. In order to realize stereoscopic observation, we apply the cycle pitch composing the stereoscopic image. The cycle pitch composing the stereoscopic image is the method to bring the pitch constituting the L/R image closer to the ideal value by periodically increasing the number of dots constituting the L/R image. To confirm the effectiveness of the proposed method, the crosstalk of the prototypes using film barriers were measured. Crosstalk was less than or equal to 10% at the viewing distance of 421 mm to 1238 mm. That crosstalk can be reduced regardless of observation distance was confirmed. Â© 2019 SPIE.",,,
10.1117/12.2509954,2019,"Zabels R., Osmanis K., Narels M., Smukulis R., Osmanis I.",Integrated head-mounted display system based on a multi-planar architecture,"LightSpace Technologies have developed a prototype of integrated head-mounted stereoscopic display system based on a proprietary multi-plane optical diffuser technology. The system is entirely solid-state and has six focal planes which covers âˆ? diopters (from 32 cm to 8 m). For the operation no eye-tracking is utilized. The new display system virtually entirely eliminates vergence-accommodation conflict and adds a monocular accommodation as an important depth cue for improved 3D realism. In regards to content rendering the processing load in contrast to conventional single-focalplane stereoscopic displays with similar image resolution is only slightly increased. The differences in terms of comparative performance are the worst in the case of simple 3D scenes, while for high-complexity scenes this difference has a tendency to slightly decrease. On average the processing burden for multi-plane stereoscopic displays is no more than 1.5% higher than for conventional stereoscopic displays. Furthermore, increasing a number of physical focal planes doesn't notably worsen the image rendering performance allowing the display device to be efficiently driven by already readily available hardware-including high-performance mobile platforms. Overall, the user feedback about the developed multi-plane stereoscopic 3D display prototype confirms prior proposed assumptions of multi-plane architecture yielding higher acceptance rate due to improved 3D realism and eradicated vergence-accommodation conflict, thus currently being one of the most noteworthy advancements in the field of 3D stereoscopic displays. Â© 2019 SPIE.",,,
10.1117/12.2509038,2019,"Fujishima T., Nakamura H., Tsujino T., Hamagishi G., Yoshimoto K., Takahashi H., Matsumoto T., Kusafuka K.",A novel control method of the combination of simple active barrier pitch control and image processing to extremely expand the viewing zone in forward and backward directions of stereoscopic 3D displays,"We have previously proposed eye tracking system to expand viewing area in all directions for glasses-free 3D display. In this system, since the parallax barrier was fixed, the viewing zone was expanded by image processing corresponding to the viewing position. Thus, there was a limit to expanding the viewing zone only by image processing. On the other hand, we can expand the viewing zone by applying an active barrier that changes to the optimum barrier pattern corresponding to the viewing position by eye tracking. However, to change the active barrier pattern, complex calculation and a specially designed active barrier LC panel are required. To overcome this problem, we propose a novel control method to expand the viewing zone of stereoscopic 3D displays with active parallax barrier in depth direction. The proposed method is the combination of the simple generation method of a barrier pattern and the synthesis method of Left/Right synthetic image corresponding to the viewing position. In this method, let the optimum barrier pitch be Bp at the viewing distance d, we set the barrier pitch to xâˆ—Bp and synthesize the Left/Right image so that crosstalk is low at the same time at the viewing distance d/x. To verify the effectiveness of the proposed method, we measured the crosstalk of the prototype 3D display. The crosstalk ratios at the optimum viewing distance 1092 mm (d), 774 mm (dâˆ?/4) and 546 mm (dâˆ?/2) were 4.28%, 3.82% and 5.02%, respectively. Therefore, low crosstalk 3D images could be observed. Â© 2019 SPIE.",,,
10.1371/journal.pcbi.1006897,2019,"Cadena S.A., Denfield G.H., Walker E.Y., Gatys L.A., Tolias A.S., Bethge M., Ecker A.S.",Deep convolutional models improve predictions of macaque V1 responses to natural images,"Despite great efforts over several decades, our best models of primary visual cortex (V1) still predict spiking activity quite poorly when probed with natural stimuli, highlighting our limited understanding of the nonlinear computations in V1. Recently, two approaches based on deep learning have emerged for modeling these nonlinear computations: transfer learning from artificial neural networks trained on object recognition and data-driven convolutional neural network models trained end-to-end on large populations of neurons. Here, we test the ability of both approaches to predict spiking activity in response to natural images in V1 of awake monkeys. We found that the transfer learning approach performed similarly well to the data-driven approach and both outperformed classical linear-nonlinear and waveletbased feature representations that build on existing theories of V1. Notably, transfer learning using a pre-trained feature space required substantially less experimental time to achieve the same performance. In conclusion, multi-layer convolutional neural networks (CNNs) set the new state of the art for predicting neural responses to natural images in primate V1 and deep features learned for object recognition are better explanations for V1 computation than all previous filter bank theories. This finding strengthens the necessity of V1 models that are multiple nonlinearities away from the image domain and it supports the idea of explaining early visual cortex based on high-level functional goals. Â© 2019 Cadena et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,,
10.1109/ACCESS.2019.2909573,2019,"Yoo S., Jeong D.K., Jang Y.",The Study of a Classification Technique for Numeric Gaze-Writing Entry in Hands-Free Interface,"Recently, many applications are developed in numerous domains with various environments. Since some environments require hands-free applications, new technology is needed for the input interfaces other than the mouse and keyboard. Therefore, to meet the needs, many researchers have begun to investigate the gaze and voice for the input technology. In particular, there are many approaches to render virtual keyboards with the gaze. However, since the virtual keyboards hide the screen space, this technique can only be applied in limited environments. In this paper, we propose a classification technique for gaze-written numbers as the hands-free interface. Since the gaze-writing is less accurate compared to the virtual keyboard typing, we apply the convolutional neural network (CNN) deep learning algorithm to recognize the gaze-writing and improve the classification accuracy. Besides, we create new gaze-writing datasets for training, gaze MNIST (gMNIST), by modifying the MNIST data with features of the gaze movement patterns. For the evaluation, we compare our approach with the basic CNN structures using the original MNIST dataset. Our study will allow us to have more options for the input interfaces and expand our choices in hands-free environments. Â© 2013 IEEE.",,,
10.1007/978-3-030-14802-7_52,2019,"Sledzianowski A., Szymanski A., Drabik A., Szlufik S., Koziorowski D.M., Przybyszewski A.W.",Measurements of Antisaccades Parameters Can Improve the Prediction of Parkinsonâ€™s Disease Progression,"In this text we present the results of oculometric experiment consisting the registration of anitsaccades of patients with Parkinsonâ€™s Disease (PD) in relation to their neurological data. PD is an important and incurable neurodegenerative disease and we are looking for methods optimizing the treatment. In our previous works we used Reflexive Saccades (RS) and Pursuit Ocular Movements (POM) to check what it can tell us about the diseaseâ€™s progression expressed in the Unified Parkinsonâ€™s Disease Rating Scale (UPDRS). The UPDRS is the most commonly used scale in the clinical studies of Parkinsonâ€™s disease. In this experiment we examined antisaccades (AS) of 11 PD patients who performed eye movement tests in controlled conditions. We correlated neurological measurements of patientâ€™s motoric abilities and data describing their treatment with values of AS parameters. We used RSES and for prediction of the UPDRS scoring groups and Weka methods for presentation of the results. We achieved good results with accuracy of 91% and coverage of 100%. The AS test is a relatively easy and non-invasive method that can be used in the telemedicine in the future. Â© 2019, Springer Nature Switzerland AG.",,,
,2019,Mutlu-Bayraktar D.,Change blindness in multimedia learning environment,"This study investigates the change blindness that may occur in multimedia learning environments. For this purpose, a multimedia animation which had some changes was designed. The eye movements were examined during the process of detecting the changes in multimedia via eye tracking technics. The research model was defined as a controlled experiment method. Fifteen ungraduated students participated in the experiment. Attention levels of participants were determined by d2 Attention Test. Change detection numbers of participants were analyzed according to their attention level and their gender. The appearance of a major object on the scene was the most detected change and the change on the detail object was detected less. According to findings about the attention level and change detection, the participants at high attention level were more successful at detecting change in multimedia. Females were more successful in detecting change than males. Â© 2019 Association for the Advancement of Computing in Education. All Rights Reserved.",,,
10.1109/ACCESS.2019.2900424,2019,"Li P., Hou X., Duan X., Yip H., Song G., Liu Y.",Appearance-Based Gaze Estimator for Natural Interaction Control of Surgical Robots,"Robots are playing an increasingly important role in modern surgery. However, conventional human-computer interaction methods, such as joystick control and sound control, have some shortcomings, and medical personnel are required to specifically practice operating the robot. We propose a human-computer interaction model based on eye movement with which medical staff can conveniently use their eye movements to control the robot. Our algorithm requires only an RGB camera to perform tasks without requiring expensive eye-tracking devices. Two kinds of eye control modes are designed in this paper. The first type is the pick and place movement, with which the user uses eye gaze to specify the point where the robotic arm is required to move. The second type is user command movement, with which the user can use eye gaze to select the direction in which the user desires the robot to move. The experimental results demonstrate the feasibility and convenience of these two modes of movement. Â© 2013 IEEE.",,,
10.1109/ACCESS.2019.2896303,2019,"Yang B., Zhang X., Li Z., Du S., Wang F.",An Accurate and Robust Gaze Estimation Method Based on Maximum Correntropy Criterion,"Accurately estimating the user's gaze is important in many applications, such as human-computer interaction. Due to great convenience, appearance-based methods for gaze estimation have been a popular subject of research for many years. However, the greatest challenges in the appearance-based gaze estimation in a desktop environment are how to simplify the calibration process and deal with other issues such as image noise and low resolution. To address the problems, we adopt a mapping relationship between the high-dimensional eye image features space and the low-dimensional gaze positions and propose a robust and accurate method for gaze estimation with a webcam. First, we utilize Kullback-Leibler divergence to reduce feature dimension and keep similarity between the feature space and the gaze space. Then, we construct the objective function using the maximum correntropy criterion instead of mean squared error, which can enhance the anti-noise ability, especially for outliers or pixel corruption. A regularization term is adopted to adaptively select the sparse training samples for gaze estimation. We conducted extensive experiments in a desktop environment, which verified that the proposed method was robust and efficient in dealing with sparse training samples, pixel corruption, and low-resolution problems in gaze estimation. Â© 2013 IEEE.",,,
10.1117/1.JEI.28.1.013002,2019,"Li J., Shan Y., Li S., Chen T.",Gaze estimation using a head-mounted single full-view camera,"We present a gaze estimation method for a head-mounted full-view egocentric camera that can capture egocentric video together with users' gaze cues. While the conventional gaze recording device has two cameras, an eye camera and a scene camera, the proposed gaze recording device has only a single spherical camera with a full field of view. To determine the point of gaze on full-view images, we present an eye-model-based gaze estimation method by means of three-dimensional iris projection. First, an eye model is built according to the precalibration process of an eyeball center and the eyeball biological parameters; then, we express the 3D iris contour in the eye model and project it back to the spherical camera model. since 2D iris contours can be detected directly on images and can also be expressed under the spherical model, the relationship between the 3D iris contour and the 2D iris contour can be found. By solving this problem, the gaze direction under the camera model is determined; subsequently, the point of gaze on the images can be inferred. since in the proposed method, a single spherical camera can play the role of the conventional two cameras, not only do the proposed method results have a simpler system structure but also the cumbersome operation of the calibration for the conventional two-camera gaze measurement devices becomes unnecessary. The effectiveness of the proposed method is shown by the experimental results. Â© 2019 SPIE and IS and T.",,,
10.1007/978-3-030-11012-3_35,2019,"Yu Y., Liu G., Odobez J.-M.",Deep multitask gaze estimation with a constrained Landmark-Gaze model,"As an indicator of attention, gaze is an important cue for human behavior and social interaction analysis. Recent deep learning methods for gaze estimation rely on plain regression of the gaze from images without accounting for potential mismatches in eye image cropping and normalization. This may impact the estimation of the implicit relation between visual cues and the gaze direction when dealing with low resolution images or when training with a limited amount of data. In this paper, we propose a deep multitask framework for gaze estimation, with the following contributions. (i) we proposed a multitask framework which relies on both synthetic data and real data for end-to-end training. During training, each dataset provides the label of only one task but the two tasks are combined in a constrained way. (ii) we introduce a Constrained Landmark-Gaze Model (CLGM) modeling the joint variation of eye landmark locations (including the iris center) and gaze directions. By relating explicitly visual information (landmarks) to the more abstract gaze values, we demonstrate that the estimator is more accurate and easier to learn. (iii) by decomposing our deep network into a network inferring jointly the parameters of the CLGM model and the scale and translation parameters of eye regions on one hand, and a CLGM based decoder deterministically inferring landmark positions and gaze from these parameters and head pose on the other hand, our framework decouples gaze estimation from irrelevant geometric variations in the eye image (scale, translation), resulting in a more robust model. Thorough experiments on public datasets demonstrate that our method achieves competitive results, improving over state-of-the-art results in challenging free head pose gaze estimation tasks and on eye landmark localization (iris location) ones. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-10925-7_13,2019,"Makowski S., JÃ¤ger L.A., Abdelwahab A., Landwehr N., Scheffer T.",A discriminative model for identifying readers and assessing text comprehension from eye movements,"We study the problem of inferring readersâ€?identities and estimating their level of text comprehension from observations of their eye movements during reading. We develop a generative model of individual gaze patterns (scanpaths) that makes use of lexical features of the fixated words. Using this generative model, we derive a Fisher-score representation of eye-movement sequences. We study whether a Fisher-SVM with this Fisher kernel and several reference methods are able to identify readers and estimate their level of text comprehension based on eye-tracking data. While none of the methods are able to estimate text comprehension accurately, we find that the SVM with Fisher kernel excels at identifying readers. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-10925-7_12,2019,"Drimalla H., Landwehr N., Baskow I., Behnia B., Roepke S., Dziobek I., Scheffer T.",Detecting autism by analyzing a simulated social interaction,"Diagnosing autism spectrum conditions takes several hours by well-trained practitioners; therefore, standardized questionnaires are widely used for first-level screening. Questionnaires as a diagnostic tool, however, rely on self-reflectionâ€”which is typically impaired in individuals with autism spectrum condition. We develop an alternative screening mechanism in which subjects engage in a simulated social interaction. During this interaction, the subjectsâ€?voice, eye gaze, and facial expression are tracked, and features are extracted that serve as input to a predictive model. We find that a random-forest classifier on these features can detect autism spectrum condition accurately and functionally independently of diagnostic questionnaires. We also find that a regression model estimates the severity of the condition more accurately than the reference screening method. Â© 2019, Springer Nature Switzerland AG.",,,
10.1007/978-981-13-5977-4_41,2019,"Bhattacharjee A., Pal S.",Attention of viewers while viewing paintings changes with the different ccts of exhibition light: A quantitative approach with eye-tracking method,"Light influences the appearance of paintings in any exhibition. Few studies have experimented with correlated colour temperature (CCT) and illuminance of light to understand the lighting preference of viewers while viewing paintings. However, effect of only CCT on viewersâ€?perception is still a debatable issue. Also, previous studies in this regard have taken subjective approach with category rating that may lead to inconsistent conclusion. Therefore, a study has been designed with quantitative approach using eye-tracking method (N = 10) to verify the effect of different CCTs on viewersâ€?attention. The experimental result shows that viewersâ€?attention while viewing similar paintings changes with different CCTs of exhibition light having all other light parameters constant. Â© Springer Nature Singapore Pte Ltd 2019.",,,
10.1016/j.dss.2018.10.012,2019,"Shojaeizadeh M., Djamasbi S., Paffenroth R.C., Trapp A.C.",Detecting task demand via an eye tracking machine learning system,"Computerized systems play a significant role in today's fast-paced digital economy. Because task demand is a major factor that influences how computerized systems are used to make decisions, identifying task demand automatically provides an opportunity for designing advanced decision support systems that can respond to user needs at a personalized level. A first step for designing such advanced decision tools is to investigate possibilities for developing automatic task load detectors. Grounded in decision making, eye tracking, and machine learning literature, we argue that task demand can be detected automatically, reliably, and unobtrusively using eye movements only. To investigate this possibility, we developed an eye tracking task load detection system and tested its effectiveness. Our results revealed that our task load detection system reliably predicted increased task demand from users' eye movement data. These results and their implications for research and practice are discussed. Â© 2018 Elsevier B.V.",,,
10.1016/j.media.2018.10.010,2019,"Khosravan N., Celik H., Turkbey B., Jones E.C., Wood B., Bagci U.","A collaborative computer aided diagnosis (C-CAD) system with eye-tracking, sparse attentional model, and deep learning","Computer aided diagnosis (CAD) tools help radiologists to reduce diagnostic errors such as missing tumors and misdiagnosis. Vision researchers have been analyzing behaviors of radiologists during screening to understand how and why they miss tumors or misdiagnose. In this regard, eye-trackers have been instrumental in understanding visual search processes of radiologists. However, most relevant studies in this aspect are not compatible with realistic radiology reading rooms. In this study, we aim to develop a paradigm shifting CAD system, called collaborative CAD (C-CAD), that unifies CAD and eye-tracking systems in realistic radiology room settings. We first developed an eye-tracking interface providing radiologists with a real radiology reading room experience. Second, we propose a novel algorithm that unifies eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a graph model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve their diagnostic decisions. The C-CAD uses radiologistsâ€?search efficiency by processing their gaze patterns. Furthermore, the C-CAD incorporates a deep learning algorithm in a newly designed multi-task learning platform to segment and diagnose suspicious areas simultaneously. The proposed C-CAD system has been tested in a lung cancer screening experiment with multiple radiologists, reading low dose chest CTs. Promising results support the efficiency, accuracy and applicability of the proposed C-CAD system in a real radiology room setting. We have also shown that our framework is generalizable to more complex applications such as prostate cancer screening with multi-parametric magnetic resonance imaging (mp-MRI). Â© 2018 Elsevier B.V.",,,
10.1080/10494820.2018.1530682,2019,Mutlu-Bayraktar D.,Evaluation of change blindness in multimedia learning environment with cognitive process,"This study aims to investigate the change blindness and cognitive processes with eye-tracking method in multimedia learning environment. For this purpose, a multimedia animation which had some changes was designed. The eye movements were examined during the process of detecting the changes in multimedia via eye-tracking technics. The research model was defined as controlled experiment and survey methods. Twenty-one ungraduated students participated in the experiment. Attention and perception levels of participants were determined by d2 Attention Test and Group Embedded Figures Test. Change detection numbers of participants were analyzed according to their attention level and field dependence. The appearance of a major object on the scene was the most detected change and the change on the detail object was detected less. According to findings about the attention level and change detection, the participants at high attention level were more successful at detecting change in multimedia. It was observed that field-independent individuals could detect the change much more compared to the field-dependent individuals. Â© 2018, Â© 2018 Informa UK Limited, trading as Taylor & Francis Group.",,,
10.1007/978-3-319-96074-6_4,2019,"Park S.J., Hong S., Kim D., Hussain I., Seo Y.",Intelligent in-car health monitoring system for elderly drivers in connected car,"Introduction: Health has become a major concern nowadays. People pass significant amount of time of daily life on driving seat. Some health complexity happens during driving like heart problem, stroke etc. Driverâ€™s health abnormality may also effect safety of other vehicles. So, automotive manufacturers and users are interested to include real-time health monitoring in car system. Intelligent in-car health monitoring is considered most innovative technology which is able to measure real-time physiological parameters of drivers, feed data to web cloud, analysis using machine learning, artificial intelligence and big data. Brain stroke is most deadly diseases and effected persons lose conscience and ability to contact emergency services or hospital. Emergency medical assistance is necessary in order to survive from any kind of disability due to stroke. Purpose: The aim of our study is to develop a health monitoring system for elderly drivers using air cushion car seat and embedded IoT (Internet of Things) devices in order to detect stroke onset during driving. Method: Real-time monitoring is desired to detect stroke onset during regular activities like driving. Abnormal physiological signals, face pattern generated during stroke onset can be traced by real-time monitoring using sensors. Here, we have suggested a framework of stroke onset detection using sensors and developed a system suitable for elderly drivers. This system can measure and analyze data of ECG, EEG, heart rate, seat pressure balance data, face/eye tracking etc. using IoT sensors. Physiological data will be feed to cloud and compared with reference normal person data. Findings: If any health abnormality such as stroke is found in real-time monitoring, system will predict type and severity of stroke and suggest possible steps. System may switch car control to autonomous driving mode if available and move the car to safe place. System may also generate alarm and send message with available information such as position to relatives and emergency services to provide emergency assistance so that effected driver can be transferred to hospital/clinic. Â© Springer Nature Switzerland AG 2019.",,,
10.1007/978-3-319-93885-1_30,2019,"Lotz A., Weissenberger S.",Predicting Take-Over Times of Truck Drivers in Conditional Autonomous Driving,"Conditional autonomous driving requires the description of sufficient time reserves for drivers in take-over situations. The definition of this time reserve has not been addressed for the truck context thus far. Through the observation of physiological measures, the possibility of estimating reaction times is considered. Driver data is collected with a remote eye-tracker and body posture camera. Empirical data from a simulator study is utilized to train and compare four machine learning algorithms and generate driver features. The estimation of take-over times is defined as a classification problem with four reaction time classes, leading to a misclassification rate of a linear support vector machine (SVM) of 38.7%. Utility of driver features for reaction time estimation are discussed. Â© Springer International Publishing AG, part of Springer Nature 2019.",,,
10.1007/978-3-319-94947-5_17,2019,"Peng H., Liu S., Zhang T.",Study on human-computer interaction in the design of public self-service equipment,"Public self-service equipment has a fixed use environment, complex product function, broad age group of users and other remarkable features compared to other products. This paper takes the increased amount of information and complicated operation of self-service equipment nowadays as the breakthrough point to conduct theory, case study and evaluation. This article studies the human-computer interaction factors in public self-service equipment from two aspects: appearance function design and interface interaction design. taking the bank self-service bank card machine as an example, and several simulation interface interactive systems are designed, then through the eye-tracking for testing, to analyze the data from the tests, according to the theoretical and experimental research results, the appearance and interactive interface of bank self-service card machine are designed. The results of this article greatly enhance the usersâ€?interactive experience, and achieve the optimization and upgrading of human-computer interaction, the theoretical results of this article are of reference, the experimental results are repeatable, which are conducive to different disciplines of reference and using for reference. Â© 2019, Springer International Publishing AG, part of Springer Nature.",,,
10.1109/TPAMI.2017.2778103,2019,"Zhang X., Sugano Y., Fritz M., Bulling A.",MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation,"Learning-based methods are believed to work well for unconstrained gaze estimation, i.e. gaze estimation from a monocular RGB camera without assumptions regarding user, environment, or camera. However, current gaze datasets were collected under laboratory conditions and methods were not evaluated across multiple datasets. Our work makes three contributions towards addressing these limitations. First, we present the MPIIGaze dataset, which contains 213,659 full face images and corresponding ground-truth gaze positions collected from 15 users during everyday laptop use over several months. An experience sampling approach ensured continuous gaze and head poses and realistic variation in eye appearance and illumination. To facilitate cross-dataset evaluations, 37,667 images were manually annotated with eye corners, mouth corners, and pupil centres. Second, we present an extensive evaluation of state-of-the-art gaze estimation methods on three current datasets, including MPIIGaze. We study key challenges including target gaze range, illumination conditions, and facial appearance variation. We show that image resolution and the use of both eyes affect gaze estimation performance, while head pose and pupil centre information are less informative. Finally, we propose GazeNet, the first deep appearance-based gaze estimation method. GazeNet improves on the state of the art by 22 percent (from a mean error of 13.9 degrees to 10.8 degrees) for the most challenging cross-dataset evaluation. Â© 1979-2012 IEEE.",,,
10.1145/3293353.3293423,2018,"Lahiri A., Agarwalla A., Biswas P.K.",Unsupervised Domain Adaptation for Learning Eye Gaze from a Million Synthetic Images: An Adversarial Approach,"With contemporary advancements of graphics engines, recent trend in deep learning community is to train models on automatically annotated simulated examples and apply on real data during test time. This alleviates the burden of manual annotation. However, there is an inherent difference of distributions between images coming from graphics engine and real world. Such domain difference deteriorates test time performances of models trained on synthetic examples. In this paper we address this issue with unsupervised adversarial feature adaptation across synthetic and real domain for the special use case of eye gaze estimation which is an essential component for various downstream HCI tasks. We initially learn a gaze estimator on annotated synthetic samples rendered from a 3D game engine and then adapt the features of unannotated real samples via a zero-sum minmax adversarial game against a domain discriminator following the recent paradigm of generative adversarial networks. Such adversarial adaptation forces features of both domains to be indistinguishable which enables us to use regression models trained on synthetic domain to be used on real samples. On the challenging MPIIGaze real life dataset, we outperform recent fully supervised methods trained on manually annotated real samples by appreciable margins and also achieve 13% more relative gain after adaptation compared to the current benchmark method of SimGAN [31]. Codes available at: https://github.com/abhinavagarwalla/adversarial_da_icvgip18. Â© 2018 ACM.",,,
10.1109/CVPR.2018.00053,2018,"Wang K., Zhao R., Ji Q.",A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation,"In this work, we introduce a Hierarchical Generative Model (HGM) to enable realistic forward eye image synthesis, as well as effective backward eye gaze estimation. The proposed HGM consists of a hierarchical generative shape model (HGSM), and a conditional bidirectional generative adversarial network (c-BiGAN). The HGSM encodes eye geometry knowledge and relates eye gaze with eye shape, while c-BiGAN leverages on big data and captures the dependency between eye shape and eye appearance. As an intermediate component, eye shape connects knowledge-based model (HGSM) with data-driven model (c-BiGAN) and enables bidirectional inference. Through a top-down inference, the HGM can synthesize eye images consistent with the given eye gaze. Through a bottom-up inference, HGM can infer eye gaze effectively from a given eye image. Qualitative and quantitative evaluations on benchmark datasets demonstrate our model's effectiveness on both eye image synthesis and eye gaze estimation. In addition, the proposed model is not restricted to eye images only. It can be adapted to face images and any shape-appearance related fields. Â© 2018 IEEE.",,,
10.1109/CVPR.2018.00559,2018,"Xu Y., Dong Y., Wu J., Sun Z., Shi Z., Yu J., Gao S.",Gaze Prediction in Dynamic 360Â° Immersive Videos,"This paper explores gaze prediction in dynamic 360Â° immersive videos, i.e., based on the history scan path and VR contents, we predict where a viewer will look at an upcoming time. To tackle this problem, we first present the large-scale eye-tracking in dynamic VR scene dataset. Our dataset contains 208 360Â° videos captured in dynamic scenes, and each video is viewed by at least 31 subjects. Our analysis shows that gaze prediction depends on its history scan path and image contents. In terms of the image contents, those salient objects easily attract viewers' attention. On the one hand, the saliency is related to both appearance and motion of the objects. Considering that the saliency measured at different scales is different, we propose to compute saliency maps at different spatial scales: The sub-image patch centered at current gaze point, the sub-image corresponding to the Field of View (FoV), and the panorama image. Then we feed both the saliency maps and the corresponding images into a Convolutional Neural Network (CNN) for feature extraction. Meanwhile, we also use a Long-Short-Term-Memory (LSTM) to encode the history scan path. Then we combine the CNN features and LSTM features for gaze displacement prediction between gaze point at a current time and gaze point at an upcoming time. Extensive experiments validate the effectiveness of our method for gaze prediction in dynamic VR scenes. Â© 2018 IEEE.",,,
10.1109/CVPRW.2018.00290,2018,"Ranjan R., De Mello S., Kautz J.",Light-weight head pose invariant gaze tracking,"Unconstrained remote gaze tracking using off-the-shelf cameras is a challenging problem. Recently, promising algorithms for appearance-based gaze estimation using convolutional neural networks (CNN) have been proposed. Improving their robustness to various confounding factors including variable head pose, subject identity, illumination and image quality remain open problems. In this work, we study the effect of variable head pose on machine learning regressors trained to estimate gaze direction. We propose a novel branched CNN architecture that improves the robustness of gaze classifiers to variable head pose, without increasing computational cost. We also present various procedures to effectively train our gaze network including transfer learning from the more closely related task of object viewpoint estimation and from a large high-fidelity synthetic gaze dataset, which enable our ten times faster gaze network to achieve competitive accuracy to its current state-of-the-art direct competitor. Â© 2018 IEEE.",,,
10.1109/CVPRW.2018.00269,2018,"Koutras P., Panagiotaropoulou G., Tsiami A., Maragos P.",Audio-visual temporal saliency modeling validated by fMRI data,"In this work we propose an audio-visual model for predicting temporal saliency in videos, that we validate and evaluate in an alternative way by employing fMRI data. We intend to bridge the gap between the large improvements achieved during the last years in computational modeling, especially in deep learning, and the neurobiological and behavioral research regarding human vision. The proposed audio-visual model incorporates both state-of-the-art deep architectures for visual saliency, which were trained on eye-tracking data, and behavioral findings concerning audio-visual integration in multimedia stimuli. A new fMRI database has been collected for evaluation purposes, that includes various videos and subjects. This dataset may prove useful not only for saliency but for other computer vision problems as well. The evaluation of our model using the new fMRI database under a mixed-effect analysis shows that the proposed saliency model has strong correlation with both the visual and audio brain areas, that confirms its effectiveness and appropriateness in predicting audio-visual saliency for dynamic stimuli. Â© 2018 IEEE.",,,
10.1109/ACPR.2017.155,2018,"Sun H.-P., Yang C.-H., Lai S.-H.",A deep learning approach to appearance-based gaze estimation under head pose variations,"In this paper, we propose a deep learning based gaze estimation algorithm that estimates the gaze direction from a single face image. The proposed gaze estimation algorithm is based on using multiple convolutional neural networks (CNN) to learn the regression networks for gaze estimation from the eye images. The proposed algorithm can provide accurate gaze estimation for users with different head poses, since it explicitly includes the head pose information into the proposed gaze estimation framework. The proposed algorithm can be widely used for appearance-based gaze estimation in practice. Our experimental results show that the proposed gaze estimation system improves the accuracy of appearance-based gaze estimation under head pose variations compared to the previous methods. Â© 2017 IEEE.",,,
10.1109/CVPRW.2018.00281,2018,"Ruiz N., Chong E., Rehg J.M.",Fine-grained head pose estimation without keypoints,"Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models. Â© 2018 IEEE.",,,
10.1109/GCCE.2018.8574844,2018,"Yamamoto T., Seo M., Kitajima T., Chen Y.-W.",Eye Gaze Correction Using Generative Adversarial Networks,"Eye gaze correction is an important topic in video teleconference and video chart in order to keep the eye contact. In this paper, we propose to use a generative adversarial networks for eye gaze correction. We use pairs of front facial image (idea camera setting) and real facial image (real camera setting) to training the network. By using the trained network, we can generate a gaze corrected facial image (front facial image) for any real facial image. Experiments demonstrated the effectiveness of our proposed method. Â© 2018 IEEE.",,,
10.1109/ITSC.2018.8569655,2018,"Schwehr J., Willert V.",Multi-Hypothesis Multi-Model Driver's Gaze Target Tracking,"For a safe handover of the driving task or driver-adaptive warning strategies the driver's situation awareness is a helpful source of information. In order to estimate and track the driver's focus of attention over time in a dynamic automotive scene, a Multi-Hypothesis Multi-Model probabilistic tracking framework was developed in which we postulate consistency between machine and human perception during gaze fixations. Within this framework, we explicitly included target object motion in the spatial transition step and integrated spatiotemporal models of human-like gaze behavior for fixations and saccades in the motion transition. This elaborate design makes the target estimation robust and yet flexible. At the same time, the representation in continuous 2D coordinates makes the algorithm run in real time on a standard laptop. By incorporating dynamic and static potential gaze targets from an object list and a free space spline, the algorithm is in principle independent from the applied sensor setup. The benefit of the proposed model is presented on real world data where the filter's tracking performance as well as the driver's visual sampling are presented based on an exemplary scene. Â© 2018 IEEE.",,,
10.1145/3272127.3275094,2018,"Wang X., Koch S., Holmqvist K., Alexa M.",Tracking the gaze on objects in 3D: How do people really look at the bunny?,"We provide the first large dataset of human fixations on physical 3D objects presented in varying viewing conditions and made of different materials. Our experimental setup is carefully designed to allow for accurate calibration and measurement. We estimate a mapping from the pair of pupil positions to 3D coordinates in space and register the presented shape with the eye tracking setup. By modeling the fixated positions on 3D shapes as a probability distribution, we analysis the similarities among different conditions. The resulting data indicates that salient features depend on the viewing direction. Stable features across different viewing directions seem to be connected to semantically meaningful parts. We also show that it is possible to estimate the gaze density maps from view dependent data. The dataset provides the necessary ground truth data for computational models of human perception in 3D. Â© 2018 Association for Computing Machinery.",,,
10.1145/3272127.3275075,2018,"Nagano K., Seo J., Xing J., Wei L., Li Z., Saito S., Agarwal A., Fursund J., Li H.",Pagan: Real-time avatars using dynamic textures,"With the rising interest in personalized VR and gaming experiences comes the need to create high quality 3D avatars that are both low-cost and variegated. Due to this, building dynamic avatars from a single unconstrained input image is becoming a popular application. While previous techniques that attempt this require multiple input images or rely on transferring dynamic facial appearance from a source actor, we are able to do so using only one 2D input image without any form of transfer from a source image. We achieve this using a new conditional Generative Adversarial Network design that allows fine-scale manipulation of any facial input image into a new expression while preserving its identity. Our photoreal avatar GAN (paGAN) can also synthesize the unseen mouth interior and control the eye-gaze direction of the output, as well as produce the final image from a novel viewpoint. The method is even capable of generating fully-controllable temporally stable video sequences, despite not using temporal information during training. After training, we can use our network to produce dynamic image-based avatars that are controllable on mobile devices in real time. To do this, we compute a fixed set of output images that correspond to key blendshapes, from which we extract textures in UV space. Using a subject's expression blendshapes at run-time, we can linearly blend these key textures together to achieve the desired appearance. Furthermore, we can use the mouth interior and eye textures produced by our network to synthesize on-the-fly avatar animations for those regions. Our work produces state-of-the-art quality image and video synthesis, and is the first to our knowledge that is able to generate a dynamically textured avatar with a mouth interior, all from a single image. Â© 2018 Association for Computing Machinery.",,,
10.1109/CSCI.2017.89,2018,"Anwar S., Milanova M., Svetleff Z., Abdulla S.",Real Time Eye Gaze Estimation,"In this paper we used a set of searching techniques that allow to gain information about eye movement, its location and point of view in real time. The algorithm determines the point on the monitor at which the user is looking at. To obtain such data it is necessary to determine the relative position of the eye and the head. The first step is the initialization during which a head model is created. After initialization, the tracking phase is started using an Active Appearance Models (AAM) and Pose from Orthography and Scaling with ITerations (POSIT) algorithm for head position estimation. The purpose of eye gaze estimation or eye tracking can be used for testing the effectiveness of the text, game, or advertising message. The aim of this work is to develop and implement a system for real time eye gaze estimation using PC's webcam only without any additional hardware. Â© 2017 IEEE.",,,
10.1145/3283289.3283356,2018,"Ruan L., Chen B., Lam M.-L.",Human-computer interaction by voluntary vergence control,"Most people can voluntarily control vergence eye movements. However, the interaction possibility of using vergence as an active input remain largely unexplored. We present a novel human-computer interaction technique which allows a user to control the depth position of an object based on voluntary vergence of the eyes. Our technique is similar to the mechanism for seeing the intended 3D image of an autostereogram, which requires cross-eyed or walleyed viewing. We invite the user to look at a visual target that is mounted on a linear motor, then consciously control the eye convergence to focus at a point in front of or behind the target. A camera is used to measure the eye convergence and control the motion of the linear motor dynamically based on the measured distance. Our technique can enhance existing eye-tracking methods by providing additional information in the depth dimension, and has great potential for hands-free interaction and assistive applications. Â© 2018 Copyright held by the owner/author(s).",,,
10.1109/SPAC46244.2018.8965441,2018,"Cha X., Yang X., Feng Z., Xu T., Fan X., Tian J.",Calibration-Free Gaze Zone Estimation Using Convolutional Neural Network,"In this paper we propose a gaze zone estimation method using deep learning. Compared with traditional method, our method does not need the procedure of calibration. In the proposed method, a Kinect is used to capture the video of a computer user, which is pre-processed to suppress illumination variations. After that, haar cascade classifier is adopted to detect the face region and eye region. Then, the eye region is used to estimate the gaze zone on the monitor via a trained CNN (Convolution Neural Network). Experimental results show that the proposed method has a high accuracy, which can be applied in human-computer interaction. Â© 2018 IEEE.",,,
10.3390/s18124280,2018,"Ghiass R.S., Laurendeau D.",Highly accurate and fully automatic 3D head pose estimation and eye gaze estimation using RGB-3D sensors and 3D morphable models,"This work addresses the problem of automatic head pose estimation and its application in 3D gaze estimation using low quality RGB-D sensors without any subject cooperation or manual intervention. The previous works on 3D head pose estimation using RGB-D sensors require either an offline step for supervised learning or 3D head model construction, which may require manual intervention or subject cooperation for complete head model reconstruction. In this paper, we propose a 3D pose estimator based on low quality depth data, which is not limited by any of the aforementioned steps. Instead, the proposed technique relies on modeling the subjectâ€™s face in 3D rather than the complete head, which, in turn, relaxes all of the constraints in the previous works. The proposed method is robust, highly accurate and fully automatic. Moreover, it does not need any offline step. Unlike some of the previous works, the method only uses depth data for pose estimation. The experimental results on the Biwi head pose database confirm the efficiency of our algorithm in handling large pose variations and partial occlusion. We also evaluated the performance of our algorithm on IDIAP database for 3D head pose and eye gaze estimation. Â© 2018 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1007/s10916-018-1108-1,2018,"Lodato C., Ribino P.",A Novel Vision-Enhancing Technology for Low-Vision Impairments,"Ocular disorders such as vitreoretinal pathologies are widespread, especially in older adults. In particular, degenerative diseases of the retina such as macular senile degenerations are on the rise and affect millions of people with hundreds of thousands of new cases each year. These diseases can cause profoundly disabling visual impairments, in some cases severely compromising the central and/or the peripheral vision in one or both eyes. In this paper, we present a novel vision aids technology that allows for correcting or attenuating the perception of visual field defects due to ocular pathologies of diverse origins or traumas by using techniques of 3D visualisation, eye tracking, and image processing. The presented technology is mainly conceived for providing vision aids that can significantly improve the quality of life of people with this kind of visual disorders. As well, it could be employed for supporting the diagnosis of ocular dysfunctions and for monitoring the progression of diseases. The technology shown in this work is protected by an International Application in Patent Cooperation Treaty (PCT). Â© 2018, Springer Science+Business Media, LLC, part of Springer Nature.",,,
10.1016/j.media.2018.08.007,2018,"Lejeune L., Grossrieder J., Sznitman R.",Iterative multi-path tracking for video and volume segmentation with sparse point supervision,"Recent machine learning strategies for segmentation tasks have shown great ability when trained on large pixel-wise annotated image datasets. It remains a major challenge however to aggregate such datasets, as the time and monetary cost associated with collecting extensive annotations is extremely high. This is particularly the case for generating precise pixel-wise annotations in video and volumetric image data. To this end, this work presents a novel framework to produce pixel-wise segmentations using minimal supervision. Our method relies on 2D point supervision, whereby a single 2D location within an object of interest is provided on each image of the data. Our method then estimates the object appearance in a semi-supervised fashion by learning object-image-specific features and by using these in a semi-supervised learning framework. Our object model is then used in a graph-based optimization problem that takes into account all provided locations and the image data in order to infer the complete pixel-wise segmentation. In practice, we solve this optimally as a tracking problem using a K-shortest path approach. Both the object model and segmentation are then refined iteratively to further improve the final segmentation. We show that by collecting 2D locations using a gaze tracker, our approach can provide state-of-the-art segmentations on a range of objects and image modalities (video and 3D volumes), and that these can then be used to train supervised machine learning classifiers. Â© 2018 Elsevier B.V.",,,
10.1145/3281505.3281538,2018,"John B., Banerjee A., Raiturkar P., Jain E.",An evaluation of pupillary light response models for 2D screens and VR HMDs,"Pupil diameter changes have been shown to be indicative of user engagement and cognitive load for various tasks and environments. However, it is still not the preferred physiological measure for applied settings. This reluctance to leverage the pupil as an index of user engagement stems from the problem that in scenarios where scene brightness cannot be controlled, the pupil light response confounds the cognitive-emotional response. What if we could predict the light response of an individualâ€™s pupil, thus creating the opportunity to factor it out of the measurement? In this work, we lay the groundwork for this research by evaluating three models of pupillary light response in 2D, and in a virtual reality (VR) environment. Our results show that either a linear or an exponential model can be fit to an individual participant with an easy-to-use calibration procedure. This work opens several new research directions in VR relating to performance analysis and inspires the use of eye tracking beyond gaze as a pointer and foveated rendering. Â© 2018 Association for Computing Machinery.",,,
10.1109/ICPR.2018.8545162,2018,"Jyoti S., Dhall A.",Automatic Eye Gaze Estimation using Geometric Texture-based Networks,"Eye gaze estimation is an important problem in automatic human behavior understanding. This paper proposes a deep learning based method for inferring the eye gaze direction. The method is based on the use of ensemble of networks, which capture both the geometric and texture information. Firstly, a Deep Neural Network (DNN) is trained using the geometric features that are extracted from the facial landmark locations. Secondly, for the texture based features, three Convolutional Neural Networks (CNN) are trained i.e. For the patch around the left eye, right eye, and the combined eyes, respectively. Finally, the information from the four channels is fused with concatenation and dense layers are trained to predict the final eye gaze. The experiments are performed on the two publicly available datasets: Columbia eye gaze and TabletGaze. The extensive evaluation shows the superior performance of the proposed framework. We also evaluate the performance of the recently proposed swish activation function as compared to Rectified Linear Unit (ReLU) for eye gaze estimation. Â© 2018 IEEE.",,,
10.1109/ICPR.2018.8545635,2018,"Cao L., Gou C., Wang K., Xiong G., Wang F.-Y.",Gaze-Aided Eye Detection via Appearance Learning,"Image based eye detection and gaze estimation have a wide range of potential applications, such as medical treatment, biometrics recognition, human-computer interaction. Though a large number of researchers have attempted to solve the two problems, they still exist some challenges due to the variation in appearance and lack of annotated images. In addition, most related work perform eye detection first, followed by gaze estimation via appearance learning. In this paper, we propose a unified framework to execute the gaze estimation and the eye detection simultaneously by learning the cascade regression models from appearance around the eye related key points. Intuitively, there is coupled relationship among location of eye center, shape of eye related key points, appearance representation and gaze information. To incorporate these information, at each cascade level, we first learn a model to map the shape and appearance around current eye related key points to the three dimension gaze update. Then, with the help of estimated gaze, we further learn a regression model to map the gaze, shape and appearance information to eye location update. By leveraging the power of cascade learning, the proposed method can alternatively optimize the two tasks of eye detection and gaze estimation. The experiments are conducted on benchmarks of GI4E and MPIIGaze. Experimental results show that our proposed method can achieve preferable results in gaze estimation and outperform the state-of-the-art methods in eye detection. Â© 2018 IEEE.",,,
10.1145/3282894.3282925,2018,"Muralidhar S., Siegfried R., Gatica-Perez D., Odobez J.-M.",Facing employers and customers: What do gaze and expressions tell about soft skills?,"Eye gaze and facial expressions are central to face-to-face social interactions. These behavioral cues and their connections to first impressions have been widely studied in psychology and computing literature, but limited to a single situation. Utilizing ubiquitous multimodal sensors coupled with advances in computer vision and machine learning, we investigate the connections between these behavioral cues and perceived soft skills in two diverse workplace situations (job interviews and reception desk). Pearsonâ€™s correlation analysis shows a moderate connection between certain facial expressions, eye gaze cues and perceived soft skills in job interviews (r 2 [30, 30]) and desk (r 2 [20, 36]) situations. Results of our computational framework to infer perceived soft skills indicates a low predictive power of eye gaze, facial expressions, and their combination in both interviews (R 2 2 [0.02, 0.21]) and desk (R 2 2 [0.05, 0.15]) situations. Our work has important implications for employee training and behavioral feedback systems. Â© 2018 Association for Computing Machinery. All Rights Reserved.",,,
10.1109/ISSREW.2018.00-38,2018,"Singh M., Walia G.S., Goswami A.",Using Supervised Learning to Guide the Selection of Software Inspectors in Industry,"Software development is a multi-phase process that starts with requirement engineering. Requirements elicited from different stakeholders are documented in natural language (NL) software requirement specification (SRS) document. Due to the inherent ambiguity of NL, SRS is prone to faults (e.g., ambiguity, incorrectness, inconsistency). To find and fix faults early (where they are cheapest to find), companies routinely employ inspections, where skilled inspectors are selected to review the SRS and log faults. While other researchers have attempted to understand the factors (experience and learning styles) that can guide the selection of effective inspectors but could not report improved results. This study analyzes the reading patterns (RPs) of inspectors recorded by eye-tracking equipment and evaluates their abilities to find various fault-types. The inspectors' characteristics are selected by employing ML algorithms to find the most common RPs w.r.t each fault-types. Our results show that our approach could guide the inspector selection with an accuracy ranging between 79.3% and 94% for various fault-types. Â© 2018 IEEE.",,,
10.1016/j.neucom.2018.07.010,2018,"Yun X., Sun Y., Wang S., Shi Y., Lu N.",Multi-layer convolutional network-based visual tracking via important region selection,"The convolutional network-based tracking (CNT) algorithm provides a training network with warped target regions in the first frame instead of large auxiliary datasets, which solves the problem of convolutional neural network (CNN)-based tracking requiring very long training time and a large number of auxiliary training samples. However, the two-layer CNT uses only gray feature that causes sensitivity to appearance variations. Besides, some samples with useless information should be removed to avoid drifting problems. For these reasons, a multi-layer convolutional network-based visual tracking algorithm via important region selection (IRST) is proposed in this paper. The proposed important region selection model is built via high entropy selection and background discrimination, which enables the training samples to be informative in order to provide enough stable information and also be discriminative so as to resist distractors. The feature maps are also obtained by weighting the template filters with cluster weights. Instead of simple gray features, IRST adds the Gabor layer to explore the texture feature of the target that is effective on coping with illumination and rotation variations. Extensive experiments show that the proposed algorithm achieves superior performances in many challenging visual tracking tasks. Â© 2018 Elsevier B.V.",,,
10.1145/3284103.3284105,2018,"Cai L., Yang R., Tao Z.",A new method of evaluating signage system using mixed reality and eye tracking,"Signage system is utilized to identify the emergency exit when the accident occurs. To evaluate the design of indoor signage system, a new mobile eye tracking method, that integrated eye tracker in a mixed reality application, was proposed in mixed reality environment. A virtual scene was reconstructed from the real world using mixed reality technique. Moreover, different virtual exit signs were placed on the wall in the mixed reality environment. Accurate measurement of movement and gaze with a precise timestamp were obtained in the environment. The data was used to project 3D fixation point onto the environment. The amount of fixation point and fixation time were simultaneously counted to evaluate the signage system. To demonstrate the feasibility of the method, an experiment was conducted on the 10th floor of LiuQing building in Tsinghua University. The results showed that the new mobile eye tracking method can not only easily set up the experimental evacuation environment, but also capture quantitative fixation data, providing technical support for evaluating signage system. Â© 2018 Association for Computing Machinery.",,,
10.1145/3267851.3267853,2018,"Castillo S., Hahn P., Legde K., Cunningham D.W.",Personality analysis of embodied conversational agents,"People tend to personify machines. Giving machines the ability to actually produce social information can help improve human-machine interactions. Embodied Conversational Agents (ECAs) are virtual software agents that can process and produce speech, facial expressions, gestures and eye gaze, enabling natural, multimodal, human-machine communication. On the one hand, the field of personality psychology provides insights into how we could describe and measure the virtual personality of ECAs. On the other hand, ECAs provide a method to systematically examine how different factors affect the perception of personality. This paper shows that standardized, validated personality questionnaires can be used to evaluate ECAs psychologically, and that state of the art ECAs can manipulate their perceived personality through appearance and behavior. Â© 2018 Association for Computing Machinery.",,,
10.1145/3267851.3267881,2018,"Hahn P., Castillo S., Cunningham D.W.",Look Me in the lines: The impact of stylization on the recognition of expressions and perceived personality,"We are increasingly approaching the point where computer-based technology is truly ambient and omnipresent. People tend to personify their technical servants, including giving them human names as well as attributing personality traits and intentions to them. The more those devices advance from simple tools to intelligent assistants the more seriously we need to take this personification. That is, if the computers perform human-like tasks in collaboration with humans, and humans already tend to treat computers as human-like, it is only reasonable to give those devices a human-like appearance and conversational abilities. Therefore, one approach to design advanced human-machine interfaces relies heavily on the so-called Embodied Conversational Agents (ECAs). An ECA is a virtual software agent that can process and produce speech, facial expressions, gestures and eye-gaze and, as a result, enables natural, multimodal, human-machine communication. Decades of research in psychology and related fields have shown that the visual channel is especially important in human-human-communication, with subtle changes in both appearance and motion altering how a conversational partner is perceived. In this work, we examine the effectiveness of modifying a virtual characterâ€™s visual appearance using well-known stylization techniques in order to alter its perceived personality. We also explore the effect of these techniques on the recognizability, intensity and sincerity of the characterâ€™s displayed emotions. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3267851.3267863,2018,"Zalake M., Kapoor A., Woodward J., Lok B.",Assessing the impact of virtual humanâ€™s appearance on usersâ€?trust levels,"Virtual humans are used to facilitate interactions in sensitive contexts such as health-care. In such contexts, trust in the information source plays an important role in reception of the information. Prior work has shown that physical appearance affects trustworthiness in human-human interactions; therefore, we examined the effect of virtual humanâ€™s appearance on usersâ€?trust. We ran a between-users study with 12 adult participants, who watched a video of a virtual human with professional attire (e.g., lab coat) or with general attire (e.g., button-down shirt). We examined the duration of eye fixation on the virtual humanâ€™s face along with participantsâ€?self-reported trust levels. We found that there was no statistical difference in eye contact or trust between the two test conditions. Â© 2018 Copyright held by the owner/author(s).",,,
10.1109/UEMCON.2018.8796625,2018,"Griswold-Steiner I., Fyke Z., Ahmed M., Serwadda A.",Morph-a-Dope: Using Pupil Manipulation to Spoof Eye Movement Biometrics,"Eye Tracking Authentication - a mechanism where eye movement patterns are used to verify a user's identity - is increasingly being explored for use as a layer of security in computing systems. Despite being widely studied, there is barely any research investigating how these systems could be attacked by a determined attacker. In particular, the relationship between pupil characteristics and lighting is one that could lead to vulnerabilities in improperly secured systems.This paper presents Morph-a-Dope, an attack that leverages lighting manipulations to defeat eye tracking authentication systems that heavily rely on features derived from pupil sizes. Across 20 attacker-victim pairs, the attack increased the EER by an average of over 50% as compared to the zero-effort attack by the overall population, and as much as 500% for individual victims. Our research calls for a greater emphasis on manipulation-resistant pupil size features or system designs that otherwise avoid such vulnerabilities. Â© 2018 IEEE.",,,
10.1145/3272127.3275094,2018,"Wang X., Koch S., Holmqvist K., Alexa M.",Tracking the gaze on objects in 3d: How do people really look at the bunny?,"We provide the first large dataset of human fixations on physical 3D objects presented in varying viewing conditions and made of different materials. Our experimental setup is carefully designed to allow for accurate calibration and measurement. We estimate a mapping from the pair of pupil positions to 3D coordinates in space and register the presented shape with the eye tracking setup. By modeling the fixated positions on 3D shapes as a probability distribution, we analysis the similarities among different conditions. The resulting data indicates that salient features depend on the viewing direction. Stable features across different viewing directions seem to be connected to semantically meaningful parts. We also show that it is possible to estimate the gaze density maps from view dependent data. The dataset provides the necessary ground truth data for computational models of human perception in 3D. Â© 2018 Association for Computing Machinery.",,,
10.1145/3272127.3275075,2018,"Nagano K., Seo J., Xing J., Wei L., Li Z., Saito S., Agarwal A., Fursund J., Li H.",PaGAN: Real-time avatars using dynamic textures,"With the rising interest in personalized VR and gaming experiences comes the need to create high quality 3D avatars that are both low-cost and variegated. Due to this, building dynamic avatars from a single unconstrained input image is becoming a popular application. While previous techniques that attempt this require multiple input images or rely on transferring dynamic facial appearance from a source actor, we are able to do so using only one 2D input image without any form of transfer from a source image. We achieve this using a new conditional Generative Adversarial Network design that allows fine-scale manipulation of any facial input image into a new expression while preserving its identity. Our photoreal avatar GAN (paGAN) can also synthesize the unseen mouth interior and control the eye-gaze direction of the output, as well as produce the final image from a novel viewpoint. The method is even capable of generating fully-controllable temporally stable video sequences, despite not using temporal information during training. After training, we can use our network to produce dynamic image-based avatars that are controllable on mobile devices in real time. To do this, we compute a fixed set of output images that correspond to key blendshapes, from which we extract textures in UV space. Using a subject's expression blendshapes at run-time, we can linearly blend these key textures together to achieve the desired appearance. Furthermore, we can use the mouth interior and eye textures produced by our network to synthesize on-The-fly avatar animations for those regions. Our work produces state-ofthe-art quality image and video synthesis, and is the first to our knowledge that is able to generate a dynamically textured avatar with a mouth interior, all from a single image. Â© 2018 Association for Computing Machinery.",,,
10.1016/j.image.2018.07.009,2018,"Fang Y., Zhang X., Imamoglu N.",A novel superpixel-based saliency detection model for 360-degree images,"Effective visual attention modeling is a key factor that helps enhance the overall Quality of Experience (QoE) of VR/AR data. Although a huge number of algorithms have been developed in recent years to detect salient regions in flat-2D images, the research on 360-degree image saliency is limited. In this study, we propose a superpixel-level saliency detection model for 360-degree images by figure-ground law of Gestalt theory. First, the input image is segmented into superpixels. CIE Lab color space is then used to extract the perceptual features. We extract luminance and texture features for 360-degree images from L channel, while color features are extracted from a and b channels. We compute two components for saliency prediction by figure-ground law of Gestalt theory: feature contrast and boundary connectivity. The feature contrast is computed on superpixel level by luminance and color features. The boundary connectivity is predicted for background measure and it describes the spatial layout of image region with two image boundaries (upper and lower boundary). The final saliency map of 360-degree image is calculated by fusing feature contrast map and boundary connectivity map. Experimental results on a public eye tracking database of 360-degree images show promising performance of saliency prediction from the proposed method. Â© 2018 Elsevier B.V.",,,
10.1109/TPAMI.2017.2782819,2018,"Masse B., Ba S., Horaud R.",Tracking Gaze and Visual Focus of Attention of People Involved in Social Interaction,"The visual focus of attention (VFOA) has been recognized as a prominent conversational cue. We are interested in estimating and tracking the VFOAs associated with multi-party social interactions. We note that in this type of situations the participants either look at each other or at an object of interest; therefore their eyes are not always visible. Consequently both gaze and VFOA estimation cannot be based on eye detection and tracking. We propose a method that exploits the correlation between eye gaze and head movements. Both VFOA and gaze are modeled as latent variables in a Bayesian switching state-space model (also referred switching Kalman filter). The proposed formulation leads to a tractable learning method and to an efficient online inference procedure that simultaneously tracks gaze and visual focus. The method is tested and benchmarked using two publicly available datasets, Vernissage and LAEO, that contain typical multi-party human-robot and human-human interactions. Â© 1979-2012 IEEE.",,,
10.1109/TPAMI.2017.2737423,2018,"Kononenko D., Ganin Y., Sungatullina D., Lempitsky V.",Photorealistic Monocular Gaze Redirection Using Machine Learning,"We propose a general approach to the gaze redirection problem in images that utilizes machine learning. The idea is to learn to re-synthesize images by training on pairs of images with known disparities between gaze directions. We show that such learning-based re-synthesis can achieve convincing gaze redirection based on monocular input, and that the learned systems generalize well to people and imaging conditions unseen during training. We describe and compare three instantiations of our idea. The first system is based on efficient decision forest predictors and redirects the gaze by a fixed angle in real-time (on a single CPU), being particularly suitable for the videoconferencing gaze correction. The second system is based on a deep architecture and allows gaze redirection by a range of angles. The second system achieves higher photorealism, while being several times slower. The third system is based on real-time decision forests at test time, while using the supervision from a 'teacher' deep network during training. The third system approaches the quality of a teacher network in our experiments, and thus provides a highly realistic real-time monocular solution to the gaze correction problem. We present in-depth assessment and comparisons of the proposed systems based on quantitative measurements and a user study. Â© 1979-2012 IEEE.",,,
10.1109/GEM.2018.8516529,2018,"Lemley J., Kar A., Corcoran P.",Eye Tracking in Augmented Spaces: A Deep Learning Approach,The use of deep learning for estimating eye gaze in augmented spaces is investigated in this work. There are two primary ways of interacting with augmented spaces. The first involves the use of AR/VR systems; the second involves devices that respond to the user's gaze directly. This domain can overlap with AR/VR environments but is not exclusive to them and contains its own unique set of issues. Deep learning methods for eye tracking that are capable of performing with minimal power consumption are investigated for both problems. Â© 2018 IEEE.,,,
10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00-17,2018,"Xia Y., Lou J., Dong J., Li G., Yu H.",SDM-Based means of gradient for eye center localization,"For eye gaze estimation and eye tracking, localizing eye center is a crucial requirement. This task is challenging work because of the significant variability of eye appearance in illumination, shape, color and viewing angles. In this paper, we improve the performance of means of gradient method in low resolution images, which could locate the eye center more accurately. The proposed method applies Supervised Descent Method (SDM), which has remarkable achievement in the field of face alignment, to improve the traditional means of gradient method in localizing eye center. We extensively evaluate our method on BioID database which is very challenging and realistic for eye center localization. Moreover, we have com-pared our method with existing state of the art methods and the results of the experiment confirm that the proposed method is an attractive alternative for eye center localization. Â© 2018 IEEE.",,,
10.3390/info9110262,2018,Melo E.V.,Improving collaborative filtering-based image recommendation through use of Eye Gaze tracking,"Due to the overwhelming variety of products and services currently available on electronic commerce sites, the consumer finds it difficult to encounter products of preference. It is common that product preference be influenced by the visual appearance of the image associated with the product. In this context, Recommendation Systems for products that are associated with Images (IRS) become vitally important in aiding consumers to find those products considered as pleasing or useful. In general, these IRS use the Collaborative Filtering technique that is based on the behaviour passed on by users. One of the principal challenges found with this technique is the need for the user to supply information concerning their preference. Therefore, methods for obtaining implicit information are desirable. In this work, the author proposes an investigation to discover to which extent information concerning user visual attention can aid in producing a more precise IRS. This work proposes therefore a new approach, which combines the preferences passed on from the user, by means of ratings and visual attention data. The experimental results show that our approach exceeds that of the state of the art. Â© 2018 by the authors.",,,
10.1145/3240508.3240570,2018,"Song G., Zheng J., Cai J., Zhang J., Cham T.-J., Fuchs H.",Real-time 3D face-eye performance capture of a person wearing vr headset,"Teleconference or telepresence based on virtual reality (VR) head-mount display (HMD) device is a very interesting and promising application since HMD can provide immersive feelings for users. However, in order to facilitate face-to-face communications for HMD users, real-time 3D facial performance capture of a person wearing HMD is needed, which is a very challenging task due to the large occlusion caused by HMD. The existing limited solutions are very complex either in setting or in approach as well as lacking the performance capture of 3D eye gaze movement. In this paper, we propose a convolutional neural network (CNN) based solution for real-time 3D face-eye performance capture of HMD users without complex modification to devices. To address the issue of lacking training data, we generate massive pairs of HMD face-label dataset by data synthesis as well as collecting VR-IR eye dataset from multiple subjects. Then, we train a dense-fitting network for facial region and an eye gaze network to regress 3D eye model parameters. Extensive experimental results demonstrate that our system can efficiently and effectively produce in real time a vivid personalized 3D avatar with the correct identity, pose, expression and eye motion corresponding to the HMD user. Â© 2018 Association for Computing Machinery.",,,
10.1145/3241539.3241578,2018,"Li T., Zhou X.",Battery-free eye tracker on glasses,"This paper presents a battery-free wearable eye tracker that tracks both the 2D position and diameter of a pupil based on its light absorption property. With a few near-infrared (NIR) lights and photodiodes around the eye, NIR lights sequentially illuminate the eye from various directions while photodiodes sense spatial patterns of reflected light, which are used to infer pupil's position and diameter on the fly via a lightweight inference algorithm. The system also exploits characteristics of different eye movement stages and adjusts its sensing and computation accordingly for further energy savings. A prototype is built with off-the-shelf hardware components and integrated into a regular pair of glasses. Experiments with 22 participants showthat the system achieves 0.8-mm mean error in tracking pupil position (2.3 mm at the 95th percentile) and 0.3-mm mean error in tracking pupil diameter (0.9 mm at the 95th percentile) at 120-Hz output frame rate, consuming 395Î¼W mean power supplied by two small, thin solar cells on glasses side arms. Â© 2018 Association for Computing Machinery.",,,
10.1145/3266037.3266090,2018,"Ishibashi T., Sugano Y., Matsushita Y.",Gaze-guided Image Classification for Reflecting Perceptual Class Ambiguity,"Despite advances in machine learning and deep neural networks, there is still a huge gap between machine and human image understanding. One of the causes is the annotation process used to label training images. In most image categorization tasks, there is a fundamental ambiguity between some image categories and the underlying class probability differs from very obvious cases to ambiguous ones. However, current machine learning systems and applications usually work with discrete annotation processes and the training labels do not reflect this ambiguity. To address this issue, we propose an new image annotation framework where labeling incorporates human gaze behavior. In this framework, gaze behavior is used to predict image labeling difficulty. The image classifier is then trained with sample weights defined by the predicted difficulty. We demonstrate our approach's effectiveness on four-class image classification tasks. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3266037.3266119,2018,"Hirzle T., Gugenheimer J., Geiselhart F., Bulling A., Rukzio E.",Towards a symbiotic human-machine depth sensor: Exploring 3D gaze for object reconstruction,"Eye tracking is expected to become an integral part of future augmented reality (AR) head-mounted displays (HMDs) given that it can easily be integrated into existing hardware and provides a versatile interaction modality. To augment objects in the real world, AR HMDs require a three-dimensional understanding of the scene, which is currently solved using depth cameras. In this work we aim to explore how 3D gaze data can be used to enhance scene understanding for AR HMDs by envisioning a symbiotic human-machine depth camera, fusing depth data with 3D gaze information. We present a first proof of concept, exploring to what extend we are able to recognise what a user is looking at by plotting 3D gaze data. To measure 3D gaze, we implemented a vergence-based algorithm and built an eye tracking setup consisting of a Pupil Labs headset and an OptiTrack motion capture system, allowing us to measure 3D gaze inside a 50x50x50 cm volume. We show first 3D gaze plots of ""gazed-at"" objects and describe our vision of a symbiotic human-machine depth camera that combines a depth camera and human 3D gaze information. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3267305.3267675,2018,"Herurkar D., Dengel A., Ishimaru S.",Poster: Combining software-based eye tracking and a wide-angle lens for sneaking detection,"This paper proposes Sneaking Detector, a system which recognizes sneaking on a laptop screen by other people and alerts the owner through several interventions. We utilize a pre-trained deep learning network to estimate eye gaze of sneakers captured by a front-facing camera. Since most of the cameras equipped on laptop computers cannot cover a wide enough range, a commercial wide-angle lens attachment and an image processing are applied in our system. On the dataset involving nine participants following four experiments, it has been realized that our system can estimate the horizontal eye gaze and recognizes whether a sneaker is looking at a screen or not with 78% accuracy. Â© Copyright held by the owner/author(s).",,,
10.1109/ICCI-CC.2018.8482037,2018,"Hosseinkhani J., Joslin C.",Significance of Bottom-Up Attributes in Video Saliency Detection without Cognitive Bias,"Saliency in an image or video is the region of interest that stands out relative to its neighbors and consequently attracts more human attention. To determine the salient areas within a scene, visual importance and distinctiveness of the regions must be measured. A key factor in designing saliency detection algorithms for videos is to understand how different visual cues affect the human perceptual and visual system. To this end, we investigated the bottom-up features including color, texture, and motion in video sequences for both one-by-one and combined scenarios to provide a ranking system stating the most dominant circumstances for each feature individually and in combination with other features as well. In this work, we only considered the individual features and various visual saliency attributes investigated under conditions in which we had no cognitive bias. Human cognition refers to a systematic pattern of perceptual and rational judgements and decision-making actions. Since computers do not typically have this ability, we tried to minimize this bias in the design of our experiment. First, we modelled our test data as 2D images and videos in a virtual environment to avoid any cognitive bias. Then, we performed an experiment using human subjects to determine which colors, textures, motion directions, and motion speeds attract human attention more. The proposed ranking system of salient visual attention stimuli was achieved using an eye tracking procedure. This work provides a benchmark to specify the most salient stimulus with comprehensive information. Â© 2018 IEEE.",,,
10.1145/3242969.3243011,2018,"Guo W., Wang J.",Understanding mobile reading via camera based gaze tracking and kinematic touch modeling,"Despite the ubiquity and rapid growth of mobile reading activities, researchers and practitioners today either rely on coarse-grained metrics such as click-through-rate (CTR) and dwell time, or expensive equipment such as gaze trackers to understand users' reading behavior on mobile devices. We present Lepton, an intelligent mobile reading system and a set of dual-channel sensing algorithms to achieve scalable and fine-grained understanding of users' reading behaviors, comprehension, and engagement on unmodified smartphones. Lepton tracks the periodic lateral patterns, i.e. saccade, of users' eye gaze via the front camera, and infers their muscle stiffness during text scrolling via a Mass-Spring-Damper (MSD) based kinematic model from touch events. Through a 25-participant study, we found that both the periodic saccade patterns and muscle stiffness signals captured by Lepton can be used as expressive features to infer users' comprehension and engagement in mobile reading. Overall, our new signals lead to significantly higher performances in predicting users' comprehension (correlation: 0.36 vs. 0.29), concentration (0.36 vs. 0.16), confidence (0.5 vs. 0.47), and engagement (0.34 vs. 0.16) than using traditional dwell-time based features via a user-independent model. Â© 2018 Association for Computing Machinery.",,,
10.1145/3264877.3264885,2018,"Li T., Zhou X.",Battery-free eye tracker on glasses,"We propose the design of a battery-free wearable eye tracker that achieves sub-millimeter tracking accuracy at high track ing rates. It tracks pupilâ€™s 2D position based on pupilâ€™s light absorption effect. With a few near-infrared (NIR) lights and photodiodes around the eye, NIR lights sequentially illu minate the eye from various directions while photodiodes sense spatial patterns of reflected light, which are used to infer pupil positions on the fly through a lightweight infer ence algorithm. The system also exploits characteristics of different eye movement stages and adjusts its sensing and computation accordingly for further energy savings. We have built a prototype with off-the-shelf hardware components and integrated it into a regular pair of glasses. Experiments with ten participants show that the system achieves 0.9-mm mean tracking accuracy (2.4 mm at the 95th percentile) at 120-Hz output frame rate, consuming 395ÂµW mean power supplied by two small, thin solar cells on glasses side arms. Â© 2018 Association for Computing Machinery.",,,
10.1007/s11042-018-5837-4,2018,"Banitalebi-Dehkordi A., Nasiopoulos P.",Saliency inspired quality assessment of stereoscopic 3D video,"To study the visual attentional behavior of Human Visual System (HVS) on 3D content, eye tracking experiments are performed and Visual Attention Models (VAMs) are designed. One of the main applications of these VAMs is in quality assessment of 3D video. The usage of 2D VAMs in designing 2D quality metrics is already well explored. This paper investigates the added value of incorporating 3D VAMs into Full-Reference (FR) and No-Reference (NR) quality assessment metrics for stereoscopic 3D video. To this end, state-of-the-art 3D VAMs are integrated to quality assessment pipeline of various existing FR and NR stereoscopic video quality metrics. Performance evaluations using a large scale database of stereoscopic videos with various types of distortions demonstrated that using saliency maps generally improves the performance of the quality assessment task for stereoscopic video. However, depending on the type of distortion, utilized metric, and VAM, the amount of improvement will change. Â© 2018, Springer Science+Business Media, LLC, part of Springer Nature.",,,
10.1109/TIP.2018.2848465,2018,"Nai K., Li Z., Li G., Wang S.",Robust Object Tracking via Local Sparse Appearance Model,"In this paper, we propose a novel local sparse representation-based tracking framework for visual tracking. To deeply mine the appearance characteristics of different local patches, the proposed method divides all local patches of a candidate target into three categories, which are stable patches, valid patches, and invalid patches. All these patches are assigned different weights to consider the different importance of the local patches. For stable patches, we introduce a local sparse score to identify them, and discriminative local sparse coding is developed to decrease the weights of background patches among the stable patches. For valid patches and invalid patches, we adopt local linear regression to distinguish the former from the latter. Furthermore, we propose a weight shrinkage method to determine weights for different valid patches to make our patch weight computation more reasonable. Experimental results on public tracking benchmarks with challenging sequences demonstrate that the proposed method performs favorably against other state-of-the-art tracking methods. Â© 2018 IEEE.",,,
10.1007/s11548-018-1808-5,2018,"Wyder S., Cattin P.C.",Eye tracker accuracy: quantitative evaluation of the invisible eye center location,"Purpose: We present a new method to evaluate the accuracy of an eye tracker-based eye localization system. Measuring the accuracy of an eye trackerâ€™s primary intention, the estimated point of gaze, is usually done with volunteers and a set of fixation points used as ground truth. However, verifying the accuracy of the location estimate of a volunteerâ€™s eye center in 3D space is not easily possible. This is because the eye center, the center of corneal curvature, is an intangible point. Methods: We evaluate the eye location accuracy by using an eye phantom instead of eyes of volunteers. For this, we developed a testing stage with a realistic artificial eye and a corresponding kinematic model, which we trained with Î¼CT data. This enables us to precisely evaluate the eye location estimate of an eye tracker. Results: We show that the proposed testing stage with the corresponding kinematic model is suitable for such a validation. Further, we evaluate a particular eye tracker-based navigation system and show that this system is able to successfully determine the eye center with a mean accuracy of 0.68 mm. Conclusion: We show the suitability of the evaluated eye tracker for eye interventions, using the proposed testing stage and the corresponding kinematic model. The results further enable specific enhancements of the navigation system to potentially get even better results. Â© 2018, CARS.",,,
10.1007/s11042-018-5787-x,2018,"Xiao F., Huang K., Qiu Y., Shen H.","Accurate iris center localization method using facial landmark, snakuscule, circle fitting and binary connected component","Iris centers have been widely used in machine vision for face matching, gaze estimation, etc. However, in low resolution eye images, the iris and its surrounding region present a variety of appearance characteristics, which make it difficult to accurately locate the iris center. In this paper, we propose a robust, accurate and real-time iris center localization method by combining the facial landmark, snakuscule, circle fitting and binary connected component. Facial landmarks are used to extract an accurate eye Region of Interest (ROI). Thereafter, a fixed size circle-based active contour snakuscule is used to detect the iris center. Based on the snakuscule center and inner radius, a novel method is proposed to extract accurate iris edges for circle fitting. In addition, the quality of the detected iris center is evaluated by a circle-binary quality evaluation method. Binary connected component method is used to improve the accuracies in those unqualified images. The proposed method is tested on three publicly available databases BioID, GI4E and Talking Face Video. The result shows that it could achieve an accuracy of 94.35% on the BioID database when the normalized error is smaller than 0.05, which outperforms all state-of-the-art methods. Â© 2018, Springer Science+Business Media, LLC, part of Springer Nature.",,,
10.3390/app8101769,2018,"Wan Z., Wang X., Yin L., Zhou K.",A method of free-space point-of-regard estimation based on 3D eye model and stereo vision,"This paper proposes a 3D point-of-regard estimation method based on 3D eye model and a corresponding head-mounted gaze tracking device. Firstly, a head-mounted gaze tracking system is given. The gaze tracking device uses two pairs of stereo cameras to capture the left and right eye images, respectively, and then sets a pair of scene cameras to capture the scene images. Secondly, a 3D eye model and the calibration process are established. Common eye features are used to estimate the eye model parameters. Thirdly, a 3D point-of-regard estimation algorithm is proposed. Three main parts of this method are summarized as follows: (1) the spatial coordinates of the eye features are directly calculated by using stereo cameras; (2) the pupil center normal is used to the initial value for the estimation of optical axis; (3) a pair of scene cameras are used to solve the actual position of the objects being watched in the calibration process, and the calibration for the proposed eye model does not need the assistance of the light source. Experimental results show that the proposed method can output the coordinates of 3D point-of-regard more accurately. Â© 2018 by the authors.",,,
10.1145/3278681.3278706,2018,"Beelders T.R., Du Plessis J.-P.L.",Reading usability of ereaders and ebooks for Information technology students,"Electronic reading devices and books present a convenient, cheap solution to university students who are required to buy expensive textbooks. However, these devices may not be suitable for reading academic texts, particularly in specialized fields such as Information Technology. This study investigated whether reading IT academic texts are influenced by the presentation medium, namely a Kindle eReader, an iPad, an Android tablet, a PC and paper. Eye gaze is an established means of detecting whether reading difficulty is being experienced. Reading speed on the text portions was not significantly different between devices. However, reading speed on the iPad was significantly faster than on the Kindle. paper and PC for the entire text. No significant difference was found in fixation durations when reading code, but there was a significant difference when reading text only, where, fixations were, on average, significantly longer on the PC than the other devices. When reading code, the PC had significantly fewer fixations and visits but visits were longer when reading text. The tablet, had significantly more fixations and longer visits than the iPad, Kindle and paper and the Kindle had significantly fewer fixations than the paper. Reactions to all the devices were very positive in terms of ease of use, readability and appearance. The appearance was also positively experienced and navigation was found to be easy. Â© 2018 Association for Computing Machinery.",,,
10.1016/j.neucom.2018.05.007,2018,"Feng P., Xu C., Zhao Z., Liu F., Guo J., Yuan C., Wang T., Duan K.",A deep features based generative model for visual tracking,"In this work, we propose a novel visual tracking algorithm based on a framework of generative model. In order to make the algorithm robust to various challenging appearance changes, we adopt the powerful deep features in the description of tracking object appearance. The features are extracted from a Convolutional Neural Network (CNN), which is a modified one based on the VGG-M nets but constructed with fewer convolution layers and sequences exclusively full connection layers. In the pretraining process, we add a special convolution layer called coefficients layer before the full connection layers. In the tracking process after the network being pretrained, we remove the coefficients layer and just update the full connection layers conditionally. To decide the new target's positions, we compute the compositive similarity scores containing three kinds of similarities with different weights. The first kind is similarities between candidates and the target in the first frame, and the second kind is between candidates and tracking results in the last frame. The third kind is related to the important object appearance variations in the tracking process. We design a simple mechanism to produce a collection to record those historical templates when the object appearance changed largely. With similarities between candidates and the historical templates, the drift problem can be alleviated to some extent, because similar historical appearances sometimes appear repeatedly and the recorded historical templates can provide important information. We use the outputs of the convolution part before the full connection layers as features and weight them with the coefficients layer's filter weights to compute all similarities. Finally, candidates with the highest scores will be regarded as new targets in the current frame. The evaluated results on CVPR2013 Online Object Tracking Benchmark show that our algorithm can achieve outstanding performance compared with state-of-the-art trackers. Â© 2018 Elsevier B.V.",,,
10.1016/j.neucom.2018.04.019,2018,"Wu L., Xu M., Zhu G., Wang J., Rao T.",Appearance features in Encoding Color Space for visual surveillance,"Person re-identification and visual tracking are two important tasks in video surveillance. Many works have been done on appearance modeling for these two tasks. However, existing feature descriptors are mainly constructed on three-channel color spaces, such like RGB, HSV and XYZ. These color spaces somehow enable meaningful representation for color, yet may lack distinctiveness for real-world tasks. In this paper, we propose a multi-channel Encoding Color Space (ECS), and consider the color distinction with the design of image feature descriptor. In order to overcome the illumination variation and shape deformation, we design features on the basis of the Encoding Color Space and Histogram of Oriented Gradient (HOG), which enables rich color-gradient characteristics. Additionally, we extract Second Order Histogram (SOH) on the descriptor constructed to capture abstract information with layout constrains. Exhaustive experiments are performed on datasets VIPeR, CAVIAR, CUHK01 and Visual Tracking Benchmark. Experimental results on these datasets show that our feature descriptors could achieve promising performance. Â© 2018 Elsevier B.V.",,,
10.5194/isprs-archives-XLII-4-149-2018,2018,"DÃ­az-VilariÃ±o L., GonzÃ¡lez-De Santos L., Verbree E., Michailidou G., Zlatanova S.",From point clouds to 3D isovists in indoor environments,"Visibility is a common measure to describe the spatial properties of an environment related to the spatial behaviour. Isovists represent the space that can be seen from one observation point, and they are used to analyse the existence of obstacles affecting or blocking intervisibility in an area. Although point clouds depict the as-built reality in a very detailed and accurate way, literature addressing the analysis of visibility in 3D, and more specifically the usage of point clouds to visibility analysis, is rather limited. In this paper, a methodology to evaluate visibility from point clouds in indoor environments is proposed, resulting in the creation of 3D isovists. Point cloud is firstly discretized in a voxel-based structure and voxels are labelled into â€˜exteriorâ€? â€˜occupiedâ€? â€˜visibleâ€?and â€˜occludedâ€?based on an occupancy followed by a visibility analysis performed from a ray-tracing algorithm. 3D Isovists are created from the boundary of visible voxels from an observer position and considering as input parameters the visual angle, maximum line of sight, and eye gaze direction. Â© Authors 2018.",,,
10.1145/3274005.3274029,2018,"Cantoni V., Lacovara T., Porta M., Wang H.",A study on gaze-controlled pin input with biometric data analysis,"Common methods for checking a user's identity (e.g., passwords) do not consider personal elements characterizing a subject. In this paper, we present a study on the exploitation of eye information for biometric purposes. Data is acquired when the user enters a PIN (Personal Identification Number) through the gaze, by means of an on-screen virtual numeric keypad. Both identification (i.e., the recognition of a subject in a group) and verification (i.e., the confirmation of an individual's claimed identity) are considered. Using machine learning algorithms, we performed two kinds of analysis, one for the entire PIN sequence and one for each key (i.e., digit) in the series. Overall, the achieved results can be considered satisfying in the context of â€œsoft biometricsâ€? which does not require very high success rates and is meant to be used along with other identification or verification techniques-in our case, the PIN itself-as an additional security level. Â© 2018 Association for Computing Machinery. ACM",,,
10.1109/CSCWD.2018.8465363,2018,"Hong S., Cheng H., Mao B.",Visual Saliency Detection Framework for 3D Environment using Virtual Reality Devices,"Visual saliency is essential for attention analysis and it is usually measured by eye tracking system, which is widely used in the fields of design evaluation, user's gaze and behavior analysis. This study combines virtual reality with eye tracking technology and applies HTC Vive VR helmet to simulate eye tracker for both two-dimensional and three-dimensional cases study. We collect user's gaze points data and draw the scatter diagram and the thermodynamic map to analyze the user's visual saliency in completely 3D environment. The quantitative error of user's visual saliency data in virtual reality environment is analyzed by setting the average offset distance and offset degree indexes. The experiment results indicate that the proposed framework can be used to detect the visual saliency in 3D environment accurately that is has practical significance to the application of user's visual saliency analysis in the virtual reality environment. Â© 2018 IEEE.",,,
10.1109/ICRA.2018.8460800,2018,"Geisler D., Fox D., Kasneci E.",Real-time 3D Glint Detection in Remote Eye Tracking Based on Bayesian Inference,"As human gaze provides information on our cognitive states, actions, and intentions, gaze-based interaction has the potential to enable a fluent and natural human-robot collaboration. In this work, we focus on reliable gaze estimation in remote eye tracking based on calibration-free methods. Although these methods work well in controlled settings, they fail when illumination conditions change or other objects induce noise. We propose a novel, adaptive method based on a probabilistic model, which reliably detects glints from stereo images and evaluate our method using a data set that contains different challenges with regarding to light and reflections. Â© 2018 IEEE.",,,
10.1109/ICDIM.2018.8846967,2018,"Carette R., Elbattah M., Dequen G., Guerin J.-L., Cilia F.",Visualization of eye-tracking patterns in autism spectrum disorder: Method and dataset,"Autism spectrum disorder (ASD) is a lifelong condition generally characterized by social and communication impairments. One of the characteristic hallmarks of ASD is the difficulty of making or maintaining eye contact. In this respect, the eye-tracking technology has come into prominence to support the study and analysis of autism. This paper develops a methodology to visualize the eye-tracking patterns of ASD-diagnosed individuals with particular focus on children at early stages of development. The key idea is to transform the dynamics of eye motion into a visual representation, and hence diagnosis-related tasks could be approached using image-based techniques. The visualizations produced are made publicly available in an image dataset to be used by other studies aiming to experiment the potentials of eye-tracking within the ASD context. It is believed that the dataset can allow for developing further useful applications or discovering interesting insights using Machine Learning or data mining techniques. Â© 2018 IEEE.",,,
10.1007/s41095-018-0113-0,2018,"Koskela M.K., Immonen K.V., Viitanen T.T., JÃ¤Ã¤skelÃ¤inen P.O., Multanen J.I., Takala J.H.",Instantaneous foveated preview for progressive Monte Carlo rendering,"Progressive rendering, for example Monte Carlo rendering of 360Â° content for virtual reality headsets, is a time-consuming task. If the 3D artist notices an error while previewing the rendering, they must return to editing mode, make the required changes, and restart rendering. We propose the use of eye-tracking-based optimization to significantly speed up previewing of the artistâ€™s points of interest. The speed of the preview is further improved by sampling with a distribution that closely follows the experimentally measured visual acuity of the human eye, unlike the piecewise linear models used in previous work. In a comprehensive user study, the perceived convergence of our proposed method was 10 times faster than that of a conventional preview, and often appeared to be instantaneous. In addition, the participants rated the method to have only marginally more artifacts in areas where it had to start rendering from scratch, compared to conventional rendering methods that had already generated image content in those areas. Â© 2018, The Author(s).",,,
10.1109/AIM.2018.8452320,2018,"Hoonkwon K., Oh H.M., Kim M.Y.",Multiple RGB-D camera-based user intent position and object estimation,"Human gaze represents the area of interests of the person. By analyzing a time-series of these areas, it is possible to obtain user behavioral pattern that can be used in various fields. Well-known techniques for estimating human gaze are inconvenient because they require a wearable device, or the measurement area is relatively narrow. In this paper, a method to implement gaze estimation system using 3D view tracking with multi RGB-D camera is proposed. Surround 3D cameras are used to extract the region of interest of the user from 3D gaze estimation without wearable device in living space. To implement proposed method, first, 3D space mapping through multiple RGB-D camera calibration is performed. The resulting 3D map is the measurement area, which depends on the number and specifications of the RGB-D cameras used for this purpose. Then, when a person enters the 3D map, the face region is detected using both 2D/3D data, and 3D view tracking is implemented by detecting the gaze vector using the facial feature point and the head data center point extracted from the 3D map. Finally, when the gaze vector line intersects a specific point within the mapping space, the image coordinates corresponding to that point are extracted to implement user Intent position estimation. Applying object detection and classification algorithm to the extracted image can also estimate the intent object at that time. Â© 2018 IEEE.",,,
10.1109/VR.2018.8447552,2018,"Ortega M., Stuerzlinger W.",Pointing at Wiggle 3D Displays,"This paper presents two new pointing techniques for wiggle 3D displays, which present the 2D projection of 3D content with automatic (rotatory) motion parallax. Standard pointing at targets in wiggle 3D displays is challenging as the content is constantly in motion. The two pointing techniques presented here take advantage of the cursor's current position or the user's gaze direction for collocating the wiggle rotation center and potential targets. We evaluate the performance of the pointing techniques with a novel methodology that integrates 3D distractors into the ISO-9241-9 standard task. The experimental results indicate that the new techniques are significantly more efficient than standard pointing techniques in wiggle 3D displays. Given that we observed no performance variation for different targets, our new techniques seem to negate any interaction performance penalties of wiggle 3D displays. Â© 2018 IEEE.",,,
10.1109/VR.2018.8446494,2018,"Chen S.-Y., Gao L., Lai Y.-K., Rosin P.L., Xia S.",Real-Time 3D Face Reconstruction and Gaze Tracking for Virtual Reality,"With the rapid development of virtual reality (VR) technology, VR glasses, a.k.a. Head-Mounted Displays (HMDs) are widely available, allowing immersive 3D content to be viewed. A natural need for truly immersive VR is to allow bidirectional communication: The user should be able to interact with the virtual world using facial expressions and eye gaze, in addition to traditional means of interaction. Typical application scenarios include VR virtual conferencing and virtual roaming, where ideally users are able to see other users' expressions and have eye contact with them in the virtual world. Despite significant achievements in recent years for reconstruction of 3D faces from RGB or RGB-D images, it remains a challenge to reliably capture and reconstruct 3D facial expressions including eye gaze when the user is wearing VR glasses, because the majority of the face is occluded, especially those areas around the eyes which are essential for recognizing facial expressions and eye gaze. In this paper, we introduce a novel real-Time system that is able to capture and reconstruct 3D faces wearing HMDs and robustly recover eye gaze. We demonstrate the effectiveness of our system using live capture and more results are shown in the accompanying video. Â© 2018 IEEE.",,,
10.1109/VR.2018.8446242,2018,"Mei C., Zahed B.T., Mason L., Ouarles J.",Towards Joint Attention Training for Children with ASD-a VR Game Approach and Eye Gaze Exploration,"Joint attention is critical to the education and development of a child. Deficits in joint attention are considered by many researchers to be an early predictor of children with Autism Spectrum Disorder (ASD). Training of joint attention have been a significant topic in ASD intervention education research. We propose a novel joint attention training approach using a Customizable Virtual Human (CVH) and a Virtual Reality (VR) game to assist with joint attention training. Previous work has shown that CVHs potentially help the users with ASD to increase their performance in hand-eye coordination, motivate the users to play longer, as well as improve user experience in a training game. Based upon these discovered CVH benefits, we hypothesize that CVHs may also be beneficial in training joint attention for users with ASD. To test our hypothesis, we developed a CVH with customizable facial features in an educational game-Imagination Drums-and conducted a user study on adolescents with high functioning ASD to investigate the effects of CVHs. We collected users' eye-gaze data and task performance during the game to evaluate the users' joint attention with CVHs and the effectiveness of CVHs compared with Non-Customizable Virtual Humans (NCVHs). The study results showed that the CVH make the participants gaze less at the irrelevant area of the game's storyline (i.e. background), but surprisingly, also provided evidence that participants react slower to the CVH's joint attention bids, compared with NCVH. Overall, the study reveals insights of how users with ASD interact with CVHs and how these interactions affect joint attention. Â© 2018 IEEE.",,,
10.1145/3225153.3225157,2018,"Pfeil K., Taranta E.M., II, Kulshreshth A., Wisniewski P., LaViola J.J., Jr.",A comparison of eye-head coordination between virtual and physical realities,"Past research has shown that humans exhibit certain eye-head responses to the appearance of visual stimuli, and these natural reactions change during different activities. Our work builds upon these past observations by offering new insight to how humans behave in Virtual Reality (VR) compared to Physical Reality (PR). Using eye- and head- tracking technology, and by conducting a study on two groups of users - participants in VR or PR - we identify how often these natural responses are observed in both environments. We find that users statistically move their heads more often when viewing stimuli in VR than in PR, and VR users also move their heads more in the presence of text. We open a discussion for identifying the HWD factors that cause this difference, as this may not only affect predictive models using eye movements as features, but also VR user experience overall. Â© 2018 Association for Computing Machinery.",,,
10.1109/METROI4.2018.8428330,2018,"Ivorra E., Ortega M., Alcaniz M., Garcia-Aracil N.",Multimodal Computer Vision Framework for Human Assistive Robotics,"This paper presents a multimodal computer vision framework for human assistive robotics with the purpose of giving accessibility to persons with disabilities. The user is capable of interacting with the system just by staring. Specifically, it is possible to select the desired object as well as to indicate the intention to grasp it just by staring at it. This gaze information is provided by Â©Tobii Glasses 2 that in combination with a deep learning algorithm gives the class id of the desirable object. Later, the object's pose is estimated using a RGB-D camera with a new developed technique. This technique mixes a template based algorithm with a deep learning algorithm giving a precise, realtime method for pose estimation. Once the pose is obtained, it is transformed to a grasping position in the coordinate system of the assistive robot that performs the grasping operation. Â© 2018 IEEE.",,,
10.1109/ICGCIoT.2018.8753026,2018,"Kulkarni S.V., Sangeeta K.",Techniques for visual analysis of eye tracking data,"Eye tracking is the process of estimating as well as recording gaze positions and eye movements of an individual. Eye tracking technology has many statistical factors which are significant in generating knowledge and values. In most of the approaches an insight is presented with the help of traditional attention maps as well as gaze plots. There is no any single visualization type for all possible requirements. The appropriate choice of a visualization method depends on the format of the data, analysis task specific to the requirements. The objective of this work is to visualize eye tracking data using various visualization especially 3D visuals and animation of eye gazes. These implementations have respective benefits over the other methods of eye tracking visualizations and can be used to generate more knowledge and value extraction from eye tracking metrics. Â© 2018 IEEE.",,,
10.1109/AMS.2017.21,2018,"Al Arabi A., Tipu R.S., Bashar M.R., Barman B., Monica S.A., Amin M.A.",Implementation of Low Cost Stereo Humanoid Adaptive Vision for 3D Positioning and Distance Measurement for Robotics Application with Self-Calibration,"Robots are getting smarter everyday with the implementation of computer vision system in it. It is now highly required for any robot to have a natural vision system or more likely humanoid vision system to interact with real life incidents. On the perspective of such imaging and vision, we propose an efficient method in order to determine the absolute view point of any desired image location. We used self calibration system and humanoid vision mechanism via stereo cameras to find the region of convergent of an object which with the help of a mathematical model can measure the distance of the object. With comparing different objects position it is also possible to determine the relative distance of the objects. Our system shows that, the real human eye tracking system used, can be possible for getting a realistic view of the image at the 3D point positioning. Â© 2017 IEEE.",,,
10.3390/s18082408,2018,"Ivorra E., Ortega M., CatalÃ¡n J.M., Ezquerro S., LledÃ³ L.D., Garcia-Aracil N., AlcaÃ±iz M.",Intelligent multimodal framework for human assistive robotics based on computer vision algorithms,"Assistive technologies help all persons with disabilities to improve their accessibility in all aspects of their life. The AIDE European project contributes to the improvement of current assistive technologies by developing and testing a modular and adaptive multimodal interface customizable to the individual needs of people with disabilities. This paper describes the computer vision algorithms part of the multimodal interface developed inside the AIDE European project. The main contribution of this computer vision part is the integration with the robotic system and with the other sensory systems (electrooculography (EOG) and electroencephalography (EEG)). The technical achievements solved herein are the algorithm for the selection of objects using the gaze, and especially the state-of-the-art algorithm for the efficient detection and pose estimation of textureless objects. These algorithms were tested in real conditions, and were thoroughly evaluated both qualitatively and quantitatively. The experimental results of the object selection algorithm were excellent (object selection over 90%) in less than 12 s. The detection and pose estimation algorithms evaluated using the LINEMOD database were similar to the state-of-the-art method, and were the most computationally efficient. Â© 2018 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.1016/j.ijhcs.2018.04.005,2018,"Mollahosseini A., Abdollahi H., Sweeny T.D., Cole R., Mahoor M.H.",Role of embodiment and presence in human perception of robotsâ€?facial cues,"Both robotic and virtual agents could one day be equipped with social abilities necessary for effective and natural interaction with human beings. Although virtual agents are relatively inexpensive and flexible, they lack the physical embodiment present in robotic agents. Surprisingly, the role of embodiment and physical presence for enriching human-robot-interaction is still unclear. This paper explores how these unique features of robotic agents influence three major elements of human-robot face-to-face communication, namely the perception of visual speech, facial expression, and eye-gaze. We used a quantitative approach to disentangle the role of embodiment from the physical presence of a social robot, called Ryan, with three different agents (robot, telepresent robot, and virtual agent), as well as with an actual human. We used a robot with a retro-projected face for this study, since the same animation from a virtual agent could be projected to this robotic face, thus allowing comparison of the virtual agent's animation behaviors with both telepresent and the physically present robotic agents. The results of our studies indicate that the eye gaze and certain facial expressions are perceived more accurately when the embodied agent is physically present than when it is displayed on a 2D screen either as a telepresent or a virtual agent. Conversely, we find no evidence that either the embodiment or the presence of the robot improves the perception of visual speech, regardless of syntactic or semantic cues. Comparison of our findings with previous studies also indicates that the role of embodiment and presence should not be generalized without considering the limitations of the embodied agents. Â© 2018 Elsevier Ltd",,,
10.1109/TIP.2018.2797482,2018,"Qi Y., Qin L., Zhang J., Zhang S., Huang Q., Yang M.-H.",Structure-aware local sparse coding for visual tracking,"Sparse coding has been applied to visual tracking and related vision problems with demonstrated success in recent years. Existing tracking methods based on local sparse coding sample patches from a target candidate and sparsely encode these using a dictionary consisting of patches sampled from target template images. The discriminative strength of existing methods based on local sparse coding is limited as spatial structure constraints among the template patches are not exploited. To address this problem, we propose a structure-aware local sparse coding algorithm, which encodes a target candidate using templates with both global and local sparsity constraints. For robust tracking, we show the local regions of a candidate region should be encoded only with the corresponding local regions of the target templates that are the most similar from the global view. Thus, a more precise and discriminative sparse representation is obtained to account for appearance changes. To alleviate the issues with tracking drifts, we design an effective template update scheme. Extensive experiments on challenging image sequences demonstrate the effectiveness of the proposed algorithm against numerous state-of-the-art methods. Â© 1992-2012 IEEE.",,,
10.1109/IIAI-AAI.2018.00112,2018,"Kamimura R., Takeuchi H.",Autoeconder-Based Excessive Information Generation for Improving and Interpreting Multi-layered Neural Networks,"The present paper aims to propose a new type of learning method to increase information content in input patterns with multiple steps to be used in supervised learning. Unsupervised pre-training to train multi-layered neural networks turned out to be not so effective as has been expected, because connection weights obtained by the unsupervised learning tend to lose their original characteristics immediately in supervised training. To keep original information by unsupervised learning, we here try to increase information in input patterns as much as possible to overcome the vanishing information problem. In particular, for acquiring detailed information more appropriately, we gradually increases detailed information through multiple steps. We applied the method to the actual real data set of the eye-tracking, and two step information augmentation approach was taken. The results confirmed that generalization performance could be improved. In addition, we could interpret the importance of input variables more easily by treating all connection weights collectively. Â© 2018 IEEE.",,,
10.1145/3213586.3225234,2018,"Villamor M., Rodrigo M.M.",Predicting successful collaboration in a pair programming eye tracking experiment,"The context of collaboration is of great importance. Attempts have been made to objectively define what comprises a successful collaboration. Questions like ""When can we say that a collaboration is successful?"" or ""Is there a way to predict that a collaboration would be successful?"" have been asked. In this paper, we look at the output of the collaboration, which are the debugging scores of the pairs, and we consider a collaboration to be successful if it leads to good debugging scores. We choose pair programming because it is an example of a collaboration paradigm. In order to find out what are the potential factors that could possibly predict success in the context of a pair program tracing and debugging task, we performed a dual eye tracking experiment on pairs of novice programmers. We tracked and recorded their fixation sequences and analyzed them using Cross-Recurrence Quantification Analysis (CRQA). Two machine learning algorithms were used, such as Naive Bayes and Logistic Regression. Our findings reveal that CRQA results alone are inadequate to come up with a model with an acceptable performance. Hence, we added the pairs' proficiency level to the model. Between the two models, the Logistic Regression model turned out to be the better model. However, the performance is still not quite unacceptable to predict success so other features are needed to enhance the model. Â© 2018 Association for Computing Machinery.",,,
10.1145/3185524,2018,"Smith J., Legg P., Matovic M., Kinsey K.",Predicting user confidence during visual decision making,"People are not infallible consistent â€œoraclesâ€? their confidence in decision-making may vary significantly between tasks and over time. We have previously reported the benefits of using an interface and algorithms that explicitly captured and exploited usersâ€?confidence: error rates were reduced by up to 50% for an industrial multi-class learning problem; and the number of interactions required in a design-optimisation context was reduced by 33%. Having access to usersâ€?confidence judgements could significantly benefit intelligent interactive systems in industry, in areas such as intelligent tutoring systems and in health care. There are many reasons for wanting to capture information about confidence implicitly. Some are ergonomic, but others are more â€œsocialâ€â€”such as wishing to understand (and possibly take account of) usersâ€?cognitive state without interrupting them. We investigate the hypothesis that usersâ€?confidence can be accurately predicted from measurements of their behaviour. Eye-tracking systems were used to capture usersâ€?gaze patterns as they undertook a series of visual decision tasks, after each of which they reported their confidence on a 5-point Likert scale. Subsequently, predictive models were built using â€œconventionalâ€?machine learning approaches for numerical summary features derived from usersâ€?behaviour. We also investigate the extent to which the deep learning paradigm can reduce the need to design features specific to each application by creating â€œgaze mapsâ€â€”visual representations of the trajectories and durations of usersâ€?gaze fixationsâ€”and then training deep convolutional networks on these images. Treating the prediction of user confidence as a two-class problem (confident/not confident), we attained classification accuracy of 88% for the scenario of new users on known tasks, and 87% for known users on new tasks. Considering the confidence as an ordinal variable, we produced regression models with a mean absolute error of â‰?.7 in both cases. Capturing just a simple subset of non-task-specific numerical features gave slightly worse, but still quite high accuracy (e.g., MAE â‰?1.0). Results obtained with gaze maps and convolutional networks are competitive, despite not having access to longer-term information about users and tasks, which was vital for the â€œsummaryâ€?feature sets. This suggests that the gaze-map-based approach forms a viable, transferable alternative to handcrafting features for each different application. These results provide significant evidence to confirm our hypothesis, and offer a way of substantially improving many interactive artificial intelligence applications via the addition of cheap non-intrusive hardware and computationally cheap prediction algorithms. Â© 2018 ACM",,,
10.1145/3182644,2018,"Thies J., ZollhÃ¶fer M., Stamminger M., Theobalt C., Niebner M.",FaceVR: Real-time gaze-aware facial reenactment in virtual reality,"We propose FaceVR, a novel image-based method that enables video teleconferencing in VR based on self-reenactment. State-of-the-art face tracking methods in the VR context are focused on the animation of rigged 3D avatars (Li et al. 2015; Olszewski et al. 2016). Although they achieve good tracking performance, the results look cartoonish and not real. In contrast to these model-based approaches, FaceVR enables VR teleconferencing using an image-based technique that results in nearly photo-realistic outputs. The key component of FaceVR is a robust algorithm to perform realtime facial motion capture of an actor who is wearing a head-mounted display (HMD), as well as a new data-driven approach for eye tracking from monocular videos. Based on reenactment of a prerecorded stereo video of the person without the HMD, FaceVR incorporates photo-realistic re-rendering in real time, thus allowing artificial modifications of face and eye appearances. For instance, we can alter facial expressions or change gaze directions in the prerecorded target video. In a live setup, we apply these newly introduced algorithmic components. Â© 2018 ACM.",,,
10.1007/s00138-018-0940-0,2018,"Martinikorena I., Cabeza R., Villanueva A., Urtasun I., Larumbe A.",Fast and robust ellipse detection algorithm for head-mounted eye tracking systems,"In head-mounted eye tracking systems, the correct detection of pupil position is a key factor in estimating gaze direction. However, this is a challenging issue when the videos are recorded in real-world conditions, due to the many sources of noise and artifacts that exist in these scenarios, such as rapid changes in illumination, reflections, occlusions and an elliptical appearance of the pupil. Thus, it is an indispensable prerequisite that a pupil detection algorithm is robust in these challenging conditions. In this work, we present one pupil center detection method based on searching the maximum contribution point to the radial symmetry of the image. Additionally, two different center refinement steps were incorporated with the aim of adapting the algorithm to images with highly elliptical pupil appearances. The performance of the proposed algorithm is evaluated using a dataset consisting of 225,569 head-mounted annotated eye images from publicly available sources. The results are compared with the better algorithm found in the bibliography, with our algorithm being shown as superior. Â© 2018, The Author(s).",,,
10.1016/j.patcog.2018.01.031,2018,"Wang K., Ji Q.",3D gaze estimation without explicit personal calibration,"Model-based 3D gaze estimation represents a dominant technique for eye gaze estimation. It allows free head movement and gives good estimation accuracy. But it requires a personal calibration, which may significantly limit its practical utility. Various techniques have been proposed to replace intrusive and subject-unfriendly calibration methods. In this paper, we introduce a new implicit calibration method that takes advantage of four natural constraints during eye gaze tracking. The first constraint is based on two complementary gaze estimation methods. The underlying assumption is that different gaze estimation methods, though based on different principles and mechanisms, ideally predict exactly the same gaze point at the same time. The second constraint is inspired by the well-known center prior principle, it is assumed that most fixations are concentrated on the center of the screen with natural viewing scenarios. The third constraint arises from the fact that for console based eye tracking, human's attention/gaze are always within the screen region. The final constraint comes from eye anatomy, where the value of eye parameters must be within certain regions. The four constraints are integrated jointly and help formulate the implicit calibration as a constrained unsupervised regression problem, which can be effectively solved through the proposed iterative hard EM algorithm. Experiments on two everyday interactions Web-browsing and Video-watching demonstrate the effectiveness of the proposed implicit calibration method. Â© 2018 Elsevier Ltd",,,
10.1145/3197768.3197775,2018,"Amrouche S., Ferscha A., Gollan B., Heftberger J.",Activity segmentation and identification based on eye gaze features,"In coherence with the ongoing digitalization of production processes, Human Computer Interaction (HCI) technologies have evolved rapidly in industrial applications, providing abundant numbers of the versatile tracking and monitoring devices suitable to address complex challenges. This paper focuses on Activity Segmentation and Activity Identification as one of the most crucial challenges in pervasive computing, applying only visual attention features captured through mobile eye-tracking sensors. We propose a novel, application-independent approach towards segmentation of task executions in semi-manual industrial assembly setup via exploiting the expressive properties of the distribution-based gaze feature Nearest Neighbor Index (NNI) to build a dynamic activity segmentation algorithm. The proposed approach is enriched with a machine learning validation model acting as a feedback loop to classify segments qualities. The approach is evaluated in an alpine ski assembly scenario with real-world data reaching an overall of 91% detection accuracy. Â© 2018 Association for Computing Machinery.",,,
10.1109/DAS.2018.59,2018,"Augereau O., Jacquet C., Kise K., Journet N.",Vocabulometer: A web platform for document and reader mutual analysis,"We present the Vocabulometer, a reading assistant system designed to record the reading activity of a user with an eye tracker and to extract mutual information about the users and the read documents. The Vocabulometer stands as a web platform and can be used for analyzing the comprehension of the user, the comprehensibility of the document, predicting the difficult words, recommending document according to the reader's in order to increase his skills, etc. Since the last years, with the development of low-cost eye trackers, the technology is now accessible for many people, which will allow using data mining and machine learning algorithms for the mutual analysis of documents and readers. Â© 2018 IEEE.",,,
10.1145/3216723.3216729,2018,"Behroozi M., Parnin C.",Can we predict stressful technical interview settings through eye-tracking?,"Recently, eye-tracking analysis for finding the cognitive load and stress while problem-solving on the whiteboard during a technical interview is finding its way in software engineering society. However, there is no empirical study on analyzing how much the interview setting characteristics affect the eye-movement measurements. Without knowing that, the results of a research on eye-movement measurements analysis for stress detection will not be reliable. In this paper, we analyzed the eye-movements of 11 participants in two interview settings, one on the whiteboard and the other on the paper, to find out if the characteristics of the interview settings affect the analysis of participants' stress. To this end, we applied 7 Machine Learning classification algorithms on three different labeling strategies of the data to suggest researchers of the domain a useful practice of checking the reliability of the eye-measurements before reporting any results. Â© 2018 ACM.",,,
10.1145/3205929.3205932,2018,Popelka S.,Eye-tracking evaluation of 3D thematic maps,"Although many 3D thematic cartography methods exist, the effectiveness of their use is not known. The described experiment comprised two parts focusing on the evaluation of two 3D thematic cartography methods (Prism Map and Illuminated Choropleth Map) compared to a simple choropleth map. The task in both parts of the experiment was to determine which of the marked areas showed a higher value of the displayed phenomenon. The correctness of answers, response time and selected eye-tracking metrics were analysed. In the first part of the experiment, a higher number of correct answers was found for Prism Maps than for simple choropleth maps, but it required more time to solve the task. The Illuminated Choropleth Map showed a higher proportion of correct answers than a simple choropleth map. During evaluation of the eye-tracking metrics, a statistically significant difference was not found in most cases. Â© 2018 ACM.",,,
10.1145/3208031.3208033,2018,"Martinikorena I., Villanueva A., Cabeza R., Porta S.",Introducing I2head database,I2Head database has been created with the aim to become an optimal reference for low cost gaze estimation. It exhibits the following outstanding characteristics: it takes into account key aspects of low resolution eye tracking technology; it combines images of users gazing at different grids of points from alternative positions with registers of userâ€™s head position and it provides calibration information of the camera and a simple 3D head model for each user. Hardware used to build the database includes a 6D magnetic sensor and a webcam. A careful calibration method between the sensor and the camera has been developed to guarantee the accuracy of the data. Different sessions have been recorded for each user including not only static head scenarios but also controlled displacements and even free head movements. The database is an outstanding framework to test both gaze estimation algorithms and head pose estimation methods. Â© 2018 Copyright held by the owner/author(s).,,,
10.1145/3206343.3206351,2018,"Elmadjian C., Shukla P., Tula A.D., Morimoto C.H.",3D gaze estimation in the scene volume with a head-mounted eye tracker,"Most applications involving gaze-based interaction are supported by estimation techniques that find a mapping between gaze data and corresponding targets on a 2D surface. However, in Virtual and Augmented Reality (AR) environments, interaction occurs mostly in a volumetric space, which poses a challenge to such techniques. Accurate point-of-regard (PoR) estimation, in particular, is of great importance to AR applications, since most known setups are prone to parallax error and target ambiguity. In this work, we expose the limitations of widely used techniques for PoR estimation in 3D and propose a new calibration procedure using an uncalibrated headmounted binocular eye tracker coupled with an RGB-D camera to track 3D gaze within the scene volume. We conducted a study to evaluate our setup with real-world data using a geometric and an appearance-based method. Our results show that accurate estimation in this setting still is a challenge, though some gaze-based interaction techniques in 3D should be possible.",,,
10.1145/3204493.3204551,2018,"Larumbe A., Cabeza R., Villanueva A.",Supervised Descent Method (SDM) applied to accurate pupil detection in off-the-shelf eye tracking systems,"The precise detection of pupil/iris center is key to estimate gaze accurately. This fact becomes specially challenging in low cost frameworks in which the algorithms employed for high performance systems fail. In the last years an outstanding effort has been made in order to apply training-based methods to low resolution images. In this paper, Supervised Descent Method (SDM) is applied to GI4E database. The 2D landmarks employed for training are the corners of the eyes and the pupil centers. In order to validate the algorithm proposed, a cross validation procedure is performed. The strategy employed for the training allows us to affirm that our method can potentially outperform the state of the art algorithms applied to the same dataset in terms of 2D accuracy. The promising results encourage to carry on in the study of training-based methods for eye tracking. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3204574,2018,Nakayama M.,Ocular reactions in response to impressions of emotion-evoking pictures,"Oculomotor indicies in response to emotional stimuli were analysed chronologically in order to investigate the relationships between eye behaviour and emotional activity in human visual perception. Seven participants classified visual stimuli into two emotional groups using subjective ratings of images, such as â€œPleasantâ€?and â€œUnpleasantâ€? Changes in both eye movements and pupil diameters between the two groups of images were compared. Both the mean saccade lengths and the cross power spectra of eye movements for â€œUnpleasantâ€?ratings were significantly higher than for other ratings of eye movements in regards to certain the duration of certain pictures shown. Also, both mean pupil diameters and their power spectrum densities were significantly higher when the durations of pictures presented were lengthened. When comparing the response latencies, pupil reactions followed the appearance of changes in the direction of eye movements. The results suggest that at specific latencies, â€œUnpleasantâ€?images activate both eye movements and pupil dilations. Â© 2018 Association for Computing Machinery.",,,
10.1145/3204493.3204572,2018,"Hiroe M., Yamamoto M., Nagamatsu T.",Implicit user calibration for gaze-tracking systems using an averaged saliency map around the optical axis of the eye,"A 3D gaze-tracking method that uses two cameras and two light sources can measure the optical axis of the eye without user calibration. The visual axis of the eye (line of sight) is estimated by conducting a single-point user calibration. This single-point user calibration estimates the angle ? that is offset between the optical and visual axes of the eye, which is a user-dependent parameter. We have proposed an implicit user calibration method for gaze-tracking systems using a saliency map around the optical axis of the eye. We assume that the peak of the average of the saliency maps indicates the visual axis of the eye in the eye coordinate system. We used both-eye restrictions effectively. The experimental result shows that the proposed system could estimate angle ? without explicit personal calibration. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3204583,2018,"Keyvanara M., Allison R.S.",Sensitivity to natural 3D image transformations during eye movements,"The saccadic suppression effect, in which visual sensitivity is reduced significantly during saccades, has been suggested as a mechanism for masking graphic updates in a 3D virtual environment. In this study, we investigate whether the degree of saccadic suppression depends on the type of image change, particularly between different natural 3D scene transformations. The user observed 3D scenes and made a horizontal saccade in response to the displacement of a target object in the scene. During this saccade the entire scene translated or rotated. We studied six directions of transformation corresponding to the canonical directions for the six degrees of freedom. Following each trial, the user made a forced-choice indication of direction of the scene change. Results show that during horizontal saccades, the most recognizable changes were rotations along the roll axis. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3204560,2018,"Bannier K., Jain E., Le Meur O.",DeepComics: Saliency estimation for comics,"A key requirement for training deep learning saliency models is large training eye tracking datasets. Despite the fact that the accessibility of eye tracking technology has greatly increased, collecting eye tracking data on a large scale for very specific content types is cumbersome, such as comic images, which are different from natural images such as photographs because text and pictorial content is integrated. In this paper, we show that a deep network trained on visual categories where the gaze deployment is similar to comics outperforms existing models and models trained with visual categories for which the gaze deployment is dramatically different from comics. Further, we find that it is better to use a computationally generated dataset on visual category close to comics one than real eye tracking data of a visual category that has different gaze deployment. These findings hold implications for the transference of deep networks to different domains. Â© 2018 Association for Computing Machinery.",,,
10.1145/3204493.3204555,2018,"Outram B.I., Pai Y.S., Person T., Minamizawa K., Kunze K.",AnyOrbit: Orbital navigation in virtual environments with eye-tracking,"Gaze-based interactions promise to be fast, intuitive and effective in controlling virtual and augmented environments. Yet, there is still a lack of usable 3D navigation and observation techniques. In this work: 1) We introduce a highly advantageous orbital navigation technique, AnyOrbit, providing an intuitive and hands-free method of observation in virtual environments that uses eye-tracking to control the orbital center of movement; 2) The versatility of the technique is demonstrated with several control schemes and use-cases in virtual/augmented reality head-mounted-display and desktop setups, including observation of 3D astronomical data and spectator sports. Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.",,,
10.1145/3204493.3204559,2018,"Fuhl W., Geisler D., Santini T., Appel T., Rosenstiel W., Kasneci E.",CBF: Circular binary features for robust and real-time pupil center detection,"Modern eye tracking systems rely on fast and robust pupil detection, and several algorithms have been proposed for eye tracking under real world conditions. In this work, we propose a novel binary feature selection approach that is trained by computing conditional distributions. These features are scalable and rotatable, allowing for distinct image resolutions, and consist of simple intensity comparisons, making the approach robust to different illumination conditions as well as rapid illumination changes. The proposed method was evaluated on multiple publicly available data sets, considerably outperforming state-of-the-art methods, and being real-time capable for very high frame rates. Moreover, our method is designed to be able to sustain pupil center estimation even when typical edge-detection-based approaches fail â€?e.g., when the pupil outline is not visible due to occlusions from reflections or eye lids / lashes. As a consequece, it does not attempt to provide an estimate for the pupil outline. Nevertheless, the pupil center suffices for gaze estimation â€?e.g., by regressing the relationship between pupil center and gaze point during calibration. Â© 2018 Association for Computing Machinery.",,,
10.1145/3204493.3204545,2018,"Park S., Zhang X., Bulling A., Hilliges O.",Learning to find eye region landmarks for remote gaze estimation in unconstrained settings,"Conventional feature-based and model-based gaze estimation methods have proven to perform well in settings with controlled illumination and specialized cameras. In unconstrained real-world settings, however, such methods are surpassed by recent appearance-based methods due to difficulties in modeling factors such as illumination changes and other visual artifacts. We present a novel learning-based method for eye region landmark localization that enables conventional methods to be competitive to latest appearance-based methods. Despite having been trained exclusively on synthetic data, our method exceeds the state of the art for iris localization and eye shape registration on real-world imagery. We then use the detected landmarks as input to iterative model-fitting and lightweight learning-based gaze estimation methods. Our approach outperforms existing model-fitting and appearance-based methods in the context of person-independent and personalized gaze estimation. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3204524,2018,"Velloso E., Coutinho F.L., Kurauchi A., Morimoto C.H.",Circular orbits detection for gaze interaction using 2D correlation and profile matching algorithms,"Recently, interaction techniques in which the user selects screen targets by matching their movement with the input device have been gaining popularity, particularly in the context of gaze interaction (e.g. Pursuits, Orbits, AmbiGaze, etc.). However, though many algorithms for enabling such interaction techniques have been proposed, we still lack an understanding of how they compare to each other. In this paper, we introduce two new algorithms for matching eye movements: Profile Matching and 2D Correlation, and present a systematic comparison of these algorithms with two other state-ofthe-art algorithms: The Basic Correlation algorithm used in Pursuits and the Rotated Correlation algorithm used in PathSync. We also examine the effects of two thresholding techniques and post-hoc filtering. We evaluated the algorithms on a user dataset and found the 2D Correlation with one-level thresholding and post-hoc filtering to be the best performing algorithm. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3204548,2018,"Zhang X., Sugano Y., Bulling A.",Revisiting data normalization for appearance-based gaze estimation,"Appearance-based gaze estimation is promising for unconstrained real-world settings, but the significant variability in head pose and user-camera distance poses significant challenges for training generic gaze estimators. Data normalization was proposed to cancel out this geometric variability by mapping input images and gaze labels to a normalized space. Although used successfully in prior works, the role and importance of data normalization remains unclear. To fill this gap, we study data normalization for the first time using principled evaluations on both simulated and real data. We propose a modification to the current data normalization formulation by removing the scaling factor and show that our new formulation performs significantly better (between 9.5% and 32.7%) in the different evaluation settings. Using images synthesized from a 3D face model, we demonstrate the benefit of data normalization for the efficiency of the model training. Experiments on real-world images confirm the advantages of data normalization in terms of aze estimation performance. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3204538,2018,"Steil J., Huang M.X., Bulling A.",Fixation detection for head-mounted eye tracking based on visual similarity of gaze targets,"Fixations are widely analysed in human vision, gaze-based interaction, and experimental psychology research. However, robust fixation detection in mobile settings is profoundly challenging given the prevalence of user and gaze target motion. These movements feign a shift in gaze estimates in the frame of reference defined by the eye trackerâ€™s scene camera. To address this challenge, we present a novel fixation detection method for head-mounted eye trackers. Our method exploits that, independent of user or gaze target motion, target appearance remains about the same during a fixation. It extracts image information from small regions around the current gaze position and analyses the appearance similarity of these gaze patches across video frames to detect fixations. We evaluate our method using fine-grained fixation annotations on a five-participant indoor dataset (MPIIEgoFixation) with more than 2,300 fixations in total. Our method outperforms commonly used velocity- and dispersion-based algorithms, which highlights its significant potential to analyse scene image information for eye movement detection. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3204525,2018,"Dierkes K., Kassner M., Bulling A.","A novel approach to single camera, glint-free 3D eye model fitting including corneal refraction","Model-based methods for glint-free gaze estimation typically infer eye pose using pupil contours extracted from eye images. Existing methods, however, either ignore or require complex hardware setups to deal with refraction effects occurring at the corneal interfaces. In this work we provide a detailed analysis of the effects of refraction in glint-free gaze estimation using a single near-eye camera, based on the method presented by [Swirski and Dodgson 2013]. We demonstrate systematic deviations in inferred eyeball positions and gaze directions with respect to synthetic ground-truth data and show that ignoring corneal refraction can result in angular errors of several degrees. Furthermore, we quantify gaze direction dependent errors in pupil radius estimates. We propose a novel approach to account for corneal refraction in 3D eye model fitting and by analyzing synthetic and real images show that our new method successfully captures refraction effects and helps to overcome the shortcomings of the state of the art approach. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3204584,2018,"Wang H., Pi J., Qin T., Shen S., Shi B.E.",SLAM-based localization of 3D gaze using a mobile eye tracker,"Past work in eye tracking has focused on estimating gaze targets in two dimensions (2D), e.g. on a computer screen or scene camera image. Three-dimensional (3D) gaze estimates would be extremely useful when humans are mobile and interacting with the real 3D environment. We describe a system for estimating the 3D locations of gaze using a mobile eye tracker. The system integrates estimates of the userâ€™s gaze vector from a mobile eye tracker, estimates of the eye tracker pose from a visual-inertial simultaneous localization and mapping (SLAM) algorithm, a 3D point cloud map of the environment from a RGB-D sensor. Experimental results indicate that our system produces accurate estimates of 3D gaze over a much larger range than remote eye trackers. Our system will enable applications, such as the analysis of 3D human attention and more anticipative human robot interfaces. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3208346,2018,"Startsev M., Agtzidis I., Dorr M.",Deep learning vs. manual annotation of eye movements,"Deep Learning models have revolutionized many research fields already. However, the raw eye movement data is still typically processed into discrete events via threshold-based algorithms or manual labelling. In this work, we describe a compact 1D CNN model, which we combined with BLSTM to achieve end-to-end sequence-to-sequence learning. We discuss the acquisition process for the ground truth that we use, as well as the performance of our approach, in comparison to various literature models and manual raters. Our deep method demonstrates superior performance, which brings us closer to human-level labelling quality. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3208345,2018,"Eivazi S., Santini T., KÃ¼bler T.C., Kasneci E.",An inconspicuous and modular head-mounted eye tracker,"State of the art head mounted eye trackers employ glasses like frames, making their usage uncomfortable or even impossible for prescription eyewear users. Nonetheless, these users represent a notable portion of the population (e.g. the Prevent Blindness America organization reports that about half of the USA population use corrective eyewear for refractive errors alone). Thus, making eye tracking accessible for eyewear users is paramount to not only improve usability, but is also key for the ecological validity of eye tracking studies. In this work, we report on a novel approach for eye tracker design in the form of a modular and inconspicuous device that can be easily attached to glasses; for users without glasses, we also provide a 3D printable frame blueprint. Our prototypes include both low cost Commerical Out of The Shelf (COTS) and more expensive Original Equipment manufacturer (OEM) cameras, with sampling rates ranging between 30 and 120 fps and multiple pixel resolutions. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3204575,2018,"Hild J., KÃ¼hnle C., Voit M., Beyerer J.",Predicting observerâ€™s task from eye movement patterns during motion image analysis,"Predicting an observerâ€™s tasks from eye movements during several viewing tasks has been investigated by several authors. This contribution adds task prediction from eye movements tasks occurring during motion image analysis: Explore, Observe, Search, and Track. For this purpose, gaze data was recorded from 30 human observers viewing a motion image sequence once under each task. For task decoding, the classification methods Random Forest, LDA, and QDA were used; features were fixation- or saccade-related measures. Best accuracy for prediction of the three tasks Observe, Search, Track from the 4-minute gaze data samples was 83.7% (chance level 33%) using Random Forest. Best accuracy for prediction of all four tasks from the gaze data samples containing the first 30 seconds of viewing was 59.3% (chance level 25%) using LDA. Accuracy decreased significantly for task prediction on small gaze data chunks of 5 and 3 seconds, being 45.3% and 38.0% (chance 25%) for the four tasks, and 52.3% and 47.7% (chance 33%) for the three tasks. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3208350,2018,"Orlov P., Shafti A., Auepanwiriyakul C., Songur N., Faisal A.A.",A Gaze-contingent Intention Decoding Engine for human augmentation,"Humans process high volumes of visual information to perform everyday tasks. In a reaching task, the brain estimates the distance and position of the object of interest, to reach for it. Having a grasp intention in mind, human eye-movements produce specific relevant patterns. Our Gaze-Contingent Intention Decoding Engine uses eye-movement data and gaze-point position to indicate the hidden intention. We detect the object of interest using deep convolution neural networks and estimate its position in a physical space using 3D gaze vectors. Then we trigger the possible actions from an action grammar database to perform an assistive movement of the robotic arm, improving action performance in physically disabled people. This document is a short report to accompany the Gaze-contingent Intention Decoding Engine demonstrator, providing details of the setup used and results obtained. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3204493.3204528,2018,"Singh K., Kalash M., Bruce N.",Capturing real-world gaze behaviour: Live and unplugged,"Understanding human gaze behaviour has benefits from scientific understanding to many application domains. Current practices constrain possible use cases, requiring experimentation restricted to a lab setting or controlled environment. In this paper, we demonstrate a flexible unconstrained end-to-end solution that allows for collection and analysis of gaze data in real-world settings. To achieve these objectives, rich 3D models of the real world are derived along with strategies for associating experimental eye-tracking data with these models. In particular, we demonstrate the strength of photogrammetry in allowing these capabilities to be realized, and demonstrate the first complete solution for 3D gaze analysis in large-scale outdoor environments using standard camera technology without fiducial markers. The paper also presents techniques for quantitative analysis and visualization of 3D gaze data. As a whole, the body of techniques presented provides a foundation for future research, with new opportunities for experimental studies and computational modeling efforts. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3208159.3208180,2018,"He H., She Y., Xiahou J., Yao J., Li J., Hong Q., Ji Y.",Real-time eye-gaze based interaction for human intention prediction and emotion analysis,"The human eye's state of motion and content of interest can express people's cognitive status and emotional status based on their situation. When observing the surrounding things, the human eyes make different eye movements according to the observed objects which reflects human's attention and interest. In this paper, we capture and analyze patterns of human eye-gaze behavior and head motion and classify them into different categories. Besides, we compute and train the eye-object movement attention model and eye-object feature preference model based on different peoples' eye-gaze behaviors by using machine learning algorithms. These models are used to predict humans' object of interest and the interaction intention according to people's real-time situation. Furthermore, the eye-gaze behavior and head motion patterns can be used as a modality of non-verbal information in the computing of human emotional states based on the PAD affective computing model. Our methodology analyzes human emotion and cognition status from the aspect of eye-gaze behavior and head motion, understands the cognitive information that human eyes can express, and effectively improves the efficiency of human-computer interaction in different circumstances. Â© 2018 ACM.",,,
10.1109/FG.2018.00085,2018,"Eigbe N., BaltruÅ¡aitis T., Morency L.-P., Pestian J.",Toward visual behavior markers of suicidal ideation,"Suicide is an increasingly present issue in our society whose eradication could be greatly aided by decision support technologies that can objectively identify behavior markers of suicidal ideation. In this paper, we examine the predictive ability of a variety of smiling and eye gaze behaviors in categorizing hospital patients by mental health status: patients with suicidal ideation, patients with other mental illnesses such as depression, or control group without suicidal ideation or mental illness. We study three main research questions related to suicide behavior markers: (1) Do people with suicidal ideation smile with different dynamics (e.g. genuine vs fake smile)? (2) Do smiles while speaking, listening, and laughing show different levels of occurrence between the three groups? (3) Is gaze aversion (e.g. looking down) also a useful behavior marker? To answer these questions, we created new behavioral annotations on 74 semi-structured interviews from hospital patients, each of them within one of the three mental health conditions. Our data analysis identified behavior markers of mental health status from both smiling and eye gaze behaviors. Using these behavioral features, we created predictive models that show promising results when distinguishing between these three mental health conditions, especially when differentiating suicidal from non-suicidal patients. Â© 2018 IEEE.",,,
10.1109/FG.2018.00086,2018,"Kononenko D., Lempitsky V.",Semi-supervised learning for monocular gaze redirection,"We present a new approach to monocular learning-based gaze redirection problem in images that is able to train on raw sequences of eye images with unknown gaze directions and a small amount of eye images, where the gaze direction is known. The proposed approach is based on a pair of deep networks, where the first encoder-like network maps eye images to a latent space, while the second network maps pairs of latent representations to warping fields implementing the transformation between the pair of the original images. In the proposed system, both networks are trained in an unsupervised manner, while the gaze-annotated images are only used to estimate displacements in the latent space that are characteristic to certain gaze redirections. Quantitative and qualitative evaluation suggests that such characteristic displacement vectors in the learned latent space can be learned from few examples and are transferable across different people and different imaging conditions. Â© 2018 IEEE.",,,
10.1109/FG.2018.00019,2018,"Baltrusaitis T., Zadeh A., Lim Y.C., Morency L.-P.",OpenFace 2.0: Facial behavior analysis toolkit,"Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace 2.0 - a tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace 2.0 is an extension of OpenFace toolkit and is capable of more accurate facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace 2.0 demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, unlike a lot of modern approaches or toolkits, OpenFace 2.0 source code for training models and running them is freely available for research purposes. Â© 2018 IEEE.",,,
10.3390/mti2020031,2018,"Spiliotopoulos K., Rigou M., Sirmakessis S.",A comparative study of skeuomorphic and flat design from a ux perspective,"A key factor influencing the effectiveness of a user interface is the usability resulting from its design, and the overall experience generated while using it, through any kind of device. The two main design trends that prevail in the field of user interface design is skeuomorphism and flat design. Skeuomorphism was used in UI design long before flat design and it is built upon the notion of metaphors and affordances. Flat design is the main design trend used in most UIs today and, unlike skeuomorphic design, it is considered as a way to explore the digital medium without trying to reproduce the appearance of the physical world. This paper investigates how users perceive the two design approaches at the level of icon design (in terms of icon recognizability, recall and effectiveness) based on series of experiments and on data collected via a Tobii eye tracker. Moreover, the paper poses the question whether users perceive an overall flat design as more aesthetically attractive or more usable than a skeuomorphic equivalent. All tested hypotheses regarding potential effect of design approach on icon recognizability, task completion time, or number of errors were rejected but users perceived flat design as more usable. The last issue considered was how users respond to functionally equivalent flat and skeuomorphic variations of websites when given specific tasks to execute. Most tested hypotheses that website design affects task completion durations, user expected and experienced difficulty, or SUS (System Usability Scale) and meCUE questionnaires scores were rejected but there was a correlation between skeuomorphic design and increased experienced difficulty, as well as design type and SUS scores but not in both websites examined. Â© 2018 by the authors. Licensee MDPI, Basel, Switzerland.",,,
10.5626/JCSE.2018.12.2.77,2018,"Sharma A., Abrol P.",Design and analysis of improved iris-based gaze estimation model,"The detection accuracy of gaze direction mainly depends on the performance of features extracted from eye images. Limitations on the estimation of gaze direction include harmful infrared (IR) light, expensive devices, static thresholding, inappropriate and complex segmentation techniques, corneal reflections, etc. In this study, an efficient appearance cum feature-based detection model, namely, iris center-based gaze estimation (ICGE), has been proposed. The model is an extension of the earlier proposed glint-based gaze direction estimation (GDE) model and overcomes the above limitations. The ICGE model has been analyzed for GDE based on iris center coordinates using a local adaptive thresholding technique. An indigenous database using more than two hundred images of different subjects on a five quadrant map screen generates almost 90% accurate results for iris and gaze quadrant detection. The distinguishing features of the low cost, non-intrusive proposed model include a lack of IR and affordable ubiquitous H/W designing, large subject-camera distance and screen dimensions, no glint dependency, and many more. The proposed model also shows significantly better results in the lower periphery corners of the quadrant map than traditional models. In addition, aside from the comparison with the GDE model, the proposed model has also been compared with other existing techniques. Â© 2018. The Korean Institute of Information Scientists and Engineers.",,,
10.25300/MISQ/2018/14124,2018,"Vance A., Jenkins J.L., Anderson B.B., Bjornn D.K., Kirwan C.B.","Tuning out security warnings: A longitudinal examination of habituation through fMRI, eye tracking, and field experiments","Research in the fields of information systems and human-computer interaction has shown that habituation-decreased response to repeated stimulation-is a serious threat to the effectiveness of security warnings. Although habituation is a neurobiological phenomenon that develops over time, past studies have only examined this problem cross-sectionally. Further, past studies have not examined how habituation influences actual security warning adherence in the field. For these reasons, the full extent of the problem of habituation is unknown. We address these gaps by conducting two complementary longitudinal experiments. First, we performed an experiment collecting fMRI and eye-tracking data simultaneously to directly measure habituation to security warnings as it develops in the brain over a five-day workweek. Our results show not only a general decline of participants' attention to warnings over time but also that attention recovers at least partially between workdays without exposure to the warnings. Further, we found that updating the appearance of a warning-that is, a polymorphic design-substantially reduced habituation of attention. Second, we performed a three-week field experiment in which users were naturally exposed to privacy permission warnings as they installed apps on their mobile devices. Consistent with our fMRI results, users' warning adherence substantially decreased over the three weeks. However, for users who received polymorphic permission warnings, adherence dropped at a substantially lower rate and remained high after three weeks, compared to users who received standard warnings. Together, these findings provide the most complete view yet of the problem of habituation to security warnings and demonstrate that polymorphic warnings can substantially improve adherence. Â© 2018. The Authors.",,,
10.1016/j.imavis.2018.04.001,2018,"Li N., Busso C.","Calibration free, user-independent gaze estimation with tensor analysis","Human gaze directly signals visual attention, therefore, estimation of gaze has been an important research topic in fields such as human attention modeling and human-computer interaction. Accurate gaze estimation requires user, system and even session dependent parameters, which can be obtained by calibration process. However, this process has to be repeated whenever the parameter changes (head movement, camera movement, monitor movement). This study aims to eliminate the calibration process of gaze estimation by building a user-independent, appearance-based gaze estimation model. The system is ideal for multimodal interfaces, where the gaze is tracked without the cooperation from the users. The main goal is to capture the essential representation of the gaze appearance of the target user. We investigate the tensor analysis framework that decomposes the high dimension gaze data into different factors including individual differences, gaze differences, user-screen distances and session differences. The axis that is representative for a particular subject is automatically chosen in the tensor analysis framework using LASSO regression. The proposed approaches show promising results on capturing the test subject gaze changes. To address the estimation shift caused by the variations in individual heights, or relative position to the monitor, we apply domain adaptation to adjust the gaze estimation, observing further improvements. These promising results suggest that the proposed gaze estimation approach is a feasible and flexible scheme to facilitate gaze-based multimodal interfaces. Â© 2018 Elsevier B.V.",,,
10.1145/3206505.3206522,2018,"Khamis M., Oechsner C., Alt F., Bulling A.",VRPursuits: Interaction in virtual reality using smooth pursuit eye movements,"Gaze-based interaction using smooth pursuit eye movements (Pursuits) is attractive given that it is intuitive and overcomes the Midas touch problem. At the same time, eye tracking is becoming increasingly popular for VR applications. While Pursuits was shown to be effective in several interaction contexts, it was never explored in-depth for VR before. In a user study (N=26), we investigated how parameters that are specific to VR settings influence the performance of Pursuits. For example, we found that Pursuits is robust against different sizes of virtual 3D targets. However performance improves when the trajectory size (e.g., radius) is larger, particularly if the user is walking while interacting. While walking, selecting moving targets via Pursuits is generally feasible albeit less accurate than when stationary. Finally, we discuss the implications of these findings and the potential of smooth pursuits for interaction in VR by demonstrating two sample use cases: 1) gaze-based authentication in VR, and 2) a space meteors shooting game. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3231884.3231890,2018,"Morales A., Costela F.M., Tolosana R., Woods R.L.",Saccade landing point prediction: A novel approach based on recurrent neural networks,"A saccade is a fast eye movement that allows the change of visualfixation from one object of interest to another. These movementsare characterized by very high angular velocity peaks that canreach up to 1,000Â°/s, making them as one of the fastestneuromotor activities in the human body. Modeling such acomplex movement remains a challenge. Saccadic eye movementscan be defined by initial and landing points, duration, amplitude,and velocity profile. The landing point is important as it definesthe new fixation region and, therefore, the region of interest of theviewer. Its prediction may reduce problems caused by displayupdate latency in gaze-contingent systems that make real-timechanges in the display based on eye tracking. The maincontribution of this work is to propose the use of state-of-the-artmachine learning techniques (i.e., Recurrent Neural Networks) forsaccade landing point prediction in real-world scenarios. Ourmethod was evaluated using 220,000 saccades from 75 subjectsacquired during viewing video from ""Hollywood"" movies. Theresults obtained using our proposed methods outperform existingapproaches with improvements of up to 40% error reduction. Ourresults show that dynamic temporal relationships exploited byRecurrent Neural Networks can improve the performance oftraditional Feed Forward Neural Networks. Â© 2018 Association for Computing Machinery.",,,
10.1109/WACV.2018.00171,2018,"Gupta P., Gupta S., Jayagopal A., Pal S., Sinha R.",Saliency prediction for mobile user interfaces,"We introduce models for saliency prediction for mobile user interfaces. A mobile interface may include elements like buttons and text in addition to natural images which enable performing a variety of tasks. Saliency in natural images is a well studied topic. However, given the difference in what constitutes a mobile interface, and the usage context of these devices, we postulate that saliency prediction for mobile interface images requires a fresh approach. Mobile interface design involves operating on elements, the building blocks of the interface. We first collected eye-gaze data from mobile devices for a free viewing task. Using this data, we develop a novel autoencoder based multi-scale deep learning model that provides saliency prediction at the mobile interface element level. Compared to saliency prediction approaches developed for natural images, we show that our approach performs significantly better on a range of established metrics. Â© 2018 IEEE.",,,
10.1109/WACV.2018.00022,2018,"Kowalski M., Nasarzewski Z., Galinski G., Garbat P.",HoloFace: Augmenting Human-to-Human Interactions on HoloLens,"We present HoloFace, an open-source framework for face alignment, head pose estimation and facial attribute retrieval for Microsoft HoloLens. HoloFace implements two state-of-the-art face alignment methods which can be used interchangeably: one running locally and one running on a remote backend. Head pose estimation is accomplished by fitting a deformable 3D model to the landmarks localized using face alignment. The head pose provides both the rotation of the head and a position in the world space. The parameters of the fitted 3D face model provide estimates of facial attributes such as mouth opening or smile. Together the above information can be used to augment the faces of people seen by the HoloLens user, and thus their interaction. Potential usage scenarios include facial recognition, emotion recognition, eye gaze tracking and many others. We demonstrate the capabilities of our framework by augmenting the faces of people seen through the HoloLens with various objects and animations. Â© 2018 IEEE.",,,
10.1016/j.neucom.2017.02.106,2018,"Yang H., Qu S., Zhu F., Zheng Z.",Robust objectness tracking with weighted multiple instance learning algorithm,"A novel improved online weighted multiple instance learning algorithm(IWMIL) for visual tracking is proposed. In the IWMIL algorithm, the importance of each sample contributing to bag probability is evaluated based on the objectness estimation with object properties (superpixel straddling). To reduce the computation cost, a coarse-to-fine sample detection method is employed to detect sample for a new arriving frame. Then, an adaptive learning rate, which exploits the maximum classifier score to assign different weights to tracking result and template, is presented to update the classifiers. Furthermore, an object similarity constraint strategy is used to estimate tracking drift. Experimental results on challenging sequences show that the proposed method is robust to occlusion and appearance changes. Â© 2018 Elsevier B.V.",,,
10.1587/transinf.2017MVP0020,2018,"Ogawa T., Nakazawa A., Nishida T.",Point of gaze estimation using corneal surface reflection and omnidirectional camera image,"We present a human point of gaze estimation system using corneal surface reflection and omnidirectional image taken by spherical panorama cameras, which becomes popular recent years. Our system enables to find where a user is looking at only from an eye image in a 360Â° surrounding scene image, thus, does not need gaze mapping from partial scene images to a whole scene image that are necessary in conventional eye gaze tracking system. We first generate multiple perspective scene images from an omnidirectional (equirectangular) image and perform registration between the corneal reflection and perspective images using a corneal reflection-scene image registration technique. We then compute the point of gaze using a corneal imaging technique leveraged by a 3D eye model, and project the point to an omnidirectional image. The 3D eye pose is estimate by using the particle-filter-based tracking algorithm. In experiments, we evaluated the accuracy of the 3D eye pose estimation, robustness of registration and accuracy of PoG estimations using two indoor and five outdoor scenes, and found that gaze mapping error was 5.546 [deg] on average. Â© 2018 The Institute of Electronics, Information and Communication Engineers.",,,
10.1016/j.media.2018.03.002,2018,"Gessert N., SchlÃ¼ter M., Schlaefer A.",A deep learning approach for pose estimation from volumetric OCT data,"Tracking the pose of instruments is a central problem in image-guided surgery. For microscopic scenarios, optical coherence tomography (OCT) is increasingly used as an imaging modality. OCT is suitable for accurate pose estimation due to its micrometer range resolution and volumetric field of view. However, OCT image processing is challenging due to speckle noise and reflection artifacts in addition to the imagesâ€?3D nature. We address pose estimation from OCT volume data with a new deep learning-based tracking framework. For this purpose, we design a new 3D convolutional neural network (CNN) architecture to directly predict the 6D pose of a small marker geometry from OCT volumes. We use a hexapod robot to automatically acquire labeled data points which we use to train 3D CNN architectures for multi-output regression. We use this setup to provide an in-depth analysis on deep learning-based pose estimation from volumes. Specifically, we demonstrate that exploiting volume information for pose estimation yields higher accuracy than relying on 2D representations with depth information. Supporting this observation, we provide quantitative and qualitative results that 3D CNNs effectively exploit the depth structure of marker objects. Regarding the deep learning aspect, we present efficient design principles for 3D CNNs, making use of insights from the 2D deep learning community. In particular, we present Inception3D as a new architecture which performs best for our application. We show that our deep learning approach reaches errors at our ground-truth label's resolution. We achieve a mean average error of 14.89 Â± 9.3 Âµm and 0.096 Â± 0.072Â° for position and orientation learning, respectively. Â© 2018 Elsevier B.V.",,,
10.1016/j.entcom.2018.02.003,2018,"Wibirama S., Nugroho H.A., Hamamoto K.",Depth gaze and ECG based frequency dynamics during motion sickness in stereoscopic 3D movie,"The simulator sickness questionnaire (SSQ) has been a prevalent method to observe motion sickness in stereoscopic 3D motion picture. However, previous works do not provide adequate comprehension of the relationship between SSQ, depth gaze behavior, and heart rate variability in the stereoscopic 3D motion picture. To fill this research gap, we present a novel investigation of motion sickness in stereoscopic 3D movies using SSQ, electrocardiography (ECG), and 3D gaze tracking. Forty participants (N=40) watched only one of two 3D contentsâ€?D content with a strong or a moderate sensation of vection. We observed that viewers of the 3D content with an intense feeling of vection more frequently reported symptoms of nausea (p<0.005) and disorientation (p<0.05) than their counterpart. SSQ, ECG, and 3D gaze tracking data show that sickness level could be reduced by persistently gazing at a particular point during exposure of 3D contents (p < 0.001). Additionally, we found that individuals who were prone to motion sickness experienced depth gaze oscillation during several provoking scenes in dynamic 3D contents. Our experimental results may be used as a guideline in the development of a motion sickness predictor for various stereoscopic 3D motion pictures. Â© 2018 Elsevier B.V.",,,
10.1145/3192714.3192819,2018,"Yaneva V., An Ha L., Eraslan S., Yesilada Y., Mitkov R.",Etecting autism based on eye-tracking data from web searching tasks,"The ASD diagnosis requires a long, elaborate, and expensive prnnocedure, which is subjective and is currently restricted to behavioural, historical, and parent-report information. In this paper, we present an alternative way for detecting the condition based on the atypical visual-attention patterns of people with autism. We collect gaze data from two different kinds of tasks related to processing of information from web pages: Browsing and Searching. The gaze data is then used to train a machine learning classifier whose aim is to distinguish between participants with autism and a control group of participants without autism. In addition, we explore the effects of the type of the task performed, different approaches to defining the areas of interest, gender, visual complexity of the web pages and whether or not an area of interest contained the correct answer to a searching task. Our best-performing classifier achieved 0.75 classification accuracy for a combination of selected web pages using all gaze features. These preliminary results show that the differences in the way people with autism process web content could be used for the future development of serious games for autism screening. The gaze data, R code, visual stimuli and task descriptionare made freely available for replication purposes. Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1109/INFOTEH.2018.8345537,2018,"Arsenovic M., Sladojevic S., Stefanovic D., Anderla A.",Deep neural network ensemble architecture for eye movements classification,"Up to now, eye tracking technologies have been used for different purposes in various industries, from medical to gaming. Eye tracking methods could include predicting fixations, gaze mapping or movement classification. Recent advances in deep learning techniques provide possibilities for solving many computer vision tasks with high accuracy. Authors of this paper propose a novel deep learning based architecture for eye movement classification task. Proposed architecture is an ensemble approach which employs deep convolutional neural networks that run in parallel, for both eyes separately, for visual feature extractions along with recurrent layers for temporal information gathering. Dataset images for training and validation were gathered from standard web camera and pre-processed automatically using dedicated tools. Overall accuracy of developed classifier on the validation set was 92%. Proposed architecture uses relatively small networks which brings the possibility of real time usage (successfully tested on 15-20fps) on regular CPU. Classifier achieved overall accuracy of 88% on the real-time test, using standard laptop and web camera. Â© 2018 IEEE.",,,
10.1145/3170427.3188664,2018,"Panwar P., Collins C.",Detecting negative emotion for mixed initiative visual analytics,"The paper describes an efficient model to detect negative mind states caused by visual analytics tasks. We have developed a method for collecting data from multiple sensors, including GSR and eye-tracking, and quickly generating labelled training data for the machine learning model. Using this method we have created a dataset from 28 participants carrying out intentionally difficult visualization tasks. We have concluded the paper by a discussing the best performing model, Random Forest, and its future applications for providing just-in-time assistance for visual analytics. Copyright held by the owner/author(s).",,,
10.1145/3170427.3170661,2018,Fridman L.,Deep learning for understanding the human,"We will explore how deep learning approaches can be used for perceiving and interpreting the state and behavior of human beings in images, video, audio, and text data. The course will cover how convolutional, recurrent and generative neural networks can be used for applications of face recognition, eye tracking, cognitive load estimation, emotion recognition, natural language processing, voice-based interaction, and activity recognition. The course is open to beginners and is designed for those who are new to deep learning, but it can also benefit advanced researchers in the field looking for a practical overview of deep learning methods and their application. Copyright held by the owner/author(s).",,,
10.1145/3173574.3173862,2018,"Pfeuffer K., Li Y.",Analysis and modeling of grid performance on touchscreen mobile devices,"Touchscreen mobile devices can afford rich interaction behaviors but they are complex to model. Scrollable twodimensional grids are a common user interface on mobile devices that allow users to access a large number of items on a small screen by direct touch. By analyzing touch input and eye gaze of users during grid interaction, we reveal how multiple performance components come into play in such a task, including navigation, visual search and pointing. These findings inspired us to design a novel predictive model that combines these components for modeling grid tasks. We realized these model components by employing both traditional analytical methods and data-driven machine learning approaches. In addition to showing high accuracy achieved by our model in predicting human performance on a test dataset, we demonstrate how such a model can lead to a significant reduction in interaction time when used in a predictive user interface. Â© 2018 Copyright held by the owner/author(s).",,,
10.1145/3173574.3174198,2018,"Zhang X., Huang M.X., Sugano Y., Bulling A.",Training person-specific gaze estimators from user interactions with multiple devices,"Learning-based gaze estimation has significant potential to enable attentive user interfaces and gaze-based interaction on the billions of camera-equipped handheld devices and ambient displays. While training accurate person- and device-independent gaze estimators remains challenging, person-specific training is feasible but requires tedious data collection for each target device. To address these limitations, we present the first method to train person-specific gaze estimators across multiple devices. At the core of our method is a single convolutional neural network with shared feature extraction layers and device-specific branches that we train from face images and corresponding on-screen gaze locations. Detailed evaluations on a new dataset of interactions with five common devices (mobile phone, tablet, laptop, desktop computer, smart TV) and three common applications (mobile game, text editing, media center) demonstrate the significant potential of cross-device training. We further explore training with gaze locations derived from natural interactions, such as mouse or touch input. Â© 2018 Copyright held by the owner/author(s).",,,
10.1109/ICICICT1.2017.8342605,2018,"Jeevithashree D.V., Ray P., Natarajan P., Pradipta B.",Automating the process of gaze tracking data using soft clustering,"The aim of the paper is to automate the processing of gaze tracking data through soft clustering techniques. Standard analysis software for eye gaze tracking data requires users to define areas of interest, which may not be best option for exploratory analysis, where users may want to analyze eye gaze tracking data to know the area of interest. We have presented results on using Fuzzy c-means and Expectation Maximization algorithms on gaze tracking data and using an entropy based cluster validation index, we tried to automate identification of areas of interest. In our study, data from search task in digitally rendered 2D architectural plans have been explored and results indicated that irrespective of clustering technique, users fixated attention only 2 or 3 times for individual image. We have also presented GUI of a tool that can automatically identify areas of interest for any gaze tracking data sample using FCM or EM Algorithms. Â© 2017 IEEE.",,,
10.1016/j.neucom.2018.01.068,2018,"Bai B., Zhong B., Ouyang G., Wang P., Liu X., Chen Z., Wang C.",Kernel correlation filters for visual tracking with adaptive fusion of heterogeneous cues,"Although the correlation filter-based trackers have achieved competitive results both on accuracy and robustness, the performance of trackers can still be improved because the most existing trackers either use a fixed scale or a sole filtering template to represent a target object. In this paper, to effectively handle the scale variation and the drifting problem, we propose a correlation filter-based tracker by adaptively fusing the heterogeneous cues. Firstly, to tackle the problems of the fixed template size, the scale of a target object is estimated from a set of possible scales. Secondly, an adaptive set of filtering templates is learned to alleviate the drifting problem by carefully selecting object candidates in different situations to jointly capture the target appearance variations. Finally, a variety of simple yet effective features (e.g., the HOG and color name features) are effectively integrated into the learning process of filters to further improve the discriminative power of the filters. Consequently, the proposed correlation filter-based tracker can simultaneous utilizes different types of cues to effectively estimate the target's location and scale while alleviating the drifting problem. We have done extensive experiments on the CVPR2013 tracking benchmark dataset with 50 challenging sequences. The proposed tracker successfully tracked the targets in about 90% videos and outperformed the state-of-the-art trackers. Â© 2018 Elsevier B.V.",,,
10.1109/SITIS.2017.64,2018,"Wirawan C., Qingyao H., Yi L., Yean S., Lee B.-S., Ran F.",Pholder: An Eye-Gaze Assisted Reading Application on Android,"Eye-gaze has been used extensively in human computer interface design, web layout design and as assistive technology. We successfully built a reading application with automatic scrolling, using the images captured by the in-build camera to determine the eye-gaze. The application, Pholder, uses the appearance-based method for gaze estimation and tracking of gaze movement directions for scrolling of the screen. We used an innovative technique, using the integration of pixel intensity, for gaze movement estimation which is more robust then other techniques. Â© 2017 IEEE.",,,
10.1109/ICSC.2018.00067,2018,"Watkins D., Gallardo G., Chau S.",Pilot Support System: A Machine Learning Approach,"Pilots can be one of the factors in many air traffic accidents. When one or both pilots are impaired (e.g. fatigue, drunk or distracted), one or both pilots are disabled, one or both pilots are capable but wrong-headed, both pilots don't have sufficient training, both pilots are fully capable but distracted, both pilots miscommunicate with the air traffic controller, or both pilots follow wrong instructions from the air traffic controller, the risk of accident will increase dramatically. In some of these cases, the risk can be mitigated by using big data and machine learning. The learning machine will collect and analyze large amount of data about the state of the aircraft, e.g., the flight path, the immediate environment around the aircraft, the weather and terrain information, and the pilots' input to control the aircraft. Additional sensors such as eye tracking devices and biological monitor can also be added to determine the condition of the pilots. If the pilots' input do not match proper reaction to the situation or the pilots are impaired, the learning machine will first provide an advisory to the pilot. When the situation becomes more urgent, the advisory will be elevated to warning. If there is at least one capable pilot, these advisories and warnings may help the pilot take proper actions. If it is both pilots are impaired or incapable, a warning will be sent to the air traffic controllers so that they can take actions such as redirect the flight management system and activate the autopilot. Â© 2018 IEEE.",,,
10.23919/ELINFOCOM.2018.8330605,2018,"Adithya B., Pavan Kumar B.N., Lee H., Kim J.Y., Moon J.C., Chai Y.H.",An experimental study on relationship between foveal range and FoV of a human eye using eye tracking devices,"Various methodologies have been scrutinized to model a human eye. Most of them have failed to consider aspects pertaining to free movement of the head and mainly focus on the gaze of a Human Eye. Today's eye trackers offer gaze data with respect to the normalized coordinate system. In this paper, experimental results are presented that infer that the point of gaze of a human eye, highly lies within the foveal view and drifts along the foveal view as the user traces the gaze points on the 2D plane. Â© 2018 Institute of Electronics and Information Engineers.",,,
10.1111/jcal.12232,2018,"Prieto L.P., Sharma K., Kidzinski Å., RodrÃ­guez-Triana M.J., Dillenbourg P.",Multimodal teaching analytics: Automated extraction of orchestration graphs from wearable sensor data,"The pedagogical modelling of everyday classroom practice is an interesting kind of evidence, both for educational research and teachers' own professional development. This paper explores the usage of wearable sensors and machine learning techniques to automatically extract orchestration graphs (teaching activities and their social plane over time) on a dataset of 12 classroom sessions enacted by two different teachers in different classroom settings. The dataset included mobile eye-tracking as well as audiovisual and accelerometry data from sensors worn by the teacher. We evaluated both time-independent and time-aware models, achieving median F1 scores of about 0.7â€?.8 on leave-one-session-out k-fold cross-validation. Although these results show the feasibility of this approach, they also highlight the need for larger datasets, recorded in a wider variety of classroom settings, to provide automated tagging of classroom practice that can be used in everyday practice across multiple teachers. Â© 2018 John Wiley & Sons Ltd",,,
10.1109/ICCE.2018.8326062,2018,"Hwang H., Kang D.",User-friendly inter-pupillary distance calibration method using a single camera for autostereoscopic 3D displays,"The accurate inter-pupillary distance (IPD) of a user plays an important role and is a prerequisite for eye-tracking-based autostereoscopic three-dimensional (3D) display systems by calculating the precise 3D eye position of the users. We aimed to develop a robust computer-aided algorithm for each user-specific IPD calibration using a single camera in a user-friendly manner. Our algorithm consists of eye tracking, pattern rendering, user pattern selection, and IPD adjustment according to the selected patterns. Two stereo patterns were designed to clearly show the IPD differences: A 3D stereo registration pattern and a complimentary stereo pattern. We applied this algorithm to 21 users. The reference standard was provided by a commercial pupilometer. The IPD values obtained by the proposed method and the reference standard IPD values were not statistically different (64.9 Â± 4.1 mm from the algorithm and 64.2 Â± 3.4 mm from the reference standard, p = 6.64) from the students' t-test. A good agreement was observed among the 21 users in using the IPD calibration software with an agreement of 94.8% (kappa 0.89, 95% confidence interval from 0.83 to 0.96, and p < 0.0001). Our algorithm shows promising results in IPD calibration using a single camera in a user-friendly manner. Â© 2018 IEEE.",,,
10.1016/j.neucom.2017.11.068,2018,"Li C., Wu X., Zhao N., Cao X., Tang J.",Fusing two-stream convolutional neural networks for RGB-T object tracking,"This paper investigates how to integrate the complementary information from RGB and thermal (RGB-T) sources for object tracking. We propose a novel Convolutional Neural Network (ConvNet) architecture, including a two-stream ConvNet and a FusionNet, to achieve adaptive fusion of different source data for robust RGB-T tracking. Both RGB and thermal streams extract generic semantic information of the target object. In particular, the thermal stream is pre-trained on the ImageNet dataset to encode rich semantic information, and then fine-tuned using thermal images to capture the specific properties of thermal information. For adaptive fusion of different modalities while avoiding redundant noises, the FusionNet is employed to select most discriminative feature maps from the outputs of the two-stream ConvNet, and updated online to adapt to appearance variations of the target object. Finally, the object locations are efficiently predicted by applying the multi-channel correlation filter on the fused feature maps. Extensive experiments on the recently public benchmark GTOT verify the effectiveness of the proposed approach against other state-of-the-art RGB-T trackers. Â© 2017",,,
10.1145/3172944.3172987,2018,"Robal T., Zhao Y., Lofi C., Hauff C.",Webcam-based attention tracking in online learning: A feasibility study,"A main weakness of the open online learning movement is retention: a small minority of learners (on average 5-10%, in extreme cases < 1%) that start a so-called Massive Open Online Course (MOOC) complete it successfully. There are many reasons why learners are unsuccessful, among the most important ones is the lack of self-regulation: learners are often not able to self-regulate their learning behavior. Designing tools that provide learners with a greater awareness of their learning is vital to the future success of MOOC environments. Detecting learners' loss of focus during learning is particularly important, as this can allow us to intervene and return the learners' attention to the learning materials. One technological affordance to detect such loss of focus are webcams-ubiquitous pieces of hardware available in almost all laptops today. In recent years, researchers have begun to exploit eye tracking and gaze data generated from webcams as part of complex machine learning solutions to detect inattention or loss of focus. Those approaches however tend to have a high detection lag, can be inaccurate, and are complex to design and maintain. In contrast, in this paper, we explore the possibility of a simple alternative-the presence or absence of a face- to detect a loss of focus in the online learning setting. To this end, we evaluate the performance of three consumer and professional eye/face-tracking frameworks using a benchmark suite we designed specifically for this purpose: it contains a set of common xMOOC user activities and behaviours. The results of our study show that even this basic approach poses a significant challenge to current hardware and software-based tracking solutions. Â© 2018 ACM.",,,
10.1007/s13735-018-0150-6,2018,"Soleymani M., Riegler M., Halvorsen P.",Multimodal analysis of user behavior and browsed content under different image search intents,"The motivation or intent of a search for content may vary between users and use-cases. Knowledge and understanding of these underlying objectives may therefore be important in order to return appropriate search results, and studies of user search intent are emerging in information retrieval to understand why a user is searching for a particular type of content. In the context of image search, our work targets automatic recognition of usersâ€?intent in an early stage of a search session. We have designed seven different search scenarios under the intent conditions of finding items, re-finding items and entertainment. Moreover, we have collected facial expressions, physiological responses, eye gaze and implicit user interactions from 51 participants who performed seven different search tasks on a custom-built image retrieval platform, and we have analyzed the usersâ€?spontaneous and explicit reactions under different intent conditions. Finally, we trained different machine learning models to predict usersâ€?search intent from the visual content of the visited images, the user interactions and the spontaneous responses. Our experimental results show that after fusing the visual and user interaction features, our system achieved the F-1 score of 0.722 for classifying three classes in a user-independent cross-validation. Eye gaze and implicit user interactions, including mouse movements and keystrokes are the most informative features for intent recognition. In summary, the most promising results are obtained by modalities that can be captured unobtrusively and online, and the results therefore demonstrate the potential of including intent-based methods in multimedia retrieval platforms. Â© 2018, Springer-Verlag London Ltd., part of Springer Nature.",,,
10.1007/s11548-017-1653-y,2018,"PerwÃ¶g M., Bardosi Z., Freysinger W.",Experimental validation of predicted application accuracies for computer-assisted (CAS) intraoperative navigation with paired-point registration,"Purpose: The target registration error (TRE) is a crucial parameter to estimate the potential usefulness of computer-assisted navigation intraoperatively. Both image-to-patient registration on base of rigid-body registration and TRE prediction methods are available for spatially isotropic and anisotropic data. This study presents a thorough validation of data obtained in an experimental operating room setting with CT images. Methods: Optical tracking was used to register a plastic skull, an anatomic specimen, and a volunteer to their respective CT images. Plastic skull and anatomic specimen had implanted bone fiducials for registration; the volunteer was registered with anatomic landmarks. Fiducial localization error, fiducial registration error, and total target error (TTE) were measured; the TTE was compared to isotropic and anisotropic error prediction models. Numerical simulations of the experiment were done additionally. Results: The user localization error and the TTE were measured and calculated using predictions, both leading to results as expected for anatomic landmarks and screws used as fiducials. TRE/TTE is submillimetric for the plastic skull and the anatomic specimen. In the experimental data a medium correlation was found between TRE and target localization error (TLE). Most of the predictions of the application accuracy (TRE) fall in the 68% confidence interval of the measured TTE. For the numerically simulated data, a prediction of TTE was not possible; TRE and TTE show a negligible correlation. Conclusion: Experimental application accuracy of computer-assisted navigation could be predicted satisfactorily with adequate models in an experimental setup with paired-point registration of CT images to a patient. The experimental findings suggest that it is possible to run navigation and prediction of navigation application accuracy basically defined by the spatial resolution/precision of the 3D tracker used. Â© 2017, The Author(s).",,,
10.1109/TCYB.2017.2675910,2018,"Wang L., Lu H., Yang M.-H.",Constrained Superpixel Tracking,"In this paper, we propose a constrained graph labeling algorithm for visual tracking where nodes denote superpixels and edges encode the underlying spatial, temporal, and appearance fitness constraints. First, the spatial smoothness constraint, based on a transductive learning method, is enforced to leverage the latent manifold structure in feature space by investigating unlabeled superpixels in the current frame. Second, the appearance fitness constraint, which measures the probability of a superpixel being contained in the target region, is developed to incrementally induce a long-term appearance model. Third, the temporal smoothness constraint is proposed to construct a short-term appearance model of the target, which handles the drastic appearance change between consecutive frames. All these three constraints are incorporated in the proposed graph labeling algorithm such that induction and transduction, short- A nd long-term appearance models are combined, respectively. The foreground regions inferred by the proposed graph labeling method are used to guide the tracking process. Tracking results, in turn, facilitate more accurate online update by filtering out potential contaminated training samples. Both quantitative and qualitative evaluations on challenging tracking data sets show that the proposed constrained tracking algorithm performs favorably against the state-of-the-art methods. Â© 2013 IEEE.",,,
10.1007/s10586-017-0746-2,2018,"Lee S., Hooshyar D., Ji H., Nam K., Lim H.",Mining biometric data to predict programmer expertise and task difficulty,"Programming mistakes frequently waste software developersâ€?time and may lead to the introduction of bugs into their software, causing serious risks for their customers. Using the correlation between various software process metrics and defects, earlier work has traditionally attempted to spot such bug risks. However, this study departs from previous works in examining a more direct method of using psycho-physiological sensors data to detect the difficulty of program comprehension tasks and programmer level of expertise. By conducting a study with 38 expert and novice programmers, we investigated how well an electroencephalography and an eye-tracker can be utilized in predicting programmer expertise (novice/expert) and task difficulty (easy/difficult). Using data from both sensors, we could predict task difficulty and programmer level of expertise with 64.9 and 97.7% precision and 68.6 and 96.4% recall, respectively. The result shows it is possible to predict the perceived difficulty of a task and expertise level for developers using psycho-physiological sensors data. In addition, we found that while using single biometric sensor shows good results, the composition of both sensors lead to the best overall performance. Â© 2017, Springer Science+Business Media New York.",,,
10.1145/3191442.3191463,2018,"Han R., Xiao S.",Human visual scanpath prediction based on RGB-D saliency,"Human visual perception is considered as a dynamic process of information acquisition, while the visual scanpath can clearly reflect the shift of our eye fixations. In the previous study of visual attention, researchers generally do the saliency computation to predict where the regions of interest locate in the given scene, whereas less considering how our eyes saccade during the saliency generation. In this paper, we propose a novel model based on visual attention mechanism to predict the human visual scanpath of the given 3D scene. Our scanpath prediction model that can reasonably estimate the sequence of eye fixations when eyes saccade contains three important factors: RGB-D saliency computation, oculomotor biases and inhibition of return(IOR). In addition, we construct a small RGB-D eye tracking dataset with collecting eye tracking records from 91 people on 30 RGB-D images for our comparison experiments. The experiments demonstrate that our approach provides better prediction on human visual scanpath. Â© 2018 Association for Computing Machinery.",,,
10.1109/ICIP.2017.8296894,2018,"Cui W., Cui J., Zha H.",Specialized gaze estimation for children by convolutional neural network and domain adaptation,"Children's social gaze behavior modeling and evaluation has obtained increasing attentions in various research areas. In psychology research, eye gaze behavior is very important to developmental disorders diagnosis and assessment. In robotics area, gaze interaction between children and robots also draws more and more attention. However, there exists no specific gaze estimator for children in social interaction context. Current approaches usually use models trained with adults' data to estimate children's gaze. Since gaze behaviors and eye appearances of children are different from those of adults, the current approaches, especially those with free-calibration assumptions which are utilized in usual human-robot interaction systems, will result in big errors. Note that children data is difficult to collect and label, so directly learning from children data is hard to achieve. We propose a new system to solve this problem, which combines a CNN feature extractor trained from adult data and a domain adaptation unit using geodesic flow kernel to adapt the source domain (adults) classifier to the target domain (children). Our system performs well in children's gaze estimation. Â© 2017 IEEE.",,,
10.7717/peerj-cs.146,2018,"Lee J., MuÃ±oz M., Fridman L., Victor T., Reimer B., Mehler B.",Investigating the correspondence between driver head position and glance location,"The relationship between a driver's glance orientation and corresponding head rotation is highly complex due to its nonlinear dependence on the individual, task, and driving context. This paper presents expanded analytic detail and findings from an effort that explored the ability of head pose to serve as an estimator for driver gaze by connecting head rotation data with manually coded gaze region data using both a statistical analysis approach and a predictive (i.e., machine learning) approach. For the latter, classification accuracy increased as visual angles between two glance locations increased. In other words, the greater the shift in gaze, the higher the accuracy of classification. This is an intuitive but important concept that we make explicit through our analysis. The highest accuracy achieved was 83% using the method of Hidden Markov Models (HMM) for the binary gaze classification problem of (a) glances to the forward roadway versus (b) glances to the center stack. Results suggest that although there are individual differences in head-glance correspondence while driving, classifier models based on head-rotation data may be robust to these differences and therefore can serve as reasonable estimators for glance location. The results suggest that driver head pose can be used as a surrogate for eye gaze in several key conditions including the identification of high-eccentricity glances. Inexpensive driver head pose tracking may be a key element in detection systems developed to mitigate driver distraction and inattention. Â© 2018 Lee et al.",,,
10.1145/3174910.3174921,2018,"Hagihara K., Taniguchi K., Abibouraguimane I., Itoh Y., Higuchi K., Otsuka J., Sugimoto M., Sato Y.",Object-wise 3d gaze mapping in physicalworkspace,"skill in human communication. Eye behavior is an important, yet implicit communication cue. In this work, we focus on enabling people to see the users' gaze associated with objects in the 3D space, namely, we present users the history of gaze linked to real 3D objects. Our 3D gaze visualization system automatically segments objects in the workspace and projects user's gaze trajectory onto the objects in 3D for visualizing user's intention. By combining automated object segmentation and head tracking via the firstperson video from a wearable eye tracker, our system can visualize user's gaze behavior more intuitively and efficiently compared to 2D based methods and 3D methods with manual annotation. We performed an evaluation of the system to measure the accuracy of object-wise gaze mapping. In the evaluation, the system achieved 94% accuracy of gaze mapping onto 40, 30, 20, 10-centimeter cubes. We also conducted a case study of through a case study where the user looks at food products, we showed that our system was ableto predict products that the user is interested in. Â©2018 Copyright.",,,
10.1109/IIKI.2016.59,2018,"Filipovic N., Milosevic Z., Saveljic I., Nikolic D., Zdravkovic N., Kos A.",Biomechanical model for detection of vertigo disease,"Benign Paroxysmal Positional Vertigo (BPPV) is most common vestibular disorder influencing the quality of life to considerable percentage of population after the age of forty. In this study the three-dimensional biomechanical model of the semi-circular canal (SCC) is described with full 3D fluid-structure interaction of particles, wall, cupula deformation and endolymph fluid flow. Oculus Rift device was used for experimental results of head motion and eye tracking and correlation with biomechanical model. A full Navier-Stokes equations and continuity equations are used for fluid domain with Arbitrary-Lagrangian Eulerian (ALE) formulation for mesh motion. Fluid-structure interaction for fluid coupling with cupula deformation is used. Particle tracking algorithm has been used for particle motion. Different size and number of particles with their full interaction between themselves, wall and cupula deformation are used. Velocity distribution, shear stress and force from endolymph side are presented for parametric one SCC and patient specific three SCC. All the models are used for correlation with the same experimental protocols with head moving and nystagmus eye tracking. A good correlation was found with numerical simulation of membrane deflection and nystagmus response detected with tracking technology. It can be used for virtual games with detection of vestibular disorders to the users. Â© 2016 IEEE.",,,
10.1109/SSCI.2017.8285207,2018,"Castellanos J.L., Gomez M.F., Adams K.D.",Using machine learning based on eye gaze to predict targets: An exploratory study,"Play is a crucial activity for child development. Play in children with physical disabilities may be compromised due to their physical limitations, such as having difficulties reaching and manipulating objects. Assistive technology robotic systems have been used as tools for children with disabilities to play and interact with the environment. Robots have shown a positive impact on children's independence, cognitive, and social skills. The present study is the first stage of a project to develop a telerobotic haptic system, with the goal of supporting the reaching of toys during play by children with severe physical disabilities. The end goal is to provide haptic guidance towards the toys that the children want to play with. The objective of this paper was to investigate the feasibility of predicting the selection of targets in a three-block task. This prediction was based on the Point of Gaze (POG) data of five participants while performing the task using a telerobotic haptic system. Two fixation-based algorithms, longest fixation and last fixation, and two learning algorithms, a Double Q-learning and a Multi-Layer Perceptron neural network, were implemented, tested, and compared. Results showed that the learning algorithms were better at predicting the targets than the fixation-based algorithms, with above 92% accuracy. This demonstrated that the learning algorithms can be utilized for activating haptic guidance towards the targets (toys). Â© 2017 IEEE.",,,
10.1515/pjbr-2018-0002,2018,"Cazzato D., Dominio F., Manduchi R., Castro S.M.",Real-time gaze estimation via pupil center tracking,"Automatic gaze estimation not based on commercial and expensive eye tracking hardware solutions can enable several applications in the fields of human-computer interaction (HCI) and human behavior analysis. It is therefore not surprising that several related techniques and methods have been investigated in recent years. However, very few camera-based systems proposed in the literature are both real-time and robust. In this work, we propose a real-time user-calibration-free gaze estimation system that does not need person-dependent calibration, can deal with illumination changes and head pose variations, and can work with a wide range of distances from the camera. Our solution is based on a 3-D appearance-based method that processes the images from a built-in laptop camera. Real-time performance is obtained by combining head pose information with geometrical eye features to train a machine learning algorithm. Our method has been validated on a data set of images of users in natural environments, and shows promising results. The possibility of a real-time implementation, combined with the good quality of gaze tracking, make this system suitable for various HCI applications. Â© 2018 De Gruyter Open Ltd. All rights reserved.",,,
10.1109/ICUS.2017.8278368,2018,"Jie L., Jian C., Lei W.",Design of multi-mode UAV human-computer interaction system,"With continuous function expansion of military UAV, the rapid increase in the number of airborne sensors, the complexity of UAV operation is increasing. To enhance future unmanned combat aircraft control and mission planning efficiency, multi-mode UAV control method will gradually become mainstream. In this paper a Multi-Mode UAV Human-Computer Interaction(HCI) System was designed with new technology achievements of virtual reality and artificial intelligence. Based on the design principle of usability, maintainability and reliability, according to the envisaged functions, the overall design of the system was completed. The four modules that divided into the base layer and the application layer with hierarchical structure was introduced. The base layer was composed of multi-mode HCI device and data recording processing subsystem. Acting as the system hardware I/O interface, the multi-mode HCI device use multi-mode interaction of immersion display, motion capture, eye tracking and voice recognition by means of intelligent devices such as VR HMD, 3D Sensor, VR Glove and intelligent speech recognition equipment. And according to the actual situation of GCS operation, the above hardware's selection was completed. Data recording processing subsystem provides the network and database to support the underlying. Human factors engineering evaluation subsystem and UAV simulation training subsystem constitute the function application layer, realize the system functions. The whole system can provide multi-mode interaction human factors engineering validation and evaluation platform, supporting the development of new HCI design, and high-frontal manned UAV operation training environment for supporting the new CGS ground operator personalized training. Â© 2017 IEEE.",,,
10.1016/j.neucom.2017.08.039,2018,"Liu M., Wu W., Gu Z., Yu Z., Qi F., Li Y.",Deep learning based on Batch Normalization for P300 signal detection,"Detecting P300 signals from electroencephalography (EEG) is the key to establishing a P300 speller, which is a type of brainâ€“computer interface (BCI) system based on the oddball paradigm that allows users to type messages simply by controlling eye-gazes. The convolutional neural network (CNN) is an approach that has achieved good P300 detection performances. However, the standard CNN may be prone to overfitting and the convergence may be slow. To address these issues, we develop a novel CNN, termed BN3, for detecting P300 signals, where Batch Normalization is introduced in the input and convolutional layers to alleviate over-fitting, and the rectified linear unit (ReLU) is employed in the convolutional layers to accelerate training. Since our model is fully data-driven, it is capable of automatically capturing the discriminative spatio-temporal features of the P300 signal. The results obtained on previous BCI competition P300 data sets show that BN3 both achieves the state-of-the-art character recognition performance and that it outperforms existing detection approaches with small flashing epoch numbers. BN3 can be used to improve the character recognition performance in P300 speller systems. Â© 2017 Elsevier B.V.",,,
,2018,"Lee K., Kim H., Suh C.",Simulated+unsupervised learning with adaptive data generation and bidirectional mappings,"Collecting a large dataset with high quality annotations is expensive and time-consuming. Recently, Shrivastava et al. (2017) propose Simulated+Unsupervised (S+U) learning: It first learns a mapping from synthetic data to real data, translates a large amount of labeled synthetic data to the ones that resemble real data, and then trains a learning model on the translated data. Bousmalis et al. (2017b) propose a similar framework that jointly trains a translation mapping and a learning model. While these algorithms are shown to achieve the state-of-the-art performances on various tasks, it may have a room for improvement, as they do not fully leverage flexibility of data simulation process and consider only the forward (synthetic to real) mapping. Inspired by this limitation, we propose a new S+U learning algorithm, which fully leverage the flexibility of data simulators and bidirectional mappings between synthetic and real data. We show that our approach achieves the improved performance on the gaze estimation task, outperforming (Shrivastava et al., 2017). Â© Learning Representations, ICLR 2018 - Conference Track Proceedings.All right reserved.",,,
,2018,"Matsumoto T., Kusafuka K., Nakamura H., Hamagishi G., Yoshimoto K., Takahashi H.",Low crosstalk Glassless 3D HUD with expanded viewing area in all directions using novel eye tracking system,"We propose Glassless 3D HUD that enables binocular virtual image stereopsis with low crosstalk. In this system, 3D image processing algorithm and the crosstalk reduction method which perform processing based on the position of the driver's eye sensed by the driver monitor camera are applied. Â© 2018 International Display Workshops. All rights reserved.",,,
,2018,"Nakamura H., Hamagishi G., Yoshimoto K., Takahashi H., Matsumoto T., Kusafuka K.",A novel eye tracking system to expand viewing area in all directions for glasses-free 3D display displayable in both portrait and landscape modes,We propose a novel eye tracking system to expand the viewing area in all directions for glasses-free 3D display displayable in both portrait and landscape modes. Its viewing area is extremely expanded by dividing single screen into multiple areas and controlling binocular images positions of each area. Â© 2018 International Display Workshops. All rights reserved.,,,
,2018,"Bambach S., Crandall D.J., Smith L.B., Yu C.",Toddler-inspired visual object learning,"Real-world learning systems have practical limitations on the quality and quantity of the training datasets that they can collect and consider. How should a system go about choosing a subset of the possible training examples that still allows for learning accurate, generalizable models? To help address this question, we draw inspiration from a highly efficient practical learning system: the human child. Using head-mounted cameras, eye gaze trackers, and a model of foveated vision, we collected first-person (egocentric) images that represent a highly accurate approximation of the ""training data"" that toddlers' visual systems collect in everyday, naturalistic learning contexts. We used state-of-the-art computer vision learning models (convolutional neural networks) to help characterize the structure of these data, and found that child data produce significantly better object models than egocentric data experienced by adults in exactly the same environment. By using the CNNs as a modeling tool to investigate the properties of the child data that may enable this rapid learning, we found that child data exhibit a unique combination of quality and diversity, with not only many similar large, high-quality object views but also a greater number and diversity of rare views. This novel methodology of analyzing the visual ""training data"" used by children may not only reveal insights to improve machine learning, but also may suggest new experimental tools to better understand infant learning in developmental psychology. Â© 2018 Curran Associates Inc..All rights reserved.",,,
10.4108/eai.28-2-2018.154142,2018,"Milosevic Z., Saveljic I., Nikolic D., Zdravkovic N., Filipovic N., Vidanovic N.",Three-Dimensional Computer Model of Benign Paroxysmal Positional Vertigo in the Semi-Circular Canal,"Benign Paroxysmal Positional Vertigo (BPPV) is the most common vestibular disorder. In this paper we tried to investigate a model of the semi-circular canal (SCC) with parametrically defined dimension and full 3D three SCC from patient-specific 3D reconstruction. Full Navier-Stokes equations and continuity equations are used for fluid domain with Arbitrary-Lagrangian Eulerian (ALE) formulation for mesh motion. Fluid-structure interaction for fluid coupling with cupula deformation is used. Particle tracking algorithm has been used for particle motion. Velocity distribution, shear stress and force from endolymph side are presented for one parametric SCC and three patient-specific SCC. All models are used for correlation with the same experimental protocols with head moving and nystagmus eye tracking. Â© 2018 Zarko Milosevic et al.",,,
,2018,"GonzÃ¡lez-GarduÃ±o A.V., SÃ¸gaard A.",Learning to predict readability using eye-movement data from natives and learners,"Readability assessment can improve the quality of assisting technologies aimed at language learners. Eye-tracking data has been used for both inducing and evaluating general-purpose NLP/AI models, and below we show that unsurprisingly, gaze data from language learners can also improve multi-task readability assessment models. This is unsurprising, since the gaze data records the reading difficulties of the learners. Unfortunately, eye-tracking data from language learners is often much harder to obtain than eye-tracking data from native speakers. We therefore compare the performance of deep learning readability models that use native speaker eye movement data to models using data from language learners. Somewhat surprisingly, we observe no significant drop in performance when replacing learners with natives, making approaches that rely on native speaker gaze information, more scalable. In other words, our finding is that language learner difficulties can be efficiently estimated from native speakers, which suggests that, more generally, readily available gaze data can be used to improve educational NLP/AI models targeted towards language learners. Copyright Â© 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,,
10.1007/978-3-030-05171-6_23,2018,"Tiwari A., Pal R.",Gaze-Based Graphical Password Using Webcam,"Authentication refers to verification of the identity of an user. There exist various types of authentication techniques, starting from simple password based authentication up to behavior biometric based authentication. In this paper, a new way of authentication is proposed where the user provides her password through eye gaze. It is based on graphical password scheme where she can choose her password from a large image data set. At the time of authentication, she needs to recall it and look at the chosen passcode appearing in a display in correct sequence. The method uses a machine learning technique where a convolutional neural network is used to determine gaze locations using inputs from a simple web camera. It takes her cropped eye as input and provides gaze location as output. Proposed method is cost effective solution as gaze tracking is done through a simple web camera. The proposed method is also free from attacks such as shoulder surfing, smudge, brute-force attacks. Experiments have been carried out to validate the system. It has been observed to perform accurately for all the volunteers. Â© 2018, Springer Nature Switzerland AG.",,,
10.1117/12.2325050,2018,"Sharma A., Kolay S., Ghosh J.K.",Dominance of perceptual grouping over functional category: An eye tracking study of high-resolution satellite images,"The presented study is an attempt to determine the object grouping propensity of human visual system for further human-like analysis of High-Resolution Satellite (HRS) image by using an eye tracker. Perception, which is a basic characteristic of the human visual system, enables to part the visual stimuli into meaningful parts. This parting is either based on perceptual or functional grouping. However, these perceptual groups are often different from functional categories. All objects from one functional category are perceived equivalent, irrespective of their physical appearance variation. The recent advancements in eye tracking technology have provided an excellent modality to articulate the eye movement data with inner psychological factors. In this way, the presented study has exploited the fixation data to understand the perceptual organization for HRS images. The outcomes of this study has provided the analytical solution for grouping tendency of humans along with the heat maps as evidence. Â© 2018 SPIE.",,,
10.1007/978-3-030-05587-5_27,2018,"Schweikert C., Gobin L., Xie S., Shimojo S., Frank Hsu D.",Preference prediction based on eye movement using multi-layer combinatorial fusion,"Face image preference is influenced by many factors and can be detected by analyzing eye movement data. When comparing two face images, our gaze shifts within and between the faces. Eye tracking data can give us insights into the cognitive processes involved in forming a preference. In this paper, a gaze tracking dataset is analyzed using three machine learning algorithms (MLA): AdaBoost, Random Forest, and Mixed Group Ranks (MGR) as well as a newly developed machine learning framework called Multi-Layer Combinatorial Fusion (MCF) to predict a subjectâ€™s face image preference. Attributes constructed from the dataset are treated as input scoring systems. MCF involves a series of layers that consist of expansion and reduction processes. The expansion process involves performing exhaustive score and rank combinations, while the reduction process uses performance and diversity to select a subset of systems that will be passed onto the next layer of analysis. Performance and cognitive diversity are used in weighted scoring system combinations and system selection. The results outperform the Mixed Group Ranks algorithm, as well as our previous work using pairwise scoring system combinations. Â© 2018, Springer Nature Switzerland AG.",,,
10.3233/ICA-180584,2018,"Vera-Olmos F.J., Pardo E., Melero H., Malpica N.",DeepEye: Deep convolutional network for pupil detection in real environments,"Robust identification and tracking of the pupil provides key information that can be used in several applications such as controlling gaze-based HMIs (human machine interfaces), designing new diagnostic tools for brain diseases, improving driver safety, detecting drowsiness, performing cognitive research, among others. We propose a deep convolutional neural network for eye-Tracking based on atrous convolutions and spatial pyramids. DeepEye is able to handle real world problems such as varying illumination, blurring and reflections. The proposed network was trained and evaluated on 94,000 images taken from 24 data sets recorded in real world scenarios. DeepEye outperforms previous eye-Tracking methods tested with these data sets. It improves the results of the current state of the art in a 26%, achieving an accuracy of more than 70% in almost every data set in terms of percentage of pupils detected with a distance error lower than 5 pixels. DeepEye can be downloaded at: https://github.com/Fjaviervera/DeepEye. Â© 2019-IOS Press and the authors. All rights reserved.",,,
10.1007/978-3-030-05204-1_18,2018,"Ortiz J.S., Palacios-Navarro G., Carvajal C.P., Andaluz V.H.",3D virtual path planning for people with amyotrophic lateral sclerosis through standing wheelchair,"This article presents the development of an autonomous control system of an electric standing wheelchair for people with amyotrophic lateral sclerosis. The proposed control scheme is based on the autonomous maneuverability of the standing wheelchair, for which a path planner is implemented to which the desired 3D position is defined through the eye-tracking sensor. The eye-tracking is implemented in a virtual reality environment which allows selecting the desired position of the standing wheelchair. The wheelchair has a standing system that allows the user to position himself on the Z axis according to his needs independently of the displacement in the X-Y plane with respect to the inertial reference system <R>. To verify the performance of the proposed control scheme, several experimental tests are carried out. Â© 2018, Springer Nature Switzerland AG.",,,
10.3233/978-1-61499-898-3-249,2018,"Qiu S., Han T., Rauterberg M., Hu J.",Impact of simulated gaze gestures on social interaction for people with visual impairments,"Gaze and eye contact have important social meanings in our daily lives. The sighted often uses gaze gestures in communication to convey nonverbal information that a blind interlocutor cannot access and react to. In many examples, blind peopleâ€™s eyes are unattractive, and often with deformities, which makes the eye appearance less appealing to the sighted. All of these factors influence the smooth face-to-face communication between the blind and sighted people, which leads to blind peopleâ€™s poor adaptions in the communication. We implemented a working prototype, namely E-Gaze (glasses), an assistive device based on an eye tracking system. E-Gaze attempts to simulate the natural gaze for blind people, especially establishing the â€œeye contactâ€?between the blind and sighted people to enhance the engagement in the face-to-face communication. The interactive gaze behaviors of the E-Gaze are based on a model that combines a turn-taking strategy and the eye-contact mechanism. In order to test the impact of the interactive gaze model in the face-to-face communication, we conducted an experiment with sixteen participants. In the user experiment, participants had a monologue with a dummy wearing the E-Gaze. Two monologues took place under two experimental conditions (i.e., Interactive Gaze and Random Gaze) with counter balancing to avoid carry-over effects. Results well support the hypothesis that the interactive gaze model of the E-Gaze can enable the sighted to feel attention from the listener, enhancing the level of engagement in the face-to-face communication. We also obtain insights and design implications from participantsâ€?comments. Â© 2018 The authors and IOS Press.",,,
10.1007/978-3-030-00767-6_27,2018,"Wu J., Zhong S.-H., Ma Z., Heinen S.J., Jiang J.",Gaze aware deep learning model for video summarization,"Video summarization is an ideal tool for skimming videos. Previous computational models extract explicit information from the input video, such as visual appearance, motion or audio information, in order to generate informative summaries. Eye gaze information, which is an implicit clue, has proved useful for indicating important content and the viewerâ€™s interest. In this paper, we propose a novel gaze-aware deep learning model for video summarization. In our model, the position and velocity of the observersâ€?raw eye movements are processed by the deep neural network to indicate the usersâ€?preferences. Experiments on two widely used video summarization datasets show that our model is more proficient than state-of-the-art methods in summarizing video for characterizing general preferences as well as for personal preferences. The results provide an innovative and improved algorithm for using gaze information in video summarization. Â© Springer Nature Switzerland AG 2018.",,,
10.1145/3197517.3201283,2018,"Kim H., Garrido P., Tewari A., Xu W., Thies J., Niessner M., PÃ©rez P., Richardt C., ZollhÃ¶fer M., Theobalt C.",Deep video portraits,"We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted to manipulations of facial expressions only, we are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor. The realism in this rendering-to-video transfer is achieved by careful adversarial training, and as a result, we can create modified target videos that mimic the behavior of the synthetically-created input. In order to enable source-to-target video re-animation, we render a synthetic target video with the reconstructed head animation parameters from a source video, and feed it into the trained network - thus taking full control of the target. With the ability to freely recombine source and target parameters, we are able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background. For instance, we can reenact the full head using interactive user-controlled editing, and realize high-fidelity visual dubbing. To demonstrate the high quality of our output, we conduct an extensive series of experiments and evaluations, where for instance a user study shows that our video edits are hard to detect. Â© 2018 Association for Computing Machinery.",,,
10.1109/ACCESS.2018.2879619,2018,"Dawood A., Turner S., Perepa P.",Affective computational model to extract natural affective states of students with asperger syndrome (AS) in computer-based learning environment,"This paper was inspired by looking at the central role of emotion in the learning process, its impact on students' performance; as well as the lack of affective computing models to detect and infer affective-cognitive states in real time for students with and without Asperger Syndrome (AS). This model overcomes gaps in other models that were designed for people with autism, which needed the use of sensors or physiological instrumentations to collect data. The model uses a webcam to capture students' affective-cognitive states of confidence, uncertainty, engagement, anxiety, and boredom. These states have a dominant effect on the learning process. The model was trained and tested on a natural-spontaneous affective dataset for students with and without AS, which was collected for this purpose. The dataset was collected in an uncontrolled environment and included variations in culture, ethnicity, gender, facial and hairstyle, head movement, talking, glasses, illumination changes, and background variation. The model structure used deep learning (DL) techniques like convolutional neural network and long short-term memory. The DL is the-state-of-art tool that used to reduce data dimensionality and capturing non-linear complex features from simpler representations. The affective model provides reliable results with accuracy 90.06%. This model is the first model to detected affective states for adult students with AS without physiological or wearable instruments. For the first time, the occlusions in this model, like hand over face or head were considered an important indicator for affective states like boredom, anxiety, and uncertainty. These occlusions have been ignored in most other affective models. The essential information channels in this model are facial expressions, head movement, and eye gaze. The model can serve as an aided-technology for tutors to monitor and detect the behaviors of all students at the same time and help in predicting negative affective states during learning process. Â© 2013 IEEE.",,,
10.24963/ijcai.2018/214,2018,"Le Minh T., Shimizu N., Miyazaki T., Shinoda K.",Deep learning based multi-modal addressee recognition in visual scenes with utterances,"With the widespread use of intelligent systems, such as smart speakers, addressee recognition has become a concern in human-computer interaction, as more and more people expect such systems to understand complicated social scenes, including those outdoors, in cafeterias, and hospitals. Because previous studies typically focused only on pre-specified tasks with limited conversational situations such as controlling smart homes, we created a mock dataset called Addressee Recognition in Visual Scenes with Utterances (ARVSU) that contains a vast body of image variations in visual scenes with an annotated utterance and a corresponding addressee for each scenario. We also propose a multi-modal deep-learning-based model that takes different human cues, specifically eye gazes and transcripts of an utterance corpus, into account to predict the conversational addressee from a specific speaker's view in various real-life conversational scenarios. To the best of our knowledge, we are the first to introduce an end-to-end deep learning model that combines vision and transcripts of utterance for addressee recognition. As a result, our study suggests that future addressee recognition can reach the ability to understand human intention in many social situations previously unexplored, and our modality dataset is a first step in promoting research in this field. Â© 2018 International Joint Conferences on Artificial Intelligence. All right reserved.",,,
10.1007/978-3-030-01264-9_37,2018,"Jiang L., Xu M., Liu T., Qiao M., Wang Z.",Deepvs: A deep learning based video saliency prediction approach,"In this paper, we propose a novel deep learning based video saliency prediction method, named DeepVS. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which includes 32 subjectsâ€?fixations on 538 videos. We find from LEDOV that human attention is more likely to be attracted by objects, particularly the moving objects or the moving parts of objects. Hence, an object-to-motion convolutional neural network (OM-CNN) is developed to predict the intra-frame saliency for DeepVS, which is composed of the objectness and motion subnets. In OM-CNN, cross-net mask and hierarchical feature normalization are proposed to combine the spatial features of the objectness subnet and the temporal features of the motion subnet. We further find from our database that there exists a temporal correlation of human attention with a smooth saliency transition across video frames. We thus propose saliency-structured convolutional long short-term memory (SS-ConvLSTM) network, using the extracted features from OM-CNN as the input. Consequently, the inter-frame saliency maps of a video can be generated, which consider both structured output with center-bias and cross-frame transitions of human attention maps. Finally, the experimental results show that DeepVS advances the state-of-the-art in video saliency prediction. Â© 2018, Springer Nature Switzerland AG.",,,
10.24963/ijcai.2018/222,2018,"Shan H., Liu Y., Stefanov T.",A simple convolutional neural network for accurate P300 detection and character spelling in brain computer interface,"A Brain Computer Interface (BCI) character speller allows human-beings to directly spell characters using eye-gazes, thereby building communication between the human brain and a computer. Convolutional Neural Networks (CNNs) have shown better performance than traditional machine learning methods for BCI signal recognition and its application to the character speller. However, current CNN architectures limit further accuracy improvements of signal detection and character spelling and also need high complexity to achieve competitive accuracy, thereby preventing the use of CNNs in portable BCIs. To address these issues, we propose a novel and simple CNN which effectively learns feature representations from both raw temporal information and raw spatial information. The complexity of the proposed CNN is significantly reduced compared with state-of-the-art CNNs for BCI signal detection. We perform experiments on three benchmark datasets and compare our results with those in previous research works which report the best results. The comparison shows that our proposed CNN can increase the signal detection accuracy by up to 15.61% and the character spelling accuracy by up to 19.35%. Â© 2018 International Joint Conferences on Artificial Intelligence. All right reserved.",,,
10.1007/978-3-030-01264-9_7,2018,"Cheng Y., Lu F., Zhang X.",Appearance-based gaze estimation via evaluation-guided asymmetric regression,"Eye gaze estimation has been increasingly demanded by recent intelligent systems to accomplish a range of interaction-related tasks, by using simple eye images as input. However, learning the highly complex regression between eye images and gaze directions is nontrivial, and thus the problem is yet to be solved efficiently. In this paper, we propose the Asymmetric Regression-Evaluation Network (ARE-Net), and try to improve the gaze estimation performance to its full extent. At the core of our method is the notion of â€œtwo eye asymmetryâ€?observed during gaze estimation for the left and right eyes. Inspired by this, we design the multi-stream ARE-Net; one asymmetric regression network (AR-Net) predicts 3D gaze directions for both eyes with a novel asymmetric strategy, and the evaluation network (E-Net) adaptively adjusts the strategy by evaluating the two eyes in terms of their performance during optimization. By training the whole network, our method achieves promising results and surpasses the state-of-the-art methods on multiple public datasets. Â© 2018, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-01261-8_44,2018,"Park S., Spurr A., Hilliges O.",Deep pictorial gaze estimation,"Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality. Â© Springer Nature Switzerland AG 2018.",,,
10.1007/978-3-030-01225-0_38,2018,"Brau E., Guan J., Jeffries T., Barnard K.",Multiple-Gaze Geometry: Inferring Novel 3D Locations from Gazes Observed in Monocular Video,"We develop using person gaze direction for scene understanding. In particular, we use intersecting gazes to learn 3D locations that people tend to look at, which is analogous to having multiple camera views. The 3D locations that we discover need not be visible to the camera. Conversely, knowing 3D locations of scene elements that draw visual attention, such as other people in the scene, can help infer gaze direction. We provide a Bayesian generative model for the temporal scene that captures the joint probability of camera parameters, locations of people, their gaze, what they are looking at, and locations of visual attention. Both the number of people in the scene and the number of extra objects that draw attention are unknown and need to be inferred. To execute this joint inference we use a probabilistic data association approach that enables principled comparison of model hypotheses. We use MCMC for inference over the discrete correspondence variables, and approximate the marginalization over continuous parameters using the Metropolis-Laplace approximation, using Hamiltonian (Hybrid) Monte Carlo for maximization. As existing data sets do not provide the 3D locations of what people are looking at, we contribute a small data set that does. On this data set, we infer what people are looking at with 59% precision compared with 13% for a baseline approach, and where those objects are within about 0.58 m. Â© 2018, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-01249-6_21,2018,"Fischer T., Chang H.J., Demiris Y.",RT-GENE: Real-time eye gaze estimation in natural environments,"In this work, we consider the problem of robust gaze estimation in natural environments. Large camera-to-subject distances and high variations in head pose and eye gaze angles are common in such environments. This leads to two main shortfalls in state-of-the-art methods for gaze estimation: hindered ground truth gaze annotation and diminished gaze estimation accuracy as image resolution decreases with distance. We first record a novel dataset of varied gaze and head pose images in a natural environment, addressing the issue of ground truth annotation by measuring head pose using a motion capture system and eye gaze using mobile eyetracking glasses. We apply semantic image inpainting to the area covered by the glasses to bridge the gap between training and testing images by removing the obtrusiveness of the glasses. We also present a new real-time algorithm involving appearance-based deep convolutional neural networks with increased capacity to cope with the diverse images in the new dataset. Experiments with this network architecture are conducted on a number of diverse eye-gaze datasets including our own, and in cross dataset evaluations. We demonstrate state-of-the-art performance in terms of estimation accuracy in all experiments, and the architecture performs well even on lower resolution images. Â© Springer Nature Switzerland AG 2018.",,,
10.1007/978-3-030-00563-4_26,2018,"Li N., Zhao X., Ma B., Zou X.",A Visual Attention Model Based on Human Visual Cognition,"Understanding where humans look in a scene is significant for many applications. Researches on neuroscience and cognitive psychology show that human brain always pays attention on special areas when they observe an image. In this paper, we recorded and analyzed human eye-tracking data, we found that these areas mainly were focus on semantic objects. Inspired by neuroscience, deep learning concept is proposed. Fully Convolutional Neural Networks (FCN) as one of methods of deep learning can solve image objects segmentation at semantic level efficiently. So we bring forth a new visual attention model which uses FCN to stimulate the cognitive processing of human free observing a natural scene and fuses attractive low-level features to predict fixation locations. Experimental results demonstrated our model has apparently advantages in biology. Â© 2018, Springer Nature Switzerland AG.",,,
10.1007/978-3-030-01240-3_4,2018,"Recasens A., Kellnhofer P., Stent S., Matusik W., Torralba A.",Learning to zoom: A saliency-based sampling layer for neural networks,"We introduce a saliency-based distortion layer for convolutional neural networks that helps to improve the spatial sampling of input data for a given task. Our differentiable layer can be added as a preprocessing block to existing task networks and trained altogether in an end-to-end fashion. The effect of the layer is to efficiently estimate how to sample from the original data in order to boost task performance. For example, for an image classification task in which the original data might range in size up to several megapixels, but where the desired input images to the task network are much smaller, our layer learns how best to sample from the underlying high resolution data in a manner which preserves task-relevant information better than uniform downsampling. This has the effect of creating distorted, caricature-like intermediate images, in which idiosyncratic elements of the image that improve task performance are zoomed and exaggerated. Unlike alternative approaches such as spatial transformer networks, our proposed layer is inspired by image saliency, computed efficiently from uniformly downsampled data, and degrades gracefully to a uniform sampling strategy under uncertainty. We apply our layer to improve existing networks for the tasks of human gaze estimation and fine-grained object classification. Code for our method is available in: http://github.com/recasens/Saliency-Sampler. Â© Springer Nature Switzerland AG 2018.",,,
,2018,"Chopade P., Khan S., Stoeffler K., Edwards D., Rosen Y., Von Davier A.",Framework for effective teamwork assessment in collaborative learning and problem solving,"This paper presents an interactive team collaborative learning and problem-solving (ITCLP) framework for effective teamwork learning and assessment. Modeling the dynamics of a collaborative, networked system involving multimodal data presents many challenges. This framework incorporates an Artificial Intelligence (AI), a Machine Learning (ML) and computational psycho- metrics (CP) based methodology, system architecture, and algorithms to find pat- terns of learning, interactions, relationships, and effective teamwork assessment from a collaborative learning environment (CLE). Collaborative learning may take place in peer-to-peer or in large groups, to discuss concepts, or find solutions to real-time problems or working on situational judgement task (SJT). Intelligent Tutoring Systems (ITSs) have been mostly used as a supportive system for the varied needs of individual learners. The ITCLP framework enables development of ITSs for team tutoring and facilitates collaborative problem solving (CPS) by creating interactions between team members. Our team model maps team knowledge, skills, interactions, behaviors, and shared knowledge of team tasks, and performance. We will collect the team interaction log data, user eye tracking, and user portrait video/audio and will map team skills evidence based on CPS, a broad range of cross-cutting capabilities, which is part of an even broader Holistic Framework (HF) proposed by Camara and colleagues [1]. Â© 2018 CEUR-WS. All Rights Reserved.",,,
10.1007/978-3-030-00111-7_25,2018,"Stauden S., Barz M., Sonntag D.",Visual search target inference using bag of deep visual words,"Visual Search target inference subsumes methods for predicting the target object through eye tracking. A person intents to find an object in a visual scene which we predict based on the fixation behavior. Knowing about the search target can improve intelligent user interaction. In this work, we implement a new feature encoding, the Bag of Deep Visual Words, for search target inference using a pre-trained convolutional neural network (CNN). Our work is based on a recent approach from the literature that uses Bag of Visual Words, common in computer vision applications. We evaluate our method using a gold standard dataset. The results show that our new feature encoding outperforms the baseline from the literature, in particular, when excluding fixations on the target. Â© Springer Nature Switzerland AG 2018.",,,
10.2352/ISSN.2470-1173.2018.04.SDA-413,2018,"Takahashi Y., Yendo T.",Study of eye tracking type super multi-view display using time division multiplexing,"In recent year, the Head-up display is studied actively. Among them, 3D HUD attracts rising attention. In HUD application, it is assumed that stereoscopic image is displayed at far distance. Super Multi-View display provides a smooth parallax. Even if 3D image is at far distance, it be able to display 3D image which has appropriate depth. In previous studies, high resolution required for SMV. However, there are restrictions to increase the resolution. Therefore, we propose a novel SMV display using time division multiplexing and eye tracking techniques. Our new system is not required high resolution display. The proposed display consists of DMD and light source array. The ray from the light source array is reflected at DMD, and form an image at the vicinity of the pupil. The position forming an image depends on light source position. The image which is display on the DMD is changed corresponding to the focal point. To confirm the principle of the proposed method, we experiment about creating a viewing zone only in the vicinity of the pupil. From the result, we confirmed 8 viewpoints in the horizontal direction at 18.8 mm viewing zone. Â© 2018, Society for Imaging Science and Technology.",,,
10.18517/ijaseit.8.4.6500,2018,"Mahardika W., Wibirama S., Ferdiana R., Kusumawardani S.S.",A novel user experience study of parallax scrolling using eye tracking and user experience questionnaire,"Parallax scrolling technique is being devoted as a unique and an innovative trend in the web design. Parallax scrolling provides 3D perception on a web page. Previous works observed user experience issues of parallax scrolling merely based on subjective questionnaires. Their findings leave a research question whether the results are valid, as participants may perceive a written questionnaire differently. Additionally, bias and ambiguity in the questionnaire can affect the research results significantly. To solve this research problem, we present a novel user experience study of parallax scrolling in storytelling and online shop website using eye tracking and User Experience Questionnaire (UEQ). Forty (N=40) participant joined the experiment on a voluntary basis. Each participant only interacted with one out of two websites (storytelling or online shop) and only one effect (with or without parallax scrolling). We found that parallax scrolling affected UEQ score of Attractiveness of the storytelling website (p < 0.05). Our findings suggest that parallax scrolling improves user engagement in storytelling website. We also observed that the participants spent time almost two times faster to find an object of interest in an online shop with parallax scrolling compared with the similar task in an online shop without parallax scrolling (p < 0.05). We thus argue that parallax scrolling is useful during interacting with particular websites that require visual object localization. In future, web designers should consider the appropriate usage of parallax scrolling to optimize user experience while avoiding additional distraction caused by this technique. Â© IJASEIT.",,,
10.1117/12.2304868,2018,"Wan Q., Rajeev S., Kaszowska A., Panetta K., Taylor H.A., Agaian S.",Fixation oriented object segmentation using mobile eye tracker,"Eye tracking technology allows researchers to monitor position of the eye and infer one's gaze direction, which is used to understand the nature of human attention within psychology, cognitive science, marketing and artificial intelligence. Commercially available head-mounted eye trackers allow researchers to track pupil movements (saccades and fixations) using infrared camera and capture the field of vision by a front-facing scene camera. The wearable eye tracker opened a new way to research in unconstrained environment settings; however, the recorded scene video typically has non-uniform illumination, low quality image frames, and moving scene objects. One of the most important tasks for analyzing the recorded scene video data is finding the boundary between different objects in a single frame. This paper presents a multi-level fixation-oriented object segmentation method (MFoOS) to solve the above challenges in segmenting the scene objects in video data collected by the eye tracker in order to support cognition research. MFoOS shows its advancement in position-invariance, illumination, noise tolerance and is task-driven. The proposed method is tested using real-world case studies designed by our team of psychologists focused on understanding visual attention in human problem solving. The extensive computer simulation demonstrates the method's accuracy and robustness for fixation-oriented object segmentation. Moreover, a deep-learning image semantic segmentation combining MFoOS results as label data was explored to demonstrate the possibility of on-line deployment of eye tracker fixation-oriented object segmentation. Â© COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.",,,
10.1111/cgf.13355,2018,"Wood E., BaltruÅ¡aitis T., Morency L.-P., Robinson P., Bulling A.",Gazedirector: Fully articulated eye gaze redirection in video,"We present GazeDirector, a new approach for eye gaze redirection that uses model-fitting. Our method first tracks the eyes by fitting a multi-part eye region model to video frames using analysis-by-synthesis, thereby recovering eye region shape, texture, pose, and gaze simultaneously. It then redirects gaze by 1) warping the eyelids from the original image using a model-derived flow field, and 2) rendering and compositing synthesized 3D eyeballs onto the output image in a photorealistic manner. GazeDirector allows us to change where people are looking without person-specific training data, and with full articulation, i.e. we can precisely specify new gaze directions in 3D. Quantitatively, we evaluate both model-fitting and gaze synthesis, with experiments for gaze estimation and redirection on the Columbia gaze dataset. Qualitatively, we compare GazeDirector against recent work on gaze redirection, showing better results especially for large redirection angles. Finally, we demonstrate gaze redirection on YouTube videos by introducing new 3D gaze targets and by manipulating visual behavior. Â© 2017 The Authors and The Eurographics Association and John Wiley & Sons Ltd.",,,
,2018,"Parikh S., Kalva H.",Eye gaze feature classification for predicting levels of learning,"E-Learning courses reach online to millions worldwide. Amidst the geo-flexibility of registering students, the main challenges are instructor feedback and student retention. Ability to predict difficult content in real time enables eLearning systems to adapt content to students' needs dynamically. Recently, we examined eye responses as an indicator of levels of learning and introduced a non-parametric, non-probabilistic and statistical feature weighted linguistics classifier (FWLC) capable of predicting difficult words (terms) and concepts. FWLC achieved 85% accuracy for predicting levels of learning of big words using eye responses. In this paper, we analyze the performance of FWLC with five machine learning classifiers. FWLC has a higher true positive rate (TPR) and a lower ratio of FNR/FPR (the novel is a positive class). FWLC achieves a TPR gain of 43% over the best performing machine learning classifier. Prediction accuracy of FWLC for big words is lower by 6.6% than the best performing machine learning classifier. However, this accuracy tradeoff is worth the higher TPR of FWLC as the objective is to predict novel words (positive class) more accurately so that content can adapt to student's need. Â© 2018 Copyright held by the owner/author(s).",,,
10.1007/978-3-319-92052-8_7,2018,"Nagamune K., Takata K.",Analysis of human motion and cognition ability with virtual reality system: Basic mechanism of human response,"When grasping an object, a human needs to recognize the object. In general, after the center of gravity of the object is recognized from the object shape, the human grasps a position close to the center of gravity. This research analyzes the relationship between the object shape and the sight trajectory until grasping. The proposed method traces finger motions when grasping the virtual 3D objects displayed on a screen by using finger motion capture device. In the motion, the sight trajectory is also measured and analyzed by using the eye tracking device. We conducted experiments with five subjects and analyzed the relationship between the variation of the line of sight trajectory and the size of the grasped object. Â© Springer International Publishing AG, part of Springer Nature 2018.",,,
10.1007/978-3-319-92049-8_10,2018,"Ahmad B.I., Langdon P.M., Godsill S.J.","Stabilising touch interactions in cockpits, aerospace, and vibrating environments","Incorporating touch screen interaction into cockpit flight systems is increasingly gaining traction given its several potential advantages to design as well as usability to pilots. However, perturbations to the user input are prevalent in such environments due to vibrations, turbulence and high accelerations. This poses particular challenges for interacting with displays in the cockpit, for example, accidental activation during turbulence or high levels of distraction from the primary task of airplane control to accomplish selection tasks. On the other hand, predictive displays have emerged as a solution to minimize the effort as well as cognitive, visual and physical workload associated with using in-vehicle displays under perturbations, induced by road and driving conditions. This technology employs gesture tracking in 3D and potentially eye-gaze as well as other sensory data to substantially facilitate the acquisition (pointing and selection) of an interface component by predicting the item the user intents to select on the display, early in the movements towards the screen. A key aspect is utilising principled Bayesian modelling to incorporate and treat the present perturbation, thus, it is a software-based solution that showed promising results when applied to automotive applications. This paper explores the potential of applying this technology to applications in aerospace and vibrating environments in general and presents design recommendations for such an approach to enhance interactions accuracy as well as safety. Â© Springer International Publishing AG, part of Springer Nature 2018.",,,
,2018,"Kim Y.-T., Seo J., Seo W., Sung G., Kim Y., Song H., An J., Choi C.-S., Kim S., Kim H., Kim Y., Kim Y., Lee H.-S.",Holographic augmented reality head-up display with eye tracking and steering light source,"We realized a holographic head-up display using a steering light source with eye position tracking. It can represent a real augmented reality which perfectly matches virtual graphic images to the real world. Further, for the determination of the position of the light source, 3D calibration method is proposed. copyright Â© 2016 Society of Information Display. All rights reserved.",,,
10.3389/frobt.2018.00025,2018,"Fathaliyan A.H., Wang X., Santos V.J.",Exploiting three-dimensional gaze tracking for action recognition during bimanual manipulation to enhance human-robot collaboration,"Human-robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze-object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The ""gaze object sequence"" was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments. Â© 2018 Haji Fathaliyan, Wang and Santos.",,,
10.1111/cgf.13353,2018,"LavouÃ© G., Cordier F., Seo H., Larabi M.-C.",Visual attention for rendered 3D shapes,"Understanding the attentional behavior of the human visual system when visualizing a rendered 3D shape is of great importance for many computer graphics applications. Eye tracking remains the only solution to explore this complex cognitive mechanism. Unfortunately, despite the large number of studies dedicated to images and videos, only a few eye tracking experiments have been conducted using 3D shapes. Thus, potential factors that may influence the human gaze in the specific setting of 3D rendering, are still to be understood. In this work, we conduct two eye-tracking experiments involving 3D shapes, with both static and time-varying camera positions. We propose a method for mapping eye fixations (i.e., where humans gaze) onto the 3D shapes with the aim to produce a benchmark of 3D meshes with fixation density maps, which is publicly available. First, the collected data is used to study the influence of shape, camera position, material and illumination on visual attention. We find that material and lighting have a significant influence on attention, as well as the camera path in the case of dynamic scenes. Then, we compare the performance of four representative state-of-the-art mesh saliency models in predicting ground-truth fixations using two different metrics. We show that, even combined with a center-bias model, the performance of 3D saliency algorithms remains poor at predicting human fixations. To explain their weaknesses, we provide a qualitative analysis of the main factors that attract human attention. We finally provide a comparison of human-eye fixations and Schelling points and show that their correlation is weak. Â© 2018 The Author(s)and 2018 The Eurographics Association and John Wiley & Sons Ltd.",,,
10.1007/978-3-319-93846-2_94,2018,Ching-En C.,Metacognitive experience modeling using eye-tracking,"Metacognitive experience (ME) is one of the key facets of metacognition, which serves a critical cuing function in the process of self-regulated learning process. However, the study of ME is hindered by its subjective and implicit nature of and the challenges that are associated with accessing such experiences. In exploring such experiences, eye-tracking offers certain advantages over self-reporting methods. However, to date most studies tend to focus on utilizing eye-tracking to explore metacognitive skills (MS) rather than ME, with those that do explore ME also tending to require participants to self-report rather than relying on observation of possible behavioural indicators of such metacognitive processes. Based on previous works in this field, the research proposed is based on the hypothesis that eye-tracking data can provide a crucial objective measure of learnersâ€?implicit ME processes. The research will also investigate the extent to which such data can serve as the basis for automatically predicting the occurrence and intensity of ME during learning using machine learning, in a way that can support the delivery of adaptive domain-independent feedback in a variety of Intelligent Learning Environments (ILEs). Â© Springer International Publishing AG, part of Springer Nature 2018.",,,
,2018,Cheok K.C.,Eye-hand tracking simulator for training ai learning systems,"There are many techniques for modeling and characterizing system behaviors. These techniques always require a sequence of steps such as acquisition of training data, training of the model and verification of the trained model performance. In this paper, we present a Matlab/Simulink-based simulator for a video-based target tracking system that is used to train AI learning models including artificial neural networks (ANN). It is shown that an ANN could emulate the eye-hand tracking (EHT) coordination of a human operator. The simulator-based experiments can be used to reveal many insights for training of the different ANN configurations for human EHT action. Its uses can be extended to system identification, adaptive control, machine learning and deep learning. Â© 2018 The International Society for Computers and Their Applications (ISCA). All Rights Reserved.",,,
10.1007/978-3-319-91464-0_36,2018,"Howe A., Nguyen P.",SAT reading analysis using eye-gaze tracking technology and machine learning,"We propose a method using eye-gaze tracking technology and machine learning for the analysis of the reading section of the Scholastic Aptitude Test (SAT). An eye-gaze tracking device tracks where the reader is looking on the screen and provides the coordinates of the gaze. This collected data allows us to analyze the reading patterns of test takers and discover what features enable test takers to score higher. Using a machine learning approach, we found that the time spent on the passage at the beginning of the test (in minutes), number of times switching between the passage and the questions, and the total time spent doing the reading test (in minutes) have the greatest impact in distinguishing higher scores from lower scores. Â© Springer International Publishing AG, part of Springer Nature 2018.",,,
10.1007/978-3-319-91464-0_6,2018,"Doumbouya R., Benlamine M.S., Dufresne A., Frasson C.",Game scenes evaluation and playerâ€™s dominant emotion prediction,"In this paper, we present a solution for computer assisted emotional analysis of game session. The proposed approach combines eye movements and facial expressions to annotate the perceived game objects with the expressed dominate emotions. Moreover, our system EMOGRAPH (Emotional Graph) gives easy access to information about user experience and predicts playerâ€™s emotions. The prediction mainly uses both subjective measures through questionnaire and objective measures through brain wave activity (electroencephalography - EEG) combined with eye tracking data. EMOGRAPHâ€™s method was experimented on 21 participants playing horror game â€œOutlastâ€? Our results show the effectiveness of our method in the identification of the emotions and their triggers. We also present our emotion prediction approach using game sceneâ€™s design goal (defined by OCC variables from the model of emotionsâ€?cognitive evaluation of Ortony, Clore and Collins [1]) to annotate the playerâ€™s situation in a scene and machine learning algorithms. The prediction results are promising and would widen possibilities in game design. Â© Springer International Publishing AG, part of Springer Nature 2018.",,,
10.1155/2018/6929762,2018,"Han Y.-J., Kim W., Park J.-S.",Efficient Eye-Blinking Detection on Smartphones: A Hybrid Approach Based on Deep Learning,"We propose an efficient method that can be used for eye-blinking detection or eye tracking on smartphone platforms in this paper. Eye-blinking detection or eye-tracking algorithms have various applications in mobile environments, for example, a countermeasure against spoofing in face recognition systems. In resource limited smartphone environments, one of the key issues of the eye-blinking detection problem is its computational efficiency. To tackle the problem, we take a hybrid approach combining two machine learning techniques: SVM (support vector machine) and CNN (convolutional neural network) such that the eye-blinking detection can be performed efficiently and reliably on resource-limited smartphones. Experimental results on commodity smartphones show that our approach achieves a precision of 94.4% and a processing rate of 22 frames per second. Â© 2018 Young-Joo Han et al.",,,
10.5220/0006637705960602,2018,"Guezou-Philippe A., Huet S., Pellerin D., Graff C.",Prototyping and evaluating sensory substitution devices by spatial immersion in virtual environments,"Various audio-vision Sensory Substitution Devices (SSDs) are in development to assist people without sight. They all convert optical information extracted from a camera, into sound parameters but are evaluated for different tasks in different contexts. The use of 3D environments is proposed here to compare the advantages and disadvantages of not only software (transcoding) solutions but also of hardware (component) specifics, in various situations and activities. By use of a motion capture system, the whole person, not just a guided avatar, was immersed in virtual places that were modelled and that could be replicated at will. We evaluated the ability to hear depth for various tasks: detecting and locating an open window, moving and crossing an open door. Participants directed the modelled depth-camera with a real pointing device that was either held in the hand or fastened on the head. Mixed effects on response delays were analyzed with a linear model to highlight the respective importance of the pointing device, the target specifics and the individual participants. Results are encouraging to further exploit our prototyping set-up and test many solutions by implementing e.g., environments, sensor devices, transcoding rules, and pointing devices including the use of an eye-tracker. Copyright Â© 2018 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",,,
10.5220/0006562002090217,2018,"Puttemans S., Callemein T., Goedeme T.",Building robust industrial applicable object detection models using transfer learning and single pass deep learning architectures,"The uprising trend of deep learning in computer vision and artificial intelligence can simply not be ignored. On the most diverse tasks, from recognition and detection to segmentation, deep learning is able to obtain state-of-the-art results, reaching top notch performance. In this paper we explore how deep convolutional neural networks dedicated to the task of object detection can improve our industrial-oriented object detection pipelines, using state-of-the-art open source deep learning frameworks, like Darknet. By using a deep learning architecture that integrates region proposals, classification and probability estimation in a single run, we aim at obtaining real-time performance. We focus on reducing the needed amount of training data drastically by exploring transfer learning, while still maintaining a high average precision. Furthermore we apply these algorithms to two industrially relevant applications, one being the detection of promotion boards in eye tracking data and the other detecting and recognizing packages of warehouse products for augmented advertisements. Â© 2018 by SCITEPRESS â€?Science and Technology Publications, Lda. All rights reserved.",,,
,2018,"Chanijani S.S.M., Raue F., Hassanzadeh S.D., Agne S., Bukhari S.S., Dengel A.",Reading type classification based on generative models and bidirectional long short-term memory,"Measuring the attention of users is necessary to design smart Human Computer Interaction (HCI) systems. Particularly, in reading, the reading types, so-called reading, skimming, and scanning are signs to express the degree of attentiveness. Eye movements are informative spatiotemporal data to measure quality of reading. Eye tracking technology is the tool to record eye movements. Even though there is increasing usage of eye trackers in research and especially in psycholinguistics, collecting appropriate task-specific eye movements data is expensitive and time consuming. Moreover, machine learning tools like Recurrent Neural Networks need large enough samples to be trained. Hence, designing a generative model in order to have reliable research-oriented synthetic eye movements is desirable. This paper has two main contributions. First, a generative model in order to synthesize reading, skimming, and scanning in reading is developed. Second, in order to evaluate the generative model, a bidirectional Long Short- Term Memory (BLSTM) is proposed. It was trained with synthetic data and tested with real-world eye movements to classify reading, skimming, and scanning where more than 95% classification accuracy is achieved. Â© 2018 Copyright for the individual papers remains with the authors.",,,
10.1007/978-981-10-8612-0_26,2018,"PÃ©rez-Belis V., Agost M.-J., Vergara M.",Consumersâ€?visual attention and emotional perception of sustainable product information: Case study of furniture,"Transparency about product information is increasingly accessible, due to factors such as market globalization and the fast growing of new communication technologies. This fact has facilitated the appearance of well-informed consumers who are concerned about the repercussions of their purchasing choices on, for example, their own health, the environment, or even the social conditions of workers. Information about sustainable aspects of product lifecycle is often presented through labels located on the product itself or on its packaging. Apart from providing information to consumers, these labels need to connect to their environmental subjective perception, in order to integrate them into their purchase criteria. However, literature shows that some of them are not effective or well understood by consumers. Through the application of product semantics and eye-tracking, an objective technique measuring visual attention, this communication shows the results of a study on consumer perception of different sustainable labels applied to furniture. Twenty-six subjects were recruited to analyze their perception and knowledge about labels related to three different sustainable dimensions: environment, workers respect and customer health, through semantic differential evaluation. Besides, visual attention was measured using eye-tracking technology. Some descriptive and comparative statistical analyses about visual behavior on different areas of interest were completed considering eye-tracking metrics. Results show some significant differences in the time spent looking at label areas, and also in the number of right answers, depending on different factors. Â© 2018, Springer Nature Singapore Pte Ltd.",,,
10.1007/978-3-319-75420-8_41,2018,"Åšledzianowski A., SzymaÅ„ski A., Szlufik S., Koziorowski D.",Rough Set Data Mining Algorithms and Pursuit Eye Movement Measurements Help to Predict Symptom Development in Parkinsonâ€™s Disease,"This article presents research on pursuit eye movements tests conducted on patients with Parkinsonâ€™s disease in various stages of disease and phases of treatment. The aim of described experiment was to develop algorithms allowing for measurements of parameters of pursuit eye movement in order to reference calculated results to the previously collected neurological data of patients. An additional objective of the experiment was to develop an example of data-mining procedure, allowing for classification of neurological symptoms based on oculometric measurements. Definition of such correlation enables assignment of particular patient to a given neurological group on the base of parameters values of pursuit eye movements. By using created decision table, we have achieved good results of prediction of the neurological parameter UPDRS, with total accuracy of 93.3% and total coverage of 60%. This allows for better evaluation of stage of the disease and its progression. It also might provide additional tool in determining efficacy of different disease treatments. Â© Springer International Publishing AG, part of Springer Nature 2018.",,,
10.1007/978-981-10-8530-7_41,2018,"Zhao T., Wang Y., Fu X.",Refining eye synthetic images via coarse-to-fine adversarial networks for appearance-based gaze estimation,"Recently, several models have achieved great success in terms of reducing the gap between synthetic and real image distributions with large unlabeled real data. However, collecting such large amounts of real data costs a lot of labouring and training them requires high memory. To reduce the gap with less real data, we propose a coarse-to-fine refine eye image method combining coarse model net and fine model net through adversarial training. Coarse model net is a feed-forward convolutional neural network aiming to transform synthetic eye images into coarse images. Fine model net is a modified Generative Adversarial Networks (GANs) which add realism to coarse images using unlabeled real data. Experimental results show that the proposed method achieves similar distributions as recent work but decreasing real data at least one order of magnitude. In addition, a significant accuracy improvement for gaze estimation with refined synthetic eye images is observed. Â© Springer Nature Singapore Pte Ltd. 2018.",,,
10.1007/978-3-319-76213-5_11,2018,"Carette R., Cilia F., Dequen G., Bosche J., Guerin J.-L., Vandromme L.",Automatic Autism Spectrum Disorder Detection Thanks to Eye-Tracking and Neural Network-Based Approach,"Autism spectrum disorder (ASD) is a neurodevelopmental disorder quite wide and its numerous variations render diagnosis hard. Some works have proven that children suffering from autism have trouble keeping their attention and tend to have a less focused sight. On top of that, eye-tracking systems enable the recording of precise eye focus on a screen. This paper deals with automatic detection of autism spectrum disorder thanks to eye-tracked data and an original Machine Learning approach. Focusing on data that describes the saccades of the patientâ€™s sight, we distinguish, out of our six test patients, young autistic individuals from those with no problems in 83% (five) of tested patients, with a results confidence up to 95%. Â© 2018, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.",,,
10.1007/978-3-319-75786-5_38,2018,"Wu X., Li J., Wu Q., Sun J., Yan H.",Block-Wise Gaze Estimation Based on Binocular Images,"Appearance-based gaze estimation methods have been proved to be highly effective. Different from the previous methods that estimate gaze direction based on left or right eye image separately, we propose a binocular-image based gaze estimation method. Considering the challenges in estimating the precise gaze points via regression models, we estimate the block-wise gaze position by classifying the binocular images via convolutional neural network (CNN) in the proposed method. We divide the screen of the desktop computer into 2 Ã— 3 and 6 Ã— 9 blocks respectively, label the binocular images with their corresponding gazed block positions, train a convolutional neural network model to classify the eye images according to their labels, and estimate the gazed block through the CNN-based classification. The experimental results demonstrate that the proposed gaze estimation method based on binocular images can reach higher accuracy than those based on monocular images. And the proposed method shows its great potential in practical touch screen-based applications. Â© 2018, Springer International Publishing AG, part of Springer Nature.",,,
10.1007/978-3-319-75786-5_20,2018,"Podder P.K., Paul M., Murshed M.",A Novel No-reference Subjective Quality Metric for Free Viewpoint Video Using Human Eye Movement,"The free viewpoint video (FVV) allows users to interactively control the viewpoint and generate new views of a dynamic scene from any 3D position for better 3D visual experience with depth perception. Multiview video coding exploits both texture and depth video information from various angles to encode a number of views to facilitate FVV. The usual practice for the single view or multiview quality assessment is characterized by evolving the objective quality assessment metrics due to their simplicity and real time applications such as the peak signal-to-noise ratio (PSNR) or the structural similarity index (SSIM). However, the PSNR or SSIM requires reference image for quality evaluation and could not be successfully employed in FVV as the new view in FVV does not have any reference view to compare with. Conversely, the widely used subjective estimator- mean opinion score (MOS) is often biased by the testing environment, viewers mode, domain knowledge, and many other factors that may actively influence on actual assessment. To address this limitation, in this work, we devise a no-reference subjective quality assessment metric by simply exploiting the pattern of human eye browsing on FVV. Over different quality contents of FVV, the participants eye-tracker recorded spatio-temporal gaze-data indicate more concentrated eye-traversing approach for relatively better quality. Thus, we calculate the Length, Angle, Pupil-size, and Gaze-duration features from the recorded gaze trajectory. The content and resolution invariant operation is carried out prior to synthesizing them using an adaptive weighted function to develop a new quality metric using eye traversal (QMET). Tested results reveal that the proposed QMET performs better than the SSIM and MOS in terms of assessing different aspects of coded video quality for a wide range of FVV contents. Â© 2018, Springer International Publishing AG, part of Springer Nature.",,,
10.1109/TIP.2017.2762594,2018,"Li J., Xia C., Chen X.",A benchmark dataset and saliency-guided stacked autoencoders for video-based salient object detection,"Image-based salient object detection (SOD) has been extensively studied in past decades. However, video-based SOD is much less explored due to the lack of large-scale video datasets within which salient objects are unambiguously defined and annotated. Toward this end, this paper proposes a video-based SOD dataset that consists of 200 videos. In constructing the dataset, we manually annotate all objects and regions over 7650 uniformly sampled keyframes and collect the eye-tracking data of 23 subjects who free-view all videos. From the user data, we find that salient objects in a video can be defined as objects that consistently pop-out throughout the video, and objects with such attributes can be unambiguously annotated by combining manually annotated object/region masks with eye-tracking data of multiple subjects. To the best of our knowledge, it is currently the largest dataset for video-based salient object detection. Based on this dataset, this paper proposes an unsupervised baseline approach for video-based SOD by using saliency-guided stacked autoencoders. In the proposed approach, multiple spatiotemporal saliency cues are first extracted at the pixel, superpixel, and object levels. With these saliency cues, stacked autoencoders are constructed in an unsupervised manner that automatically infers a saliency score for each pixel by progressively encoding the high-dimensional saliency cues gathered from the pixel and its spatiotemporal neighbors. In experiments, the proposed unsupervised approach is compared with 31 state-of-the-art models on the proposed dataset and outperforms 30 of them, including 19 image-based classic (unsupervised or non-deep learning) models, six image-based deep learning models, and five video-based unsupervised models. Moreover, benchmarking results show that the proposed dataset is very challenging and has the potential to boost the development of video-based SOD. Â© 2017 IEEE.",,,
10.1016/j.knosys.2017.10.010,2018,"Wang Y., Zhao T., Ding X., Peng J., Bian J., Fu X.",Learning a gaze estimator with neighbor selection from large-scale synthetic eye images,"Appearance-based gaze estimation works well in inferring human gaze under real-world condition. But one of the significant limitations in appearance-based methods is the need for huge amounts of training data. Eye image synthesis addresses this problem by generating huge amounts of synthetic eye images with computer graphics. To fully use the large-scale synthetic eye images, a simple-but-effective appearance-based gaze estimation framework with neighbor selection is proposed in this paper. The proposed framework hierarchically fuses multiple k-NN queries (in head pose, pupil center and eye appearance spaces) to choose closest samples with more relevant features. Considering the structure characters of the closet samples, neighbor regression methods then can be applied to predict the gaze directions. Experimental results demonstrate that the representative neighbor regression methods under the proposed framework achieve better performance for within-subject and cross-subject gaze estimation. Â© 2017 Elsevier B.V.",,,
10.1007/978-3-319-63859-1_4,2018,"Sun Y., Li Q., Zhang H., Zou J.",The application of eye tracking in education,"The application of emerging information technologies to traditional teaching methods can not only enhance the value of technologies, but also improve the teaching progress and integrate different fields in education with efficiency. 3D printing, virtual reality and eye tracking technology have been found more and more applications in education recently. In this paper, through the improvement of eye tracking algorithm, we developed an education software package based on eye tracking technology. By analyzing the studentsâ€?eye movement data, teachers are able to improve the teaching quality by improving the teaching framework. Students can also focus on their own interests more to develop a reasonable learning plan. The application of eye tracking technology in the field of education has a great potential to promote the application of technology and to improve the educational standards. Â© Springer International Publishing AG 2018.",,,
10.1007/978-3-319-60642-2_16,2018,"Zhang Y., Yang H., Xu Y., Feng L.",Comparison of visual comfort and fatigue between watching different types of 3D TVS as measured by eye tracking,"An eye movement study was conducted to make clear whether different types of 3D TVs would help to relieve visual fatigue after watching films for a long time. 64 undergraduates and ordinary researchers were measured to compare the difference of watching different 3D TVs by Eye-tracking. 64 participants were divided into four groups after being matched, and the four matched groups were separately arranged to watch Switched 3D TV, polarized 3D TV, naked 3D TV and 2D TV. They watched the same video contents which were scenery video and a film, while eye movement data were recorded. The results showed that: (1) with the increase of watching time, participantsâ€?fatigue also increased. The blink frequency, blink counts, blink total duration, saccade angle and saccade velocity of the four group participants who watched different types of 3D TVs remarkably increased in the overall trend with time; (2) There was a remarkable difference between the participants watching polarized 3D TV and others watching 3D TV and 2D TV in average saccade duration, saccade duration peak, average saccade angle and saccade total angle, which might indicate that the principle of polarized 3D TV would affect usersâ€?saccade duration and saccade distance during watching videos. (3) The saccade amplitude of switched 3D TV was significantly higher than that of other three conditions, which might indicate a greater influence of saccade amplitude in switched 3D TV. Â© Springer International Publishing AG 2018.",,,
10.1007/978-3-319-60642-2_25,2018,"Wulff-Jensen A., Bruni L.E.",Evaluating ann efficiency in recognizing eeg and eye-tracking evoked potentials in visual-game-events,"EEG and Eye-tracking signals have customarily been analyzed and inspected visually in order to be correlated to the controlled stimuli. This process has proven to yield valid results as long as the stimuli of the experiment are under complete control (e.g.: the order of presentation). In this study, we have recorded the subjectâ€™s electroencephalogram and eye-tracking data while they were exposed to a 2D platform game. In the game we had control over the design of each level by choosing the diversity of actions (i.e. events) afforded to the player. However we had no control over the order in which these actions were undertaken. The psychophysiological signals were synchronized to these game events and used to train and test an artificial neural network in order to evaluate how efficiently such a tool can help us in establishing the correlation, and therefore differentiating among the different categories of events. The highest average accuracies were between 60.25%â€?2.07%, hinting that it is feasible to recognize reactions to complex uncontrolled stimuli, like game events, using artificial neural networks. Â© Springer International Publishing AG 2018.",,,
10.1007/s11042-016-4334-x,2018,Jan F.,Pupil localization in image data acquired with near-infrared or visible wavelength illumination,"Pupil localization in human face/eye images has numerous applications, e.g., eye tracking, iris recognition, cataract assessment and surgery, diabetic retinopathy screening, neuropsychiatric disorders diagnosing, and aliveness detection. In real scenario, the pupil localization task suffers from many complications such as pupilâ€™s constriction and dilation moments, light reflections, eyelids and eyelashes, and cataract disease. To resolve this issue, this study proposes an accurate and fast pupil localization scheme. It performs relatively well for eyeimages acquired either with the near infrared (NIR) or visible wavelength (VW) illumination. First, it effectively preprocesses the input eyeimage. Next, it coarsely marks pupil location using a scheme comprising an adaptive threshold and two-dimensional (2D) object properties. Then, it validates pupil location via an effective test involving global gray-level statistics. If it finds pupil location invalid, then it localizes pupil through a hybrid of the Hough transform and image global gray-level statistics. Finally, it localizes the fine pupillary boundary through a hybrid of the Fourier series and imageâ€™s gradients. Its experimental results obtained on numerous publically available iris datasets demonstrate its superiority over most of the contemporary schemes. Â© 2017, Springer Science+Business Media New York.",,,
10.1109/CAC.2017.8242861,2017,"Zhong M., Lan X., Wang K.",Saliency-based joint distortion model for 3D video coding,"In 3D video system, storing and transmitting the large amount of high definition videos is the main challenge. To further reduce video content related redundant data in 3D video encoded with HEVC, this paper proposes an saliency-based joint distortion model for 3D video encoding, which jointly considers the distortion of texture and depth, as well as synthesis distortion. With this model, two optimization methods are proposed. One method weighs the distortion of each coding unit (CU) according to the saliency information to protect the salient regions. And the other method optimizes the distribution of depth video according to the locality of virtual synthesis distortion (VSD), and weighs the CU-level distortion of texture video and VSD with the saliency information of texture video. Different from existing methods, saliency information based CU-level bit allocation is used in both texture and depth videos. Then we optimize saliency-based joint distortion to minimize the bits while keeping the visual quality the same. The experimental results of visual quality show that the proposed methods have gains on eyetracking PSNR (EWPSNR) with the same bitrate as latest HEVC and an existing saliency encoding method for 3D video. Besides, with the same visual quality, our method costs fewer bits. Â© 2017 IEEE.",,,
10.1109/HUMANOIDS.2017.8246889,2017,"Bilac M., Chamoux M., Lim A.",Gaze and filled pause detection for smooth human-robot conversations,"Let the human speak! Interactive robots and voice interfaces such as Pepper, Amazon Alexa, and OK Google are becoming more and more popular, allowing for more natural interaction compared to screens or keyboards. One issue with voice interfaces is that they tend to require a 'robotic' flow of human speech. Humans must be careful to not produce disfluencies, such as hesitations or extended pauses between words. If they do, the agent may assume that the human has finished their speech turn, and interrupts them mid-Thought. Interactive robots often rely on the same limited dialogue technology built for speech interfaces. Yet humanoid robots have the potential to also use their vision systems to determine when the human has finished their speaking turn. In this paper, we introduce HOMAGE (Human-rObot Multimodal Audio and Gaze End-of-Turn), a multimodal turntaking system for conversational humanoid robots. We created a dataset of humans spontaneously hesitating when responding to a robot's open-ended questions such as, 'What was your favorite moment this year?'. Our analyses found that users produced both auditory filled pauses such as 'uhhh', as well as gaze away from the robot to keep their speaking turn. We then trained a machine learning system to detect the auditory filled pauses and integrated it along with gaze into the Pepper humanoid robot's real-Time dialog system. Experiments with 28 naive users revealed that adding auditory filled pause detection and gaze tracking significantly reduced robot interruptions. Furthermore, user turns were 2.1 times longer (without repetitions), suggesting that this strategy allows humans to express themselves more, toward less time pressure and better robot listeners. Â© 2017 IEEE.",,,
10.1109/ICCV.2017.341,2017,"Deng H., Zhu W.",Monocular Free-Head 3D Gaze Tracking with Deep Learning and Geometry Constraints,"Free-head 3D gaze tracking outputs both the eye location and the gaze vector in 3D space, and it has wide applications in scenarios such as driver monitoring, advertisement analysis and surveillance. A reliable and low-cost monocular solution is critical for pervasive usage in these areas. Noticing that a gaze vector is a composition of head pose and eyeball movement in a geometrically deterministic way, we propose a novel gaze transform layer to connect separate head pose and eyeball movement models. The proposed decomposition does not suffer from head-gaze correlation overfitting and makes it possible to use datasets existing for other tasks. To add stronger supervision for better network training, we propose a two-step training strategy, which first trains sub-tasks with rough labels and then jointly trains with accurate gaze labels. To enable good cross-subject performance under various conditions, we collect a large dataset which has full coverage of head poses and eyeball movements, contains 200 subjects, and has diverse illumination conditions. Our deep solution achieves state-of-the-art gaze tracking accuracy, reaching 5.6Â° cross-subject prediction error using a small network running at 1000 fps on a single CPU (excluding face alignment time) and 4.3Â° cross-subject error with a deeper network. Â© 2017 IEEE.",,,
10.1109/ICCV.2017.354,2017,"Jiang M., Zhao Q.",Learning Visual Attention to Identify People with Autism Spectrum Disorder,"This paper presents a novel method for quantitative and objective diagnoses of Autism Spectrum Disorder (ASD) using eye tracking and deep neural networks. ASD is prevalent, with 1.5% of people in the US. The lack of clinical resources for early diagnoses has been a long-lasting issue. This work differentiates itself with three unique features: first, the proposed approach is data-driven and free of assumptions, important for new discoveries in understanding ASD as well as other neurodevelopmental disorders. Second, we concentrate our analyses on the differences in eye movement patterns between healthy people and those with ASD. An image selection method based on Fisher scores allows feature learning with the most discriminative contents, leading to efficient and accurate diagnoses. Third, we leverage the recent advances in deep neural networks for both prediction and visualization. Experimental results show the superior performance of our method in terms of multiple evaluation metrics used in diagnostic tests. Â© 2017 IEEE.",,,
10.1109/ICCV.2017.114,2017,"Wang K., Ji Q.",Real Time Eye Gaze Tracking with 3D Deformable Eye-Face Model,"3D model-based gaze estimation methods are widely explored because of their good accuracy and ability to handle free head movement. Traditional methods with complex hardware systems (Eg. infrared lights, 3D sensors, etc.) are restricted to controlled environments, which significantly limit their practical utilities. In this paper, we propose a 3D model-based gaze estimation method with a single web-camera, which enables instant and portable eye gaze tracking. The key idea is to leverage on the proposed 3D eye-face model, from which we can estimate 3D eye gaze from observed 2D facial landmarks. The proposed system includes a 3D deformable eye-face model that is learned offline from multiple training subjects. Given the deformable model, individual 3D eye-face models and personal eye parameters can be recovered through the unified calibration algorithm. Experimental results show that the proposed method outperforms state-of-the-art methods while allowing convenient system setup and free head movement. A real time eye tracking system running at 30 FPS also validates the effectiveness and efficiency of the proposed method. Â© 2017 IEEE.",,,
10.1109/IROS.2017.8206301,2017,"Dini A., Murko C., Yahyanejad S., Augsdorfer U., Hofbaur M., Paletta L.",Measurement and prediction of situation awareness in human-robot interaction based on a framework of probabilistic attention,"Human attention processes play a major role in the optimization of human-robot interaction (HRI) systems. This work describes a novel methodology to measure and predict situation awareness and from this overall performance from gaze features in real-time. The awareness about scene objects of interest is described by 3D gaze analysis using data from wearable eye tracking glasses and a precise optical tracking system. A probabilistic framework of uncertainty considers coping with measurement errors in eye and position estimation. Comprehensive experiments on HRI were conducted with typical tasks including handover in a lab based prototypical manufacturing environment. The methodology is proven to predict standard measures of situation awareness (SAGAT, SART) as well as performance in the HRI task in real-time and will open new opportunities for human factors based performance optimization in HRI applications. Â© 2017 IEEE.",,,
10.1109/ROMAN.2017.8172275,2017,"Matsui T., Yamada S.",Entropy-based eye-tracking analysis when a user watches a PRVA's recommendations,"We conducted three experiments to discover the effect of a virtual agent's state transition on a user's eye gaze. Many previous studies showed that an agent's state transition affects a user's state. We focused on two kinds of transitions, the internal state transition and appearance state transition. In this research, we used a product recommendation virtual agent (PRVA) and aimed to discover the effect of its state transitions on users' eye gaze as it made recommendations. We used entropy-based analysis to visualise the deviation of a user's fixations. In experiment 1, the PRVA made recommendations without state transitions. In experiment 2, the amount of the PRVA's knowledge transitioned from low to high during the recommendations. This is an internal state transition. In experiment 3, the PRVA's facial expressions and gestures transitioned from a neutral to positive emotion during the recommendations. This is an appearance state transition. As a result, both the entropy-based analysis and fixation duration based analysis showed significant differences in experiment 3. These results show that an agent's appearance state transitions cause a user's eye gaze to transition. Â© 2017 IEEE.",,,
10.1109/TVCG.2016.2641442,2017,"Wen Q., Xu F., Yong J.-H.",Real-time 3D eye performance reconstruction for RGBD cameras,"This paper proposes a real-time method for 3D eye performance reconstruction using a single RGBD sensor. Combined with facial surface tracking, our method generates more pleasing facial performance with vivid eye motions. In our method, a novel scheme is proposed to estimate eyeball motions by minimizing the differences between a rendered eyeball and the recorded image. Our method considers and handles different appearances of human irises, lighting variations and highlights on images via the proposed eyeball model and the L0-based optimization. Robustness and real-time optimization are achieved through the novel 3D Taylor expansion-based linearization. Furthermore, we propose an online bidirectional regression method to handle occlusions and other tracking failures on either of the two eyes from the information of the opposite eye. Experiments demonstrate that our technique achieves robust and accurate eye performance reconstruction for different iris appearances, with various head/face/eye motions, and under different lighting conditions. Â© 1995-2012 IEEE.",,,
10.1016/j.cag.2017.08.017,2017,"Conti J., Ozell B., Paquette E., Renaud P.",Adjusting stereoscopic parameters by evaluating the point of regard in a virtual environment,"Despite the growth in research and development in the area of virtual reality over the past few years, virtual worlds do not yet convey a feeling of presence that matches reality. This is particularly due to the difference in visual perception of flat images as compared to actual 3D. We studied the impact of two parameters of the stereoscopic configuration, namely, the inter-camera distance (ICD) and the presence of a depth of field blur (DOF blur). We conducted an experiment involving 18 participants in order to evaluate this impact, based on both subjective and objective criteria. We examined six configurations which differed in the presence or absence of DOF blur and the value of the ICD: fixed and equal to the anatomical interpupillary distance, fixed and chosen by the participant, or variable, depending on the depth of the viewer's point of regard (POR). The DOF blur and the variable ICD require the use of an eye tracking system in order to be adjusted with respect to the POR. To our knowledge, no previously published research has tested a gaze-contingent variable ICD along with dynamic DOF blur in a Cave Automatic Virtual Environment. Our results show that the anatomical and variable ICD performed similarly regarding each criterion of the experiment, both being more efficient than the fixed ICD. Besides, as with earlier similar attempts, the configurations with DOF blur obtained lower subjective evaluations. Although mainly not significant, the results obtained by the variable ICD and DOF blur are likely due to a noticeable delay in the parameters update. We also designed a new methodology to objectively compare the geometry and depth rendering, based on the reproduction of the same scene in the real and virtual setups, and then on the study of resulting ocular convergence and angular deviation from a target. This leads to a new comparative criterion for the perceptual realism of immersive virtual environments based on the visual behavior similarity between real and virtual setups. Â© 2017 Elsevier Ltd",,,
10.1016/j.displa.2017.09.002,2017,"Hsu C.-C., Fann S.-C., Chuang M.-C.",Relationship between eye fixation patterns and Kansei evaluation of 3D chair forms,"Understanding how to induce Kansei (emotion or affect) in consumers through form is critical in product design and development. Conventional Kansei evaluations, which involve subjectively evaluating the overall form of a product, do not clarify the effects of the individual parts of a product on people's Kansei evaluation. A microscale analysis of eye movement of people looking at product form may redeem this flaw in subjective evaluation. However, simultaneously recording eye movement when people making Kansei evaluation is challenging, previous studies have typically investigated either the relationship between form and eye movement or the relationship between form and Kansei separately. The eye movement of people while performing Kansei evaluations on product forms still has not been clarified. To address this issue, the present study used an eye tracking system to analyze the changes in the fixation points of people performing various Kansei evaluations. Twenty participants were recruited for 8 Kansei evaluations on the form of 16 chairs by using the semantic differential (SD) rating, while their eye movements on these evaluations were tracked simultaneously. Through factor analysis on the data of Kansei evaluations, two principal factors, valence (pleasure) and arousal, were extracted from the 8 Kansei scales to constitute a Kansei plane which is compatible to Russell's circumplex model (plane) of affect By adopting the factor scores of the 16 chairs as coordinates, the 16 chairs were mapped into the Kansei plane. Further analysis on the eye fixation on the chairs located in this plane concluded the following results: (a) Pleasure had a more significant effect on the participantsâ€?visual attention compared to arousal; the participants required more fixation points when evaluating the chair form that induced displeasure. (b) The participants typically fixated on two parts of the chairs during their Kansei evaluations, namely the seat and the backrest, indicating that seats and backrests are the two primary features people consider when evaluating chairs. The results clarify the effect of various Kansei on eye movements; thereby enable predicting people's Kansei evaluations of product forms through analyzing their eye movement. Â© 2017",,,
10.1109/SIPROCESS.2017.8124555,2017,"Ma K.-T., Xu Q., Lim R., Li L., Sim T., Kankanhalli M.",Eye-2-I: Eye-tracking for just-in-time implicit user profiling,"For many applications, such as targeted advertising and content recommendation, knowing users' traits and interests is a prerequisite. User profiling is a helpful approach for this purpose. However, current methods, i.e. self-reporting, web-activity monitoring and social media mining are either intrusive or require data over long periods of time. Recently, there is growing evidence in cognitive science that a variety of users' profile is significantly correlated with eye-tracking data. A novel just-in-time implicit profiling method, Eye-2-I, which learns the user's demographic and personality traits from the eye-tracking data while the user is watching videos is proposed. Although seemingly conspicuous by closely monitoring the user's eye behaviors, the proposed method is unobtrusive and privacy-preserving owing to its unique combination of speed and implicitness. As a proof-of-concept, the proposed method is evaluated in a user study with 51 subjects. Â© 2017 IEEE.",,,
10.1016/j.neucom.2017.05.050,2017,"Fang Y., Lei J., Li J., Xu L., Lin W., Callet P.L.",Learning visual saliency from human fixations for stereoscopic images,"In the previous years, a lot of saliency detection algorithms have been designed for saliency computation of visual content. Recently, stereoscopic display techniques have developed rapidly, which results in much requirement of stereoscopic saliency detection for emerging stereoscopic applications. Different from 2D saliency prediction, stereoscopic saliency detection methods have to consider depth factor. We design a novel stereoscopic saliency detection algorithm by machine learning technique. First, the features of luminance, color and texture are extracted to calculate the feature contract for predicting feature maps of stereoscopic images. Furthermore, the depth features are extracted for depth feature map computation. Sematic features including the center-bias factor and other top-down cues are also applied as the features in the proposed stereoscopic saliency detection method. Support Vector Regression (SVR) is applied to learn the saliency detection model of stereoscopic images. Experimental results obtained on a public large-scale eye tracking database demonstrate that the proposed method can predict better saliency results for stereoscopic images than other existing ones. Â© 2017 Elsevier B.V.",,,
10.1109/SMC.2017.8122781,2017,"Siddek A.M., Rashwan M.A., Eshrah I.A.",3D camouflaging object using RGB-D sensors,"This paper proposes a new optical camouflage system that uses RGB-D cameras, for acquiring point cloud of background scene, and tracking observers' eyes. This system enables a user to conceal an object located behind a display that surrounded by 3D objects. If we considered here the tracked point of observer's eyes is a light source, the system will work on estimating shadow shape of the display device that falls on the objects in background. The system uses the 3d observer's eyes and the locations of display corners to predict their shadow points which have nearest neighbors in the constructed point cloud of background scene. Â© 2017 IEEE.",,,
10.1109/SMC.2017.8122973,2017,Siddek A.M.,Depth-level based camouflaging using RGB-D sensor,"Optical Camouflage is the process of concealing objects in visual spectrum range. This paper proposes a system which, can conceal any 2D object that is in front of an observer using RGB-D sensor and LCD display. This sensor is Kinect v2 sensor which, is used for depth sensing of background scene behind the object, and for 3D tracking of an observer's eyes. The LCD display covers the object which is required to be concealed. Images which are outputted on the display, are a real-time processing video frame of the background region, which are unseen by the observer and occluded by the object. These images should be observed from the viewpoints of the observer rather than camera's viewpoints. Â© 2017 IEEE.",,,
10.1109/MMSP.2017.8122270,2017,"Wu X., Li J., Wu Q., Sun J.",Appearance-Based gaze block estimation via CNN classification,"Appearance-based gaze estimation methods have received increasing attention in the field of human-computer interaction (HCI). These methods tried to estimate the accurate gaze point via Convolutional Neural Network (CNN) model, but the estimated accuracy can't reach the requirement of gazebased HCI when the regression model is used in the output layer of CNN. Given the popularity of button-touch-based interaction, we propose an appearance-based gaze block estimation method, which aims to estimate the gaze block, not the gaze point. In the proposed method, we relax the estimation from point to block, so that the gaze block can be estimated by CNN-based classification instead of the previous regression model. We divide the screen into square blocks to imitate the button-touch interface, and build an eye-image dataset, which contains the eye images labelled by their corresponding gaze blocks on the screen. We train the CNN model according to this dataset to estimate the gaze block by classifying the eye images. The experiments on 6- A nd 54-block classifications demonstrate that the proposed method has high accuracy in gaze block estimation without any calibration, and it is promising in button-touch-based interaction. Â© 2017 IEEE.",,,
10.1145/3145749.3149423,2017,"Koskela M., JÃ¤Ã¤skelÃ¤inen P., Immonen K., Multanen J., Viitanen T., Takala J.",Foveated instant preview for progressive rendering,"Progressive rendering, for example Monte Carlo rendering of 360? content for virtual reality headsets, is a time-consuming task. If the 3D artist notices an error while previewing the rendering, he or she must return to editing mode, do the required changes, and restart rendering. Restart is required because the rendering system cannot know which pixels are affected by the change. We propose the use of eye-tracking-based optimization to significantly speed up previewing the artistâ€™s points of interest. Moreover, we derive an optimized version of the visual acuity model, which follows the original model more accurately than previous work. The proposed optimization was tested with a comprehensive user study. The participants felt that preview with the proposed method converged instantly, and the recorded split times show that the preview is 10 times faster than conventional preview. In addition, the system does not have measurable drawbacks on computational performance. Â© 2017 Copyright held by the owner/author(s).",,,
10.1145/3132787.3139201,2017,"Lee Y., Shin C., Piumsomboon T., Lee G., Billinghurst M.",Automated enabling of head mounted display using gaze-depth estimation,"Recently, global companies have released OST-HMDs (Optical Seethrough Head Mounted Displays) for Augmented Reality. The main feature of these HMDs is that you can see virtual objects while seeing real space. However, if you do not want to see a virtual object and you want to focus on a real object, this functionality is inconvenient. In this paper, we propose a method to turn on off the screen of HMD according to user 's gaze when using an augmented reality HMD. The proposed method uses the eye-Tracker attached to the mobile HMD to determine the line of sight along the distance. We put this data into a neural network to create a learning model. After the learning is completed, the gaze data is input in real time to obtain the gaze predicted distance. Through various experiments, the possibilities and limits of machine learning algorithms are grasped and suggestions for improvement are suggested. Â© 2017 Copyright held by the owner/author(s).",,,
10.1109/ISMAR.2017.30,2017,"Wiesner C.A., Ruf M., Sirim D., Klinker G.",3D-FRC: Depiction of the future road course in the head-up-display,"The introduction of Head-Up-Displays (HUDs) have opened up avenues for a whole range of novel AR applications. However, until these applications become available for the mass market a number of problems need to be tackled. For example, the field of view (FoV) of current HUDs is extremely limited, and real world tracking and 3D reconstruction are still not precise enough to show driving information embedded into wide areas of complex traffic environment. It is not possible to show true AR-visualizations in the display areas provided by the current FoVs. In this paper, we investigate how an AR-like visualization approach in current HUDs (with a limited FoV) can support drivers in foreseeing the future road course. This visualisation uses the already established concept of an electronic horizon. By complying with automotive standards, our application can be easily adapted for series production. With this visualisation we performed a user study, investigating the effect on drivers' gaze behaviour. For this reason the test subjects were equipped with an eye tracking system. The results showed a decrease in both, the number of gazes as well as total glance time on the head unit and the instrument cluster. We also investigated the test subjects' braking behaviour around sharp bends of the road which showed an overall improvement when the visualisation was enabled. Furthermore it showed an increase of the mean glance duration in the area of the HUD. Note that the eye tracking system is not capable of distinguishing between glances at the visualisation in the HUD and the users' glance at objects behind the visualisation - overlapping with the HUD. This would require tracking the test persons' depth of focus. The study showed that developers need to be concerned about not displaying excessively in the HUD, so as not to distract drivers. It furthermore showed that AR-like visualizations have the potential to decrease the time the driver is not looking at the road creating a safer driving experience. Â© 2017 IEEE.",,,
10.1145/3130800.3130889,2017,"Jang C., Bang K., Moon S., Kim J., Lee S., Lee B.",Retinal 3D: Augmented reality near-eye display via pupil-tracked light field projection on retina,"We introduce an augmented reality near-eye display dubbed â€œRetinal 3D.â€?Key features of the proposed display system are as follows: Focus cues are provided by generating the pupil-tracked light eld that can be directly projected onto the retina. Generated focus cues are valid over a large depth range since laser beams are shaped for a large depth of eld (DOF). Pupil-tracked light eld generation signicantly reduces the needed information/computation load. Also, it provides â€œdynamic eye-boxâ€?which can be a break-through that overcome the drawbacks of retinal projection-type displays. For implementation, we utilized a holographic optical element (HOE) as an image combiner, which allowed high transparency with a thin structure. Compared with current augmented reality displays, the proposed system shows competitive performances of a large eld of view (FOV), high transparency, high contrast, high resolution, as well as focus cues in a large depth range. Two prototypes are presented along with experimental results and assessments. Analysis on the DOF of light rays and validity of focus cue generation are presented as well. Combination of pupil tracking and advanced near-eye display technique opens new possibilities of the future augmented reality. Â© 2017 Copyright held by the owner/author(s).",,,
10.1109/OECC.2017.8114991,2017,"Zhang L., Surman P., Zheng Y.",Dynamic head tracked 3D display using fast spatial light modulator,"A laser-based eye-tracked autostereoscopic three-dimensional (3D) display has been developed, which is composed of a scanning laser, a ferroelectric liquid crystal (FLC) spatial light modulator (SLM) array, an assembled liquid crystal display (LCD) screen and an eye tracker. In the proposed system, the SLM array generates dynamic aperture pairs to modulate the scanning laser. Each aperture pair projects two exist pupils for single viewer's left and right eyes. By tracking the positions of the viewers' eyes, the dynamic exist pupils are regenerated simultaneously, which allows free movements of multi-viewers. A prototype system has been set up to verify this method. The experimental result shows that the image resolution is greatly improved by the addition of time multiplexing technique. High brightness and low crosstalk are achieved due to collimated laser backlight. Multi users are enabled to move freely without wearing any glasses. Â© 2017 IEEE.",,,
10.1145/3139513.3139514,2017,"Thomas C., Jayagopi D.B.",Predicting student engagement in classrooms using facial behavioral cues,"Student engagement is the key to successful classroom learning. Measuring or analyzing the engagement of students is very important to improve learning as well as teaching. In this work, we analyze the engagement or attention level of the students from their facial expressions, headpose and eye gaze using computer vision techniques and a decision is taken using machine learning algorithms. Since the human observers are able to well distinguish the attention level from student's facial expressions,head pose and eye gaze, we assume that machine will also be able to learn the behavior automatically. The engagement level is analyzed on 10 second video clips. The performance of the algorithm is better than the baseline results. Our best accuracy results are 10 % better than the baseline. The paper also gives a detailed review of works related to the analysis of student engagement in a classroom using vision based techniques. Â© 2017 Association for Computing Machinery.",,,
10.15439/2017F413,2017,"Hamotskyi S., Rojbi A., Stirenko S., Gordienko Y.",Automatized generation of alphabets of symbols,"In this paper, we discuss the generation of symbols (and alphabets) based on specific user requirements (medium, priorities, type of information that needs to be conveyed). A framework for the generation of alphabets is proposed, and its use for the generation of a shorthand writing system is explored. We discuss the possible use of machine learning and genetic algorithms to gather inputs for generation of such alphabets and for optimization of already generated ones. The alphabets generated using such methods may be used in very different fields, from the creation of synthetic languages and constructed scripts to the creation of sensible commands for multimodal interaction through Human-Computer Interfaces, such as mouse gestures, touchpads, body gestures, eye-tracking cameras, and brain-computing Interfaces, especially in applications for elderly care and people with disabilities. Â© 2017 PTI.",,,
10.15439/2017F16,2017,"Pohl D., Jungmann D., Taudul B., Membarth R., Hariharan H., Herfet T., Grau O.","The next generation of in-home streaming: Light fields, 5K, 10 GbE, and foveated compression","Interacting with real-time rendered 3D content from powerful machines on smaller devices is becoming ubiquitous through commercial products that enable in-home streaming within the same local network. However, support for high resolution, low latency in-home streaming at high image quality is still a challenging problem. To enable this, we enhance an existing open source framework for in-home streaming. We add highly optimized DXT1 (DirectX Texture Compression) support for thin desktop and notebook clients. For rendered light fields, we improve the encoding algorithms for higher image quality. Within a 10 Gigabit Ethernet (10 GbE) network, we achieve streaming up to 5K resolution at 55 frames per second. Through new low-level algorithmic improvements, we increase the compression speed of ETC1 (Ericsson Texture Compression) by a factor of 5. We are the first to bring ETC2 compression to real-time speed, which increases the streamed image quality. Last, we reduce the required data rate by more than a factor of 2 through foveated compression with real-time eye tracking. Â© 2017 PTI.",,,
10.15439/2017F425,2017,"Fornalczyk K., Wojciechowski A.",Robust face model based approach to head pose estimation,"Head pose estimation from camera images is a computational problem that may influence many sociological, cognitive, interaction and marketing researches. It is especially crucial in the process of visual gaze estimation which accuracy depends not only on eye region analysis, but head inferring as well. Presented method exploits a 3d head model for a user head pose estimation as it outperforms, in the context of performance, popular appearance based approaches and assures efficient face head pose analysis. The novelty of the presented approach lies in a default head model refinement according to the selected facial features localisation. The new method not only achieves very high precision (about 4Â°), but iteratively improves the reference head model. The results of the head pose inferring experiments were verified with professional Vicon motion tracking system and head model refinement accuracy was verified with high precision Artec structural light scanner. Â© 2017 PTI.",,,
10.1145/3131672.3131682,2017,"Li T., Liu Q., Zhou X.",Ultra-Low Power Gaze Tracking for Virtual Reality,"Tracking userâ€™s eye fixation direction is crucial to virtual reality (VR): it eases userâ€™s interaction with the virtual scene and enables intelligent rendering to improve userâ€™s visual experiences and save system energy. Existing techniques commonly rely on cameras and active infrared emitters, making them too expensive and power-hungry for VR headsets (especially mobile VR headsets). We present LiGaze, a low-cost, low-power approach to gaze tracking tailored to VR. It relies on a few low-cost photodiodes, eliminating the need for cameras and active infrared emitters. Reusing light emitted from the VR screen, LiGaze leverages photodiodes around a VR lens to measure reflected screen light in different directions. It then infers gaze direction by exploiting pupilâ€™s light absorption property. The core of LiGaze is to deal with screen light dynamics and extract changes in reflected light related to pupil movement. LiGaze infers a 3D gaze vector on the fly using a lightweight regression algorithm. We design and fabricate a LiGaze prototype using off-the-shelf photodiodes. Our comparison to a commercial VR eye tracker (FOVE) shows that LiGaze achieves 6.3â—?and 10.1â—?mean within-user and cross-user accuracy. Its sensing and computation consume 791ÂµW in total and thus can be completely powered by a credit-card sized solar cell harvesting energy from indoor lighting. LiGazeâ€™s simplicity and ultra-low power make it applicable in a wide range of VR headsets to better unleash VRâ€™s potential. Â© 2017 Association for Computing Machinery.",,,
10.1109/CVPR.2017.343,2017,"Liu Y., Zhang S., Xu M., He X.",Predicting salient face in multiple-face videos,"Although the recent success of convolutional neural network (CNN) advances state-of-the-art saliency prediction in static images, few work has addressed the problem of predicting attention in videos. On the other hand, we and that the attention of different subjects consistently focuses on a single face in each frame of videos involving multiple faces. Therefore, we propose in this paper a novel deep learning (DL) based method to predict salient face in multiple-face videos, which is capable of learning features and transition of salient faces across video frames. In particular, we first learn a CNN for each frame to locate salient face. Taking CNN features as input, we develop a multiple-stream long short-term memory (M-LSTM) network to predict the temporal transition of salient faces in video sequences. To evaluate our DL-based method, we build a new eye-tracking database of multiple-face videos. The experimental results show that our method outperforms the prior state-of-the-art methods in predicting visual attention on faces in multipleface videos. Â© 2017 IEEE.",,,
10.1145/3136755.3136793,2017,"Siegfried R., Yu Y., Odobez J.-M.",Towards the use of social interaction conventions as prior for gaze model adaptation,"Gaze is an important non-verbal cue involved in many facets of social interactions like communication, attentiveness or attitudes. Nevertheless, extracting gaze directions visually and remotely usually suffers large errors because of lowresolution images, inaccurate eye cropping, or large eye shape variations across the population, amongst others. This paper hypothesizes that these challenges can be addressed by exploiting multimodal social cues for gaze model adaptation on top of an head-pose independent 3D gaze estimation framework. First, a robust eye cropping refinement is achieved by combining a semantic face model with eye landmark detections. Investigations on whether temporal smoothing can overcome instantaneous refinement limitations is conducted. Secondly, to study whether social interaction convention could be used as priors for adaptation, we exploited the speaking status and head pose constraints to derive soft gaze labels and infer person-specific gaze bias using robust statistics. Experimental results on gaze coding in natural interactions from two different settings demonstrate that the two steps of our gaze adaptation method contribute to reduce gaze errors by a large margin over the baseline and can be generalized to several identities in challenging scenarios. Â© 2017 ACM.",,,
10.1007/s13218-017-0502-z,2017,"Dietz M., Schork D., Damian I., Steinert A., Haesner M., AndrÃ© E.",Automatic Detection of Visual Search for the Elderly using Eye and Head Tracking Data,"With increasing age we often find ourselves in situations where we search for certain items, such as keys or wallets, but cannot remember where we left them before. Since finding these objects usually results in a lengthy and frustrating process, we propose an approach for the automatic detection of visual search for older adults to identify the point in time when the users need assistance. In order to collect the necessary sensor data for the recognition of visual search, we develop a completely mobile eye and head tracking device specifically tailored to the requirements of older adults. Using this device, we conduct a user study with 30 participants aged between 65 and 80 years (avg= 71.7 , 50% female) to collect training and test data. During the study, each participant is asked to perform several activities including the visual search for objects in a real-world setting. We use the recorded data to train a support vector machine (SVM) classifier and achieve a recognition rate of 97.55% with the leave-one-user-out evaluation method. The results indicate the feasibility of an approach towards the automatic detection of visual search in the wild. Â© 2017, Springer-Verlag GmbH Deutschland.",,,
10.1007/s13218-017-0503-y,2017,"Mussgnug M., Singer D., Lohmeyer Q., Meboldt M.",Automated interpretation of eyeâ€“hand coordination in mobile eye tracking recordings: Identifying demanding phases in humanâ€“machine interactions,"Mobile eye tracking is beneficial for the analysis of humanâ€“machine interactions of tangible products, as it tracks the eye movements reliably in natural environments, and it allows for insights into human behaviour and the associated cognitive processes. However, current methods require a manual screening of the video footage, which is time-consuming and subjective. This work aims to automatically detect cognitive demanding phases in mobile eye tracking recordings. The approach presented combines the userâ€™s perception (gaze) and action (hand) to isolate demanding interactions based upon a multi-modal feature level fusion. It was validated in a usability study of a 3D printer with 40 participants by comparing the usability problems found to a thorough manual analysis. The new approach detected 17 out of 19 problems, while the time for manual analyses was reduced by 63%. More than eye tracking alone, adding the information of the hand enriches the insights into human behaviour. The field of AI could significantly advance our approach by improving the hand-tracking through region proposal CNNs, by detecting the parts of a product and mapping the demanding interactions to these parts, or even by a fully automated end-to-end detection of demanding interactions via deep learning. This could set the basis for machines providing real-time assistance to the machineâ€™s users in cases where they are struggling. Â© 2017, Springer-Verlag GmbH Deutschland.",,,
10.20965/jaciii.2017.p1291,2017,"Touyama H., Sakuda M.",Online control of a virtual object with collaborative SSVEP,"In this paper, we propose a brain-computer interface (BCI) based on collaborative steady-state visually evoked potential (SSVEP). A technique for estimating the common direction of the gaze of multiple subjects is studied with a view to controlling a virtual object in a virtual environment. The electro-encephalograms (EEG) of eight volunteers are simultaneously recorded with two virtual cubes as visual stimuli. These two virtual cubes flicker at different rates, 6 Hz and 8 Hz, and the corresponding SSVEP is observed around the occipital area. The amplitude spectra of the EEG activity of individual subjects are analyzed, averaged, and synthesized to obtain the collaborative SSVEP. Machine learning is applied to estimate the common gaze direction of the eight subjects with the supervised data from fewer than eight subjects. The estimation accuracy is perfect only in the case of the collaborative SSVEP. One-dimensional control of a virtual ball is performed by controlling the common eye gaze direction, which induces the collaborative SSVEP.",,,
10.1016/j.cmpb.2017.08.018,2017,"Kalaie S., Gooya A.",Vascular tree tracking and bifurcation points detection in retinal images using a hierarchical probabilistic model,"Background and Objective Retinal vascular tree extraction plays an important role in computer-aided diagnosis and surgical operations. Junction point detection and classification provide useful information about the structure of the vascular network, facilitating objective analysis of retinal diseases. Methods In this study, we present a new machine learning algorithm for joint classification and tracking of retinal blood vessels. Our method is based on a hierarchical probabilistic framework, where the local intensity cross sections are classified as either junction or vessel points. Gaussian basis functions are used for intensity interpolation, and the corresponding linear coefficients are assumed to be samples from class-specific Gamma distributions. Hence, a directed Probabilistic Graphical Model (PGM) is proposed and the hyperparameters are estimated using a Maximum Likelihood (ML) solution based on Laplace approximation. Results The performance of proposed method is evaluated using precision and recall rates on the REVIEW database. Our experiments show the proposed approach reaches promising results in bifurcation point detection and classification, achieving 88.67% precision and 88.67% recall rates. Conclusions This technique results in a classifier with high precision and recall when comparing it with Xu's method. Â© 2017 Elsevier B.V.",,,
10.1142/S0218001417560158,2017,"Kim H.-I., Kim J.-B., Park R.-H.",Efficient and Fast Iris Localization Using Binary Radial Gradient Features for Human-Computer Interaction,"This paper proposes an efficient and fast iris localization method. It uses support vector machine learning of iris features that represent closed outer and inner iris boundaries encompassing a low-intensity region. In addition, depending on the location of the iris in an eye image, an iris detection method is proposed based on three sub-datasets of eye images (middle, right, and left sub-datasets) with different iris features. The proposed method is implemented using fast sliding window and fast computation of the iris detection score with binary features. Compared with state-of-the-art methods, experimental results show that the proposed method is twice as fast and has comparable accuracy, even when factoring in head rotation, glasses, and highlights. Â© 2017 World Scientific Publishing Company.",,,
10.1007/s11042-016-4155-y,2017,"Banitalebi-Dehkordi A., Pourazad M.T., Nasiopoulos P.",A learning-based visual saliency prediction model for stereoscopic 3D video (LBVS-3D),"Saliency prediction models provide a probabilistic map of relative likelihood of an image or video region to attract the attention of the human visual system. Over the past decade, many computational saliency prediction models have been proposed for 2D images and videos. Considering that the human visual system has evolved in a natural 3D environment, it is only natural to want to design visual attention models for 3D content. Existing monocular saliency models are not able to accurately predict the attentive regions when applied to 3D image/video content, as they do not incorporate depth information. This paper explores stereoscopic video saliency prediction by exploiting both low-level attributes such as brightness, color, texture, orientation, motion, and depth, as well as high-level cues such as face, person, vehicle, animal, text, and horizon. Our model starts with a rough segmentation and quantifies several intuitive observations such as the effects of visual discomfort level, depth abruptness, motion acceleration, elements of surprise, size and compactness of the salient regions, and emphasizing only a few salient objects in a scene. A new fovea-based model of spatial distance between the image regions is adopted for considering local and global feature calculations. To efficiently fuse the conspicuity maps generated by our method to one single saliency map that is highly correlated with the eye-fixation data, a random forest based algorithm is utilized. The performance of the proposed saliency model is evaluated against the results of an eye-tracking experiment, which involved 24 subjects and an in-house database of 61 captured stereoscopic videos. Our stereo video database as well as the eye-tracking data are publicly available along with this paper. Experiment results show that the proposed saliency prediction method achieves competitive performance compared to the state-of-the-art approaches. Â© 2016, Springer Science+Business Media New York.",,,
10.1109/ISMAR-Adjunct.2017.72,2017,"Piumsomboon T., Dey A., Ens B., Lee G., Billinghurst M.",CoVAR: Mixed-Platform Remote Collaborative Augmented and Virtual Realities System with Shared Collaboration Cues,"We present CoVAR, a novel Virtual Reality (VR) and Augmented Reality (AR) system for remote collaboration. It supports collaboration between AR and VR users by sharing a 3D reconstruction of the AR user's environment. To enhance this mixed platform collaboration, it provides natural inputs such as eye-gaze and hand gestures, remote embodiment through avatar's head and hands, and awareness cues of field-of-view and gaze cue. In this paper, we describe the system architecture, setup and calibration procedures, input methods and interaction, and collaboration enhancement features. Â© 2017 IEEE.",,,
10.1109/ICCSE.2017.8085473,2017,"Zou J., Zhang H., Weng T.",New 2D pupil and spot center positioning technology under real-Time eye tracking,"Eye tracking technology is an important technology in the field of artificial intelligence(AI). Eye tracking will promote the development of human-computer interaction(HCI). Spot Center Corneal Reflex (PCCR) is an eye tracking technique that relies on pupils and reflected light spots. Therefore, it is significant to accurately locate the pupil position and reflected spot position. The traditional algorithm used the edge and the gray information of the image to extract the contours of the pupil and the spot, and then determine the location through the fitting. However, the collected images will be affected by many environmental factors, the boundary point and the fitting calculation will greatly affect the efficiency and stability of the algorithm. In this paper, a new method combining image gradient information with threshold segmentation is proposed. Gradient detection and threshold segmentation are carried out in the region of interest, and the pupil and reflection spot are extracted directly. So, this paper use the centroid method to calculate the center coordinates more accurately. The algorithm has a good robust performance to avoid noise and environmental effects. The algorithm used to develop human eye tracking system to achieve real-Time eye tracking, while ensuring accuracy. Â© 2017 IEEE.",,,
10.1016/j.neucom.2016.05.115,2017,"Deng C., Wang B., Lin W., Huang G.-B., Zhao B.",Effective visual tracking by pairwise metric learning,"For robust visual tracking, appearance modeling should be able to well separate the object from its backgrounds, while accurately adapt to its appearance variations. However, most of the existing tracking methods mainly focus on one of the two aspects; or design two different modules to combine them with the price of double computational cost. In this paper, by using pairwise metric learning, we present a novel appearance model for robust visual tracking. Specifically, visual tracking is viewed as a pairwise regression problem, and extreme learning machine (ELM) is utilized to construct the pairwise regression framework. In ELM-based pairwise training, two constraints are enforced: the target observations must have different regression outputs from those background ones; while the various target observations during tracking should have approximate regression outputs. Thus, the discriminative and generative capabilities are fully considered in a single object tracking model. Moreover, online sequential ELM (OS-ELM) is used to update the resulting appearance model, thereby leading to a more robust tracking process. Extensive experimental evaluations on challenging video sequences demonstrate the effectiveness and efficiency of the proposed tracker. Â© 2017 Elsevier B.V.",,,
10.1145/3126594.3126614,2017,"Zhang X., Sugano Y., Bulling A.",Everyday eye contact detection using unsupervised gaze target discovery,"Eye contact is an important non-verbal cue in social signal processing and promising as a measure of overt attention in human-object interactions and attentive user interfaces. However, robust detection of eye contact across different users, gaze targets, camera positions, and illumination conditions is notoriously challenging. We present a novel method for eye contact detection that combines a state-of-the-art appearancebased gaze estimator with a novel approach for unsupervised gaze target discovery, i.e. without the need for tedious and time-consuming manual data annotation. We evaluate our method in two real-world scenarios: detecting eye contact at the workplace, including on the main work display, from cameras mounted to target objects, as well as during everyday social interactions with the wearer of a head-mounted egocentric camera. We empirically evaluate the performance of our method in both scenarios and demonstrate its effectiveness for detecting eye contact independent of target object type and size, camera position, and user and recording environment. Â© 2017 Copyright is held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/3126594.3126653,2017,"Bylinskii Z., Kim N., O'Donovan P., Alsheikh S., Madan S., Pfister H., Durand F., Russell B., Hertzmann A.",Learning visual importance for graphic designs and data visualizations,"Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process. Â© 2017 ACM.",,,
10.1109/ISPA.2017.8073565,2017,"Kupas D., Harangi B., Czifra G., Andrassy G.",Decision support system for the diagnosis of neurological disorders based on gaze tracking,"Current diagnosis of neurological disorders is an expensive and time-consuming task. Our goal is to make this procedure easier and more accurate using a digital eye scanner. Our system can help in making diagnoses, assists in the practice and shortens the time needed to find the appropriate treatment. First and foremost we collect all important visual effects in the field of neurological examination and create a video to make possible the testing of the eye movement of the patient during the video. Their gaze data is collected by an appropriate eye tracker, then we analyze the gaze information in order to evaluate the mental state of the patient using machine learning based algorithms. According to the experimental results, our proposed technique can separate the healthy and ill patients from each other using their gaze data. Â© 2017 IEEE.",,,
10.1145/3109761.3158376,2017,"Al Raisi S.F., Edirisinghe E.",A machine learning based approach to human observer behaviour analysis in CCTV video analytics & forensics,"Human observer behaviour analysis in image and video inspection in many areas of practical application is conducted based on using data captured by eye tracking devices. Such data is analysed using statistical approaches leading to the creation of useful information and the ability to make decisions about the content. CCTV observer behaviour analysis is one example of a most widely used application. Unfortunately, the information and knowledge that such statistical approaches to data analysis can create is rather limited, especially the trends and patterns of data cannot be easily analysed. Thus, important information and knowledge that the data can provide may not be identifiable. In this paper, we proposed a novel approach to human observer eye tracking data analysis based on machine learning algorithms. Further, in order to conduct a more detailed and practically useful data analysis, we specifically analyse the attention human observers given instructions to search for specified content. We provide experimental results to demonstrate the significance and novelty of the information and knowledge that this novel approach to data analysis can provide. To the authors' knowledge, there is no work in literature that has proposed the use of machine learning in eye tracking data analysis. Â© 2017 Association for Computing Machinery.",,,
10.1145/3131277.3132180,2017,"Pfeuffer K., Mayer B., Mardanbegi D., Gellersen H.",Gaze + Pinch interaction in virtual reality,"Virtual reality affords experimentation with human abilities beyond whatâ€™s possible in the real world, toward novel senses of interaction. In many interactions, the eyes naturally point at objects of interest while the hands skilfully manipulate in 3D space. We explore a particular combination for virtual reality, the Gaze + Pinch interaction technique. It integrates eye gaze to select targets, and indirect freehand gestures to manipulate them. This keeps the gesture use intuitive like direct physical manipulation, but the gestureâ€™s effect can be applied to any object the user looks at - whether located near or far. In this paper, we describe novel interaction concepts and an experimental system prototype that bring together interaction technique variants, menu interfaces, and applications into one unified virtual experience. Proof-of-concept application examples were developed and informally tested, such as 3D manipulation, scene navigation, and image zooming, illustrating a range of advanced interaction capabilities on targets at any distance, without relying on extra controller devices. Â© 2017 Association for Computing Machinery.",,,
10.1145/3131277.3134352,2017,"Yamamoto M., Sakiyama H., Fukumori S., Nagamatsu T.",An unobservable and untraceable input method for public spaces by reconstructing points of gaze only on servers,We propose a gaze-based secure interaction by measuring only the optical axis of the eye on a computer and reconstructing the visual axis of the eye and point of gaze on a server. This method enables unobservable and untraceable input in public spaces even when using public computers. Â© 2017 Copyright is held by the owner/author(s).,,,
10.1109/CIBCB.2017.8058565,2017,"Chen O.T.-C., Chen P.-C., Tsai Y.-T.",Attention estimation system via smart glasses,"Attention plays a critical role in effective learning. By means of attention assessment, it helps learners improve and review their learning processes, and even discover Attention Deficit Hyperactivity Disorder (ADHD). Hence, this work employs modified smart glasses which have an inward facing camera for eye tracking, and an inertial measurement unit for head pose estimation. The proposed attention estimation system consists of eye movement detection, head pose estimation, and machine learning. In eye movement detection, the central point of the iris is found by the locally maximum curve via the Hough transform where the region of interest is derived by the identified left and right eye corners. The head pose estimation is based on the captured inertial data to generate physical features for machine learning. Here, the machine learning adopts Genetic Algorithm (GA)-Support Vector Machine (SVM) where the feature selection of Sequential Floating Forward Selection (SFFS) is employed to determine adequate features, and GA is to optimize the parameters of SVM. Our experiments reveal that the proposed attention estimation system can achieve the accuracy of 93.1% which is fairly good as compared to the conventional systems. Therefore, the proposed system embedded in smart glasses brings users mobile, convenient, and comfortable to assess their attention on learning or medical symptom checker. Â© 2017 IEEE.",,,
10.1109/TIP.2017.2721112,2017,"Fang Y., Zhang C., Li J., Lei J., Da Silva M.P., Le Callet P.",Visual Attention Modeling for Stereoscopic Video: A Benchmark and Computational Model,"In this paper, we investigate the visual attention modeling for stereoscopic video from the following two aspects. First, we build one large-scale eye tracking database as the benchmark of visual attention modeling for stereoscopic video. The database includes 47 video sequences and their corresponding eye fixation data. Second, we propose a novel computational model of visual attention for stereoscopic video based on Gestalt theory. In the proposed model, we extract the low-level features, including luminance, color, texture, and depth, from discrete cosine transform coefficients, which are used to calculate feature contrast for the spatial saliency computation. The temporal saliency is calculated by the motion contrast from the planar and depth motion features in the stereoscopic video sequences. The final saliency is estimated by fusing the spatial and temporal saliency with uncertainty weighting, which is estimated by the laws of proximity, continuity, and common fate in Gestalt theory. Experimental results show that the proposed method outperforms the state-of-the-art stereoscopic video saliency detection models on our built large-scale eye tracking database and one other database (DML-ITRACK-3D). Â© 1992-2012 IEEE.",,,
10.1109/TCYB.2016.2588526,2017,"Zhang L., Suganthan P.N.",Visual Tracking with Convolutional Random Vector Functional Link Network,"Deep neural network-based methods have recently achieved excellent performance in visual tracking task. As very few training samples are available in visual tracking task, those approaches rely heavily on extremely large auxiliary dataset such as ImageNet to pretrain the model. In order to address the discrepancy between the source domain (the auxiliary data) and the target domain (the object being tracked), they need to be finetuned during the tracking process. However, those methods suffer from sensitivity to the hyper-parameters such as learning rate, maximum number of epochs, size of mini-batch, and so on. Thus, it is worthy to investigate whether pretraining and fine tuning through conventional back-prop is essential for visual tracking. In this paper, we shed light on this line of research by proposing convolutional random vector functional link (CRVFL) neural network, which can be regarded as a marriage of the convolutional neural network and random vector functional link network, to simplify the visual tracking system. The parameters in the convolutional layer are randomly initialized and kept fixed. Only the parameters in the fully connected layer need to be learned. We further propose an elegant approach to update the tracker. In the widely used visual tracking benchmark, without any auxiliary data, a single CRVFL model achieves 79.0% with a threshold of 20 pixels for the precision plot. Moreover, an ensemble of CRVFL yields comparatively the best result of 86.3%. Â© 2017 IEEE.",,,
10.1109/EMBC.2017.8037186,2017,"Liu C., Herrup K., Shi B.E.",Remote gaze tracking system for 3D environments,"Eye tracking systems are typically divided into two categories: remote and mobile. Remote systems, where the eye tracker is located near the object being viewed by the subject, have the advantage of being less intrusive, but are typically used for tracking gaze points on fixed two dimensional (2D) computer screens. Mobile systems such as eye tracking glasses, where the eye tracker are attached to the subject, are more intrusive, but are better suited for cases where subjects are viewing objects in the three dimensional (3D) environment. In this paper, we describe how remote gaze tracking systems developed for 2D computer screens can be used to track gaze points in a 3D environment. The system is non-intrusive. It compensates for small head movements by the user, so that the head need not be stabilized by a chin rest or bite bar. The system maps the 3D gaze points of the user onto 2D images from a scene camera and is also located remotely from the subject. Measurement results from this system indicate that it is able to estimate gaze points in the scene camera to within one degree over a wide range of head positions. Â© 2017 IEEE.",,,
10.1109/EMBC.2017.8036944,2017,"Wang H., Antonelli M., Shi B.E.",Using point cloud data to improve three dimensional gaze estimation,"This paper addresses the problem of estimating gaze location in the 3D environment using a remote eye tracker. Instead of relying only on data provided by the eye tracker, we investigate how to integrate gaze direction with the point-cloud-based representation of the scene provided by a Kinect sensor. The algorithm first combines the gaze vectors for the two eyes provided by the eye tracker into a single gaze vector emanating from a point in between the two eyes. The gaze target in the three dimensional environment is then identified by finding the point in the 3D point cloud that is closest to the gaze vector. Our experimental results demonstrate that the estimate of the gaze target location provided by this method is significantly better than that provided when considering gaze information alone. It is also better than two other methods for integrating point cloud information: (1) finding the 3D point closest to the gaze location as estimated by triangulating the gaze vectors from the two eyes, and (2) finding the 3D point with smallest average distance to the two gaze vectors considered individually. The proposed method has an average error of 1.7 cm in a workspace of 25 Ã— 23 Ã— 24 cm located at a distance of 60 cm from the user. Â© 2017 IEEE.",,,
10.1145/3123024.3123200,2017,"Ishimaru S., Dengel A.",ARFLED: Ability recognition framework for learning and education,"Learning is one of the vital behaviors of human beings. This paper demonstrates a framework to augment learning activities by packaging two key ideas: Eyetifact and HyperMind. Eyeti-fact is a system that converts data of eye movements beyond the difference of sensing devices to collect a large amount of training data for machine learning. HyperMind is a digital textbook that displays learning materials dynamically based on a learner's cognitive states as measured by several sensors. In order to implement these two ideas, we have conducted experiments related to eyewear computing, textbook reading behavior analysis, and stress sensing. The contributions of this research are to investigate approaches that recognize human abilities and to transfer them from experts to others. Copyright Â© 2017 ACM.",,,
10.1145/3098279.3119924,2017,BÃ¢ce M.,"Augmenting human interaction capabilities with proximity, natural gestures, and eye gaze","Nowadays, humans are surrounded by many complex computer systems. When people interact among each other, they use multiple modalities including voice, body posture, hand gestures, facial expressions, or eye gaze. Currently, computers can only understand a small subset of these modalities, but such cues can be captured by an increasing number of wearable devices. This research aims to improve traditional human-human and human-machine interaction by augmenting humans with wearable technology and developing novel user interfaces. More specifically, (i) we investigate and develop systems that enable a group of people in close proximity to interact using in-air hand gestures and facilitate effortless information sharing. Additionally, we focus on (ii) eye gaze which can further enrich the interaction between humans and cyber-physical systems.",,,
10.1177/0278364917726587,2017,"Potapova E., Zillich M., Vincze M.",Survey of recent advances in 3D visual attention for robotics,"3D visual attention plays an important role in both human and robotics perception that yet has to be explored in full detail. However, the majority of computer vision and robotics methods are concerned only with 2D visual attention. This survey presents findings and approaches that cover 3D visual attention in both human and robot vision, summarizing the last 30 years of research and also looking beyond computational methods. First, we present work in such fields as biological vision and neurophysiology, studying 3D attention in human observers. This provides a view of the role attention plays at the system level for biological vision. Then, we cover computer and robot vision approaches that take 3D visual attention into account. We compare approaches with respect to different categories, such as feature-based, data-based, or depth-based visual attention, and draw conclusions on what advances will help robotics to cope better with complex real-world settings and tasks. Â© The Author(s) 2017.",,,
10.1145/3127588,2017,"Breeden K., Hanrahan P.",Gaze data for the analysis of attention in feature films,"Film directors are masters at controlling what we look at when we watch a film. However, there have been few quantitative studies of how gaze responds to cinematographic conventions thought to influence attention. We have collected and are releasing a dataset designed to help investigate eye movements in response to higher level features such as faces, dialogue, camera movements, image composition, and edits. The dataset, which will be released to the community, includes gaze information for 21 viewers watching 15 clips from live action 2D films, which have been hand annotated for high level features. This work has implications for the media studies, display technology, immersive reality, and human cognition. Â© 2017 ACM.",,,
10.1109/TIP.2017.2675343,2017,"Lebeda K., Hadfield S., Bowden R.",TMAGIC: A Model-Free 3D Tracker,"Significant effort has been devoted within the visual tracking community to rapid learning of object properties on the fly. However, state-of-the-art approaches still often fail in cases such as rapid out-of-plane rotation, when the appearance changes suddenly. One of the major contributions of this paper is a radical rethinking of the traditional wisdom of modeling 3D motion as appearance changes during tracking. Instead, 3D motion is modeled as 3D motion. This intuitive but previously unexplored approach provides new possibilities in visual tracking research. First, 3D tracking is more general, as large out-of-plane motion is often fatal for 2D trackers, but helps 3D trackers to build better models. Second, the tracker's internal model of the object can be used in many different applications and it could even become the main motivation, with tracking supporting reconstruction rather than vice versa. This effectively bridges the gap between visual tracking and structure from motion. A new benchmark data set of sequences with extreme out-of-plane rotation is presented and an online leader-board offered to stimulate new research in the relatively underdeveloped area of 3D tracking. The proposed method, provided as a baseline, is capable of successfully tracking these sequences, all of which pose a considerable challenge to 2D trackers (error reduced by 46%). Â© 2017 IEEE.",,,
10.1016/j.ijhcs.2017.04.002,2017,"Deng S., Jiang N., Chang J., Guo S., Zhang J.J.",Understanding the impact of multimodal interaction using gaze informed mid-air gesture control in 3D virtual objects manipulation,"Multimodal interactions provide users with more natural ways to manipulate virtual 3D objects than using traditional input methods. An emerging approach is gaze modulated pointing, which enables users to perform object selection and manipulation in a virtual space conveniently through the use of a combination of gaze and other interaction techniques (e.g., mid-air gestures). As gaze modulated pointing uses different sensors to track and detect user behaviours, its performance relies on the user's perception on the exact spatial mapping between the virtual space and the physical space. An underexplored issue is, when the spatial mapping differs with the user's perception, manipulation errors (e.g., out of boundary errors, proximity errors) may occur. Therefore, in gaze modulated pointing, as gaze can introduce misalignment of the spatial mapping, it may lead to user's misperception of the virtual environment and consequently manipulation errors. This paper provides a clear definition of the problem through a thorough investigation on its causes and specifies the conditions when it occurs, which is further validated in the experiment. It also proposes three methods (Scaling, Magnet and Dual-gaze) to address the problem and examines them using a comparative study which involves 20 participants with 1040 runs. The results show that all three methods improved the manipulation performance with regard to the defined problem where Magnet and Dual-gaze delivered better performance than Scaling. This finding could be used to inform a more robust multimodal interface design supported by both eye tracking and mid-air gesture control without losing efficiency and stability. Â© 2017 Elsevier Ltd",,,
10.1111/cgf.12869,2017,"Bock A., Svensson Ã…., Kleiner A., Lundberg J., Ropinski T.",A Visualization-Based Analysis System for Urban Search & Rescue Mission Planning Support,"We propose a visualization system for incident commanders (ICs) in urban searchÂ andÂ rescue scenarios that supports path planning in post-disaster structures. Utilizing point cloud data acquired from unmanned robots, we provide methods for the assessment of automatically generated paths. As data uncertainty and a priori unknown information make fully automated systems impractical, we present the IC with a set of viable access paths, based on varying risk factors, in a 3D environment combined with visual analysis tools enabling informed decision making and trade-offs. Based on these decisions, a responder is guided along the path by the IC, who can interactively annotate and reevaluate the acquired point cloud and generated paths to react to the dynamics of the situation. We describe visualization design considerations for our system and decision support systems in general, technical realizations of the visualization components, and discuss the results of two qualitative expert evaluation; one online study with nine searchÂ andÂ rescue experts and an eye-tracking study in which four experts used the system on an application case. Â© 2016 The Authors Computer Graphics Forum Â© 2016 The Eurographics Association and John Wiley & Sons Ltd.",,,
10.3837/tiis.2017.08.012,2017,"Lim S., Lee D.",Real-time eye tracking using ir stereo camera for indoor and outdoor environments,"We propose a novel eye tracking method that can estimate 3D world coordinates using an infrared (IR) stereo camera for indoor and outdoor environments. This method first detects dark evidences such as eyes, eyebrows and mouths by fast multi-level thresholding. Among these evidences, eye pair evidences are detected by evidential reasoning and geometrical rules. For robust accuracy, two classifiers based on multiple layer perceptron (MLP) using gradient local binary patterns (GLBPs) verify whether the detected evidences are real eye pairs or not. Finally, the 3D world coordinates of detected eyes are calculated by region-based stereo matching. Compared with other eye detection methods, the proposed method can detect the eyes of people wearing sunglasses due to the use of the IR spectrum. Especially, when people are in dark environments such as driving at nighttime, driving in an indoor carpark, or passing through a tunnel, human eyes can be robustly detected because we use active IR illuminators. In the experimental results, it is shown that the proposed method can detect eye pairs with high performance in real-time under variable illumination conditions. Therefore, the proposed method can contribute to human-computer interactions (HCIs) and intelligent transportation systems (ITSs) applications such as gaze tracking, windshield head-up display and drowsiness detection. Â© 2017 KSII.",,,
10.1109/CVPRW.2017.284,2017,"Zhang X., Sugano Y., Fritz M., Bulling A.",It's Written All over Your Face: Full-Face Appearance-Based Gaze Estimation,"Eye gaze is an important non-verbal cue for human affect analysis. Recent gaze estimation work indicated that information from the full face region can benefit performance. Pushing this idea further, we propose an appearance-based method that, in contrast to a long-standing line of work in computer vision, only takes the full face image as input. Our method encodes the face image using a convolutional neural network with spatial weights applied on the feature maps to flexibly suppress or enhance information in different facial regions. Through extensive evaluation, we show that our full-face method significantly outperforms the state of the art for both 2D and 3D gaze estimation, achieving improvements of up to 14.3% on MPIIGaze and 27.7% on EYEDIAP for person-independent 3D gaze estimation. We further show that this improvement is consistent across different illumination conditions and gaze directions and particularly pronounced for the most challenging extreme head poses. Â© 2017 IEEE.",,,
10.1109/AIM.2017.8013985,2017,"Venkataraman H., Assfalg R.",Driver performance detection & recommender system in vehicular environment using video streaming analytics,"This tutorial deals with how the overall human performance while working can be detected, under different conditions and scenarios. The tutorial will talk about how regular and real-time monitoring of people can be carried out through eye-tracking and how it can be integrated with other environment factors to develop a recommender system. Particularly, the speakers would articulate a vehicular driving scenario and explain how a combined use of eye-tracking and face-tracking can not only help the drivers but also significantly assist in reducing the road accidents, thereby increase the road safety. Finally, the speakers will present the use of existing eye-trackers along with the integrated eye-tracker and face-tracker that is under indigenous development. This tutorial is intended for a wide section of audience - ranging from clusters of automobile designers/manufacturers, researchers in mechatronics and students researching on different aspects of connected cars, computer vision, signal processing, image processing and machine learning and data analytics. Â© 2017 IEEE.",,,
10.1145/3105971.3105977,2017,"Serim B., Chech L., Vasilieva M., Papa L., Gamberini L., Jacucci G.",Exploring gaze-adaptive features for interacting with multi-document visualizations,"We present a preliminary design study for utilizing eye tracking to support interacting with a multi-document visualization. Complex information seeking tasks can involve collection and comparison of multiple documents, resulting in long and sustained search sessions. The sustained and evolving nature of the session also provides the search interface the opportunity to gather information on the user state and interaction history, which can be used to adapt the information content and representation. We designed a system to evaluate how eye tracking information can be used for adapting the visual salience of information entities. The interface features documents and related keywords that are arranged in a radial layout configuration called the intent radar. Reading history and visual attention, as registered by eye tracking data, are respectively used to trace read items and for visual cueing. We evaluated the interface with 16 participants to gather subjective feedback about specific components and features of the interface. The overall results show that the interface and gaze-related appearance of keywords was positively received by the users. Â© 2017 Copyright held by the owner/author(s).",,,
10.1016/j.neucom.2017.03.070,2017,"Yun X., Xiao G.",Spiral visual and motional tracking,"Constructing a visual appearance model is essential for visual tracking. However, relying only on the visual model during appearance changes is insufficient and may even interfere with achieving good results. Although several visual tracking algorithms emphasize motional tracking that estimates the motion state of the object center between consecutive frames, they suffer from accumulated error during runtime. As neither visual nor motional trackers are capable of performing well separately, several groups have recently proposed simultaneous visual and motional tracking algorithms. However, because tracking problems are often NP-hard, these algorithms cannot provide good solutions for the reason that they are driven top-down with low flexibility and often encounter drift problems. This paper proposes a spiral visual and motional tracking (SVMT) algorithm which, unlike existing algorithms, builds a strong tracker by cyclically combining weak trackers from both the visual and motional layers. In the spiral-like framework, an iteration model is used to search for the optimum until convergence, with the potential for achieving optimization. Three learned procedures including visual classification, motional estimation, and risk analysis are integrated into the generalized framework and implement corresponding modifications with regard to their performances. The experimental results demonstrate that SVMT performs well in terms of accuracy and robustness. Â© 2017 Elsevier B.V.",,,
10.1007/s00138-017-0852-4,2017,"Huang Q., Veeraraghavan A., Sabharwal A.",TabletGaze: dataset and analysis for unconstrained appearance-based gaze estimation in mobile tablets,"We study gaze estimation on tablets; our key design goal is uncalibrated gaze estimation using the front-facing camera during natural use of tablets, where the posture and method of holding the tablet are not constrained. We collected a large unconstrained gaze dataset of tablet users, labeled Rice TabletGaze dataset. The dataset consists of 51 subjects, each with 4 different postures and 35 gaze locations. Subjects vary in race, gender and in their need for prescription glasses, all of which might impact gaze estimation accuracy. We made three major observations on the collected data and employed a baseline algorithm for analyzing the impact of several factors on gaze estimation accuracy. The baseline algorithm is based on multilevel HoG feature and Random Forests regressor, which achieves a mean error of 3.17Â cm. We perform extensive evaluation on the impact of various practical factors such as person dependency, dataset size, race, wearing glasses and user posture on the gaze estimation accuracy. Â© 2017, Springer-Verlag GmbH Germany.",,,
10.1007/s11517-016-1594-6,2017,"Khalil A., Faisal A., Lai K.W., Ng S.C., Liew Y.M.",2D to 3D fusion of echocardiography and cardiac CT for TAVR and TAVI image guidance,"This study proposed a registration framework to fuse 2D echocardiography images of the aortic valve with preoperative cardiac CT volume. The registration facilitates the fusion of CT and echocardiography to aid the diagnosis of aortic valve diseases and provide surgical guidance during transcatheter aortic valve replacement and implantation. The image registration framework consists of two major steps: temporal synchronization and spatial registration. Temporal synchronization allows time stamping of echocardiography time series data to identify frames that are at similar cardiac phase as the CT volume. Spatial registration is an intensity-based normalized mutual information method applied with pattern search optimization algorithm to produce an interpolated cardiac CT image that matches the echocardiography image. Our proposed registration method has been applied on the short-axis â€œMercedes Benzâ€?sign view of the aortic valve and long-axis parasternal view of echocardiography images from ten patients. The accuracy of our fully automated registration method was 0.81Â Â±Â 0.08 and 1.30Â Â±Â 0.13Â mm in terms of Dice coefficient and Hausdorff distance for short-axis aortic valve view registration, whereas for long-axis parasternal view registration it was 0.79Â Â±Â 0.02 and 1.19Â Â±Â 0.11Â mm, respectively. This accuracy is comparable to gold standard manual registration by expert. There was no significant difference in aortic annulus diameter measurement between the automatically and manually registered CT images. Without the use of optical tracking, we have shown the applicability of this technique for effective fusion of echocardiography with preoperative CT volume to potentially facilitate catheter-based surgery. Â© 2016, International Federation for Medical and Biological Engineering.",,,
10.1007/s11042-016-3662-1,2017,Li J.,A synthetic research on the multimedia data encryption based mobile computing security enhancement model and multi-channel mobile human computer interaction framework,"With the development of computer network, people could obtain information through the network with stronger impulsion, the dependence of the requirements also becomes higher, this is not only reflected in the increase of information, but to obtain and submit more reflected in real-time and easy to access to information on the pressing needs of the. Therefore, people devoted all aspects from the terminal, network and software platforms to make unremitting efforts. Under this basis, we conduct synthetic research on the data encryption based mobile computing security enhancement model and multi-channel mobile human computer interaction framework in this paper. We firstly introduce the discrete-time Hopfield neural network based data encryption algorithm beyond the analysis on the information flow security and type based model, data security review and attack model under mobile computing environment. We improve learning algorithm of the MD method to avoid the sample mobile and cross interference problems of Hebb rule. Later, we integrate the multi-channel concept to propose the new multi-channel manâ€“machine interaction. In our framework, the 3D interactive technology, speech recognition and synthesis technology, natural language understanding and processing technology, eye tracking technology, posture, input, tactile, force display basic technology are taken into consideration for the synthetic analysis. The result from the experimental simulation proves that our methodology obtains better effectiveness and feasibility from both the angels of data security and interface experience. Â© 2016, Springer Science+Business Media New York.",,,
10.1145/3084822.3084841,2017,"Thies J., ZollhÃ¶fer M., Stamminger M., Theobalt C., NieÃŸner M.",Demo of FaceVR: Real-time facial reenactment and eye gaze control in virtual reality,"We introduce FaceVR, a novel method for gaze-aware facial reenactment in the Virtual Reality (VR) context. The key component of FaceVR is a robust algorithm to perform real-time facial motion capture of an actor who is wearing a head-mounted display (HMD), as well as a new data-driven approach for eye tracking from monocular videos. In addition to these face reconstruction components, FaceVR incorporates photo-realistic re-rendering in real time, thus allowing artificial modifications of face and eye appearances. For instance, we can alter facial expressions, change gaze directions, or remove the VR goggles in realistic re-renderings. In a live setup with a source and a target actor, we apply these newly-introduced algorithmic components. We assume that the source actor is wearing a VR device, and we capture his facial expressions and eye movement in real-time. For the target video, we mimic a similar tracking process; however, we use the source input to drive the animations of the target video, thus enabling gaze-aware facial reenactment. To render the modified target video on a stereo display, we augment our capture and reconstruction process with stereo data. In the end, FaceVR produces compelling results for a variety of applications, such as gaze-aware facial reenactment, reenactment in virtual reality, removal of VR goggles, and re-targeting of somebody's gaze direction in a video conferencing call. Â© 2017 Copyright held by the owner/author(s).",,,
10.1109/ISUVR.2017.13,2017,"Lee Y., Shin C., Plopski A., Itoh Y., Piumsomboon T., Dey A., Lee G., Kim S., Billinghurst M.",Estimating Gaze Depth Using Multi-Layer Perceptron,"In this paper we describe a new method for determining gaze depth in a head mounted eye-tracker. Eye-trackers are being incorporated into head mounted displays (HMDs), and eye-gaze is being used for interaction in Virtual and Augmented Reality. For some interaction methods, it is important to accurately measure the x-and y-direction of the eye-gaze and especially the focal depth information. Generally, eye tracking technology has a high accuracy in x-and y-directions, but not in depth. We used a binocular gaze tracker with two eye cameras, and the gaze vector was input to an MLP neural network for training and estimation. For the performance evaluation, data was obtained from 13 people gazing at fixed points at distances from 1m to 5m. The gaze classification into fixed distances produced an average classification error of nearly 10%, and an average error distance of 0.42m. This is sufficient for some Augmented Reality applications, but more research is needed to provide an estimate of a user's gaze moving in continuous space. Â© 2017 IEEE.",,,
10.1109/ICRA.2017.7989194,2017,"Zhou X., Cai H., Li Y., Liu H.",Two-eye model-based gaze estimation from a Kinect sensor,"In this paper, we present an effective and accurate gaze estimation method based on two-eye model of a subject with the tolerance of free head movement from a Kinect sensor. To accurately and efficiently determine the point of gaze, i) we employ two-eye model to improve the estimation accuracy; ii) we propose an improved convolution-based means of gradients method to localize the iris center in 3D space; iii) we present a new personal calibration method that only needs one calibration point. The method approximates the visual axis as a line from the iris center to the gaze point to determine the eyeball centers and the Kappa angles. The final point of gaze can be calculated by using the calibrated personal eye parameters. We experimentally evaluate the proposed gaze estimation method on eleven subjects. Experimental results demonstrate that our gaze estimation method has an average estimation accuracy around 1.99Â°, which outperforms many leading methods in the state-of-the-art. Â© 2017 IEEE.",,,
10.1109/ICRA.2017.7989707,2017,"Grammatikopoulou M., Yang G.-Z.",Gaze contingent control for optical micromanipulation,"Optical Tweezers (OT) have the advantage of non-contact interaction with target objects such as cells, overcoming the pitfall of obstructive adhesion forces which are present in contact micromanipulation. It is also feasible to manipulate a number of small microparts simultaneously or 3D structures by using multiple laser traps. These capabilities give rise to the potential to develop a human-robot interface to facilitate microassembly tasks. This paper presents a gaze contingent control framework and a method for 3D orientation estimation for optical micromanipulation. The proposed strategy aims to use OT as an interactive microassembly platform. The framework comprises I) a strategy to recognize the operator's intentions in order to interactively place and reconfigure the optical traps using the operator's eye fixation point, II) haptic constraints generated from the user's eye gaze to assist positioning of the assembled microparts and III) a method for 3D orientation estimation. The performance of the proposed framework is assessed through a set of experiments comparing it to the standard OT user interface. Three-dimensional manipulation and orientation estimation of a non-spherical microstructure are also performed. Â© 2017 IEEE.",,,
10.1109/ICRA.2017.7989697,2017,"Penkov S., Bordallo A., Ramamoorthy S.",Physical symbol grounding and instance learning through demonstration and eye tracking,"It is natural for humans to work with abstract plans which are often an intuitive and concise way to represent a task. However, high level task descriptions contain symbols and concepts which need to be grounded within the environment if the plan is to be executed by an autonomous robot. The problem of learning the mapping between abstract plan symbols and their physical instances in the environment is known as the problem of physical symbol grounding. In this paper, we propose a framework for Grounding and Learning Instances through Demonstration and Eye tracking (GLIDE). We associate traces of task demonstration to a sequence of fixations which we call fixation programs and exploit their properties in order to perform physical symbol grounding. We formulate the problem as a probabilistic generative model and present an algorithm for computationally feasible inference over the proposed model. A key aspect of our work is that we estimate fixation locations within the environment which enables the appearance of symbol instances to be learnt. Instance learning is a crucial ability when the robot does not have any knowledge about the model or the appearance of the symbols referred to in the plan instructions. We have conducted human experiments and demonstrate that GLIDE successfully grounds plan symbols and learns the appearance of their instances, thus enabling robots to autonomously execute tasks in initially unknown environments. Â© 2017 IEEE.",,,
10.1109/CYBConf.2017.7985779,2017,"Deng S., Chang J., Hu S.-M., Zhang J.J.",Gaze modulated disambiguation technique for gesture control in 3D virtual objects selection,"Inputs with multimodal information provide more natural ways to interact with virtual 3D environment. An emerging technique that integrates gaze modulated pointing with mid-air gesture control enables fast target acquisition and rich control expressions. The performance of this technique relies on the eye tracking accuracy which is not comparable with the traditional pointing techniques (e.g., mouse) yet. This will cause troubles when fine grainy interactions are required, such as selecting in a dense virtual scene where proximity and occlusion are prone to occur. This paper proposes a coarse-to-fine solution to compensate the degradation introduced by eye tracking inaccuracy using a gaze cone to detect ambiguity and then a gaze probe for decluttering. It is tested in a comparative experiment which involves 12 participants with 3240 runs. The results show that the proposed technique enhanced the selection accuracy and user experience but it is still with a potential to be improved in efficiency. This study contributes to providing a robust multimodal interface design supported by both eye tracking and mid-air gesture control. Â© 2017 IEEE.",,,
10.23919/MVA.2017.7986840,2017,"Liu Y., Lee B.-S., McKeown M.",A new reconstruction method in gaze estimation with natural head movement,"We present a novel reconstruction method for the appearance-based gaze estimation that allows inferring persons' gaze under natural head movement. We first study that the locally linear combination in the respective manifolds consisting of stable left and right eye appearances is efficient. The local structure of the manifolds is destroyed when there is head movement. This is due to the destruction of the intrinsic relations between the two eyes(left and right) when we do locally linear combinations. We then introduce a new combination of both eye appearances, which maintains the relation embedding into the reconstruction of the training stage. Through comparison with other well known methods, we show that the proposed method achieves an optimal performance with head pose variation. Â© 2017 MVA Organization All Rights Reserved.",,,
10.23919/MVA.2017.7986842,2017,"Ogawa T., Nakazawa A., Nishida T.",Point of gaze estimation using corneal surface reflection and omnidirectional camera image,"We present a human point of gaze estimation system using corneal surface reflection and omni-directional image taken by a fish eye. Only capturing an eye image, our system entables to find where a user is looking in 360P surrounding scene image. We first generates multiple perspective scene images from an equi-rectangular image and perform registration between corneal reflection and perspective images. We then compute the point of gaze using a 3D eye model and project the point to an omni-directional image. We evaluated the robustness of registration and accuracy of PoG estimations using two indoor and five outdoor scenes, and found that gaze mapping error was 5.526[deg] on average. This result shows the potential to the marketing and outdoor training system. Â© 2017 MVA Organization All Rights Reserved.",,,
10.1145/3079628.3079669,2017,"Hutt S., Mills C., Bosch N., Krasich K., Brockmole J., D'mello S.","Out of the Fr-""Eye""-ing Pan: Towards gaze-based models of attention during learning with technology in the classroom","Attention is critical to learning. Hence, advanced learning technologies should benefit from mechanisms to monitor and respond to learners' attentional states. We study the feasibility of integrating commercial off-The-shelf (COTS) eye trackers to monitor attention during interactions with a learning technology called GuruTutor. We tested our implementation on 135 students in a noisy computer-enabled high school classroom and were able to collect a median 95% valid eye gaze data in 85% of the sessions where gaze data was successfully recorded. Machine learning methods were employed to develop automated detectors of mind wandering (MW) -A phenomenon involving a shift in attention from task-related to task-unrelated thoughts that is negatively correlated with performance. Our student-independent, gaze-based models could detect MW with an accuracy (F1 of MW = 0.59) significantly greater than chance (F1 of MW = 0.24). Predicted rates of mind wandering were negatively related to posttest performance, providing evidence for the predictive validity of the detector. We discuss next steps towards developing gaze-based, attention-Aware, learning technologies that can be deployed in noisy, real-world environments. Â©2017 ACM.",,,
10.1109/ICDAR.2017.221,2017,"Garain U., Pandit O., Augereau O., Okoso A., Kise K.",Identification of Reader Specific Difficult Words by Analyzing Eye Gaze and Document Content,"This paper presents an approach for identifying reader specific difficult words while someone is reading a textual document. The work is motivated by the need of developing human-document interaction systems, in general and creating person-specific online educational content, in particular. Eye gaze information gives person specific behavior whereas textual content is analyzed to get general linguistic aspect of the document content. These two pieces of information are fused together through machine learning algorithms to identify the set of difficult words for a particular reader reading a particular document. An annotated dataset has been created where each word in a document is marked with its bounding box information and each reader identifies a set of difficult words while reading the document. The dataset consists of sixteen documents and each document is read by five subjects. The method is evaluated through recall-precision analysis. The impressive precision at high recall attests the feasibility of building a practical application based on this research. The experiment further brings out several interesting facts about human reading behaviour. Â© 2017 IEEE.",,,
10.1109/IC3D.2017.8251912,2017,"Fajnzylber V., Gonzalez L., Maldonado P., Del Villar R., Yanez R., Madariaga S., Magdics M., Sbert M.",Augmented film narrative by use of non-photorealistic rendering,"Our research is about the effects of non-photorealistic rendering in the perception of stereoscopic cinema. We conducted a study with 27 participants, using eye-tracker to evaluate ocular behavior during the free viewing of a stereoscopic film. We obtained ocular fixation data that were correlated with visual entropy. Gaze data leads us to believe that facing a highly abstract non-photorealistic scene the gaze seems to react under the effect of a perceptive anxiety. This ocular response seems to spontaneously assign greater sensorimotor resources to the analysis of the character's behavior, transforming his face into the main key to analyse the information and sense of the global scene. This behavior appears to us as an adaptive response to a visual environment endowed with abstraction and cognitive uncertainty. Our results could be useful for conceiving new non-photorealistic rendering tools for 3D films and cinematic virtual reality. Â© 2017 IEEE.",,,
10.1109/INCIT.2017.8257860,2017,"Lee H., Li Y., Yeh S.-C., Huang Y., Wu Z., Du Z.",ADHD assessment and testing system design based on virtual reality,"Attention deficit hyperactivity disorder (ADHD) is a common psychological and behavioral disorder in children, adolescents, and even some adults. Several symptoms are observed in ADHD patients, such as difficulty in paying attention, hyperactivity, impulsivity, and cognitive abnormalities. In the past, clinical diagnosis of ADHD was mainly conducted through a paper test scale based on relevant standards. Subsequently, although the time efficiency of test based on computer has improved, disadvantages have remained, for those tests being too tedious to attract the interest of individuals and being restricted by the content and the type. In this study, a combination of virtual reality, eye tracking, and electroencephalogram (EEG) signal acquisition technologies was used to conduct diagnostic assessment and testing on patients with ADHD. Selective attention, sustained attention, abstract reasoning, and cognitive transfer abilities were evaluated by performing visual and auditory continuous performance test (CPT) and the Wisconsin card sorting test (WCST) in a 3D virtual classroom with optional distraction factors. This study also preliminarily analyzed the eye tracking and EEG data and validated their effectiveness and convenience in ADHD diagnosis. The system may provide a deeper level of ADHD diagnosis and cognitive rehabilitation. Â© 2017 IEEE.",,,
10.1093/iwc/iww035,2017,"Ã–zacar K., HincapiÃ©-Ramos J.D., Takashima K., Kitamura Y.",3D Selection Techniques for Mobile Augmented Reality Head-Mounted Displays,"We conducted a user study evaluating five selection techniques for augmented reality in optical see-through head-mounted displays (OST-HMDs). The techniques we studied aim at supporting mobile usage scenarios where the devices do not need external tracking tools or special environments, and therefore we selected techniques that rely solely on tracking technologies built into conventional commercially available OST-HMDs [i.e. gesture trackers, gaze tracking and inertial measurement units (IMUs)]. While two techniques are based on raycasting using built-in IMU sensing, three techniques are based on a hand-controlled 3D cursor using gestural tracking. We compared these techniques in an experiment with 12 participants. Our results show that raycasting using head orientation (i.e. IMU on the headset) was the fastest, fatigueless and the most preferable technique to select spatially arranged objects. We discuss the implications of our findings for design of interaction techniques in mobile OST-HMDs. Â© The Author 2016. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved.",,,
10.1177/0301006617694188,2017,"Gosselin F., Faghel-Soubeyrand S.",Stationary Objects Flashed Periodically Appear to Move During Smooth Pursuit Eye Movement,"We discovered that a white disc flashed twice at the same location appears to move during smooth pursuit eye tracking in the direction opposite to that of the eye movement. We called this novel phenomenon movement-induced apparent motion (MIAM). Using the method of constant stimuli, we measured the required displacement of the second appearance of the disc in the pursuit direction to null the effect during the closed-loop stage of smooth pursuit eye tracking. We observed a strong linear relationship between the points of subjective stationarity and the inter-stimuli intervals for four smooth pursuit eye movement speeds. The slopes and y-intercepts of these linear fits were well predicted by the hypothesis according to which subjects saw illusory motion from the first to the second retinal projections of the flashed disc during smooth pursuit eye movement, with no extra-retinal signal compensation. Â© 2017, Â© The Author(s) 2017.",,,
10.1007/s11548-017-1580-y,2017,"Kogkas A.A., Darzi A., Mylonas G.P.",Gaze-contingent perceptually enabled interactions in the operating theatre,"Purpose: Improved surgical outcome and patient safety in the operating theatre are constant challenges. We hypothesise that a framework that collects and utilises information â€”especially perceptually enabled onesâ€”from multiple sources, could help to meet the above goals. This paper presents some core functionalities of a wider low-cost framework under development that allows perceptually enabled interaction within the surgical environment. Methods: The synergy of wearable eye-tracking and advanced computer vision methodologies, such as SLAM, is exploited. As a demonstration of one of the frameworkâ€™s possible functionalities, an articulated collaborative robotic arm and laser pointer is integrated and the set-up is used to project the surgeonâ€™s fixation point in 3D space. Results: The implementation is evaluated over 60 fixations on predefined targets, with distances between the subject and the targets of 92â€?12Â cm and between the robot and the targets of 42â€?93Â cm. The median overall system error is currently 3.98Â cm. Its real-time potential is also highlighted. Conclusions: The work presented here represents an introduction and preliminary experimental validation of core functionalities of a larger framework under development. The proposed framework is geared towards a safer and more efficient surgical theatre. Â© 2017, The Author(s).",,,
10.1007/s11227-016-1817-5,2017,"Kim B.C., Ko D., Jang U., Han H., Lee E.C.",3D Gaze tracking by combining eye- and facial-gaze vectors,"We propose a 3D gaze-tracking method that combines accurate 3D eye- and facial-gaze vectors estimated from a Kinect v2 high-definition face model. Using accurate 3D facial and ocular feature positions, gaze positions can be calculated more accurately than with previous methods. Considering the image resolution of the face and eye regions, two gaze vectors are combined as a weighted sum, allocating more weight to facial-gaze vectors. Hence, the facial orientation mainly determines the gaze position, and eye-gaze vectors then perform minor manipulations. The 3D facial-gaze vector is first defined, and the 3D rotational center of the eyeball is then estimated; together, these define the 3D eye-gaze vector. Finally, the intersection point between the 3D gaze vector and the physical display plane is calculated as the gaze position. Experimental results show that the average gaze estimation root-mean-square error was approximately 23 pixels from the desired position at a resolution of 1920 Ã— 1080. Â© 2016, Springer Science+Business Media New York.",,,
10.1145/3078072.3079740,2017,"Papavlasopoulou S., Giannakos M.N., Sharma K., Jaccheri L.",Using eye-tracking to unveil differences between kids and teens in coding activities,"Computational thinking and coding is gradually becoming an important part of K-12 education. Most parents, policy makers, teachers, and industrial stakeholders want their children to attain computational thinking and coding competences, since learning how to code is emerging as an important skill for the 21st century. Currently, educators are leveraging a variety of technological tools and programming environments, which can provide challenging and dynamic coding experiences. Despite the growing research on the design of coding experiences for children, it is still difficult to say how children of different ages learn to code, and to cite differences in their task-based behaviour. This study uses eye-tracking data from 44 children (here divided into ""kids"" [age 8-12] and ""teens"" [age 13-17]) to understand the learning process of coding in a deeper way, and the role of gaze in the learning gain and the different age groups. The results show that kids are more interested in the appearance of the characters, while teens exhibit more hypothesis-testing behaviour in relation to the code. In terms of collaboration, teens spent more time overall performing the task than did kids (higher similarity gaze). Our results suggest that eye-tracking data can successfully reveal how children of different ages learn to code. Â© 2017 Copyright is held by the owner/author(s).",,,
10.1145/3078971.3078995,2017,"Soleymani M., Riegler M., Halvorsen P.",Multimodal analysis of image search intent: Intent recognition in image search from user behavior and visual content,"Users search for multimedia content with different underlying motivations or intentions. Study of user search intentions is an emerging topic in information retrieval since understanding why a user is searching for a content is crucial for satisfying the user's need. In this paper, we aimed at automatically recognizing a user's intent for image search in the early stage of a search session. We designed seven different search scenarios under the intent conditions of finding items, re-finding items and entertainment. We collected facial expressions, physiological responses, eye gaze and implicit user interactions from 51 participants who performed seven different search tasks on a custom-built image retrieval platform. We analyzed the users' spontaneous and explicit reactions under different intent conditions. Finally, we trained machine learning models to predict users' search intentions from the visual content of the visited images, the user interactions and the spontaneous responses. After fusing the visual and user interaction features, our system achieved the F-1 score of 0.722 for classifying three classes in a user-independent cross-validation. We found that eye gaze and implicit user interactions, including mouse movements and keystrokes are the most informative features. Given that the most promising results are obtained by modalities that can be captured unobtrusively and online, the results demonstrate the feasibility of deploying such methods for improving multimedia retrieval platforms. Â© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1364/JOSAA.34.000838,2017,"Jiang Z., Das M., Gifford H.C.",Analyzing visual-search observers using eye-tracking data for digital breast tomosynthesis images,"Visual-search (VS) model observers have the potential to provide reliable predictions of human-observer performance in detection-localization tasks. The purpose of this work was to examine some characteristics of human gaze on breast images with the goal of informing the design of our VS observers. Using a helmet-mounted eye-tracking system, we recorded the movement of gaze from human observers as they searched for masses in sets of 2D digital breast tomosynthesis (DBT) images. The masses in this study were of a single profile. The DBT images were extracted from image volumes reconstructed with the filtered backprojection method. Fixation times associated with observer points of interest were computed from the observer data. We used the k-mean clustering algorithm to get dwell times of gaze data. The dwell times were then compared to sets of morphological feature values extracted from the images. These features, extracted as cross correlations involving the mass profile and the test image, included the matched filter (MF), gradient MF, Laplacian MF, and adaptive MF. The adaptive MF combining four feature maps was computed using a hotelling discriminant generated from training data. For this investigation, we computed correlation coefficients between the fixation times and the feature values. We also conducted a significance test by computing p-values of correlation coefficients for five features. Of all these features, the adaptive MF provided the highest correlation coefficients for DBT images with different densities. Â© 2017 Optical Society of America.",,,
10.1177/2041669517705447,2017,"Coates D.R., Wagemans J., Sayim B.",Diagnosing the periphery: Using the Rey-Osterrieth Complex Figure drawing test to characterize peripheral visual function,"Peripheral vision is strongly limited by crowding, the deleterious influence of neighboring stimuli on target perception. Many quantitative aspects of this phenomenon have been characterized, but the specific nature of the perceptual degradation remains elusive. We utilized a drawing technique to probe the phenomenology of peripheral vision, using the Rey-Osterrieth Complex Figure, a standard neuropsychological clinical instrument. The figure was presented at 12Â° or 6Â° in the right visual field, with eye tracking to ensure that the figure was only presented when observers maintained stable fixation. Participants were asked to draw the figure with free viewing, capturing its peripheral appearance. A foveal condition was used to measure copying performance in direct view. To assess the drawings, two raters used standard scoring systems that evaluated feature positions, spatial distortions, and omission errors. Feature scores tended to decrease with increasing eccentricity, both within and between conditions, reflecting reduced resolution and increased crowding in peripheral vision. Based on evaluation of the drawings, we also identified new error classes unique to peripheral presentation, including number errors for adjacent similar features and distinctive spatial distortions. The multifaceted nature of the Rey- Osterrieth Complex Figure-containing configural elements, detached compound features, and texture-like components-coupled with the flexibility of the free-response drawing paradigm and the availability of standardized scoring systems, provides a promising method to probe peripheral perception and crowding. Â© The Author(s) 2017.",,,
10.1016/j.entcom.2017.04.003,2017,"Wibirama S., Nugroho H.A., Hamamoto K.",Evaluating 3D gaze tracking in virtual space: A computer graphics approach,"Increasing usage of stereoscopic 3D technology in virtual reality, video games, entertainment, and visualization has risen concern on development of gaze-based interaction. To develop intuitive and accurate gaze-based interaction, estimation of 3D gaze in virtual space should be validated experimentally. 3D gaze tracking is generally performed in real space with rigid object as validation target. Thus, researchers in virtual reality are constrained on choosing appropriate evaluation method when 3D gaze tracking has to be performed in virtual space. To fill this research gap, we present design and development of a new evaluation method for 3D gaze tracking in virtual space. We have implemented computer graphics technology to develop virtual plane containing virtual 3D object as validation target. Experimental results show that the proposed evaluation method was able to support real experiment by proving the accuracy of our 3D gaze tracking system with average Euclidean error less than 1Â cm (MeanÂ =Â 0.95Â cm; S.DÂ =Â 0.55Â cm) in 74Â cm depth of workspace. Compared with evaluation method for 3D gaze tracking in real space, the proposed method even can be implemented when space of experiment room is limited by adjusting the distance of virtual plane programmatically. Â© 2017 Elsevier B.V.",,,
10.1109/IRC.2017.44,2017,"El Hafi L., Ding M., Takamatsu J., Ogasawara T.",Gaze tracking and object recognition from eye images,"This paper introduces a method to identify the focused object in eye images captured from a single camera in order to enable intuitive eye-based interactions using wearable devices. Indeed, eye images allow to not only obtain natural user responses from eye movements, but also the scene reflected on the cornea without the need for additional sensors such as a frontal camera, thus making it more socially acceptable. The proposed method relies on a 3D eye model reconstruction to evaluate the gaze direction from the eye images. The gaze direction is then used in combination with deep learning algorithms to classify the focused object reflected on the cornea. Finally, the experimental results using a wearable prototype demonstrate the potential of the proposed method solely based on eye images captured from a single camera. Â© 2017 IEEE.",,,
10.1109/WACV.2017.78,2017,"Schoning J., Faion P., Heidemann G., Krumnack U.",Providing video annotations in multimedia containers for visualization and research,"There is an ever increasing amount of video data sets which comprise additional metadata, such as object labels, tagged events, or gaze data. Unfortunately, metadata are usually stored in separate files in custom-made data formats, which reduces accessibility even for experts and makes the data inaccessible for non-experts. Consequently, we still lack interfaces for many common use cases, such as visualization, streaming, data analysis, machine learning, high-level understanding and semantic web integration. To bridge this gap, we want to promote the use of existing multimedia container formats to establish a standardized method of incorporating content and metadata. This will facilitate visualization in standard multimedia players, streaming via the Internet, and easy use without conversion, as shown in the attached demonstration video and files. In two prototype implementations, we embed object labels, gaze data from eye-Tracking and the corresponding video into a single multimedia container and visualize this data using a media player. Based on this prototype, we discuss the benefit of our approach as a possible standard. Finally, we argue for the inclusion of MPEG-7 in multimedia containers as a further improvement. Â© 2017 IEEE.",,,
10.1016/j.neucom.2016.09.031,2017,"Zhao Z., Feng P., Wang T., Liu F., Yuan C., Guo J., Zhao Z., Cui Z.",Dual-scale structural local sparse appearance model for robust object tracking,"Recently, sparse representation has been applied in object tracking successfully. However, the existing sparse representation captures either the holistic features of the target or the local features of the target. In this paper, we propose a dual-scale structural local sparse appearance (DSLSA) model based on overlapped patches, which can capture the quasi-holistic features and the local features of the target simultaneously. This paper first proposes two-scales structural local sparse appearance models based on overlapped patches. The larger-scale model is used to capture the structural quasi-holistic feature of the target, and the smaller-scale model is used to capture the structural local features of the target. Then, we propose a new mechanism to associate these two scale models as a new dual-scale appearance model. Both qualitative and quantitative analyses on challenging benchmark image sequences indicate that the tracker with our DSLSA model performs favorably against several state-of-the-art trackers. Â© 2016 Elsevier B.V.",,,
10.1145/3025453.3025601,2017,"Karolus J., Wozniak P.W., Chuang L.L., Schmidt A.",Robust gaze features for enabling language proficiency awareness,"We are often confronted with information interfaces designed in an unfamiliar language, especially in an increasingly globalized world, where the language barrier inhibits interaction with the system. In our work, we explore the design space for building interfaces that can detect the user's language proficiency. Specifically, we look at how a user's gaze properties can be used to detect whether the interface is presented in a language they understand. We report a study (N=21) where participants were presented with questions in multiple languages, whilst being recorded for gaze behavior. We identified fixation and blink durations to be effective indicators of the participants' language proficiencies. Based on these findings, we propose a classification scheme and technical guidelines for enabling language proficiency awareness on information displays using gaze data. Â© 2017 ACM.",,,
10.1109/TIP.2017.2676346,2017,"Duffner S., Garcia C.",Fast Pixelwise Adaptive Visual Tracking of Non-Rigid Objects,"In this paper, we present a new algorithm for real-Time single-object tracking in videos in unconstrained environments. The algorithm comprises two different components that are trained 'in one shot' at the first video frame: A detector that makes use of the generalized Hough transform with color and gradient descriptors and a probabilistic segmentation method based on global models for foreground and background color distributions. Both components work at pixel level and are used for tracking in a combined way adapting each other in a co-Training manner. Moreover, we propose an adaptive shape model as well as a new probabilistic method for updating the scale of the tracker. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging benchmarks, and outperforms the state-of-The-Art tracking methods designed for the same task. Finally, a very efficient implementation of the proposed models allows for extremely fast tracking. Â© 1992-2012 IEEE.",,,
10.1109/BHI.2017.7897264,2017,"Vu T., Tran H., Cho K.W., Song C., Lin F., Chen C.W., Hartley-Mcandrew M., Doody K.R., Xu W.",Effective and efficient visual stimuli design for quantitative autism screening: An exploratory study,"Autism spectrum disorder (ASD) is one of the most common childhood developmental disorders. Early detection and intervention for ASD are critical for increasing child success. In the past decade, utilizing the abnormal eye gaze characteristics of children with autism in regard to certain visual stimuli is emerging as a screening approach due to its cost-efficiency and promising accuracy. However, the effect of visual stimulus on children with ASD has not been considered as a diagnostic consideration in the past. In this paper, we first create a visual stimuli database based on an extensive literature review, then we examine the impact of picture stimuli and exposure time on the quantitative accuracy of screenings for ASD. This is done by extracting gaze distribution in a 2D space and comparing children with ASD to typical peers using the 1st Wasserstein distance. A group of 32 participants with ASD and typical development (TD) were recruited for the study. The f-score accuracy results demonstrate the impact of implementing visual stimulus on screening for ASD. Our study demonstrates that the parsing of 'social scene' stimulus with 5-second exposure time has the best performance at 98.24%. Â© 2017 IEEE.",,,
10.1109/TAFFC.2016.2582490,2017,"Zhang L., Wade J., Bian D., Fan J., Swanson A., Weitlauf A., Warren Z., Sarkar N.",Cognitive Load Measurement in a Virtual Reality-Based Driving System for Autism Intervention,"Autism Spectrum Disorder (ASD) is a highly prevalent neurodevelopmental disorder with enormous individual and social cost. In this paper, a novel virtual reality (VR)-based driving system was introduced to teach driving skills to adolescents with ASD. This driving system is capable of gathering eye gaze, electroencephalography, and peripheral physiology data in addition to driving performance data. The objective of this paper is to fuse multimodal information to measure cognitive load during driving such that driving tasks can be individualized for optimal skill learning. Individualization of ASD intervention is an important criterion due to the spectrum nature of the disorder. Twenty adolescents with ASD participated in our study and the data collected were used for systematic feature extraction and classification of cognitive loads based on five well-known machine learning methods. Subsequently, three information fusion schemes - feature level fusion, decision level fusion and hybrid level fusion - were explored. Results indicate that multimodal information fusion can be used to measure cognitive load with high accuracy. Such a mechanism is essential since it will allow individualization of driving skill training based on cognitive load, which will facilitate acceptance of this driving system for clinical use and eventual commercialization. Â© 2010-2012 IEEE.",,,
10.1109/TVCG.2017.2657018,2017,"Orlosky J., Itoh Y., Ranchet M., Kiyokawa K., Morgan J., Devos H.",Emulation of Physician Tasks in Eye-Tracked Virtual Reality for Remote Diagnosis of Neurodegenerative Disease,"For neurodegenerative conditions like Parkinson's disease, early and accurate diagnosis is still a difficult task. Evaluations can be time consuming, patients must often travel to metropolitan areas or different cities to see experts, and misdiagnosis can result in improper treatment. To date, only a handful of assistive or remote methods exist to help physicians evaluate patients with suspected neurological disease in a convenient and consistent way. In this paper, we present a low-cost VR interface designed to support evaluation and diagnosis of neurodegenerative disease and test its use in a clinical setting. Using a commercially available VR display with an infrared camera integrated into the lens, we have constructed a 3D virtual environment designed to emulate common tasks used to evaluate patients, such as fixating on a point, conducting smooth pursuit of an object, or executing saccades. These virtual tasks are designed to elicit eye movements commonly associated with neurodegenerative disease, such as abnormal saccades, square wave jerks, and ocular tremor. Next, we conducted experiments with 9 patients with a diagnosis of Parkinson's disease and 7 healthy controls to test the system's potential to emulate tasks for clinical diagnosis. We then applied eye tracking algorithms and image enhancement to the eye recordings taken during the experiment and conducted a short follow-up study with two physicians for evaluation. Results showed that our VR interface was able to elicit five common types of movements usable for evaluation, physicians were able to confirm three out of four abnormalities, and visualizations were rated as potentially useful for diagnosis. Â© 1995-2012 IEEE.",,,
10.1109/TIP.2017.2657880,2017,"Lu F., Chen X., Sato Y.",Appearance-based gaze estimation via uncalibrated gaze pattern recovery,"Aiming at reducing the restrictions due to person/scene dependence, we deliver a novel method that solves appearance-based gaze estimation in a novel fashion. First, we introduce and solve an 'uncalibrated gaze pattern' solely from eye images independent of the person and scene. The gaze pattern recovers gaze movements up to only scaling and translation ambiguities, via nonlinear dimension reduction and pixel motion analysis, while no training/calibration is needed. This is new in the literature and enables novel applications. Second, our method allows simple calibrations to align the gaze pattern to any gaze target. This is much simpler than conventional calibrations which rely on sufficient training data to compute person and scene-specific nonlinear gaze mappings. Through various evaluations, we show that: 1) the proposed uncalibrated gaze pattern has novel and broad capabilities; 2) the proposed calibration is simple and efficient, and can be even omitted in some scenarios; and 3) quantitative evaluations produce promising results under various conditions. Â© 1992-2012 IEEE.",,,
10.1016/j.neunet.2017.02.002,2017,"Liu R., Wang D., Han Y., Fan X., Luo Z.",Adaptive low-rank subspace learning with online optimization for robust visual tracking,"In recent years, sparse and low-rank models have been widely used to formulate appearance subspace for visual tracking. However, most existing methods only consider the sparsity or low-rankness of the coefficients, which is not sufficient enough for appearance subspace learning on complex video sequences. Moreover, as both the low-rank and the column sparse measures are tightly related to all the samples in the sequences, it is challenging to incrementally solve optimization problems with both nuclear norm and column sparse norm on sequentially obtained video data. To address above limitations, this paper develops a novel low-rank subspace learning with adaptive penalization (LSAP) framework for subspace based robust visual tracking. Different from previous work, which often simply decomposes observations as low-rank features and sparse errors, LSAP simultaneously learns the subspace basis, low-rank coefficients and column sparse errors to formulate appearance subspace. Within LSAP framework, we introduce a Hadamard production based regularization to incorporate rich generative/discriminative structure constraints to adaptively penalize the coefficients for subspace learning. It is shown that such adaptive penalization can significantly improve the robustness of LSAP on severely corrupted dataset. To utilize LSAP for online visual tracking, we also develop an efficient incremental optimization scheme for nuclear norm and column sparse norm minimizations. Experiments on 50 challenging video sequences demonstrate that our tracker outperforms other state-of-the-art methods. Â© 2017 Elsevier Ltd",,,
10.1109/BIGCOMP.2017.7881713,2017,"Yafei W., Zhao T., Xueyan D., Bian J., Fu X.",Head pose-free eye gaze prediction for driver attention study,"Driver's gaze direction is an indicator of driver state and plays a significantly role in driving safety. Traditional gaze zone estimation methods based on eye model have disadvantages due to the vulnerability under large head movement. Different from these methods, an appearance-based head pose-free eye gaze prediction method is proposed in this paper, for driver gaze zone estimation under free head movement. To achieve this goal, a gaze zone classifier is trained with head vectors and eye image features by random forest. The head vector is calculated by Pose from Orthography and Scaling with ITerations (POSIT) where a 3D face model is combined with facial landmark detection. And the eye image features are derived from eye images which extracted through eye region localization. These features are presented as the combination of sparse coefficients by sparse encoding with eye image dictionary, having good potential to carry information of the eye images. Experimental results show that the proposed method is applicable in real driving environment. Â© 2017 IEEE.",,,
10.1145/3030024.3040985,2017,"Eivazi S., Slupina M., Fuhl W., Afkari H., Hafez A., Kasneci E.",Towards automatic skill evaluation in microsurgery,"In the past decade, eye tracking has emerged as a promising answer to the increasing needs of understanding surgical expertise. The implicit desire is to design an intelligent user interface (IUI) to monitor and assess the competency of surgical trainees. In this paper, for the first time in microsurgery, we explore the potential for a surgical automatic skill assessment through a combination of machine learning techniques, computational modeling, and eye tracking. We present primary findings from a random forest classification method where we achieved about 70% recognition rate for the detection of expert and novice group. This leads us to a conclusion that prediction of the micro-surgeon performance is possible, can be automated, and that the eye movement data carry important information about the skills of micro-surgeons. Copyright held by the owner/author(s).",,,
10.1145/3030024.3038268,2017,"Vergani Dambros G., Ungewiss J., KÃ¼bler T.C., Kasneci E., SpÃ¼ler M.",Monitoring response quality during campimetry via eye-tracking,"In a variety of use-cases, deriving information on user's fatigue is an important step for content adaptation. In this work, we investigate which eye-tracking related measures can predict the error rate (as a proxy of subject's fatigue) during a visual experiment. Data was collected during a 40 minutes campimetric task, where the user has to detect visual stimuli (i.e., dots) of different contrast. We found that eye-tracking measures can be used to train a machine learning model to predict the error rate of a user with an average correlation of 0.72Â±0.17. The results show that this method can be used to measure the user's response quality. Copyright is held by the owner/author(s).",,,
10.1145/3029798.3036665,2017,"Barz M., Poller P., Sonntag D.",Evaluating remote and head-worn eye trackers in multi-modal speech-based HRI (Demo),"Gaze is known to be a dominant modality for conveying spatial information, and it has been used for grounding in human-robot dialogues. In this work, we present the prototype of a gaze-supported multi-modal dialogue system that enhances two core tasks in human-robot collaboration: 1) our robot is able to learn new objects and their location from user instructions involving gaze, and 2) it can instruct the user to move objects and passively track this movement by interpreting the user's gaze. We performed a user study to investigate the impact of different eye trackers on user performance. In particular, we compare a head-worn device and an RGB-based remote eye tracker. Our results show that the head-mounted eye tracker outperforms the remote device in terms of task completion time and the required number of utterances due to its higher precision. Â© 2017 Authors.",,,
10.1145/3029798.3038322,2017,"Paletta L., Dini A., Murko C., Yahyanejad S., Schwarz M., Lodron G., LadstÃ¤tter S., Paar G., Velik R.",Towards real-time probabilistic evaluation of situation awareness from human gaze in human-robot interaction,Human attention processes play a major role for optimization in human-robot interaction (HRI). This work describes a novel methodology to measure situation awareness in real-time from gaze interaction with scene objects of interest using eye tracking glasses and 3D gaze analysis. A probabilistic framework of uncertainty considers coping with measurement errors in eye and position tracking. Comprehensive experiments on HRI were conducted with tasks including handover in a lab based prototypical manufacturing environment. The methodology is proven to predict a standard measure of situation awareness (SAGAT) in real-time and will open new opportunities for human factors based performance optimization in HRI applications. Â© 2017 Authors.,,,
10.1145/3029798.3038367,2017,"Barz M., Poller P., Sonntag D.",Evaluating remote and head-worn eye trackers in multi-modal speech-based hri,"Gaze is known to be a dominant modality for conveying spatial information, and it has been used for grounding in human-robot dialogues. In this work, we present the prototype of a gaze-supported multi-modal dialogue system that enhances two core tasks in human-robot collaboration: 1) our robot is able to learn new objects and their location from user instructions involving gaze, and 2) it can instruct the user to move objects and passively track this movement by interpreting the user's gaze. We performed a user study to investigate the impact of different eye trackers on user performance. In particular, we compare a head-worn device and an RGB-based remote eye tracker. Our results show that the head-mounted eye tracker outperforms the remote device in terms of task completion time and the required number of utterances due to its higher precision. Â© 2017 Authors.",,,
10.1016/j.neucom.2016.11.055,2017,"Zhou T., Bhaskar H., Liu F., Yang J., Cai P.",Online learning and joint optimization of combined spatial-temporal models for robust visual tracking,"Visual tracking is highly challenged by factors such as occlusion, background clutter, an abrupt target motion, illumination variation, and changes in scale and orientation. In this paper, an integrated framework for online learning of a fused temporal appearance and spatial constraint models for robust and accurate visual target tracking is proposed. The temporal appearance model aims to encapsulate historical appearance information of the target in order to cope with variations due to illumination changes and motion dynamics. On the other hand, the spatial constraint model exploits the relationships between the target and its neighbors to handle occlusion and deal with a cluttered background. For the purposes of reducing the computational complexity of the state estimation algorithm and in order to emphasize the importance of the different basis vectors, a K-nearest Local Smooth Algorithm (KLSA) is used to describe the spatial state model. Further, a customized Accelerated Proximal Gradient (APG) method is implemented for iteratively obtaining an optimal solution using KLSA. Finally, the optimal state estimate is obtained by using weighted samples within a particle filtering framework. Experimental results on large-scale benchmark sequences show that the proposed tracker achieves favorable performance compared to state-of-the-art methods. Â© 2016 Elsevier B.V.",,,
10.1016/j.neucom.2016.11.009,2017,"Feng P., Xu C., Zhao Z., Liu F., Yuan C., Wang T., Duan K.",Sparse representation combined with context information for visual tracking,"In visual tracking, the main problem is to find candidates that are most likely to be the target in successive frames, so it is important to design a proper mechanism to evaluate this. In this paper, we propose a novel sparse representation based visual tracking algorithm, which well integrates the temporal and spatial context information of tracking objects into a unified framework. Specifically, we compute the similarity between the target and its candidates, which is acquired by fusing three aspects of the target's appearance variation with different weights. For the first part, we apply a patch based sparse representation to measure the similarities between the target in the first frame and candidates in current frame. Since the tracking result in the last frame provides the latest variation information of the target, we employ an image quality assessment method to obtain the similarity scores in the second part, and the spatial context information is also exploited. As the target appearance may suffer from radical changes along the video sequence, tracking that only uses the two parts mentioned above will suffer from serious drifting problems and easily cause incorrect tracking results. In order to ease this problem, we exploit the temporal context information by generating a group of history target templates adaptively according to previous tracking results, computing similarities between each candidate with them and the maximum will be used in the third part. Finally, we combine these parts to calculate the similarity scores and take those candidates with the highest score as the new targets in current frames. The extensive experiments on twelve challenging video sequences show that our algorithm can achieve performance competitive with state-of-the-art trackers. Â© 2016 Elsevier B.V.",,,
10.1109/ETVIS.2016.7851160,2017,"Netzel R., Weiskopf D.",Hilbert attention maps for visualizing spatiotemporal gaze data,"Attention maps-often in the form of heatmaps-Are a common visualization approach to obtaining an overview of the spatial distribution of gaze data from eye tracking experiments. However, attention maps are not designed to let us easily analyze the temporal information of gaze data: They completely ignore temporal information by aggregating over time, or they use animation to build a sequence of attention maps. To overcome this issue, we introduce Hilbert attention maps: A 2D static visualization of the spatiotemporal distribution of gaze points. The visualization is based on the projection of the 2D spatial domain onto a space-filling Hilbert curve that is used as one axis of our new attention map; the other axis represents time. We visualize Hilbert attention maps either as dot displays or heatmaps. This 2D visualization works for data from individual participants or large groups of participants, it supports static and dynamic stimuli alike, and it does not require any preprocessing or definition of areas of interest. We demonstrate how our visualization allows analysts to identify spatiotemporal patterns of visual reading behavior, including attentional synchrony and smooth pursuit. Â© 2016 IEEE.",,,
10.1109/SMC.2016.7844411,2017,"Naqshbandi K., Gedeon T., Abdulla U.A.",Automatic clustering of eye gaze data for machine learning,"Eye gaze patterns or scanpaths of subjects looking at art while answering questions related to the art have been used to decode those tasks with the use of certain classifiers and machine learning techniques. Some of these techniques require the artwork to be divided into several Areas or Regions of Interest. In this paper, two ways of clustering the static visual stimuli - k-means and the density based clustering algorithm called OPTICS - were used for this purpose. These algorithms were used to cluster the gaze points before classification. The classification success rates were then compared. While it was observed that both k-means and OPTICS gave better success rates than manual clustering, which is itself higher than chance level, OPTICS consistently gave higher success rates than k-means given the right parameter settings. OPTICS also formed clusters that look more intuitive and consistent with the heat map readings than k-means, which formed clusters that look unintuitive and less consistent with the heat map. Â© 2016 IEEE.",,,
10.1016/j.neucom.2016.09.069,2017,"Bao H., Lin M., Chen Z.",Robust visual tracking based on hierarchical appearance model,"In order to track the target object effectively in the presence of significant appearance variation, e.g., occlusion, scale variation, deformation, fast motion and background clutter, we develop a new approach based on hierarchical appearance model under the Bayesian framework. The proposed approach represents the target at two levels, i.e., the local and the global levels. At the local level, a set of local patches are used to represent the target so as to adapt the changes in appearance. Likelihood defined as the weighted sum of reliability index and stability index is applied to evaluate how likely a patch pertaining to the target. At the global level, the target is represented by using double bounding boxes regarding the foreground and background, respectively. The inner bounding box only contains the target region, and the outer bounding box contains both the target region and the background region surrounding the target. The target model is encoded by using two HSV color histograms with respect to the target and the background, respectively. As this, the drifts can be effectively suppressed in the tracking process. Furthermore, the object position can be estimated by maximizing the likelihood of the target under the Bayesian framework. An experimental study is employed to illustrate the advantages of our proposed approach. The experimental results demonstrate that our method is very effective and performs favorably in comparison to the state-of-the-art trackers in terms of efficiency, accuracy and robustness. Â© 2016 Elsevier B.V.",,,
10.1109/IPTA.2016.7821028,2017,"Soliman M., Tavakoli H.R., Laaksonen J.",Towards gaze-based video annotation,"This paper presents our efforts towards a framework for video annotation using gaze. In computer vision, video annotation (VA) is an essential step in providing a ground truth for the evaluation of object detection and tracking techniques. VA is a demanding element in the development of video processing algorithms, where each object of interest should be manually labelled. Although the community has handled VA for a long time, the size of new data sets and the complexity of the new tasks pushes us to revisit it. A barrier towards automated video annotation is the recognition of the object of interest and tracking it over image sequences. To tackle this problem, we employ the concept of visual attention for enhancing video annotation. In an image, human attention naturally grasps interesting areas that provide valuable information for extracting the objects of interest, which can be exploited to annotate videos. Under task-based gaze recording, we utilize an observer's gaze to filter seed object detector responses in a video sequence. The filtered boxes are then passed to an appearance-based tracking algorithm. We evaluate the gaze usefulness by comparing the algorithm with gaze and without it. We show that eye gaze is an influential cue for enhancing the automated video annotation, improving the annotation significantly. Â© 2016 IEEE.",,,
10.1109/IPTA.2016.7821006,2017,"Xia Z., Zhang W., Tan F., Feng X., Hadid A.",An accurate eye localization approach for smart embedded system,"Eye localization is a vital procedure in many applications, such as face recognition and gaze tracking, and can further facilitate related procedures. Although many works have been devoted to localizing eyes in frontal facial images, most approaches cannot work effectively and efficiently in smart embedded systems (e.g., the vehicle system). In this paper, we propose an accurate eye localization approach for smart embedded systems. An illumination normalization procedure with the perception based model is utilized to remove the illumination effects of facial images. Then the integral projection method is employed to localize the candidate positions of eyes. The support vector machine (SVM) classifiers are trained with the spacial and intensity information to verify these candidates rapidly with compact 3-dimensional features. Based on the output of SVMs, the two candidates with top scores are determined as the final accurate eye positions. Extensive experiments on the extended Yale B, AR and ORL face datasets demonstrate that the proposed approach achieves good accuracy and fast computation results for localizing eyes. Â© 2016 IEEE.",,,
10.1109/CSA.2015.17,2017,"Huang G.-J., Du X., Zhu Y.-F.",Learning stereoscopic visual attention model for 3D video,"Various saliency detection model have been proposed for the visual attention prediction in 2D images/videos. The rapid development of stereoscopic display techniques along with the emerging 3D applications, bring the depth information. Depth is an appealing vision feature since it provides instantaneous depth perception for the scene and can affect human's visual attention. In this paper, we propose to build a model of visual attention based on learning method for stereoscopic videos. The proposed method takes the eye tracking data to build up the ground truth saliency map. The 2D features and the depth information are integrated by the learning method. Experimental results demonstrate that our method is effective and reasonable. Â© 2015 IEEE.",,,
10.1080/13875868.2016.1228654,2017,"Schrom-Feiertag H., Settgast V., Seer S.",Evaluation of indoor guidance systems using eye tracking in an immersive virtual environment,"In this article, we present a novel method for evaluating guidance systems using an immersive virtual environment in combination with a mobile eye tracking system. Accurate measurements of position, locomotion, viewing frustum, and gaze are captured in the virtual environment. They are applied to the projection of an attention map onto the virtual 3D environment for visualizing the fixation in the environment as well as the amount of time objects were fixated. To demonstrate the method's applicability, we conducted an experiment with 24 participants evaluating a guidance system of a large public infrastructure. The results show that our method allows for the creation of attention maps as well as for the identification of objects of interest based on eye tracking. Â© 2017 Taylor & Francis.",,,
10.18653/v1/d17-1107,2017,"Fraser K.C., Fors K.L., Kokkinakis D., Nordlund A.",An analysis of eye-movements during reading for the detection of mild cognitive impairment,"We present a machine learning analysis of eye-tracking data for the detection of mild cognitive impairment, a decline in cognitive abilities that is associated with an increased risk of developing dementia. We compare two experimental configurations (reading aloud versus reading silently), as well as two methods of combining information from the two trials (concatenation and merging). Additionally, we annotate the words being read with information about their frequency and syntactic category, and use these annotations to generate new features. Ultimately, we are able to distinguish between participants with and without cognitive impairment with up to 86% accuracy. Â© 2017 Association for Computational Linguistics.",,,
10.5220/0006100100670073,2017,"Suda S., Yamagishi K., Takemura K.",User calibration-free method using corneal surface image for eye tracking,"Various calibration methods to determine the point-of-regard have been proposed for eye tracking. Although user calibration can be performed for experiments carried out in the laboratory, it is unsuitable when applying an eye-Tracker in user interfaces and in public displays. Therefore, we propose a novel calibration-free approach for users that is based on the use of the corneal surface image. As the environmental information is reflected on the corneal surface, we extracted the unwarped image around the point-of-regard from the cornea. The point-of-regard is estimated on the screen by using the unwarped image, and the regression formula is solved using these points without user calibration. We implemented the framework of the algorithm, and we confirmed the feasibility of the proposed method through experiments. Â© 2017 by SCITEPRESS - Science and Technology Publications, Lda.",,,
,2017,"Fuhl W., Santini T., Geisler D., KÃ¼bler T., Kasneci E.","EyeLad: Remote eye tracking image labeling tool: Supportive eye, eyelid and pupil labeling tool for remote eye tracking videos","Ground truth data is an important prerequisite for the development and evaluation of many algorithms in the area of computer vision, especially when these are based on convolutional neural networks or other machine learning approaches that unfold their power mostly by supervised learning. This learning relies on ground truth data, which is laborious, tedious, and error prone for humans to generate. In this paper, we contribute a labeling tool (EyeLad) specifically designed for remote eye-tracking data to enable researchers to leverage machine learning based approaches in this field, which is of great interest for the automotive, medical, and human-computer interaction applications. The tool is multi platform and supports a variety of state-of-theart detection and tracking algorithms, including eye detection, pupil detection, and eyelid coarse positioning. Furthermore, the tool provides six types of point-wise tracking to automatically track the labeled points. The software is openly and freely available at: www.ti.uni-tuebingen.de/perception. Copyright Â© 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",,,
10.26615/978-954-452-049-6-097,2017,"Tokunaga T., Nishikawa H., Iwakura T.",An eye-tracking study of named entity annotation,"Utilising effective features in machine learning-based natural language processing (NLP) is crucial in achieving good performance for a given NLP task. The paper describes a pilot study on the analysis of eye-ttacking data during named entity (NE) annotation, aiming at obtaining insights into effective features for the NE recognition task. The eye gaze data were collected from 10 annotators and analysed regarding working time and fixation distribution. The results of the preliminary qualitative analysis showed that human annotators tend to look at broader contexts around the target NE than recent state-of-the-art automatic NE recognition systems and to use predicate argument relations to identify the NE categories. Â© 2018 Association for Computational Linguistics (ACL). All rights reserved.",,,
10.1007/978-3-319-70010-6_31,2017,"Tomi A., Rambli D.R.A.",A conceptual design of spatial calibration for optical see-through head mounted display using electroencephalographic signal processing on eye tracking,"One of vital issue in Optical See-Through Head Mounted Display (OST HMD) used in Augmented Reality (AR) systems is frequent (re)calibrations. OST HMD calibration that involved user interaction is time consuming. It will distract users from their application, which will reduce AR experience. Additionally, (re)calibration procedure will be prone to user errors. Nowadays, there are several approaches toward interaction-free calibration on OST HMD. In this proposed work, we propose a novel approach that uses EEG signal processing on eye movement into OST HMD calibration. By simultaneously recording eye movements through EEG during a guided eye movement paradigm, a few properties of eye movement artifacts can be useful for eye localization algorithm which can be used in interaction-free calibration for OST HMD. The proposed work is expected to enhance OST HMD calibration focusing on spatial calibration formulation in term reducing 2D projection error. Â© Springer International Publishing AG 2017.",,,
10.1007/978-3-319-67585-5_76,2017,"GonzÃ¡lez-Ortega D., GonzÃ¡lez-DÃ­az J., DÃ­az-Pernas F.J., MartÃ­nez-Zarzuela M., AntÃ³n-RodrÃ­guez M.",3D kinect-based gaze region estimation in a driving simulator,"In this paper, we present a 3D Kinect-based gaze region estimation module to add gaze pattern information in a driving simulator. Gaze region is estimated using only face orientation cues, similarly to other previous approaches in the literature. An initial user-based calibration stage is included in our approach. The module is able to detect the region, out of 7 in which the driving scene was divided, that a driver is gazing on route every processed frame. 8 people tested the module, which achieved an accuracy of 88.23%. The information provided by the gaze estimation module enriches the driving simulator data and makes it possible a multimodal driving performance analysis. Â© 2017, Springer International Publishing AG.",,,
10.1007/978-3-319-67077-5_58,2017,"Morozkin P., Swynghedauw M., Trocan M.",Neural Network Based Eye Tracking,"The EyeDee embedded eye tracking solution developed by SuriCog is the worldâ€™s first solution using the eye as a real-time mobile digital cursor, while maintaining full mobility. In order to reduce the time of eye image transmission, image compression techniques can be employed. Being hardware implemented, several standard image coding systems (JPEG and JPEG2000) were evaluated for their potential use in the next generation device of the EyeDee product line. In order to satisfy low-power, low-heat, low-MIPS requirements several non-typical approaches have been considered. One example consists in the complete replacement of currently used eye tracking algorithm based on image processing coupled with geometric eye modeling by a precisely tuned and perfectly trained neural network, which directly transforms wirelessly transmitted floating-point values of decimated eye image (result of the 3D perspective projection of a model of rotating pupil disk) into five floating-point parameters of pupilâ€™s ellipse (result of the eye tracking). Hence implementation of the eye tracking algorithm is reduced to a known challenge of neural network construction and training, preliminary results of which are presented in the paper. Â© 2017, Springer International Publishing AG.",,,
,2017,"Razin Y., Feigh K.",Learning to predict intent from gaze during robotic hand-eye coordination,"Effective human-aware robots should anticipate their user's intentions. During hand-eye coordination tasks, gaze often precedes hand motion and can serve as a powerful predictor for intent. However, cooperative tasks where a semi-autonomous robot serves as an extension of the human hand have rarely been studied in the context of hand-eye coordination. We hypothesize that accounting for anticipatory eye movements in addition to the movements of the robot will improve intent estimation. This research compares the application of various machine learning methods to intent prediction from gaze tracking data during robotic hand-eye coordination tasks. We found that with proper feature selection, accuracies exceeding 94% and AUC greater than 91% are achievable with several classification algorithms but that anticipatory gaze data did not improve intent prediction. Copyright Â© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,,
10.1007/978-3-319-67534-3_12,2017,"Lejeune L., Christoudias M., Sznitman R.",Expected Exponential Loss for Gaze-Based Video and Volume Ground Truth Annotation,"Many recent machine learning approaches used in medical imaging are highly reliant on large amounts of image and ground truth data. In the context of object segmentation, pixel-wise annotations are extremely expensive to collect, especially in video and 3D volumes. To reduce this annotation burden, we propose a novel framework to allow annotators to simply observe the object to segment and record where they have looked at with a $200 eye gaze tracker. Our method then estimates pixel-wise probabilities for the presence of the object throughout the sequence from which we train a classifier in semi-supervised setting using a novel Expected Exponential loss function. We show that our framework provides superior performances on a wide range of medical image settings compared to existing strategies and that our method can be combined with current crowd-sourcing paradigms as well. Â© 2017, Springer International Publishing AG.",,,
10.1007/978-3-319-65289-4_31,2017,"Wan Z., Xiong C.",Estimating 3D gaze point on object using stereo scene cameras,"3D eye gaze estimation in real environment is still challenging. A novel method of scene-based 3D gaze estimation is proposed in this paper. As this model combines two models, the 2D nonlinear polynomial mapping model of traditional regression-based gaze estimation and the 3D visual axis linear ray model of traditional geometry-based gaze estimation, it includes two steps. The first step is to estimate the visual axis from the pupil center in an eye camera image. The second one is to estimate the 3D gaze point which is the intersection between the visual axis and the scene object, which can be obtained by stereo scene cameras. As the 3D gaze points are on the object, rather than outside or inside the object like geometry-based 3D gaze estimation, this method is potential for human robot interaction in real environment. Through a simple test, the accuracy of our 3D gaze estimation system is acceptable. Â© Springer International Publishing AG 2017.",,,
10.1007/978-3-319-65289-4_33,2017,"Li Q., Xiong C., Liu K.",Eye gaze tracking based interaction method of an upper-limb exoskeletal rehabilitation robot,"Stroke is one of the leading causes of long-term disability today. Rehabilitation robot can benefit the patients to perform intensive and repetitive task-specific rehabilitation training to enhance motor recovery with less effort. Engagement in rehabilitation training, namely active rehabilitation is a key factor for patient to get effective recovery. In this paper, we propose an upper-limb exoskeletal underactuated rehabilitation robot with only 2 actuated degrees of freedom (DOFs) for task-specific rehabilitation training. To ensure the patientsâ€?engagement in performing training, we used an eye gaze tracking based interaction method to guide the end-point of the robot to move on its workspace, a 2D surface, as a result of performing task-specific rehabilitation training. Finally, some validation experiment is conducted on the interaction efficiency. Â© Springer International Publishing AG 2017.",,,
10.1016/j.procs.2017.05.032,2017,"Fernandes D.L., Siqueira-Batista R., Gomes A.P., Souza C.R., Da Costa I.T., Cardoso F.D.S.L., De Assis J.V., Caetano G.H.L., Cerqueira F.R.",Investigation of the visual attention role in clinical bioethics decision-making using machine learning algorithms,"This study proposes the use of a computational approach based on machine learning (ML) algorithms to build predictive models using eye tracking data. Our intention is to provide results that may support the study of medical investigation in the decision-making process in clinical bioethics, particularly in this work, in cases of euthanasia. The data used in the approach were collected from 75 students of the nursing undergraduate course using an eye tracker. The available data were processed through feature selection methods, and were later used to create models capable of predicting the euthanasia decision through ML algorithms. Statistical experiments showed that the predictive model resultant from the multilayer perceptron (MLP) algorithm led to the best performance compared with the other tested algorithms, presenting an accuracy of 90.7% and a mean area under the ROC curve of 0.90. Interesting knowledge (patterns and rules) for the studied bioethical decision-making was extracted using simulations with MLP models and inspecting the obtained decision-tree rules. The good performance shown by the obtained MLP predictive model demonstrates that the proposed investigation approach may be used to test scientific hypotheses related to visual attention and decision-making. Â© 2017 The Authors. Published by Elsevier B.V.",,,
10.1007/978-3-319-58475-1_7,2017,"Lin Y., Xue C., Guo Q., Zhang J., Peng N., Niu Y.",The study of presentation characteristics of the warning information and its influence on userâ€™s cognitive process based on eye tracking,"This research has adopted eye movement technique to study the influence of warning character on the information processing, which involves in two experiments. With our study, we have made our focus of research on the warning position, warning icon and warning border as a visual stimulus means. In our investigation, we have been keeping on with such commonly-made eye-movement recording parameters, such as the Fixation Count, the First Fixation and Duration by using an Eye Link II eye tracker, which is in a position to reflect the subjectâ€™s attention and conversion of the attentions. What is more, we have done a single factor variance analysis (ANOVA) in hoping to work out the experimental data due to the kinds of eye movement parameters, and the following conclusions were drawn: (1) The warning position on the warning interface affects the machining process of the warning. When the warning is embedded in text, the warning is more noticeable and the perceived hazard level is higher. (2) Consistent with the results of the relevant studies on warnings on product labels, there are also icon effects on the warning interface, and the icon can improve the salience of the warning itself and the level of perceived danger. (3) There are also border effects on the warning interface. The appearance of the border makes the warning more significant and the perceived hazard level can be improved. The research results of this paper can also be adopted as the warning message design reference, which has had a great significance in improving the identification of warning message and reducing the rate of visual accidents on interface. Â© Springer International Publishing AG 2017.",,,
10.1007/978-3-319-58640-3_7,2017,"Jiang B., Ma J., Zhou D.",The application of multi-view and multi-task learning for on-board interaction design based on visual selection,"The core of information visualization and visual selection is the mapping from abstract data to visual structure. The aim of information visualization doesnâ€™t lie in visualization itself, its ultimate aim is to collect information on the basis of visualization so as to offer support to decision making. Under the complex driving environment, Designers have to continue their research during the process of interface design. They can explore the implications and presentation methods of interface interaction inside the car in order to form an on-board interaction design system based on visual selection. This can also realize information sharing between cars and X (people, cars, roads and back-stage) and possess functions like strong sensation for complex environment, intelligent decision and mutual control. At the same time, on-board interaction equipment will have more diversified tasks. For example, the alternation of interaction and decision-making between multiple tasks like reality conformation, cluster display, gesture interaction, speech recognition, body sensation and eye tracking. At present, the new direction for interaction design is the analysis of multitask visual selection so as to realize secure, comfortable, energy-saving and efficient driving and finally the invention of a new generation of on-board interaction design system which can perform on human behalf. Through multi-view and multi-task learning, this paper gave an analysis of on-board interface design and concluded design scheme and suggestion with optimal user experience. By combing reasonable analysis of human intelligence and sensible interface design, this paper can provide new ways of thinking and methods for future on-board interface design. Â© Springer International Publishing AG 2017.",,,
10.1117/12.2263457,2017,"Dorado A., Hong S., Saavedra G., Martinez-Corral M., Javidi B.",Integral display for non-static observers,"We propose to combine the Kinect and the Integral-Imaging technologies for the implementation of Integral Display. The Kinect device permits the determination, in real time, of (x,y,z) position of the observer relative to the monitor. Due to the active condition of its IR technology, the Kinect provides the observer position even in dark environments. On the other hand, SPOC 2.0 algorithm permits to calculate microimages adapted to the observer 3D position. The smart combination of these two concepts permits the implementation, for the first time we believe, of an Integral Display that provides the observer with color 3D images of real scenes that are viewed with full parallax and which are adapted dynamically to its 3D position. Â© 2017 SPIE.",,,
10.1117/12.2264905,2017,"Yoon K.-H., Kim S.-K.",Expansion method of the three-dimensional viewing freedom of autostereoscopic 3D display with dynamic merged viewing zone (MVZ) under eye tracking,"We studied expansion method of the three-dimensional viewing freedom of autostereoscopic 3D display with dynamic MVZ under tracking of viewer's eye. The dynamic MVZ technique can provide three dimensional images with minimized crosstalk when observer move at optimal viewing distance (OVD). In order to be extended to movement in the depth direction of the observer of this technology, it is provided a new pixel mapping method of the left eye and the right eye images at the time of the depth direction movement of the observer. When this pixel mapping method is applied to common autostereoscopic 3D display, the image of the 3D display as viewed from the observer position has the nonuniformed brightness distribution of a constant period in the horizontal direction depending on depth direction distance from OVD. It makes it difficult to provide a three-dimensional image of good quality to the observer who deviates from OVD. In this study, it is simulated brightness distribution formed by the proposed pixel mapping when it is moved in the depth direction away OVD and confirmed the characteristics with the captured photos of two cameras on observer position to simulated two eyes of viewer using a developed 3D display system. As a result, we found that observer can perceive 3D images of same quality as OVD position even when he moves away from it in the developed 3D display system. Â© 2017 SPIE.",,,
10.1007/978-3-319-59126-1_6,2017,"Jensen R.R., Stets J.D., Suurmets S., Clement J., AanÃ¦s H.",Wearable gaze trackers: Mapping visual attention in 3D,"The study of visual attention in humans relates to a wide range of areas such as: psychology, cognition, usability, and marketing. These studies have been limited to fixed setups with respondents sitting in front of a monitor mounted with a gaze tracking device. The introduction of wearable mobile gaze trackers allows respondents to move freely in any real world 3D environment, removing the previous restrictions. In this paper we propose a novel approach for processing visual attention of respondents using mobile wearable gaze trackers in a 3D environment. The pipeline consists of 3 steps: modeling the 3D area-of-interest, positioning the gaze tracker in 3D space, and 3D mapping of visual attention. The approach is general, but as a case study we created 3D heat maps of respondents visiting supermarket shelves as well as finding their in-store movement relative to these shelves. The method allows for analysis across multiple respondents and to distinguish between phases of in-store orientation (far away) and product recognition/selection (up close) based on distance to shelves. Â© Springer International Publishing AG 2017.",,,
10.1007/978-3-319-56687-0_5,2017,"Jain S., Kamath S.S.",Saliency prediction for visual regions of interest with applications in advertising,"Human visual fixations play a vital role in a plethora of genres, ranging from advertising design to human-computer interaction. Considering saliency in images thus brings significant merits to Computer Vision tasks dealing with human perception. Several classification models have been developed to incorporate various feature levels and estimate free eye-gazes. However, for real-time applications (Here, real-time applications refer to those that are time, and often resource-constrained, requiring speedy results. It does not imply on-line data analysis), the deep convolution neural networks are either difficult to deploy, given current hardware limitations or the proposed classifiers cannot effectively combine image semantics with low-level attributes. In this paper, we propose a novel neural network approach to predict human fixations, specifically aimed at advertisements. Such analysis significantly impacts the brand value and assists in audience measurement. A dataset containing 400 print ads across 21 successful brands was used to successfully evaluate the effectiveness of advertisements and their associated fixations, based on the proposed saliency prediction model. Â© Springer International Publishing AG 2017.",,,
10.1007/978-3-319-54430-4_34,2017,"SzymaÅ„ski A., Szlufik S., Koziorowski D.M., Przybyszewski A.W.",Building classifiers for parkinsonâ€™s disease using new eye tribe tracking method,"Parkinson Disease (PD) is the second major neurodegenerative disease, which causes severe complications for patientsâ€?daily life. PD remains unspecified in many aspects including best treatment, prediction of its progression and precise diagnosis. In our study we have built machine learning (ML) models, which address some of those issues by helping to improve symptom evaluation precision by using advanced biomarkers such as fast eye movements. We have built and compared model accuracy relaying on data from two systems for recording eye movements: one is saccadometer (Ober Consulting), and another is based on the Eye Tribe (ET1000). We have reached 85% accuracy in prediction of neurologic attributes based on ET and 82% accuracy with saccadometer with help of rough set theory. The purpose of this study was to compare ET with clinically approved eye movement measurements saccadometer of Ober. We have demonstrated in 8 PD patients that both systems gave comparable results based on neurological and eye movement measurements attributes. Â© Springer International Publishing AG 2017.",,,
10.1007/978-3-319-58130-9_12,2017,"Saikh T., Das D., Bandyopadhayay S.",Identifying and pruning features for classifying translated and post-edited gaze durations,"The present paper reports on various experiments carried out to classify the source and target gaze fixation durations on an eye tracking dataset, namely Translation Process Research (TPR). Different features were extracted from both the source and target parts of the TPR dataset, separately and different models were developed separately by employing such features using a machine learning framework. These models were trained using Support Vector Machine (SVM) and the best accuracy of 49.01% and 59.78% were obtained with respect to cross validation for source and target gaze fixation durations, respectively. The experiments were also carried out on the post edited data set using same experimental set up and the highest accuracy of 71.70% was obtained. Finally, Information Gain based pruning has been performed in order to select the best features that are useful for classifying the gaze durations. Â© 2017, Springer International Publishing AG.",,,
10.1109/TIP.2017.2679442,2017,"Coutinho V.D.A., Cintra R.J., Bayer F.M.",Low-complexity multidimensional DCT approximations for high-order tensor data decorrelation,"In this paper, we introduce low-complexity multidimensional discrete cosine transform (DCT) approximations. 3D DCT approximations are formalized in terms of high-order tensor theory. The formulation is extended to higher dimensions with arbitrary lengths. Several multiplierless 8x8x8 approximate methods are proposed and the computational complexity is discussed for the general multidimensional case. The proposed methods complexity cost was assessed, presenting considerably lower arithmetic operations when compared with the exact 3D DCT. The proposed approximations were embedded into 3D DCT-based video coding scheme and a modified quantization step was introduced. The simulation results showed that the approximate 3D DCT coding methods offer almost identical output visual quality when compared with exact 3D DCT scheme. The proposed 3D approximations were also employed as a tool for visual tracking. The approximate 3D DCT-based proposed system performs similarly to the original exact 3D DCT-based method. In general, the suggested methods showed competitive performance at a considerably lower computational cost. Â© 2017 IEEE.",,,
,2017,"Li X.H., RÃ¶tting M., Wang W.S.",Bayesian network-based identification of driver lane-changing intents using eye tracking and vehicle-based data,"A Bayesian network decision-making method is proposed by combining driverâ€™s eye-tracking data and vehicle-based data together to identify driver lane-changing intents. First, experiments are conducted in a driving simulator with eye-tracker device to obtain the data when a subject driver makes lane-changing maneuvers. Second, collected data are analyzed in machine learning method using Bayesian decision-making approach to predict driverâ€™s lane-changing intents. Last, to show the benefits of our proposed method, comparison experiments are made between the data fusion way and only using eye tracking data or vehicle-based data. The results show that the Bayesian network with data fusion method performs better than using single information to recognize driverâ€™s lane-changing intents. At the same time, thresholds of Lane-changing probability and vehicle-based data as restricting condition choosing work is discussed in order to select the best identification parameter. Â© 2017 Taylor & Francis Group, London.",,,
10.1007/978-3-319-54526-4_18,2017,"Hsieh Y.-Y., Liu C.-C., Wang W.-L., Chuang J.-H.",Investigating size personalization for more accurate eye tracking glasses,"Personalized eyewear frame could improve the accuracy of eye tracking. To obtain the personalized frame size (temple length), we propose a new measuring instrument that consists of (i) the hardware, a 3D printed trial frame which has marks but no scales, and (ii) the software, a vision-based measurement which is view invariant. The vision-based measurement has accuracy and precision that are both 0.02 cm, while the trial frame can achieve a precision of 0.17 cm for secure wearing. Moreover, dispersion up to 2.56 cm is obtained among the personalized frame sizes for just a fairly small group of users, indicating the importance of having such a personalized measurement system. Â© Springer International Publishing AG 2017.",,,
10.1007/978-3-319-54187-7_28,2017,"Kacete A., SÃ©guier R., Collobert M., Royan J.",Unconstrained gaze estimation using random forest regression voting,"In this paper we address the problem of automatic gaze estimation using a depth sensor under unconstrained head pose motion and large user-sensor distances. To achieve robustness, we formulate this problem as a regression problem. To solve the task in hand, we propose to use a regression forest according to their high ability of generalization by handling large training set. We train our trees on an important synthetic training data using a statistical model of the human face with an integrated parametric 3D eyeballs. Unlike previous works relying on learning the mapping function using only RGB cues represented by the eye image appearances, we propose to integrate the depth information around the face to build the input vector. In our experiments, we show that our approach can handle real data scenarios presenting strong head pose changes even though it is trained only on synthetic data, we illustrate also the importance of the depth information on the accuracy of the estimation especially in unconstrained scenarios. Â© Springer International Publishing AG 2017",,,
10.1117/12.2266091,2017,"Kacete A., SÃ©guier R., Collobert M., Royan J.",Head pose free 3D gaze estimation using RGB-D camera,"In this paper, we propose an approach for 3D gaze estimation under head pose variation using RGB-D camera. Our method uses a 3D eye model to determine the 3D optical axis and infer the 3D visual axis. For this, we estimate robustly user head pose parameters and eye pupil locations with an ensembles of randomized trees trained with an important annotated training sets. After projecting eye pupil locations in the sensor coordinate system using the sensor intrinsic parameters and a one-time simple calibration by gazing a known 3D target under different directions, the 3D eyeball centers are determined for a specific user for both eyes yielding the determination of the visual axis. Experimental results demonstrate that our method shows a good gaze estimation accuracy even if the environment is highly unconstrained namely large user-sensor distances (> 1m50) unlike state-of-the-art methods which deal with relatively small distances (<1m). Â© 2017 SPIE.",,,
10.1117/12.2266109,2017,"Wibirama S., Mahesa R.R., Nugroho H.A., Hamamoto K.",Estimating 3D gaze in physical environment: A geometric approach on consumer-level remote eye tracker,"Remote eye trackers with consumer price have been used for various applications on flat computer screen. On the other hand, 3D gaze tracking in physical environment has been useful for visualizing gaze behavior, robots controller, and assistive technology. Instead of using affordable remote eye trackers, 3D gaze tracking in physical environment has been performed using corporate-level head mounted eye trackers, limiting its practical usage to niche user. In this research, we propose a novel method to estimate 3D gaze using consumer-level remote eye tracker. We implement geometric approach to obtain 3D point of gaze from binocular lines-of-sight. Experimental results show that the proposed method yielded low errors of 3.47Â±3.02 cm, 3.02Â±1.34 cm, and 2.57Â±1.85 cm in X, Y, and Z dimensions, respectively. The proposed approach may be used as a starting point for designing interaction method in 3D physical environment. Â© 2017 SPIE.",,,
10.1007/978-981-10-3518-0_24,2017,"Sharma S., Chakravarthy B.K.",Exploring visual response on form features of the autorickshaw,"Styling plays an important role in automotive appearances to attract visual attention. Features such as the headlights and windscreen become the prime carriers of style that can evoke visual interest. These features involve major to fine refinements in form as well as their compositional arrangements while designing. The feature that evokes high visual interest in isolation, or whether the combinations of features sustain visual interest for a long time, can be valuable knowledge for the designer and stakeholder teams to make visual decisions on form especially during early stages of designing. Present study deals with feature level observations of Autorickshaw concepts as Areas of Interest (AOI) to assess attention distribution towards the form features during concept development stage. The data is obtained as visual response of Product Designers (PD), Automobile Designers (AD), Autorickshaw Drivers (DR), and Passengers (PS) through eye tracking. The investigation analyzes how visual attention can be driven by features in isolation or in combination within larger units for four concept representations of the Autorickshaw. The effect of variations in visual-formal characteristics of the designed form is analyzed conjointly with by assessing the dwell time (DT) on each feature through a demarcation of specific AOIs, as well as a gridded analysis of the spatial distribution of visual attention on specific features. The findings indicate that the concept with curved features that are arranged in proximity, evoke high visual interest; instead of a concept, that supports sharp angular non-proximal features, implying that proximity and curved features can have strong combining effect to evoke and sustain visual interest in Autorickshaw styling. Overall, it was observed that the spatial distribution of attention converges, getting focused on the Cowl and the Headlights during style scrutiny. Increase in proximity of features results in an increased dwell time, thereby causing an active visual scrutiny that sustains visual interest. A combination of strategically placed proximal features, accentuates visual interest and the perception of style in observers. When available at the early stages of designing such inferential visual response feedback as objective knowledge can help the designers to anticipate sustained visual interest in a designed object, and help predict product acceptance, thereby curtailing potential market risks. Â© Springer Nature Singapore Pte Ltd. 2017.",,,
10.1109/TIP.2016.2628583,2017,"Xu M., Jiang L., Sun X., Ye Z., Wang Z.",Learning to Detect Video Saliency with HEVC Features,"Saliency detection has been widely studied to predict human fixations, with various applications in computer vision and image processing. For saliency detection, we argue in this paper that the state-of-the-art High Efficiency Video Coding (HEVC) standard can be used to generate the useful features in compressed domain. Therefore, this paper proposes to learn the video saliency model, with regard to HEVC features. First, we establish an eye tracking database for video saliency detection, which can be downloaded from https://github.com/remega/video-database. Through the statistical analysis on our eye tracking database, we find out that human fixations tend to fall into the regions with large-valued HEVC features on splitting depth, bit allocation, and motion vector (MV). In addition, three observations are obtained with the further analysis on our eye tracking database. Accordingly, several features in HEVC domain are proposed on the basis of splitting depth, bit allocation, and MV. Next, a kind of support vector machine is learned to integrate those HEVC features together, for video saliency detection. Since almost all video data are stored in the compressed form, our method is able to avoid both the computational cost on decoding and the storage cost on raw data. More importantly, experimental results show that the proposed method is superior to other state-of-the-art saliency detection methods, either in compressed or uncompressed domain. Â© 1992-2012 IEEE.",,,
10.1007/978-3-319-47024-5_11,2017,"Ma B., Jain E., Entezari A.",3D saliency from eye tracking with tomography,"This paper presents a method to build a saliency map in a volumetric dataset using 3D eye tracking. Our approach acquires the saliency information from multiple views of a 3D dataset with an eye tracker and constructs the 3D saliency volume from the gathered 2D saliency information using a tomographic reconstruction algorithm. Our experiments, on a number of datasets, show the effectiveness of our approach in identifying salient 3D features that attract userâ€™s attention. The obtained 3D saliency volume provides importance information and can be used in various applications such as illustrative visualization. Â© Springer International Publishing AG 2017.",,,
10.1007/978-3-319-47024-5_10,2017,"Wang X., Lindlbauer D., Lessig C., Alexa M.",Accuracy of monocular gaze tracking on 3D geometry,"Many applications such as data visualization or object recognition benefit from accurate knowledge of where a person is looking at. We present a system for accurately tracking gaze positions on a three dimensional object using a monocular head mounted eye tracker. We accomplish this by (1) using digital manufacturing to create stimuli whose geometry is know to high accuracy, (2) embedding fiducial markers into the manufactured objects to reliably estimate the rigid transformation of the object, and, (3) using a perspective model to relate pupil positions to 3D locations. This combination enables the efficient and accurate computation of gaze position on an object from measured pupil positions. We validate the of our system experimentally, achieving an angular resolution of 0.8Â° and a 1.5 % depth error using a simple calibration procedure with 11 points. Â© Springer International Publishing AG 2017.",,,
10.1109/TVCG.2016.2598796,2017,"Song H., Lee J., Kim T.J., Lee K.H., Kim B., Seo J.",GazeDx: Interactive Visual Analytics Framework for Comparative Gaze Analysis with Volumetric Medical Images,"We present an interactive visual analytics framework, GazeDx (abbr. of GazeDiagnosis), for the comparative analysis of gaze data from multiple readers examining volumetric images while integrating important contextual information with the gaze data. Gaze pattern comparison is essential to understanding how radiologists examine medical images, and to identifying factors influencing the examination. Most prior work depended upon comparisons with manually juxtaposed static images of gaze tracking results. Comparative gaze analysis with volumetric images is more challenging due to the additional cognitive load on 3D perception. A recent study proposed a visualization design based on direct volume rendering (DVR) for visualizing gaze patterns in volumetric images; however, effective and comprehensive gaze pattern comparison is still challenging due to a lack of interactive visualization tools for comparative gaze analysis. We take the challenge with GazeDx while integrating crucial contextual information such as pupil size and windowing into the analysis process for more in-depth and ecologically valid findings. Among the interactive visualization components in GazeDx, a context-embedded interactive scatterplot is especially designed to help users examine abstract gaze data in diverse contexts by embedding medical imaging representations well known to radiologists in it. We present the results from two case studies with two experienced radiologists, where they compared the gaze patterns of 14 radiologists reading two patients' volumetric CT images. Â© 2016 IEEE.",,,
10.1007/978-3-319-41627-4_20,2017,"Bellanca J.L., Orr T.J., Helfrich W., Macdonald B., Navoyski J., Eiter B.",Assessing Hazard identification in surface stone mines in a virtual environment,"Mine workers are expected to remain vigilant and successfully identify and mitigate hazards in both routine and non-routine locations. The goal of the current research project is to better understand how workers search and identify hazards. NIOSH researchers developed a data collection setup to measure a subjectâ€™s gaze, head position, and reaction time while examining 360Â° 2D-panoramic images at a surface mine. The data is integrated in semi real-time to determine region of interest (ROI) hit accuracy for hazards within the images. The purpose of this paper is to discuss the development and implementation of the hardware and software. The following aspects of the setup will be explored in the paper: (1) environment selection, (2) image creation, (3) stimulus display, (4) synchronization, (5) gaze mapping, and (6) region of interest (ROI) hit calculation. Â© Springer International Publishing Switzerland 2017.",,,
10.1007/s11042-015-3229-6,2017,"Qi F., Zhao D., Liu S., Fan X.",3D visual saliency detection model with generated disparity map,"Due to the remarkable distinction between human monocular vision and binocular vision, stereoscopic visual attention becomes an emerging question in the study of 3D applications. Some of existing 3D visual saliency detection models take advantage of ground-truth disparity map to compute center-surround differences of the depth features with high computational cost. In some 3D applications, the ground-truth disparity map may not be always available. In this paper, an efficient and simple 3D visual saliency detection model is proposed without using ground-truth disparity map. The proposed model is based on a band-pass filtering method which coincides with the visual perceptual process in human visual system. Firstly, the monocular luminance, color and texture features are extracted from the left viewâ€™s image; the binocular depth feature is extracted from the two viewsâ€?disparity map. Then, all the feature maps are filtered to generate three types of saliency maps, i.e., 2D saliency map, texture saliency map and depth saliency map. Subsequently, the three saliency maps are fused to one 3D saliency map by a linear pooling strategy. Finally, the final 3D visual saliency map is enhanced by the center-bias factor. Experimental results on a public eye tracking database show that the proposed model achieves better detection performance with low computational cost among the existing 3D visual saliency detection models. Â© 2016, Springer Science+Business Media New York.",,,
10.1007/s11042-015-3182-4,2017,"Guo Z., Zhou Q., Liu Z.",Appearance-based gaze estimation under slight head motion,"At present a lot of gaze estimation methods can get accurate result under ideal conditions, but some practical issues are still the biggest challenges affect the accuracy such as head motion and eye blinking. Improving the accuracy of gaze estimation and the tolerance of head motion are common tasks in the field of gaze estimation. Therefore, this paper aims to propose an accurate gaze estimation method without fixed head pose. The core problem is how to build the mapping relationship between image features and gaze position, and how to resist the head motion through the training samples. To this end, at first, a new input feature, which can well reflect the change of eye image features with different gaze positions, is proposed and it is based on appearance feature and distance feature. So the number of training samples in the process of calibration is significantly reduced. Then â„?-optimization is used to select an optimal set, which represents the mapping relationship between input feature and gaze position. At last, a linear equation is fitted to correct the initial estimation bias which is brought by head motion. In this paper, the experimental results demonstrate that our system achieves accurate result with one camera and a small number of calibration points. The accuracy of final gaze estimation is improved by 22Â % through compensation equation. In addition, our system is robustness to eye blink and distance change. Â© 2016, Springer Science+Business Media New York.",,,
10.1016/j.neucom.2016.08.070,2016,"Zhuang B., Wang L., Lu H.",Visual tracking via shallow and deep collaborative model,"In this paper, we propose a robust tracking method based on the collaboration of a generative model and a discriminative classifier, where features are learned by shallow and deep architectures, respectively. For the generative model, we introduce a block-based incremental learning scheme, in which a local binary mask is constructed to deal with occlusion. The similarity degrees between the local patches and their corresponding subspace are integrated to formulate a more accurate global appearance model. In the discriminative model, we exploit the advances of deep learning architectures to learn generic features which are robust to both background clutters and foreground appearance variations. To this end, we first construct a discriminative training set from auxiliary video sequences. A deep classification neural network is then trained offline on this training set. Through online fine-tuning, both the hierarchical feature extractor and the classifier can be adapted to the appearance change of the target for effective online tracking. The collaboration of these two models achieves a good balance in handling occlusion and target appearance change, which are two contradictory challenging factors in visual tracking. Both quantitative and qualitative evaluations against several state-of-the-art algorithms on challenging image sequences demonstrate the accuracy and the robustness of the proposed tracker. Â© 2016 Elsevier B.V.",,,
10.1109/CVPRW.2016.104,2016,"Jeni L.A., Cohn J.F.",Person-Independent 3D Gaze Estimation Using Face Frontalization,"Person-independent and pose-invariant estimation of eye-gaze is important for situation analysis and for automated video annotation. We propose a fast cascade regression based method that first estimates the location of a dense set of markers and their visibility, then reconstructs face shape by fitting a part-based 3D model. Next, the reconstructed 3D shape is used to estimate a canonical view of the eyes for 3D gaze estimation. The model operates in a feature space that naturally encodes local ordinal properties of pixel intensities leading to photometric invariant estimation of gaze. To evaluate the algorithm in comparison with alternative approaches, three publicly-available databases were used, Boston University Head Tracking, Multi-View Gaze and CAVE Gaze datasets. Precision for head pose and gaze averaged 4 degrees or less for pitch, yaw, and roll. The algorithm outperformed alternative methods in both datasets. Â© 2016 IEEE.",,,
10.1109/CVPRW.2016.120,2016,"Jones A., Nagano K., Busch J., Yu X., Peng H.-Y., Barreto J., Alexander O., Bolas M., Debevec P., Unger J.",Time-Offset Conversations on a Life-Sized Automultiscopic Projector Array,"We present a system for creating and displaying interactive life-sized 3D digital humans based on pre-recorded interviews. We use 30 cameras and an extensive list of questions to record a large set of video responses. Users access videos through a natural conversation interface that mimics face-to-face interaction. Recordings of answers, listening and idle behaviors are linked together to create a persistent visual image of the person throughout the interaction. The interview subjects are rendered using flowed light fields and shown life-size on a special rear-projection screen with an array of 216 video projectors. The display allows multiple users to see different 3D perspectives of the subject in proper relation to their viewpoints, without the need for stereo glasses. The display is effective for interactive conversations since it provides 3D cues such as eye gaze and spatial hand gestures. Â© 2016 IEEE.",,,
10.1109/ICCOINS.2016.7783257,2016,"Tomi A.B., Rambli D.R.A.",Automated calibration for optical see-through head mounted display using display screen space based eye tracking,"A crucial problem in Augmented Reality (AR) applications that used optical see-through head mounted display (OST HMD) is frequent (re)calibrations. It is considered time consuming and distract users from their application if each such frequent (re)calibrations involve user interactions. Furthermore, it will be prone to user-dependent errors into the system setup thus leads to reduce user's acceptance on OST HMD in AR applications. Nowadays, a few studies toward interaction-free calibration implementation on OST HMD have been done. Recent implementation on interaction-free calibration issues are associated with the virtual display calibration which is related to the eye and world camera relation with display screen. Thus, we propose a novel approach that utilizes a display screen space based eye tracking into OST HMD calibration. By simultaneously measuring eye movements and map the tracked eye coordinate into a 2D display screen space, a calibration formulation can be done without involving the eye tracker properties. This work is expected to be established for efficient and high accuracy interaction-free OST HMD calibration which will be beneficial in AR applications that used OST HMD. Â© 2016 IEEE.",,,
10.1109/CVPR.2016.375,2016,"Yu P., Zhou J., Wu Y.",Learning Reconstruction-Based Remote Gaze Estimation,"It is a challenging problem to accurately estimate gazes from low-resolution eye images that do not provide fine and detailed features for eyes. Existing methods attempt to establish the mapping between the visual appearance space to the gaze space. Different from the direct regression approach, the reconstruction-based approach represents appearance and gaze via local linear reconstruction in their own spaces. A common treatment is to use the same local reconstruction in the two spaces, i.e., the reconstruction weights in the appearance space are transferred to the gaze space for gaze reconstruction. However, this questionable treatment is taken for granted but has never been justified, leading to significant errors in gaze estimation. This paper is focused on the study of this fundamental issue. It shows that the distance metric in the appearance space needs to be adjusted, before the same reconstruction can be used. A novel method is proposed to learn the metric, such that the affinity structure of the appearance space under this new metric is as close as possible to the affinity structure of the gaze space under the normal Euclidean metric. Furthermore, the local affinity structure invariance is utilized to further regularize the solution to the reconstruction weights, so as to obtain a more robust and accurate solution. Effectiveness of the proposed method is validated and demonstrated through extensive experiments on different subjects. Â© 2016 IEEE.",,,
10.1145/3029610.3029614,2016,"Ishizuka A., Yorozu A., Takahashi M.",Experimental verification for motion control of a powered wheelchair using a gazing feature in an environment,"This paper describes the motion control system for a powered wheelchair using a gaze in an unknown environment. Recently, new Human-Computer Interfaces (HCIs) that have replaced joysticks have been developed for a person with a disability of the upper body. In this paper, movement of the eyes is used as an HCI. The wheelchair control system proposed in this study aims to achieve an operation such that a passenger gazes towards the direction he or she wants to move in the unknown environment. The gazing feature of the passenger in the 3D environment is acquired in real time and the wheelchair is subsequently controlled. The features include the entrance of the area of the passage and the gazing feature is acquired by obtaining the features and the gazing point of the passenger. The acquired information about the direction in which the passenger wants to move becomes operation input to the wheelchair. The wheelchair is controlled by obtaining this operation input and the information of the environment. The conventional motion control system can perform safe and smooth movement by avoiding obstacles. The effectiveness of the proposed system is demonstrated through experiments in a real environment with three participants. Â© 2016 ACM.",,,
10.1145/3012009,2016,"Schneider B., Sharma K., Cuendet S., Zufferey G., Dillenbourg P., Pea E.R.",Using mobile eye-trackers to unpack the perceptual benefits of a tangible user interface for collaborative learning,"In this study, we investigated the way users memorize, analyze, collaborate, and learn new concepts on a Tangible User Interface (TUI). Twenty-seven pairs of apprentices in logistics (N = 54) interacted with an interactive simulation of a warehouse. Their task was to discover efficient design principles for building storehouses. In a between-subjects experimental design, half of the participants used 3D physical shelves, whereas the other half used 2D paper shelves. This manipulation allowed us to control for the ""representational effect"" of 3D tangibles: the first group saw the warehouse as a small-scale model with realistic shelves, whereas the second group had access to a more abstract layout with rectangular pieces of paper. Both groups interacted with the system in the same way. We found that participants in the first group (i.e., who used 3D realistic shelves) better memorized a warehouse layout, built a more efficient model, and scored higher on a learning test. Additionally, students wore eye-tracking goggles while completing those tasks; preliminary results suggest that 3D interfaces increased joint visual attention, which was found to be a significant predictor for participants' task performance and learning gains. Implications for designing TUIs in collaborative settings are discussed. Â© 2016 ACM.",,,
10.1016/j.dss.2016.09.010,2016,"Anderson B.B., Jenkins J.L., Vance A., Kirwan C.B., Eargle D.",Your memory is working against you: How eye tracking and memory explain habituation to security warnings,"Security warnings are critical to the security of end users and their organizations, often representing the final defense against an attack. Because warnings require users to make a contextual judgment, it is critical that they pay close attention to warnings. However, research shows that users routinely disregard them. A major factor contributing to the ineffectiveness of warnings is habituation, the decreased response to a repeated warning. Although previous research has identified the problem of habituation, the phenomenon has only been observed indirectly through behavioral measures. Therefore, it is unclear how habituation develops in the brain in response to security warnings, and how this in turn influences usersâ€?perceptions of these warnings. This paper contributes by using eye tracking to measure the eye movement-based memory (EMM) effect, a neurophysiological manifestation of habituation in which people unconsciously scrutinize previously seen stimuli less than novel stimuli. We show that habituation sets in after only a few exposures to a warning and progresses rapidly with further repetitions. Using guidelines from the warning science literature, we design a polymorphic warning artifact which repeatedly changes its appearance. We demonstrate that our polymorphic warning artifact is substantially more resistant to habituation than conventional security warnings, offering an effective solution for practice. Finally, our results highlight the value of applying neuroscience to the domain of information security behavior. Â© 2016 Elsevier B.V.",,,
10.1016/j.neunet.2016.08.012,2016,"MilovanoviÄ‡ M.B., AntiÄ‡ D.S., MilojkoviÄ‡ M.T., NikoliÄ‡ S.S., PeriÄ‡ S.L., SpasiÄ‡ M.D.",Adaptive PID control based on orthogonal endocrine neural networks,"A new intelligent hybrid structure used for online tuning of a PID controller is proposed in this paper. The structure is based on two adaptive neural networks, both with built-in Chebyshev orthogonal polynomials. First substructure network is a regular orthogonal neural network with implemented artificial endocrine factor (OENN), in the form of environmental stimuli, to its weights. It is used for approximation of control signals and for processing system deviation/disturbance signals which are introduced in the form of environmental stimuli. The output values of OENN are used to calculate artificial environmental stimuli (AES), which represent required adaptation measure of a second networkâ€”orthogonal endocrine adaptive neuro-fuzzy inference system (OEANFIS). OEANFIS is used to process control, output and error signals of a system and to generate adjustable values of proportional, derivative, and integral parameters, used for online tuning of a PID controller. The developed structure is experimentally tested on a laboratory model of the 3D crane system in terms of analysing tracking performances and deviation signals (error signals) of a payload. OENN-OEANFIS performances are compared with traditional PID and 6 intelligent PID type controllers. Tracking performance comparisons (in transient and steady-state period) showed that the proposed adaptive controller possesses performances within the range of other tested controllers. The main contribution of OENN-OEANFIS structure is significant minimization of deviation signals (17%â€?9%) compared to other controllers. It is recommended to exploit it when dealing with a highly nonlinear system which operates in the presence of undesirable disturbances. Â© 2016 Elsevier Ltd",,,
10.1016/j.media.2016.05.003,2016,"Rieke N., Tan D.J., Amat di San Filippo C., Tombari F., Alsheakhali M., Belagiannis V., Eslami A., Navab N.",Real-time localization of articulated surgical instruments in retinal microsurgery,"Real-time visual tracking of a surgical instrument holds great potential for improving the outcome of retinal microsurgery by enabling new possibilities for computer-aided techniques such as augmented reality and automatic assessment of instrument manipulation. Due to high magnification and illumination variations, retinal microsurgery images usually entail a high level of noise and appearance changes. As a result, real-time tracking of the surgical instrument remains challenging in in-vivo sequences. To overcome these problems, we present a method that builds on random forests and addresses the task by modelling the instrument as an articulated object. A multi-template tracker reduces the region of interest to a rectangular area around the instrument tip by relating the movement of the instrument to the induced changes on the image intensities. Within this bounding box, a gradient-based pose estimation infers the location of the instrument parts from image features. In this way, the algorithm does not only provide the location of instrument, but also the positions of the tool tips in real-time. Various experiments on a novel dataset comprising 18 in-vivo retinal microsurgery sequences demonstrate the robustness and generalizability of our method. The comparison on two publicly available datasets indicates that the algorithm can outperform current state-of-the art. Â© 2016",,,
10.1109/TSP.2016.7760973,2016,"Yilmaz C.M., Kose C.",Local binary pattern histogram features for on-screen eye-gaze direction estimation and a comparison of appearance based methods,"Human Computer Interaction (HCI) has become an important focus of both computer science researches and industrial applications. And, on-screen gaze estimation is one of the hottest topics in this rapidly growing field. Eye-gaze direction estimation is a sub-research area of on-screen gaze estimation and the number of studies that focused on the estimation of on-screen gaze direction is limited. Due to this, various appearance-based video-oculography methods are investigated in this work. Firstly, a new dataset is created via user images taken from daylight censored cameras located at computer screen. Then, Local Binary Pattern Histogram (LBPH), which is used in this work for the first time to obtain on-screen gaze direction information, and Principal Component Analysis (PCA) methods are employed to extract image features. And, parameter optimized Support Vector Machine (SVM), Artificial Neural Networks (ANNs) and k-Nearest Neighbor (k-NN) learning methods are adopted in order to estimate on-screen gaze direction. Finally, these methods' abilities to correctly estimate the on-screen gaze direction are compared using the resulting classification accuracies of applied methods and previous works. The best classification accuracy of 96.67% is obtained when using LBPH and SVM method pair which is better than previous works. The results also show that appearance based methods are pretty applicable for estimating on-screen gaze direction. Â© 2016 IEEE.",,,
10.1109/IROS.2016.7759379,2016,"Gras G., Yang G.-Z.",Intention recognition for gaze controlled robotic minimally invasive laser ablation,"Eye tracking technology has shown promising results for allowing hands-free control of robotically-mounted cameras and tools. However existing systems present only limited capabilities in allowing the full range of camera motions in a safe, intuitive manner. This paper introduces a framework for the recognition of surgeon intention, allowing activation and control of the camera through natural gaze behaviour. The system is resistant to noise such as blinking, while allowing the surgeon to look away safely at any time. Furthermore, this paper presents a novel approach to control the translation of the camera along its optical axis using a combination of eye tracking and stereo reconstruction. Combining eye tracking and stereo reconstruction allows the system to determine which point in 3D space the user is fixating, enabling a translation of the camera to achieve the optimal viewing distance. In addition, the eye tracking information is used to perform automatic laser targeting for laser ablation. The desired target point of the laser, mounted on a separate robotic arm, is determined with the eye tracking thus removing the need to manually adjust the laser's target point before starting each new ablation. The calibration methodology used to obtain millimetre precision for the laser targeting without the aid of visual servoing is described. Finally, a user study validating the system is presented, showing clear improvement with median task times under half of those of a manually controlled robotic system. Â© 2016 IEEE.",,,
10.1016/j.neucom.2016.05.079,2016,"Liu F., Zhou T., Yang J.",Geometric affine transformation estimation via correlation filter for visual tracking,"Correlation filter achieves promising performance with high speed in visual tracking. However, conventional correlation filter based trackers cannot tackle affine transformation issues such as scale variation, rotation and skew. To address this problem, in this paper, we propose a part-based representation tracker via kernelized correlation filter (KCF) for visual tracking. A Spatialâ€“Temporal Angle Matrix (STAM), severed as confidence metric, is proposed to select reliable patches from parts via multiple correlation filters. These stable patches are used to estimate a 2D affine transformation matrix of the target in a geometric method. Specially, the whole combination scheme for these stable patches is proposed to exploit sampling space in order to obtain numerous affine matrices and their corresponding candidates. The diversiform candidates would help to seek for the optimal candidate to represent the object's accurate affine transformation in a higher probability. Both qualitative and quantitative evaluations on VOT2014 challenge and Object Tracking Benchmark (OTB) show that the proposed tracking method achieves favorable performance compared with other state-of-the-art methods. Â© 2016 Elsevier B.V.",,,
10.1016/j.neucom.2016.06.058,2016,"Li G., Ma B., Huang J., Huang Q., Zhang W.",Beyond appearance model: Learning appearance variations for object tracking,"In this paper, we present a novel appearance variation prediction model which can be embedded into the existing generative appearance model based tracking framework. Different from the existing works, which online learn appearance model with obtained tracking results, we propose to predict appearance reconstruction error. We notice that although the learned appearance model can precisely describe the target in the previous frames, the tracking result is still not accurate if in the following frame, the patch that is most similar to appearance model is assumed to be the target. We first investigate the above phenomenon by conducting experiments on two public sequences and discover that in most cases the best target is not the one with minimal reconstruction error. Then we design three kinds of features which can encode motion, appearance, appearance reconstruction error information of target's surrounding image patches, and capture potential factors that may cause variations of target's appearance as well as its reconstruction error. Finally, with these features, we learn an effective random forest for predicting reconstruction error of the target during tracking. Experiments on various datasets demonstrate that the proposed method can be combined with many existing trackers and improve their performances significantly. Â© 2016 Elsevier B.V.",,,
10.1016/j.neucom.2016.06.048,2016,"Chen W., Zhang K., Liu Q.",Robust visual tracking via patch based kernel correlation filters with adaptive multiple feature ensemble,"Both patch based and correlation filter-based tracking methods have achieved competitive results on accuracy and robustness, but there is still a large room to improve their overall performance if carefully dealing with the challenging factors in visual tracking. In this paper, we present a patch based tracker which adaptively integrates the kernel correlation filters with multiple effective features. To take full advantage of the useful information from different parts of the target, we train each template patch by kernel correlation filtering method, and adaptively set the weight of each patch for each particle in a particle filtering framework. Experiments illustrate that this scheme can effectively handle the occlusion problem. Moreover, the effective features including the HOG features and color name features are effectively integrated to learn the correlations between the target and the background, the candidate patches and template ones, which further boosts the overall performance. Extensive experimental results on the CVPR2013 tracking benchmark demonstrate that the proposed approach performs favorably against some representative state-of-the-art tracking algorithms. Â© 2016 Elsevier B.V.",,,
10.1016/j.neucom.2015.10.149,2016,"Yao R., Xia S., Zhou Y., Niu Q.",Robust lifelong visual tracking using compact binary feature with color attributes,"In this paper, we address the problem of visual tracking where the target object undergo appearance variations due to illumination variation, occlusion, motion, background clutter and deformation. To deal with the significant appearance variations, we introduce color attributes into traditional shape feature at cell level, the new feature representation takes into consideration both its photometric invariance as well as its discriminative power. We construct compact binary code for the shape-color feature to reduce the high dimensions, and update the hash function in an online manner. A discriminative lifelong learning model is built to construct an appearance model that optimally separates the object from its surrounds. The lifelong learner uses the shared latent basis to transfer historical observations to simple classifier while a new frame arrives. Experimental results on tracking benchmark demonstrate that the proposed tracking algorithm outperforms state-of-the-art methods. Â© 2016 Elsevier B.V.",,,
10.1145/3009939.3009952,2016,"Mahfoud E., Lu A.",Gaze-directed immersive visualization of scientific ensembles,"The latest advances in head-mounted displays (HMDs) for augmented reality (AR) and mixed reality (MR) have produced commercialized devices that are gradually accepted by the public. These HMDs are generally equipped with head tracking, which provides an excellent input to explore immersive visualization and interaction techniques for various AR/MR applications. This paper explores the head tracking function on the latest Microsoft HoloLens - where gaze is defined as the ray starting at the head location and points forward. We present a gaze-directed visualization approach to study ensembles of 2D oil spill simulations in mixed reality. Our approach allows users to place an ensemble as an image stack in a real environment and explore the ensemble with gaze tracking. The prototype system demonstrates the challenges and promising effects of gaze-based interaction in the state-of-the-art mixed reality.",,,
10.1145/2993369.2993407,2016,"Greenwald S.W., Loreti L., Funk M., Zilberman R., Maes P.",Eye gaze tracking with Google cardboard using Purkinje images,"Mobile phone-based Virtual Reality (VR) is rapidly growing as a platform for stereoscopic 3D and non-3D digital content and applications. The ability to track eye gaze in these devices would be a tremendous opportunity on two fronts: firstly, as an interaction technique, where interaction is currently awkward and limited, and secondly, for studying human visual behavior. We propose a method to add eye gaze tracking to these existing devices using their on-board display and camera hardware, with a minor modification to the headset enclosure. We present a proof-of-concept implementation of the technique and show results demonstrating its feasibility. The software we have developed will be made available as open source to benefit the research community.",,,
10.1145/2980179.2980230,2016,"Kellnhofer P., Didyk P., Ritschel T., Masia B., Myszkowski K., Seidel H.-P.",Motion parallax in stereo 3D: Model and applications,"Binocular disparity is the main depth cue that makes stereoscopic images appear 3D. However, in many scenarios, the range of depth that can be reproduced by this cue is greatly limited and typically fixed due to constraints imposed by displays. For example, due to the low angular resolution of current automultiscopic screens, they can only reproduce a shallow depth range. In this work, we study the motion parallax cue, which is a relatively strong depth cue, and can be freely reproduced even on a 2D screen without any limits. We exploit the fact that in many practical scenarios, motion parallax provides sufficiently strong depth information that the presence of binocular depth cues can be reduced through aggressive disparity compression. To assess the strength of the effect we conduct psychovisual experiments that measure the influence of motion parallax on depth perception and relate it to the depth resulting from binocular disparity. Based on the measurements, we propose a joint disparity-parallax computational model that predicts apparent depth resulting from both cues. We demonstrate how this model can be applied in the context of stereo and multiscopic image processing, and propose new disparity manipulation techniques, which first quantify depth obtained from motion parallax, and then adjust binocular disparity information accordingly. This allows us to manipulate the disparity signal according to the strength of motion parallax to improve the overall depth reproduction. This technique is validated in additional experiments. Â© 2016 Copyright held by the owner/author(s).",,,
10.11591/ijeecs.v4.i2.pp439-446,2016,"Hajraoui A., Sabri M.",Generic and robust method for head pose estimation,"Head pose estimation has fascinated the research community due to its application in facial motion capture, human-computer interaction and video conferencing. It is a pre-requisite to gaze tracking, face recognition, and facial expression analysis. In this paper, we present a generic and robust method for model-based global 2D head pose estimation from single RGB Image. In our approach we use of the one part the Gabor filters to conceive a robust pose descriptor to illumination and facial expression variations, and that target the pose information. Moreover, we ensure the classification of these descriptors using a SVM classifier. The approach has proved effective view the rate for the correct pose estimations that we got. Â© 2016 Institute of Advanced Engineering and Science. All rights reserved.",,,
10.1109/TMM.2016.2613681,2016,"Liang H., Liang R., Sun G.",Looking Into Saliency Model via Space-Time Visualization,"We introduce a visual analytics method to analyze eye-tracking data and saliency models for dynamic stimuli, such as video or animated graphics. The focus lies on the analysis of the different performance of saliency models in contrast to human observers to identify trends in the general viewing behavior, including time sequences of attentional synchrony and objects with a strong attentional focus. By using a space-time cube visualization in combination with clustering, the dynamic stimuli and associated eye gazes as well as the attention maps from saliency models can be analyzed in a static three-dimensional representation. We propose algorithms to keep the appearance of the computer's attention data in line with the human's eye-tracking data. The analytical process is supported by multiple coordinated views that allow the user to focus on different aspects of spatial and temporal information in eye gaze data and saliency map. By comparing attention data from both human and computer incorporated with the spatiotemporal characteristics, we are able to find the different patterns within human and computer algorithms. We list our key findings to help developing better saliency detection algorithms. Â© 2016 IEEE.",,,
10.1007/s00138-016-0776-4,2016,"Fuhl W., Tonsen M., Bulling A., Kasneci E.",Pupil detection for head-mounted eye tracking in the wild: an evaluation of the state of the art,"Robust and accurate detection of the pupil position is a key building block for head-mounted eye tracking and prerequisite for applications on top, such as gaze-based humanâ€“computer interaction or attention analysis. Despite a large body of work, detecting the pupil in images recorded under real-world conditions is challenging given significant variability in the eye appearance (e.g., illumination, reflections, occlusions, etc.), individual differences in eye physiology, as well as other sources of noise, such as contact lenses or make-up. In this paper we review six state-of-the-art pupil detection methods, namely ElSeÂ (Fuhl etÂ al. in Proceedings of the ninth biennial ACM symposium on eye tracking research & applications, ACM. New York, NY, USA, pp 123â€?30, 2016), ExCuSeÂ (Fuhl et al. in Computer analysis of images and patterns. Springer, New York, pp 39â€?1, 2015), Pupil LabsÂ (Kassner et al. in Adjunct proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing (UbiComp), pp 1151â€?160, 2014. doi:10.1145/2638728.2641695), SETÂ (Javadi et al. in Front Neuroeng 8, 2015), StarburstÂ (Li et al. in Computer vision and pattern recognition-workshops, 2005. IEEE Computer society conference on CVPR workshops. IEEE, pp 79â€?9, 2005), and Åšwirski (Åšwirski et al. in Proceedings of the symposium on eye tracking research and applications (ETRA). ACM, pp 173â€?76, 2012. doi:10.1145/2168556.2168585). We compare their performance on a large-scale data set consisting of 225,569 annotated eye images taken from four publicly available data sets. Our experimental results show that the algorithm ElSeÂ (Fuhl et al.Â 2016) outperforms other pupil detection methods by a large margin, offering thus robust and accurate pupil positions on challenging everyday eye images. Â© 2016, Springer-Verlag Berlin Heidelberg.",,,
10.1109/ICCChina.2016.7636835,2016,"Zhang W., Wang H., Zhao F.",Improved eye center location system in low-resolution images,"The position of the eyes contains many valuable information, which is used in a wide range of applications such as face recognition or eye tracking. However, most commercial eye-gaze trackers are often expensive and require the user to be equipped with a head mounted device which is inconvenient and unattractive. In order to locate eye centers solely on appearance, several methods have been proposed in the literature, but these methods often fail to accurately locate the eye centers in low-resolution images taken from a simple webcam. Therefore, we propose an improved system for accurate and robust eye center location by combining variance projection and ellipse fitting. First we utilize variance projection to detect the candidate region of the eyes. In this step, for eyeglass wearers, we proposed a method to remove the disruption of the glasses. Then least squares ellipse fitting is used to locate the accurate eye center. In this paper, we extensively test our system for its accuracy and robustness to changes in illumination, head pose and eye rotation. We demonstrate that our system outperforms conventional ones. Â© 2016 IEEE.",,,
10.1145/2984511.2984536,2016,"Sugano Y., Zhang X., Bulling A.",AggreGaze: Collective estimation of audience attention on public displays,"Gaze is frequently explored in public display research given its importance for monitoring and analysing audience attention. However, current gaze-enabled public display interfaces require either special-purpose eye tracking equipment or explicit personal calibration for each individual user. We present AggreGaze, a novel method for estimating spatio-temporal audience attention on public displays. Our method requires only a single off-the-shelf camera attached to the display, does not require any personal calibration, and provides visual attention estimates across the full display. We achieve this by 1) compensating for errors of state-of-the-art appearance-based gaze estimation methods through on-site training data collection, and by 2) aggregating uncalibrated and thus inaccurate gaze estimates of multiple users into joint attention estimates. We propose different visual stimuli for this compensation: a standard 9-point calibration, moving targets, text and visual stimuli embedded into the display content, as well as normal video content. Based on a two-week deployment in a public space, we demonstrate the effectiveness of our method for estimating attention maps that closely resemble ground-truth audience gaze distributions.",,,
10.1145/2984751.2984772,2016,"Lee J., Cheon M., Moon S.-E., Lee J.-S.",Peripersonal space in virtual reality: Navigating 3D space with different perspectives,"We introduce the concept of ""peripersonal space"" of an avatar in 3D virtual reality and discuss how it plays an important role on 3D navigation with different perspectives. By analyzing the eye-gaze data of avatar-based navigation with first-person perspective and third-person perspective, we examine the effects of an avatar's peripersonal space on the users' perceptual scopes within 3D virtual environments. We propose that manipulating peripersonal space of an avatar with various perspectives has the immediate effects on the users' scopes of perception as well as the patterns of attentional capture. This study provides a helpful guideline for designing more effective navigation system with an avatar in 3D virtual environment. Â© 2016 Copyright held by the author/owner(s).",,,
10.1016/j.knosys.2016.07.038,2016,"Wang Y., Shen T., Yuan G., Bian J., Fu X.",Appearance-based gaze estimation using deep features and random forest regression,"Conventional appearance-based gaze estimation methods employ local or global features as eye gaze appearance descriptor. But these methods don't work well under natural light with free head movement. To solve this problem, we present an appearance-based gaze estimation method using deep feature representation and feature forest regression. The deep feature is learned through hierarchical extraction of deep Convolutional Neural Network (CNN). And random forest regression with cluster-to-classify node splitting rules is used to take advantage of data distribution in sparse feature space. Experimental results demonstrate that the deep feature has a better performance than local features on calibrated gaze regression. The combination of deep features and random forest regression provides an effective solution for gaze estimation in a natural environment. Â© 2016",,,
10.1007/s11257-016-9179-5,2016,"LallÃ© S., Conati C., Carenini G.",Prediction of individual learning curves across information visualizations,"Confident usage of information visualizations is thought to be influenced by cognitive aspects as well as amount of exposure and training. To support the development of individual competency in visualization processing, it is important to ascertain if we can track usersâ€?progress or difficulties they might have while working with a given visualization. In this paper, we extend previous work on predicting in real time a userâ€™s learning curveâ€”a mathematical model that can represent a userâ€™s skill acquisition abilityâ€”when working with a visualization. First, we investigate whether results we previously obtained in predicting usersâ€?learning curves during visualization processing generalize to a different visualization. Second, we study to what extent we can make predictions on a userâ€™s learning curve without information on the visualization being used. Our models leverage various data sources, including a userâ€™s gaze behavior, pupil dilation, and cognitive abilities. We show that these models outperform a baseline that leverages knowledge on user task performance so far. Our best performing model achieves good accuracies in predicting usersâ€?learning curves even after observing usersâ€?performance on a few tasks only. These results represent an important step toward understanding how to support users in learning a new visualization. Â© 2016, Springer Science+Business Media Dordrecht.",,,
10.1007/s11042-015-2600-y,2016,"Xiong C., Huang L., Liu C.",Remote gaze estimation based on 3D face structure and iris centers under natural light,"Remote gaze estimation under natural light is still a challenging problem. Appearance based methods are seriously sensitive to illumination variation in the visual spectrum and usually can hardly handle the problem of head movements. And most existing feature-based gaze estimation methods strongly rely on cornea reflections, which are unstable to glasses, head movements and especially useless for natural light condition. In this paper, we propose a novel feature based gaze estimation method without use of cornea reflections. A stereo camera system is built for the proposed method. Firstly, 3D Active Shape Models (ASM) is reconstructed using stereo vision to represent 3D face structure. Then, without use of cornea reflections, a 3D Iris-Eye-Contours based descriptor is proposed to represent human gaze information. Iris centers are used in natural light just like the pupil centers in condition of near-infrared light. Whatâ€™s more, precise estimation of head poses based on 3D face structure is employed to rectify the 3D iris centers and eye contours for improving the ability of tolerance to head movements. Experiments on several subjects show that the system is accurate and allows natural head movements under natural light. Â© 2015, Springer Science+Business Media New York.",,,
10.1145/2968220.2968235,2016,"West R., Kajihara M., Parola M., Holloway M., Hays K., Hillard L., Carlew A., Deutsch J., Lane B., John B., Sanandaji A., Grimm C.",Eliciting tacit expertise in 3D volume segmentation,"The output of 3D volume segmentation is crucial to a wide range of endeavors. Producing accurate segmentations often proves to be both ineficient and challenging, in part due to lack of imaging data quality (contrast and resolution), and because of ambiguity in the data that can only be resolved with higher-level knowledge of the structure and the context wherein it resides. Automatic and semi-Automatic approaches are improving, but in many cases still fail or require substantial manual clean-up or intervention. Expert manual segmentation and review is therefore still the gold standard for many applications. Unfortunately, existing tools (both custom-made and commercial) are often designed based on the underlying algorithm, not the best method for expressing higher-level intention. Our goal is to analyze manual (or semi-Automatic) segmentation to gain a better understanding of both low-level (perceptual tasks and actions) and high-level decision making. This can be used to produce segmentation tools that are more accurate, effcient, and easier to use. Questioning or observation alone is insu ffcient to capture this information, so we utilize a hybrid capture protocol that blends observation, surveys, and eye tracking. We then developed, and validated, data coding schemes capable of discerning low-level actions and overall task structures. Â© 2016 Copyright.",,,
10.1145/2968219.2971389,2016,"Barz M., Sonntag D.",Gaze-guided object classification using deep neural networks for attention-based computing,"Recent advances in eye tracking technologies opened the way to design novel attention-based user interfaces. This is promising for pro-Active and assistive technologies for cyber-physical systems in the domains of, e.g., healthcare and industry 4.0. Prior approaches to recognize a user's attention are usually limited to the raw gaze signal or sensors in instrumented environments. We propose a system that (1) incorporates the gaze signal and the egocentric camera of the eye tracker to identify the objects the user focuses at; (2) employs object classification based on deep learning which we recompiled for our purposes on a GPU-based image classification server; (3) detects whether the user actually draws attention to that object; and (4) combines these modules for constructing episodic memories of egocentric events in real-Time. Â© 2016 ACM.",,,
10.1145/2957265.2965014,2016,"Kit D., Sullivan B.",Classifying mobile eye tracking data with hidden Markov models,"Naturalistic eye movement behavior has been measured in a variety of scenarios [15] and eye movement patterns appear indicative of task demands [16]. However, systematic task classification of eye movement data is a relatively recent development [1,3,7]. Additionally, prior work has focused on classification of eye movements while viewing 2D screen based imagery. In the current study, eye movements from eight participants were recorded with a mobile eye tracker. Participants performed five everyday tasks: Making a sandwich, transcribing a document, walking in an office and a city street, and playing catch with a flying disc [14]. Using only saccadic direction and amplitude time series data, we trained a hidden Markov model for each task and classified unlabeled data by calculating the probability that each model could generate the observed sequence. We present accuracy and time to recognize results, demonstrating better than chance performance.",,,
10.1145/2957265.2965016,2016,"Toivanen M., HÃ¤kkinen J., PuolamÃ¤ki K., Radun J., Lukander K.",Inferring user action with mobile gaze tracking,"Gaze tracking in psychological, cognitive, and user interaction studies has recently evolved toward mobile solutions, as they enable direct assessing of users' visual attention in natural environments, and augmented and virtual reality (AR/VR) applications. Productive approaches in analyzing and predicting user actions with gaze data require a multi-disciplinary approach with experts in cognitive and behavioral sciences, machine vision, and machine learning. This workshop brings together a cross-domain group of individuals to (i) discuss and contribute to the problem of using mobile gaze tracking for inferring user action, (ii) advance the sharing of data and analysis algorithms as well as device solutions, and (iii) increase understanding of behavioral aspects of gaze-action sequences in natural environments and AR/VR applications.",,,
10.1016/j.neucom.2015.10.133,2016,"Liu J.-W., Sun W.-P., Xia T.",Adaptive structured sub-blocks tracking,"Visual object tracking algorithms based on middle level appearance have been widely studied for their effective representation to non-rigid appearance variation and partial occlusion. Sub-blocks are often adopted as local feature in mid-level based tracking algorithms. How to select representative sub-blocks to reveal the spatial structure of objects and retain the flexibility to model non-rigid deformation has not been adequately addressed. Exploiting discrimination, uniqueness and historical prediction accuracy of sub-blocks of a target, we propose a local feature selection method which includes rough initial subblock selection and refined subblock-sample particle bi-directional selection under particle filter tracking framework. A quantitative evaluation is conducted on 10 sequences. Experimental results show the robustness of our proposed algorithm in tackling with non-rigid deformation and partial occlusion. Â© 2016 Elsevier B.V.",,,
10.1109/TMM.2016.2576284,2016,"Lu F., Gao Y., Chen X.",Estimating 3D gaze directions using unlabeled eye images via synthetic iris appearance fitting,"Estimating three-dimensional (3D) human eye gaze by capturing a single eye image without active illumination is challenging. Although the elliptical iris shape provides a useful cue, existing methods face difficulties in ellipse fitting due to unreliable iris contour detection. These methods may fail frequently especially with low resolution eye images. In this paper, we propose a synthetic iris appearance fitting (SIAF) method that is model-driven to compute 3D gaze direction from iris shape. Instead of fitting an ellipse based on exactly detected iris contour, our method first synthesizes a set of physically possible iris appearances and then optimizes inside this synthetic space to find the best solution to explain the captured eye image. In this way, the solution is highly constrained and guaranteed to be physically feasible. In addition, the proposed advanced image analysis techniques also help the SIAF method be robust to the unreliable iris contour detection. Furthermore, with multiple eye images, we propose a SIAF-joint method that can further reduce the gaze error by half, and it also resolves the binary ambiguity which is inevitable in conventional methods based on simple ellipse fitting. Â© 1999-2012 IEEE.",,,
10.1016/j.chb.2016.04.015,2016,"Stuijfzand B.G., Van Der Schaaf M.F., Kirschner F.C., Ravesloot C.J., Van Der Gijp A., Vincken K.L.",Medical students' cognitive load in volumetric image interpretation: Insights from human-computer interaction and eye movements,"Medical image interpretation is moving from using 2D- to volumetric images, thereby changing the cognitive and perceptual processes involved. This is expected to affect medical students' experienced cognitive load, while learning image interpretation skills. With two studies this explorative research investigated whether measures inherent to image interpretation, i.e. human-computer interaction and eye tracking, relate to cognitive load. Subsequently, it investigated effects of volumetric image interpretation on second-year medical students' cognitive load. Study 1 measured human-computer interactions of participants during two volumetric image interpretation tasks. Using structural equation modelling, the latent variable 'volumetric image information' was identified from the data, which significantly predicted self-reported mental effort as a measure of cognitive load. Study 2 measured participants' eye movements during multiple 2D and volumetric image interpretation tasks. Multilevel analysis showed that time to locate a relevant structure in an image was significantly related to pupil dilation, as a proxy for cognitive load. It is discussed how combining human-computer interaction and eye tracking allows for comprehensive measurement of cognitive load. Combining such measures in a single model would allow for disentangling unique sources of cognitive load, leading to recommendations for implementation of volumetric image interpretation in the medical education curriculum. Â© 2016 Elsevier Ltd. All rights reserved.",,,
10.1109/ICME.2016.7552998,2016,"Ahn S., Kim J., Kim H., Lee S.",Visual attention analysis on stereoscopic images for subjective discomfort evaluation,"By analyzing the statistical behaviors on human visual attention, we discover a clue that the fixation behaviors are highly correlated with how much the viewers feel visual discomfort on stereoscopic images differently from conventional subjective assessments. In order to quantify the correlation between visual attention and discomfort, we explore a novel methodology termed transition of visual attention (ToVA) according to various disparities, which accounts depth attributes of 3D images by eye-tracker experiments. Moreover, the saliency entropy is defined to quantify the distribution of fixations for 3D images. Then, we measure ToVA in terms of the relative saliency entropy using Kullback-Leibler divergence. In order to evaluate the effectiveness of ToVA, a successful example application is also provided, whereby ToVA is applied to obtaining subjective results of measuring discomfort experienced when viewing 3D displays rather than relying on the conventional subjective test by using scoring system. Â© 2016 IEEE.",,,
10.1109/COMPSAC.2016.220,2016,"Li J., Ngai G., Leong H.V., Chan S.C.F.",Your Eye Tells How Well You Comprehend,"Systems that adapt to changes in human needs automatically are useful, built upon advancements in human-computer interaction research. In this paper, we investigate the problem of how well the eye movement of a user when reading an article can predict the level of reading comprehension, which could be exploited in intelligent adaptive e-learning systems. We characterize the eye movement pattern in the form of eye gaze signal. We invite human subjects in reading articles of different difficulty levels being induced to different comprehension levels. Machine-learning techniques are applied to identify useful features to recognize when readers are experiencing difficulties in understanding their reading material. Finally, a detection model that can identify different levels of user comprehension is built. We achieve a performance improvement of over 30% above the baseline, translating over 50% reduction in detection error. Â© 2016 IEEE.",,,
10.1016/j.neucom.2016.03.010,2016,"Wang J., Yu N., Zhu F., Zhuang L.",Multi-level visual tracking with hierarchical tree structural constraint,"Recently, part-based model has drawn much attention in visual tracking for its promising results in handling occlusion and deformation. However how to divide the target into parts and how to model the relationships between parts are still open problems. In this paper, we propose a robust tracker based on multi-level target representation and hierarchical tree structural constraint. The multi-level target representation models the target at three different levels: the bounding box (top) level, the superpixel (middle) level and the keypoint (bottom) level. The relationships between parts at all levels are modeled by the proposed hierarchical tree which includes intra-layer and inter-layer structural constraints. The positions of all the parts are optimized jointly in a unified objective function taking into account both the appearance similarity and the hierarchical tree structural constraint. The appearance model and the hierarchical tree structure are updated online to adapt to the changes of the target in both appearance and structure. Extensive experiments on various challenging video sequences demonstrate that the proposed method outperforms the state-of-the-art trackers significantly. Â© 2016 Elsevier B.V.",,,
10.1016/j.neucom.2016.03.016,2016,"Wang J., Wang Y.",Multi-period visual tracking via online DeepBoost learning,"In this paper, we propose a novel accurate and robust boosting-style tracking-by-detection method. The proposed algorithm adopts a flexible and capacity-conscious object appearance model, which combines the strengths of both local and global visual representations. We firstly propose a joint local-global visual representation, in which main local and global spatial structure information of the target is flexibly embedded in the candidate classifier set with members from multiple complexity families. In addition, to avoid over-fitting our tracker adopts an effective online DeepBoost learning method (ODB). The key capacity-conscious ability of ODB helps to avoid over-fitting and generate a more adaptive and robust tracker. Furthermore, we propose a multi-period tracking framework (MPTF) to enhance the tracker's recovery ability for tracking failures. The proposed Multi-period DeepBoost-Tracker (MPDBT) can well encode the object spatial structures and excellently handle object appearance variations, and it can also recover from tracking failures with the help of the proposed MPTF. The experimental results demonstrate that our tracker outperforms the state-of-the-art trackers. Â© 2016 Elsevier B.V.",,,
10.1109/ICIP.2016.7532937,2016,"Li J., Li S.",Two-phase approach - Calibration and iris contour estimation - For gaze tracking of head-mounted eye camera,"The fitting of an ellipse to the iris contour is usually performed using five unknown parameters. In this study, we divide the continuous gaze estimation of a head-mounted eye camera into two phases. One phase, known as the calibration phase, is used to estimate the eyeball center position in relation to the coordinate system of the head-mounted eye camera. The other phase is used to fit the iris contour in 2D images employing only two parameters for gaze estimation. As seen from the experimental results, the proposed method demonstrates both credible eyeball center estimation and an accurate iris contour estimation in comparison with the conventional five unknown parameter approach. Â© 2016 IEEE.",,,
10.1109/ICIP.2016.7532425,2016,"Vater S., LeÃ³n F.P.",Combining isophote and cascade classifier information for precise pupil localization,"This paper investigates precise pupil center localization in low-resolution images. Being an essential preprocessing step in many applications such as gaze estimation, face alignment as well as human-computer interaction, robust, precise, and efficient methods are necessary. We present a method for accurate eye center localization operating with images from simple off-the-shelf hardware such as webcams. The proposed method utilizes the isophote representation that allows to find pupil center candidates by introducing a novel voting mechanism for pixel weights. To cope with multiple local maxima resulting from the isophote voting map, we combine this information with quasi-continuous responses of a modified cascade classifier framework utilizing appearance-based features. We conduct experiments on the BioID database and show that the presented method outperforms results of existing methods within an error range of the pupil diameter while running at 10 fps on a standard CPU with 3.3 GHz in a Matlab implementation. Â© 2016 IEEE.",,,
10.1109/ICIP.2016.7532442,2016,"Li J., Su L., Wu B., Pang J., Wang C., Wu Z., Huang Q.",Webpage saliency prediction with multi-features fusion,"We proposed a novel model to predict human's visual attention when free-viewing webpages. Compared with natural images, webpages are usually full of salient regions such as logos, text, and faces, while few of them attract human's attention in a short sight. Moreover, webpages perform distinct viewing patterns which are quite different from the natural images. In this paper, we introduced multi-features according to our observation on webpages characters and related eye-tracking data. Further, in order to achieve a flexible adaptation to various types of webpages, we employed a machine-learning framework based on our proposed features. Experimental results demonstrate that our model outperforms other state-of-the-art methods in webpage saliency prediction. Â© 2016 IEEE.",,,
10.1109/ICCPCT.2016.7530336,2016,"Suni S.S., Gopakumar K.",A real time decision support system using head nod and shake,"Human gestures are very prominent means to interface with intelligent systems naturally and nonverbally. This paper presents a robust and real time vision based decision support system that automatically predicts the head nod and shake gestures for decision making. Here we apply the Gentle Adaboost algorithm that detects the faces in the video frames automatically. Real time eye tracking is performed to find the center coordinates of the eyes from the detected face region. In this work, we propose support vector machine classifier, a robust algorithm based on machine learning for predicting head nod and shake gestures. Moreover, this system can function as real time decision support tool in an online environment. The proposed system is implemented and tested for several real time videos. The experimental results show that this system is able to detect head nod and shake gestures with a detection rate of 91.1%. Â© 2016 IEEE.",,,
10.1016/j.cviu.2016.02.016,2016,"Damen D., Leelasawassuk T., Mayol-Cuevas W.","You-Do, I-Learn: Egocentric unsupervised discovery of objects and their modes of interaction towards video-based guidance","This paper presents an unsupervised approach towards automatically extracting video-based guidance on object usage, from egocentric video and wearable gaze tracking, collected from multiple users while performing tasks. The approach (i)Â discovers task relevant objects, (ii) builds a model for each, (iii)Â distinguishes different ways in which each discovered object has been used and (iv)Â discovers the dependencies between object interactions. The work investigates using appearance, position, motion and attention, and presents results using each and a combination of relevant features. Moreover, an online scalable approach is presented and is compared to offline results. The paper proposes a method for selecting a suitable video guide to be displayed to a novice user indicating how to use an object, purely triggered by the user's gaze. The potential assistive mode can also recommend an object to be used next based on the learnt sequence of object interactions. The approach was tested on a variety of daily tasks such as initialising a printer, preparing a coffee and setting up a gym machine. Â© 2016 Elsevier Inc.",,,
10.1016/j.imavis.2016.06.006,2016,"Stefic D., Patras I.",Action recognition using saliency learned from recorded human gaze,"This paper addresses the problem of recognition and localization of actions in image sequences, by utilizing, in the training phase only, gaze tracking data of people watching videos depicting the actions in question. First, we learn discriminative action features at the areas of gaze fixation and train a Convolutional Network that predicts areas of fixation (i.e. salient regions) from raw image data. Second, we propose a Support Vector Machine-based recognition method for joint recognition and localization, in which the bounding box of the action in question is considered as a latent variable. In our formulation the optimization attempts to both minimize the classification cost and maximize the saliency within the bounding box. We show that the results obtained with the optimization where saliency within the bounding box is maximized outperform the results obtained when saliency within the bounding box is not maximized, i.e. when only classification cost is minimized. Furthermore, the results that we obtain outperform the state-of-the-art results on the UCF sports dataset. Â© 2016 Elsevier B.V.",,,
10.1016/j.ijhcs.2016.04.009,2016,"Kyritsis M., Gulliver S.R., Feredoes E.",Environmental factors and features that influence visual search in a 3D WIMP interface,"The challenge of moving past the classic Window Icons Menus Pointer (WIMP) interface, i.e. by turning it '3D', has resulted in much research and development. To evaluate the impact of 3D on the 'finding a target picture in a folder' task, we built a 3D WIMP interface that allowed the systematic manipulation of visual depth, visual aides, semantic category distribution of targets versus non-targets; and the detailed measurement of lower-level stimuli features. Across two separate experiments, one large sample web-based experiment, to understand associations, and one controlled lab environment, using eye tracking to understand user focus, we investigated how visual depth, use of visual aides, use of semantic categories, and lower-level stimuli features (i.e. contrast, colour and luminance) impact how successfully participants are able to search for, and detect, the target image. Moreover in the lab-based experiment, we captured pupillometry measurements to allow consideration of the influence of increasing cognitive load as a result of either an increasing number of items on the screen, or due to the inclusion of visual depth. Our findings showed that increasing the visible layers of depth, and inclusion of converging lines, did not impact target detection times, errors, or failure rates. Low-level features, including colour, luminance, and number of edges, did correlate with differences in target detection times, errors, and failure rates. Our results also revealed that semantic sorting algorithms significantly decreased target detection times. Increased semantic contrasts between a target and its neighbours correlated with an increase in detection errors. Finally, pupillometric data did not provide evidence of any correlation between the number of visible layers of depth and pupil size, however, using structural equation modelling, we demonstrated that cognitive load does influence detection failure rates when there is luminance contrasts between the target and its surrounding neighbours. Results suggest that WIMP interaction designers should consider stimulus-driven factors, which were shown to influence the efficiency with which a target icon can be found in a 3D WIMP interface. Â© 2016 Elsevier Ltd. All rights reserved.",,,
10.1007/s11548-015-1309-8,2016,"Fusaglia M., Hess H., Schwalbe M., Peterhans M., Tinguely P., Weber S., Lu H.",A clinically applicable laser-based image-guided system for laparoscopic liver procedures,"Purpose: Laser range scanners (LRS) allow performing a surface scan without physical contact with the organ, yielding higher registration accuracy for image-guided surgery (IGS) systems. However, the use of LRS-based registration in laparoscopic liver surgery is still limited because current solutions are composed of expensive and bulky equipment which can hardly be integrated in a surgical scenario. Methods: In this work, we present a novel LRS-based IGS system for laparoscopic liver procedures. A triangulation process is formulated to compute the 3D coordinates of laser points by using the existing IGS system tracking devices. This allows the use of a compact and cost-effective LRS and therefore facilitates the integration into the laparoscopic setup. The 3D laser points are then reconstructed into a surface to register to the preoperative liver model using a multi-level registration process. Results: Experimental results show that the proposed system provides submillimeter scanning precision and accuracy comparable to those reported in the literature. Further quantitative analysis shows that the proposed system is able to achieve a patient-to-image registration accuracy, described as target registration error, of 3.2Â±0.57mm. Conclusions: We believe that the presented approach will lead to a faster integration of LRS-based registration techniques in the surgical environment. Further studies will focus on optimizing scanning time and on the respiratory motion compensation. Â© 2015, CARS.",,,
10.1145/2931002.2948726,2016,"Simon D., Sridharan S., Sah S., Ptucha R., Kanan C., Bailey R.",Automatic scanpath generation with deep recurrent neural networks,"Many computer vision algorithms are biologically inspired and designed based on the human visual system. Convolutional neural networks (CNNs) are similarly inspired by the primary visual cortex in the human brain. However, the key difference between current visual models and the human visual system is how the visual information is gathered and processed. We make eye movements to collect information from the environment for navigation and task performance. We also make specific eye movements to important regions in the stimulus to perform the task-at-hand quickly and efficiently. Researchers have used expert scanpaths to train novices for improving the accuracy of visual search tasks. One of the limitations of such a system is that we need an expert to examine each visual stimuli beforehand to generate the scanpaths. In order to extend the idea of gaze guidance to a new unseen stimulus, there is a need for a computational model that can automatically generate expert-like scanpaths. We propose a model for automatic scanpath generation using a convolutional neural network (CNN) and long short-term memory (LSTM) modules. Our model uses LSTMs due to the temporal nature of eye movement data (scanpaths) where the system makes fixation predictions based on previous locations examined. Â© 2016 Copyright held by the owner/author(s).",,,
10.1145/2897824.2925866,2016,"Kellnhofer P., Didyk P., Myszkowski K., Hefeeda M.M., Seidel H.-P., Matusik W.",GazeStereo3D: Seamless disparity manipulations,"Producing a high quality stereoscopic impression on current displays is a challenging task. The content has to be carefully prepared in order to maintain visual comfort, which typically affects the quality of depth reproduction. In this work, we show that this problem can be significantly alleviated when the eye fixation regions can be roughly estimated. We propose a new method for stereoscopic depth adjustment that utilizes eye tracking or other gaze prediction information. The key idea that distinguishes our approach from the previous work is to apply gradual depth adjustments at the eye fixation stage, so that they remain unnoticeable. To this end, we measure the limits imposed on the speed of disparity changes in various depth adjustment scenarios, and formulate a new model that can guide such seamless stereoscopic content processing. Based on this model, we propose a real-time controller that applies local manipulations to stereoscopic content to find the optimum between depth reproduction and visual comfort. We show that the controller is mostly immune to the limitations of low-cost eye tracking solutions. We also demonstrate benefits of our model in off-line applications, such as stereoscopic movie production, where skillful directors can reliably guide and predict viewers' attention or where attended image regions are identified during eye tracking sessions. We validate both our model and the controller in a series of user experiments. They show significant improvements in depth perception without sacrificing the visual quality when our techniques are applied. Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/2897824.2925947,2016,"Wang C., Shi F., Xia S., Chai J.",Realtime 3D eye gaze animation using a single RGB camera,"This paper presents the first realtime 3D eye gaze capture method that simultaneously captures the coordinated movement of 3D eye gaze, head poses and facial expression deformation using a single RGB camera. Our key idea is to complement a realtime 3D facial performance capture system with an efficient 3D eye gaze tracker. We start the process by automatically detecting important 2D facial features for each frame. The detected facial features are then used to reconstruct 3D head poses and large-scale facial deformation using multi-linear expression deformation models. Next, we introduce a novel user-independent classification method for extracting iris and pupil pixels in each frame. We formulate the 3D eye gaze tracker in the Maximum A Posterior (MAP) framework, which sequentially infers the most probable state of 3D eye gaze at each frame. The eye gaze tracker could fail when eye blinking occurs. We further introduce an efficient eye close detector to improve the robustness and accuracy of the eye gaze tracker. We have tested our system on both live video streams and the Internet videos, demonstrating its accuracy and robustness under a variety of uncontrolled lighting conditions and overcoming significant differences of races, genders, shapes, poses and expressions across individuals. Â© 2016 ACM.",,,
10.1109/AICCSA.2015.7507101,2016,"Ben taher F., Ben Amor N., Jallouli M.",An extended Eye Movement Tracker system for an electric wheelchair movement control,"To compensate the incapability of severely disabled person, intelligent wheelchairs are commonly adopted using several non manual command techniques. Eye tracking is one of such techniques. Although it has been the subject of intense research for many years, Eye tracking has not yet reached the level of perfection to be commercially used in an intelligent wheelchair to control electrical powered wheelchairs (EPW). This paper presents an eye tracking system based on fuzzy logic controller to control an EPW with a simple web cam placed in front of the user. Simulation results indicate that the fuzzy logic controller gives better results compared to other techniques. Â© 2015 IEEE.",,,
10.1109/ICMLC.2016.7873017,2016,"Chen M.-H., Wen J., Zhu Y., Xing H.-Y., Wang Y.",Multi-level thresholding for pupil location in eye-gaze tracking systerm,"A new pupil location methodis proposed in eye-gaze tracking system. Firstly, input images are enhanced in order to reduce the influence of illumination. Secondly, multiple candidate thresholds are obtained in terms of the valleys in histogram, and then different segmentation results are available. Further, a fusion method based on the overlaps of segmentation results is proposed which acquired candidate regions and the eye region is obtained according to the entropy information of candidate regions. Thirdly, a simple and effective threshold segmentation method based on eye characteristic is employed and pupil area is obtained. Finally, the center of pupil is acquired by ellipse fitting. Experimental results demonstrated that the proposed methods were effective to improve pupil location accuracy. Â© 2016 IEEE.",,,
10.1109/ICMLC.2016.7873019,2016,"Zou Y.-H., Wen J., Xing H.-Y., Zhu Y.",Rapid eye movement tracking method based on FPGA,"In this study, an eye-tracking system based on FPGA hardware and the center of gravity algorithm is proposed. It captures video via LUPA300 high-speed CMOS and obtains the motion of eye pupil and bright spot through the continuous video frames by using a FPGA implemented the center of gravity algorithm. To obtain a reliable tracking accuracy, a series of binarization and mathematical morphology operations (such as corrosion and expansion) are adopted as a pre-procession step. The results show that FPGA can implement the center of gravity algorithm to track eye motion at high speed, the binarization and corrosion expansion pretreatment can improve the tracking accuracy effectively. Â© 2016 IEEE.",,,
10.1007/s11548-015-1318-7,2016,"Beyl T., Nicolai P., Comparetti M.D., Raczkowsky J., De Momi E., WÃ¶rn H.",Time-of-flight-assisted Kinect camera-based people detection for intuitive human robot cooperation in the surgical operating room,"Background: Scene supervision is a major tool to make medical robots safer and more intuitive. The paper shows an approach to efficiently use 3D cameras within the surgical operating room to enable for safe human robot interaction and action perception. Additionally the presented approach aims to make 3D camera-based scene supervision more reliable and accurate. Methods: A camera system composed of multiple Kinect and time-of-flight cameras has been designed, implemented and calibrated. Calibration and object detection as well as people tracking methods have been designed and evaluated. Results: The camera system shows a good registration accuracy of 0.05Â m. The tracking of humans is reliable and accurate and has been evaluated in an experimental setup using operating clothing. The robot detection shows an error of around 0.04Â m. Conclusions: The robustness and accuracy of the approach allow for an integration into modern operating room. The data output can be used directly for situation and workflow detection as well as collision avoidance. Â© 2015, CARS.",,,
10.1109/QoMEX.2016.7498924,2016,"Vigier T., Baveye Y., Rousseau J., Le Callet P.",Visual attention as a dimension of QoE: Subtitles in UHD videos,"With the ever-growing availability of multimedia content produced, broadcast and consumed worldwide, subtitling is becoming an essential service to quickly share understandable content. Simultaneously, the increased resolution of the ultra high definition (UHD) standard comes with wider screens and new viewing conditions. Services as the display of subtitles thus require adaptation to better fit the new induced viewing visual angle. This paper aims at evaluating quality of experience of subtitled movies in UHD to propose guidelines for the appearance of subtitles. From an eye-tracking experiment conducted on 68 observers and 30 video sequences, viewing behavior and visual saliency are analyzed with and without subtitles and for different subtitle styles. Various metrics based on eye-tracking data, such as the Reading Index for Dynamic Texts (RIDT), are computed to objectively measure the ease of reading and subtitle disturbance. The results mainly show that doubling the visual angle of subtitles from HD to UHD guarantees subtitle readability without compromising the enjoyment of the video content. Â© 2016 IEEE.",,,
10.1109/MELCON.2016.7495417,2016,"Stavroulia K.-E., Ruiz-Harisiou A., Manouchou E., Georgiou K., Sella F., Lanitis A.",A 3D virtual environment for training teachers to identify bullying,"Incidents involving bullying in schools present a difficult challenge for educators who need to identify potentially dangerous behaviors in comparison to 'innocent' types of interstudent interactions. In this paper we investigate the use of a dedicated Virtual Reality application as a means of training educators to identify alarming bullying activities. The Virtual Reality environment simulates a typical middle school, where different bullying-related incidents take place. The virtual environment is visualized using a Virtual Reality headset in conjunction with eye tracking techniques that facilitate user interaction. In order to make realistic animations that resemble human motions, state of the art motion tracking equipment was used for generating realistic avatar movements. According to the scenario of the application, the user witness different types of student behavior and he/she has to take decisions on how to control and deal with the situation. The prototype was tested by active teachers who reported that the incidents within the simulation were similar to real life experiences and suggested that the application could be used in teacher training education providing inexperienced teachers feedback on how to recognize and manage bullying. Â© 2016 IEEE.",,,
10.1145/2932206.2933558,2016,"Akahori W., Morishima S., Hirai T., Kawamura S.",Region-of-interest-based subtitle placement using eye-tracking data of multiple viewers,"We present a subtitle-placement method that reduces viewer's eye movement without interfering with the target region of interest (ROI) in a video scene. Subtitles help viewers understand foreign-language videos. However, subtitles tend to attract viewers' line of sight, which cause viewers to lose focus on the video content. To address this problem, previous studies have attempted to improve viewer experiences by dynamically shifting subtitle positions. Nevertheless, in their user studies, some participants felt that the visual appearance of such subtitles was unnatural and caused them fatigue. We propose a method that places subtitles below the ROI, which is calculated by eye-tracking data from multiple viewers. Two experiments were conducted to evaluate viewer impression and compare line of sight for videos with subtitles placed by the proposed and previous methods. Copyright is held by the owner/author(s).",,,
10.1109/ICRA.2016.7487502,2016,"Tostado P.M., Abbott W.W., Faisal A.A.",3D gaze cursor: Continuous calibration and end-point grasp control of robotic actuators,"Eye movements are closely related to motor actions, and hence can be used to infer motor intentions. Additionally, eye movements are in some cases the only means of communication and interaction with the environment for paralysed and impaired patients with severe motor deficiencies. Despite this, eye-tracking technology still has a very limited use as a human-robot control interface and its applicability is highly restricted to 2D simple tasks that operate on screen based interfaces and do not suffice for natural physical interaction with the environment. We propose that decoding the gaze position in 3D space rather than in 2D results into a much richer spatial cursor signal that allows users to perform everyday tasks such as grasping and moving objects via gaze-based robotic teleoperation. Eye tracking in 3D calibration is usually slow - we demonstrate here that by using a full 3D trajectory for system calibration generated by a robotic arm rather than a simple grid of discrete points, gaze calibration in the 3 dimensions can be successfully achieved in short time and with high accuracy. We perform the non-linear regression from eye-image to 3D-end point using Gaussian Process regressors, which allows us to handle uncertainty in end-point estimates gracefully. Our telerobotic system uses a multi-joint robot arm with a gripper and is integrated with our in-house GT3D binocular eye tracker. This prototype system has been evaluated and assessed in a test environment with 7 users, yielding gaze-estimation errors of less than 1cm in the horizontal, vertical and depth dimensions, and less than 2cm in the overall 3D Euclidean space. Users reported intuitive, low-cognitive load, control of the system right from their first trial and were straightaway able to simply look at an object and command through a wink to grasp this object with the robot gripper. Â© 2016 IEEE.",,,
10.1109/ACPR.2015.7486590,2016,"Park H., Kim D.",Gaze classification on a mobile device by using deep belief networks,"In this paper, we introduce a gaze classification method which classifies the locations of human gaze on a display of a mobile device. For example, when the user see the upper part of a display, our method classifies the gaze as the upper part among the available choices: upper, middle, and lower parts of the display. Our method uses appearance-based gaze estimation and gray-scale images captured from a camera of a mobile device. This method does not require any personal calibration. We train gaze classifiers by using Deep Belief Networks with considering head poses in general environments for a mobile device. The gaze classification method is applied to human-computer interaction and various application programs. Â© 2015 IEEE.",,,
10.1007/s11548-016-1393-4,2016,"Du X., Allan M., Dore A., Ourselin S., Hawkes D., Kelly J.D., Stoyanov D.",Combined 2D and 3D tracking of surgical instruments for minimally invasive and robotic-assisted surgery,"Purpose: Computer-assisted interventions for enhanced minimally invasive surgery (MIS) require tracking of the surgical instruments. Instrument tracking is a challenging problem in both conventional and robotic-assisted MIS, but vision-based approaches are a promising solution with minimal hardware integration requirements. However, vision-based methods suffer from drift, and in the case of occlusions, shadows and fast motion, they can be subject to complete tracking failure. Methods: In this paper, we develop a 2D tracker based on a Generalized Hough Transform using SIFT features which can both handle complex environmental changes and recover from tracking failure. We use this to initialize a 3D tracker at each frame which enables us to recover 3D instrument pose over long sequences and even during occlusions. Results: We quantitatively validate our method in 2D and 3D with ex vivo data collected from a DVRK controller as well as providing qualitative validation on robotic-assisted in vivo data. Conclusions: We demonstrate from our extended sequences that our method provides drift-free robust and accurate tracking. Our occlusion-based sequences additionally demonstrate that our method can recover from occlusion-based failure. In both cases, we show an improvement over using 3D tracking alone suggesting that combining 2D and 3D tracking is a promising solution to challenges in surgical instrument tracking. Â© 2016, The Author(s).",,,
10.1007/s11263-015-0863-4,2016,"Funes-Mora K.A., Odobez J.-M.",Gaze Estimation in the 3D Space Using RGB-D Sensors: Towards Head-Pose and User Invariance,"We address the problem of 3D gaze estimation within a 3D environment from remote sensors, which is highly valuable for applications in humanâ€“human and humanâ€“robot interactions. To the contrary of most previous works, which are limited to screen gazing applications, we propose to leverage the depth data of RGB-D cameras to perform an accurate head pose tracking, acquire head pose invariance through a 3D rectification process that renders head pose dependent eye images into a canonical viewpoint, and computes the line-of-sight in the 3D space. To address the low resolution issue of the eye image resulting from the use of remote sensors, we rely on the appearance based gaze estimation paradigm, which has demonstrated robustness against this factor. In this context, we do a comparative study of recent appearance based strategies within our framework, study the generalization of these methods to unseen individual, and propose a cross-user eye image alignment technique relying on the direct registration of gaze-synchronized eye images. We demonstrate the validity of our approach through extensive gaze estimation experiments on a public dataset as well as a gaze coding task applied to natural job interviews. Â© 2015, Springer Science+Business Media New York.",,,
10.1109/THMS.2015.2477507,2016,"Li J., Li S.",Gaze Estimation From Color Image Based on the Eye Model With Known Head Pose,"This paper proposes a novel method of gaze estimation based on an eye model with known head pose. The most crucial factors in the eye-model-based approach to gaze estimation are the 3-D positions of the eyeball and iris centers. In the proposed method, an RGB-D camera, Kinect sensor, is used to obtain the head pose as well as the eye region of the color image. The 3-D position of the eyeball center is determined in the calibration phase by gazing at the center of the color image camera. Then, to estimate the 3-D position of the iris center, the 3-D contour of the iris is projected onto the color image with the known head pose obtained from color and depth cues of an RGB-D camera. Thus, the ellipse of the iris in the image can be described using only two parameters: the yaw and pitch angles of the eyeball in the iris coordinate system, rather than the conventional five parameters of an ellipse. The proposed method can fit an iris that is not complete due to eyelid occlusion. The average errors of vertical and horizontal angles of the gaze estimation for seven subjects are 5.9Â° and 4.4Â°, respectively. The processing speed is as high as 330 ms per frame. However, for lower resolution and poor illumination images, as tested on the public database EYEDIAP, the performance of the proposed eye-model-based method is inferior to that of the-state-of-the-art appearance-based method. Â© 2015 IEEE.",,,
10.1145/2912124,2016,"Boi P., Fenu G., Spano L.D., Vargiu V.",Reconstructing user's attention on the web through mouse movements and perception-based content identification,"Eye tracking is one of the most exploited techniques in literature for finding usability problems in web-based user interfaces (UIs). However, it is usually employed in a laboratory setting, considering that an eye-tracker is not commonly used in web browsing. In contrast, web application providers usually exploit remote techniques for large-scale user studies (e.g. A/B testing), tracking low-level interactions such as mouse clicks and movements. In this article, we discuss a method for predicting whether the user is looking at the content pointed by the cursor, exploiting the mouse movement data and a segmentation of the contents in a web page. We propose an automatic method for segmenting content groups inside a web page that, applying both image and code analysis techniques, identifies the user-perceived group of contents with a mean pixel-based error around the 20%. In addition, we show through a user study that such segmentation information enhances the precision and the accuracy in predicting the correlation between between the user's gaze and the mouse position at the content level, without relaying on user-specific features. Â© 2016 ACM.",,,
10.1016/j.neucom.2016.01.052,2016,"Yi S., Jiang N., Wang X., Liu W.",Individual adaptive metric learning for visual tracking,"Recent attempts demonstrate that learning an appropriate distance metric in visual tracking applications can improve the tracking performance. However, the existing metric learning methods learn and adjust the distance between all pairwise sample points in an iterative way, which raises the time consumption issue in real-time tracking applications. To address this problem, this paper proposes a novel metric learning method and applies it to visual tracking. The main idea of the proposed method is to adapt the distance from each individual sample point to a few anchor points instead of the distance between all pairs of samples, so as to reduce the number of distances to be adjusted. Based on this idea, we construct a convex matrix function that collapses the sample points to their class centers and maximizes the inter-class distance. Given n training samples in d-dimensional space, the equation can be solved in a closed form with the computational complexity of O(d2n). This is much more computationally efficient than traditional methods with the computational complexity of O(dn2) in each iteration (normally in tracking applications, dâ‰ªn). Furthermore, the proposed method can be learned in an online manner which is able to accelerate the learning process and improve the matching accuracy in visual tracking applications. Experiments on UCI datasets demonstrate that the proposed learning method is comparable with the traditional metric learning methods in term of classification accuracy but much more time efficient. The comparison experiments on benchmark video sequences show that the tracking algorithm based on our learning approach can outperform the state-of-the-art tracking algorithms. Â© 2016 Elsevier B.V.",,,
10.1109/WACV.2016.7477553,2016,"Baltrusaitis T., Robinson P., Morency L.-P.",OpenFace: An open source facial behavior analysis toolkit,"Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system. Â© 2016 IEEE.",,,
10.1145/2858036.2858320,2016,"Mauderer M., Flatla D.R., Nacenta M.A.",Gaze-contingent manipulation of color perception,"Using real time eye tracking, gaze-contingent displays can modify their content to represent depth (e.g., through additional depth cues) or to increase rendering performance (e.g., by omitting peripheral detail). However, there has been no research to date exploring how gaze-contingent displays can be leveraged for manipulating perceived color. To address this, we conducted two experiments (color matching and sorting) that manipulated peripheral background and object colors to influence the user's color perception. Findings from our color matching experiment suggest that we can use gaze-contingent simultaneous contrast to affect color appearance and that existing color appearance models might not fully predict perceived colors with gaze-contingent presentation. Through our color sorting experiment we demonstrate how gaze-contingent adjustments can be used to enhance color discrimination. Gazecontingent color holds the promise of expanding the perceived color gamut of existing display technology and enabling people to discriminate color with greater precision.",,,
10.1145/2858036.2858078,2016,"Kulshreshth A., La Viola J.J., Jr.",Dynamic stereoscopic 3D parameter adjustments for enhanced depth discrimination,"Most modern stereoscopic 3D applications use fixed stereoscopic 3D parameters (separation and convergence) to render the scene on a 3D display. But, keeping these parameters fixed during usage does not always provide the best experience since it can reduce the amount of depth perception possible in some applications which have large variability in object distances. We developed two stereoscopic rendering techniques which actively vary the stereo parameters based on the scene content. Our first algorithm calculates a low resolution depth map of the scene and chooses ideal stereo parameters based on that depth map. Our second algorithm uses eye tracking data to get the gaze direction of the user and chooses ideal stereo parameters based on the distance of the gazed object. We evaluated our techniques in an experiment that uses three depth judgment tasks: depth ranking, relative depth judgment and path tracing. Our results indicate that variable stereo parameters provide enhanced depth discrimination compared to static parameters and were preferred by our participants over the traditional fixed parameter approach. We discuss our findings and possible implications on the design of future stereoscopic 3D applications. Â© 2016 ACM.",,,
10.1145/2858036.2858578,2016,"Templier T., Bektas K., Hahnloser R.H.R.",Eye-trace: Segmentation of volumetric microscopy images with eyegaze,"We introduce an image annotation approach for the analysis of volumetric electron microscopic imagery of brain tissue. The core task is to identify and link tubular objects (neuronal fibers) in images taken from consecutive ultrathin sections of brain tissue. In our approach an individual 'flies' through the 3D data at a high speed and maintains eye gaze focus on a single neuronal fiber, aided by navigation with a handheld gamepad controller. The continuous foveation on a fiber of interest constitutes an intuitive means to define a trace that is seamlessly recorded with a desktop eyetracker and transformed into precise 3D coordinates of the annotated fiber (skeleton tracing). In a participant experiment we validate the approach by demonstrating a tracing accuracy of about the respective radiuses of the traced fibers with browsing speeds of up to 40 brain sections per second. Â© 2016 ACM.",,,
10.1145/2912886,2016,"Bulling A., Kunze K.",Eyewear computers for human-computer interaction,"Head-worn displays and eye trackers, augmented and virtual reality glasses, egocentric cameras, and other smart eyewear have recently emerged as a research platform in fields such as ubiquitous computing, computer vision, and cognitive and social science. Recent technological advances have made eyewear unobtrusive and lightweight, and more suitable for daily use. Vision-based eye tracking relies on special-purpose eye cameras along with infrared illumination, and provides gaze direction in either 2D-scene-camera or 3D-world coordinates.",,,
10.1145/2883851.2883927,2016,"Prieto L.P., Sharma K., Dillenbourg P., JesÃºs M.",Teaching analytics: Towards automatic extraction of orchestration graphs using wearable sensors,"Teaching analytics' is the application of learning analytics techniques to understand teaching and learning processes, and eventually enable supportive interventions. However, in the case of (often, half-improvised) teaching in face-to-face classrooms, such interventions would require first an understanding of what the teacher actually did, as the starting point for teacher reection and inquiry. Currently, such teacher enactment characterization requires costly manual coding by researchers. This paper presents a case study exploring the potential of machine learning techniques to automatically extract teaching actions during classroom enactment, from five data sources collected using wearable sensors (eye-tracking, EEG, accelerometer, audio and video). Our results highlight the feasibility of this approach, with high levels of accuracy in determining the social plane of interaction (90%, k=0.8). The reliable detection of concrete teaching activity (e.g., explanation vs. questioning) accurately still remains challenging (67%, k=0.56), a fact that will prompt further research on multimodal features and models for teaching activity extraction, as well as the collection of a larger multimodal dataset to improve the accuracy and generalizability of these methods. Â© 2016 ACM.",,,
10.1109/IWW-BCI.2016.7457461,2016,"Chun J., Bae B., Jo S.",BCI based hybrid interface for 3D object control in virtual reality,"People attempts to apply the virtual reality (VR) technology in various fields recently, however, there are many limitations to apply the VR technology in existing interfaces in various fields such as 3D object control. To solve this problem, we propose a combination of eye-tracking and BCI technique to control 3D objects in a three-dimensional VR as an alternative interface. In our proposed interface, users select a virtual 3D object in VR by eye-gazing which is detect by the eye-tracking module of the system and manipulate the object by concentrating their mind via the BCI module. To evaluate the performance of our system, subjects perform the same experiments using the proposed system comparing to other existing interfaces. The result shows that the proposed interface has similar or better performance than other interfaces. This result suggests that our proposed interface can be used as an alternative interface of VR. Â© 2016 IEEE.",,,
10.1109/TMECH.2015.2470522,2016,"Adiba A.I., Tanaka N., Miyake J.",An adjustable gaze tracking system and its application for automatic discrimination of interest objects,"Gaze tracking (GT) systems have attracted the attention of researchers as well as the commercial sector. Commercial systems are readily available, but they are usually fabricated at the same size. In this study, we propose a three-dimensional (3-D)-printable frame and an open-source system to fabricate a wearable GT system with low-cost configuration and reasonable performance. A 3-D printer achieves repeatable and adjustable design. The estimated price to make the GT hardware is less than â‚?00, and the system's average accuracy is 2.58Â°. Our GT system has a 24-Hz sampling rate, which can analyze human interest points. Another contribution of this study is developing the automatic discrimination of interest objects using our proposed GT and machine learning. The output of this combination system is a database that consists of labeled objects. The label with the highest frequency indicates the object of the highest interest for a corresponding user. Our combination system has a 7-Hz sampling rate and a 3.8% classification error. Â© 1996-2012 IEEE.",,,
10.1016/j.neucom.2015.04.127,2016,"Berger M., De Souza A.F., Neto J.D.O., de Aguiar E., Oliveira-Santos T.",Visual tracking with VG-RAM Weightless Neural Networks,"We present a biologically inspired long-term object tracking system based on Virtual Generalizing Random Access Memory (VG-RAM) Weightless Neural Networks (WNN). VG-RAM WNN is an effective machine learning technique that offers simple implementation and fast training. Our system models the biological saccadic eye movement, the transformation suffered by the images captured by the eyes from the retina to the Superior Colliculus (SC), and the response of SC neurons to previously seen patterns. We evaluated the performance of our system using a well-known visual tracking database. Our experimental results show that our approach is capable of reliably and efficiently track an object of interest in a video with accuracy equivalent or superior to related work. Â© 2016 Elsevier B.V.",,,
10.1016/j.neucom.2015.07.125,2016,"Lu F., Chen X.",Person-independent eye gaze prediction from eye images using patch-based features,"This paper delivers a preliminary attempt towards person-independent appearance-based gaze estimation. Conventional methods need to assume training and test data collected from the same person, otherwise eye shape difference due to individuality will affect the estimation severely. To solve this problem, the key idea in this paper is to extract from eye images more advanced eye features, which helps learn a person-independent relationship between eye gaze change and eye appearance variation. To this end, we propose employing the advantages of recent sparse auto-encoding techniques. We partition any eye image into small patches which can overlap with each other. With patches from many images, we learn a codebook comprising a set of bases, which can reconstruct any eye image patch with sparse coefficients. By examining these coefficients, we can analyze the eye shape more effectively. Finally, we produce the eye features by pooling the coefficients at different scales, and then combine these subfeatures from different codebooks. Experimental results show that the proposed method achieves good accuracy on a public dataset and it also outperforms conventional methods by a large margin. Â© 2015 Elsevier B.V.",,,
10.1145/2857491.2857530,2016,"Mansouryar M., Steil J., Sugano Y., Bulling A.",3D gaze estimation from 2D pupil positions on monocular head-mounted eye trackers,"3D gaze information is important for scene-centric attention analysis, but accurate estimation and analysis of 3D gaze in real-world environments remains challenging. We present a novel 3D gaze estimation method for monocular head-mounted eye trackers. In contrast to previous work, our method does not aim to infer 3D eyeball poses, but directly maps 2D pupil positions to 3D gaze directions in scene camera coordinate space. We first provide a detailed discussion of the 3D gaze estimation task and summarize different methods, including our own. We then evaluate the performance of different 3D gaze estimation approaches using both simulated and real data. Through experimental validation, we demonstrate the effectiveness of our method in reducing parallax error, and we identify research challenges for the design of 3D calibration procedures. Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/2857491.2857515,2016,"Wang K., Wang S., Ji Q.",Deep eye fixation map learning for calibration-free eye gaze tracking,"The existing eye trackers typically require an explicit personal calibration procedure to estimate subject-dependent eye parameters. Despite efforts in simplifying the calibration process, such a calibration process remains unnatural and bothersome, in particular for users of personal and mobile devices. To alleviate this problem, we introduce a technique that can eliminate explicit personal calibration. Based on combining a new calibration procedure with the eye fixation prediction, the proposed method performs implicit personal calibration without active participation or even knowledge of the user. Specifically, different from traditional deterministic calibration procedure that minimizes the differences between the predicted eye gazes and the actual eye gazes, we introduce a stochastic calibration procedure that minimizes the differences between the probability distribution of the predicted eye gaze and the distribution of the actual eye gaze. Furthermore, instead of using saliency map to approximate eye fixation distribution, we propose to use a regression based deep convolutional neural network (RCNN) that specifically learns image features to predict eye fixation. By combining the distribution based calibration with the deep fixation prediction procedure, personal eye parameters can be estimated without explicit user collaboration. We apply the proposed method to both 2D regression-based and 3D model-based eye gaze tracking methods. Experimental results show that the proposed method outperforms other implicit calibration methods and achieve comparable results to those that use traditional explicit calibration methods. Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/2857491.2857508,2016,"Thomaz C.E., Amaral V., Gillies D.F., Rueckert D.",Priori-driven dimensions of face-space: Experiments incorporating eye-tracking information,"Face-space has become established as an effective model for representing the dimensions of variation that occur in collections of human faces. For example, a change of expression from neutral to smiling can be represented by one axis in a face space. Principal components can be used to determine the axes of a face-space, however, standard principal components are based entirely on the data set from which they are computed, and do not express any domain specific information about the application of interest. In this paper, we propose a face-space analysis that combines the variance criterion used in principal components with some prior knowledge about the task-driven experiment. The priors are based on measuring eye movements of participants to frontal 2D faces during separate gender and facial expression categorization tasks. Our findings show that saccades to faces are task-driven, especially from 500 to 1000 milliseconds, and automatic recognition performance does not improve with additional exposure time. Â© 2016 Copyright held by the owner/author(s).",,,
10.1145/2857491.2884059,2016,"Sakai D., Yamamoto M., Nagamatsu T., Fukumori S.",Enter your PIN code securely! - Utilization of personal difference of angle kappa,"In previous studies, the angle kappa, the offset between the optical axis and visual axis, was considered to be calibrated when eye trackers based on a 3D model were used. However, we found that the angle kappa could be used as personal information, which is immeasurable from outside the human body. This paper proposes a concept for PIN entry by considering the characteristics of the angle kappa. Thus, we measured the distribution of the angle kappa and developed a prototype of the system. We demonstrated the effectiveness of the method. Â© 2016 Copyright held by the owner/author(s).",,,
10.1145/2857491.2857496,2016,"Borsato F.H., Morimoto C.H.",Episcleral surface tracking: Challenges and possibilities for using mice sensors for wearable eye tracking,"Video-based eye trackers (VETs) have become the dominant eye tracking technology due to its reasonable cost, accuracy, and easy of use. VETs require real-time image processing to detect and track eye features such as the center of the pupil and corneal reflection to estimate the point of regard. Despite the continuous evolution of cameras and computers that made head mounted eye trackers easier to use in natural activities, real-time processing of high resolution images in mobile devices remains a challenge. In this paper we investigate the feasibility of a novel eye-tracking technique intended for wearable applications that use mice chips as imaging sensors. Such devices are widely available at very low cost, and provide high speed and accurate 2D tracking data. Though mice chips have been used for many purposes other than a computer's pointing device, to our knowledge this is the first attempt to use it as an eye tracker. To validate the technique, we built an episcleral database with about 100 high resolution episcleral patches from 7 individuals. The episclera is the outer most layer of the sclera, which is the white part of the eye, and consists of dense vascular connective tissue. We have used the patches to determine if the episclera contains enough texture to be reliably tracked. We also present results from a prototype built using an off-the-shelf mouse sensor. Our results show that a mouse-based eye tracker has the potential to be very accurate, precise, and fast (measuring 2.1' of visual angle at 1 KHz speed), with little overhead for the wearable computer. Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/2857491.2857492,2016,"Wood E., BaltruÅ¡aitis T., Morency L.-P., Robinson P., Bulling A.",Learning an appearance-based gaze estimator from one million synthesised images,"Learning-based methods for appearance-based gaze estimation achieve state-of-the-art performance in challenging real-world settings but require large amounts of labelled training data. Learningby-synthesis was proposed as a promising solution to this problem but current methods are limited with respect to speed, appearance variability, and the head pose and gaze angle distribution they can synthesize. We present UnityEyes, a novel method to rapidly synthesize large amounts of variable eye region images as training data. Our method combines a novel generative 3D model of the human eye region with a real-time rendering framework. The model is based on high-resolution 3D face scans and uses real-time approximations for complex eyeball materials and structures as well as anatomically inspired procedural geometry methods for eyelid animation. We show that these synthesized images can be used to estimate gaze in difficult in-the-wild scenarios, even for extreme gaze angles or in cases in which the pupil is fully occluded. We also demonstrate competitive gaze estimation results on a benchmark in-the-wild dataset, despite only using a light-weight nearestneighbor algorithm. We are making our UnityEyes synthesis framework available online for the benefit of the research community. Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/2857491.2857532,2016,"Pfeiffer T., Renner P., Pfeiffer-LeÃŸmann N.",EyeSee3D 2.0: Model-based real-time analysis of mobile eye-tracking in static and dynamic three-dimensional scenes,"With the launch of ultra-portable systems, mobile eye tracking finally has the potential to become mainstream. While eye movements on their own can already be used to identify human activities, such as reading or walking, linking eye movements to objects in the environment provides even deeper insights into human cognitive processing. We present a model-based approach for the identification of fixated objects in three-dimensional environments. For evaluation, we compare the automatic labelling of fixations with those performed by human annotators. In addition to that, we show how the approach can be extended to support moving targets, such as individual limbs or faces of human interaction partners. The approach also scales to studies using multiple mobile eye-tracking systems in parallel. The developed system supports real-time attentive systems that make use of eye tracking as means for indirect or direct humancomputer interaction as well as off-line analysis for basic research purposes and usability studies. Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/2857491.2857545,2016,"Pieszala J., Diaz G., Pelz J., Speir J., Bailey R.",3D Gaze Point Localization and Visualization Using LiDAR-based 3D reconstructions,"We present a novel pipeline for localizing a free roaming eye tracker within a LiDAR-based 3D reconstructed scene with high levels of accuracy. By utilizing a combination of reconstruction algorithms that leverage the strengths of global versus local capture methods and user-assisted refinement, we reduce drift errors associated with Dense-SLAM techniques. Our framework supports regionof-interest (ROI) annotation and gaze statistics generation and the ability to visualize gaze in 3D from an immersive first person or third person perspective. This approach gives unique insights into viewers' problem solving and search task strategies and has high applicability in complex static environments such as crime scenes. Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/2857491.2857540,2016,"Kothari R., Binaee K., Matthis J.S., Bailey R., Diaz G.J.",Novel apparatus for investigation of eye movements when walking in the presence of 3D projected obstacles,"This manuscript details a novel approach for the study of gaze behavior in a controlled naturalistic walking environment. In a single case study, a motion-tracked participant walked along path obstructed by augmented reality obstacles. Despite measurable effects of obstacle size on the kinematics of walking behavior, the subject's gaze behavior remained unchanged. These findings suggest that gaze behavior is incredibly robust to competing intentional demands in the one subject tested. More importantly, this study validates a novel apparatus as suitable for recording both the gaze direction and gait kinematics of a human walker. Future plans will further explore the intricacies of human behavior through alternative experimental designs. For example, increasing the complexity of either the ground plane (e.g. with additional obstacles) or the distractor task, perhaps can exaggerate the consequences of gaze shifts to and from competing tasks on walking behavior. Similarly, one might emphasize the role of vision in walking by manipulating the visibility of obstacles based upon the gaze location during locomotion, perhaps degrading the observer's ability to use information in the fovea during locomotion. One can similarly manipulate parameters of the ground plane based upon the time evolving kinematics of walking behavior. Â© 2016 Copyright held by the owner/author(s).",,,
10.1145/2857491.2888591,2016,"Wang K., Ji Q.",Hybrid model and appearance based eye tracking with Kinect,"Existing gaze estimation methods rely mainly on 3D eye model or 2D eye appearance. While both methods have validated their effectiveness in various fields and applications, they are still limited in practice, such as portable and non-intrusive system and robust eye gaze tracking in different environments. To this end, we investigate on combining eye model with eye appearance to perform gaze estimation and eye gaze tracking. Specifically, unlike traditional 3D model based methods which rely on cornea reflections, we plan to retrieve 3D information from depth sensor (Eg, Kinect). Kinect integrates camera sensor and IR illuminations into one single device, thus enable more flexible system settings. We further propose to utilize appearance information to help the basic model based methods. Appearance information can help better detection of gaze related features (Eg, pupil center). Plus, eye model and eye appearance can benefit each other to enable robust and accurate gaze estimation. Â© 2016 Copyright held by the owner/author(s).",,,
10.1145/2857491.2884060,2016,"TopiÄ‡ G., Yamaya A., Aizawa A., MartÃ­nez-GÃ³mez P.",FixFix: Fixing the fixations,"FixFix is a web-based tool for editing reading gaze fixation datasets. The purpose is to provide gaze researchers focusing on reading an easy-to-use interface that will facilitate manual interpretation, but even more so to create gold standard datasets for machine learning and data mining. It allows the users to identify fixations, then move them either singly or in groups, in order to correct both variable and systematic gaze sampling errors. Â© 2016 Copyright held by the owner/author(s).",,,
10.1145/2857491.2857541,2016,"Pfeiffer T., Memili C.",Model-based real-time visualization of realistic three-dimensional heat maps for mobile eye tracking and eye tracking in virtual reality,"Heat maps, or more generally, attention maps or saliency maps are an often used technique to visualize eye-tracking data. With heat maps qualitative information about visual processing can be easily visualized and communicated between experts and laymen. They are thus a versatile tool for many disciplines, in particular for usability engineering, and are often used to get a first overview about recorded eye-tracking data. Today, heat maps are typically generated for 2D stimuli that have been presented on a computer display. In such cases the mapping of overt visual attention on the stimulus is rather straight forward and the process is well understood. However, when turning towards mobile eye tracking and eye tracking in 3D virtual environments, the case is much more complicated. In the first part of the paper, we discuss several challenges that have to be considered in 3D environments, such as changing perspectives, multiple viewers, object occlusions, depth of fixations, or dynamically moving objects. In the second part, we present an approach for the generation of 3D heat maps addressing the above mentioned issues while working in real-time. Our visualizations provide high-quality output for multi-perspective eye-tracking recordings of visual attention in 3D environments. Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/2857491.2857538,2016,"Sanandaji A., Grimm C., West R., Parola M., Kajihara M., Deutsch J., Carlew A., Yates D.",Where do experts look while doing 3D image segmentation,"3D image segmentation is a fundamental process in many scientific and medical applications. Automatic algorithms do exist, but there are many use cases where these algorithms fail. The gold standard is still manual segmentation or review. Unfortunately, even for an expert this is laborious, time consuming, and prone to errors. Existing 3D segmentation tools do not currently take into account human mental models and low-level perception tasks. Our goal is to improve the quality and efficiency of manual segmentation and review by analyzing how experts perform segmentation. As a pre-Uminary step we conducted a field study with 8 segmentation experts, recording video and eye tracking data. We developed a novel coding scheme to analyze this data and verified that it successfully covers and quantifies the low-level actions, tasks and behaviors of experts during 3D image segmentation. Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1145/2857491.2857494,2016,"KÃ¼bler T.C., Rittig T., Kasneci E., Ungewiss J., Krauss C.",Rendering refraction and reflection of eyeglasses for synthetic eye tracker images,"While for the evaluation of robustness of eye tracking algorithms the use of real-world data is essential, there are many applications where simulated, synthetic eye images are of advantage. They can generate labelled ground-truth data for appearance based gaze estimation algorithms or enable the development of model based gaze estimation techniques by showing the influence on gaze estimation error of different model factors that can then be simplified or extended. We extend the generation of synthetic eye images by a simulation of refraction and reflection for eyeglasses. On the one hand this allows for the testing of pupil and glint detection algorithms under different illumination and reflection conditions, on the other hand the error of gaze estimation routines can be estimated in conjunction with different eyeglasses. We show how a polynomial function fitting calibration performs equally well with and without eyeglasses, and how a geometrical eye model behaves when exposed to glasses. Â© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,,
10.1109/ICCE.2016.7430728,2016,"Kang S.-J., Jeong Y.-W., Yun J.-J., Bae S.",Real-time eye tracking techni que for multiview 3D systems,"This paper presents the real-time multi-user eye tracking technique for multiview 3D systems. The proposed technique used the Haar feature-based face detection and classification. Then, it calculated the best matching template, and extracts eye positions based on biological proportion. Simulation results showed the proposed method enhanced the average F1 score up to 0.312, compared with conventional methods. Â© 2016 IEEE.",,,
10.3389/fninf.2016.00007,2016,"Bigdely-Shamlo N., Makeig S., Robbins K.A.",Preparing laboratory and real-world EEG data for large-scale analysis: A containerized approach,"Large-scale analysis of EEG and other physiological measures promises new insights into brain processes and more accurate and robust brainâ€“computer interface models. However, the absence of standardized vocabularies for annotating events in a machine understandable manner, the welter of collection-specific data organizations, the difficulty in moving data across processing platforms, and the unavailability of agreed-upon standards for preprocessing have prevented large-scale analyses of EEG. Here we describe a â€œcontainerizedâ€?approach and freely available tools we have developed to facilitate the process of annotating, packaging, and preprocessing EEG data collections to enable data sharing, archiving, large-scale machine learning/data mining and (meta-)analysis. The EEG Study Schema (ESS) comprises three data â€œLevels,â€?each with its own XML-document schema and file/folder convention, plus a standardized (PREP) pipeline to move raw (Data Level 1) data to a basic preprocessed state (Data Level 2) suitable for application of a large class of EEG analysis methods. Researchers can ship a study as a single unit and operate on its data using a standardized interface. ESS does not require a central database and provides all the metadata data necessary to execute a wide variety of EEG processing pipelines. The primary focus of ESS is automated in-depth analysis and meta-analysis EEG studies. However, ESS can also encapsulate meta-information for the other modalities such as eye tracking, that are increasingly used in both laboratory and real-world neuroimaging. ESS schema and tools are freely available at www.eegstudy.org and a central catalog of over 850 GB of existing data in ESS format is available at studycatalog.org. These tools and resources are part of a larger effort to enable data sharing at sufficient scale for researchers to engagee in truly large-scale EEG analysis and data mining (BigEEG.org). Â© 2016 Bigdely-Shamlo, Makeig and Robbins.",,,
10.1007/s11257-015-9167-1,2016,"Bixler R., Dâ€™Mello S.",Automatic gaze-based user-independent detection of mind wandering during computerized reading,"Mind wandering is a ubiquitous phenomenon where attention involuntarily shifts from task-related thoughts to internal task-unrelated thoughts. Mind wandering can have negative effects on performance; hence, intelligent interfaces that detect mind wandering can improve performance by intervening and restoring attention to the current task. We investigated the use of eye gaze and contextual cues to automatically detect mind wandering during reading with a computer interface. Participants were pseudorandomly probed to report mind wandering while an eye tracker recorded their gaze during the reading task. Supervised machine learning techniques detected positive responses to mind wandering probes from eye gaze and context features in a user-independent fashion. Mind wandering was detected with an accuracy of 72Â % (expected accuracy by chance was 60Â %) when probed at the end of a page and an accuracy of 67Â % (chance was 59Â %) when probed in the midst of reading a page. Global gaze features (gaze patterns independent of content, such as fixation durations) were more effective than content-specific local gaze features. An analysis of the features revealed diagnostic patterns of eye gaze behavior during mind wandering: (1) certain types of fixations were longer; (2) reading times were longer than expected; (3) more words were skipped; and (4) there was a larger variability in pupil diameter. Finally, the automatically detected mind wandering rate correlated negatively with measures of learning and transfer even after controlling for prior knowledge, thereby providing evidence of predictive validity. Possible improvements to the detector and applications that utilize the detector are discussed. Â© 2015, Springer Science+Business Media Dordrecht.",,,
10.1109/APSIPA.2015.7415350,2016,"Zhang Y., Zheng X., Hong W., Mou X.",A comparison study of stationary and mobile eye tracking on EXITs design in a wayfinding system,"""Wayfinding system"" is an interdisciplinary research between the environment design and human interface of information system. The design of EXIT is to efficiently help people to identify the emergency exit when the accident occurred. In this paper, eye tracking techniques are utilized to aided EXIT design in the wayfinding system. The mobile eye tracking was used to collect the human eye movement data in the building where different EXIT designs were displayed. While the stationary eye tracking techniques was used to collect the eye movement data on the same building's EXIT designs on virtual 3D sketches to get the quantitative data comparing with the mobile eye tracker. Finally, some general conclusions were obtained from the views of visual elements selecting, EXIT appearance design and EXIT's placement in the building, which is very valuable and can be commonly referred in wayfinding system. The research methods in the paper is very original and has not been mentioned in other papers of the related fields before, which is well worthy data and empirical methodologies can be introduced in wayfinding and space decision making research. Â© 2015 Asia-Pacific Signal and Information Processing Association.",,,
10.1109/SII.2015.7404959,2016,"El Hafi L., Takemura K., Takamatsu J., Ogasawara T.",Model-based approach for gaze estimation from corneal imaging using a single camera,"This paper describes a method to estimate the gaze direction using cornea images captured by a single camera. The purpose is to develop wearable devices capable of obtaining natural user responses, such as interests and behaviors, from eye movements and scene images reflected on the cornea. From an image of the eye, an ellipse is fitted on the colored iris area. A 3D eye model is reconstructed from the ellipse and rotated to simulate projections of the iris area for different eye poses. The gaze direction is then evaluated by matching the iris area of the current image with the corresponding projection obtained from the model. We finally conducted an experiment using a head-mounted prototype to demonstrate the potential of such an eye-tracking method solely based on cornea images captured from a single camera. Â© 2015 IEEE.",,,
10.1007/978-3-319-29971-6_18,2016,"De Beugher S., BrÃ´ne G., GoedemÃ© T.",Semi-automatic hand annotation of egocentric recordings,"We present a fast and accurate algorithm for the detection of human hands in real-life 2D image sequences. We focus on a specific application of hand detection, viz. the annotation of egocentric recordings. A well known type of egocentric camera is the mobile eye-tracker, which is often used in research on human-human interaction. Nowadays, this type of data is typically annotated manually for relevant features (e.g. visual fixations of gestures), which is a time-consuming and error-prone task. We present a semi-automatic approach for the detection of human hands in images. Such an approach reduces the amount of manual analysis drastically while guaranteeing high accuracy. In our algorithm we combine several well-known detection techniques together with an advanced elimination scheme to reduce false detections. We validate our approach using a challenging dataset containing over 4300 hand instances. This validation allows us to explore the capabilities and boundaries of our approach. Â© Springer International Publishing Switzerland 2016.",,,
10.1002/ecj.11776,2016,"Tamura K., Hashimoto K., Aoki Y.",Head pose-invariant eyelid and iris tracking method,"These days, there is more demand for a camera-based gaze estimation method for new interfaces and new marketing measurement tools. Considering these applications, the system should track a new user without any operation such as calibration. It should also admit user's natural head pose changes. Previous methods, however, need a calibration procedure before execution and have less accuracy in a head moving situation. In this paper, we propose a method which tracks the user's eyelids and iris automatically and accurately. Our method is a pretreatment of gaze estimation without any calibration and head pose restraint. First of all, we track the facial feature points from an input face image and estimate its head pose, extracting the eye region image. On the eye region image, we track the eyelid shape based on an eyelid shape model generated beforehand from PCA. Finally we track the iris inside the eyelid based on the eyeball model. The eyelid and iris tracking is processed by Particle Filter. An evaluation against a database including head pose changes confirmed that the accuracy of eyelid and iris tracking was improved compared with previous methods. Â© 2016 Wiley Periodicals, Inc.",,,
10.1016/j.ijhcs.2015.10.004,2016,"Pan Y., Steed A.",Effects of 3D perspective on head gaze estimation with a multiview autostereoscopic display,"Head gaze, or the orientation of the head, is a very important attentional cue in face to face conversation. Some subtleties of the gaze can be lost in common teleconferencing systems, because a single perspective warps spatial characteristics. A recent random hole display is a potentially interesting display for group conversation, as it allows multiple stereo viewers in arbitrary locations, without the restriction of conventional autostereoscopic displays on viewing positions. We represented a remote person as an avatar on a random hole display. We evaluated this system by measuring the ability of multiple observers with different horizontal and vertical viewing angles to accurately and simultaneously judge which targets the avatar is gazing at. We compared three perspective conditions: a conventional 2D view, a monoscopic perspective-correct view, and a stereoscopic perspective-correct views. In the latter two conditions, the random hole display shows three and six views simultaneously. Although the random hole display does not provide high quality view, because it has to distribute display pixels among multiple viewers, the different views are easily distinguished. Results suggest the combined presence of perspective-correct and stereoscopic cues significantly improved the effectiveness with which observers were able to assess the avatar's head gaze direction. This motivates the need for stereo in future multiview displays. Â© 2015 Elsevier Ltd.",,,
10.2312/egve.20161432,2016,"Moniri M.M., Luxenburger A., Schuffert W., Sonntag D.",Real-time 3D peripheral view analysis,"Human peripheral vision suffers from several limitations that differ among various regions of the visual field. Since these limitations result in natural visual impairments, many interesting intelligent user interfaces based on eye tracking could benefit from peripheral view calculations that aim to compensate for events occurring outside the very center of gaze. We present a general peripheral view calculation model which extends previous work on attention-based user interfaces that use eye gaze. An intuitive, two dimensional visibility measure based on the concept of solid angle is developed for determining to which extent an object of interest observed by a user intersects with each region of the underlying visual field model. The results are weighted considering the visual acuity in each visual field region to determine the total visibility of the object. We exemplify the proposed model in a virtual reality car simulation application incorporating a head-mounted display with integrated eye tracking functionality. In this context, we provide a quantitative evaluation in terms of a runtime analysis of the different steps of our approach. We provide also several example applications including an interactive web application which visualizes the concepts and calculations presented in this paper. Â© 2016 The Author(s)",,,
,2016,"Mills C., Bixler R., Wang X., D'Mello S.K.",Automatic gaze-based detection of mind wandering during narrative film comprehension,"Mind wandering (MW) reflects a shift in attention from task-related to task-unrelated thoughts. It is negatively related to performance across a range of tasks, suggesting the importance of detecting and responding to MW in real-time. Currently, there is a paucity of research on MW detection in contexts other than reading. We addressed this gap by using eye gaze to automatically detect MW during narrative film comprehension, an activity that is used across a range of learning environments. In the current study, students self-reported MW as they watched a 32.5-minute commercial film. Studentsâ€?eye gaze was recorded with an eye tracker. Supervised machine learning models were used to detect MW using global (content-independent), local (content-dependent), and combined global+local features. We achieved a student-independent score (MW F1) of .45, which reflected a 29% improvement over a chance baseline. Models built using local features were more accurate than the global and combined models. An analysis of diagnostic features revealed that MW primarily manifested as a breakdown in attentional synchrony between eye gaze and visually salient areas of the screen. We consider limitations, applications, and refinements of the MW detector. Â© 2016 International Educational Data Mining Society. All rights reserved.",,,
,2016,"Kim Y.-T., Seo J., Seo W., Sung G., Kim Y., Song H., An J., Choi C.-S., Kim S., Kim H., Kim Y., Kim Y., Lee H.-S.",Holographic augmented reality head-up display with eye tracking and steering light source,"We realized a holographic head-up display using a steering light source with eye position tracking. It can represent a real augmented reality which perfectly matches virtual graphic images to the real world. Further, for the determination of the position of the light source, 3D calibration method is proposed. Â© 2016 Society for Information Display. All rights reserved.",,,
,2016,"Kim Y.-T., Seo J., Seo W., Sung G., Kim Y., Song H., An J., Choi C.-S., Kim S., Kim H., Kim Y., Kim Y., Lee H.-S.",Holographic augmented reality head-up display with eye tracking and steering light source,"We realized a holographic head-up display using a steering light source with eye position tracking. It can represent a real augmented reality which perfectly matches virtual graphic images to the real world. Further, for the determination of the position of the light source, 3D calibration method is proposed. Â© 2016 Chinese Laser Press.",,,
,2016,Farahi B.,Caress of the gaze: A gaze actuated 3D printed body architecture,"This paper describes the design process behind Caress of the Gaze, a project that represents a new approach to the design of a gaze-actuated, 3D printed body architecture - as a form of proto-architectural study - providing a framework for an interactive dynamic design. The design process engages with three main issues. Firstly, it aims to look at form or geometry as a means of controlling material behavior by exploring the tectionic propertes of mult-material 3D printing technologies. Secondly, it addresses novel actuation systems by using Shape Memory Alloy (SMA) in order to achieve life-like behavior. Thirdly, it explores the possibility of engaging with interactive systems by investgating how our clothing could interact with other people as a primary interface, using vision-based eye-gaze tracking technologies. In so doing, this paper describes a radically alternative approach not only to the production of garments but also to the ways we interact with the world around us. Therefore, the paper addresses the emerging field of shape-changing 3D printed structures and interactive systems that bridge the worlds of robotcs, architecture, technology, and design. Â© 2016 CURRAN-CONFERENCE. All rights reserved.",,,
10.2312/egp.20161054,2016,"Wood E., BaltruÅ¡aitis T., Morency L.-P., Robinson P., Bulling A.",A 3D morphable model of the eye region,"We present the first 3D morphable model that includes the eyes, enabling gaze estimation and gaze re-targetting from a single image. Morphable face models are a powerful tool and are used for a range of tasks including avatar animation and facial expression transfer. However, previous work has avoided the eyes, even though they play an important role in human communication. We built a new morphable model of the facial eye-region from high-quality head scan data, and combined this with a parametric eyeball model constructed from anatomical measurements and iris photos. We fit our models to an input RGB image, solving for shape, texture, pose, and scene illumination simultaneously. This provides us with an estimate of where a person is looking in a 3D scene without per-user calibration - a still unsolved problem in computer vision. It also allows us to re-render a person's eyes with different parameters, thus redirecting their perceived attention. Â© 2016 The Eurographics Association.",,,
10.2352/ISSN.2470-1173.2016.5.SDA-454,2016,"Kim S., Takahashi M., Watanabe K., Kawai T.",The effects of functional binocular disparity on route memory in stereoscopic images,"In this study, the effects of functional binocular disparity on route memory were experimentally verified in the context of learning of evacuation routes in disaster prevention and mitigation training. Functional binocular disparity in 3D images using cognitive characteristics such as the perspective of a specific location correlated memory in this paper. Depth maps were manipulated with the objective of assisting memorization and intuitive understanding of evacuation routes. In particular, with respect to deciding the advancing direction of the evacuation route in buildings without explicit signs, for a specific building, depth maps that could work as guide marks for the advancing route direction were manipulated to augment functional binocular parallax. In the experimental stimuli, eight locations within the building were selected to form the evacuation route, and recording was conducted using a 3D camera. The four conditions simulated in the experiment were 3D conditions using 3D images, 2D conditions using only the left image of the 3D images, adding depth map manipulation and functional binocular disparity to 2D, and placing guide marks at locations in directions that are different from the actual advancing direction to create distracted 3D conditions. 32 participants were given the route recognition task two times, once immediately after the interference task and once more after an interval of one week. The results suggest that, the participants who observed the evacuation route images modified into functional binocular disparity, remembered the correct path more easily after an interval of one week and were able to better focus their eye-gaze onto the parallax augmented locations. Â© 2016 Society for Imaging Science and Technology.",,,
10.2352/ISSN.2169-2629.2017.32.152,2016,"Hirasawa Y., Yamamoto S., Domon R., Kintou H., Tsumura N.",Viewpoint entropy for material appearance,"In this paper, we proposed an evaluation model to quantify the viewing condition that enhances the material appearance of object without dependence on shape of object. The proposed model is based on viewpoint entropy which is used to find appropriate eye position. In order to establish this model, we first clarify the surface that have important information related to material appearance by measuring the gaze point with eye tracking equipment. Next, we added experimental results as a weight coefficient of material appearance into the conventional viewpoint entropy. We verified that our model is applicable to metal and ceramic objects by comparing the results between subjective evaluation and computational evaluation using the proposed model. Â© Society for Imaging Science and Technology 2016.",,,
10.1109/ICPR.2016.7899814,2016,"Augereau O., Fujiyoshi H., Kise K.",Towards an automated estimation of English skill via TOEIC score based on reading analysis,"Estimating automatically the degree of language skill by analyzing the eye movements is a promising way to help people from all over the world to learn a new language. In this study, we focus on the English skills of non-native speakers. Our aim is to provide an algorithm that can assess accurately and automatically the TOEIC score after reading English texts for few minutes. As a first step towards this direction, we propose an algorithm that can predict accurately this score after reading and answering some questions about the comprehension of few English texts. We use an eye tracker in order to record the eye gaze, i.e. the positions where the reader is looking at. Then we extract several features to characterize the behavior, and consequently the skill of the reader. We also add a feature based on the number of correct answers to the questions. By using a machine learning based on multivariate regression, the score is estimated user independently. A backward stepwise feature selection is used to select the relevant features and to optimize the estimation. As a main result, the TOEIC score is estimated with 21.7 points of mean absolute error for 21 subjects after reading and answering the questions of only 3 documents. Â© 2016 IEEE.",,,
10.1109/ICPR.2016.7900052,2016,"Kang W., Ji Q.",Real time eye gaze tracking with Kinect,"Traditional gaze tracking systems rely on explicit infrared lights and high resolution cameras to achieve high performance and robustness. These systems, however, require complex setup and thus are restricted in lab research and hard to apply in practice. In this paper, we propose to perform gaze tracking with a consumer level depth sensor (Kinect). Leveraging on Kinect's capability to obtain 3D coordinates, we propose an efficient model-based gaze tracking system. We first build a unified 3D eye model to relate gaze directions and eye features (pupil center, eyeball center, cornea center) through subject-dependent eye parameters. A personal calibration framework is further proposed to estimate the subject-dependent eye parameters. Finally we can perform real time gaze tracking given the 3D coordinates of eye features from Kinect and the subject-dependent eye parameters from personal calibration procedure. Experimental results with 6 subjects prove the effectiveness of the proposed 3D eye model and the personal calibration framework. Furthermore, the gaze tracking system is able to work in real time (20 fps) and with low resolution eye images. Â© 2016 IEEE.",,,
10.1109/ROBIO.2016.7866350,2016,"Zhou X., Cai H., Shao Z., Yu H., Liu H.",3D eye model-based gaze estimation from a depth sensor,"In this paper, we address the 3D eye gaze estimation problem using a low-cost, simple-setup, and non-intrusive consumer depth sensor (Kinect sensor). We present an effective and accurate method based on 3D eye model to estimate the point of gaze of a subject with the tolerance of free head movement. To determine the parameters involved in the proposed eye model, we propose i) an improved convolution-based means of gradients iris center localization method to accurately and efficiently locate the iris center in 3D space; ii) a geometric constraints-based method to estimate the eyeball center under the constraints that all the iris center points are distributed on a sphere originated from the eyeball center and the sizes of two eyeballs of a subject are identical; iii) an effective Kappa angle calculation method based on the fact that the visual axes of both eyes intersect at a same point with the screen plane. The final point of gaze is calculated by using the estimated eye model parameters. We experimentally evaluate our gaze estimation method on five subjects. The experimental results show the good performance of the proposed method with an average estimation accuracy of 3.78Â°, which outperforms several state-of-the-arts. Â© 2016 IEEE.",,,
,2016,"Hutt S., Mills C., White S., Donnelly P.J., Dâ€™Mello S.K.",The eyes have it: Gaze-based Detection of Mind Wandering during Learning with an Intelligent Tutoring System,"Mind wandering (MW) is a ubiquitous phenomenon characterized by an unintentional shift in attention from task-related to task-unrelated thoughts. MW is frequent during learning and negatively correlates with learning outcomes. Therefore, the next generation of intelligent learning technologies should benefit from mechanisms that detect and combat MW. As an initial step in this direction, we used eye-gaze and contextual information (e.g., time into session) to build an automated MW detector as students interact with GuruTutor â€?an intelligent tutoring system (ITS) for biology. Students self-reported MW by responding to pseudorandom thought-probes during the tutoring session while a consumer-grade eye tracker monitored their eye movements. We used supervised machine learning techniques to discriminate between positive and negative responses to the probes in a student-independent fashion. Our best results for detecting MW (F1 of 0.49) were obtained with an evolutionary approach to develop topologies for neural network classifiers. These outperformed standard classifiers (F1 of 0.43 with a Bayes net) and a chance baseline (F1 of 0.19). We discuss our results in the context of integrating MW detection into an attention-aware version of GuruTutor. Â© 2016 International Educational Data Mining Society. All rights reserved.",,,
10.3390/app6060174,2016,"Wang J., Zhang G., Shi J.",2D gaze estimation based on Pupil-Glint vector using an artificial neural network,"Gaze estimation methods play an important role in a gaze tracking system. A novel 2D gaze estimation method based on the pupil-glint vector is proposed in this paper. First, the circular ring rays location (CRRL) method and Gaussian fitting are utilized for pupil and glint detection, respectively. Then the pupil-glint vector is calculated through subtraction of pupil and glint center fitting. Second, a mapping function is established according to the corresponding relationship between pupil-glint vectors and actual gaze calibration points. In order to solve the mapping function, an improved artificial neural network (DLSR-ANN) based on direct least squares regression is proposed. When the mapping function is determined, gaze estimation can be actualized through calculating gaze point coordinates. Finally, error compensation is implemented to further enhance accuracy of gaze estimation. The proposed method can achieve a corresponding accuracy of 1.29Â°, 0.89Â°, 0.52Â°, and 0.39Â° when a model with four, six, nine, or 16 calibration markers is utilized for calibration, respectively. Considering error compensation, gaze estimation accuracy can reach 0.36Â°. The experimental results show that gaze estimation accuracy of the proposed method in this paper is better than that of linear regression (direct least squares regression) and nonlinear regression (generic artificial neural network). The proposed method contributes to enhancing the total accuracy of a gaze tracking system.",,,
10.1115/DETC201660293,2016,"Seshadri P., Simons R., Bi Y., Hartley J., Bhatia J., Reid T.",Evaluations that matter: Customer preferences using industry-based evaluations and eye-gaze data,"This study is the first stage of a research program aimed at understanding differences in how people process 2D and 3D automotive stimuli, using psychophysiological tools such as galvanic skin response (GSR), eye tracking, electroencephalography (EEG), and facial expressions coding, along with respondent ratings. The current study uses just one measure, eye tracking, and one stimulus format, 2D realistic renderings of vehicles, to reveal where people expect to find information about brand and other industry-relevant topics, such as sportiness. The eye-gaze data showed differences in the percentage of fixation time that people spent on different views of cars while evaluating the ""Brand"" and the degree to which they looked ""Sporty/Conservative"", ""Calm/Exciting"", and ""Basic/Luxurious"". The results of this work can give designers insights on where they can invest their design efforts when considering brand and styling cues. Copyright Â© 2016 by ASME.",,,
10.1007/978-3-319-50832-0_24,2016,"Ekong S., Borst C.W., Woodworth J., Chambers T.L.",Teacher-student VR telepresence with networked depth camera mesh and heterogeneous displays,"We present a novel interface for a teacher guiding students immersed in virtual environments. Our approach uses heterogeneous displays, with a teacher using a large 2D monitor while multiple students use immersive head-mounted displays. The teacher is sensed by a depth camera (Kinect) to capture depth and color imagery, which are streamed to student stations to inject a realistic 3D mesh of the teacher into the environment. To support communication needed for an educational application, we introduce visual aids to help teachers point and to help them establish correct eye gaze for guiding students. The result allowed an expert guide in one city to guide users located in another city through a shared educational environment. We include substantial technical details on mesh streaming, rendering, and the interface, to help other researchers. Â© Springer International Publishing AG 2016.",,,
,2016,"Mishra A., Kanojia D., Bhattacharyya P.",Predicting readers' sarcasm understandability by modeling gaze behavior,"Sarcasm understandability or the ability to understand textual sarcasm depends upon readers' language proficiency, social knowledge, mental state and attentiveness. We introduce a novel method to predict the sarcasm understandability of a reader. Presence of incongruity in textual sarcasm often elicits distinctive eye-movement behavior by human readers. By recording and analyzing the eye-gaze data, we show that eyemovement patterns vary when sarcasm is understood vis-a-vis when it is not. Motivated by our observations, we propose a system for sarcasm understandability prediction using supervised machine learning. Our system relies on readers' eyemovement parameters and a few textual features, thence, is able to predict sarcasm understandability with an F-score of 93%, which demonstrates its efficacy. The availability of inexpensive embedded-eye-trackers on mobile devices creates avenues for applying such research which benefits web-content creators, review writers and social media analysts alike. Â© 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,,
10.5220/0006043100390043,2016,"GuimarÃ£es C.P., Balbio V., Cid G.L., Orselli M.I.V., Xavier A.P., Neto A.S., CorrÃªa S.C.",3D interactive environment applied to fencing training,"The purpose of this study was to present a 3D interactive environment - a Digital Platform to help in fencing training. The first fencing motion described and analysed at the 3D platform was lunge in epee fencing. The platform was able to show kinematic variables of upper and lower limbs and the center of mass that characterized a good performance in epee fencing. The platform also incorporates a digital database of eye track motions of the fencers. An OptiTrack motion capture system was used to capture the lunge motion of five skilled amateur fencing athletes in the presence or not of a static opponent and an Eye Track System Tobbi II was used to track the eye movements of the fencers when performing a lunge attack with a target. The 3D platform was developed using Unity3D and can present some interesting results to improve available information to coaches. That highlights the importance of visualization biomechanical results based on coach criteria in a more understandable way to help athletic training. Â© Copyright 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",,,
,2016,"Ghali R., Frasson C., Ouellet S.",Towards real time detection of learners' need of help in serious games,"Providing an adequate help to a learner remains a challenge. In this paper we aim to find how to provide learners with real time help in an educational game. Detecting that a player is engaged or motivated is a good sign that he is progressing. For these reasons we need to assess learner's states while learning. In this study we gather a variety of data using three types of sensors (electroencephalography, eye tracking and automatic facial expression recognition) to build a reliable user adaptation system. The data result from an interaction of 40 players with LewiSpace game, that we built for experimental purpose to learn construction of Lewis diagrams. We used machine learning algorithms in order to identify the most important features gathered from each sensor. Two models were trained with these data: a generalized model, trained on all data available, and a personalized model, trained only on the current user during an early phase of the game experience. The predictive results showed that personalized model could outperform the generalized model. Â© 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,,
,2016,"Muhammad A., Addenan M.F., Latiff M.M., Haris B., Surip S.S., Mohamed A.S.A.",Interactive sign language interpreter using skeleton tracking,"The aim of this paper is to introduce an interactive communication system that will benefit both people with hearing and verbal difficulties to convey in the form of the sign language naturally. The idea is to provide two ways of communication between two users by converting sign language to voice and text and provides means of returning communication feedback whereby the other party can speak or key-in text and translates it into sign language movement performed by a three-dimensional (3D) model. A Microsoft Kinect device is used to captures the sign movements by optimizing the skeleton tracking algorithm to understand specific hands movements and dictates using the pre-recorded gesture library to digitized voice and using the same apparatus, speech is translated back into sign language. Research leads in helping the disables have been carried out extensively and majority focuses on only using single type of motion sensing technology such as TOBII (eye tracking) and LEAP (leap motion) which are either costly or limited to a small workable space. Microsoft Kinect technology would be a genuinely equipment used to create a cost-effective and capable technology prototype that enables sign-language communication between signer and non-signer, thus, offers translation into Bahasa Malaysia text.",,,
10.1007/978-3-319-48881-3_18,2016,"Liu Y., Lee B.S., Sluzek A., Rajan D., McKeown M.",Feasibility analysis of eye typing with a standard webcam,"With the development of assistive technology, eye typing has become an alternative form of text entry for physically challenged people with severe motor disabilities. However, additional eye-tracking devices need to be used to track eye movements which is inconvenient in some cases. In this paper, we propose an appearance-based method to estimate the personâ€™s gaze point using a webcam, and also investigate some practical issues of the method. The experimental results demonstrate the feasibility of eye typing using the proposed method. Â© Springer International Publishing Switzerland 2016.",,,
,2016,"Ghiass R.S., Arandjelovic O.",Highly accurate gaze estimation using a consumer RGB-D Sensor,"Determining the direction in which a person is looking is an important problem in a wide range of HCI applications. In this paper we describe a highly accurate algorithm that performs gaze estimation using an affordable and widely available device such as Kinect. The method we propose starts by performing accurate head pose estimation achieved by fitting a person specific morphable model of the face to depth data. The ordinarily competing requirements of high accuracy and high speed are met concurrently by formulating the fitting objective function as a combination of terms which excel either in accurate or fast fitting, and then by adaptively adjusting their relative contributions throughout fitting. Following pose estimation, pose normalization is done by re-rendering the fitted model as a frontal face. Finally gaze estimates are obtained through regression from the appearance of the eyes in synthetic, normalized images. Using EYEDIAP, the standard public dataset for the evaluation of gaze estimation algorithms from RGB-D data, we demonstrate that our method greatly outperforms the state of the art.",,,
10.22260/isarc2016/0035,2016,"Pinheiro R.B.O., Pradhananga N., Jianu R., Orabi W.",Eye-tracking technology for construction safety: A feasibility study,"A construction site is a harsh environment demanding entire human senses and attention, even for regular scheduled tasks. Many accidents occur on construction sites because of inability of workers to identify hazards and make timely decisions. To understand why some hazards go unseen, it is crucial to study how workers perceive the site. The objective of this research is to leverage eye-tracking technology to study workers' gazing pattern in a construction environment. A real picture from an active construction site is modified to introduce hazards and a desktop experiment is conducted, in which, subjects are asked to identify the hazards. A different group of subjects are made to make similar observations on a 2D sketch-representation of the same construction scenario. Eye-tracking data gathered from their observations is analyzed to understand when, how, and which hazards do they recognize and the pattern of recognition is studied. The results of this study will enhance our understanding on the visual factors that govern attention and help workers recognize potential hazards in a construction site. The comparison between the observation pattern in real and sketch-representation is done to assess how subjects respond to artificial images compared to real images. This comparison will test the feasibility of using virtual reality for safety training and simulations.",,,
10.1007/978-3-319-47437-3_93,2016,"Smith M.A., Wiese E.",Look at me now: Investigating delayed disengagement for ambiguous human-robot stimuli,"Human-like appearance has been shown to positively affect perception of and attitudes towards robotic agents. In particular, the more human-like robots look, the more participants are willing to ascribe human-like states to them (i.e., having a mind, emotions, agency). The positive effect of human-likeness on agent ratings, however, does not translate to better performance in human-robot interaction (HRI). Performance first increases as human-likeness increases, then drops dramatically as soon as human-likeness reaches around 70 % to finally reach its maximum at 100 % humanness. The goal of the current paper is to investigate whether attentional mechanisms, in particular delayed disengagement, are responsible for the drop in performance for very human-like, but not perfectly human agents. The idea is that robots with a high degree of human-likeness capture attention and thus make it harder to orient attention away from them towards task-relevant stimuli in the periphery resulting in bad performance. To investigate this question, faces of differing degrees of human-likeness (0 %, 30 %, 70 %, 100 %, non-social control) are presented to participants in an eye-tracking experiment and the time it takes participants to orient towards a peripheral stimulus is measured. Results show significant delayed disengagement for all stimuli, but no stronger delayed disengagement for very human-like agents, making delayed disengagement an unlikely source for the negative effect of human-like appearance on performance in HRI. Â© Springer International Publishing AG 2016.",,,
10.1007/978-3-319-46448-0_18,2016,"Wood E., BaltruÅ¡aitis T., Morency L.-P., Robinson P., Bulling A.",A 3D morphable eye region model for gaze estimation,"Morphable face models are a powerful tool, but have previously failed to model the eye accurately due to complexities in its material and motion. We present a new multi-part model of the eye that includes a morphable model of the facial eye region, as well as an anatomy-based eyeball model. It is the first morphable model that accurately captures eye region shape, since it was built from high-quality head scans. It is also the first to allow independent eyeball movement, since we treat it as a separate part. To showcase our model we present a new method for illumination- and head-poseâ€“invariant gaze estimation from a single RGB image. We fit our model to an image through analysis-bysynthesis, solving for eye region shape, texture, eyeball pose, and illumination simultaneously. The fitted eyeball pose parameters are then used to estimate gaze direction. Through evaluation on two standard datasets we show that our method generalizes to both webcam and high-quality camera images, and outperforms a state-of-the-art CNN method achieving a gaze estimation accuracy of 9.44Â° in a challenging user-independent scenario. Â© Springer International Publishing AG 2016.",,,
10.1007/978-3-319-46604-0_41,2016,"Qodseya M., Sanzari M., Ntouskos V., Pirri F.",A3D: A device for studying gaze in 3D,"A wearable device for capturing 3D gaze information in indoor and outdoor environments is proposed. The hardware and software architecture of the device provides an estimate in quasi-real-time of 2.5D points of regard (POR) and then lift their estimations to 3D, by projecting them into the 3D reconstructed scene. The estimation procedure does not need any external device, and can be used both indoor and outdoor and with the subject wearing it moving, though some smooth constraint in the motion are required. To ensure a great flexibility with respect to depth a novel calibration method is proposed, which provides eye-scene calibration that explicitly takes into account depth information, in so ensuring a quite accurate estimation of the PORs. The experimental evaluation demonstrates that both 2.5D and 3D POR are accurately estimated. Â© Springer International Publishing Switzerland 2016.",,,
10.1117/12.2224190,2016,"Mannaru P., Balasingam B., Pattipati K., Sibley C., Coyne J.",On the use of hidden Markov models for gaze pattern modeling,"Some of the conventional metrics derived from gaze patterns (on computer screens) to study visual attention, engagement and fatigue are saccade counts, nearest neighbor index (NNI) and duration of dwells/fixations. Each of these metrics has drawbacks in modeling the behavior of gaze patterns; one such drawback comes from the fact that some portions on the screen are not as important as some other portions on the screen. This is addressed by computing the eye gaze metrics corresponding to important areas of interest (AOI) on the screen. There are some challenges in developing accurate AOI based metrics: firstly, the definition of AOI is always fuzzy; secondly, it is possible that the AOI may change adaptively over time. Hence, there is a need to introduce eye-gaze metrics that are aware of the AOI in the field of view; at the same time, the new metrics should be able to automatically select the AOI based on the nature of the gazes. In this paper, we propose a novel way of computing NNI based on continuous hidden Markov models (HMM) that model the gazes as 2D Gaussian observations (x-y coordinates of the gaze) with the mean at the center of the AOI and covariance that is related to the concentration of gazes. The proposed modeling allows us to accurately compute the NNI metric in the presence of multiple, undefined AOI on the screen in the presence of intermittent casual gazing that is modeled as random gazes on the screen. Â© 2016 SPIE.",,,
10.1117/12.2219703,2016,"Monfort S.S., Sibley C.M., Coyne J.T.",Using machine learning and real-time workload assessment in a high-fidelity UAV simulation environment,"Future unmanned vehicle operations will see more responsibilities distributed among fewer pilots. Current systems typically involve a small team of operators maintaining control over a single aerial platform, but this arrangement results in a suboptimal configuration of operator resources to system demands. Rather than devoting the full-time attention of several operators to a single UAV, the goal should be to distribute the attention of several operators across several UAVs as needed. Under a distributed-responsibility system, operator task load would be continuously monitored, with new tasks assigned based on system needs and operator capabilities. The current paper sought to identify a set of metrics that could be used to assess workload unobtrusively and in near real-time to inform a dynamic tasking algorithm. To this end, we put 20 participants through a variable-difficulty multiple UAV management simulation. We identified a subset of candidate metrics from a larger pool of pupillary and behavioral measures. We then used these metrics as features in a machine learning algorithm to predict workload condition every 60 seconds. This procedure produced an overall classification accuracy of 78%. An automated tasker sensitive to fluctuations in operator workload could be used to efficiently delegate tasks for teams of UAV operators. Â© 2016 SPIE.",,,
10.1007/978-3-319-46073-4_20,2016,"TÅ‘sÃ©r Z., Rill R.A., FaragÃ³ K., Jeni L.A., LÅ‘rincz A.",Personalization of gaze direction estimation with deep learning,"There is a growing interest in behavior based biometrics. Although biometric data has considerable variations for an individual and may be faked, yet the combination of such â€˜weak expertsâ€?can be rather strong. A remotely detectable component is gaze direction estimation and thus, eye movement patterns. Here, we present a novel personalization method for gaze estimation systems, which does not require a precise calibration setup, can be non-obtrusive, is fast and easy to use. We show that it improves the precision of gaze direction estimation algorithms considerably. The method is convenient; we exploit 3D face model reconstruction for the enrichment of a small number of collected data artificially. Â© Springer International Publishing AG 2016.",,,
,2016,"Abrahamson D., Shayan S., Bakker A., Van Der Schaaf M.F.",Exposing piaget's scheme: Empirical evidence for the ontogenesis of coordination in learning a mathematical concept,"The combination of two methodological resources-natural-user interfaces (NUI) and multimodal learning analytics (MMLA)-is creating opportunities for educational researchers to empirically evaluate seminal models for the hypothetical emergence of concepts from situated sensorimotor activity. 76 participants (9-14 yo) solved tablet-based non-symbolic manipulation tasks designed to foster grounded meanings for the mathematical concept of proportional equivalence. Data gathered in task-based semi-structured clinical interviews included action logging, eye-gaze tracking, and videography. Successful task performance coincided with spontaneous appearance of stable dynamical gaze-path patterns soon followed by multimodal articulation of strategy. Significantly, gaze patterns included uncued non-salient screen locations. We present cumulative results to argue that these 'attentional anchors' mediated participants' problem solving. We interpret the findings as enabling us to revisit, support, refine, and elaborate on central claims of Piaget's theory of genetic epistemology and in particular his insistence on the role of situated motor-action coordination in the process of reflective abstraction. Â© 2016 ISLS.",,,
10.5194/isprsarchives-XLI-B2-433-2016,2016,"Rautenbach V., Coetzee S., Hankel M.",Exploratory user study to evaluate the effect of street name changes on route planning using 2D maps,"This paper presents the results of an exploratory user study using 2D maps to observe and analyse the effect of street name changes on prospective route planning. The study is part of a larger research initiative to understand the effect of street name changes on wayfinding. The common perception is that street name changes affect our ability to navigate an environment, but this has not yet been tested with an empirical user study. A combination of a survey, the thinking aloud method and eye tracking was used with a group of 20 participants, mainly geoinformatics students. A within-subject participant assignment was used. Independent variables were the street network (regular and irregular) and orientation cues (street names and landmarks) portrayed on a 2D map. Dependent variables recorded were the performance (were the participant able to plan a route between the origin and destination?); the accuracy (was the shortest path identified?); the time taken to complete a task; and fixation points with eye tracking. Overall, the results of this exploratory study suggest that street name changes impact the prospective route planning performance and process that individuals use with 2D maps. The results contribute to understanding how route planning changes when street names are changed on 2D maps. It also contributes to the design of future user studies. To generalise the findings, the study needs to be repeated with a larger group of participants.",,,
10.5194/isprsarchives-XLI-B2-637-2016,2016,"Dolezalova J., Popelka S.",Evaluation of the user strategy on 2D and 3D city maps based on novel scanpath comparison method and graph visualization,"The paper is dealing with scanpath comparison of eye-tracking data recorded during case study focused on the evaluation of 2D and 3D city maps. The experiment contained screenshots from three map portals. Two types of maps were used - standard map and 3D visualization. Respondents' task was to find particular point symbol on the map as fast as possible. Scanpath comparison is one group of the eye-tracking data analyses methods used for revealing the strategy of the respondents. In cartographic studies, the most commonly used application for scanpath comparison is eyePatterns that output is hierarchical clustering and a tree graph representing the relationships between analysed sequences. During an analysis of the algorithm generating a tree graph, it was found that the outputs do not correspond to the reality. We proceeded to the creation of a new tool called ScanGraph. This tool uses visualization of cliques in simple graphs and is freely available at www.eyetracking.upol.cz/scangraph. Results of the study proved the functionality of the tool and its suitability for analyses of different strategies of map readers. Based on the results of the tool, similar scanpaths were selected, and groups of respondents with similar strategies were identified. With this knowledge, it is possible to analyse the relationship between belonging to the group with similar strategy and data gathered from the questionnaire (age, sex, cartographic knowledge, etc.) or type of stimuli (2D, 3D map).",,,
10.5194/isprsarchives-XLI-B2-641-2016,2016,"Dong W., Liao H.",Eye tracking to explore the impacts of photorealistic 3D representations in pedstrian navigation performance,"Despite the now-ubiquitous two-dimensional (2D) maps, photorealistic three-dimensional (3D) representations of cities (e.g., Google Earth) have gained much attention by scientists and public users as another option. However, there is no consistent evidence on the influences of 3D photorealism on pedestrian navigation. Whether 3D photorealism can communicate cartographic information for navigation with higher effectiveness and efficiency and lower cognitive workload compared to the traditional symbolic 2D maps remains unknown. This study aims to explore whether the photorealistic 3D representation can facilitate processes of map reading and navigation in digital environments using a lab-based eye tracking approach. Here we show the differences of symbolic 2D maps versus photorealistic 3D representations depending on users' eye-movement and navigation behaviour data. We found that the participants using the 3D representation were less effective, less efficient and were required higher cognitive workload than using the 2D map for map reading. However, participants using the 3D representation performed more efficiently in self-localization and orientation at the complex decision points. The empirical results can be helpful to improve the usability of pedestrian navigation maps in future designs.",,,
10.1007/978-3-319-39513-5_3,2016,"MÃ¤rtin C., Rashid S., Herdin C.",Designing responsive interactive applications by emotion-tracking and pattern-based dynamic user interface adaptation,"Model-based user interface development environments (MB-UIDEs) can be enhanced by pattern-based frameworks to allow for richer design capabilities and more flexible responsive behavior during the runtime of the implemented target application. In this paper an experimental system prototype for integrating facial analysis and eye-tracking into a pattern-based dynamic user interface adaptation process is discussed. The resulting system evaluates the emotional state of the system users to trigger the HCI-pattern-based adaptation of the user interface. By monitoring the emotional states of the users over longer time periods, while the system changes its behavior and its appearance, conclusions about the perceived quality dimensions of the interactive application can be drawn. Â© Springer International Publishing Switzerland 2016.",,,
10.1117/12.2211823,2016,"Hong J.-Y., Lee C.-K., Park S.-G., Kim J., Cha K.-H., Kang K.H., Lee B.",See-through multi-view 3D display with parallax barrier,"In this paper, we propose the see-through parallax barrier type multi-view display with transparent liquid crystal display (LCD). The transparency of LCD is realized by detaching the backlight unit. The number of views in the proposed system is minimized to enlarge the aperture size of parallax barrier, which determines the transparency. For compensating the shortness of the number of viewpoints, eye tracking method is applied to provide large number of views and vertical parallax. Through experiments, a prototype of see-through autostereoscopic 3D display with parallax barrier is implemented, and the system parameters of transmittance, crosstalk, and barrier structure perception are analyzed. Â© 2016 SPIE.",,,
10.1007/978-3-319-40542-1_15,2016,"Chang Z., Qiu Q., Sapiro G.",Synthesis-based low-cost gaze analysis,"Gaze analysis has gained much popularity over the years due to its relevance in a wide array of applications, including humancomputer interaction, fatigue detection, and clinical mental health diagnosis. However, accurate gaze estimation from low resolution images outside of the lab (in the wild) still proves to be a challenging task. The new Intel low-cost RealSense 3D camera, capable of acquiring submillimeter resolution depth information, is currently available in laptops, and such technology is expected to become ubiquitous in other portable devices. In this paper, we focus on low-cost, scalable and real time analysis of human gaze using this RealSense camera. We exploit the direct measurement of eye surface geometry captured by the RGB-D camera, and perform gaze estimation through novel synthesis-based training and testing. Furthermore, we synthesize different eye movement appearances using a linear approach. From each 3D eye training sample captured by the RealSense camera, we synthesize multiple novel 2D views by varying the view angle to simulate head motions expected at testing. We then learn from the synthesized 2D eye images a gaze regression model using regression forests. At testing, for each captured RGB-D eye image, we first repeat the same synthesis process. For each synthesized image, we estimate the gaze from our gaze regression model, and factor-out the associated camera/head motion. In this way, we obtain multiple gaze estimations for each RGB-D eye image, and the consensus is adopted. We show that this synthesis-based training and testing significantly improves the precision in gaze estimation, opening the door to true low-cost solutions. Â© Springer International Publishing Switzerland 2016.",,,
10.1007/978-3-319-40216-1_23,2016,"Bampatzia S., Antoniou A., Lepouras G.",Comparing game input modalities: A study for the evaluation of player experience by measuring self reported emotional states and learning outcomes,"As new game controllers such as the Microsoft Kinect for Xbox are introduced into the market, new forms of game interaction are introduced such as gestures, voice and eye tracking, which raise some questions regarding the user experience. Is it possible that different input methods provide a more usable game setting and affect the playerâ€™s emotions and learning process? In this paper, a 2D game about the history of photography was designed and implemented to test these hypotheses. Two prototypes of this game were created, with the first requiring input only via mouse, while the second requiring input via voice and gestures (Kinect). Two different groups tested these two prototypes. The findings from previous pilot experiments indicated that using Kinect as an input method caused higher valence and dominance levels than the use of mouse and were further validated here. Additionally, the learning outcomes of players were not affected by the input method. Â© Springer International Publishing Switzerland 2016.",,,
10.1007/978-3-319-39627-9_26,2016,"Zhang Y., Wilcockson T., Kim K.I., Crawford T., Gellersen H., Sawyer P.",Monitoring dementia with automatic eye movements analysis,"Eye movement patterns are found to reveal human cognitive and mental states that can not be easily measured by other biological signals. With the rapid development of eye tracking technologies, there are growing interests in analysing gaze data to infer information about peopleâ€?cognitive states, tasks and activities performed in naturalistic environments. In this paper, we investigate the link between eye movements and cognitive function. We conducted experiments to record subjectâ€™s eye movements during video watching. By using computational methods, we identified eye movement features that are correlated to peopleâ€™s cognitive health measures obtained through the standard cognitive tests. Our results show that it is possible to infer peopleâ€™s cognitive function by analysing natural gaze behaviour. This work contributes an initial understanding of monitoring cognitive deterioration and dementia with automatic eye movement analysis. Â© Springer International Publishing Switzerland 2016.",,,
10.1007/978-3-319-32270-4_22,2016,"SÃ¡rkÃ¡ny A., TÅ‘sÃ©r Z., VerÅ‘ A.L., LÅ‘rincz A., Toyama T., Toosi E.N., Sonntag D.",Maintain and improve mental health by smart virtual reality serious games,"Serious games for mental health is seen as the groundwork for assistive technology to maintain and improve mental health. We present a technical system layout we partly implemented for demonstration purposes and highlight vision-based perception and manipulation capabilities. These include physical interactions employing artificial general intelligence in virtual reality applications. We employ hand gesture tracking, as well as an Oculus Rift integrated gaze and eye tracking system. The resulting serious games should eventually cover daily life activities, which we additionally monitor. The dynamic and contextual modelling of obstacles are central issues, and capabilities required for serious games include knowledge about the 3D world. Such knowledge include gaze and hand sensors interpretations for multimedia information extraction in causal relationships. Towards this goal, we envision to make use of virtual reality with a physics engine (rigid and soft body dynamics including collision detection) for the observed objects. We also exploit semantic networks to enable the machine to filter information and infer ongoing complex events including hidden BDI (beliefs, desires, intentions) variables. We see this combination of employed technology as the relevant groundwork for reaching human-level general intelligence and to enable real-world applications. Future applications and user groups we target on include dementia patients. Â© Springer International Publishing Switzerland 2016.",,,
10.1109/TCYB.2015.2400821,2016,"Zhang L., Li X., Nie L., Yang Y., Xia Y.",Weakly supervised human fixations prediction,"Automatically predicting human eye fixations is a useful technique that can facilitate many multimedia applications, e.g., image retrieval, action recognition, and photo retargeting. Conventional approaches are frustrated by two drawbacks. First, psychophysical experiments show that an object-level interpretation of scenes influences eye movements significantly. Most of the existing saliency models rely on object detectors, and therefore, only a few prespecified categories can be discovered. Second, the relative displacement of objects influences their saliency remarkably, but current models cannot describe them explicitly. To solve these problems, this paper proposes weakly supervised fixations prediction, which leverages image labels to improve accuracy of human fixations prediction. The proposed model hierarchically discovers objects as well as their spatial configurations. Starting from the raw image pixels, we sample superpixels in an image, thereby seamless object descriptors termed object-level graphlets (oGLs) are generated by random walking on the superpixel mosaic. Then, a manifold embedding algorithm is proposed to encode image labels into oGLs, and the response map of each prespecified object is computed accordingly. On the basis of the object-level response map, we propose spatial-level graphlets (sGLs) to model the relative positions among objects. Afterward, eye tracking data is employed to integrate these sGLs for predicting human eye fixations. Thorough experiment results demonstrate the advantage of the proposed method over the state-of-the-art. Â© 2015 IEEE.",,,
10.1007/978-3-319-29504-6_33,2016,"Ciupe A., Florea C., Orza B., Vlaicu A., Petrovan B.",A bag of words model for improving automatic stress classification,"Most of the existing stress assessment frameworks rely on physiological signals measurements (EEG, ECG, GSR, ST, etc.), which involve direct physical contact with the patient in a medical setup. Present technologies rely on capturing moods and emotions through remote devices (cameras), further processed by computer vision and machine learning techniques. The proposed work describes a method of automatic stress classification where stress information is modeled based on pupil diameter non-intrusive measurements, recorded by an eye tracking remote system. The signal extracted from the pupil Dataset has been processed using the Bag-Of-Words model, with a SVM classification and results have been compared to similar experiments in order to validate the applicability and consistency of the Bag-Of-Words model on stress assessment and classification. Â© Springer International Publishing Switzerland 2016.",,,
10.1007/s12021-015-9275-4,2016,"Andreu-Perez J., Solnais C., Sriskandarajah K.","EALab (Eye Activity Lab): a MATLAB Toolbox for Variable Extraction, Multivariate Analysis and Classification of Eye-Movement Data","Recent advances in the reliability of the eye-tracking methodology as well as the increasing availability of affordable non-intrusive technology have opened the door to new research opportunities in a variety of areas and applications. This has raised increasing interest within disciplines such as medicine, business and education for analysing human perceptual and psychological processes based on eye-tracking data. However, most of the currently available software requires programming skills and focuses on the analysis of a limited set of eye-movement measures (e.g., saccades and fixations), thus excluding other measures of interest to the classification of a determined state or condition. This paper describes â€˜EALabâ€? a MATLAB toolbox aimed at easing the extraction, multivariate analysis and classification stages of eye-activity data collected from commercial and independent eye trackers. The processing implemented in this toolbox enables to evaluate variables extracted from a wide range of measures including saccades, fixations, blinks, pupil diameter and glissades. Using EALab does not require any programming and the analysis can be performed through a user-friendly graphical user interface (GUI) consisting of three processing modules: 1) eye-activity measure extraction interface, 2) variable selection and analysis interface, and 3) classification interface. Â© 2015, Springer Science+Business Media New York.",,,
10.1016/j.neucom.2015.07.091,2016,"Ma C., Liu C., Peng F.",Two dimensional ensemble hashing for visual tracking,"Appearance model in visual tracking is a key component to attain robustness and efficiency. In the last decades, many complex appearance models have been proposed to improve performance of tracking algorithm. However, these models are difficult to maintain accuracy and efficiency simultaneously. In this paper, we observe that data-dependent hashing method could improve processing speed by generating compact representation for the visual object. But applying the method to visual tracking is still a challenging task. To reinforce the performance of hashing technique, a novel hashing method called two dimensional ensemble hashing is proposed. In our tracker, image samples are hashed to binary matrices, and the Hamming distance is used to measure their confidences. Moreover, for adapting situation change, the hash functions are updated by the learning model at each frame. Experimental results not only demonstrate the accuracy and effectiveness of our tracker, but also show that the tracking algorithm outperforms other state-of-the-art trackers. Â© 2015 Elsevier B.V.",,,
,,SEOK Y C; LEE T H; SHI Y; LI T; LEE T,"User terminal for tracking eye gaze of user, has photographing device for taking picture of face image of user and setting up vector from face image, and eye gaze tracking device inputting pupil image and tracing eye gaze of user","NOVELTY - The terminal (102) has a photographing device (204) for taking a picture of a face image of a user and setting up a vector from the face image. An eye gaze tracking device (206) inputs a pupil image and traces eye gaze of the user. A learning data collecting part (208) collects learning data comprising the face image of the user photographed in point of time when receiving action input from the user staring at a spot established with intra and location information of the set up spot. The eye gaze tracking device obtains the learning data in a deep learning model (210) and traces the eye gaze of the user using the deep running model learning the learning data. USE - User terminal for tracking eye gaze of a user. ADVANTAGE - The terminal obtains direction of the face image of the user, tracks the eye gaze based on the deep learning model, uses the pupil image as input data of the deep learning model, and uses the spot established with intra as the learning data of the deep learning model for tracking the eye gaze when receiving action comprising touch and voice, input from the user staring the spot so that reliability and accuracy of eye gaze tracking can be improved. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) an eye gaze tracking device(2) an eye gaze tracking method. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a user terminal. '(Drawing includes non-English language text)'User terminal for tracking eye gaze of user (102)Photographing device (204)Eye gaze tracking device (206)Learning data collecting part (208)Deep learning model (210)",,,
10.1109/TII.2019.2933481,2020,"Su, Dan; Li, You-Fu; Chen, Hao",Cross-Validated Locally Polynomial Modeling for 2-D/3-D Gaze Tracking With Head-Worn Devices,"In the context of wearable gaze tracking techniques, the problems of two-dimensional (2-D) and three-dimensional (3-D) gaze estimation can be viewed as inferring 2-D epipolar lines and 3-D visual axes from eye monitoring cameras. To this end, in this article, a simple local polynomial model is proposed to back-project a pupil center onto its corresponding visual axis. Based on this approximation, a homographylike relation is derived in a local manner, and via the Leave-One-Out cross-validation criterion, training gaze samples at one certain depth is leveraged to partition entire input space into multiple overlapping subregions. Then, the gaze data at another depth are utilized to recover the epipolar point, i.e., the image eyeball center. Thus, given a pupil image, the corresponding epipolar line can be determined by the resolved homographylike mapping and the epipolar point. By using the same partition structure, 3-D gaze prediction model can be inferred by solving a nonlinear optimization problem, which aims to minimize the angular disparities between training visual directions and prediction ones. Meanwhile, it is necessary to form a good starting point and suitable constraints for the optimization problem. Otherwise, it may end up with trivial solutions, i.e., faraway eye positions. To facilitate the practical implementation of our proposed method, we also analyze how the spatial distribution of calibration points impacts the model learning accuracy. The experiment results justify the effectiveness of our proposed gaze estimation method for both the normal vision and eyewear users.",,,
10.1109/ICCVW.2019.00568,2019,"Chaudhary, Aayush K.; Kothari, Rakshit; Acharya, Manoj; Dangi, Shusil; Nair, Nitinraj; Bailey, Reynold; Kanan, Christopher; Diaz, Gabriel; Pelz, Jeff B.",RITnet: Real-time Semantic Segmentation of the Eye for Gaze Tracking,"Accurate eye segmentation can improve eye-gaze estimation and support interactive computing based on visual attention; however, existing eye segmentation methods suffer from issues such as person-dependent accuracy, lack of robustness, and an inability to be run in real-time. Here, we present the RITnet model, which is a deep neural network that combines U-Net and DenseNet. RITnet is under 1 MB and achieves 95.3% accuracy on the 2019 OpenEDS Semantic Segmentation challenge. Using a GeForce GTX 1080 Ti, RITnet tracks at > 300Hz, enabling real-time gaze tracking applications. Pre-trained models and source code are available (1).",,,
,,IYER V V; KRISHNAKUMAR K,"IHS for EGT handoff, includes a processor and a memory coupled to the processor and where memory has program instructions stored that upon execution causes the IHS to detect an event during a virtual, augmented, or mixed reality application","NOVELTY - The Information Handling System (IHS) (103) includes a processor (301) and a memory (305) coupled to the processor. The memory has program instructions stored that upon execution by the processor causes the IHS to detect an event during a virtual, augmented, or mixed reality (XR) application. A user wears a Head-Mounted Device (HMD) consisting of a first Eye-Gaze Tracking (EGT) system. The user operates an external display having a second EGT system. The first or second EGT system is selected in response to the event. The first and second EGT systems each have a visual spectrum camera or a near-infrared (NIR) camera. The first and second EGT systems each employ a three-dimensional (3D) model gaze estimation or a two-dimensional (2D) model gaze estimation. USE - Information Handling Systems (IHSs) for Eye-Gaze Tracking (EGT) handoff. ADVANTAGE - The IHS generally processes, compiles, stores, and/or communicates information or data for business, personal, or other purposes thus allowing users to take advantage of the value of the information. The HMD transmits information to the host IHS regarding the state of the user, which in turn enables the host IHS to determine which image or frame to show to the user next, and from which perspective, as the user moves in space. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for(1) a method for EGT handoff; and(2) a hardware memory having program instructions stored. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the electronic components of the IHS.IHS (103)Processor (301)Chipset (302)Memory (305)Network interface (309)",,,
,,ZHU Y; SUN W; LI J,"Apparatus for eye gaze tracking, has processors which are configured to reconstruct three dimensional (3D) point cloud based on symmetry plane, and detect and track eye gaze of face of person based on reconstructed 3D point cloud","NOVELTY - The apparatus has a visible wavelength camera, an infrared (IR) camera, and several processors (108) are configured to generate a three-dimensional (3D) point cloud of a person's face from IR data captured from the IR camera. A two-dimensional image of the person's face is generated from visible wavelength data captured from the visible wavelength camera. A symmetry plane of the person's face is detected based on the 3D point cloud and the two-dimensional image. The symmetry plane divides the 3D point cloud into two portions. The 3D point cloud is reconstructed based on the symmetry plane. The eye gaze of the person's face is detected and tracked based on the reconstructed 3D point cloud. USE - Apparatus for eye gaze tracking. ADVANTAGE - The computing time and power can be saved. The electronic device might highlight the option in response to the user gazing at the option, and maintain the highlighting while the user keeps their gaze fixed on the option. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for eye gaze tracking; and(2) an electronic device for detecting eye gaze. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an electronic device for eye gaze tracking.Electronic device (100)Display screen (102)Capture device (104)Processor (108)Memory (116)",,,
10.1109/TIM.2021.3065437,2021,"Liu, Meng; Li, Youfu; Liu, Hai",Robust 3-D Gaze Estimation via Data Optimization and Saliency Aggregation for Mobile Eye-Tracking Systems,"In order to precisely predict 3-D gaze points, calibration is needed for each subject prior to first use the mobile gaze tracking system. However, traditional calibration methods normally expect the user to stare at predefined targets in the scene, which is troublesome and time-consuming. In this study, we proposed a novel method to remove the explicit user calibration and achieve robust 3-D gaze estimation in the room-scale area. Our proposed framework treats salient regions in the scene as possible 3-D locations of gaze points. To improve the efficiency of predicting 3-D gaze from visual saliency, the bag-of-word algorithm is adopted for eliminating redundant scene image data based on their similarities. After the elimination, saliency maps are generated from those scene images, and the geometrical relationship among the scene and eye cameras is obtained through aggregating 3-D salient targets with eye visual directions. Finally, we calculate the 3-D point of regard (PoR) by utilizing 3-D structures of the scene. The experimental results indicate that our method enhances the reliability of saliency maps and achieves promising performances on 3-D gaze estimation with different subjects.",,,
10.1109/TCE.2019.2899869,2019,"Lemley, Joseph; Kar, Anuradha; Drimbarean, Alexandru; Corcoran, Peter",Convolutional Neural Network Implementation for Eye-Gaze Estimation on Low-Quality Consumer Imaging Systems,"Accurate and efficient eye gaze estimation is important for emerging consumer electronic systems, such as driver monitoring systems and novel user interfaces. Such systems are required to operate reliably in difficult, unconstrained environments with low power consumption and at minimal cost. In this paper, a new hardware friendly, convolutional neural network (CNN) model with minimal computational requirements is introduced and assessed for efficient appearance-based gaze estimation. The model is tested and compared against existing appearance-based CNN approaches, achieving better eye gaze accuracy with significantly fewer computational requirements.",,,
10.3390/s18072292,2018,"Wan, Zijing; Wang, Xiangjun; Zhou, Kai; Chen, Xiaoyun; Wang, Xiaoqing",A Novel Method for Estimating Free Space 3D Point-of-Regard Using Pupillary Reflex and Line-of-Sight Convergence Points,"In this paper, a novel 3D gaze estimation method for a wearable gaze tracking device is proposed. This method is based on the pupillary accommodation reflex of human vision. Firstly, a 3D gaze measurement model is built. By uniting the line-of-sight convergence point and the size of the pupil, this model can be used to measure the 3D Point-of-Regard in free space. Secondly, a gaze tracking device is described. By using four cameras and semi-transparent mirrors, the gaze tracking device can accurately extract the spatial coordinates of the pupil and eye corner of the human eye from images. Thirdly, a simple calibration process of the measuring system is proposed. This method can be sketched as follows: (1) each eye is imaged by a pair of binocular stereo cameras, and the setting of semi-transparent mirrors can support a better field of view; (2) the spatial coordinates of the pupil center and the inner corner of the eye in the images of the stereo cameras are extracted, and the pupil size is calculated with the features of the gaze estimation method; (3) the pupil size and the line-of-sight convergence point when watching the calibration target at different distances are computed, and the parameters of the gaze estimation model are determined. Fourthly, an algorithm for searching the line-of-sight convergence point is proposed, and the 3D Point-of-Regard is estimated by using the obtained line-of-sight measurement model. Three groups of experiments were conducted to prove the effectiveness of the proposed method. This approach enables people to obtain the spatial coordinates of the Point-of-Regard in free space, which has great potential in the application of wearable devices.",,,
,,SEOK Y C; LEE T H; CHOI S D; LEE T; CHOI S; SHI Y; LI T; CUI S; SUK Y C,"System for tracking eye gaze of user in three-dimensional space, has eye tracking unit for estimating location of area gazed by user and mapping spatial coordinate corresponding to location of area on three-dimensional map","NOVELTY - The system (100) has a data collector (110) configured to acquire face information of a user and location information of the user from an image photographed by a photographing apparatus (104) installed at multiple points set in a three-dimensional (3D) space. An eye tracking unit (120) estimates a location of an area gazed by the user on the 3D space from the face information and the location information, and maps a spatial coordinate corresponding to the location of the area on a 3D map corresponding to the 3D space, where the face information includes multiple face positions, a pupil position, a face vector, and a pupil vector of the user. USE - System for tracking eye gaze of a user in a 3D space. ADVANTAGE - The system acquires the face information of the user and the location information of the user from the image photographed by the photographing apparatus and predicts the location of the area gazed by the user on the 3D space, so that the eye of the user can be accurately tracked in the 3D space. The system acquires the user's face information and the location information of the user photographed by the photographing apparatus and predicts a user's movement trajectory and gaze trajectory for seeing the eye movements in an accurate manner. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for tracking an eye gaze of a user in a 3D space. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a system for tracking eye gaze of a user in a 3D space. '(Drawing includes non-English language text)'System for tracking eye gaze of user in 3D space (100)Gaze tracking device (102)Photographing apparatus (104)Data collector (110)Eye tracking unit (120)",,,
10.3390/s20133739,2020,"Cazzato, Dario; Leo, Marco; Distante, Cosimo; Voos, Holger",When I Look into Your Eyes: A Survey on Computer Vision Contributions for Human Gaze Estimation and Tracking,"The automatic detection of eye positions, their temporal consistency, and their mapping into a line of sight in the real world (to find where a person is looking at) is reported in the scientific literature as gaze tracking. This has become a very hot topic in the field of computer vision during the last decades, with a surprising and continuously growing number of application fields. A very long journey has been made from the first pioneering works, and this continuous search for more accurate solutions process has been further boosted in the last decade when deep neural networks have revolutionized the whole machine learning area, and gaze tracking as well. In this arena, it is being increasingly useful to find guidance through survey/review articles collecting most relevant works and putting clear pros and cons of existing techniques, also by introducing a precise taxonomy. This kind of manuscripts allows researchers and technicians to choose the better way to move towards their application or scientific goals. In the literature, there exist holistic and specifically technological survey documents (even if not updated), but, unfortunately, there is not an overview discussing how the great advancements in computer vision have impacted gaze tracking. Thus, this work represents an attempt to fill this gap, also introducing a wider point of view that brings to a new taxonomy (extending the consolidated ones) by considering gaze tracking as a more exhaustive task that aims at estimating gaze target from different perspectives: from the eye of the beholder (first-person view), from an external camera framing the beholder's, from a third-person view looking at the scene where the beholder is placed in, and from an external view independent from the beholder.",,,
,,PARK K R; YOON H; RIZWAN A N,"Device for tracking gaze of vehicle driver based on deep learning, has extraction unit outputting deep learning feature set of region images, and driver gaze tracking unit classifying driver gaze area and storing gaze area feature sets","NOVELTY - The device (100) has a driver image input unit (110) for receiving a driver image. A feature region detection unit (120) tracks face feature identification points in the input driver image and extracts a face region image, a left eye region image, and a right eye region image based on the detected face feature identification points. A deep learning feature extraction unit (130) outputs a deep learning feature set of region images by using the face region image, the left eye region image, and the right eye region image. A driver gaze tracking unit (140) classifies a driver gaze area by using the output feature set and stored gaze area feature sets. A deep learning unit learns face gaze tracking using deep learning based convolutional neural networks (CNNs). USE - Device for tracking gaze of a vehicle driver based on deep learning. ADVANTAGE - The device improves gaze tracking accuracy of the vehicle driver based on the deep learning in consideration of head movement and eye movement of the driver without requiring an initial user calibration step. The device accurately performs driver gaze tracking to determine drowsy driving, distraction, and stare of the driver when converting from autonomous driving to driver driving in an autonomous vehicle, and provides accurate driver status information. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for tracking gaze of a vehicle driver based on deep learning(2) a computer program comprising a set of instructions for tracking gaze of a vehicle driver based on deep learning. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a device for tracking gaze of a vehicle driver based on deep learning. '(Drawing includes non-English language text)'Device for tracking gaze of vehicle driver based on deep learning (100)Driver image input unit (110)Feature region detection unit (120)Deep learning feature extraction unit (130)Driver gaze tracking unit (140)",,,
,2019,"Lambe, Eoin; Cuffe, Paul",Design and Test of a Headset Activation Platform Controlled by Eye Gaze Tracking,"For years, eye tracking technology has been making huge improvements and has become very popular in the robotics industry. This paper covers a new concept resulting in the creation of a fully functioning prototype utilising eye tracking technology that can aid with such things as improved visibility or better communication for people in the workplace or at home. The concept involves a portable headset that can track the user's gaze direction and accurately point an illumination source at the point of eye fixation. Use cases for such a device could include assisting people working in low light conditions by directing a beam of light in the gaze direction or for people living with disabilities that possess substandard verbal communication skills, by acting as an eye-controlled laser pointer. The prototype uses digital servo motors, a 3D printed pan-tilt platform, a microcontroller, an illumination source and off-the-shelf eye-tracking glasses. The result is a responsive and accurate device that could potentially be transformative for many people around the globe.",,,
,,HEE K L; BYOUNG H H; SANG W C; IN W H; HYUN C K; JEONG I S; IN J L,"Device for tracing eye gaze of user, has narrow angle camera control unit for controlling narrow angle camera, and image acquisition part for obtaining infrared ray image to remove lamp diffuse reflection, which is generated by tool","NOVELTY - The device has a coordinate determining unit (102) for determining three dimensional (3D) coordinate about an eye point of a user (112) according to forward direction movement of a user. A narrow angle camera control unit (103) controls a narrow angle camera (108) for obtaining the image associated with a user eye according to the 3D coordinate. An illumination control part (104) controls flash of a lamp (107) by utilizing lamp trigger signal. An image acquisition part (105) obtains an infrared (IR) ray image to remove lamp diffuse reflection, which is generated by a time auxiliary tool. USE - Device for tracing eye gaze of a user. ADVANTAGE - The device traces eye point of the user when the user is freely moved, and controls direction of the camera through pan/tilt/zoom/focus/iris apparatus as real-time so as to broad motion range of the user when eye gaze tracking is performed and secure a high resolution eye image. The device eliminates lamp diffuse reflection from being generated in wearing glasses so as to facilitate stable eye gaze tracking in wearing glasses. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an eye gaze tracing method. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of an eye gaze tracing device. '(Drawing includes non-English language text)'Coordinate determining unit (102)Narrow angle camera control unit (103)Illumination control part (104)Image acquisition part (105)Lamp (107)Narrow angle camera (108)User (112)",,,
,2017,"Soccini, Agata Marta",Gaze Estimation Based on Head Movements in Virtual Reality Applications using Deep Learning,"Gaze detection in Virtual Reality systems is mostly performed using eye-tracking devices. The coordinates of the sight, as well as other data regarding the eyes, are used as input values for the applications. While this trend is becoming more and more popular in the interaction design of immersive systems, most visors do not come with an embedded eye-tracker, especially those that are low cost and maybe based on mobile phones.We suggest implementing an innovative gaze estimation system into virtual environments as a source of information regarding users intentions.We propose a solution based on a combination of the features of the images and the movement of the head as an input of a Deep Convolutional Neural Network capable of inferring the 2D gaze coordinates in the imaging plane.",,,
,,KLINGSTROEM A; BRAND M,"Eye gaze tracking method of three-dimensional display, involves obtaining reference distance based on three-dimensional position of object, and calculating updated convergence distance using obtained gaze convergence and reference distances","NOVELTY - The method involves visualizing the objects using a three-dimensional (3D) display (210) each at a known 3D position, determining an object of the visualized objects at which a user is watching based on a gaze point, obtaining a gaze convergence distance indicative of a depth the user is watching at, obtaining a reference distance based on the 3D position of the determined object, and calculating an updated convergence distance using the obtained gaze convergence distance and the reference distance. The focal-plane of a multifocal 3D display is selected using the updated convergence distance. USE - Eye gaze tracking method of three-dimensional display. ADVANTAGE - Reduces the effects of vergence-accommodation conflict, that the time required to identify a stereoscopic stimulus is reduced, increases stereo-acuity in a time-limited task, reduces the distortions in perceived depth, and reduces user fatigue and discomfort. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a computer; and(2) a computer program. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of the convergence distance calculated based on visual axes.Three-dimensional display (210)Depth (310A,310B,310C)First eye (320A)Second eye (320B)Convergence point (330)",,,
,2017,"Alruwaythi, Omar F.; Sears, Matthew H.; Goodrum, Paul M.",The Impact of Engineering Information Formats on Craft Worker Eye Gaze Patterns,"Productivity of craft workers in construction projects can be negatively impacted by ineffective communication of the project's design and information. Traditional delivery of Mechanical, Electrical, and Piping (MEP) designs rely on two-dimensional isometric drawings. Developments in three-dimensional (3D) computer aided design (CAD) and 3D printing have provided new format options for delivering engineering information, however providing them to crafts for use at the construction workface remains relatively rare. The objective of this research is to understand how eye gaze patterns of construction craft workers are influenced by information formats when completing complex spatial tasks. A series of field trials with MEP workers was conducted to examine the influence of information format on their eye gaze patterns in completing a scale model task assembly. Participants were provided eye tracking glasses with one of three information formats: traditional, two-dimensional (2D) isometric drawings; 2D isometric drawings supplemented with a three-dimensional (3D) image of the assembly; and 2D isometric drawings supplemented with a 3D physical model of the assembly. Eye gaze patterns were examined to understand if performance can be improved by alternative formats of information displays and/or training.",,,
10.1109/TIM.2020.2983525,2020,"Bottos, Stephen; Balasingam, Balakumar",Tracking the Progression of Reading Using Eye-Gaze Point Measurements and Hidden Markov Models,"In this article, we consider the problem of tracking the eye gaze of individuals while they engage in reading. In particular, we develop the ways to accurately track the line being read by an individual using commercially available eye-tracking devices. Such an approach will enable futuristic functionalities, such as comprehension evaluation, interest level detection, and user-assisting applications such as hand-free navigation and automatic scrolling. Furthermore, the proposed approach will pave the way to develop technology that may generate valuable feedback to content makers, such as web designers, authors, educators, and social media users. The existing commercial eye trackers provide an estimated location of the eye-gaze points every few milliseconds. However, these estimated gaze points are not sufficient to quantify reading progression-a specific eye-gaze activity. In this article, we propose algorithms to bridge the commercial gaze tracker outputs and informative eye-gaze patterns while reading. The proposed system consists of Kalman filters and hidden Markov models to parameterize these statistical models and to accurately detect the line being read. The proposed approach is shown to yield an improvement of 27.1% in line detection accuracy over line tracking using estimated eye-gaze points alone by the eye tracker.",,,
10.3390/s20020543,2020,"Brousseau, Braiden; Rose, Jonathan; Eizenman, Moshe",Hybrid Eye-Tracking on a Smartphone with CNN Feature Extraction and an Infrared 3D Model,"This paper describes a low-cost, robust, and accurate remote eye-tracking system that uses an industrial prototype smartphone with integrated infrared illumination and camera. Numerous studies have demonstrated the beneficial use of eye-tracking in domains such as neurological and neuropsychiatric testing, advertising evaluation, pilot training, and automotive safety. Remote eye-tracking on a smartphone could enable the significant growth in the deployment of applications in these domains. Our system uses a 3D gaze-estimation model that enables accurate point-of-gaze (PoG) estimation with free head and device motion. To accurately determine the input eye features (pupil center and corneal reflections), the system uses Convolutional Neural Networks (CNNs) together with a novel center-of-mass output layer. The use of CNNs improves the system's robustness to the significant variability in the appearance of eye-images found in handheld eye trackers. The system was tested with 8 subjects with the device free to move in their hands and produced a gaze bias of 0.72 degrees. Our hybrid approach that uses artificial illumination, a 3D gaze-estimation model, and a CNN feature extractor achieved an accuracy that is significantly (400%) better than current eye-tracking systems on smartphones that use natural illumination and machine-learning techniques to estimate the PoG.",,,
,,RYU K J,"Machine learning-based reading ability diagnosis device using gaze tracking, has diagnostic element information generator and diagnosis result information generation unit that is configured to generate diagnosis result information","NOVELTY - The device (300) has a diagnostic element information generator which is configured to generate diagnostic element information included with reading process gaze information, reading process search information, reading comprehension information, and reading ability abnormality information by analyzing diagnostic document information included with the eye tracking data and the problem solving result data, metadata about the diagnostic document, and setting information of a region of interest. The user information, the diagnostic document information, and the diagnostic element information are inputted into a reading ability diagnosis classification model learned in advance with a machine learning algorithm, and gaze-based reading ability diagnosis information. The diagnosis result information generation unit is configured to generate diagnosis result information included with solution-based reading ability diagnosis information and gaze-based cognitive risk diagnosis information. USE - Machine learning-based reading ability diagnosis device using gaze tracking. ADVANTAGE - The reading process is included with reading fluency, exploration efficiency, and concentration. The reliability of the diagnosis result is improved through diagnosis method. The risk of ocular movement-related disorders is diagnosed. The user reading ability is easily, quickly and continuously diagnosed and managed, and the diagnosis results of other users are easily collected and utilized. The accuracy of diagnosing reading ability through machine learning algorithms is improved. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a machine learning-based reading ability diagnosis method using gaze tracking; and(2) a computer program stored on medium for diagnosing machine learning-based reading ability. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram illustrating the configuration of apparatus for diagnosing reading ability. (Drawing includes non-English language text)Machine learning-based reading ability diagnosis device (300)Control unit (310)Transmission and reception management unit (320)Reading ability diagnosis unit (330)Database (340)",,,
10.1061/(ASCE)CO.1943-7862.0001706,2019,"Alruwaythi, Omar; Goodrum, Paul",A Difference in Perspective: Impact of Different Formats of Engineering Information and Spatial Cognition on Craft-Worker Eye-Gaze Patterns,"Individuals vary widely in their ability to translate complex spatial information into performing physical tasks. Much of the variability can be explained by their experience in using different information formats and differences in individual spatial cognition. Traditional delivery of mechanical, electrical, and plumbing (MEP) designs rely on two-dimensional isometric drawings. Advancements in three-dimensional (3D) computer-aided design (CAD), augmented reality, virtual reality, and 3D printing have provided new format options for delivering engineering information; however, providing them to crafts for use at the construction workface remains relatively rare. The objective of this research is to understand how eye-gaze patterns of construction craft workers is influenced by information formats and spatial cognition when building a complex spatial task. A series of field trials with MEP workers was conducted to examine the influence of information format and spatial cognition on their eye-gaze patterns in building a scale-model pipe assembly. Participants were provided eye-tracking glasses along with one of three information formats: two-dimensional (2D) isometric drawings, 2D isometric drawings supplemented with a 3D image of the assembly, and 2D isometric drawings supplemented with a 3D physical model of the assembly. Card rotation and cube comparison tests were administered to measure spatial cognition. The results of this paper reveal that the information format and spatial cognition significantly influenced workers' eye-gaze patterns; there are different gaze patterns for different information formats, and these differences in gaze patterns were associated with differences in spatial cognition abilities. Additionally, the improvement in performance when using different engineering information formats is associated with different eye-gaze patterns.",,,
10.1109/TNSRE.2020.2991675,2020,"Yaneva, Victoria; Le An Ha; Eraslan, Sukru; Yesilada, Yeliz; Mitkov, Ruslan",Detecting High-Functioning Autism in Adults Using Eye Tracking and Machine Learning,"The purpose of this study is to test whether visual processing differences between adults with and without high-functioning autism captured through eye tracking can be used to detect autism. We record the eye movements of adult participants with and without autism while they look for information within web pages. We then use the recorded eye-tracking data to train machine learning classifiers to detect the condition. The data was collected as part of two separate studies involving a total of 71 unique participants (31 with autism and 40 control), which enabled the evaluation of the approach on two separate groups of participants, using different stimuli and tasks. We explore the effects of a number of gaze-based and other variables, showing that autism can be detected automatically with around 74% accuracy. These results confirm that eye-tracking data can be used for the automatic detection of high-functioning autism in adults and that visual processing differences between the two groups exist when processing web pages.",,,
10.1109/ACCESS.2018.2867235,2018,"Kang, Mun-Cheon; Yoo, Cheol-Hwan; Uhm, Kwang-Hyun; Lee, Dae-Hong; Ko, Sung-Jea",A Robust Extrinsic Calibration Method f or Non-Contact Gaze Tracking in the 3-D Space,"In general, 3-D gaze tracking methods employ both a frontal-viewing camera and an eye-capturing camera facing the opposite direction to precisely estimate the point-of-regard (POR) in the 3-D space. The extrinsic calibration of these two cameras for accurate 3-D gaze tracking is a challenging task. This paper presents a robust extrinsic calibration method for non-contact gaze tracking in the 3-D space. Even in a noisy environment, the extrinsic calibration parameters are precisely estimated by minimizing the proposed cost function consisting of both the angular and Euclidean errors. Furthermore, using the estimated parameters, the 3-D POR is exactly determined based on the two-view geometry. Compared with the conventional methods, the proposed method provides superior results in experiments considering various factors such as the noise level, head movement, and camera configuration. In real experiments, we achieved an average Euclidean error of 12.6 cm and the average angular error of 0.98 degrees when estimating the 3-D coordinates of PORs that were 4-8 m away from the user.",,,
,,LINDEN E,"Method for detecting three dimensional (3D) gaze based on deep learning system, involves generating 3D gaze information for user eye based on two dimensional (2D) gaze origin, 2D gaze direction, and corrected distance by eye tracking system","NOVELTY - The method involves generating (1104) a warped image centered around a user eye from a 2D image, by an eye tracking system based on the 2D image, and the 2D image is generated by a camera associated with the eye tracking system. The warped image is inputted (1110), by the eye tracking system to a neural network, and the neural network predicts a distance correction, a 2D gaze origin of the user eye in the warped image, and a 2D gaze direction of the user eye in the warped image based on the warped image. A corrected distance is generated (1114) between the user eye and the camera by updating an estimated distance based on the distance correction, by the eye tracking system. A 3D gaze information is generated (1116) for the user eye based on the 2D gaze origin, the 2D gaze direction, and the corrected distance, by the eye tracking system. USE - Method for detecting three dimensional (3D) gaze based on deep learning system using eye tracking system (claimed). ADVANTAGE - The eye tracking system provides a better understanding of the 3D gaze prediction techniques. The eye warped image input to the neural network is the same in terms of orientation, thus simplifying the architecture and the training of the neural network. The computer system iteratively updates the n calibration parameters while maintaining the network parameters through back-propagation until the loss function is minimized. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an eye tracking system; and(2) a non-transitory computer-readable storage medium storing program for detecting three dimensional (3D) gaze. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart illustrating the method for predicting 3D gaze based on a deep learning system.Step for generating a warped image (1104)Step for inputting the warped image (1110)Step for receiving the distance correction (1112)Step for generating a corrected distance (1114)Step for generating 3D gaze information (1116)",,,
,,FU Y; SUN B,"System for generating three-dimensional landmarks associated with object in two-dimensional image by element of neural network, has depth coordinate estimator receiving image and planar coordinates to estimate depth coordinate","NOVELTY - The system has a detector producing planar coordinates of landmarks at points of an object (110) i.e. face (112), in a two-dimensional (2D) image (106), where the planar coordinates include planar coordinate pairs. A depth coordinate estimator receives the 2D image and the planar coordinates to estimate a depth coordinate for each planar coordinate of each landmark to generate three-dimensional (3D) landmarks (108). The system body applies the 3D landmarks for face alignment, virtual face makeup, face recognition, eye gaze tracking and face synthesis. A feature-generating backbone includes a neural network element for employing a depthwise convolutional block and pointwise convolutional block, where the pointwise convolutional block receives features output by the depthwise convolutional block and output feature-generating-backbone output features. USE - System for generating 3D landmarks associated with an object i.e. face, in a 2D image by an element of a neural network (claimed). ADVANTAGE - The system reduces computational cost and parameters to retain generating accuracy, and resists accuracy loss caused by significant reduction of parameters. The system decreases computation and memory costs by enabling the neural network on a mobile device. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for generating 3D landmarks associated with an object in a 2D image(2) a non-transitory computer-readable medium comprising a set of instructions for generating 3D landmarks associated with an object in a 2D image(3) a neural network element(4) a neural network(5) an augmented reality system. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a mobile device with a neural network and a person using the mobile device.Mobile device (102)2D image (106)3D landmarks (108)Object (110)Face (112)",,,
10.3390/s18020456,2018,"Naqvi, Rizwan Ali; Arsalan, Muhammad; Batchuluun, Ganbayar; Yoon, Hyo Sik; Park, Kang Ryoung",Deep Learning-Based Gaze Detection System for Automobile Drivers Using a NIR Camera Sensor,"A paradigm shift is required to prevent the increasing automobile accident deaths that are mostly due to the inattentive behavior of drivers. Knowledge of gaze region can provide valuable information regarding a driver's point of attention. Accurate and inexpensive gaze classification systems in cars can improve safe driving. However, monitoring real-time driving behaviors and conditions presents some challenges: dizziness due to long drives, extreme lighting variations, glasses reflections, and occlusions. Past studies on gaze detection in cars have been chiefly based on head movements. The margin of error in gaze detection increases when drivers gaze at objects by moving their eyes without moving their heads. To solve this problem, a pupil center corneal reflection (PCCR)-based method has been considered. However, the error of accurately detecting the pupil center and corneal reflection center is increased in a car environment due to various environment light changes, reflections on glasses surface, and motion and optical blurring of captured eye image. In addition, existing PCCR-based methods require initial user calibration, which is difficult to perform in a car environment. To address this issue, we propose a deep learning-based gaze detection method using a near-infrared (NIR) camera sensor considering driver head and eye movement that does not require any initial user calibration. The proposed system is evaluated on our self-constructed database as well as on open Columbia gaze dataset (CAVE-DB). The proposed method demonstrated greater accuracy than the previous gaze classification methods.",,,
,,SONG Z; YE Y; ZHAO J,"Method for gaze tracking, involves determining direction of optical axis, and determining direction of visual axis according to angular difference between direction of optical axis and direction of optical visual axis","NOVELTY - The method involves obtaining (S101) the texture image and three-dimensional (3D) point cloud data of the user face. The corneal center coordinates and eyeball center coordinates of the user eyes is determined (S102) according to the texture image and the 3D point cloud data. The direction of the optical axis is determined (S103) according to the corneal center coordinates and the eyeball center coordinates. The direction of the visual axis is determined (S104) according to the angular difference between the direction of the optical axis and the direction of the optical visual axis. The eye center coordinates of the user is determined according to the eye area, the iris area and the 3D point cloud data. USE - Method for gaze tracking. ADVANTAGE - The accurate tracking of the visual axis direction is realized and the accuracy of eye control operation and interaction is improved to a certain extent. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a device for gaze tracking; and(2) a computer readable storage medium storing program for gaze tracking. DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart illustrating a method for gaze tracking. (Drawing includes non-English language text)Step for obtaining the texture image and 3D point cloud data of the user face (S101)Step for determining corneal center coordinates and eyeball center coordinates of the user eyes according to the texture image and the 3D point cloud data (S102)Step for determining direction of the optical axis according to the corneal center coordinates and the eyeball center coordinates (S103)Step for determining direction of the visual axis according to the angular difference between the direction of the optical axis and the direction of the optical visual axis (S104)",,,
10.1109/TIM.2019.2956612,2020,"Liu, Jiahui; Chi, Jiannan; Lu, Ning; Yang, Zuoyun; Wang, Zhiliang",Iris Feature-Based 3-D Gaze Estimation Method Using a One-Camera-One-Light-Source System,"Multicamera or multilight-source is generally used to estimate the 3-D coordinates of the cornea center for 3-D gaze estimation in the existing gaze trackers. Although some model-based methods use one-camera systems to estimate 3-D gazes, which include some user-dependent eye parameters preset by fixed values, a one-camera-one-light-source system is still unable to achieve 3-D gaze estimation by considering individual differences. In this article, an iris feature-based method using one camera and one light source for 3-D gaze estimation is proposed. The iris radius and the kappa angle are determined during user calibration. The optical axis is reconstructed by the iris center and its normal vector from only the iris features. Therefore, with the real-time estimation of kappa angle, the 3-D gaze is estimated by an optimization method. Computer simulations and practical experiments have been performed to analyze the feasibility and robustness of the proposed method. This article's innovation lies in achieving 3-D gaze estimation based on only one-camera and one-light source with calibrated eye-specific parameters, which breaks through the limitation of the existing methods using a 3-D cornea center on the system's hardware requirements of 3-D gaze estimation. The simplified hardware system has great application values, especially in the portable mobile devices widely used today. Moreover, a binocular model is used to optimize the results of the point of regard, which improves the accuracy of 3-D gaze estimation effectively.",,,
,2017,"Su, Dan; Li, You Fu; Guo, Yao",Precise Gaze Estimation for Mobile Gaze Trackers based on Hybrid Two-view Geometry,"In this paper, we propose a novel calibration framework for the gaze estimation of mobile gaze tracking systems. In our method, the user's eye and the eye camera are modeled as a central catadioptric camera. Thus the epipolar geometry of the mobile gaze tracker can be described by the hybrid two-view geometry. To calibrate this model, the user is asked to gaze at the calibration points distributed in 3-D space but not all located on one plane. In the light of binocular training data, we apply a 3x6 local hybrid-fundamental matrix to register pupil centers with epipolar lines in the scene image. Thus the image gaze point viewed from different depths can be uniquely determined as the intersection of two epipolar lines calculated by binocular data. The simulation and experimental results show the effectiveness of our proposed calibration framework for mobile gaze trackers.",,,
10.3145/epi.2020.nov.27,2020,"Silva-Torres, Juan-Jose; Martinez-Martinez, Luz; Cuesta-Cambra, Ubaldo",Design of a visual attention model for communication campaigns: the case of Covid-19,"Health is one of the main concerns of society. Empirical evidence underscores the growing importance of prevention and health education as a fundamental instrument to improve the quality of public health. Recent health crises, such as Ebola, influenza A, SARS, and Covid-19, have highlighted the importance of communication. When designing communication campaigns during a crisis, the speed of the creation of messages and their effectiveness have relevant social consequences. The objective of this work is to design and develop a mathematical tool, based on Machine Learning techniques, to enable predictions of areas of visual attention quickly and accurately without the use of eye-tracking technology. The methodology combines deep learning algorithms, to extract the characteristics of the images, and supervised modeling mathematical techniques, to predict the areas of attention. Validation is carried out by analyzing various institutional communications from the Covid-19 campaign, comparing the results with the areas of attention obtained using an eye-tracking solution with proven accuracy. The results obtained using the tool in the investigated Covid-19 communication pieces are analyzed, resulting in conclusions of interest for the development of new campaigns.",,,
,,KLINGSTROM A; BRAND M; KLINGSTROEM A,Calculating sight convergence distance using computer comprises making each of multiple objects to visible at known three-dimensional position and calculating updated convergence distance using obtained line-of-sight convergence distance,"NOVELTY - Calculating sight convergence distance using computer comprises making each of the multiple objects to visible at known three-dimensional position using three-dimensional display, determining the object that the user is looking at among the visible objects based on the gaze point, obtaining the convergence distance indicating the depth at which the user is watching, obtaining the reference distance based on the determined three-dimensional position of the object and calculating updated convergence distance using the obtained line-of-sight convergence distance and the reference distance, selecting the focal plane of the multi-focus three-dimensional display using the updated convergence distance. USE - The method is useful calculating sight convergence distance using computer (claimed). DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for computer program product comprising a set of instructions for calculating sight convergence distance. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation of the cross-sectional view of the eye.",,,
10.1109/CVPR.2019.00793,2019,"Xiong, Yunyang; Kim, Hyunwoo J.; Singh, Vikas",Mixed Effects Neural Networks (MeNets) with Applications to Gaze Estimation,"There is much interest in computer vision to utilize commodity hardware for gaze estimation. A number of papers have shown that algorithms based on deep convolutional architectures are approaching accuracies where streaming data from mass-market devices can offer good gaze tracking performance, although a gap still remains between what is possible and the performance users will expect in real deployments. We observe that one obvious avenue for improvement relates to a gap between some basic technical assumptions behind most existing approaches and the statistical properties of the data used for training. Specifically, most training datasets involve tens of users with a few hundreds (or more) repeated acquisitions per user The non i.i.d. nature of this data suggests better estimation may be possible if the model explicitly made use of such repeated measurements from each user as is commonly done in classical statistical analysis using so-called mixed effects models. The goal of this paper is to adapt these mixed effects ideas from statistics within a deep neural network architecture for gaze estimation, based on eye images. Such a formulation seeks to specifically utilize information regarding the hierarchical structure of the training data - each node in the hierarchy is a user who provides tens or hundreds of repeated samples. This modification yields an architecture that offers state of the art performance onvarious publicly available datasets improving results by 10-20%.",,,
,,REDZ A; OLLESSON N; MASKO D; IVARSSON M; LARSEN R A; OLSSON N,"Method for performing three-dimensional (3D) position estimation for cornea center, involves updating cornea movement filter (CMF) by setting estimated initial 3D position of cornea center to current 3D position of cornea center",NOVELTY - The method involves identifying (840) first candidate glint in first image captured by an imaging device at second time instance. Second candidate glint is identified (850) in the first image or in second image captured by the imaging device at third time instance and comprising the portion of the cornea of the eye and a glint generated by second illuminator. A pair of first and second candidate glints is selected (860) out of possible pairs of the first candidate glint and the second candidate glint having highest probability of corresponding to predicted first and second glint positions based on a probability estimation function using a processing circuitry. Current 3D position of the cornea is estimated (870) using the processing circuitry based on positions of the selected pair of first and second candidate glints. CMF is updated using the processing circuitry by setting estimated initial 3D position of a cornea center to the current 3D position of the cornea center. USE - Method for performing three-dimensional (3D) position estimation for cornea center of eye of user using remote eye tracking system (claimed). ADVANTAGE - The improved 3D cornea position estimation with regard to accuracy and reliability is achieved. The accurate false glint detection provides an accurate estimate of the cornea position. The eye tracking or gaze tracking result becomes more accurate since the tracking system is able to more reliably estimate the user 3D cornea position when the cornea is moving. The visual result and user experience are improved when the eye tracking or gaze tracking result becomes more accurate. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an eye tracking system for performing three-dimensional position estimation for cornea center of eye of user; and(2) a non-transitory computer-readable storage medium storing program for performing three-dimensional position estimation for cornea center of eye of user. DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart illustrating a method for performing 3D position estimation for cornea center of eye of user.Step for generating CMF comprising an estimated initial 3D position and an estimated initial 3D velocity of the cornea center of the eye at a first time instance (810)Step for identifying first candidate glint in first image captured by the imaging device at the second time instance (840)Step for identifying second candidate glint in the first image or in second image captured by the imaging device at a third time instance (850)Step for selecting a pair of first and second candidate glint out of possible pairs of first candidate glint and second candidate glint having the highest probability of corresponding to the predicted first and second glint positions based on a probability estimation function (860)Step for estimating the current 3D position of the cornea using the processing circuitry (870),,,
10.1049/iet-its.2020.0087,2020,"Rahman, Md Junaedur; Beauchemin, Steven S.; Bauer, Michael A.",Predicting driver behaviour at intersections based on driver gaze and traffic light recognition,"This work introduces and evaluates a model for predicting driver behaviour, namely turns or proceeding straight, at traffic light intersections from driver three-dimensional gaze data and traffic light recognition. Based on vehicular data, this work relates the traffic light position, the driver's gaze, head movement, and distance from the centre of the traffic light to build a model of driver behaviour. The model can be used to predict the expected driver manoeuvre 3 to 4 s prior to arrival at the intersection. As part of this study, a framework for driving scene understanding based on driver gaze is presented. The outcomes of this study indicate that this deep learning framework for measuring, accumulating and validating different driving actions may be useful in developing models for predicting driver intent before intersections and perhaps in other key-driving situations. Such models are an essential part of advanced driving assistance systems that help drivers in the execution of manoeuvres.",,,
10.3390/ijgi6090274,2017,"Liu, Bing; Dong, Weihua; Meng, Liqiu",Using Eye Tracking to Explore the Guidance and Constancy of Visual Variables in 3D Visualization,"An understanding of guidance, which means guiding attention, and constancy, meaning that an area can be perceived for what it is despite environmental changes, of the visual variables related to three-dimensional (3D) symbols is essential to ensure rapid and consistent human perception in 3D visualization. Previous studies have focused on the guidance and constancy of visual variables related to two-dimensional (2D) symbols, but these aspects are not well documented for 3D symbols. In this study, we used eye tracking to analyze the visual guidance from shapes, hues and sizes, and the visual constancy that is related to the shape, color saturation and size of 3D symbols in different locations. Thirty-six subjects (24 females and 12 males) participated in the study. The results indicate that hue and shape provide a high level of visual guidance, whereas guidance from size, a variable that predominantly guides attention in 2D visualization, is much more limited in 3D visualization. Additionally, constancy of shape and saturation are perceived with relatively high accuracy, whereas constancy of size is perceived with only low accuracy. These first empirical studies are intended to pave the way for a more comprehensive user understanding of 3D visualization design.",,,
,,MOLIN D; MARTINI T; GORDON M; DAVIES A; DANIELSSON O,"Method for detecting three-dimensional (3D) gaze using eye tracking system involves generating by eye tracking system, 3D gaze information for first eye of user based on two-dimensional (2D) gaze origin and 2D gaze direction","NOVELTY - The method involves obtaining (802) by a head pose prediction algorithm head pose parameter. A first eye image comprises a first eye of a user based on a first 2D image is generated (804) by the eye tracking system. The first eye image is inputted (806) by the eye tracking system to a neural network (NN). The obtained head pose parameter and the first feature vector is inputted by the eye tracking system to a concatenation layer comprised in the NN. The concatenation layer is configured to generate a resulting feature vector, by concatenating the obtained head pose parameter and the first feature vector. The generated resulting feature vector is inputted (814) by the eye tracking system to a fully connected (FC) module. The 3D gaze information for the first eye of the user based on the 2D gaze origin and the 2D gaze direction is generated (816) by the eye tracking system. USE - Method for detecting 3D gaze using eye tracking system. ADVANTAGE - The robust and reliable 3D gaze generation is obtained, more robust and reliable eye tracking is enabled and a high quality user experience is ensured. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an eye tracking system for detecting 3D gaze;(2) a non-transitory computer-readable storage medium storing program for detecting 3D gaze. DESCRIPTION Of DRAWING(S) - The drawing shows the flow chart of a method for predicting 3D gaze based on a deep learning system.Step for obtaining by a head pose prediction algorithm head pose parameter (802)Step for generating first eye image comprises a first eye of a user based on a first two dimensional image (804)Step for inputing to a neural network (806)Step for inputting by the eye tracking system to a fully connected (FC) module (814)Step for generating three-dimensional gaze information for the first eye of the user based on the two-dimensional gaze origin and the two-dimensional gaze direction (816)",,,
,2017,"Maimon-Dror, Roni O.; Fernandez-Quesada, Jorge; Zito, Giuseppe A.; Konnaris, Charalambos; Dziemian, Sabine; Faisal, A. Aldo",Towards free 3D end-point control for robotic-assisted human reaching using binocular eye tracking,"Eye-movements are the only directly observable behavioural signals that are highly correlated with actions at the task level, and proactive of body movements and thus reflect action intentions. Moreover, eye movements are preserved in many movement disorders leading to paralysis (or amputees) from stroke, spinal cord injury, Parkinson's disease, multiple sclerosis, and muscular dystrophy among others. Despite this benefit, eye tracking is not widely used as control interface for robotic interfaces in movement impaired patients due to poor human-robot interfaces. We demonstrate here how combining 3D gaze tracking using our GT3D binocular eye tracker with custom designed 3D head tracking system and calibration method enables continuous 3D end-point control of a robotic arm support system. The users can move their own hand to any location of the workspace by simple looking at the target and winking once. This purely eye tracking based system enables the end-user to retain free head movement and yet achieves high spatial end point accuracy in the order of 6 cm RMSE error in each dimension and standard deviation of 4 cm. 3D calibration is achieved by moving the robot along a 3 dimensional space filling Peano curve while the user is tracking it with their eyes. This results in a fully automated calibration procedure that yields several thousand calibration points versus standard approaches using a dozen points, resulting in beyond state-of-the-art 3D accuracy and precision.",,,
,,PATNEY A; SALVI M; KIM J; KAPLANYAN A S; WYMAN C R; BENTY N; LUEBKE D P; LEFOHN A E,"Method for rendering foveated images, involves filtering foveated image using contrast-enhancing filter to generate filtered foveated image","NOVELTY - The method (100) involves receiving (102) a three-dimensional scene. The 3D scene is rendered (104) according to a foveated rendering algorithm to generate a foveated image. The foveated image is filtered (106) using a contrast-enhancing filter to generate a filtered foveated image. The filtered foveated image is displayed (108) on a gaze-tracking display. The gaze-tracking display includes one or more sensors for determining a fixation point of a viewer. The fixation point is provided as input to the foveated rendering algorithm. USE - Method for rendering foveated images. ADVANTAGE - The foveated rendering algorithm is combined with temporal anti-aliasing techniques, so as to reduce artifacts in the foveated image. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system for rendering images; and(2) a non-transitory, computer-readable storage medium storing instructions for rendering images. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method for rendering a foveated image.Method for rendering images (100)Step for receiving a three-dimensional scene (102)Step for rendering the 3D scene according to a foveated rendering algorithm to generate a foveated image (104)Step for filtering the foveated image using a contrast-enhancing filter to generate a filtered foveated image (106)Step for displaying the filtered foveated image on a gaze-tracking display (108)",,,
10.1109/TIM.2020.3006681,2020,"Chi, Jiannan; Liu, Jiahui; Wang, Feng; Chi, Yingkai; Hou, Zeng-Guang",3-D Gaze-Estimation Method Using a Multi-Camera-Multi-Light-Source System,"The image of the center of a 3-D circular target is not the center of its imaging ellipse due to the imaging distortion of the spatial circular targets; therefore, traditional 3-D gaze-estimation methods with the multi-camera-multi-light-source (MCMLS) systems generally replace the projection of the pupil center with the virtual image of the pupil center for 3-D gaze estimation. However, this introduces a large gaze-estimation error when the oblique angle between the optical axis of the eye and the camera optical axis is large. To eliminate the error caused by using the virtual image of the pupil center, a 3-D gaze-estimation method using an MCMLS system is developed in this study. We first estimate the cornea center and then determine the matching points of the pupil imaging ellipse using the special polar plane. After the cornea radius and the kappa angle are calibrated, the optical axis of the eye is reconstructed by the refraction planes constructed by the edge points of the pupil imaging ellipses. Thus, the 3-D gaze is estimated by the real-time calculated transformation matrix represented by the kappa angle. The feasibility and performance of the method have been analyzed by the simulations and the experiments. Since the estimation of the spatial pupil center or the construction of the refraction plane using the virtual image of the pupil center is not required, the proposed method mitigates the inherent errors caused by optical axis reconstruction in the traditional methods and simplifies the algorithm for gaze estimation, which has a practical value.",,,
,,RUECKNER D,"Method for providing augmented reality feedback from physical hand-held camera system with dynamic camera field of view for, e.g. mobile gaze tracking, involves geometrically mapping camera field of view to field of view of user","NOVELTY - The method involves providing a head-mounted augmented reality display partially overlaying a dynamic field of view of a user which is independent of the field of view of the user. The camera field of view is mapped geometrically to the field of view of the user, using an estimation of relative poses that defines an epipolar geometry relationship between the camera field of view and the field of view of the user. The field of view of the user is augmented through the display by highlighting a representation of a boundary of the dynamic camera field of view over the field of view of the user. USE - Method for providing augmented reality feedback from physical hand-held camera system with dynamic camera field of view (claimed) for mobile gaze tracking, photo composition and cinematography, object tracking and scene segmentation, digital camera technology, computer vision, computational photography, mobile augmented reality (AR) and three-dimensional (3D) localization, mapping, and modeling. ADVANTAGE - The cameras can be controlled more quickly, while allowing transient events to be captured. The cameras can be controlled without touching them, which helps prevent bumps and allows gimbal-stabilized and remote cameras to be operated. The cameras can be operated by a user. By reducing the cognitive overhead, the interactions are made with cameras more efficient. By capturing extra image data rather than physically moving a camera, the photographers are given more flexibility after-the-fact. By avoiding physically moving components, the camera system is made more stable, cheaper, and less likely to break. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an augmented reality camera feedback system; and(2) a non-transitory computer readable medium storing instructions for providing augmented reality feedback from a physical hand-held camera system having a dynamic camera field of view. DESCRIPTION Of DRAWING(S) - The drawings show the front and back perspective view of a pair eye tracking glasses intended for active users.Gaze tracking glasses (100)Reference cameras (101)Camera (102)Frame (103)Infrared illuminating LED (104)",,,
10.1109/TCDS.2019.2959071,2021,"Guo, Jing; Liu, Yi; Qiu, Qing; Huang, Jie; Liu, Chao; Cao, Zhiguang; Chen, Yue",A Novel Robotic Guidance System With Eye-Gaze Tracking Control for Needle-Based Interventions,"The robotic technologies have been widely used in the operating room for decades. Among them, needle-based percutaneous interventions have attracted much attention from engineering and medical communities. However, the currently used robotic systems for interventional procedures are too cumbersome, requiring a large footprint in the operating room. Recently developed light-weight puncture robotic systems for needle positioning are able to reduce the size, but has the limitation of awkward ergonomics. In this article, we design a compact robotic guidance system that could accurately realize the needle position and orientation within the operating room. The eye-gaze tracking-based approach is proposed to control the position and orientation of the needle toward the desired location in a more intuitive manner, and the forward and inverse kinematics of this 4-DoF robot are analyzed. The robot operating system (ROS)-based experimental studies are performed to evaluate the needle placement accuracy during interventional therapy. The result indicated the effectiveness of the proposed robotic hardware and the eye-gaze-based control framework, which can achieve a distance error of the robot's end effector to the target point within 1 mm.",,,
10.1109/TBME.2017.2677902,2017,"Li, Songpo; Zhang, Xiaoli; Webb, Jeremy D.",3-D-Gaze-Based Robotic Grasping Through Mimicking Human Visuomotor Function for People With Motion Impairments,"Objective: The goal of this paper is to achieve a novel 3-D-gaze-based human-robot-interaction modality, with which a user with motion impairment can intuitively express what tasks he/she wants the robot to do by directly looking at the object of interest in the real world. Toward this goal, we investigate 1) the technology to accurately sense where a person is looking in real environments and 2) the method to interpret the human gaze and convert it into an effective interaction modality. Looking at a specific object reflects what a person is thinking related to that object, and the gaze location contains essential information for object manipulation. Methods: A novel gaze vector method is developed to accurately estimate the 3-D coordinates of the object being looked at in real environments, and a novel interpretation framework that mimics human visuomotor functions is designed to increase the control capability of gaze in object grasping tasks. Results: High tracking accuracy was achieved using the gaze vector method. Participants successfully controlled a robotic arm for object grasping by directly looking at the target object. Conclusion: Human 3-D gaze can be effectively employed as an intuitive interaction modality for robotic object manipulation. Significance: It is the first time that 3-D gaze is utilized in a real environment to command a robot for a practical application. Three-dimensional gaze tracking is promising as an intuitive alternative for human-robot interaction especially for disabled and elderly people who cannot handle the conventional interaction modalities.",,,
10.1051/matecconf/201925203021,2019,"Chmielewska, Magdalena; Dzienkowski, Mariusz; Bogucki, Jacek; Kocki, Wojciech; Kwiatkowski, Bartlomiej; Pelka, Jaroslaw; Tuszynska-Bogucka, Wioletta",Affective computing with eye-tracking data in the study of the visual perception of architectural spaces,"In the presented study the usefulness of eye-tracking data for classification of architectural spaces as stressful or relaxing was examined. The eye movements and pupillary response data were collected using the eye-tracker from 202 adult volunteers in the laboratory experiment in a well-controlled environment. Twenty features were extracted from the eye-tracking data and after the selection process the features were used in automated binary classification with a variety of machine learning classifiers including neural networks. The results of the classification using eye-tracking data features yielded 68% accuracy score, which can be considered satisfactory. Moreover, statistical analysis showed statistically significant differences in eye activity patterns between visualisations labelled as stressful or relaxing.",,,
,,LINDEN E,"Method for generating three-dimensional (3D) gaze predictions of person based on deep learning system, involves receiving prediction from neural network based on image and other calibration parameter","NOVELTY - The method involves updating the first calibration parameter and a network parameter of the neural network based on minimizing loss function of the neural network. The calibration image and second calibration parameter are input to the neural network. The calibration image showing an eye of the user based on image data is generated by a camera associated with an eye tracking system of the user. The three-dimensional (3D) gaze information for the user is generated upon completion of the calibrating. The image and the second calibration parameter are input to the neural network. The prediction is received from the neural network based on the image and the second calibration parameter. The prediction comprises a distance correction, a two-dimensional (2D) gaze origin of the eye of the user in the image, and 2D gaze direction of the eye of the user in the image. USE - Method for generating three-dimensional (3D) gaze predictions of person based on deep learning system. ADVANTAGE - During the training, the neural network learns to predict distance correction from the warped image rather that predicting the correction based on warped images around the user eyes and around the user face. The computer system iteratively updates the calibration parameters while maintaining the network parameters through backpropagation until the loss function is minimized. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the eye tracking system.Eye tracking system (100)Eye tracking module (110)Illuminator (111)Image sensor (113)Processor (120)",,,
,,SHOUSHTARI S H N; LIN K; TSIN Y,"Device for glint-assisted gaze tracking in a virtual reality head-mounted display comprises a camera that captures images of eye and a controller that has multiple processors, where controller is provided to estimate a cornea center of eye","NOVELTY - The device comprises a camera that captures images of an eye and a controller that has multiple processors, where the controller is provided to estimate a cornea center of the eye based on one of the images of the eye captured by the camera. A pupil center of the eye is estimated based on the image of the eye. An optical axis of the eye is reconstructed based on the cornea center and the pupil center. The optical axis passes through the cornea center and the pupil center. A visual axis of the eye is reconstructed based on the optical axis and a three dimensional (3D) geometric model of the eye. The visual axis passes through a fovea of the eye and the pupil center. USE - Device for glint-assisted gaze tracking in a virtual reality head-mounted display. ADVANTAGE - Virtual reality allows users to experience and interact with an immersive artificial environment, such that the user feels as if they were physically in that environment. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a device glint-assisted gaze tracking in a virtual reality head-mounted display.",,,
,2020,"Lollett, Catherine; Hayashi, Hiroaki; Kamezaki, Mitsuhiro; Sugano, Shigeki",A Robust Driver's Gaze Zone Classification using a Single Camera for Self-occlusions and Non-aligned Head and Eyes Direction Driving Situations,"Distracted driving is one of the most common causes of traffic accidents around the world. Recognizing the driver's gaze direction during a maneuver could be an essential step for avoiding the matter mentioned above. Thus, we propose a gaze zone classification system that serves as a base of supporting systems for driver's situation awareness. However, the challenge is to estimate the driver's gaze inside not ideal scenarios, specifically in this work, scenarios where may occur self-occlusions or non-aligned head and eyes direction of the driver. Firstly, towards solving miss classifications during self-occlusions scenarios, we designed a novel protocol where a 3D full facial geometry reconstruction of the driver from a single 2D image is made using the state-of-the-art method PRNet. To solve the miss classification when the driver's head and eyes direction are not aligned, eyes and head information are extracted. After this, based on a mix of different data pre-processing and deep learning methods, we achieved a robust classifier in situations where self-occlusions or non-aligned head and eyes direction of the driver occur. Our results from the experiments explicitly measure and show that the proposed method can make an accurate classification for the two before-mentioned problems. Moreover, we demonstrate that our model generalizes new drivers while being a portable and extensible system, making it easy-adaptable for various automobiles.",,,
,2019,"Elbattah, Mahmoud; Carette, Romuald; Dequen, Gilles; Guerin, Jean-Luc; Cilia, Federica",Learning Clusters in Autism Spectrum Disorder: Image-Based Clustering of Eye-Tracking Scanpaths with Deep Autoencode,"Autism spectrum disorder (ASD) is a lifelong condition characterized by social and communication impairments. This study attempts to apply unsupervised Machine Learning to discover clusters in ASD. The key idea is to learn clusters based on the visual representation of eye-tracking scanpaths. The clustering model was trained using compressed representations learned by a deep autoencoder. Our experimental results demonstrate a promising tendency of clustering structure. Further, the clusters are explored to provide interesting insights into the characteristics of the gaze behavior involved in autism.",,,
10.1016/bs.mie.2020.04.006,2020,"Jung, Kwan Ho; Zhang, Xin",Fluorogenic detection of protein aggregates in live cells using the AggTag method,"Protein aggregation is a process that occurs through the self-assembly of misfolded proteins to form soluble oligomers and insoluble aggregates. While there has been significant interest in protein aggregation for neurodegenerative diseases, progress in this field of research has been limited by the lack of effective methods to detect and interrogate these species in live cells. To resolve this issue, we have developed a new imaging method named the AggTag to report on protein aggregation in live cells with fluorescence microscopy. The AggTag method utilizes a genetic fusion of a protein of interest (POI) to a protein tag to conjugate with the AggTag probe, which contains a fluorophore that turns on its fluorescence upon interaction with protein aggregates. Unlike the conventional methods, this method enables one to detect soluble misfolded oligomers that were previously invisible. Furthermore, the AggTag method has been applied for the simultaneous detection of co-aggregation between two different POIs by a dual-color and orthogonal tagging system. This chapter aims to provide step-by-step procedures of the AggTag method for researchers who intend to study aggregation of POIs in mammalian cell lines.",,,
,,MISHRA A; VIJIL E C; NAGAR S; DEY K,"Computer-based method for handling dialogs based on user behavior data, involves extracting answer to input question based on input paragraph, input question, and predicted eye-gaze attributes of input paragraph and transmitting answer","NOVELTY - The method involves receiving an input paragraph comprising factual sentences (502) by a system comprises processors. Each of the factual sentences includes words. An input question (504) is received which comprises words by the system. A word-level gaze prediction is performed on the input paragraph to identify predicted eye-gaze attributes for the input paragraph by the system. An answer (526) is extracted to the input question based on the input paragraph, the input question, and the predicted eye-gaze attributes of the input paragraph by the system. The extracted answer is transmitted by the system. USE - Computer-based method for handling dialogs based on user behavior data. ADVANTAGE - The cloud computing is a model of service delivery for enabling convenient, on-demand network access to a shared pool of configurable computing resources that is rapidly provisioned and released with minimal management effort or interaction with a provider of the service. The cloud consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with the provider of the service. The capabilities are rapidly and elastically provisioned to quickly scale out and rapidly released to quickly scale in. The cloud computing environment is allowed to offer infrastructure, platforms, and software as services for which a cloud consumer does not need to maintain resources on a local computing device. The system and associated methods are provided that augment a memory network having a multitask-bidirectional long-short term memory (LSTM) neural network to predict eye-gaze activities of a human reader to improve a question answering (QA)ability of the system to comprehend an input factual paragraph and answer an input question. The user device is configured to allow users to send and receive information to user device from dialog system, which in turn allows users to access eye-gaze tracking component, and machine learning component. The encoded question vector is added with the weighted sum output helps the system remove redundant words and paraphrases that are present in both the question and fact and eventually identify words and phrases of an answer that are not present in the question. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a computer program product for handling dialogs based on human eye-gaze behavior data; and(2) a system for handling dialogs based on user behavior data. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a system architecture of an dialog system.Dialog system (500)Factual sentence (502)Input question (504)Memory encoder (516)Answer (526)",,,
10.1145/3267305.3267675,2018,"Herurkar, Dayananda; Ishimaru, Shoya; Dengel, Andreas",Combining Software-Based Eye Tracking and a Wide-Angle Lens for Sneaking Detection,"This paper proposes Sneaking Detector, a system which recognizes sneaking on a laptop screen by other people and alerts the owner through several interventions. We utilize a pre-trained deep learning network to estimate eye gaze of sneakers captured by a front-facing camera. Since most of the cameras equipped on laptop computers cannot cover a wide enough range, a commercial wide-angle lens attachment and an image processing are applied in our system. On the dataset involving nine participants following four experiments, it has been realized that our system can estimate the horizontal eye gaze and recognizes whether a sneaker is looking at a screen or not with 78% accuracy.",,,
,,LU Y; HAN Y; YANG Z; GONG L; QIAN J,"Eye tracking based quick sample image obtaining device, has information collecting device fixed on robot, and information processing device for constructing labeled image sample set according to needs of view image and gaze position","NOVELTY - The device has an information collecting device fixed on a robot and collecting a view image of the robot in real-time, where the view image of the robot is projected on a head-mounted display for an operator through an information processing device. A human eye gaze point tracking module monitors gaze position of the operator in real time in the view image according to relationship between eye gaze direction and the head-mounted display. The information processing device constructs labeled image sample set according to needs of the view image and the gaze position. USE - Eye tracking based quick sample image obtaining device. ADVANTAGE - The device quickly obtain the labeled image sample set in real time in large amount during construction of target detection, recognition and other deep learning models in the robot to complete corresponding functions. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an eye tracking based quick sample image obtaining device control method. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of an eye tracking based quick sample image obtaining device. '(Drawing includes non-English language text)'",,,
,,SIDDIQUI M M; RAY S J; SUNDARARAJAN A; BARDIA R; WEI Z; YUAN C,"System for performing pupil localization and gaze tracking, includes a control system operatively coupled to first and second image capture devices, and an initial pupil location for a first eye determined using a first image","NOVELTY - The system (900) includes an image capture device that has a first field of view. A first light emitter and a second light emitter are arranged on a first side of the image capture device and configured to emit light into the first field of view. A control system is operatively coupled to the first and second image capture devices. The control system is configured to cause the first light emitter and the second light emitter to alternately emit light. The image capture device captures a first image with a first eye concurrently with the first light emitter emitting light. The first image capture device captures a second image of the first eye concurrently with the second light emitter emitting light. It is determined that the first eye is less occluded by a glare in the first image than in the second image. An initial pupil location for the first eye is determined using the first image. USE - System for performing pupil localization and gaze tracking. ADVANTAGE - Once identified, the pupil's location is combined with 3 Dimension (3D) head pose information to generate an accurate and robust gaze detection mechanism. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a non-transitory computer readable medium comprising computer readable code to cause a first light emitter and a second light emitter to alternately emit light; and(2) a method for determining pupil location. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a system for performing pupil localization and gaze tracking.System (900)3d Pupil Triangulation Operation (920)Gaze Detection (925)Depth Sensor (930)Coarse Head Pose (935)",,,
10.1145/3290605.3300839,2019,"Wang, Xi; Ley, Andreas; Koch, Sebastian; Lindlbauer, David; Hays, James; Holmqvist, Kenneth; Alexa, Marc",The Mental mage Revealed by Gaze Tracking,"Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked, but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely recall an image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that these results generalize to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.",,,
10.1109/WACV48630.2021.00053,2021,"Zhang, Yifeng; Jiang, Ming; Zhao, Qi",Saliency Prediction with External Knowledge,"The last decades have seen great progress in saliency prediction, with the success of deep neural networks that are able to encode high-level semantics. Yet, while humans have the innate capability in leveraging their knowledge to decide where to look (e.g. people pay more attention to familiar faces such as celebrities), saliency prediction models have only been trained with large eye-tracking datasets. This work proposes to bridge this gap by explicitly incorporating external knowledge for saliency models as humans do. We develop networks that learn to highlight regions by incorporating prior knowledge of semantic relationships, be it general or domain-specific, depending on the task of interest. At the core of the method is a new Graph Semantic Saliency Network (GraSSNet) that constructs a graph that encodes semantic relationships learned from external knowledge. A Spatial Graph Attention Network is then developed to update saliency features based on the learned graph. Experiments show that the proposed model learns to predict saliency from the external knowledge and outperforms the state-of-the-art on four saliency benchmarks.",,,
10.1145/3314111.3319813,2019,"Goltz, Jonas; Grossberg, Michael; Etemadpour, Ronak",Exploring Simple Neural Network Architectures for Eye Movement ClassificationExploring Simple Neural Network Architectures for Eye Movement Classification,"Analysis of eye-gaze is a critical tool for studying human-computer interaction and visualization. Yet eye tracking systems only report eye-gaze on the scene by producing large volumes of coordinate time series data. To be able to use this data, we must first extract salient events such as eye fixations, saccades, and post-saccadic oscillations (PSO). Manually extracting these events is time-consuming, labor-intensive and subject to variability. In this paper, we present and evaluate simple and fast automatic solutions for eye-gaze analysis based on supervised learning. Similar to some recent studies, we developed different simple neural networks demonstrating that feature learning produces superior results in identifying events from sequences of gaze coordinates. We do not apply any ad-hoc post-processing, thus creating a fully automated end-to-end algorithms that perform as good as current state-of-the-art architectures. Once trained they are fast enough to be run in a near real time setting.",,,
,,LINDEN E,"Method for detecting three-dimensional gaze based on deep learning system, involves generating three-dimensional gaze information for user eye based on two-dimensional gaze origin, gaze direction and corrected distance by tracking system","NOVELTY - The method involves generating (1108) a warped image centered around a user eye from the two-dimensional (2D) image by an eye tracking system based on 2D image. The 2D image is generated by a camera associated with the eye tracking system. The warped image is inputted (1110) by the eye tracking system to a neural network. The neural network predicts a distance correction, a 2D gaze origin of the user eye in the warped image and a 2D gaze direction of the user eye in the warped image based on the warped image. A corrected distance is generated (1112) between the user eye and the camera by updating an estimated distance based on the distance correction by the eye tracking system. The three-dimensional gaze information for the user eye based on the 2D gaze origin, the 2D gaze direction and the corrected distance is generated (1114) by the eye tracking system. USE - Method for detecting three dimensional gaze based on deep learning system. ADVANTAGE - The computer system updates the calibration parameters while maintaining the network parameters through backpropagation until the loss function is minimized. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an eye tracking system for detecting three-dimensional gazegaze;(2) a non-transitory computer-readable storage medium storing program for detecting three-dimensional gaze; and(3) a computer program for detecting three-dimensional gaze. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for predicting three-dimensional gaze based on a deep learning system.Step for generating a warped image centered around a user eye from the two-dimensional image by an eye tracking system based on a two-dimensional image (1108)Step for inputting the warped image by the eye tracking system to a neural network (1110)Step for generating a corrected distance between the user eye and the camera by updating an estimated distance based on the distance correction by the eye tracking system (1112)Step for generating three-dimensional gaze information for the user eye based on the two-dimensional gaze origin, the two-dimensional gaze direction and the corrected distance by the eye tracking system (1114)Step for providing the three-dimensional gaze information to a three-dimensional gaze application (1120)",,,
,2017,"Afanaseva, Anastasia; Afonin, Sergey",On High-Precision Chessboard Detection on Static Scene Videos from Mobile Eye-Tracking Devices,"Eye-tracking analysis require annotation of a scene video by information on the target of the gaze. We develop a technique for automatic high-precision scene annotation for mobile head-mounted eye-trackers. The solution combines computer-vision techniques for scene recognition, 3-D modeling of the recognized objects, and head movement compensation using gyroscope and accelerometer information provided by the eye-tracking device. In this paper we address the problem of recognition of a chessboard, while the approach may be applied to other situations with static scenes.",,,
,,NAJAFI S S H; LIN K; TSIN Y,"Head-mounted display (HMD) for implementing gaze tracking system, has controller that estimates cornea center of users eye based on glint-light source matches and three-dimensional (3D) geometric models of components of HMD and users eye","NOVELTY - The HMD has that light sources that emit light towards a users eye and a camera that captures images of the users eye. A controller comprises processors to detect (700) glints in an image of the users eye captured by the camera in which each glint is a reflection of the light sources on a cornea surface of the user's eye. The detected glints are matched to corresponding light sources to generate glint-light source matches. The glint-light source matches are geometrically verified (708) in 3D space using 3D geometric models of components of the HMD and users eye. A cornea center of the users eye is estimated based on the glint-light source matches and the 3D geometric models of components of the HMD and users eye. USE - HMD for implementing gaze tracking system of virtual/augmented reality system for displaying visual content for viewing by for gaming, remotely controlling drone or other mechanical system, viewing digital media content and interacting with internet. ADVANTAGE - The interactive user experience is provided for multiple applications. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for operating HMD. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a glint-LED matching pipeline.Step for detecting glints (700)Step for filtering glints using filter position (702)Step for verifying glint matches geometrically in 3D space (708)Step for selecting glints for cornea center estimation (718)Step for estimating gaze (720)",,,
,,KIM M; KIM C; HAN L; KIM J; OH Y; JUNG S; HAN I,"Method for improving visibility using gaze tracking, involves moving virtual viewpoint of user that gazes at focus object along line of gaze of user, based on command of user","NOVELTY - The method involves detecting (S120) a gaze of a user by means of a camera of an electronic device. A focus object is determined (S130) at which user gazes from among one object. A second image is obtained with higher visibility than a first image. The second image with high visibility is displayed on a display of the electronic device. A virtual viewpoint of user is moved that gazes at the focus object along a line of the gaze of the user. The second image with high visibility is an actual image acquired from an actual three-dimensional (3D) map obtained by photographing cities or streets. USE - Method for improving visibility using gaze tracking for processing image photographed by camera of electronic device such as smartphone, portable phone, game player, TV, display unit, heads-up display unit for a vehicle, notebook computer, laptop computer, tablet Personal Computer (PC), Personal Media Player (PMP), Personal Digital Assistant (PDA). ADVANTAGE - The information providing method enlarges an existing image and displays text information, and to provide a high-visibility image by tracking a gaze of a user, determining a focus object, reproducing an image or replacing the image with a related image or the like, so as to enable the user to readily identify the object and to provide the user with plenty of visual information. The controller improves the matching speed and accuracy based on the information associated with the focus object received from the first electronic device or the second electronic device. The controller displays an image with high visibility by reducing the image with high visibility to be smaller than a corresponding object, or reduces a currently displayed image with high visibility and displays the reduced image. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a non-transitory machine-readable storage medium storing program for a method of improving visibility using gaze tracking; and(2) an electronic device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a visibility improvement method using gaze tracking.Step for detecting gaze of user (S110)Step for detecting gaze of user by means of camera of electronic device (S120)Step for determining focus object at which the user gazes from among one object (S130)Step for detecting focus object/area (S140)Step for searching for image and display retrieved image (S150)",,,
10.3390/s20174787,2020,"Kang, Dongwoo; Heo, Jingu",Content-Aware Eye Tracking for Autostereoscopic 3D Display,"This study develops an eye tracking method for autostereoscopic three-dimensional (3D) display systems for use in various environments. The eye tracking-based autostereoscopic 3D display provides low crosstalk and high-resolution 3D image experience seamlessly without 3D eyeglasses by overcoming the viewing position restriction. However, accurate and fast eye position detection and tracking are still challenging, owing to the various light conditions, camera control, thick eyeglasses, eyeglass sunlight reflection, and limited system resources. This study presents a robust, automated algorithm and relevant systems for accurate and fast detection and tracking of eye pupil centers in 3D with a single visual camera and near-infrared (NIR) light emitting diodes (LEDs). Our proposed eye tracker consists of eye-nose detection, eye-nose shape keypoint alignment, a tracker checker, and tracking with NIR LED on/off control. Eye-nose detection generates facial subregion boxes, including the eyes and nose, which utilize an Error-Based Learning (EBL) method for the selection of the best learnt database (DB). After detection, the eye-nose shape alignment is processed by the Supervised Descent Method (SDM) with Scale-invariant Feature Transform (SIFT). The aligner is content-aware in the sense that corresponding designated aligners are applied based on image content classification, such as the various light conditions and wearing eyeglasses. The conducted experiments on real image DBs yield promising eye detection and tracking outcomes, even in the presence of challenging conditions.",,,
10.1109/TNNLS.2020.2996386,2021,"Liu, Congcong; Chen, Yuying; Liu, Ming; Shi, Bertram E.",Using Eye Gaze to Enhance Generalization of Imitation Networks to Unseen Environments,"Vision-based autonomous driving through imitation learning mimics the behavior of human drivers by mapping driver view images to driving actions. This article shows that performance can be enhanced via the use of eye gaze. Previous research has shown that observing an expert's gaze patterns can be beneficial for novice human learners. We show here that neural networks can also benefit. We trained a conditional generative adversarial network to estimate human gaze maps accurately from driver-view images. We describe two approaches to integrating gaze information into imitation networks: eye gaze as an additional input and gaze modulated dropout. Both significantly enhance generalization to unseen environments in comparison with a baseline vanilla network without gaze, but gaze-modulated dropout performs better. We evaluated performance quantitatively on both single images and in closed-loop tests, showing that gaze modulated dropout yields the lowest prediction error, the highest success rate in overtaking cars, the longest distance between infractions, lowest epistemic uncertainty, and improved data efficiency. Using Grad-CAM, we show that gaze modulated dropout enables the network to concentrate on task-relevant areas of the image.",,,
10.1109/ISM.2020.00022,2020,"Hofbauer, Markus; Kuhn, Christopher B.; Puttner, Lukas; Petrovic, Goran; Steinbach, Eckehard",Measuring Driver Situation Awareness Using Region-of-Interest Prediction and Eye Tracking,"With increasing progress in autonomous driving, the human does not have to be in control of the vehicle for the entire drive. A human driver obtains the control of the vehicle in case of an autonomous system failure or when the vehicle encounters an unknown traffic situation it cannot handle on its own. A critical part of this transition to human control is to ensure a sufficient driver situation awareness. Currently, no direct method to explicitly estimate driver awareness exists. In this paper, we propose a novel system to explicitly measure the situation awareness of the driver. Our approach is inspired by methods used in aviation. However, in contrast to aviation, the situation awareness in driving is determined by the detection and understanding of dynamically changing and previously unknown situation elements. Our approach uses machine learning to define the best possible situation awareness. We also propose to measure the actual situation awareness of the driver using eye tracking. Comparing the actual awareness to the target awareness allows us to accurately assess the awareness the driver has of the current traffic situation. To test our approach, we conducted a user study. We measured the situation awareness score of our model for 8 unique traffic scenarios. The results experimentally validate the accuracy of the proposed driver awareness model.",,,
,,ZHOU K,"Machine learning based eye-control tracking method, involves performing feature deep learning training process, and obtaining analytic information by processing unit, and generating eye pointer according to analysis information","NOVELTY - The method involves establishing eye-control tracking mode to obtain tracked pupil by a camera. Eyelids shape characteristic of a head posture and is obtained. Combining feature data is obtained. The feature data is analyzed by using machine learning. Feature deep learning training process is performed to obtain classifier with strong recognition performance. Analytic information is obtained by a processing unit. Eye pointer is generated according to the analysis information. Starting instruction is obtained according to voice and touch command. The eye tracking control mode is started. USE - Machine learning based eye-control tracking method. ADVANTAGE - The method enables greatly improving efficiency, robustness of eye-tracking and detection success rate with 93%, and reducing system detecting cycle with in 38ms. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a machine learning based eye-control tracking method. '(Drawing includes non-English language text)'",,,
10.1016/j.eswa.2020.114037,2021,"Klaib, Ahmad F.; Alsrehin, Nawaf O.; Melhem, Wasen Y.; Bashtawi, Haneen O.; Magableh, Aws A.","Eye tracking algorithms, techniques, tools, and applications with an emphasis on machine learning and Internet of Things technologies","Eye tracking is the process of measuring where one is looking (point of gaze) or the motion of an eye relative to the head. Researchers have developed different algorithms and techniques to automatically track the gaze position and direction, which are helpful in different applications. Research on eye tracking is increasing owing to its ability to facilitate many different tasks, particularly for the elderly or users with special needs. This study aims to explore and review eye tracking concepts, methods, and techniques by further elaborating on efficient and effective modern approaches such as machine learning (ML), Internet of Things (IoT), and cloud computing. These approaches have been in use for more than two decades and are heavily used in the development of recent eye tracking applications. The results of this study indicate that ML and IoT are important aspects in evolving eye tracking applications owing to their ability to learn from existing data, make better decisions, be flexible, and eliminate the need to manually re-calibrate the tracker during the eye tracking process. In addition, they show that eye tracking techniques have more accurate detection results compared with traditional event-detection methods. In addition, various motives and factors in the use of a specific eye tracking technique or application are explored and recommended. Finally, some future directions related to the use of eye tracking in several developed applications are described.",,,
10.1016/j.wneu.2020.12.092,2021,"Khakhar, Rutvik; You, Fang; Chakkalakal, Denny; Dobbelstein, David; Picht, Thomas",Hands-free Adjustment of the Microscope in Microneurosurgery,"BACKGROUND: In microneurosurgery, the operating microscope plays a vital role. The classical neurosurgical operation is bimanual, that is, the microsurgical instruments are operated with both hands. Often, operations have to be carried out in narrow corridors at the depth of several centimeters. With current technology, the operator must manually adjust the field of view during surgery-which poses a disruption in the operating flow. Until now, technical adjuncts existed in the form of a mouthpiece to move the stereo camera unit or voice commands and foot pedals to control other interaction tasks like optical configuration. However, these have not been widely adopted due to usability issues. This study tests 2 novel hands-free interaction concepts based on head positioning and gaze tracking as an attempt to reduce the disruption during microneurosurgery and increase the efficiency of the user.METHODS: Technical equipment included the Pentero 900 microscope (Carl Zeiss Microscopy GmbH, Jena, Germany), HTC Vive Pro (HTC, Taoyuan District (HQ), Taiwan), and an inbuilt 3D-printed target probe. Eleven neurosurgeons including 7 residents and 4 consultants participated in the study. The tasks created for this study were with the intention to mimic real microneurosurgical tasks to maintain applicative accuracy while testing the interaction concepts. The tasks involved visualization system adjustment to the specific target and touching the target. The first trial was conducted in a virtual reality setting applying the novel hands-free interaction concepts, and the second trial was conducted performing the same tasks on a 3D-printed target probe using manual field of view adjustment. The participants completed both trials with the same predetermined tasks, in order to validate the feasibility of the novel technology. The data collected for this study were obtained with the help of review protocols, detailed post-trial interviews, video and audio recordings, along with time measurements while performing the tasks.RESULTS: The user study conducted at the Charite Hospital in Berlin found that the gaze-tracking and head-positioning- based microscope adjustment were 18% and 29% faster, respectively, than the classical bimanual adjustment of the microscope. Focused user interviews showed the users' proclivity for the new interaction concepts, as they offered minimal disruption between the simultaneous target selection and camera position adjustment.CONCLUSIONS: The hands-free interaction concepts presented in this study demonstrated a more efficient execution of the microneurosurgical tasks than the classical manual microscope and were assessed to be more preferable by both residents and consultant neurosurgeons.",,,
10.1145/3453988,2021,"Khan, Anam Ahmad; Newn, Joshua; Kelly, Ryan M.; Srivastava, Namrata; Bailey, James; Velloso, Eduardo",GAVIN: Gaze-Assisted Voice-Based Implicit Note-taking,"Annotation is an effective reading strategy people often undertake while interacting with digital text. It involves highlighting pieces of text and making notes about them. Annotating while reading in a desktop environment is considered trivial but, in a mobile setting where people read while hand-holding devices, the task of highlighting and typing notes on a mobile display is challenging. In this article, we introduce GAVIN, a gaze-assisted voice note-taking application, which enables readers to seamlessly take voice notes on digital documents by implicitly anchoring them to text passages. We first conducted a contextual enquiry focusing on participants' note-taking practices on digital documents. Using these findings, we propose a method which leverages eye-tracking and machine learning techniques to annotate voice notes with reference text passages. To evaluate our approach, we recruited 32 participants performing voice note-taking. Following, we trained a classifier on the data collected to predict text passage where participants made voice notes. Lastly, we employed the classifier to built GAVIN and conducted a user study to demonstrate the feasibility of the system. This research demonstrates the feasibility of using gaze as a resource for implicit anchoring of voice notes, enabling the design of systems that allow users to record voice notes with minimal effort and high accuracy.",,,
,2019,"Ferreira, Daniel S.; Ramalho, Geraldo L. B.; Medeiros, Fatima N. S.; Bianchi, Andrea G. C.; Cameiro, Claudia M.; Ushizima, Daniela M.",SALIENCY-DRIVEN SYSTEM WITH DEEP LEARNING FOR CELL IMAGE CLASSIFICATION,"This paper describes our automatic cell image classification algorithm that explores expert's eye tracking data combined to convolutional neural networks. Our framework selects regions of interest that attract cytologists attention, then it focuses computation on cell classification of these specific sub-images. Our contribution is to fuse deep learning to saliency maps from eye-tracking into an approach that bypasses segmentation to detect abnormal cells from Pap smear microscopy under real noisy conditions, artifacts and occlusion. Preliminary results show high classification accuracy of similar to 90% during tasks of locating and identifying critical cells within three levels: normal, low-risk disease and high-risk disease. We validate our results on 111 images containing 3,183 cells and obtained an average runtime of 4.5 seconds per image.",,,
,,KIM S H; LEE S H; SHI H K; SANG H L,"Apparatus i.e. head-mounted display, for replaying content, has and processing unit for collecting perspective and focus in content for stereoscopic three-dimensional displays based on calculated focal length corresponding to coordinate","NOVELTY - The apparatus has an operation unit for calculating a three-dimensional coordinate using gaze information obtained through gaze-tracking. An extraction unit extracts a focal length corresponding to the calculated three-dimensional coordinate. A processing unit collects a perspective and a focus (311-314) in a content for stereoscopic three-dimensional (3D) displays based on the calculated focal length. The operation unit calculates a three-dimensional coordinate corresponding to a two-dimensional coordinate by applying a picking technique to the obtained two-dimensional coordinate. USE - Apparatus i.e. head-mounted display, for replaying content. Uses include but are not limited to games, 3D content, education, teleconferencing, training, advanced programming, remote manipulation, remote satellite surface exploration, data analysis, and scientific visualization. ADVANTAGE - The apparatus improves accuracy of a gaze-tracking result when a distance between a user's pupil and a monitor is fixed due to characteristics of a wearable virtual reality device. The apparatus prevents dizziness due to mismatch by greatly adjusting inter-pupillary distance of a stereo camera when a target object is close to the stereo camera. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for operating an apparatus for replaying content. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of change in a focal point of an eye lens and a convergence angle of eyes including an apparatus for replaying content.Object (310)Focuses (311-314)",,,
10.1109/TCSVT.2019.2940479,2020,"Zhuang, Ning; Ni, Bingbing; Xu, Yi; Yang, Xiaokang; Zhang, Wenjun; Li, Zefan; Gao, Wen",MUGGLE: MUlti-Stream Group Gaze Learning and Estimation,"Being able to accurately predict the common gaze point of a group of persons is of particular interest to precise marketing and automatic group attention assessment. Group gaze estimation faces challenges including small face/head size and outlier observers. To address these challenges, we proposed a novel framework called Multi-stream Group Gaze Learning and Estimation (MUGGLE). The MUGGLE infrastructure includes two inference streams: 1) a holistic stream which utilizes fused attention map as input to a global deep convolutional structure to explore the global geometric configurations and contexts of interesting persons in the scene; and 2) an aggregative stream which robustly aggregates individual gazes via a recurrent structure (e.g., LSTM) to obtain outlier-tolerant estimation. Both streams are seamlessly integrated via a fusion network. Extensive experiments are performed on a fully annotated group gaze image dataset with 8,000+ images and 100,000+ faces (which is publicly releasable). The results demonstrate the effectiveness of the proposed MUGGLE framework in group gaze estimation.",,,
10.1109/CAC51589.2020.9327358,2020,"Wu, Jinghan; Lu, Meiqi; Lin, Yuping; Zhang, Xuetao",Scanpaths Generation for Target Search Based on Deep Learning,"Scanpath is a sequence of gaze fixations changing with time when browsing something, which records eyes' movement dynamically. Accurate prediction of scanpaths can help computers better predict which area human pay attention to and drive the development of next-generation systems that can understand human behavior and needs. Therefore, this paper introduces an algorithm of scanpaths generation for target search. Based on one-shot learning network, the algorithm extracts the target map with task information, and then imports it into a visual model of the superior colliculus to predict the task-drive scanpaths. The results are similar to the gaze behavior of human eyes in shape, direction and length. At the same time, the algorithm also has the ability to predict the scanpaths based on unseen categories.",,,
,,SUN X; ZHENG S,"Personality prediction method based on eye gaze thermogram, involves applying pre-stored deep learning model according to facial activity video to obtain distribution map of eye point of tested person","NOVELTY - The method involves obtaining (S1) a facial activity video when the test person views the stimulation video. The eye gaze heat map of the tested person is determined (S2) according to the facial activity video. The features are extracted (S3) from the eye gaze thermogram. The pre-trained personality prediction model is applied from the feature extracted from the eye gaze thermogram to obtain the personality prediction value of the tested person. A pre-stored deep learning model is applied (S4) according to the facial activity video to obtain a distribution map of the eye point of the tested person. USE - Personality prediction method based on eye gaze thermogram. ADVANTAGE - The personality prediction value of the tested person is obtained, thus realizing personality prediction based on the eye gaze heat map. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the Personality prediction method based on eye gaze thermogram. (Drawing includes non-English language text)Step for obtaining facial activity video when test person views stimulation video (S1)Step for determining eye gaze heat map of tested person according to facial activity video (S2)Step for extracting features from eye gaze thermogram (S3)Step for applying pre-stored deep learning model according to facial activity video to obtain distribution map of eye point of tested person (S4)",,,
,,LI Y; SU D,"Method for estimating gaze point using mobile HMGT, involves processing training data and determining local-learning base gaze estimation model based on training data for determining one or both of 2D gaze points and 3D gaze points",NOVELTY - The method involves processing training data and determining one or more local-learning base gaze estimation model based on the training data for determining one or both of two-dimensional (2D) gaze points in a scene image and three-dimensional (3D) gaze points in scene camera coordinates. The 2D gaze points are determined in the scene image. The 3D gaze points are determined in the scene camera coordinates. The data is obtained for determining the training data. The movement trajectories of pupil centers of a user eyes and movement trajectory of calibration point in a scene image at a first calibration depth are recorded. USE - Method for estimating gaze point using mobile head-mounted gaze trackers (HMGT). ADVANTAGE - The user is required to fixate at a single calibration point while smoothly rotating his/her head. The wide-angle range of head-rotation motion reduces the extrapolation risk in cases of large viewing fields and aids the better estimation on eye positions. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for gaze estimation. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a 3D point of regard (POR) determined by two visual axes and the corresponding 2D POR on an object.,,,
,,GORDON J C,"Method for modifying user interface based upon user's brain activity and gaze in eye tracking system, involves causing user interface provided by computing device to operate in accordance with selected user interface state","NOVELTY - The method involves training a machine learning model using data (202) identifying first user interface (UI) state for a computing device, data (106A) identifying first brain activity of a user (102), and data (109A) identifying first location of gaze of the user. Data (106B) identifying second brain activity of the user and data (109B) identifying second location of the gaze are received, while operating the computing device. The model, the second activity data, and the second location data are used to select second UI state. The UI is caused to operate in accordance with the second state. USE - Method for modifying a UI based upon a user's brain activity and gaze in a gaze tracking system i.e. eye tracking system, by a computing device. Uses include but are not limited to a laptop computer, desktop computer, tablet computing device, mobile telephone, smart phone, server computer, head mounted augmented reality display device and a head mounted virtual reality device. ADVANTAGE - The method enables generating or adjusting the state of the UI provided by the computing device based upon the user's current brain activity and gaze, thus permitting the computing device to be operated in more efficient manner, while reducing power consumption of the computing device, reducing number of processor cycles utilized by the computing device, and potentially extending battery life of the computing device. The method enables a transparent display to allow the wearer to view the visual content and actual surroundings of the wearer simultaneously. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) an apparatus for modifying a UI based upon a user's brain activity and gaze(2) a computer storage medium comprising a set of instructions for modifying a UI based upon a user's brain activity and gaze. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a mechanism for training a machine learning classifier to identify UI state based upon current brain activity of a user and user's gaze.User (102)Brain activity data (106A, 106B)Gaze data (109A, 109B)Biological data (110A, 110B)UI state data (202)",,,
,,CHAUDHURI A,"Distributed system for building several user profile, has video data processor that is provided with processor selected from tracking module, expression recognition module, facial recognition module, analysis module, and combination","NOVELTY - The system has a behavior learning processor that is configured for determining a behavioral response using a visual behavior data and an audio behavior data. A profile building system incorporates user profile data into user profile. The user profile data is provided with a visitor data and a behavioral response. The profile building system is provided with behavioral response analysis system for associating the behavioral response to the visitor data, where user profile data is provided with information and measurements about the visitor in a retail environment. A data communication network for communication between a profile building system, the behavior learning system, and the data input device. The video data processor is provided with a video data processor (110) selected from the group consisting of a gaze-tracking module (201), a facial expression recognition module (202), a facial recognition module (244), a demographic analysis module (203), and combination. USE - Distributed system for building user profiles. ADVANTAGE - The machine learning system refers to computerized systems with the ability to automatically learn and improve from experience without being explicitly programmed. The user profile helps the retailer to generate a customer profile, which allows the retailer to provide the customer with an enhanced or even customized experience. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for building user profile. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the data input device.Video data processor (110)Gaze-tracking module (201)Facial expression recognition module (202)Demographic analysis module (203)Facial recognition module (244)",,,
10.1108/IJCST-05-2016-0060,2017,"Stapleton, Timothy; Koo, Helen Sumin",Bicyclist biomotion visibility aids: a 3D eye-tracking analysis,"Purpose-The purpose of this paper is to investigate the effectiveness of biomotion visibility aids for nighttime bicyclists compared to other configurations via 3D eye-tracking technology in a blind between-subjects experiment.Design/methodology/approach-A total of 40 participants were randomly assigned one of four visibility aid conditions in the form of videos: biomotion (retroreflective knee and ankle bands), non-biomotion (retroreflective vest configuration), pseudo-biomotion (vertical retroreflective stripes on the back of the legs), and control (all-black clothing). Gaze fixations on a screen were measured with a 3D eye-tracking system; coordinate data for each condition were analyzed via one-way ANOVA and Tukey's post-hoc analyses with supplementary heatmaps. Post-experimental questionnaires addressed participants' qualitative assessments.Findings-Significant differences in eye gaze location were found between the four reflective clothing design conditions in X-coordinate values (p <0.01) and Y-coordinate values (p <0.05). Practical implications-This research has the potential to further inform clothing designers and manufacturers on how to incorporate biomotion to increase bicyclist visibility and safety. Social implications-This research has the potential to benefit both drivers and nighttime bicyclists through a better understanding of how biomotion can increase visibility and safety.Originality/value-There is lack of literature addressing the issue of the commonly administered experimental task of recognizing bicyclists and its potential bias on participants' attention and natural driving state. Eye-tracking has the potential to implicitly determine attention and visibility, devoid of biases to attention. A new retroreflective visibility aid design, pseudo-biomotion, was also introduced in this experiment.",,,
,,KIM P J,"Three-dimensional head-up display system for displaying a three-dimensional image to a user, comprises a sensor unit that measures information on an object located in front of the vehicle, which is a moving direction of the vehicle","NOVELTY - The three-dimensional head-up display system (100) comprises a sensor unit (120) that measures information on an object located in front of the vehicle, which is a moving direction of the vehicle. A photographing unit (130) acquires image information with the object. A communication unit (110) receives the information on surrounding conditions according to the location of the vehicle provided from a server and additional information related to an object in the image information. A gaze tracking unit (140) detects a gaze direction and a gaze position of a user in the vehicle. A control unit (190) generates the three-dimensional (3D) content to be provided to the user based on the information, the detected gaze direction, and the gaze position of the user. A three-dimensional display (160) outputs the 3D content generated by the control unit. A reflector (170) reflects the 3D content output by the 3D display. USE - Three-dimensional head-up display system for displaying a three-dimensional image to a user with an image located at the desired distance using a three-dimensional display capable of providing different information to both eyes. ADVANTAGE - The process configures the entire system in a compact size, reduces the cost, the distance between the display and the eye is closed, provides a wider usage environment even with a smaller sized 3D display, expresses the information on distant objects, and provides an effect of being able to express information about an existing object. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for providing the information on an object that is located in front of the vehicle. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the three-dimensional head-up display system. (Drawing includes non-English language text).Three-dimensional head-up display system (100)Communication unit (110)Sensor unit (120)Photographing unit (130)Gaze tracking unit (140)Three-dimensional display (160)Reflector (170)Control unit (190)",,,
10.1109/ISBI48211.2021.9433863,2021,"Sharma, Harshita; Drukker, Lior; Papageorghiou, Aris T; Alison Noble, J","Multi-Modal Learning from Video, Eye Tracking, and Pupillometry for Operator Skill Characterization in Clinical Fetal Ultrasound.","This paper presents a novel multi-modal learning approach for automated skill characterization of obstetric ultrasound operators using heterogeneous spatio-temporal sensory cues, namely, scan video, eye-tracking data, and pupillometric data, acquired in the clinical environment. We address pertinent challenges such as combining heterogeneous, small-scale and variable-length sequential datasets, to learn deep convolutional neural networks in real-world scenarios. We propose spatial encoding for multi-modal analysis using sonography standard plane images, spatial gaze maps, gaze trajectory images, and pupillary response images. We present and compare five multi-modal learning network architectures using late, intermediate, hybrid, and tensor fusion. We build models for the Heart and the Brain scanning tasks, and performance evaluation suggests that multi-modal learning networks outperform uni-modal networks, with the best-performing model achieving accuracies of 82.4% (Brain task) and 76.4% (Heart task) for the operator skill classification problem.",,,
10.1117/12.2590801,2021,"Kameda, Yoshinari; Ferrer, Cesar Daniel Rojas; Ohnishi, Sho; Shishido, Hidehiko",A New Verification Approach for Subjective Evaluation of Actions in HMD-VR with EEG,"We propose a new verification approach for the subjective evaluation of actions and reactions in virtual reality. We have been working on subject behavior analysis in VR. A head- tracking HMD allows users to move around and take actions up to room size, and their experiences can be measured by their head position, motion estimation, and gaze tracking when they are available. However, in most cases, subjective evaluation for total VR experience is conducted after their experiences are over. We have successfully integrated a new EEG device that can measure EEG and mind indexes in real-time with gaze-trackable and head-trackable HMD. By referring to the gaze tracking results and EEG analysis and mind indexes when a situation is presented in VR, we can check the subject is undoubtedly watching the objects with their attention in the situation and their attention level in his/her mind. It may support the reliability of the score of subjective evaluation. This is very important when we need to conduct many subjective evaluations with fewer subjects. We have developed our preliminary system for two applications - sports action analysis and traffic safety evaluation and confirmed that it is a promising approach.",,,
,,ZHANG S; WANG X; LIN H; ZHU T; DONG Z; HUANG T; TAN H; LIU S,"Method for recognizing fatigue state based on deep learning, involves comparing comprehensive evaluation value with set value to determine whether person corresponding to face picture is in fatigue state","NOVELTY - The method involves obtaining (S1) a video stream data, and a face picture is obtained from the video stream data. A face detection and face key point detection are performed (S2) on the face picture. A facial feature information of the face picture is obtained (S3) through the face alignment model. The gaze estimation, head pose estimation, and mouth estimation are performed (S4) on the facial feature information to obtain gaze estimation information, head pose estimation information, and mouth estimation information, respectively. The posture estimation information, the mouth gaze estimation information, the head estimation information are combined (S5) to obtain a comprehensive evaluation value for fatigue state recognition. A comprehensive evaluation value is compared (S6) with a set value to determine whether the person corresponding to the face picture is in a fatigue state. USE - Method for recognizing fatigue state based on deep learning. ADVANTAGE - The fatigue state of the person is detected in real time. The accurate rate of the identification is effectively improved. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system for recognizing fatigue state based on deep learning; and(2) a computer storage medium storing program for recognizing fatigue state based on deep learning. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for recognizing fatigue state based on deep learning. (Drawing includes non-English language text)Step for obtaining a video stream data (S1)Step for performing the face detection and face key point detection (S2)Step for obtaining the facial feature information of the face picture (S3)Step for performing the gaze estimation, head pose estimation, and mouth estimation (S4)Step for combining the posture estimation information, the mouth gaze estimation information and the head estimation information (S5)Step for comparing the comprehensive evaluation value with a set value to determine whether the person corresponding to the face picture is in a fatigue state (S6)",,,
10.1109/TPAMI.2019.2957373,2021,"Liu, Gang; Yu, Yu; Mora, Kenneth A. Funes; Odobez, Jean-Marc",A Differential Approach for Gaze Estimation,"Most non-invasive gaze estimation methods regress gaze directions directly from a single face or eye image. However, due to important variabilities in eye shapes and inner eye structures amongst individuals, universal models obtain limited accuracies and their output usually exhibit high variance as well as subject dependent biases. Thus, increasing accuracy is usually done through calibration, allowing gaze predictions for a subject to be mapped to her actual gaze. In this article, we introduce a novel approach, which works by directly training a differential convolutional neural network to predict gaze differences between two eye input images of the same subject. Then, given a set of subject specific calibration images, we can use the inferred differences to predict the gaze direction of a novel eye sample. The assumption is that by comparing eye images of the same user, annoyance factors (alignment, eyelid closing, illumination perturbations) which usually plague single image prediction methods can be much reduced, allowing better prediction altogether. Furthermore, the differential network itself can be adapted via finetuning to make predictions consistent with the available user reference pairs. Experiments on 3 public datasets validate our approach which constantly outperforms state-of-the-art methods even when using only one calibration sample or those relying on subject specific gaze adaptation.",,,
,,CHANG M H; JIN S O; KANG D G; JOON K M; BAE Y M; SHIN K Y,"System for evaluating quality of subjective image, has image quality estimation part for producing image quality estimation result of test image by utilizing video information when weighting value is provided","NOVELTY - The system has an image acquisition part (110) for obtaining a test image. An image information calculation unit (120) obtains pre-set video information about the test image by utilizing machine learning i.e. deep neural network base learning. An importance map output unit (130) obtains importance map about the test image by utilizing the machine learning. An information integrating part (140) provides a weighted value to the video information by utilizing the importance map. An image quality estimation part (150) produces an image quality estimation result of the test image by utilizing the video information when the weighting value is provided. The neural network base learning for the importance map computation is learned by utilizing eye gaze tracking data for the image and the image. The neural network base learning for video information computation is learned by utilizing the video information, which is indicated in the image. USE - System for evaluating quality of a subjective image. ADVANTAGE - The system performs estimation of image quality of the image without the reference image and obtains video information and the importance map by utilizing machine learning test image. The system extracts video information without concern about type and extent of the distortion by utilizing the deep neural network so as to produce high importance map of performance with top-down cue and bottom-up cue. The system obtains video information, calculates size of importance map based on the deep neural network with importance and confirms size of the video information with the map so as to perform estimation of image quality. The system calculates importance of the video information when calculated importance is different according to the performance of the map and position, performs analysis process and improves performance about the different extent for reflecting the map importance according to the video information. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a subjective image quality evaluation method(2) a recording medium for storing a set of instructions to perform a subjective image quality evaluation method. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a subjective image quality evaluating system. '(Drawing includes non-English language text)'Image acquisition part (110)Image information calculation unit (120)Importance map output unit (130)Information integrating part (140)Image quality estimation part (150)",,,
,,PARK S S,"Eye gaze information analysis system through head direction of prediction image signal based on deep learning, includes image preprocessor, moving area and head region extractors, neural network model and gaze region mapping module","NOVELTY - The analysis system includes an image preprocessor, a moving area extractor, a head region extractor, a neural network model and a gaze region mapping module. The head region of the human is extracted from the image data received through a camera and is applied to the neural network model in order to classify direction in which the person is heading, and predict a spot in which the eye gaze of human through the angle of the head. USE - Eye gaze information analysis system through head direction of prediction image signal based on deep learning. ADVANTAGE - The human head region is extracted from the image data received through the camera and is applied to the neural network model to classify the direction in which the human is heading. Thereby it is possible to know whether the human eye gaze stays on the right side or left side of the escalator by rearranging the display of a store or by installing the escalator camera. Therefore the exposure of the advertising board is increased. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the eye gaze information analysis system through head direction of prediction image signal based on deep learning. (Drawing includes non-English language text)",,,
,,RIMON N,"Method for sharing gameplay of video game, involves generating three-dimensional (3-D) video clip by converting two-dimensional (2-D) video clip to 3-D format and sharing 3-D video clip to social network","NOVELTY - The method involves recording a 2-D video clip to a storage by a computer (106). The 2-D video clip is generated from gameplay of a video game. A request is received to share the 2-D video clip to a social network by computer. A 3-D video clip is generated by converting the 2-D video clip to a 3-D format by the computer. The 3-D video clip is configured for viewing through a head mounted display (HMD) (102). The 3-D video clip is shared to social network by computer. The 2-D video clip is shared to social network. The social network is configured to provide 3-D video clip to a client device. USE - Method for sharing gameplay of video game. ADVANTAGE - The gameplay video is optimized so that objects or regions which the player was viewing are presented in the gameplay video with increased graphical fidelity and emphasis, thus highlighting to the viewer those areas which the player was viewing during gameplay. The direction of the field of view presented in the video clip can be determined based on gaze tracking information, so as to track the player's viewing direction during gameplay and ensure presentation of regions that the player viewed during gameplay. By zooming out when movement of the player's gaze direction increases, the gameplay video clip can appear less jittery during periods of high movement by the player, and thus provide a smoother viewing experience for the viewer. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for sharing video to a social network; and(2) a method for sharing three-dimensional video. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of a system for interactive gameplay of a video game.User (100)HMD (102)Controller (104)Computer (106)Network (110)",,,
10.1016/j.compenvurbsys.2021.101685,2021,"Lu, Xi; Tomkins, Adam; Hehl-Lange, Sigrid; Lange, Eckart",Finding the difference: Measuring spatial perception of planning phases of high-rise urban developments in Virtual Reality,"Planning is a process in which the contents of planning is gradually refined. However, research in planning communication and perception is often conducted using contrasting scenarios, e.g. by comparing a with/without case. It is not surprising that drastic differences in planning content and representation result in significant differences in perception. Instead, and as a reflection of sequential and gradually evolving projects in planning practice, we are focusing on two planning phases with only subtle differences (2015 and 2018) for a new high-rise development district in Guangzhou. We introduce 3D gaze-tracking and spatial perception experiments to investigate how participants respond to virtual representations of the two planning phases. The results provide implications for planning and design practice and suggest more substantial roles for the general public in participatory planning processes.",,,
10.1109/ICIT46573.2021.9453581,2021,"Pettersson, Julius; Falkman, Petter",Human Movement Direction Prediction using Virtual Reality and Eye Tracking,One way of potentially improving the use of robots in a collaborative environment is through prediction of human intention that would give the robots insight into how the operators are about to behave. An important part of human behaviour is arm movement and this paper presents a method to predict arm movement based on the operator's eye gaze. A test scenario has been designed in order to gather coordinate based hand movement data in a virtual reality environment. The results shows that the eye gaze data can successfully be used to train an artificial neural network that is able to predict the direction of movement similar to 500 ms ahead of time.,,,
,2020,"Lorenzo, J.; Parra, I; Wirth, F.; Stiller, C.; Llorca, D. F.; Sotelo, M. A.",RNN-based Pedestrian Crossing Prediction using Activity and Pose-related Features,"Pedestrian crossing prediction is a crucial task for autonomous driving. Numerous studies show that an early estimation of the pedestrian's intention can decrease or even avoid a high percentage of accidents. In this paper, different variations of a deep learning system are proposed to attempt to solve this problem. The proposed models are composed of two parts: a CNN-based feature extractor and an RNN module. All the models were trained and tested on the JAAD dataset. The results obtained indicate that the choice of the features extraction method, the inclusion of additional variables such as pedestrian gaze direction and discrete orientation, and the chosen RNN type have a significant impact on the final performance.",,,
10.1109/ACCESS.2020.3033382,2020,"Wang, Ying; Wang, Xiaoming; Wu, Yaowu",A Simple Model of Reading Eye Movement Based on Deep Learning,"At present, an increasing amount of research is being conducted on human cognitive behaviour, and reading eye-movement modelling is a research hotspot in cognitive linguistics. However, existing reading eye-movement models are complicated and require a large number of hand-crafted features. To address these issues, this paper improves upon the fixation granularity processing mode and the regression processing mode of the traditional reading eye-movement models and proposes a reading eye-movement fixation sequence labelling method to construct a simpler model. The proposed model is based on a multi-input deep-learning neural network, which takes advantage of deep learning to reduce the number of required hand-crafted features and integrates knowledge from the field of cognitive psychology to increase its accuracy. To meet the data-size requirements of the deep-learning model, this paper also proposes a reading eye-movement data augmentation method. The experimental results show that the proposed method can describe the actual process of reading eye-movement intuitively and that the simple reading eye-movement models based on this method can obtain a similar accuracy with existing models by using fewer hand-crafted features.",,,
10.1145/3311784,2019,"Hsu, Chih-Fan; Wang, Yu-Shuen; Lei, Chin-Laung; Chen, Kuan-Ta",Look at Me! Correcting Eye Gaze in Live Video Communication,"Although live video communication is widely used, it is generally less engaging than face-to-face communication because of limitations on social, emotional, and haptic feedback. Missing eye contact is one such problem caused by the physical deviation between the screen and camera on a device. Manipulating video frames to correct eye gaze is a solution to this problem. In this article, we introduce a system to rotate the eyeball of a local participant before the video frame is sent to the remote side. It adopts a warping-based convolutional neural network to relocate pixels in eye regions. To improve visual quality, we minimize the L2 distance between the ground truths and warped eyes. We also present several newly designed loss functions to help network training. These new loss functions are designed to preserve the shape of eye structures and minimize color changes around the periphery of eye regions. To evaluate the presented network and loss functions, we objectively and subjectively compared results generated by our system and the state-of-the-art, DeepWarp, in relation to two datasets. The experimental results demonstrated the effectiveness of our system. In addition, we showed that our system can perform eye-gaze correction in real time on a consumer-level laptop. Because of the quality and efficiency of the system, gaze correction by postprocessing through this system is a feasible solution to the problem of missing eye contact in video communication.",,,
,,ZHANG F; LIN G; SONG J,"Staff work based deep learning images attention detection method, involves performing monitoring and recording of human eyes in closing state, where head pose angle exceeds predetermined threshold value","NOVELTY - The method involves obtaining a working image of a worker based on deep learning of a human face detection model. A face rectangular frame and a face key point are obtained by a rectangular frame to obtain head pose angle to face a head towards Euler angles. Utilization of human face key points is performed in eye detection. Eyes are detected in the rectangular frame. Eye state identification is performed to obtain an eye opening/closing state. An eye gaze direction is determined based on the head posture angle and the double-eye gaze direction. Monitoring and recording of human eyes are performed in a closing state, where the head pose angle exceeds a predetermined threshold value and the double-eye gaze direction exceeds a prescribed range. USE - Staff work based deep learning images attention detection method. ADVANTAGE - The method enables improving monitoring efficiency of the working state of staff and high reliability for monitoring an operation state of the X-ray machine in an inspection and monitoring room. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a staff work based deep learning images attention detection system. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram illustrating an operation of a staff work based deep learning images attention detection method. '(Drawing includes non-English language text)'",,,
10.3390/ijgi7070281,2018,"Dong, Weihua; Wang, Shengkai; Chen, Yizhou; Meng, Liqiu",Using Eye Tracking to Evaluate the Usability of Flow Maps,"Flow maps allow users to perceive not only the location where interactions take place, but also the direction and volume of events. Previous studies have proposed numerous methods to produce flow maps. However, how to evaluate the usability of flow maps has not been well documented. In this study, we combined eye-tracking and questionnaire methods to evaluate the usability of flow maps through comparisons between (a) straight lines and curves and (b) line thicknesses and color gradients. The results show that curved flows are more effective than straight flows. Maps with curved flows have more correct answers, fixations, and percentages of fixations in areas of interest. Furthermore, we find that the curved flows require longer finish times but exhibit smaller times to first fixation than straight flows. In addition, we find that using color gradients to indicate the flow volume is significantly more effective than the application of different line thicknesses, which is mainly reflected by the presence of more correct answers in the color-gradient group. These empirical studies could help improve the usability of flow maps employed to visualize geo-data.",,,
,,HENDRICKS J G; VAN CLEAVE J T,"System for augmented and mixed reality medical training of user, has computing device that assesses user's identification of virtual critical cues in augmented reality view by determining by gaze tracking that virtual cues are viewed","NOVELTY - The system (100) comprises a wearable mixed reality device (102), a computing device (104) with a display screen, a physical object, and fiducial markers positioned on a surface of the physical object. The computing device has memory executable instructions for anchoring a virtual image of a portion of a virtual object to a fixed location which is in a known relationship to the fiducial markers, and storing a scenario including parameters for visualization of a condition in the virtual object. The computing device selects the scenario, and displays the virtual image on the mixed reality device and on the display screen to present one or more virtual critical cues. The computing device assesses a user's identification of the virtual critical cues in a three-dimensional (3D) augmented reality view by monitoring the display screen and determining by gaze tracking that the virtual critical cues are viewed. USE - System for augmented and mixed reality medical training of user. ADVANTAGE - The system enables trainees to build robust mental models and improve their ability to visualize the implications of the injury, anticipate future states, and identify appropriate treatment options, by situating instructional animations of injury processes in realistic scenarios. The visual cue can appear in the trainee's field of vision together with a visual indication of where and how to make an intervention, which is beneficial for novice and intermediate trainees. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of the system for augmented and mixed reality medical training.System for medical training (100)Wearable mixed reality device (102)Computing device (104)Data capture computing system (110)Wireless network (114)",,,
,,LU Z,"Human eye tracking method, involves determining binocular pupil orientation information of user, and determining three-dimensional binocular pupil orientation information of user according to binocular pupil orientation information",NOVELTY - The method involves invoking a first camera to capture a user image in a preset viewing area. Three-dimensional (3D) head orientation information is determined according to the user image corresponding to a user in the preset viewing area. A preset number of target second cameras are determined from multiple camera heads according to the 3D head orientation information. A target second camera is invoked to collect a facial image of a user. Two-dimensional (2D) binocular pupil orientation information of the user is determined according to the facial image. 3D binocular pupil orientation information of the user is determined according to the 2D binocular pupil orientation information. USE - Human eye tracking method. ADVANTAGE - The method enables improving calculation speed and calculation precision in multi-user human eye tracking scene. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a human eye tracking device(2) a human eye tracking system(3) a device comprising a processor and a memory for executing a human eye tracking method(4) a computer-readable storage medium for storing a set of instructions for executing a human eye tracking method. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a human eye tracking method. '(Drawing includes non-English language text)',,,
10.1002/jsid.653,2018,"Jia, Lixiu; Tu, Yan; Wang, Lili; Zhong, Xuefei; Wang, Ying",Study of image quality using event-related potentials and eye tracking measurement,"The image quality was investigated using electroencephalography and eye-tracking measurements simultaneously. Different image qualities were obtained by changing sigma values in the Gaussian kernel function. The experimental results illustrated that the measurable event-related potentials and eye-tracking parameters varied with the change of the image quality. The more blurred the image, the higher and earlier the P300 (event-related potentials at latency about 300-550ms) was elicited. Meanwhile, the visual fixation duration decreased and the detection rate increased with the increase of sigma. The blurred image quality could be evaluated based on the synthetic results of the reciprocal of P300 latency and the visual fixation duration. The result showed a good correlation with the subjective rating scores, which proved the feasibility of utilizing electroencephalography and eye-tracking technology to evaluate the blurred image quality.",,,
10.1109/TBME.2021.3058805,2021,"Tsai, Meng-Chang; Chung, Chia-Ru; Chen, Chun-Chuan; Chen, Jyun-Yu; Yeh, Shih-Ching; Lin, Chun-Han; Chen, Ying-Ju; Tsai, Ming-Che; Wang, Ya-Ling; Lin, Chia-Ju; Wu, Eric Hsiao-Kuang",An Intelligent Virtual-Reality System With Multi-Model Sensing for Cue-Elicited Craving in Patients With Methamphetamine Use Disorder,"Methamphetamine abuse is getting worse amongst the younger population. While there is methadone or buprenorphine harm-reduction treatment for heroin addicts, there is no drug treatment for addicts with methamphetamine use disorder (MUD). Recently, non-medication treatment, such as the cue-elicited craving method integrated with biofeedback, has been widely used. Further, virtual reality (VR) is proposed to simulate an immersive virtual environment for cue-elicited craving in therapy. In this study, we developed a VR system equipped with flavor simulation for the purpose of inducing cravings for MUD patients in therapy. The VR system was integrated with multi-model sensors, such as an electrocardiogram (ECG), galvanic skin response (GSR) and eye tracking to measure various physiological responses from MUD patients in the virtual environment. The goal of the study was to validate the effectiveness of the proposed VR system in inducing the craving of MUD patients via the physiological data. Clinical trials were performed with 20 MUD patients and 11 healthy subjects. VR stimulation was applied to each subject and the physiological data was measured at the time of pre-VR stimulation and post-VR stimulation. A variety of features were extracted from the raw data of heart rate variability (HRV), GSR and eye tracking. The results of statistical analysis found that quite a few features of HRV, GSR and eye tracking had significant differences between pre-VR stimulation and post-VR stimulation in MUD patients but not in healthy subjects. Also, the data of post-VR stimulation showed a significant difference between MUD patients and healthy subjects. Correlation analysis was made and several features between HRV and GSR were found to be correlated. Further, several machine learning methods were applied and showed that the classification accuracy between MUD and healthy subjects at post-VR stimulation attained to 89.8%. In conclusion, the proposed VR system was validated to effectively induce the drug craving in MUD patients.",,,
,2020,"SeoJeon, Park; Kim, ByungGyu",Development of Low-Cost Vision-based Eye Tracking Algorithm for Information Augmented Interactive System,"Deep Learning has become the most important technology in the field of artificial intelligence machine learning, with its high performance overwhelming existing methods in various applications. In this paper, an interactive window service based on object recognition technology is proposed. The main goal is to implement an object recognition technology using this deep learning technology to remove the existing eye tracking technology, which requires users to wear eye tracking devices themselves, and to implement an eye tracking technology that uses only usual cameras to track users' eye. We design an interactive system based on efficient eye detection and pupil tracking method that can verify the user's eye movement. To estimate the view-direction of userâ€™s eye, we initialize to make the reference (origin) coordinate. Then the view direction is estimated from the extracted eye pupils from the origin coordinate. Also, we propose a blink detection technique based on the eye apply ratio (EAR). With the extracted view direction and eye action, we provide some augmented information of interest without the existing complex and expensive eye-tracking systems with various service topics and situations. For verification, the user guiding service is implemented as a proto-type model with the school map to inform the location information of the desired location or building.",,,
,,RUDCHENKO D; BADGER E N; KAZA A; COHEN J D; KULKARNI H S,"Method for analyzing eye-gaze input on electronic device, involves predicting response, providing predicted response, receiving indication related to predicted response, and performing action based on indication","NOVELTY - The method (200) involves receiving (202) eye-gaze input on an electronic device. The gaze location associated with the eye-gaze input is determined (204). The machine-learning algorithm is applied (208) to one gaze location. The response is predicted based on the application of one machine-learning algorithm. The predicted response is provided (210). An indication related to one predicted response is received (212). The action is performed (214) is based on indication. USE - Method for analyzing eye-gaze input on electronic device e.g. mobile computing device such as mobile telephone, smart phone, wearable computer such as smart watch or head-mounted display, tablet computer, laptop computer, etc. ADVANTAGE - The method can quickly analyze eye-gaze input to identify desired words for typing without reliance on dwell time. The user can quickly glance at each character in order to input the desired word. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a computing device; and(2) a processor-readable storage media storing instructions for performing a method for analyzing eye-gaze input. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram illustrating a method for an intelligent eye-gaze response.Method for analyzing eye-gaze input on electronic device (200)Step for receiving eye-gaze input on an electronic device (202)Step for determining gaze location associated with the eye-gaze input (204)Step for applying machine-learning algorithm to gaze location (208)Step for providing predicted response (210)Step for receiving an indication related to predicted response (212)Step for performing action (214)",,,
,2017,"Venkataraman, Hrishikesh; Assfalg, Rolf",Driver Performance Detection & Recommender System in Vehicular Environment using Video Streaming Analytics TUTORIAL TALK BY,"This tutorial deals with how the overall human performance while working can be detected, under different conditions and scenarios. The tutorial will talk about how regular and real-time monitoring of people can be carried out through eye-tracking and how it can be integrated with other environment factors to develop a recommender system. Particularly, the speakers would articulate a vehicular driving scenario and explain how a combined use of eye-tracking and face-tracking can not only help the drivers but also significantly assist in reducing the road accidents, thereby increase the road safety. Finally, the speakers will present the use of existing eye-trackers along with the integrated eye-tracker and face-tracker that is under indigenous development. This tutorial is intended for a wide section of audience-ranging from clusters of automobile designers/manufacturers, researchers in mechatronics and students researching on different aspects of connected cars, computer vision, signal processing, image processing and machine learning and data analytics.",,,
10.1016/j.compbiomed.2019.03.025,2019,"Larrazabal, A. J.; Garcia Cena, C. E.; Martinez, C. E.",Video-oculography eye tracking towards clinical applications: A review,"Most neurological diseases are usually accompanied by a broad spectrum of oculomotor alterations. Being able to record and analyze these different types of eye movements would be a valuable tool to understand the functional integrity of brain structures. Nowadays, video-oculography is the most widely used eye-movements assessing method. This paper presents a study of the existing eye tracking video-oculography techniques and also analyzes the importance of measuring slight head movements for diseases diagnosis. In particular, two types of methods are reviewed and compared, including appearance-based and feature-based methods which are further subdivided into 2D-mapping and 3D model-based approaches. In order to demonstrate the advantages and disadvantages of these different eye tracking methods for disease diagnosis, a series of comparisons are conducted between them, addressing the complexity of the system, the accuracy achieved, the ability to measure head movements and the external conditions for which they have been designed. Lastly, it also highlights the open challenges in this research field and discusses possible future directions.",,,
10.1109/LRA.2021.3059619,2021,"Kim, Heecheol; Ohmura, Yoshiyuki; Kuniyoshi, Yasuo",Gaze-Based Dual Resolution Deep Imitation Learning for High-Precision Dexterous Robot Manipulation,"A high-precision manipulation task, such as needle threading, is challenging. Physiological studies have proposed connecting low-resolution peripheral vision and fast movement to transport the hand into the vicinity of an object, and using high-resolution foveated vision to achieve the accurate homing of the hand to the object. The results of this study demonstrate that a deep imitation learning based method, inspired by the gaze-based dual resolution visuomotor control system in humans, can solve the needle threading task. First, we recorded the gaze movements of a human operator who was teleoperating a robot. Then, we used only a high-resolution image around the gaze to precisely control the thread position when it was close to the target. We used a low-resolution peripheral image to reach the vicinity of the target. The experimental results obtained in this study demonstrate that the proposed method enables precise manipulation tasks using a general-purpose robot manipulator and improves computational efficiency.",,,
,,SENGELAUB T,"Low-power eye tracking system for detecting position and movement of eyes of user in head-mounted device (HMD), has three-dimensional (3D) models that are updated to indicate new position of eyes of user with respect to camera by processor","NOVELTY - The system has a HMD (100) that comprising a camera (140) configured to capture images of eyes (192) of user (190) at a frame rate of N frames per second. Multiple head motion sensors are configured to detect motion of the HMD with respect to the user's head. A controller (160) comprising multiple processors that are configured to perform an initial 3D reconstruction based on an image captured by the camera to generate initial 3D models of the user's eyes. A two-dimensional (2D) image processing of multiple subsequent images captured by the camera is performed to track movement of the user's eyes with respect to the HMD. The 3D models are updated to indicate a new position of the user's eyes with respect to the camera, upon detecting that the HMD has shifted on the user's head based on signals from the multiple head motion sensors. USE - Low-power eye tracking system for detecting position and movement of eyes of user in head-mounted device (HMD) such as headset, helmet, goggles or glasses used in virtual and mixed or augmented reality (VR/AR) application. ADVANTAGE - The 3D reconstruction to be performed only when movement of the device with respect to the user's eyes is detected, thus significantly reducing power consumption by the eye tracking system. The eye tracking method allows the frame rate of the eye tracking cameras to be reduced, and also allows 3D reconstruction to be performed much less often, thus significantly reducing power consumption by the eye tracking system. The 3D models of the eyes indicate the 3D position of the eyes with respect to the eye tracking cameras, which allows the eye tracking algorithms executed by the controller to accurately track eye movement. The information used by the controller to adjust the 3D models, which allows the eye tracking algorithms executed by the controller to continue to accurately track eye movement with respect to the HMD using 2D image processing without requiring expensive 3D reconstruction to be performed. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for detecting position and movement of eyes of user in HMD; and(2) a device for detecting position and movement of eyes of user in HMD. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the VR/AR HMD that implements the eye tracking system.Head-mounted device (100)Camera (140)Controller (160)User (190)Eye (192)",,,
,,JAEKYU P; HAN C M; KIM K S; SEONG J,"Deep learning image analysis-based face recognition and iris position recognition system, has analysis device for obtaining eye tracking information from tracking result to obtain progressive multifocal design information","NOVELTY - The system has a measurement terminal (200) for acquiring a measurement image taken by a user, where the measurement terminal is detachable cradle (100). An analysis device (20) obtains progressive multifocal design information by analyzing an acquired measurement image based on deep learning. The analysis device detects an eyeball from the acquired measurement image, where the eyeball is detected from the detected eyeball. The analysis device obtains eye pupil data by detecting pupils, irises and sclera by extracting an iris borders and sclera borders to track movements of the detected pupils, irises and sclera, where machine learning is performed by using deep learning techniques. The analysis device obtains eye tracking information from a tracking result to obtain progressive multifocal design information based on a comparison result. USE - Deep learning image analysis-based face recognition and iris position recognition system. ADVANTAGE - The system accurately calculates line distance and line angle of the eye by automatically recognizing and tracking the eye position by using a deep learning technique, and captures a measurement image by a user so as to improve efficiency of work of an optometrist by using the analysis device, so that improving accuracy of the measurement image, thus reduces need for manually change a mode of the analysis device without need for a person, and hence automatically changing a shooting mode according to the state of the measurement terminal by the analysis device, and determines position of an object moving in the measurement image, and adopts the analysis device so as to obtain template data based on extracted feature values, thus increasing accuracy of the template data through iterative learning. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a deep learning image analysis-based face recognition and iris position recognition system. '(Drawing includes non-English language text)'Analysis device (20)Detachable cradle (100)Measurement terminal (200)",,,
,,DEY K; JOSHI A U; RAMACHANDRAN P G; SCOTT W L,"Method for dynamically modifying electronic information based on eye gaze, involves collecting eye gaze data points to create eye gaze corpus of information by processors, and eye gaze data points describe eye gaze of viewers","NOVELTY - The method involves collecting (404) eye gaze data points to create an eye gaze corpus of information by one or more processors. The eye gaze data points describe an eye gaze of viewers of a first set of at least one user interface. An unsupervised machine learning algorithm is applied (406) to the eye gaze corpus to generate clusters based upon the eye gaze data points. A target action performance of the first set of at least one user interface is determined (408) for each of the clusters. The current users are segmented (412) based upon the clusters by analyzing patterns among the real-time eye gaze data. A computer-based interaction is modified (414) for at least one segment of the current users in order to maximize target action performance of the second set of user interface. USE - Method for dynamically modifying electronic information based on eye gaze. ADVANTAGE - The system become more efficient in generating clarifications to graphical user interface content, and further improving a user experience. The computer-based interaction and user experience is improved based on historical scenarios. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a computer program product for modifying a computer-based interaction based on historical eye gaze data; and(2) a computer system for modifying a computer-based interaction based on historical eye gaze data. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of steps performed by one or more processors.Collecting eye gaze data points to create an eye gaze corpus of information (404)Applying an unsupervised machine learning algorithm to the eye gaze corpus (406)Determining a target action performance of the first set of at least one user interface (408)Segmenting the current users based upon clusters by analyzing patterns among the real-time eye gaze data (412)Modifying a computer-based interaction for at least one segment of the current users (414)",,,
10.1016/j.oceaneng.2018.08.026,2018,"Dogan, Kemal Mert; Suzuki, Hiromasa; Gunpinar, Erkan",Eye tracking for screening design parameters in adjective-based design of yacht hull,"Adjective-based design is a method that translates human perception into design parameters quantitatively in order to achieve better understanding between designers and clients. In this approach, adjectives are used to describe product designs, which are generated via design parameters in terms of geometry. As a requirement of the concept, relations between hull adjectives (e.g., comfortable and aesthetic) and design parameters (e.g., length and width) are learned via a machine-learning algorithm. Nevertheless, the relations cannot be represented by some of the design parameters, although they are in the learning process. This issue shows that the parameters do not impact the adjective choices but add noises to the learning process. Therefore, in this study, visual evaluations are made using eye tracking technology for screening the parameters based on their attractiveness and establishing relations between the attractive ones and the adjectives to enhance quality of the relation representations. Eye tracking is used in perceptual research, which proves the existence of correlations between gaze data and human preferences. The main advantage of eye tracking is that reliable human perception data can more likely be collected compared to the user tests, since the evaluation is based on subjects' attention rather than applying solely questionnaires that are limited by the question content. In light of the benefits and finding, an eye tracking device is used to collect gaze data, and then, eye tracking tools such as Area of Interest (AOI), scan path, and heat map are used to evaluate attractiveness of the design parameters. Finally, regression analysis is used to represent relations between gaze data of design parameters and the adjectives.",,,
,2020,"McLaren, Lorcan; Koutsombogera, Maria; Vogel, Carl",A Heuristic Method for Automatic Gaze Detection in Constrained Multi-Modal Dialogue Corpora,"We describe a heuristic-based approach to determining gaze allocation automatically in a multi-modal task oriented dialogue corpus. We present the development of the system and the evaluation of its performance and discuss the findings, including the shortcomings and the perspectives of the implemented approach.",,,
10.1007/s11257-019-09228-5,2019,"Hutt, Stephen; Krasich, Kristina; Mills, Caitlin; Bosch, Nigel; White, Shelby; Brockmole, James R.; D'Mello, Sidney K.",Automated gaze-based mind wandering detection during computerized learning in classrooms,"We investigate the use of commercial off-the-shelf (COTS) eye-trackers to automatically detect mind wandering-a phenomenon involving a shift in attention from task-related to task-unrelated thoughts-during computerized learning. Study 1 (N = 135 high-school students) tested the feasibility of COTS eye tracking while students learn biology with an intelligent tutoring system called GuruTutor in their classroom. We could successfully track eye gaze in 75% (both eyes tracked) and 95% (one eye tracked) of the cases for 85% of the sessions where gaze was successfully recorded. In Study 2, we used this data to build automated student-independent detectors of mind wandering, obtaining accuracies (mind wandering F-1 = 0.59) substantially better than chance (F-1 = 0.24). Study 3 investigated context-generalizability of mind wandering detectors, finding that models trained on data collected in a controlled laboratory more successfully generalized to the classroom than the reverse. Study 4 investigated gaze- and video- based mind wandering detection, finding that gaze-based detection was superior and multimodal detection yielded an improvement in limited circumstances. We tested live mind wandering detection on a new sample of 39 students in Study 5 and found that detection accuracy (mind wandering F-1 = 0.40) was considerably above chance (F1 = 0.24), albeit lower than offline detection accuracy from Study 1 (F-1 = 0.59), a finding attributable to handling of missing data. We discuss our next steps towards developing gaze-based attention-aware learning technologies to increase engagement and learning by combating mind wandering in classroom contexts.",,,
,2020,"Droste, R.; Chatelain, P.; Drukker, L.; Sharma, H.; Papageorghiou, A. T.; Noble, J. A.",DISCOVERING SALIENT ANATOMICAL LANDMARKS BY PREDICTING HUMAN GAZE,"Anatomical landmarks are a crucial prerequisite for many medical imaging tasks. Usually, the set of landmarks for a given task is predefined by experts. The landmark locations for a given image are then annotated manually or via machine learning methods trained on manual annotations. In this paper, in contrast, we present a method to automatically discover and localize anatomical landmarks in medical images. Specifically, we consider landmarks that attract the visual attention of humans, which we term visually salient landmarks. We illustrate the method for fetal neurosonographic images. First, full-length clinical fetal ultrasound scans are recorded with live sonographer gaze-tracking. Next, a convolutional neural network (CNN) is trained to predict the gaze point distribution (saliency map) of the sonographers on scan video frames. The CNN is then used to predict saliency maps of unseen fetal neurosonographic images, and the landmarks are extracted as the local maxima of these saliency maps. Finally, the landmarks are matched across images by clustering the landmark CNN features. We show that the discovered landmarks can be used within affine image registration, with average landmark alignment errors between 4.1% and 10.9% of the fetal head long axis length.",,,
,,YANG Z,"Scene-based human face portrait method, involves inputting data into pre-trained deep learning model, and outputting face position, positions of multiple key points of human face, and classification results of human face social attributes","NOVELTY - The method involves acquiring an image of a specified object captured by a camera. A deep learning model is established. The image is pre-processed to obtain data matched with input data of the pre-established deep learning model. The data is input into a pre-trained deep learning model. A face position, positions of multiple key points of the human face, and classification results of human face social attributes are simultaneously output, where the human face key point comprises two eyes, nose, two mouth corner edge points and cheekbone, and human face social attribute is age and sex. USE - Scene-based human face portrait method. ADVANTAGE - The method enables effectively determining degree of interest of each person in the object in an actual scene and combining social attributes such as person age, gender, and client image in the actual scene with degree of interest so as to provide objective and effective data basis for subsequent information recommendation. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a scene-based human face portrait system(2) an eye gaze counting method(3) an eye gaze counting system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a scene-based human face portrait method. '(Drawing includes non-English language text)'",,,
,,KIM J; STENGEL M; MAJERCIK Z; DE MELLO S; LAINE S; MCGUIRE M; LUEBKE D; KING J; STANGL M; MAJELTSK Z; MERLOT S D; LIBKI D,"Processor for generating an image for use in training machine learning model, includes ALUs to calculate multiple activation values of multiple neural networks trained to infer eye gaze information based on eye position of multiple images","NOVELTY - The processor includes multiple arithmetic logic units (ALUs) to calculate multiple activation values of multiple neural networks trained to infer eye gaze information based, on eye position of multiple images of multiple faces indicated by an infrared light reflection from the multiple images. Multiple neural networks are trained to infer the eye gaze information based on labels associated with the multiple images. USE - Processor for generating an image for use in training a machine learning model. ADVANTAGE - Processor reduces the error associated with predictions of line of sight, pupil location, and other eye gaze information outputted by the neural network(s) from the images. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for generating an image for use in training a machine learning model; and(2) a medium for generating an image for use in training a machine learning model. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a processor for generating an image for use in training a machine learning model.Generating the geometric representation of at least the portion of the face (302)Rendering surfaces in multiple images uses refractive indices match observed properties of the surfaces under monochromatic infrared imaging (306)Performing subsurface scattering of the infrared light in multiple images based on the refractive indices (308)",,,
,2017,"Adiba, Amalia I.; Asatani, Satoshi; Tagawa, Seiichi; Niioka, Hirohiko; Miyake, Jun",Gaze Tracking in 3D Space with a Convolution Neural Network See What I See,"This paper presents integrated architecture to estimate gaze vectors under unrestricted head motions. Since previous approaches focused on estimating gaze toward a small planar screen, calibration is needed prior to use. With a Kinect device, we develop a method that relies on depth sensing to obtain robust and accurate head pose tracking and obtain the eye-in-head gaze direction information by training the visual data from eye images with a Neural Network (NN) model. Our model uses a Convolution Neural Network (CNN) that has five layers: two sets of convolution-pooling pairs and a fully connected-output layer. The filters are taken from the random patches of the images in an unsupervised way by k-means clustering. The learned filters are fed to a convolution layer, each of which is followed by a pooling layer, to reduce the resolution of the feature map and the sensitivity of the output to the shifts and the distortions. In the end, fully connected layers can be used as a classifier with a feed-forward-based process to obtain the weight. We reconstruct the gaze vectors from a set of head and eye pose orientations. The results of this approach suggest that the gaze estimation error is 5 degrees. This model is more accurate than a simple NN and an adaptive linear regression (ALR) approach.",,,
10.1109/TIP.2020.3007841,2020,"Huang, Yifei; Cai, Minjie; Li, Zhenqiang; Lu, Feng; Sato, Yoichi",Mutual Context Network for Jointly Estimating Egocentric Gaze and Action,"In this work, we address two coupled tasks of gaze prediction and action recognition in egocentric videos by exploring their mutual context: the information from gaze prediction facilitates action recognition and vice versa. Our assumption is that during the procedure of performing a manipulation task, on the one hand, what a person is doing determines where the person is looking at. On the other hand, the gaze location reveals gaze regions which contain important and information about the undergoing action and also the non-gaze regions that include complimentary clues for differentiating some fine-grained actions. We propose a novel mutual context network (MCN) that jointly learns action-dependent gaze prediction and gaze-guided action recognition in an end-to-end manner. Experiments on multiple egocentric video datasets demonstrate that our MCN achieves state-of-the-art performance of both gaze prediction and action recognition. The experiments also show that action-dependent gaze patterns could be learned with our method.",,,
,,WANG J,"Electronic device e.g. mobile phone for identifying human eye gaze direction, has processor that extracts three dimensional (3D) eyeball image from 3D face image, and that determines human eye gaze direction","NOVELTY - The device (100) has a processor (110), and a face recognition device (120) connected to the processor. The face recognition device is configured to obtain a 3D face image. A processor extracts a 3D eyeball image from the 3D face image, and determines a human eye gaze direction according to the convex characteristic image of the 3D eyeball. USE - Electronic device such as mobile phone and tablet computer for identifying human eye gaze direction. ADVANTAGE - The direction of the human eye is obtained according to the characteristic, since the surface of the 3D eyeball image is convex surface and has convex surface characteristic. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a user gaze direction recognition method; and(2) a computer-readable storage medium for identifying human eye gaze direction. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the electronic device.Electronic device (100)Processor (110)Face recognition device (120)Environmental sensor (130)Display screen (140)",,,
10.1109/VRW50115.2020.00-79,2020,"Heo, Hwan; Lee, Minho; Kim, Sungjei; Hwang, Youngbae",Gaze plus Gesture Interface: Considering Social Acceptability,"In public places like cafes, the usage of smart glasses with interfaces including controller, touchpad, voice, or mid-air gesture not only receives a lot of interest from people around him or her but also cannot protect the privacy of individuals. If smart glasses become more advanced and more popular than regular glasses in the future, socially acceptable user interfaces can be required. In this paper, we propose a user interface on HoloLens by using gaze tracking instead of head tracking for navigation, and unobtrusive gesture based on deep learning instead of mid-air gestures for selection/manipulation, that is more socially acceptable than the existing user interfaces on smart glasses. A study was conducted to investigate social acceptability from the users' perspective, and the results showed the advantages of the proposed method to improve social acceptability.",,,
,2020,"Zhao, Xin; Lin, Hanhe; Guo, Pengfei; Saupe, Dietmar; Liu, Hantao",DEEP LEARNING VS. TRADITIONAL ALGORITHMS FOR SALIENCY PREDICTION OF DISTORTED IMAGES,"Saliency has been widely studied in relation to image quality assessment (IQA). The optimal use of saliency in IQA metrics, however, is nontrivial and largely depends on whether saliency can be accurately predicted for images containing various distortions. Although tremendous progress has been made in saliency modelling, very little is known about whether and to what extent state-of-the-art methods are beneficial for saliency prediction of distorted images. In this paper, we analyse the ability of deep learning versus traditional algorithms in predicting saliency, based on an IQA-aware saliency benchmark, the SIQ288 database. Building off the variations in model performance, we make recommendations for model selections for IQA applications.",,,
10.1109/TMC.2019.2962764,2021,"Fan, Xiaoyi; Wang, Feng; Song, Danyang; Lu, Yuhe; Liu, Jiangchuan",GazMon: Eye Gazing Enabled Driving Behavior Monitoring and Prediction,"Automobiles have become one of the necessities of modern life, but also introduced numerous traffic accidents that threaten drivers and other road users. Most state-of-the-art safety systems are passively triggered, reacting to dangerous road conditions or driving maneuvers only after they happen and are observed, which greatly limits the last chances for collision avoidances. Timely tracking and predicting the driving maneuvers calls for a more direct interface beyond the traditional steering wheel/brake/gas pedal. In this paper, we argue that a driver's eyes are the interface, as it is the first and the essential window that gathers external information during driving. Our experiments suggest that a driver's gaze patterns appear prior to and correlate with the driving maneuvers for driving maneuver prediction. We accordingly present GazMon, an active driving maneuver monitoring and prediction framework for driving assistance applications. GazMon extracts the gaze information through a front-camera and analyzes the facial features, including facial landmarks, head pose, and iris centers, through a carefully constructed deep learning architecture. Both our on-road experiments and driving simulator based evaluations demonstrate the superiority of our GazMon on predicting driving maneuvers as well as other distracted behaviors. It is readily deployable using RGB cameras and allows reuse of existing smartphones towards more safely driving.",,,
,2017,"Han, Sang Yoon; Lee, Sang Hwa; Cho, Nam Ik",Gaze Estimation Using 3-D Eyeball Model under HMD Circumstance,"This paper presents a gaze estimation algorithm using 3-D eyeball model and 2-D pupil center - inner eye corner(PC-IEC) vector. The conventional methods using feature points in the eye images need lots of calibration markers and long calibration time. However, since the pupil and gaze movements are closely related to the 3-D rotation of eyeball, the long and complicated calibrations are not necessary. This paper derives the relationship between the 3-D eyeball model and 2-D PC-IEC vector with a single reference calibration point which is located at the center of the screen. Also, the proposed algorithm compensates for the eyeball movements using the eyelid height against the inner eye corner. According to the experiment, the proposed method estimates the gaze within 2 degree error.",,,
10.1109/CVPRW.2019.00109,2019,"Koutras, Petros; Maragos, Petros","SUSiNet: See, Understand and Summarize it","In this work we propose a multi-task spatio-temporal network, called SUSiNet, that can jointly tackle the spatio-temporal problems of saliency estimation, action recognition and video summarization. Our approach employs a single network that is jointly end-to-end trained for all tasks with multiple and diverse datasets related to the exploring tasks. The proposed network uses a unified architecture that includes global and task specific layer and produces multiple output types, i.e., saliency maps or classification labels, by employing the same video input. Moreover, one additional contribution is that the proposed network can be deeply supervised through an attention module that is related to human attention as it is expressed by eye-tracking data. From the extensive evaluation, on seven different datasets, we have observed that the multi-task network performs as well as the state-of-the-art single-task methods (or in some cases better), while it requires less computational budget than having one independent network per each task.",,,
,,RASKAR R; CHOUBEY N,"Method for determining vergence of user using combination of eye tracking based approach, involves adjusting multiple configurations associated with head-mounted display based on predicted vergence distance of user","NOVELTY - The method involves determining that a performance metric of an eye tracking system is below a performance threshold, such that the eye tracking system is associated with a head-mounted display (300) worn by a user. Multiple contents being displayed by the head-mounted display is identified in response to the determination that the performance metric is below the performance threshold. Multiple properties of multiple contents are accessed based on the identified contents. A vergence distance (342) of the user is predicted based on the properties associated with the display contents, and multiple configurations associated with the head-mounted display is adjusted based on the predicted vergence distance of the user. USE - Method for determining vergence of user using combination of eye tracking based approach e.g. three-dimensional (3D) eye tracking, machine learning based eye tracking, body-based approach e.g. head position or movement, hand position or movement, body position or movement and content-based approach e.g. Z-buffer, face detection, application-developer provided information. ADVANTAGE - The headset system can take a sequence of images of the eyes of the user wearing the headset e.g., using a 3D eye tracking system and use the machine learning (ML) algorithm to process the images and output vergence information. The headset system can determine multiple performance metrics and compare the performance metrics to multiple performance thresholds to evaluate of the eye tracking system performance and determine the combination of approaches accordingly. The headset system can actuate the varifocal system dynamically based on the head-hand position of the user to keep the virtual object in focus for the user. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a computer-readable non-transitory storage medium storing program for determining vergence of user using combination of eye tracking based approach; and(2) a system for determining vergence of user using combination of eye tracking based approach. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a situation for vergence accommodation conflict in a head-mounted display.Head mounted display (300)Eyes (302)Display (320)Virtual object (322)Vergence distance (342)",,,
10.1109/ICCV.2017.188,2017,"Leifman, George; Rudoy, Dmitry; Swedish, Tristan; Bayro-Corrochano, Eduardo; Raskar, Ramesh",Learning Gaze Transitions from Depth to Improve Video Saliency Estimation,"In this paper we introduce a novel Depth-Aware Video Saliency approach to predict human focus of attention when viewing videos that contain a depth map (RGBD) on a 2D screen. Saliency estimation in this scenario is highly important since in the near future 3D video content will be easily acquired yet hard to display. Despite considerable progress in 3D display technologies, most are still expensive and require special glasses for viewing, so RGBD content is primarily viewed on 2D screens, removing the depth channel from the final viewing experience. We train a generative convolutional neural network that predicts the 2D viewing saliency map for a given frame using the RGBD pixel values and previous fixation estimates in the video. To evaluate the performance of our approach, we present a new comprehensive database of 2D viewing eye-fixation ground-truth for RGBD videos. Our experiments indicate that it is beneficial to integrate depth into video saliency estimates for content that is viewed on a 2D display. We demonstrate that our approach outperforms state-of-the-art methods for video saliency, achieving 15% relative improvement.",,,
10.1145/3377325.3377540,2020,"Aydin, Ali Selman; Feiz, Shirin; Ashok, Vikas; Ramakrishnan, I V",SaIL: Saliency-Driven Injection of ARIA Landmarks.,"Navigating webpages with screen readers is a challenge even with recent improvements in screen reader technologies and the increased adoption of web standards for accessibility, namely ARIA. ARIA landmarks, an important aspect of ARIA, lets screen reader users access different sections of the webpage quickly, by enabling them to skip over blocks of irrelevant or redundant content. However, these landmarks are sporadically and inconsistently used by web developers, and in many cases, even absent in numerous web pages. Therefore, we propose SaIL, a scalable approach that automatically detects the important sections of a web page, and then injects ARIA landmarks into the corresponding HTML markup to facilitate quick access to these sections. The central concept underlying SaIL is visual saliency, which is determined using a state-of-the-art deep learning model that was trained on gaze-tracking data collected from sighted users in the context of web browsing. We present the findings of a pilot study that demonstrated the potential of SaIL in reducing both the time and effort spent in navigating webpages with screen readers.",,,
10.1109/LRA.2020.2965416,2020,"Rudenko, Andrey; Kucner, Tomasz P.; Swaminathan, Chittaranjan S.; Chadalavada, Ravi T.; Arras, Kai O.; Lilienthal, Achim J.",THOR: Human-Robot Navigation Data Collection and Accurate Motion Trajectories Dataset,"Understanding human behavior is key for robots and intelligent systems that share a space with people. Accordingly, research that enables such systems to perceive, track, learn and predict human behavior as well as to plan and interact with humans has received increasing attention over the last years. The availability of large human motion datasets that contain relevant levels of difficulty is fundamental to this research. Existing datasets are often limited in terms of information content, annotation quality or variability of human behavior. In this article, we present THOR, a new dataset with human motion trajectory and eye gaze data collected in an indoor environment with accurate ground truth for position, head orientation, gaze direction, social grouping, obstacles map and goal coordinates. THOR also contains sensor data collected by a 3D lidar and involves a mobile robot navigating the space. We propose a set ofmetrics to quantitatively analyze motion trajectory datasets such as the average tracking duration, ground truth noise, curvature and speed variation of the trajectories. In comparison to prior art, our dataset has a larger variety in human motion behavior, is less noisy, and contains annotations at higher frequencies.",,,
10.1109/ACCESS.2017.2782219,2018,"Lee, Seungjae; Cho, Jaebum; Lee, Byounghyo; Jo, Youngjin; Jang, Changwon; Kim, Dongyeon; Lee, Byoungho",Foveated Retinal Optimization for See-Through Near-Eye Multi Layer Displays,"In order to implement 3-D displays with focus cues, several technologies, including multi-layer displays, have been introduced and studied. In multi-layer displays, a volumetric 3-D scene is represented by 2-D layer images via optimization process. Although this methodology has been thoroughly explored and discussed in optical aspect, the optimization method has not been fully analyzed. In this paper, we deal with pupil movement that may prevent effcient synthesis of layer images. We propose a novel optimization method called foveated retinal optimization, which considers the foveated visual acuity of human. Exploiting the characteristic of human vision, our method has tolerance for pupil movement without gaze tracking while maintaining image definition and accurate focus cues. We demonstrate and verify our method in terms of contrast, visual metric, and experimental results. In experiment, we implement a see- through near-eye display that consists of two display modules, a light guide, and a holographic lens. The holographic lens enables us to design a more compact prototype as performing the roles of an image combiner and floating lens, simultaneously. Our system achieves 38 degrees x19 degrees field of view, continuous focus cues, low aberration, small form factor, and clear see-through property.",,,
10.1117/1.JMI.4.4.045501,2017,"Aizenman, Avi; Drew, Trafton; Ehinger, Krista A; Georgian-Smith, Dianne; Wolfe, Jeremy M",Comparing search patterns in digital breast tomosynthesis and full-field digital mammography: an eye tracking study.,"As a promising imaging modality, digital breast tomosynthesis (DBT) leads to better diagnostic performance than traditional full-field digital mammograms (FFDM) alone. DBT allows different planes of the breast to be visualized, reducing occlusion from overlapping tissue. Although DBT is gaining popularity, best practices for search strategies in this medium are unclear. Eye tracking allowed us to describe search patterns adopted by radiologists searching DBT and FFDM images. Eleven radiologists examined eight DBT and FFDM cases. Observers marked suspicious masses with mouse clicks. Eye position was recorded at 1000Hz and was coregistered with slice/depth plane as the radiologist scrolled through the DBT images, allowing a 3-D representation of eye position. Hit rate for masses was higher for tomography cases than 2-D cases and DBT led to lower false positive rates. However, search duration was much longer for DBT cases than FFDM. DBT was associated with longer fixations but similar saccadic amplitude compared with FFDM. When comparing radiologists' eye movements to a previous study, which tracked eye movements as radiologists read chest CT, we found DBT viewers did not align with previously identified driller or scanner strategies, although their search strategy most closely aligns with a type of vigorous drilling strategy.",,,
10.1109/JSEN.2018.2885355,2019,"Ni, Yuming; Sun, Biao",A Remote Free-Head Pupillometry Based on Deep Learning and Binocular System,"Objective: Pupillometer plays a key role in a variety of research areas, including disease diagnosis, human-machine interaction, and education. Here, we set out to leverage the deep learning theory to develop a remote binocular vision system for pupil diameter estimation. Approach: the system consists of three parts: eye detection, eye tracking, and pupil diameter estimation. We first train a convolutional neural network based on YOLO V2 to perform eye detection, leading to high accuracy and robustness under ambient light interference. By exploring the similarity of binocular camera images, we then propose a master-slave structure for eye tracking, surpassing the traditional parallel structure in tracking speed while keeping considerable accuracy. Furthermore, we develop a pupil diameter estimation algorithm based on binocular vision, avoiding the personal calibration procedure and reducing the measurement distortion error. Main results: Experimental results on real datasets reveal that our system exhibits the state-of-the-art performance with high eye detection accuracy (90.6%), fast eye tracking speed (<11 ms per frame), low pupil diameter estimation error [(0.022 +/- 0.017) mm mean absolute error, and (0.6 +/- 0.7)% percentage of the mean absolute errorl and excellent flexibility. Significance: in contrast with previous pupillometers, which lead to pupil diameter measurement distortion error through a 2-D projection image on a single camera, our system measures pupil diameter in 3-D space without distortion influence, thus improving its robustness to head angle variation and making it more practical for real applications.",,,
10.1109/JSEN.2020.2999625,2020,"Sun, Jie; Lu, Shengli",An Improved Single Shot Multibox for Video-Rate Head Pose Prediction,"Head pose estimation plays a crucial role in attention detection, behavior analysis, human-computer interaction, and eye tracking, etc. The existing landmark-to-pose methods require two steps, not only to detect the key points of the human face, but also to solve the 2D to 3D mapping problem usually by the average head model. We propose an efficient and robust method to monitor the driver's attention. Firstly, the prevailing object detection algorithm SSD which has inherent capabilities of simultaneous classify and regress, is used to create a lightweight network, which avoids the shortcomings of high coupling and time-consuming of the existing methods. Then, single-scale anchors, which have a less computational cost than multi-scale anchors, are adopted for vehicle environments where the ambient light changes dramatically. Finally, by binning continuous angles into specific classes, the 3D angle regression problem is converted into angle classification and face box regression, and our model directly outputs Euler angles (Yaw, Pitch, and Roll) without detecting face landmarks. Experiments on YawDD result that our approach can efficiently perform detection tasks and estimation tasks under the actual driving environment of various luminosity. The mean average errors of prediction in AFLW2000 and 300W-LP are 6.01 degrees and 2.38 degrees, which demonstrates the accuracy of the proposed algorithm.",,,
,,MENGONI M; GENEROSI A,"Method for tracking, analyzing and reacting to user behavior in digital and physical spaces, involves detecting gender and age of user from detected face, and determining reactions on basis of data about characteristics and behavior of user","NOVELTY - The method involves acquiring image using image acquisition device. The face of user is detected within the acquired image. The facial expression is determined from the detected face and, from the facial expression, relevant and predefined category of emotions, and gaze tracking of the user's from the detected face. The gender and age of the user are detected from the detected face. The reactions are determined on the basis of data about the characteristics and behavior of the user comprising the combination of determined facial expression, the gaze detection and the detected gender and age by the fact that it comprises step of activation of reaction by means of active interaction means with the user. USE - Method for tracking, analyzing and reacting to user behavior in digital and physical spaces through use of deep learning technologies. ADVANTAGE - The set of data allows the machine to recognize, automatically and in real time, which actions are best taken on the basis of patterns obtained during the training phase, just as it happens for the recognition of emotions, gender, and age. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for tracking, analyzing and reacting to user behavior in digital and physical spaces. DESCRIPTION Of DRAWING(S) - The drawings show the schematic diagram of the possible application to the generic physical space of the method and system.Natural person (1)Sensor (2)Exhibition area (3)Area (R)",,,
,2019,"Costescu, Cristina; Rosan, Adrian; Brigitta, Nagy; Hathazi, Andrea; Heldal, Ilona; Helgesen, Carsten; Kovari, Attila; Katona, Jozsef; Thill, Serge; Demeter, Robert; Efrem, Igor",Assessing Visual Attention in Children Using GP3 eye Tracker,"The aim of this research is to propose an integrated platform for assessing visual attention in school-aged children. Due to several challenges in the use of standard psychological tests, technology-based instruments can represent a future opportunity for increasing accuracy in the psychological evaluations. The use of GP3 Eye Tracker together with OGAMA application can help to identify children that are at risk of developing learning or attention problems. By developing an integrated platform we will be able to accurately assess their visual attention skills, interpret accurately the data and predict their reading abilities.",,,
,,ZENG H; QU C; LIU Q; LIN Z; CHAI C; PENG Y; HU C,"Method for controlling robot arm grabbing object by combining eye tracking and computer interface, involves finishing grasping and releasing process of gaze object by mechanical arm according to position coordinates and controlling command","NOVELTY - The method involves collecting data of user pupil hole relative position of an eye tracker, a sight line and intersecting position of the eye tracker in a vertical plane. An EEG signal of the user is collected by an EEG helmet. The EEG signal is transmitted to a controller. The data of the eye tracker is analyzed and solved by a controller to obtain three-dimensional (3D) space coordinates of a current gaze point. The three-dimensional space coordinates of the current gaze point is transmitted to a mechanical arm by the controller. Grasping and releasing process of a gaze object is finished by the mechanical arm according to received position coordinates and controlling command. USE - Method for controlling mechanical arm grabbing object by combining 3D eye tracking and brain-computer interface. ADVANTAGE - The method enables realizing human sight and mind controlling process and machine arm rehabilitating process, reducing defect of 2D eye tracking technology in real scene of human-robot interacting process and improving independence ability in daily life. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for controlling mechanical arm grabbing object by combining 3D eye tracking and brain-computer interface. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for controlling mechanical arm grabbing object by combining 3D eye tracking and brain-computer interface. '(Drawing includes non-English language text)'",,,
10.1016/j.apergo.2019.102883,2019,"Iskander, Julie; Hossny, Mohammed; Nahavandi, Saeid",Using biomechanics to investigate the effect of VR on eye vergence system,"Vergence-accommodation conflict (VAC) is the main contributor to visual fatigue during immersion in virtual environments. Many studies have investigated the effects of VAC using 3D displays and expensive complex apparatus and setup to create natural and conflicting viewing conditions. However, a limited number of studies targeted virtual environments simulated using modern consumer-grade VR headsets. Our main objective, in this work, is to test how the modern VR headsets (VR simulated depth) could affect our vergence system, in addition to investigating the effect of the simulated depth on the eye-gaze performance. The virtual scenario used included a common virtual object (a cube) in a simple virtual environment with no constraints placed on the head and neck movement of the subjects. We used ocular biomechanics and eye tracking to compare between vergence angles in matching (ideal) and conflicting (real) viewing conditions. Real vergence angle during immersion was significantly higher than ideal vergence angle and exhibited higher variability which leads to incorrect depth cues that affects depth perception and also leads to visual fatigue for prolonged virtual experiences. Additionally, we found that as the simulated depth increases, the ability of users to manipulate virtual objects with their eyes decreases, thus, decreasing the possibilities of interaction through eye gaze. The biomechanics model used here can be further extended to study muscular activity of eye muscles during immersion. It presents an efficient and flexible assessment tool for virtual environments.",,,
,2018,"Saran, Akanksha; Majumdar, Srinjoy; Short, Elaine Schaertl; Thomaz, Andrea; Niekum, Scott",Human Gaze Following for Human-Robot Interaction,"Gaze provides subtle informative cues to aid fluent interactions among people. Incorporating human gaze predictions can signify how engaged a person is while interacting with a robot and allow the robot to predict a human's intentions or goals. We propose a novel approach to predict human gaze fixations relevant for human-robot interaction tasks-both referential and mutual gaze-in real time on a robot. We use a deep learning approach which tracks a human's gaze from a robot's perspective in real time. The approach builds on prior work which uses a deep network to predict the referential gaze of a person from a single 2D image. Our work uses an interpretable part of the network, a gaze heat map, and incorporates contextual task knowledge such as location of relevant objects, to predict referential gaze. We find that the gaze heat map statistics also capture differences between mutual and referential gaze conditions, which we use to predict whether a person is facing the robot's camera or not. We highlight the challenges of following a person's gaze on a robot in real time and show improved performance for referential gaze and mutual gaze prediction.",,,
10.1007/s10209-016-0521-9,2018,"Arellano, Diana; Rauh, Reinhold; Krautheim, Benjamin; Spicker, Marc; Schaller, Ulrich Max; Helzle, Volker; Deussen, Oliver",Interactive testbed for research in autism-the SARA project,"The project Stylized Animations for Research on Autism (SARA) aims to develop a better understanding of the cognitive processes behind emotional categorization in children and adolescents with high-functioning autism spectrum disorder (ASD), in comparison with neurotypically developed peers. To this end, we combine novel real-time non-photorealistic rendering (NPR) algorithms, emotional facial animations, and eye-tracking technologies in a framework that serves as an interactive testbed for empirical research. In this paper, we focus on three experiments that: (1) validate real-time facial animations of virtual characters, (2) evaluate the NPR algorithms to create abstracted facial expressions, and (3) elucidate the relation between eye gaze behavior, ASD and alexithymia (i.e., difficulties in expressing ones emotions). The results show that our animations can be used in the proposed experiments; however, more evaluation is needed regarding the NPR abstractions, especially with individuals with ASD. Finally, even though no correlation was found between gaze behavior, ASD and alexithymia, the study opened several questions that will be addressed in future work.",,,
,,COLLART T,"System for authorizing rendering of objects in three-dimensional (3D) space, has processor that is configured to apply rules for portions of 3D space to objects that correspond to portions of 3D space","NOVELTY - The system has a communication device that is configured to provide the request which consists of a request to render the volume of 3D space, a request to retrieve the rules for the volume of 3D space, and a request to retrieve the objects for the volume of 3D space. The processor is configured to receive the request to render the volume of 3D space. The processor is configured to retrieve the objects for the volume of 3D space of multiple objects from a first system. The processor is configured to retrieve the rules that are associated with portions of the 3D space included in the volume of 3D space. The processor is configured to apply the rules for portions of 3D space to the objects that correspond to the portions of 3D space and authorize or deauthorize the objects that satisfy the rules, or authorize or deauthorize the objects that do not satisfy the rules. USE - System for authorizing rendering of objects in three-dimensional (3D) space. ADVANTAGE - The rules and rights are defined to minimize or eliminate such misuse, since gaze-tracking is abused to influence peoples biases and decision-making. The registry enables the ability to prevent against unauthorized uses such as preventing children from seeing promotions, advertising, or information related to porn, strip clubs, alcohol, tobacco and guns, preventing children from viewing such experiences while in public places such as restaurants, preventing unwanted promotions, advertising, or other information related to advertisement, and preventing certain experiences or preventing the children from accessing them such as virtual strip clubs or other virtual experiences and games that are deemed inappropriate by the parent. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an extended reality (xR) registration systemConsumer (105)User device (110)Rendering device (115)Secondary device (120)Internet (125)",,,
10.1016/j.aei.2019.100987,2019,"Li, Fan; Chen, Chun-Hsien; Xu, Gangyan; Khoo, Li Pheng; Liu, Yisi",Proactive mental fatigue detection of traffic control operators using bagged trees and gaze-bin analysis,"Most of existing eye movement-based fatigue detectors utilize statistical analysis of fixations, saccades, and blinks as inputs. Nevertheless, these parameters require long recording time and heavily depend on eye trackers. In an effort to facilitate proactive detection of mental fatigue, we introduced a complemental fatigue indicator, named gaze-bin analysis, which simply presents the eye-tracking data with histograms. A method which engaged the gaze-bin analysis as inputs of semisupervised bagged trees was developed. A case study in a vessel traffic service center demonstrated that this approach can alleviate the burden of manual labeling as well as improve the performance of fatigue detection model. In addition, the results show that the approach can achieve an excellent accuracy of 89%, which outperformed other methods. In general, this study provided a complemental indicator for detecting mental fatigue as well as enabled the application of a low sampling rate eye tracker in the traffic control center.",,,
,,HUENIG D,"Stereoscopic head-up display (HUD) for rendering autostereoscopic three dimensional (3D) image, has additional input data comprises desired position of virtual 3D object beyond screen and position of user head relative to transparent screen","NOVELTY - The HUD has a transparent screen for displaying left and right field of a stereoscopic image to generate virtual 3D object perceivable by a user (10) looking through a screen (30). An image processor receiving 3D image data describes the virtual 3D object and additional input data. The image processor calculates based on the 3D image data and additional input data left and right fields of the stereoscopic image (204). The additional input data comprises a desired position of the virtual 3D object beyond the screen and a position of the user head relative to the transparent screen. USE - Stereoscopic head-up display (HUD) for use in commercial aircraft, automobiles, and other applications renders autostereoscopic three dimensional (3D) images. ADVANTAGE - The HUDs should be designed to create a virtual image that is perceived at a virtual depth beyond the physical position of the pane to avoid the need for the driver to adjust accommodation of his/her eyes when changing the focus from the outside scenery to the displayed information. The visual perception of the augmented reality additional parameters is improved to measured and considered by the algorithm which calculates the stereoscopic images from 3D input data. The head and gaze tracking unit used for input datas which need not be static but can rather be dynamically adapted to the actual situation. The actual 3D image is overlaid on real-world elements in a virtual image plane that is beyond the windshield, the actual image projection on the windshield is optically outof focus and a phenomenon referred to as focal blur can be observed, which is also have an undesired impact on the quality of the 3D image. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for operating a head-up display (HUD). DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of lines of sight of a user for the system.User (10)Screen (30)Stereoscopic image (204)",,,
10.3390/s20082384,2020,"Lim, Jia Zheng; Mountstephens, James; Teo, Jason","Emotion Recognition Using Eye-Tracking: Taxonomy, Review and Current Challenges","The ability to detect users' emotions for the purpose of emotion engineering is currently one of the main endeavors of machine learning in affective computing. Among the more common approaches to emotion detection are methods that rely on electroencephalography (EEG), facial image processing and speech inflections. Although eye-tracking is fast in becoming one of the most commonly used sensor modalities in affective computing, it is still a relatively new approach for emotion detection, especially when it is used exclusively. In this survey paper, we present a review on emotion recognition using eye-tracking technology, including a brief introductory background on emotion modeling, eye-tracking devices and approaches, emotion stimulation methods, the emotional-relevant features extractable from eye-tracking data, and most importantly, a categorical summary and taxonomy of the current literature which relates to emotion recognition using eye-tracking. This review concludes with a discussion on the current open research problems and prospective future research directions that will be beneficial for expanding the body of knowledge in emotion detection using eye-tracking as the primary sensor modality.",,,
,,LINDEN E,"Method for training deep learning systems, such as neural networks to detect three dimensional gaze involves accessing training images that comprise set of training images, and first training image is inputted to a neural network","NOVELTY - The method involves accessing training images that comprise a first set of training images and a second set of training images. The first set of training images show user eyes associated with gaze points in a plane of a camera. The second set of training images show user eyes associated with gaze points outside the plane of the camera. A first training image is inputted to a neural network. The first training image belongs to the first set of training images. A second training image is inputted to the neural network. The second training image belongs to the second set of training images. A loss function of the neural network is minimized based on a distance between a gaze point and a gaze line. The gaze line is predicted by the neural network for an user eye shown in the one of the first training image or the second training image. USE - Method for training deep learning systems, such as neural networks to detect three dimensional (3D) gaze. ADVANTAGE - The eye warped image input to the neural network is the same in terms of orientation, that simplify the architecture and the training of the neural network. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the eye tracking system.Eye tracking module (110)First illuminator (111)Second illuminator (112)Image sensor (113)Processor (120)",,,
,,LIU T; WANG Q; QIAN C; WANG K,"Method for judging gaze point, involves obtaining three-dimensional (3D) coordinates of feature points of central region of eyeball in preset three-dimensional coordinate system","NOVELTY - The method involves acquiring (102) two-dimensional (2D) coordinates of an eye feature point of eye of a face in the image. The 3D coordinates of the corresponding eyeball central region feature points is obtained (104) in the 3D face model corresponding to the face in the image in a preset 3D coordinate system. The coordinates of the feature points of the central region of the eyeball is obtained (106) in a preset three-dimensional coordinate system, according to the two-dimensional coordinates of the feature points other than the feature points of the central region of the eyeball. The judgment result of the position of the eye gaze point of the face. USE - Method for judging gaze point. ADVANTAGE - The position of the eye gaze point of the face in the image is obtained. As a result, with the learning of finer information around the eyes of the face in the image, a more accurate judgment of the eye state is achieved, so that more accurate eye state information is obtained. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a gaze point judging device; and(2) a computer storage medium storing program for judging gaze point. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for judging gaze point. (Drawing includes non-English language text)Step for acquiring two-dimensional coordinates of an eye feature point (102)Step for obtaining three-dimensional coordinates of the corresponding eyeball central region feature points (104)Step for obtaining coordinates of the feature points of the central region of the eyeball (106)",,,
10.3390/s18082614,2018,"Hwang, Hyoseok",Automated Calibration Method for Eye-Tracked Autostereoscopic Display,"In this paper, we propose an automated calibration system for an eye-tracked autostereoscopic display (ETAD). Instead of calibrating each device sequentially and individually, our method calibrates all parameters of the devices at the same time in a fixed environment. To achieve this, we first identify and classify all parameters by establishing a physical model of the ETAD and describe a rendering method based on a viewer's eye position. Then, we propose a calibration method that estimates all parameters at the same time using two images. To automate the proposed method, we use a calibration module of our own design. Consequently, the calibration process is performed by analyzing two images captured by onboard camera of the ETAD and the external camera of the calibration module. For validation, we conducted two types of experiments, one with simulation for quantitative evaluation, and the other with a real prototype ETAD device for qualitative assessment. Experimental results demonstrate the crosstalk of the ETAD was improved to 8.32%. The visual quality was also improved to 30.44% in the peak-signal-to-noise ratio (PSNR) and 40.14% in the structural similarity (SSIM) indexes when the proposed calibration method is applied. The whole calibration process was carried out within 1.5 s without any external manipulation.",,,
,2020,"Fang, Yi; Duan, Huiyu; Shi, Fangyu; Min, Xiongkuo; Zhai, Guangtao",IDENTIFYING CHILDREN WITH AUTISM SPECTRUM DISORDER BASED ON GAZE-FOLLOWING,"This paper presents a novel method to identify children with Autism Spectrum Disorder (ASD) based on the stimuli with gaze-following. Individuals with ASD are characterized by having atypical visual attention patterns, especially in social scenes. Gaze-following is considered to be a key element in understanding social scenarios, and it is reasonable to use stimuli with gaze-following to identify the children with ASD. Thus in this paper, we first construct a dataset of eye movements in gaze-following scenes for children with ASD (i.e., GazeFollow4ASD dataset), including 300 images with gaze-following information inside them and the corresponding eye movement data collected from 8 children with ASD and 10 healthy controls. We propose a novel deep neural network (DNN) model to extract discriminative features and classify children with ASD and healthy controls on single images. The proposed model shows the best performance among all compared methods on all datasets.",,,
10.1007/s11042-016-3531-y,2017,"Hsu, Che-Hao; Cheng, Weng-Huang; Hua, Kai-Lung",HoloTabletop: an anamorphic illusion interactive holographic-like tabletop system,"HoloTabletop is a low-cost holographic-like tabletop interactive system. This system analyzes user's head position and gaze location in a real time setting and computes the corresponding anamorphic illusion image. The anamorphic illusion image is displayed on a 2D horizontally-located monitor, yet offers stereo vision to the user. The user is able to view and interact with the 3D virtual objects without wearing any special glasses or devices. The experimental results and user studies verify that the proposed HoloTabletop system offers excellent stereo vision while no visual fatigue will be caused to human eyes. This system is a great solution for many interactive applications such as 3D board games and stereo map browsing.",,,
,,PARK K R; YOON H,"Apparatus for tracking gaze of driver in vehicle, has image synthesizing unit configured for synthesizing ROI images into single three-channel image, and gaze area determination unit that calculates gaze area of driver based on image","NOVELTY - The apparatus (10) has an image acquisition unit (200) that acquires a first face image and a second face image. A feature point detector (300) is configured for detecting feature points in the first and second face images, respectively. A region of interest (ROI) extracting unit (400) is configured for extracting an ROI image based on the feature points. An image synthesizing unit (500) is configured for synthesizing several the ROI images into a single three-channel image. A gaze area determination unit (600) calculates a gaze area of the driver based on the image. USE - Apparatus for tracking gaze of driver in vehicle. ADVANTAGE - The apparatus increases execution speed by synthesizing a driver face images with different imaging angles into a single three-channel image without going through a calibration process. Since the deep learning-based driver tracking device acquires and uses images of different locations in the same situation with dual cameras, accuracy of gaze tracking is improved. The near-infrared LED lighting has little glare, so that does not interfere with the driver's vision or affect driving, and an image for detecting the driver's eyes regardless of the external lighting environment at day and night is acquired. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for tracking a driver's gaze; and(2) a computer program for tracking gaze of driver. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the apparatus for tracking gaze of driver. (Drawing includes non-English language text)Apparatus for tracking gaze of driver (10)Image acquisition unit (200)Feature point detector (300)ROI extracting unit (400)Image synthesizing unit (500)Gaze area determination unit (600)",,,
10.3966/160792642020112106013,2020,"Cheng, Shichao; Zhang, Bocheng; Li, Jianjun; Tang, Zheng; Cengiz, Korhan",Appearance Based Gaze Estimation Using Eye Region Landmarks and Math Approach,"The gaze direction can be defined by the pupil and the center of the eyeball, the latter cannot be observed in the 2D image, which can cause ill-posed problems and cannot achieve highly accurate gaze estimation. Therefore, we try to extract several effective landmarks around the eyeball and iris from the monocular input for gaze estimation. Instead of directly returning the two angles for the pitch and yaw of the eyeball, we return to an intermediate graphical representation, which in turn simplifies the task of 3D gaze estimation. We try to use a novel learning-based method to locate the landmarks of an eyeball with the appearance-based method. Through these high-accuracy feature points, we have proposed a new and effective formula for drawing more accurate gaze directions. As for the individual gaze estimation of independent people, our method is superior to existing model fitting and appearance-based methods.",,,
,,LEE M; LI W,"Human eye tracking based naked eye 3D product viewing and displaying method, involves monitoring image sequence frames by comfort correction module to monitor eye fatigue, and reducing screen effect disparity value to left and right eyes","NOVELTY - The method involves performing face recognition on each frame of an image. Viewing zones and viewing angle of left and right eyes relative to a mobile phone screen are determined by utilizing prediction tracking algorithm. A correct naked eye viewing zone is obtained for a user to view by performing naked eye vision region correction process. A naked eye output view area is adjusted by a naked eye vision zone correction module when receiving view information of a viewing area from a human eye tracking module. A naked eye display mode is changed by the naked eye vision zone correction module according to a current display mode of a 3 D-screen mobile phone application. Image sequence frames are monitored by a comfort correction module to monitor eye fatigue. A screen effect disparity value is reduced to the left and the right eyes. USE - Human eye tracking based naked eye 3D product viewing and displaying method. ADVANTAGE - The method enables allowing a mobile phone user to experience unconstrained naked eye 3D effect, so that a 3D product is viewed by a user for a prolonged time through a naked eye. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a human eye tracking based naked eye 3D product viewing and displaying method. '(Drawing includes non-English language text)'",,,
10.1109/TCSVT.2016.2580425,2017,"Fairchild, Allen J.; Campion, Simon P.; Garcia, Arturo S.; Wolff, Robin; Fernando, Terrence; Roberts, David J.",A Mixed Reality Telepresence System for Collaborative Space Operation,"This paper presents a mixed reality (MR) system that results from the integration of a telepresence system and an application to improve collaborative space exploration. The system combines free viewpoint video with immersive projection technology to support nonverbal communication (NVC), including eye gaze, interpersonal distance, and facial expression. Importantly, these features can be interpreted together as people move around the simulation, maintaining a natural social distance. The application is a simulation of Mars, within which the collaborators must come to agreement over; for example, where the Rover should land and go. The first contribution is the creation of an MR system supporting contextualization of NVC. Two technological contributions are prototyping a technique to subtract a person from a background that may contain physical objects and/or moving images and a lightweight texturing method for multiview rendering, which provides balance in terms of visual and temporal quality. A practical contribution is the demonstration of pragmatic approaches to sharing space between display systems of distinct levels of immersion. A research tool contribution is a system that allows comparison of conventional authored and video-based reconstructed avatars, within an environment that encourages exploration and social interaction. Aspects of system quality, including the communication of facial expression and end-to-end latency are reported.",,,
,,HENRY D J; GLASS D E; CUNNIEN M J; TAYLOR M G; ARMSTRONG M J,System for increasing situational awareness of flight crew member on airborne aircraft has synthetic vision system (SVIS) scene signal generating system that simultaneously generates different two-dimensional (2D) SVIS scene,"NOVELTY - The system (10) has a SVIS scene signal generating system (322) that simultaneously generates a different 2D SVIS scene for each of two different lines of sight (LOS). A three-dimensional (3D) stereoscopic display system (14) provides a different 2D image generated from one of the two 2D SVIS scenes to each eye of a person. The situational awareness of the person is increased when two 2D SVIS scenes are combined into a 3D image. USE - System for increasing situational awareness of flight crew member on airborne aircraft. ADVANTAGE - Enables visual cortex integration of disparate input video channels in an SVIS to provide stereoscopic 3D display. Provides 3D SVIS cost effectively. Provides dynamically variably disparate LOS paths as input into the SVIS projections as a function of phases of flight and environmental conditions, especially under degraded visual environment (DVE) conditions. Allows creating for pilots an alert feature which suggests a sense of urgency of an object where the range in front of the aircraft is close and rapidly decreasing. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method of increasing situational awareness of a member of a flight crew on an aircraft. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the 3D SVIS.System (10)3D stereoscopic display system (14)External dynamic input (32)Head/eye tracking system (36)SVIS scene signal generating system (322)",,,
10.1109/TCE.2019.2897758,2019,"Kim, Jung-Hwa; Choi, Seung-June; Jeong, Jin-Woo",Watch & Do: A Smart IoT Interaction System With Object Detection and Gaze Estimation,"The Internet of Things (IoT) attempts to help people access Internet-connected devices, applications, and services anytime and anywhere. However, how providing an efficient and intuitive method of interaction between people and IoT devices is still an open challenge. In this paper, we propose a novel interaction system called Watch & Do, where users can control an IoT device by gazing at it and doing simple gestures. The proposed system mainly consists of: 1) object detection module; 2) gaze estimation module; 3) hand gesture recognition module; and 4) IoT controller module. The target device is identified by various deep learning-based gaze estimation and object detection techniques. Afterwards, hand gesture recognition is applied to generate an IoT device control command which is transmitted to the IoT platform. The experimental results and case studies demonstrate the feasibility of the proposed system and imply the future research directions.",,,
,,LEE K; ZHENG P; LI K; ZHANG P,"Method for conducting online market research, involves training processing unit using training set that comprises hemoglobin concentration (HC) changes of subjects with known emotional states","NOVELTY - The method involves processing an image sequence using a processing unit that is configured to determine a set of bitplanes of multiple images in the captured image sequence that represent a HC changes of a participant. The participant invisible emotional states are detected (707) based on the HC changes. The detected invisible emotional states are outputted. The processing unit is trained using a training set that comprises HC changes of subjects with known emotional states. USE - Method for conducting online market research. ADVANTAGE - The improved gaze-tracking is achieved, since the calibration is performed by presenting the participant with icons or other images at set locations on the display and directing the participant to look at them or simply at the corners or edges of the display, while capturing images of the participant eyes. The improved signal to noise ratio (SNR) is achieved, since the result is fed back to the machine learning process repeatedly until the SNR reaches an optimal asymptote. The optimized signal differentiation between different emotional states on the facial epidermis is achieved by the improved method. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for conducting online market research. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the fully automated transdermal optical imaging and invisible emotion detection system.Step for registering the image (701)Step for extracting the hemoglobin image (702)Step for selecting the region of interest (703)Step for mapping the data model (706)Step for detecting the participant invisible emotional states (707)",,,
10.1109/TBME.2020.3043215,2021,"Thakoor, Kaveri A.; Koorathota, Sharath C.; Hood, Donald C.; Sajda, Paul",Robust and Interpretable Convolutional Neural Networks to Detect Glaucoma in Optical Coherence Tomography Images,"Recent studies suggest that deep learning systems can now achieve performance on par with medical experts in diagnosis of disease. A prime example is in the field of ophthalmology, where convolutional neural networks (CNNs) have been used to detect retinal and ocular diseases. However, this type of artificial intelligence (AI) has yet to be adopted clinically due to questions regarding robustness of the algorithms to datasets collected at new clinical sites and a lack of explainability of AI-based predictions, especially relative to those of human expert counterparts. In this work, we develop CNN architectures that demonstrate robust detection of glaucoma in optical coherence tomography (OCT) images and test with concept activation vectors (TCAVs) to infer what image concepts CNNs use to generate predictions. Furthermore, we compare TCAV results to eye fixations of clinicians, to identify common decision-making features used by both AI and human experts. We find that employing fine-tuned transfer learning and CNN ensemble learning create end-to-end deep learning models with superior robustness compared to previously reported hybrid deep-learning/machine-learning models, and TCAV/eye-fixation comparison suggests the importance of three OCT report sub-images that are consistent with areas of interest fixated upon by OCT experts to detect glaucoma. The pipeline described here for evaluating CNN robustness and validating interpretable image concepts used by CNNs with eye movements of experts has the potential to help standardize the acceptance of new AI tools for use in the clinic.",,,
10.3390/ijgi6030060,2017,"Liao, Hua; Dong, Weihua",An Exploratory Study Investigating Gender Effects on Using 3D Maps for Spatial Orientation in Wayfinding,"3D representations in applications that provide self-localization and orientation in wayfinding have become increasingly popular in recent years because of technical advances in the field. However, human factors have been largely ignored while designing 3D representations in support of pedestrian navigation. This exploratory study aims to explore gender effects on using 3D maps for spatial orientation. We designed a 3D map that combines salient 3D landmarks and 2D layouts, and evaluated gender differences in their performance during direction-pointing tasks by administrating an eye tracking experiment. The results indicate there was no significant overall gender difference on performance and visual attention. However, we observed that males using the 3D map paid more attention to landmarks in the environment and performed better than when using the conventional 2D map, whereas female performance did not show any significant difference between the two types of map usage. We also observed contrary gender differences in visual attention on landmarks between the 3D and 2D maps. While males fixated longer on landmarks than females when using the 3D map, females paid more visual attention to landmarks than males when using the 2D map. In addition, verbal protocols showed that males had more confidence while make decisions. These empirical results can be helpful in the design of map-based wayfinding enhancement tools.",,,
,2017,"Rajesh, Adarsh; Mantur, Megha",Eyeball Gesture Controlled Automatic Wheelchair Using Deep Learning,"Traditional wheelchair control is very difficult for people suffering from quadriplegia and are hence, mostly restricted to their beds. Other alternatives include Electroencephalography (EEG) based and Electrooculography (EOG) based automatic wheelchairs which use electrodes to measure neuronal activity in the brain and eye respectively. These are expensive and uncomfortable, and are almost impossible to procure for someone from a backward economy. We present a wheelchair system that can be completely controlled with eye movements and blinks that uses deep convolutional neural networks for classification. We have developed a working prototype based on only a small video camera and a microprocessor that shows upwards of 99% accuracy. We also demonstrate the significant improvement in performance over traditional image processing algorithms for the same. This will allow such patients to be more independent in their day to day lives and significantly improve quality of life at an affordable cost.",,,
,,NELSON D; PALCHETTI J L,"Method for operating e.g. video gaming terminal, utilized in casino or arcade for tracking player's eye gaze, involves changing Z-coordinate of apparent spatial position of three-dimensional game component","NOVELTY - The method involves displaying a three-dimensional game component to a viewer of an electronic gaming machine (EGM) (100) detecting a gaze direction of the viewer. The gaze direction of the viewer is compared to an apparent spatial position of the three-dimensional game component. Determination is made to check whether that gaze of the viewer is directed toward the three-dimensional game component. Z-coordinate of the apparent spatial position of the three-dimensional game component is changed in responsive to determining that the gaze of the viewer is directed toward the three-dimensional game component. USE - Method for operating an EGM (claimed) e.g. video gaming terminal and online gaming system, utilized in casino or arcade for tracking a player's eye gaze. ADVANTAGE - The method enables utilizing two cameras to facilitate increased size of a player interaction zone, reduce need for high data rates and significant processing time or delays for image analysis and prevent real time physical player feedback or sensation. The method enables providing ability for a game controller to predict a location of an eye gaze of a player so as to allow a display controller to reduce resolution of game components for increasing efficiency of processing power of the EGM. The method enables utilizing a gaze detection unit may to perform operations of processing image data from data capture camera devices so as to detect a real-time position of the player's eyes in a three-dimensional (3D) space and focus of player's gaze in a two dimensional-space (2D) or 3D space. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an EGM. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of an EGM.Ticket acceptor (24)Bill acceptor (26)Card reader (34)Gaming device (35)Touchpad (36)EGM (100)",,,
,,DAE S K; KWAK N T,"Method for tracing eye gaze for extracting interest region of user at three-dimensional real estate, involves extracting hollow information, defining fitted oval as hollow, and defining center coordinate as center coordinate of hollow",NOVELTY - The method involves determining a highest pixel value as a boundary value after determining pixels of a smoothed pupil area and whitening a hollow candidate area for taking a boundary value of an image. A pupil area and noise area are labeled and classified to remove noise. A binary shape for removing the noise is obtained through labeling phase. The pupil area and hollow information destroyed by fire among images are extracted after reconstitution in an oval fitting. Fitted oval is defined as a hollow. A center coordinate is defined as center coordinate of the hollow. USE - Method for tracing eye gaze for extracting an interest region of a user at three-dimensional (3D) real estate. ADVANTAGE - The method enables extracting coordinate of minute hollow through a wearable type hollow detecting technology using an infrared ray image and eye gaze so as to perform minute extraction of an interest region at 3D real estate by reducing error according to distance among multiple wearers through eye gaze. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a system for tracing eye gaze. '(Drawing includes non-English language text)'Image processing unit (5)Hollow detection module (51)Base estimating gaze module (52)Base position estimation module (53)Interest region module (54),,,
,,ZENG W; LIU L; CUI S,"Driving simulator training system, has eyeball tracking device connected with signal collecting port and signal collecting terminal, three-dimensional target fixed in virtual driving scene object according to evaluation point","NOVELTY - The system has an eye tracking device for collecting data of subject eye gaze point coordinate. A host is provided with first and second signal collecting ports. The second signal collecting port is connected with first and second signal output ports. The first signal collecting port is connected with an output end of a simulation driving device. A display screen displays a virtual driving scene of the simulation driving device. The first signal collecting port collects a subject virtual driving scene executed on the simulation driving device based on three-dimensional (3D) operation data. The second signal collecting port is connected with an eyeball tracking device that is connected with a signal collecting terminal. A three-dimensional target is fixed in a virtual driving scene object of the 3D according to an evaluation point. USE - Driving simulator training system. ADVANTAGE - The system determines eyeball tracking device gazing point coordinates, and has safety performance, high risk identification ability and high defensive driving capability. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a driving simulator training method. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of a driving simulator training system.",,,
,,LI G; HU Y; HE Y; SONG Q,"Three-dimensional human eye tracking device for use in operating room, has camera device provided with camera for obtaining label information to human eyes, and system operation monitoring screen connected with host computer","NOVELTY - The utility model claims a 3D human eye tracking device used in operating room, belonging to the field of medical equipment, the controlling module and the human eye tracking module, determining the target tracking object and eye-trace in the plurality of medical staff; ideal visible area judging module, the ideal visual area adjustment module and a 3 D-image synthesis module, double-eye pair of main knife is moved for tracking to obtain the doctor on the distance between pupil for eyes and double-eye distance to the naked eye 3 D screen centre, obtaining the eye position information to judge whether the ideal visual area falls into two eye positions of main knife doctor at, if it does not fall into the ideal visual area, to move the position of an ideal visual area by adjusting the deflection angle of grating, the ideal visual area into two eye positions of main knife doctor at, then the doctor double-eye position information and image of the patient from the cavity lens is 3 D image obtained by synthesizing information, at last the 3 D image presented by 3 D display screen.",,,
,,LACEY P; MILLER S A; KRAMER N A; LUNDMARK D C,"Wearable system for facilitating interaction with virtual object in three-dimensional (3D) environment, has processor that performs interaction command based on inputs from hand gesture, head pose sensor, and eye gaze sensor","NOVELTY - The wearable system (200) comprises a head pose sensor that is configured to determine a head pose of a user of the wearable system. An eye gaze sensor is configured to determine an eye gaze direction of the user of the wearable system. A gesture sensor is configured to determine a hand gesture of the user of the wearable system. A processor is programmed to determine a first vergence between the eye gaze direction and the head pose of the user relative to an object. A first interaction command associated with the object is performed based on inputs from the head pose sensor and the eye gaze sensor. A second vergence of the hand gesture with the eye gaze direction and the head pose of the user are determined relative to the object. A second interaction command associated with the object is performed based on inputs from the hand gesture, the head pose sensor, and the eye gaze sensor. USE - Wearable system for facilitating interaction with virtual object in three-dimensional (3D) environment. ADVANTAGE - The system minimizes latency for gesture detection for both event gestures and dynamic hand tracking. The threshold for minimum acceptable gesture performance is a bit lower in social experiences, bleeding-edge interactions, and third party application. The wearable system automatically switches the depth sensor to the appropriate gesture mode, and giving feedback to the user. The sensory state is evaluated against the predicted sensory state to generate an error signal, which is leveraged as feedback by the inverse model to correct current movement or improves system performance. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system for generating a transmodal input command; and(2) a method for transmodal input fusion for wearable system. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a wearable system configured to use transmodal input fusion technique.Wearable system (200)Display system (220)Frame (230)Audio sensor (232)Data module (260)",,,
,,CHAPPELL R C; MANNING M; KALBANDE S,"Method for securely processing medical-related eye-tracking data, involves providing dashboard user interface with visual representation of stimulus and prediction from applying previously-trained machine learning model to eye-tracking data","NOVELTY - The method (500) involves presenting a visual stimulus to an experimental subject. The eye-tracking data associated with the behavior of experimental subject during the presentation of the visual stimulus is acquired and stored. The eye-tracking data includes fixation data, saccade data, pupil-size data stored as secondary data and image data stored as primary encrypted data. The secondary and primary data are transmitted in response to a research request authorized by subject for storage in a research database subsequent to decrypting the primary data using the secret key. A machine learning model trained using the eye-tracking data stored in research database is implemented. A dashboard user interface that includes a visual representation of the visual stimulus or the eye-tracking data and a prediction resulting from applying the previously-trained machine learning model to eye-tracking data generated by the experimental subject of visual stimulus is provided to a subscriber system. USE - Method for securely processing and storing medical-related eye-tracking data of eye-tracking system for use by researchers to diagnose medical conditions. ADVANTAGE - The method provides a framework that allows researchers to analyze and build models based on eye-gazing data in which non-sensitive information is made available to the researcher and more sensitive information is provided with permission of the patient and is otherwise encrypted using a secret key known to the patient. The experimental subject is prompted to complete a digital questionnaire that is subsequently stored by the eye-tracking analytics system and provided to the research database. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for securely processing of medical-related eye-tracking data of user-centric use-case.Method for securely processing of medical-related eye-tracking data (500)Step for obtaining consent of patient to undergo test (501)Step for presenting questionnaire for completion by patient (502)Step for performing actual eye-tracking test on patient (503)Step for encrypting primary data using encryption key (505)",,,
10.1007/978-3-319-69900-4_2,2017,"Ali, Asad; Alsufyani, Nawal; Hoque, Sanaul; Deravi, Farzin",Biometric Counter-Spoofing for Mobile Devices Using Gaze Information,"With the rise in the use of biometric authentication on mobile devices, it is important to address the security vulnerability of spoofing attacks where an attacker using an artefact representing the biometric features of a genuine user attempts to subvert the system. In this paper, techniques for presentation attack detection are presented using gaze information with a focus on their applicability for use on mobile devices. Novel features that rely on directing the gaze of the user and establishing its behaviour are explored for detecting spoofing attempts. The attack scenarios considered in this work include the use of projected photos, 2D and 3D masks. The proposed features and the systems based on them were extensively evaluated using data captured from volunteers performing genuine and spoofing attempts. The results of the evaluations indicate that gaze-based features have the potential for discriminating between genuine attempts and imposter attacks on mobile devices.",,,
,,LAGIES A U; ORTIZ-EGEA S; PACE M E,"Method for pre-calibrating eye-tracking apparatus of field-ready head-mounted computing device , involves generating training data set that has first image of trainer's eye and image label, and capturing image of user's eye by image sensor",NOVELTY - The method (400) involves generating a training data set (402) that has the first image of a trainer's eye and an image label. The first image is captured by a first image sensor on the data-collection head-mounted computing device at a moment in time when the trainer is gazing at the designated location. A trained machine learning model is generated (404) using the training data set as a training input by using a regression process. The trained machine learning model is saved (406) to computer memory on the field-ready head-mounted computing device. An image of a user's eye is captured by an image sensor on the field-ready head-mounted computing device. USE - Method for pre-calibrating an eye-tracking apparatus of a field-ready head-mounted computing device. ADVANTAGE - Accurately determines the gaze detection of a specific user. The augmented-reality images may be accurately displayed in desired real-world locations with desired orientations. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computer storage media for pre-calibrating an eye-tracking apparatus of a field-ready head-mounted computing device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of an eye-tracking apparatus pre-calibrating method.Eye-tracking apparatus pre-calibrating method (400)Generating a training data set (402)Generating a trained machine learning model (404)Saving the trained machine learning model (406),,,
10.1109/ACCESS.2020.3041606,2020,"Martinez-Cebrian, Javier; Fernandez-Torres, Miguel-Angel; Diaz-De-Maria, Fernando",Interpretable Global-Local Dynamics for the Prediction of Eye Fixations in Autonomous Driving Scenarios,"Human eye movements while driving reveal that visual attention largely depends on the context in which it occurs. Furthermore, an autonomous vehicle which performs this function would be more reliable if its outputs were understandable. Capsule Networks have been presented as a great opportunity to explore new horizons in the Computer Vision field, due to their capability to structure and relate latent information. In this article, we present a hierarchical approach for the prediction of eye fixations in autonomous driving scenarios. Context-driven visual attention can be modeled by considering different conditions which, in turn, are represented as combinations of several spatio-temporal features. With the aim of learning these conditions, we have built an encoder-decoder network which merges visual features' information using a global-local definition of capsules. Two types of capsules are distinguished: representational capsules for features and discriminative capsules for conditions. The latter and the use of eye fixations recorded with wearable eye tracking glasses allow the model to learn both to predict contextual conditions and to estimate visual attention, by means of a multi-task loss function. Experiments show how our approach is able to express either frame-level (global) or pixel-wise (local) relationships between features and contextual conditions, allowing for interpretability while maintaining or improving the performance of black-box related systems in the literature. Indeed, our proposal offers an improvement of 29% in terms of information gain with respect to the best performance reported in the literature.",,,
,,PARK W C; HWANG I,"Device for displaying three-dimensional foveated image during ray tracing process, has image generator for changeably producing foveated image based on eyes position while setting up central zone setting up eyes position at image","NOVELTY - The device (100) has an image display unit (110) for displaying an image. An eye gaze detection unit (120) detects a user eye gaze for measuring user sight before displaying of an image and looking at the image. A foveated image generator (130) changeably produces a foveated image based on eyes position while setting up a central zone setting up eyes position at an image as a central point. The foveated image generator renders the central zone through ray tracing to first resolution and a remaining area to second resolution. The foveated image generator interpolation renders a part in the remaining area among multiple part blocks. USE - Device for displaying a three-dimensional (3D) foveated image during ray tracing process. ADVANTAGE - The device produces the 3D image stored in a recording medium according to recognizable factor of human. The device traces user eye gaze in a real-time to perform image quality difference being inputted from a central zone of eyes position, thus rendering penalty according to recognizable factor of human. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for displaying a three-dimensional foveated image during ray tracing process(2) a recording medium comprising a set of instructions for displaying a three-dimensional foveated image during ray tracing process. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a device for displaying a 3D foveated image during ray tracing process. '(Drawing includes non-English language text)'Device for displaying 3D foveated image during ray tracing process (100)Image display unit (110)Eye gaze detection unit (120)Foveated image generator (130)Rendering module (134)",,,
10.1088/1748-3190/abfe40,2021,"Rajendran, Sunil Kumar; Wei, Qi; Zhang, Feitian","Two degree-of-freedom robotic eye: design, modeling, and learning-based control in foveation and smooth pursuit","With increasing ocular motility disorders affecting human eye movement, the need to understand the biomechanics of the human eye rises constantly. A robotic eye system that physically mimics the human eye can serve as a useful tool for biomedical researchers to obtain an intuitive understanding of the functions and defects of the extraocular muscles and the eye. This paper presents the design, modeling, and control of a two degree-of-freedom (2-DOF) robotic eye, driven by artificial muscles, in particular, made of super-coiled polymers (SCPs). Considering the highly nonlinear dynamics of the robotic eye system, this paper applies deep deterministic policy gradient (DDPG), a machine learning algorithm to solve the control design problem in foveation and smooth pursuit of the robotic eye. To the best of our knowledge, this paper presents the first modeling effort to establish the dynamics of a robotic eye driven by SCP actuators, as well as the first control design effort for robotic eyes using a DDPG-based control strategy. A linear quadratic regulator-type reward function is proposed to achieve a balance between system performances (convergence speed and tracking accuracy) and control efforts. Simulation results are presented to demonstrate the effectiveness of the proposed control strategy for the 2-DOF robotic eye.",,,
10.1117/12.2293366,2018,"Mall, Suneeta; Brennan, Patrick C.; Mello-Thoms, Claudia",A deep (learning) dive into visual search behaviour of breast radiologists,"Visual search, the process of detecting and identifying objects using the eye movements (saccades) and the foveal vision, has been studied for identification of root causes of errors in the interpretation of mammography. The aim of this study is to model visual search behaviour of radiologists and their interpretation of mammograms using deep machine learning approaches. Our model is based on a deep convolutional neural network, a biologically-inspired multilayer perceptron that simulates the visual cortex, and is reinforced with transfer learning techniques.Eye tracking data obtained from 8 radiologists (of varying experience levels in reading mammograms) reviewing 120 two-view digital mammography cases (59 cancers) have been used to train the model, which was pre-trained with the ImageNet dataset for transfer learning. Areas of the mammogram that received direct (foveally fixated), indirect (peripherally fixated) or no (never fixated) visual attention were extracted from radiologists' visual search maps (obtained by a head mounted eye tracking device). These areas, along with the radiologists' assessment (including confidence of the assessment) of suspected malignancy were used to model: 1) Radiologists' decision; 2) Radiologists' confidence on such decision; and 3) The attentional level (i.e. foveal, peripheral or none) obtained by an area of the mammogram. Our results indicate high accuracy and low misclassification in modelling such behaviours.",,,
,2018,"Yang, Fang-Ying; Tsai, Meng-Jung; Chiou, Guo-Li; Lee, Silvia Wen-Yu; Chang, Cheng-Chieh; Chen, Li-Ling",Instructional Suggestions Supporting Science Learning in Digital Environments Based on a Review of Eye Tracking Studies,"The main purpose of this study was to provide instructional suggestions for supporting science learning in digital environments based on a review of eye tracking studies in e-learning related areas. Thirty-three eye-tracking studies from 2005 to 2014 were selected from the Social Science Citation Index (SSCI) database for review. Through a literature analysis program, CATA,and in-depth content analysis of the research methods and findings, five research theme clusters were abstracted from the selected papers, namely, cognitive activities in processing multimedia presentations, multimedia effects, roles of personal factors, effects of instructional design, and learning with dynamic e-platforms. Based on the results of the in-depth theme analyses, it is suggested that the design of e-learning instruction should consider placing related text and graphics in adjacent areas, using one verbal mode at a time, and providing explicit and clear verbal explanations. When using animations, instructors need to explain carefully the goals and contents of the animations to reduce the extraneous cognitive load. In the dynamic learning environment, a pre-training program is necessary for students to become familiar with the new environment. Finally, individual differences such as background knowledge, cognitive abilities and cognitive styles should be taken into consideration in the instructional design.",,,
,,SULLIVAN M A; PRATT J H; STETTLER G L,"System for adjusting display orientation on screen of devices based on user orientation, has processor for adjusting display orientation associated with content displayed on device such that content is aligned with eye gaze plane of user","NOVELTY - The system has a processor for adjusting a content display orientation associated with content (112) displayed on a display device such that the content is aligned with an eye gaze plane of a user, where the content display orientation is based on the eye gaze plane and a position of a body part proximally positioned relative to an eye of the user. The processor determines the eye gaze plane based on a position of the eye and a position of another eye of the user. The processor determines if a position of the user is changed by a threshold amount. USE - System for adjusting content display orientation on a screen of devices based on user orientation for works, entertainments and tasks. Uses include but are not limited to mobile phones, computer tablets, interactive displays and laptops. ADVANTAGE - The system ensures that the orientation of the content is based on the eye gaze plane and on the position of user's nose relative to the eye gaze plane such that the user enjoys content on the device without worrying about how the user holds the device or if the user changes position. The device processes 3-D content and adjusts user's orientation such that the user can effectively enjoy the content, and is ensured to provide enjoyable viewing experience, thus providing highly accurate and reliable for 3-D scenes while minimizing distortions and errors. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for adjusting content display orientation on a screen of devices based on user orientation(2) a computer-readable device comprising a set of instructions for adjusting content display orientation on a screen of devices based on user orientation. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a display device of in a tilted position that displays media content oriented based on the user eye gaze plane and nose.Device (110)Content (112)Image capture device (115)Reference zone (129)",,,
,,JUNG K; KANG N; LEE K; HONG S; BOO J K; HYUP K N; CHANG L K; HOON H S; ZHENG Y; HONG H,"Method for displaying virtual route in vehicle e.g. aircraft, involves generating three-dimensional (3D) virtual route by registering driving environment model corresponding to driving environment and position of vehicle in map information","NOVELTY - The method involves estimating (110) a position of a vehicle based on sensing data received asynchronously from sensors. A three-dimensional (3D) virtual route is generated (120) by registering a driving environment model corresponding to a driving environment of the vehicle and the position of the vehicle in map information. The 3D virtual route outputted (130). The sensors comprises of a global positioning system (GPS) sensor, an inertial measurement unit (IMU) sensor, an on-board diagnostics (OBD) sensor, and a camera sensor. The sensing data is sampled separately for each of the sensors. The information related to a processing time is inserted into each portion of the sensing data at which the corresponding portion of the sensing data is processed. The position of the vehicle is estimated based on the sensing data in which each includes the information related to the processing time. USE - Method for displaying virtual route in vehicle e.g. aircraft, truck, tractor, scooter, motorcycle, amphibious vehicle, snowmobile, boat, bus, monorail, train, autonomous or automated driving vehicle, unmanned aerial vehicle, drone, etc. ADVANTAGE - The processor tracks the viewpoint of the driver of the vehicle by tracking 3D positions of both eyes of the driver, to predict a transformation relation between the viewpoint of the driver and a virtual image displayed through the head-up display (HUD) based on the 3D positions of both the eyes of the user, and transforms the 3D virtual route based on the transformation relation. The display apparatus transforms a 3D virtual route to match accurately to a road on which the vehicle is driven in reality through driver head or gaze tracking even when a viewpoint of a driver is changed. The position of the vehicle is estimated accurately without using high computing power to synchronize the sensing speeds of the sensors in response to asynchronous reception of the sensing data. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) A non-transitory computer-readable storage medium storing program for displaying virtual route; and(2) an apparatus for displaying virtual route. DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart illustrating a method of displaying a virtual route.Step for estimating a position of a vehicle (110)Step for generating a three-dimensional virtual route by registering a driving environment model (120)Step for outputting the 3D virtual route (130)",,,
10.1007/978-3-030-37218-7_117,2020,"Vani, A.; Mamatha, M. N.",Advanced Techniques to Enhance Human Visual Ability - A Review,"Human eye is one of the most complex organs in human body. Eyes, the integral part of Visual system receives signals and brain computes the obtained visual details. The brain helps in detection and interpretation of information in the visible light. This Complex process includes reception of light, binocular perception, estimation of distance and categorization of objects. Any anomaly in such process causes Blindness or Visually Impairment. According to WHO survey approximately 1.3 billion people live with some form of vision impairment. The need for visual aid is essential to enhance visual ability for visually impaired. In this paper a detailed review of different types of advance techniques to enhance vision is discussed.",,,
,,HONG I; RAVINDRAN S; YOO Y; POLLEY M O; POLLEY M,"Method for performing eye tracking in a head-mountable device, involves determining three dimensional location of eye tracking calibration point within environment based on location of eye tracking calibration point","NOVELTY - The method involves creating a 3D reconstruction map of the environment. A 3D location of the eye tracking calibration point is determined within the environment based on a location of the eye tracking calibration point in the 3D reconstruction map. The detected gaze point is compared to an area of the environment. The HMD is calibrated to correct a difference between the eye tracking calibration point and the detected gaze point using a processor. The eye tracking calibration point is determined based on the detected gaze point being outside of the area. The object is rendered by the head-mountable device (HMD). The gaze point of the user is detected at a time, when the user interacts with object. USE - Method for performing eye tracking in a head-mountable device (Claimed). ADVANTAGE - Natural background calibration processes can maintain accurate eye tracking by recalibrating eye tracking during natural use of a head-mountable device without interrupting a user. Eye tracking calibration typically attempts to correct errors in eye tracking by adjusting an estimated gaze point to match an actual point in 3D space. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a head-mountable device with processor; and(2) a non-transitory computer readable medium having stored instructions for implementing the method for performing eye tracking in a head-mountable device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of method for performing eye tracking in a head-mountable device.Performing initial calibration (702)Determining object within extended reality environment (704)Determining three dimensional location of eye tracking calibration point (706)Detecting gaze point of user (708)Comparing detected gaze point to area (710)",,,
,,SEO J T; KIM M J; TAE S Y; CHOI G H; CHOI J Y,"Method for controlling smart phone controller for controlling three dimensional position of user in space, involves indicating goal eye gaze pin by gaze control pad, and outputting shift coordinate of character of virtual space in device","NOVELTY - The method involves sensing signal of a touch pad by a gyro sensor when a user interface is outputted in a smart phone (100) screen. A goal eye gaze pin (111) is indicated by a goal eye gaze control pad (110). A moving direction confirmation circular pin (121) is indicated by moving the pin in a position of a circular pad. A button (130) is touched to provide interaction for character on multiple parts. A main part is decided by a moving direction confirmation pad (120) after the goal eye gaze control pad is fixed. Shift coordinate of a character of a virtual space is outputted in device. USE - Method for controlling a smart phone controller for controlling a three dimensional (3D) position of a user in a virtual space. ADVANTAGE - The method enables manipulating the character through the smart phone, controlling the character of the virtual space in the smart phone and controlling the goal eye gaze, moving direction and transition speed of the character by utilizing the gyro sensor of the smart phone by utilizing the pin as a virtual operation lever through touch. DETAILED DESCRIPTION - The shift coordinate is xz coordinate. DESCRIPTION Of DRAWING(S) - The drawing shows a front view of a smart phone controller.Smartphone (100)Goal eye gaze control pad (110)Goal eye gaze pin (111)Moving direction confirmation pad (120)Moving direction confirmation circular pin (121)Button (130)",,,
,,PARK C W; WOO P C,"Apparatus for unlocking door i.e. gate door, of e.g. office, has number detection unit for determining whether number of ten-key display unit is included in coordinate area in which three-dimensional coordinate is calculated in calculator","NOVELTY - The apparatus has a ten-key display unit (110) for displaying number for door lock revocation. Photographing units (121-125) are arranged in the ten-key display unit with predetermined gap for taking photograph of a face of a human. A selecting unit selects the photographing units for taking picture of the human and determines position of the human in the image taken by the photographing unit. A coordinate calculator calculates three-dimensional (3-D) coordinate of a spot in which eye gaze is stared. A number detection unit determines whether number of the ten-key display unit is included in a coordinate area in which 3-D coordinate is calculated in a coordinate calculator and detects target number coincided with predetermined number. USE - Apparatus for unlocking a door (claimed) i.e. gate door, of a house, an officetel and an office. ADVANTAGE - The apparatus selects key pad number of a door lock through eye gaze of the human so as to enhance security from exposure or hacking of password. The key pad is arranged in a camera of the door lock that is moved along top and bottom directions so as to recognize eye gaze according to key of the human and accurately sense direction of the eye gaze by selecting the proper camera according to position. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for unlocking a door using eye gaze recognition. DESCRIPTION Of DRAWING(S) - The drawing shows a top view of an apparatus for unlocking a door.Ten-key display unit (110)Photographing units (121-125)",,,
,,BOIKO M; AROL A; PIRSHTUK D,"Method for tracking eye-related parameter during user interaction with electronic computing device, involves inputting time series of eye-gaze vectors into activity tracking neural network, and outputting measure of user engagement",NOVELTY - The method involves obtaining a visual input comprising multiple representations of an eye of a user (101) to continuously track the representations over predetermined time duration. An eye-gaze movement tracking (EGMT) algorithm is applied to the visual input to form a time series of eye-gaze vectors by a processor. The time series of eye-gaze vectors is continuously input into an activity tracking neural network (ATNN) to classify activity of the user over the predetermined time duration by the processor. Measure of the user engagement with the classified activity is output. USE - Method for tracking eye-related parameter during user interaction with an electronic computing device. ADVANTAGE - The method enables executing software by the processor to receive the visual input collected by a camera so as to provide the visual input to the machine learning algorithms to dynamically process the visual input to detect and track a user face in a real-time manner. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for tracking eye-related parameter during user interaction with an electronic computing device. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a system for tracking eye-related parameter during user interaction with an electronic computing device.Environment (100)User (101)Computer (103)Camera (104)Server (105),,,
10.1109/TNSRE.2020.2992885,2020,"Krausz, Nili E.; Lamotte, Denys; Batzianoulis, Iason; Hargrove, Levi J.; Micera, Silvestro; Billard, Aude",Intent Prediction Based on Biomechanical Coordination of EMG and Vision-Filtered Gaze for End-Point Control of an Arm Prosthesis,"We propose a novel controller for powered prosthetic arms, where fused EMG and gaze data predict the desired end-point for a full arm prosthesis, which could drive the forward motion of individual joints. We recorded EMG, gaze, and motion-tracking during pick-and-place trials with 7 able-bodied subjects. Subjects positioned an object above a random target on a virtual interface, each completing around 600 trials. On average across all trials and subjects gaze preceded EMG and followed a repeatable pattern that allowed for prediction. A computer vision algorithm was used to extract the initial and target fixations and estimate the target position in 2D space. Two SVRs were trained with EMG data to predict the x- and y- position of the hand; results showed that the y-estimate was significantly better than the x-estimate. The EMG and gaze predictions were fused using a Kalman Filter-based approach, and the positional error from using EMG-only was significantly higher than the fusion of EMG and gaze. The final target position Root Mean Squared Error (RMSE) decreased from 9.28 cm with an EMG-only prediction to 6.94 cm when using a gaze-EMG fusion. This error also increased significantly when removing some or all arm muscle signals. However, using fused EMG and gaze, there were no significant difference between predictors that included all muscles, or only a subset of muscles.",,,
10.1117/1.JMI.5.3.035502,2018,"Mall, Suneeta; Brennan, Patrick C; Mello-Thoms, Claudia",Modeling visual search behavior of breast radiologists using a deep convolution neural network.,"Visual search, the process of detecting and identifying objects using eye movements (saccades) and foveal vision, has been studied for identification of root causes of errors in the interpretation of mammograms. The aim of this study is to model visual search behavior of radiologists and their interpretation of mammograms using deep machine learning approaches. Our model is based on a deep convolutional neural network, a biologically inspired multilayer perceptron that simulates the visual cortex and is reinforced with transfer learning techniques. Eye-tracking data were obtained from eight radiologists (of varying experience levels in reading mammograms) reviewing 120 two-view digital mammography cases (59 cancers), and it has been used to train the model, which was pretrained with the ImageNet dataset for transfer learning. Areas of the mammogram that received direct (foveally fixated), indirect (peripherally fixated), or no (never fixated) visual attention were extracted from radiologists' visual search maps (obtained by a head mounted eye-tracking device). These areas along with the radiologists' assessment (including confidence in the assessment) of the presence of suspected malignancy were used to model: (1)radiologists' decision, (2)radiologists' confidence in such decisions, and (3)the attentional level (i.e., foveal, peripheral, or none) in an area of the mammogram. Our results indicate high accuracy and low misclassification in modeling such behaviors.",,,
10.3390/s20072101,2020,"Pierdicca, Roberto; Paolanti, Marina; Quattrini, Ramona; Mameli, Marco; Frontoni, Emanuele",A Visual Attentive Model for Discovering Patterns in Eye-Tracking Data-A Proposal in Cultural Heritage,"In the Cultural Heritage (CH) context, art galleries and museums employ technology devices to enhance and personalise the museum visit experience. However, the most challenging aspect is to determine what the visitor is interested in. In this work, a novel Visual Attentive Model (VAM) has been proposed that is learned from eye tracking data. In particular, eye-tracking data of adults and children observing five paintings with similar characteristics have been collected. The images are selected by CH experts and are-the three Ideal Cities (Urbino, Baltimore and Berlin), the Inlaid chest in the National Gallery of Marche and Wooden panel in the Studiolo del Duca with Marche view. These pictures have been recognized by experts as having analogous features thus providing coherent visual stimuli. Our proposed method combines a new coordinates representation from eye sequences by using Geometric Algebra with a deep learning model for automated recognition (to identify, differentiate, or authenticate individuals) of people by the attention focus of distinctive eye movement patterns. The experiments were conducted by comparing five Deep Convolutional Neural Networks (DCNNs), yield high accuracy (more than 80%), demonstrating the effectiveness and suitability of the proposed approach in identifying adults and children as museums' visitors.",,,
,,ZHANG X; LI S,"System for creating control output signals from visual information, comprises a light source, where light source directs light at multiple eyes of user, and optical device, where optical device monitors has multiple eyes and pupils of user","NOVELTY - The control output signals creating system comprises a light source, where the light source directs light at multiple eyes of a user. An optical device, where the optical device monitors has multiple eyes and pupils of a user. A processing element determines and measures movement of the eye and pupil, analyzes the movement to identify intentional movement, and translates the measurement into data describing three dimensional movement, translates the data into an output signal and creates an output signal. The processing element uses fuzzy logic. The output signal is transferred to a robot or assistive device. The robot or assistive device is controlled by multiple output signals. The processing element uses a neural network. The light source forms a pattern of reflections in the multiple eyes. The pattern of reflections defines a coordinate system. USE - System for creating control output signals from visual information for a robotic device, such as such as wheelchairs or robotic prosthetics based on the gaze point. ADVANTAGE - System allows for user to control robotic device, such as robotic assistive device, in smooth and continuous manner and at different speeds based solely on the gaze position of their eyes. The system aid in keeping control procedures simple and aid in reducing the control complexity for human beings. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device for creating control output signals from visual information. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the circuit diagram for the 3D gaze tracking system.",,,
10.1038/sdata.2018.291,2018,"Hollenstein, Nora; Rotsztejn, Jonathan; Troendle, Marius; Pedroni, Andreas; Zhang, Ce; Langer, Nicolas","ZuCo, a simultaneous EEG and eye-tracking resource for natural sentence reading","We present the Zurich Cognitive Language Processing Corpus (ZuCo), a dataset combining electroencephalography (EEG) and eye-tracking recordings from subjects reading natural sentences. ZuCo includes high-density EEG and eye-tracking data of 12 healthy adult native English speakers, each reading natural English text for 4-6 hours. The recordings span two normal reading tasks and one task-specific reading task, resulting in a dataset that encompasses EEG and eye-tracking data of 21,629 words in 1107 sentences and 154,173 fixations. We believe that this dataset represents a valuable resource for natural language processing (NLP). The EEG and eye-tracking signals lend themselves to train improved machine-learning models for various tasks, in particular for information extraction tasks such as entity and relation extraction and sentiment analysis. Moreover, this dataset is useful for advancing research into the human reading and language understanding process at the level of brain activity and eye-movement.",,,
10.3390/rs12030587,2020,"Naqvi, Rizwan Ali; Arsalan, Muhammad; Rehman, Abdul; Rehman, Ateeq Ur; Loh, Woong-Kee; Paul, Anand",Deep Learning-Based Drivers Emotion Classification System in Time Series Data for Remote Applications,"Aggressive driving emotions is indeed one of the major causes for traffic accidents throughout the world. Real-time classification in time series data of abnormal and normal driving is a keystone to avoiding road accidents. Existing work on driving behaviors in time series data have some limitations and discomforts for the users that need to be addressed. We proposed a multimodal based method to remotely detect driver aggressiveness in order to deal these issues. The proposed method is based on change in gaze and facial emotions of drivers while driving using near-infrared (NIR) camera sensors and an illuminator installed in vehicle. Driver's aggressive and normal time series data are collected while playing car racing and truck driving computer games, respectively, while using driving game simulator. Dlib program is used to obtain driver's image data to extract face, left and right eye images for finding change in gaze based on convolutional neural network (CNN). Similarly, facial emotions that are based on CNN are also obtained through lips, left and right eye images extracted from Dlib program. Finally, the score level fusion is applied to scores that were obtained from change in gaze and facial emotions to classify aggressive and normal driving. The proposed method accuracy is measured through experiments while using a self-constructed large-scale testing database that shows the classification accuracy of the driver's change in gaze and facial emotions for aggressive and normal driving is high, and the performance is superior to that of previous methods.",,,
,,SHIN Y; LEE W,"Setting method of glasses-free three-dimensional (3D) display device used in mobile device such as cellular phone, involves storing first setting parameters of adjusted cover alignment pattern image in storage of display device","NOVELTY - The method involves executing (S820) a cover setting program in the display device for a 3D viewing cover mountable on a mobile device. A cover alignment pattern image is displayed (S840) on a flat panel display of the display device based on an eye tracking operation for a face of a user. The cover alignment pattern image is adjusted (S850) and first setting parameter of the adjusted cover alignment pattern image is stored (S860) in storage of the display device. An input of design parameters of the 3D viewing cover into the cover setting program is received. A distance is determined from the display device to eyes of the user by performing the eye tracking operation for the face of the user. The cover alignment pattern image is obtained based on design parameters of the 3D viewing cover and the determined distance from the display device to the eyes of the user. USE - Setting method of glasses-free 3D display device used in mobile device such as cellular phone, smart phone, tablet computer, notebook computer, net-book computer, electronic book (e-book) terminal, navigation, portable multimedia player (PMP), digital multimedia broadcasting (DMB) terminal, and personal digital assistant (PDA). ADVANTAGE - The mobile device can selectively view one of the 3D image and the two-dimensional (2D) image, so that the problem of the image quality of the 2D image being decreased is resolved. The fastening portion can be configured in a variety of methods as long as the fastening portion fixes the cover frame to the mobile device such that the cover frame may be stabilized, held or prevented from moving. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a setting method of glasses-free 3D display device.Step for mounting 3D viewing cover onto mobile device (S810)Step for executing a cover setting program in the display device for a 3D viewing cover mountable on a mobile device (S820)Step for displaying cover alignment pattern image on a flat panel display of the display device (S840)Step for adjusting cover alignment pattern image (S850)Step for storing first setting parameters of adjusted cover alignment pattern image in storage of display device (S860)",,,
10.1117/12.2268617,2017,"Olesova, Veronika; Benesova, Wanda; Polatsek, Patrik",Visual Attention in Egocentric Field-of-view using RGB-D Data.,"Most of the existing solutions predicting visual attention focus solely on referenced 2D images and disregard any depth information. This aspect has always represented a weak point since the depth is an inseparable part of the biological vision. This paper presents a novel method of saliency map generation based on results of our experiments with egocentric visual attention and investigation of its correlation with perceived depth. We propose a model to predict the attention using superpixel representation with an assumption that contrast objects are usually salient and have a sparser spatial distribution of superpixels than their background. To incorporate depth information into this model, we propose three different depth techniques. The evaluation is done on our new RGB-D dataset created by SMI eye-tracker glasses and KinectV2 device.",,,
10.1007/s11042-018-5953-1,2018,"Wu, Jiaxin; Zhong, Sheng-hua; Ma, Zheng; Heinen, Stephen J.; Jiang, Jianmin",Foveated convolutional neural networks for video summarization,"With the proliferation of video data, video summarization is an ideal tool for users to browse video content rapidly. In this paper, we propose a novel foveated convolutional neural networks for dynamic video summarization. We are the first to integrate gaze information into a deep learning network for video summarization. Foveated images are constructed based on subjects' eye movements to represent the spatial information of the input video. Multi-frame motion vectors are stacked across several adjacent frames to convey the motion clues. To evaluate the proposed method, experiments are conducted on two video summarization benchmark datasets. The experimental results validate the effectiveness of the gaze information for video summarization despite the fact that the eye movements are collected from different subjects from those who generated summaries. Empirical validations also demonstrate that our proposed foveated convolutional neural networks for video summarization can achieve state-of-the-art performances on these benchmark datasets.",,,
10.1109/TBME.2018.2868759,2019,"Siddharth; Patel, Aashish N.; Jung, Tzyy-Ping; Sejnowski, Terrence J.",A Wearable Multi-Modal Bio-Sensing System Towards Real-World Applications,"Multi-modal bio-sensing has recently been used as effective research tools in affective computing, autism, clinical disorders, and virtual reality among other areas. However, none of the existing bio-sensing systems support multi-modality in a wearable manner outside well-controlled laboratory environments with research-grade measurements. This paper attempts to bridge this gap by developing a wearable multi-modal bio-sensing system capable of collecting, synchronizing, recording, and transmitting data from multiple bio-sensors: PPG, EEG, eye-gaze headset, body motion capture, GSR, etc., while also providing task modulation features including visual-stimulus tagging. This study describes the development and integration of various components of our system. We evaluate the developed sensors by comparing their measurements to those obtained by a standard research-grade bio-sensors. We first evaluate different sensor modalities of our headset, namely, earlobe-based PPG module with motion-noise canceling for ECG during heart-beat calculation. We also compare the steady-state visually evoked potentials measured by our shielded dry EEG sensors with the potentials obtained by commercially available dry EEG sensors. We also investigate the effect of head movements on the accuracy and precision of our wearable eye-gaze system. Furthermore, we carry out two practical tasks to demonstrate the applications of using multiple sensor modalities for exploring previously unanswerable questions in bio-sensing. Specifically, utilizing bio-sensing, we show which strategy works best for playing Where is Waldo? visual-search game, changes in EEG corresponding to true vs. false target fixations in this game, and predicting the loss/draw/win states through bio-sensing modalities while learning their limitations in a Rock-Paper-Scissors game.",,,
,2019,"Esfahlani, Shabnam Sadeghi; Shirvani, Hassan; Esfahlani, Karim Sadeghi",Video Game and Fuzzy Logic to Improve Amblyopia and Convergence Insufficiency,"Intuitive learning of visual tasks appears to be an assuring exercise for Amblyopia and Convergence Insufficiency (CI) improvement. Amblyopia is a developmental dysfunction of vision identified by poor monocular visual acuity, whereas CI is a common binocular/two-eyed vision disorder in which the eyes do not operate near efficiently. This study proposed the development of virtual reality (VR) video game platform, guided through Fuzzy Logic, targeting casualties with amblyopia and CI condition. The game enables precise control over stimulus parameters, and trains contrast sensitivity with the benefits of motivation and reward that maintain practice over long periods. The dichoptic visual training facilitated through the VR headset and game engine with eye-tracking and stimulus data collection capability. This non-invasive eye-training exercise program aims to train the patients to overcome the common conditions such as lazy eye and exophoria at near. Our vision lab made of 2D and 3D game environments facilitated exercises, which are recommended by opticians1, through three stimulating video game scenarios. The preliminary results of this study have shown that this program has the potential to be adopted for vision therapy. As such in the future study, a randomized clinical trial with participants 9 to 18 years of age will randomly be assigned to receive 12 weeks of vision lab versus home-based pencil push-ups and vision patching exercise for statistical analysis and validation.",,,
10.1109/ICCVW.2019.00543,2019,"Lu, Minlong; Liao, Danping; Li, Ze-Nian",Learning Spatiotemporal Attention for Egocentric Action Recognition,"Recognizing camera wearers' actions from videos captured by the head-mounted camera is a challenging task. Previous methods often utilize attention models to characterize the relevant spatial regions to facilitate egocentric action recognition. Inspired by the recent advances of spatiotemporal feature learning using 3D convolutions, we propose a simple yet efficient module for learning spatiotemporal attention in egocentric videos with human gaze as supervision. Our model employs a two-stream architecture which consists of an appearance-based stream and motion-based stream. Each stream has the spatiotemporal attention module (STAM) to produce an attention map, which helps our model to focus on the relevant spatiotemporal regions of the video for action recognition. The experimental results demonstrate that our model is able to outperform the state-of-the-art methods by a large margin on the standard EGTEA Gaze+ dataset and produce attention maps that are consistent with human gaze.",,,
,,SANG X; WANG K; XING S; ZHANG H,"Human eye tracking based naked eye three-dimensional video playing system, has three-dimensional image synthesis module for synthesizing two parallax images to obtain corresponding stereoscopic video image","NOVELTY - The system has a human eyes tracking module for obtaining human eyes coordinate data when playing a video file. A viewpoint arrangement matrix calculating module obtains viewpoint arrangement two-dimensional matrix according to display parameter of a display terminal, the video file and the human eyes coordinate data. A three-dimensional (3D) image synthesis module obtains two parallax images of current time in the video file according to the viewpoint arrangement two-dimensional matrix and synthesizes two parallax images to obtain corresponding stereoscopic video image, where video file is played by an Android (RTM: Linux-based operating system) system. USE - Human eye tracking based naked eye 3D video playing system. ADVANTAGE - The system solves problem of pseudo-stereoscopic image of the naked eye 3D video playing system under a mobile terminal platform by adding the human eyes. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a human eye tracking based naked eye 3D video playing method(2) an electronic device comprising a processor and a memory for performing human eye tracking based naked eye 3D video playing process; and(3) a non-transitory computer-readable storage medium for storing a set of instructions for performing human eye tracking based naked eye 3D video playing process. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a human eye tracking based naked eye 3D video playing system. (Drawing includes non-English language text).",,,
,,SENGELAUB T; GAO H; QIN H; BENNDORF J,"Method for providing remote eye tracking for electronic device e.g. mobile phone, involves determining gaze direction in three dimensional (3D) coordinate system based on first location associated with attribute and second location",NOVELTY - The method (400) involves detecting a first attribute of an eye based on pixel differences associated with different wavelengths of light in a first image of the eye. The first location associated with the first attribute in a 3D coordinate system is determined based on depth information from a depth sensor. A second attribute of the eye is detected based on a glint resulting from light of the illumination source reflecting off a cornea of the eye. A second location associated with the second attribute is determined in the 3D coordinate system based on the depth information from the depth sensor. A gaze direction in the 3D coordinate system is determined based on the first location and the second location. USE - Method for providing remote eye tracking for electronic device (claimed) e.g. mobile phone. ADVANTAGE - The tracking mode of remote eye tracking avoids the repeated detection of the specific 3D spatial eyeball rotation center position by establishing transformation between the head pose and eyeball rotation center. The tracking mode of remote eye tracking uses the infrared structured light pattern to determine the head pose and then the feature detection to provide a current second location or estimate a current 3D position of the cornea center. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an electronic device;(2) a system for providing remote eye tracking for electronic device; and(3) a non-transitory computer-readable storage medium storing program for providing remote eye tracking for electronic device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for providing remote eye tracking.Method for providing remote eye tracking (400)Eyelid detection process (410)Limbus refinement process (415)Iris reconstruction process (425)Three dimensional iris center (430),,,
,,KIM H; TAMAMA H; LEE S; HONG I; JIN H; LI S; HONG R,"Method for analyzing objects in images recorded by camera of head mounted device, involves cropping image based upon bounding box to generate cropped image and performing fine cropping of cropped image, and detecting object in cropped image","NOVELTY - The method involves performing eye tracking while recording the images. A region of interest of an image is determined based upon the eye tracking. A bounding box is generated based upon the region of interest. An image is cropped based upon the bounding box to generate a cropped image. A fine cropping of the cropped image is performed. An object is detected in the cropped image. An object detection confidence value is generated associated with the object in the image, and using the object detection confidence value to determine a region of interest of a later image. USE - Method for analyzing objects in images recorded by camera of head mounted device, using electronic device (claimed). ADVANTAGE - The method provides improved accuracy and response speed for object detection. The power consumption is reduced for object tracking while maintaining object detection/recognition accuracy. The region-of-interest (ROI) is provided to deep-learning engine for object detection/recognition to reduce the processing time while keeping high accuracy. The deep-learning network is beneficial for object detection due to flexibility, high accuracy performance, simultaneous multi-object detection features. The amount of time necessary to perform object tracking is significantly reduced by using both coarse cropping and fine cropping. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an electronic device for analyzing objects in images recorded by a camera of a head mounted device. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view illustrating the process of identifying objects associated with images captured by an image recording device.Image recording circuit (202)Resize block (204)Deep learning block (206)",,,
,,XIA Z,"Method for displaying naked 3D eye based on human eye tracking, involves determining relationship between distance from point on axis to origin of axis and width of layout period, and determining current layout period of display screen","NOVELTY - The method involves determining an axis perpendicular to a display screen. A target position is determined, where eyes of people are located at a point in space. The target position is vertically extended to the perpendicular axis to obtain a pedal. A relationship between a distance from a point on the axis to an origin of the axis and a width of the layout period is determined. A current layout period width of the display screen is determined. Three points on the axis are selected. Coordinates of each point output in a human eye tracking system are determined. The display screen is vertically arranged toward a viewer. USE - Method for displaying naked 3D eye based on a human eye tracking. ADVANTAGE - The method enables arranging perpendicular to a coordinate axis of the human eye tracking system of the display screen, obtaining correct adjustment arrangement diagram parameter according to the position of the viewer so as to avoid crosstalk problem. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a device for displaying naked 3D eye based on a human eye tracking(2) a computer readable storage medium for storing a set of instructions for displaying naked 3D eye based on a human eye tracking. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for displaying naked 3D eye based on a human eye tracking. '(Drawing includes non-English language text)'",,,
,,LINDEN E,"Non-transitory computer-readable storage medium for generating three-dimensional gaze predictions by using e.g. tablet, has set of instructions for predicting gaze line by neural network for user eye shown in one of training images","NOVELTY - The medium has a set of instructions for training a neural network based on two sets of training images. The first training image is input to the neural network, where the first training image belongs to the first set of training images. A second training image is input to the neural network, where the second training image belongs to the second set of training images. A loss function of the neural network is minimized based on a distance between a gaze point and a gaze line. The gaze point is associated with one of the first training images or the second training image. The gaze line is predicted by the neural network for a user eye shown in one of the first training image or the second training image. USE - Non-transitory computer-readable storage medium for generating 3D gaze predictions by using a computer device such as based on a deep learning system. Uses include but are not limited to a personal computer, a tablet, a smartphone and a wearable headset device e.g. virtual reality headset and augmented reality headset. ADVANTAGE - The medium has a set of instructions for predicting the gaze line by the neural network for the user eye shown in one of the first training image or the second training image, thus realizing simple neural network training operation. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a computer system(2) a computer-implemented method for generating three-dimensional (3D) gaze predictions. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an eye tracking system.Eye tracking module (110)Illuminators (111, 112)Image sensor (113)Processor (120)",,,
,,KEI K S; CLARK A W; KEI K; CLARKE A W,"Attention determining system, has machine learning analyzer which extracts user position characteristics from stream of images of user in front of computing device to be compared with images in database","NOVELTY - The attention determining system (100) has a database (102) comprising images indicative that users are paying attention to corresponding computing devices and images indicative that users are not paying attention to corresponding computing devices. A machine learning analyzer (104) extracts multiple user position characteristics from a stream of images of a user disposed in front of a computing device. A comparer (106) determines, by comparing the stream of images of the user to the images in the database, whether the user is paying attention to the computing device. USE - System for determining whether users are paying attention to their respective computing devices, e.g., desktop computers, laptops, tablets. ADVANTAGE - Provides a simple way to determine whether a single user, or multiple users, are focusing on a computing device screen by using other user position information such as head position, body position, and eye position aside from eye gaze. Relying on more than just eye gaze provides a more accurate determination as to attention. The system relying on multiple user position characteristics provides a more accurate determination of user attention to a computing device screen. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for determining attention; and(2) a tangible machine-readable storage medium encoded with instructions executable by a processor. DESCRIPTION Of DRAWING(S) - The drawing shows the block diagram of an attention determining system.Attention determining system (100)Database (102)Machine learning analyzer (104)Comparer (106)",,,
,,XIA Z,"Human naked eye tracking based 3D content displaying method, involves determining width of target arrangement diagram, and adjusting and displaying contents in drawing layout according to width of target arrangement diagram","NOVELTY - The method involves determining geometric relationship between a first camera and viewer eyes according to a second camera set on a display screen. Current distance between the viewer eyes and the display screen is determined. Width of a target arrangement diagram corresponding to the current distance of the viewer eyes is determined based on preset mapping relationship between the distance of the viewer eyes and the display screen and drawing layout period. Contents in the drawing layout are adjusted and displayed according to the width of the target arrangement diagram. USE - Human naked eye tracking based 3D content displaying method. ADVANTAGE - The method enables simplifying width target arrangement diagram determining process to adjust the content in the drawing layout in a real time manner so as to improve target arrangement diagram width determination accuracy and naked-eye 3D content displaying effect, thus increasing user experience. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a human naked eye tracking based 3D content display device(2) a computer readable storage medium for storing a set of instructions for displaying 3D image based on human naked eye tracking. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a human naked eye tracking based 3D content displaying method. '(Drawing includes non-English language text)'",,,
10.1109/AIVR46125.2019.00020,2019,"Vielhaben, Johanna; Camalan, Hueseyin; Samek, Wojciech; Wenzel, Markus",Viewport Forecasting in 360 degrees Virtual Reality Videos with Machine Learning,"Objective. Virtual reality (VR) cloud gaming and 360 degrees video streaming are on the rise. With a VR headset, viewers can individually choose the perspective they see on the head-mounted display by turning their head, which creates the illusion of being in a virtual room. In this experimental study, we applied machine learning methods to anticipate future head rotations (a) from preceding head and eye motions, and (b) from the statistics of other spherical video viewers. Approach. Ten study participants watched each 31/3 hours of spherical video clips, while head and eye gaze motions were tracked, using a VR headset with a built-in eye tracker. Machine learning models were trained on the recorded head and gaze trajectories to predict (a) changes of head orientation and (b) the viewport from population statistics. Results. We assembled a dataset of head and gaze trajectories of spherical video viewers with great stimulus variability. We extracted statistical features from these time series and showed that a Support Vector Machine can classify the range of future head movements with a time horizon of up to one second with good accuracy. Even population statistics among only ten subjects show prediction success above chance level. Significance. Viewport forecasting opens up various avenues to optimize VR rendering and transmission. While the viewer can see only a section of the surrounding 360 degrees sphere, the entire panorama has typically to be rendered and/or broadcast. The reason is rooted in the transmission delay, which has to be taken into account in order to avoid simulator sickness due to motion-to-photon latencies. Knowing in advance, where the viewer is going to look at may help to make cloud rendering and video streaming of VR content more efficient and, ultimately, the VR experience more appealing.",,,
10.1145/3131277.3132180,2017,"Pfeuffer, Ken; Mayer, Benedikt; Mardanbegi, Diako; Gellersen, Hans",Gaze plus Pinch Interaction in Virtual Reality,"Virtual reality affords experimentation with human abilities beyond what's possible in the real world, toward novel senses of interaction. In many interactions, the eyes naturally point at objects of interest while the hands skilfully manipulate in 3D space. We explore a particular combination for virtual reality, the Gaze + Pinch interaction technique. It integrates eye gaze to select targets, and indirect freehand gestures to manipulate them. This keeps the gesture use intuitive like direct physical manipulation, but the gesture's effect can be applied to any object the user looks at whether located near or far. In this paper, we describe novel interaction concepts and an experimental system prototype that bring together interaction technique variants, menu interfaces, and applications into one unified virtual experience. Proof-of-concept application examples were developed and informally tested, such as 3D manipulation, scene navigation, and image zooming, illustrating a range of advanced interaction capabilities on targets at any distance, without relying on extra controller devices.",,,
10.1016/j.neucom.2020.01.028,2020,"Sattar, Hosnieh; Fritz, Mario; Bulling, Andreas",Deep gaze pooling: Inferring and visually decoding search intents from human gaze fixations,"Predicting the target of visual search from human eye fixations (gaze) is a difficult problem with many applications, e.g. in human-computer interaction. While previous work has focused on predicting specific search target instances, we propose the first approach to predict categories and attributes of search intents from gaze data and to visually reconstruct plausible targets. However, state-of-the-art models for categorical recognition, in general, require large amounts of training data, which is prohibitive for gaze data. To address this challenge, we further propose a novel Gaze Pooling Layer that combines gaze information with visual representations from Deep Learning approaches. Our scheme incorporates both spatial and temporal aspects of human gaze behavior as well as the appearance of the fixated locations. We propose an experimental setup and novel dataset and demonstrate the effectiveness of our method for gaze-based search target prediction and reconstruction. We highlight several practical advantages of our approach, such as compatibility with existing architectures, no need for gaze training data, and robustness to noise from common gaze sources. (C) 2020 Elsevier B.V. All rights reserved.",,,
10.1109/VR50410.2021.00032,2021,"Asahina, Ray; Nomoto, Takashi; Yoshida, Takatoshi; Watanabe, Yoshihiro",Realistic 3D Swept-Volume Display with Hidden-Surface Removal Using Physical Materials,"Conventional swept-volume displays can provide accurate physical cues for depth perception. However, the corresponding texture reproduction does not have high quality because such displays employ high-speed projectors with low bit-depth and low resolution. In this study, to address the limitation of swept-volume displays while retaining their advantages, a novel swept-volume three-dimensional (3D) display is proposed by incorporating physical materials as screens. Physical materials such as wool, felt, and so on are directly used for reproducing textures on a displayed 3D surface. Furthermore, we introduce the adaptive pattern generation based on real-time viewpoint tracking to perform the hidden-surface removal. Our algorithm leverages the ray-tracing concept and can run at high speed on GPU.",,,
10.1073/pnas.1617251114,2017,"Padmanaban, Nitish; Konrad, Robert; Stramer, Tal; Cooper, Emily A.; Wetzstein, Gordon",Optimizing virtual reality for all users through gaze-contingent and adaptive focus displays,"From the desktop to the laptop to the mobile device, personal computing platforms evolve over time. Moving forward, wearable computing is widely expected to be integral to consumer electronics and beyond. The primary interface between a wearable computer and a user is often a near-eye display. However, current generation near-eye displays suffer from multiple limitations: they are unable to provide fully natural visual cues and comfortable viewing experiences for all users. At their core, many of the issues with near-eye displays are caused by limitations in conventional optics. Current displays cannot reproduce the changes in focus that accompany natural vision, and they cannot support users with uncorrected refractive errors. With two prototype neareye displays, we show how these issues can be overcome using display modes that adapt to the user via computational optics. By using focus-tunable lenses, mechanically actuated displays, and mobile gaze-tracking technology, these displays can be tailored to correct common refractive errors and provide natural focus cues by dynamically updating the system based on where a user looks in a virtual scene. Indeed, the opportunities afforded by recent advances in computational optics open up the possibility of creating a computing platform in which some users may experience better quality vision in the virtual world than in the real one.",,,
,,AHUJA K; DEY K; NAGAR S; VACULIN R,"Method for determining dynamic facial landmarks for head pose and gaze estimation by monocular cameras, involves determining set of landmarks of head from image of head, and determining gaze vector for head based on set of landmarks","NOVELTY - The method involves determining a set of landmarks of a head from an image (502A) of a head of a stream of images of the head by a system operatively coupled to a processor by adding the landmark of another set of landmarks that is visible in the image to the latter set of landmarks, where the set of landmarks comprises a defined quantity of landmarks and a landmark that is not in the latter set of landmarks of the head used for gaze estimation associated with another image of the head that is prior to the former image in the stream of images and the defined quantity is four non-planar landmarks. A gaze vector for the head is determined based on the former set of landmarks. USE - Method for creating a three-dimensional (3D) head model and determining dynamic facial landmarks for head pose and gaze estimation by monocular cameras. Uses include but are not limited to a mobile phone camera, laptop camera, tablet camera and a security camera. ADVANTAGE - The method enables avoiding wasted usage of processing, storage and network bandwidth resources by mitigating need to obtain stereoscopic image information by providing accurate real-time gaze estimation/tracking from a live stream of captured images. DESCRIPTION Of DRAWING(S) - The drawing shows a photograph representing an image with landmarks to be identified.Image (502A)Left eye corners (504A, 504C)Left eye pupil (504B)Right eye corners (504D, 504F)Right eye pupil (504E)Mouth corners (504G, 504H)",,,
,,BOOTH J A; CHAURASIA G; ICHIM A; LOCHER A; NORIS G; HORNUNG A S; WERLBERGER M,"Method for reconstructing essential visual cues in mixed reality applications involves identifying set of features associated with one or more objects in a real-world scene, and two-dimensional geometry is generated","NOVELTY - The method involves identifying a set of features associated with one or more objects in a real-world scene. The two-dimensional (2D) geometry is generated that represents two-dimensional geometric attributes of the real-world scene. Three-dimensional (3D) geometry is generated based on the 2D geometry. The 3D geometry includes a coarse geometric reconstruction of the real-world scene. A first graphical representation of the real-world scene is rendered for display. USE - Method for reconstructing essential visual cues in mixed reality applications. ADVANTAGE - The mixed reality (MR) system update reconstruction much faster compared, since the MR system maps textures derived from the real-world environment onto the geometric reconstruction in real time, and output mixed reality simulations with negligible latency. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a non-transitory computer-readable medium storing instructions; and(2) a system for reconstructing essential visual cues in mixed reality applications. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of system for implementing reconstruction method.Display (112)Eye tracking module (116)Head tracking module (118)Preprocessor (132)Feature identifier (134)",,,
,,XIA Z; XIE C; YU Y,"Eye tracking based naked-eye three-dimensional display method, involves executing translation process, determining pixel coordinate of camera according to coordinate system, and fixing grating film in arrangement diagram datum line","NOVELTY - The method involves combining pre-calibrating starting offset of both eyes according to moving direction and moving distance. Translation process is executed to display content in a display screen, where the starting offset is distance from an arrangement diagram period starting point to an arrangement diagram period reference point. A pixel coordinate of a camera is determined according to a coordinate system. A grating film is fixed in an arrangement diagram datum line. Two images are arranged within width of the arrangement diagram datum line. USE - Eye tracking based naked-eye 3D display method. ADVANTAGE - The method enables realizing ideal naked-eye 3D effect. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an eye tracking based naked-eye three-dimensional (3D) control system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating an eye tracking based naked-eye three-dimensional display method. '(Drawing includes non-English language text)'",,,
,,DAI H,"Human eye tracking based naked-eye 3D displaying method, involves determining left-eye imaging coordinate, determining current row image coordinate, and obtaining display content of mobile terminal based on current row image coordinate","NOVELTY - The method involves determining a current mobile terminal arrangement diagram corresponding to current position coordinate. Pre-camera position is determined according to the current mobile terminal arrangement diagram. Layout parameters are determined based on the pre-camera position. Judgment is made to check whether relative position of the mobile terminal screen is not changed. Left-eye imaging coordinate and right-eye imaging coordinate is determined. Current row image coordinate is determined based on the left-eye imaging coordinate and right-eye imaging coordinate. Display content of the mobile terminal is obtained based on the current row image coordinate. USE - Human eye tracking based naked-eye 3D displaying method. ADVANTAGE - The method enables realizing different posture of the mobile terminal, reducing naked eye display time and increasing layout parameter sharing efficiency. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a human eye tracking based naked-eye 3D displaying device(2) a terminal(3) a computer readable storage medium. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a human eye tracking based naked-eye 3D displaying method. '(Drawing includes non-English language text)'",,,
10.1007/s10846-016-0410-8,2017,"Muhammad, Wasif; Spratling, Michael W.",A Neural Model of Coordinated Head and Eye Movement Control,Gaze shifts require the coordinated movement of both the eyes and the head in both animals and humanoid robots. To achieve this the brain and the robot control system needs to be able to perform complex non-linear sensory-motor transformations between many degrees of freedom and resolve the redundancy in such a system. In this article we propose a hierarchical neural network model for performing 3-D coordinated gaze shifts. The network is based on the PC/BC-DIM (Predictive Coding/Biased Competition with Divisive Input Modulation) basis function model. The proposed model consists of independent eyes and head controlled circuits with mutual interactions for the appropriate adjustment of coordination behaviour. Based on the initial eyes and head positions the network resolves redundancies involved in 3-D gaze shifts and produces accurate gaze control without any kinematic analysis or imposing any constraints. Furthermore the behaviour of the proposed model is consistent with coordinated eye and head movements observed in primates.,,,
,,YU X; CHEN Q; YU F; ZHANG X; SUN W; MA S,"Method for performing personalized course recommendation based on eye tracking technology and deep learning, involves obtaining user click-browsing historical behavior data, where classification preprocessing is performed","NOVELTY - The method involves obtaining a user click-browsing historical behavior data, where the classification preprocessing is performed to obtain an input vector of a user behavior. The vectors are inputted according to the user behavior, where a deep attention factorization machines model is used to generate the user behavior feature vectors. The user low-level and high-level feature implicit vectors and the feature weights are learned respectively according to the user behavior feature vectors, which are generated by the deep attention factorization machines model. The high-level features implicit vector and the low-level feature implicit vector are combined to obtain a user behavior implicit vector through the vector splicing. USE - Method for performing the personalized course recommendation based on the eye tracking technology and the deep learning. ADVANTAGE - The user click-browsing historical behavior data is obtained, where the classification preprocessing is performed to obtain an input vector of a user behavior, and hence ensures improving the effect of personalized recommendation in an effective manner. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a personalized course recommendation system with a preprocessing module;(2) a medium stored with a program executed by a processor to implement a method; and(3) an electronic device with a processor. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a method for performing the personalized course recommendation. (Drawing includes non-English language text).",,,
10.1145/3084363.3085029,2017,"Padmanaban, Nitish; Konrad, Robert; Cooper, Emily A.; Wetzstein, Gordon",Optimizing VR for All Users Through Adaptive Focus Displays,"Personal computing devices have evolved steadily, from desktops to mobile devices, and now to emerging trends in wearable computing. Wearables are expected to be integral to consumer electronics, with the primary mode of interaction often being a near-eye display. However, current-generation near-eye displays are unable to provide fully natural focus cues for all users, which often leads to discomfort. This core limitation is due to the optics of the systems themselves, with current displays being unable to change focus as required by natural vision. Furthermore, the form factor often makes it difficult for users to wear corrective eyewear. With two prototype near-eye displays, we address these issues using display modes that adapt to the user via computational optics. These prototypes make use of focus-tunable lenses, mechanically actuated displays, and gaze tracking technology to correct common refractive errors per user, and provide natural focus cues by dynamically updating scene depth based on where a user looks. Recent advances in computational optics hint at a future in which some users experience better vision in the virtual world than in the real one.",,,
,,LANMAN D R; CHAPMAN M S; FIX A J; KAPLANYAN A S; XIAO L,"Method for generating defocus blur effects, involves receiving current eye-tracking data associated with a user of a head-mounted display, and dynamically adjusting a focal length of the head-mounted display","NOVELTY - The method involves receiving current eye-tracking data associated with a user of a head-mounted display, and dynamically adjusting a focal length of the head-mounted display based on the current eye-tracking data. An in-focus image of a scene and a corresponding depth map (511) of the scene are generated. A circle-of-confusion map is generated for the scene based on the depth map. The circle-of-confusion map encodes a desired focal surface in the scene. USE - Method for generating defocus blur effects. ADVANTAGE - The updates are made to optimize the loss function or to minimize the difference between the generated output image and the target image. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a computer-readable non-transitory storage media; and(2) a system for generating defocus blur effects. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a machine-learning model configured to synthesize a gaze-contingent defocus blur images.Machine learning model (500)Loss function (501)Depth map (511)Circle-of-confusion (512)Ground truth blur (530)",,,
,,FRUEH C; KWATRA V; SUD A,"Method for headset removal in virtual, augmented and mixed reality using eye gaze database, involves accessing first stream of information representing telemetry of pose of head mounted device (HMD) worn by user","NOVELTY - The method involves accessing a first stream of information representing telemetry of a pose of a HMD (1310) worn by a user (1305), a second stream of information representing eye gaze directions, and a third stream of information representing multiple images (1300) of a scene captured by a camera. Representations of a first portion of the user's face that is occluded by the HMD in multiple images are rendered. Multiple mixed reality images are generated by combining multiple virtual reality images, a second portion of the user's face and the rendered representations. USE - Method for headset removal in virtual, augmented and mixed reality using eye gaze database. ADVANTAGE - The tracking information acquired by the eye tracker concurrently with the camera capturing images is used to refine or improve estimates of the three dimensional (3-D) locations of the eyes in the images. The method improve robustness or noise reduction by combining results for multiple different images to reject outliers. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an apparatus for headset removal in virtual, augmented and mixed reality using eye gaze database. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a headset removal performed on an image of a user that is wearing an HMD that occludes a portion of the user's face.Image (1300)User (1305)HMD (1310)Rendered image (1315)Display (1320)",,,
,,HU Y; LI G; YU Y; GAO J; CHEN J; XIE C; PAN Y; PAN X,"Endoscopic surgery naked-eye three-dimensional (3D) image display system has processing device to convert interlaced format of surgical object image, and complete interleaving of 3D display image based on image interleaving parameter","NOVELTY - The display system has endoscope subsystem (1100) and naked-eye 3D image display subsystem (1200). The naked-eye 3D image display subsystem has a 3D image processing device (1210), a display screen (1220), and a dual camera human eye tracking device (1230). The endoscope subsystem is connected to the 3D image processing device for capturing a surgical object image and transmitting the image to the 3D image processing device. The dual camera human eye tracking device is connected to the 3D image processing device for capturing the doctor's face image and determining a spatial position of the human eye, and obtaining an image interleaving parameter according to the spatial position of human eye. The 3D image processing device is configured to convert an interlaced format of the surgical object image, and complete the interleaving of 3D display image according to the image interleaving parameter. USE - Naked-eye three-dimensional (3D) image display system for endoscopic surgery such as laparoscopic surgery. ADVANTAGE - The doctor is able to view the 3D image of the surgical object without wearing the eye device during the endoscopic surgery, and the additional burden in the operation is suppressed. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a naked-eye 3D image display method. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the naked-eye 3D image display system. (Drawing includes non-English language text)Endoscope subsystem (1100)Naked-eye three-dimensional image display subsystem (1200)Three-dimensional image processing device (1210)Display screen (1220)Dual camera human eye tracking device (1230)",,,
,2018,"Wang, Ming-Yao; Kogkas, Alexandros A.; Darzi, Ara; Mylonas, George P.","Free-View, 3D Gaze-Guided, Assistive Robotic System for Activities of Daily Living","Patients suffering from quadriplegia have limited body motion which prevents them from performing daily activities. We have developed an assistive robotic system with an intuitive free-view gaze interface. The user's point of regard is estimated in 3D space while allowing free head movement and is combined with object recognition and trajectory planning. This framework allows the user to interact with objects using fixations. Two operational modes have been implemented to cater for different eventualities. The automatic mode performs a pre-defined task associated with a gaze-selected object, while the manual mode allows gaze control of the robot's end-effector position on the user's frame of reference. User studies reported effortless operation in automatic mode. A manual pick and place task achieved a success rate of 100% on the users' first attempt.",,,
,2018,"Jha, Sumit; Busso, Carlos",Probabilistic Estimation of the Gaze Region of the Driver using Dense Classification,"The ability to monitor the visual attention of a driver is a useful feature for smart vehicles to understand the driver's intents and behaviors. The gaze angle of the driver is not deterministically related to his/her head pose due to the interplay between head and eye movements. Therefore, this study aims to establish a probabilistic relationship using deep learning. While probabilistic regression techniques such as Gaussian process regression (GPR) has been previously used to predict the visual attention of a driver, the proposed deep learning framework is a more generic approach that does not make assumptions, learning the relationship between gaze and head pose from the data. In our formulation, the continuous gaze angles are converted into intervals and the grid of the quantized angles is treated as an image for dense prediction. We rely on convolutional neural networks (CNNs) with upsampling to map the six degrees of freedom of the orientation and position of the head into gaze angles. We train and evaluate the proposed network with data collected from drivers who were asked to look at predetermined locations inside a car during naturalistic driving recordings. The proposed model obtains very promising results, where the size of the gaze region with 95% accuracy is only 11.73% of a half sphere centered at the driver, which approximates his/her field of view. The architecture offers an appealing and general solution to convert regression problems into dense classification problems.",,,
,,LIN B; LIN P,"Eye tracking and gesture control integrated automobile-mounted device, has sensing module controls feedback display of head-up display and initiating gesture identification calculation and controlling system in gesture control automobile","NOVELTY - The device has a head-up display arranged on an automobile front glass. A 3D depth sensing module is arranged on an automobile central control platform and electrically connected with the head-up display and can emit infrared diffraction speckle and projects an eye position of a driver. The 3D depth sensing module controls a feedback display of the head-up display when a sight drop falling position is located at the front wind screen of an automobile, and initiates gesture identification calculation when the sight falling point position is in a control interface of the 3D depth sensing module and controls system in a gesture control automobile. USE - Eye tracking and gesture control integrated automobile-mounted device. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an eye tracking and gesture control integrated automobile-mounted device.",,,
,,SENGELAUB T,"Low power eye tracking system has head-mounted device provided with camera used to capture image of eye of user at frame rate of N frames/second, and initial three-dimensional model indicates position of eye of user with respect to camera","NOVELTY - The system comprises a head-mounted device (HMD) provided with a camera that is configured to capture an image of an eye of a user at a frame rate of N frames/second. Head movement sensors are configured to detect movement of the HMD relative to a head of the user. A controller is provided with processors that are configured to perform initial three-dimensional (3D) reconstruction based on the image captured by the camera to generate an initial three-dimensional (3D) model of the eye of the user. The initial three-dimensional (3D) model indicates a position of the eye of the user with respect to the camera, and updates the three-dimensional (3D) model to indicate a position of the user eye relative to the at least one camera. USE - Used as a low power eye tracking system. ADVANTAGE - The system may allow the frame rate of the eye tracking camera to be reduced. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for :(1) a processing method; and(2) an apparatus. DESCRIPTION Of DRAWING(S) - The drawing shows an exemplary VR/AR HMD implementing an eye tracking system including a sensor for detecting the movement of the HMD relative to the user eyes.",,,
,2021,"Doosti, Bardia; Chen, Ching-Hui; Vemulapalli, Raviteja; Jia, Xuhui; Zhu, Yukun; Green, Bradley",Boosting Image-based Mutual Gaze Detection using Pseudo 3D Gaze,"Mutual gaze detection, i.e., predicting whether or not two people are looking at each other, plays an important role in understanding human interactions. In this work, we focus on the task of image-based mutual gaze detection, and propose a simple and effective approach to boost the performance by using an auxiliary 3D gaze estimation task during the training phase. We achieve the performance boost without additional labeling cost by training the 3D gaze estimation branch using pseudo 3D gaze labels deduced from mutual gaze labels. By sharing the head image encoder between the 3D gaze estimation and the mutual gaze detection branches, we achieve better head features than learned by training the mutual gaze detection branch alone. Experimental results on three image datasets show that the proposed approach improves the detection performance significantly without additional annotations. This work also introduces a new image dataset that consists of 33.1K pairs of humans annotated with mutual gaze labels in 29.2K images.",,,
,,CRISPIN S R; YILDIZ I B; MULLIKEN G H,"Method for adjusting visual characteristics associated with objects, involves displaying a visual characteristic associated with an object that obtaining a first physiological data associated with the first pupillary response of the user",NOVELTY - The method (300) involves displaying (310) a visual characteristic associated with an object on the display to a user. The first physiological data associated with the first pupillary response of the user is obtained (320) to the visual characteristic using the sensor. The visual characteristic is adjusted (330) to enhances the pupillary responses of the user based on the obtained first physiological data. The object is displayed (340) with the adjusted visual properties. A second physiological data associated with a second pupillary response of the user is obtained to the adjusted visual properties. The intention of the user is determined based on the obtained second physiological data to interact with the object. USE - Method for adjusting visual characteristics associated with objects to enhance a pupillary response of a user. ADVANTAGE - Method ensures the use of three-dimensional (3D) mapping in conjunction with gaze tracking that allows the user to move their head and eyes freely while reducing and eliminating the need to actively track the head using sensors or emitters on the head. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system for adjusting visual characteristics associated with objects to enhance a pupillary response of a user; and(2) a non-transitory computer-readable storage medium storing program instructions with computer-executable to perform operations. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a method for adjusting visual characteristics associated with objects.Method (300)Displaying a visual characteristic associated with an object on the display to a user (310)Obtain the first physiological data associated with the first pupillary response of the user to the visual characteristic using the sensor (320)Adjust the visual characteristic to enhances the pupillary responses of the user based on the obtained first physiological data (330)Display the object with the adjusted visual properties (340),,,
,,MOROZOV A V; MALINOVSKAYA E G; DUBYNIN S E; DRUZHIN V V; MOROZOV A; MALINOVSKAYA E; DUBYNIN S; DRUZHIN V,"Eye-tracking device used in display apparatus e.g. head-mounted display (HMD), has signal processor that is configured to determine angle of rotation of observer eye based on two dimensional (2D) intensity distribution of illumination light",NOVELTY - The device (100) has a light source that is configured to emit an illumination light. A light guide plate is configured to transmit the illumination light emitted from the light source to an observer eye and transmit the illumination light reflected from the observer eye in a direction opposite to a propagation direction of the illumination light emitted from the light source. A photodetector array (14) is configured to detect the illumination light reflected from the observer eye. A signal processor (15) is configured to determine an angle of rotation of the observer eye based on a 2D intensity distribution of the illumination light detected by the photodetector array. The light guide plate comprises a first input-output coupler (12) and a second input-output coupler (13) configured to guide the illumination light to be incident into the light guide plate and to travel inside the light guide plate and outside the light guide plate. USE - Eye-tracking device used in display apparatus (claimed) such as head-mounted display (HMD). ADVANTAGE - The light use efficiency is improved and power consumption of the eye-tracking device is reduced. The device accurately detects an angle of rotation of the observer eye over a wide angle range. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a display apparatus. DESCRIPTION Of DRAWING(S) - The drawing shows the cross-sectional view illustrating a structure of an eye-tracking device.First input-output coupler (12)Second input-output coupler (13)Photodetector array (14)Signal processor (15)Eye-tracking device (100),,,
,,YANG X; CHA X; XU T; FENG Z; LV N; FAN X,"Method for realizing human-computer interaction for integrating eye tracking and gesture recognition in virtual assembly, involves analyzing behavior type of operator according to characteristic information to complete assembly task","NOVELTY - The method involves performing gaze point tracking according to acquired eye movement data. Gesture is recognized according to the obtained gesture information. The obtained gesture recognition data and eye movement data are marked to form a training set. A multi-stream convolutional neural network (CNN) long and short-term memory network model is constructed. The multi-stream CNN-long and short-term memory network model is trained. An optimal network model obtained by training is applied to virtual assembly process. The eye movement data and gesture information of virtual assembly process are acquired. Eye movement characteristic and gesture characteristic are extracted. A behavior type of an operator is analyzed according to the characteristic information to complete assembly task. USE - Method for realizing human-computer interaction for integrating eye tracking and gesture recognition in a virtual assembly. ADVANTAGE - The method enables reducing misjudging problem of similar behaviors in a single mode, identifying behavior of operators in a video with high accuracy by using deep learning algorithm, and completing virtual assembly task to realize human-computer interaction. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a device for realizing human-computer interaction for integrating eye tracking and gesture recognition in a virtual assembly(2) a computer-readable storage medium for storing a set of instructions for realizing human-computer interaction for integrating eye tracking and gesture recognition in a virtual assembly(3) a terminal device comprising a processor and a memory for realizing human-computer interaction for realizing human-computer interaction for integrating eye tracking and gesture recognition in a virtual assembly. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a virtual assembly. '(Drawing includes non-English language text)'",,,
,,WANG H; ZHANG B; ZHAO J; LV Y; WANG Y; LUO Y; HOU G,"Image recognition based psychological testing method, involves performing feature extraction by adopting deep learning algorithm, and obtaining gaze-visual attention point transfer matrix by using regression process","NOVELTY - The method involves obtaining corresponding psychological semantics of pictures, where the pictures comprise a foreground picture and a background picture and the foreground picture is formed as a facial expression picture. The background picture and the foreground picture are displayed to person to judge a corresponding key. Key-press reaction time and eye movement track are determined when viewing the pictures. Feature extraction is performed by adopting a deep learning algorithm to classify a psychological state of the person. Gaze-visual attention point transfer matrix is obtained by using regression process. USE - Image recognition based psychological testing method. ADVANTAGE - The method enables viewing reaction time in a simple manner to send eye tracking data of the picture to an examiner using a front camera of a notebook computer with high testing precision. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an image recognition based psychological testing method. '(Drawing includes non-English language text)'",,,
,,DIAO H; HUANG L,"Eye positioning device, has an eye locator, a first black and white camera which is configured to take a first black and white image and a second black and white camera is configured to take a second black and white image","NOVELTY - The eye positioning device has an eye locator, a first black and white camera which is configured to take a first black and white image and a second black and white camera is configured to take a second black and white image. An eye positioning image processor is configured to recognize the presence of an eye based on one of the first black and white image and the second black and white image and determine the eye space based on the eye recognized in the first black and white image and the second black and white image position. An eye positioning data interface is configured to transmit eye space position information indicating the eye space position. The eye locator has an infrared emitting device. The infrared emitting device is configured to emit infrared light with a wavelength greater than or equal to 1.5 microns. USE - Eye positioning device. ADVANTAGE - The eye positioning device determines the spatial position of the user's eyes with high accuracy, so improves the 3D display quality. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a three-dimension (3D) display device;(2) an eye positioning method; and(3) a 3D display method. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic structural diagram of an eye positioning device. (Drawing includes non-English language text).",,,
,,BALLARD C; LUO B; SANTIANO D; NAIMARK M; HUANG G; JUVERA MOLINA M; ZHAO A,"Computer system e.g. workstation for capturing three-dimensional video within viewing zone of display, has processor that is connected to camera executing interpolator to combine dimensional pixel array from camera into output pixel array","NOVELTY - The system has a frame (108) that is configured to be positioned surrounding a display (110). Multiple cameras (112) are attached to the frame. Each camera is configured to generate a two-dimensional (2D) or three-dimensional (3D) array of pixels. A processor (116) is connected to the multiple cameras executing an interpolator (122) configured to combine multiple 2D or 3D arrays of pixels from the multiple cameras into an output three-dimensional array of pixels. A network interface (118) is in communication with a remote device (102). The network interface is configured to transmit the output three-dimensional array of pixels to the remote device for rendering at the display of the remote device. USE - Computer system such as workstation, telephone, desktop computer, laptop or notebook computer, server, handheld computer, mobile telephone or portable telecommunication device, media playing device, gaming system, mobile computing device, and telecommunication or media device for capturing 3D video within viewing zone of display. ADVANTAGE - The pixels are selectively removed from the merged array to maintain a uniform pixel resolution across the output image. The real time eye tracking is implemented in conjunction with 3D displays, thus, allowing two images to be rendered one for each eye with no eye box and additional images, minimizing rendering processing and maximizing resolution. The system utilizes multiple red green blue-depth (RGB-D) data sources for robust and reliable tracking of both eyes with extended coverage. The wireless communication devices and access points enhance performance, reduce costs and size, and enhance broadband applications. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for 3D video capture of viewing zone of display. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an implementation of a device for interpolative 3D imaging.Remote device (102)Frame (108)Display (110)Camera (112)Processor (116)Network interface (118)Interpolator (122)",,,
,2019,"Stolzenwald, Janis; Mayol-Cuevas, Walterio W.",Rebellion and Obedience: The Effects of Intention Prediction in Cooperative Handheld Robots,"Within this work, we explore intention inference for user actions in the context of a handheld robot setup. Handheld robots share the shape and properties of handheld tools while being able to process task information and aid manipulation. Here, we propose an intention prediction model to enhance cooperative task solving. The model derives intention from the combined information about the user's gaze pattern and task knowledge. Within experimental studies, the model is validated through a comparison of user frustration for the case where the robot follows the predicted location of the user's intended action versus doing the opposite (rebellion). The proposed model yields real-time capabilities and reliable accuracy up to 1.5 s prior to predicted actions being executed.",,,
,,PARK J; LEE S; LEESEOK,Calibration method for a three-dimensional (3D) augmented reality involves determining a first conversion parameter representing the first relationship between a first coordinate system of an eye-tracking camera,"NOVELTY - The calibration method involves determining a first conversion parameter representing the first relationship between a first coordinate system of an eye-tracking camera and a second coordinate system of a calibration camera by capturing a physical pattern using the eye-tracking camera and the calibration camera (140). The second conversion parameter is represented a second relationship between a third coordinate system of a virtual screen and the second coordinate system of the calibration camera by capturing a virtual pattern displayed on the virtual screen using the calibration camera. USE - Calibration method for a three-dimensional (3D) augmented reality. ADVANTAGE - The driver may easily grasp the information displayed on the front without moving his or her gaze while driving, and thus driving stability and safety may be enhanced. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) non-transitory computer-readable storage medium and(2) a calibration apparatus. DESCRIPTION Of DRAWING(S) - The drawing shows a schamatic representation of the method.Calibration System (100)Calibration Apparatus (110)Display Device (120)Eye-tracking Camera (130)Calibration camera (140)",,,
,,SENGELAUB T,"Virtual and mixed or augmented reality system implemented in head-mounted device (HMD), has processors process low-resolution image captured by camera to track movement of eyes with respect to HMD in intervals between high-resolution frames","NOVELTY - The system has a head-mounted device (HMD) which is configured to display visual content for viewing by a user, such that the HMD comprises a camera configured to capture (910,930) high-resolution images of the user's eyes and low-resolution images of the user's eyes at a frame rate of N frames per second. A controller comprises one or more processors which are configured to iteratively perform process a high-resolution image captured by the camera to track (940) movement of the user's eyes with respect to the HMD. The processors process low-resolution images captured by the camera to track movement of the user's eyes with respect to the HMD in intervals between high-resolution frames captured by the camera and processed by the controller. The controller is a component of the HMD. USE - Virtual and mixed or augmented reality (VR/AR) system implemented in head-mounted device (HMD) positioned on head of user. ADVANTAGE - The three-dimensional (3D) reconstruction is performed only when movement of the device with respect to the user's eyes is detected, thus significantly reducing power consumption by the eye tracking system. The frame rate of the eye tracking cameras is reduced, from 120 frames per second to 10 frames per second or less, and the 3D reconstruction is performed much less often, thus significantly reducing power consumption by the eye tracking system. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:1. a method for low-power eye tracking in VR/AR applications; and2. a non-transitory computer-readable storage media storing a program for low-power eye tracking in VR/AR applications. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the eye tracking method in which low-resolution frames are captured by eye tracking cameras and used to track movement of eyes in intervals between processing of high-resolution frames captured by eye tracking cameras.Step for capturing high-resolution frames (910)Step for generating new 3D models of user's eyes (920)Step for capturing low-resolution frames (930)Step for tracking movement of user's eyes with respect to HMD (940)Step for detecting movement, elapsing time limit or exceeding confidence threshold (950)",,,
,,HELD R T; ROBINSON R Z; AHOLT C C,"System for controlling focal parameters of a visual display based on ocular features, comprises a processor, a visual display including at least one eye tracking sensor, and controlling focal parameter of the visual display","NOVELTY - The controlling system (100) comprises a processor, a visual display including an eye tracking sensor (108). The memory is communicatively coupled to the processor and has a computer program code. The processor involves collecting an ocular metric data (112) associated with at least one ocular feature of a user's eye via the eye tracking sensor. A user age estimate (124) based on analysis of the ocular metric data using the machine learning algorithm (126) and an associated ocular metric data set (132) is calculated. A confidence value (130) is associated with the user age estimate based on the analysis of the ocular metric data. A focal parameter (114) of the visual display based on the calculated user age estimate and the associated confidence value is controlled. USE - System for controlling the focal parameters of a visual display based on ocular features. ADVANTAGE - Multiple focus displays are in development that are able to adjust optical power to match where the user is looking, and multi-focal displays could render each holographic pixel at a different focal distance, enabling users to benefit from a more comfortable and realistic mixed-reality or virtual reality experiences. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for following:(1) a method for controlling focal parameters of a visual display based on ocular features; and(2) a computer storage media having computer-executable instructions for controlling focal parameters of a visual display based on ocular features. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the system for controlling focal parameters of a visual display based on ocular features.Controlling system (100)Eye tracking sensor (108)Ocular metric data (112)Focal parameter (114)User age estimate (124)Machine learning algorithm (126)Confidence value (130)Ocular metric data set (132)",,,
,,YU Y; CHEN J; XIA Z; XIE C,"Human eye tracking system external parameter calibration tool, has computer for determining human eye tracking system external parameter corresponding to coordinates of point on chessboard grid plate",NOVELTY - The tool has a chessboard grid plate for building mapping relationship between a world coordinate system and a human eye tracking camera imaging plane coordinate system. A slide rail is arranged at a center point of a display screen for placing the chessboard grid plate. The chessboard grid plate slides on the sliding rail. A traction motor is arranged on the sliding rail. A human eye tracking camera is arranged at a position on the display screen for shooting picture of the chessboard grid plate. A computer receives pattern of the chessboard grid plate captured by the human eye tracking camera to perform image analysis. The computer determines a human eye tracking system external parameter corresponding to the coordinates of a point on the chessboard grid plate in a human eye tracking camera imaging plane coordinate system. USE - Human eye tracking system external parameter calibration tool. ADVANTAGE - The tool unifies a screen display space coordinate system and the human eye tracking camera imaging space coordinate system to allow the computer to adjust chart cycle and layout offset based on human eye tracking coordinate information to make a user to obtain three-dimensional (3D) visual effect. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human eye tracking system external parameter correction method. DESCRIPTION Of DRAWING(S) - The drawing shows a cross sectional view of a human eye tracking system external parameter calibration tool.,,,
,2017,"Xia, Chen; Qi, Fei; Shi, Guangming",AN ITERATIVE REPRESENTATION LEARNING FRAMEWORK TO PREDICT THE SEQUENCE OF EYE FIXATIONS,"Visual attention is a dynamic search process of acquiring information. However, most previous studies have focused on the prediction of static attended locations. Without considering the temporal relationship of fixations, these models usually cannot explain the dynamic saccadic behavior well. In this paper, an iterative representation learning framework is proposed to predict the saccadic scanpath. Within the proposed framework, saccade can be explained as an iterative process of finding the most uncertain area and updating the representation of scenes. In implementation, a deep autoencoder is employed for representation learning. The current fixation is predicted to be the most salient pixel, with saliency estimated by the reconstruction residual of the deep network. Image patches around this fixation are then sampled to update the network for the selection of subsequent fixations. Compared with existing models, the proposed model shows the state-of-the-art performance on several public data sets.",,,
,2021,"Guo, Tianchu; Zhang, Hui; Yoo, ByungIn; Liu, Yongchao; Kwak, Youngjun; Han, Jae-Joon","Order Regularization on Ordinal Loss for Head Pose, Age and Gaze Estimation","Ordinal loss is widely used in solving regression problems with deep learning technologies. Its basic idea is to convert regression to classification while preserving the natural order. However, the order constraint is enforced only by ordinal label implicitly, leading to the real output values not strictly in order. It causes the network to learn separable feature rather than discriminative feature, and possibly overfit on training set. In this paper, we propose order regularization on ordinal loss, which makes the outputs in order by explicitly constraining the ordinal classifiers in order. The proposed method contains two parts, i.e. similar-weights constraint, which reduces the ineffective space between classifiers, and differential-bias constraint, which enforces the decision planes in order and enhances the discrimination power of the classifiers. Experimental results show that our proposed method boosts the performance of original ordinal loss on various regression problems such as head pose, age, and gaze estimation, with significant error reduction of around 5%. Furthermore, our method outperforms the state of the art on all these tasks, with the performance gain of 14.4%, 2.2% and 6.5% on head pose, age and gaze estimation respectively.",,,
,,ORTIZ E S; GAO J F; LUNARDHI A D; BULUSU V S R; ORTIZ EGEA S,"Computer-based method for projecting circular features onto sensor plane, involves inputting main/sub ellipse parameter into eye tracking function to determine pupil orientation parameters that define spatial-angular relationship",NOVELTY - The method (2000) involves obtaining (2001) an eye tracking function that corresponds to a three-dimensional (3D) model onto a first sensor plane or a second predetermined point onto a second sensor plane. The pixel data that is generated in association with a first circular feature of a first eye of a user and a second circular feature of a second eye of the user is received (2003) from sensor. The pixel data is analyzed (2005) to identify a pixels that correspond to the first circular feature and a second pixels that correspond to the second circular feature. The first ellipse parameter is determined (2007) to fit a first ellipse to the first pixels and second ellipse parameters to fit a second ellipse to the second multiple pixels. The first ellipse parameters and the second ellipse parameters are input (2009) into the eye tracking function to determine pupil orientation parameters that define a spatial-angular relationship between the first eye and the second eye. USE - Method for projecting circular features through a predetermined point onto a sensor planes by an eye tracking function that corresponds to a three-dimensional (3D) model. ADVANTAGE - The depth in space is calculated at which the user is focused with a higher degree of accuracy than existing eye tracking system. The vergence in space between the visual axes for each of the two eyes of user provides more actual calculations of the depth in space. The method is highly sensitive and accurate in the detection of eye movements. The computational cost of calculating a single sample measurement yields incremental to increase with increases in the sampling frequency is reduced. The mathematical simplification to eye tracking problem and methodology reduces the computing resources consumed to implement eye tracking techniques. The algorithm provide increased accuracy while providing for such accurate results through the user of lookup tables with smaller samples modeled. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a near-eye-display (NED) system; and(2) an eye tracking system. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for projecting circular features through a predetermined point onto a sensor planes by an eye tracking function that corresponds to a three-dimensional (3D) model.Method for projecting circular features through a predetermined point onto a sensor planes (2000)Step for obtaining an eye tracking function that corresponds to a 3D model for projecting individual circular features through a first predetermined point onto a first sensor plane or a second predetermined point onto a second sensor plane (2001)Step for receiving pixel data that is generated in association with a first circular feature of a first eye of a user and a second circular feature of a second eye of the user from sensor (2003)Step for analyzing pixel data to identify a multiple pixels that correspond to the first circular feature and a second multiple pixels that correspond to the second circular feature (2005)Step for determining first ellipse parameter to fit a first ellipse to the first multiple pixels and second ellipse parameters to fit a second ellipse to the second multiple pixels (2007)Step for inputting first ellipse parameters and the second ellipse parameters into the eye tracking function to determine pupil orientation parameters that define a spatial-angular relationship between the first eye and the second eye (2009),,,
,,JOSEPH A; KULKARNI N,"Method for facilitating detection and extraction of e.g. pathological lesion in retinal fundus image by e.g. optometrist, involves extracting image features corresponding to regions to create rankings of features, and providing test image","NOVELTY - The method involves fitting a statistical distribution to an eye fixation data in attractor and distractor regions. Image features corresponding to the regions to create rankings of the image features are extracted from fitted distributions using a machine learning algorithm, where a fuzzy logic computational system creates a top down knowledge comprising top down feature maps for recognizing the image features. A test image is provided. Regions of interest in the test image are detected in a detection system using a top down knowledge or a bottom up map. USE - Method for facilitating detection and extraction of regions of interest e.g. optic disk and pathological lesion, in a test image i.e. retinal fundus image by an expert e.g. optometrist, radiologist and physician (all claimed). Uses include but are not limited to medical imaging applications, object detection applications, machine vision applications, geographical information systems and interactive and diagnostic applications. ADVANTAGE - The method enables facilitating auto segregation of regions derived from knowledge of expert's eye gaze pattern to improve robustness of the detection system. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a system for facilitating detection and extraction of regions of interest.Eye gaze samples (100)",,,
10.1007/978-3-319-58460-7_16,2017,"Wang, Quan; Deng, Lelai; Cheng, Hao; Fan, Hualei; Du, Xiaoping; Yang, Qinghong",Live Gaze-Based Authentication and Gaming System,"Face recognition has been widely applied to identification/authentication systems [1-6], however, a considerable drawback of conventional face recognition when used alone lies in its limited ability to distinguish between living human user and 2D photos or pre-recorded videos of the user's face. To address the risk of these systems being easily bypassed by non-living photos or recordings of users, our study proposes an interactive authentication system that requires users to follow a specific pattern displayed onscreen with their gaze. The system uses the subject's eye movement and facial features during viewing for user authentication while randomly generating the displayed pattern in real time to ensure that no prepared video can fake user authentication. Given gaze movement is an inseparable part of the face, our system guarantees that facial and eye features belong to the same human following the pattern displayed. To this its deployment in an eye controlled system, we developed a gaze-controlled game that relies on user eye movement for input and applied the authentication system to the game.",,,
,,RYU K J,"Apparatus for providing training service to user terminal for improving reading ability based on eye tracking, has recommended training content providing unit for executing recommended training content output to user terminal","NOVELTY - The apparatus has an improvement database for storing training contents for improving eye movement ability and reading ability and generating user profile information to collect user information, diagnosis element information and diagnosis result information of a user from the reading ability diagnosis device (300). A recommended training content providing unit provides recommended training content to a user terminal used by the user and executes an recommended training content output to the user terminal according to the recommended training information. USE - Apparatus for providing training service to a user by a user terminal for improving reading ability based on eye tracking. Uses include but are not limited to general desktop computer, notebook computer, tablet personal computer (PC), personal digital assistant (PDA) and a mobile communication terminal. ADVANTAGE - The apparatus performs reading ability diagnosis based on objective eye movement measurement and reading process including reading fluency, search efficiency and concentration so as to increase reliability of diagnosis results through diagnosis process based on a machine learning algorithm using large data. The apparatus diagnoses risk of eye movement-related disorders such as dyslexia, attention deficit hyperactivity disorder, autism spectrum disorder and ocular ataxia through gaze measurement by enabling early detection of high-risk groups. The apparatus can easily, quickly and continuously diagnose and manage a user's reading ability to collect and utilize the diagnosis results of other users so as to increase accuracy of diagnosing reading ability through the machine learning algorithms. The apparatus provides optimal training content for reading ability improvement training based on an objective reading ability diagnosis result so as to effectively improve users eye movement ability and reading ability. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for providing training service to a user by a user terminal for improving reading ability based on eye tracking. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of an apparatus for providing training service to a user by a user terminal for improving reading ability based on eye tracking (Drawing includes non-English language text).Display device (100)Eye tracking device (200)Reading ability diagnostic device (300)Reading ability improvement device (400)Service providing device (500)",,,
,,WRIGHT C; LAWRENSON M; DUFFY D,"Apparatus for initiating communication between users, receives second data indicative of whether match between eye-tracking data of second user and reference eye-tracking data associated with recorded content associated with first user","NOVELTY - The apparatus has a unit for rendering first content to a first user, the first content being based on recorded content associated with a second user. The eye tracking in a time interval follows the rendering of the first content to obtain eye-tracking data of the first user. The unit is for determining first data indicative of whether or not is a match between the eye-tracking data of the first user and reference eye-tracking data associated with the recorded content associated with the second user. The unit is for receiving second data indicative of whether or not there is a match between eye-tracking data of the second user and reference eye-tracking data associated with recorded content associated with the first user, whether both the first and second data are indicative of a match, then providing the first user with a telecommunication option configured to initiate communication with the second user. USE - Apparatus e.g. smartphone, smartwatch, or another type of portable personal computer, portable electronic communication device e.g. handheld electronic communication device or wearable electronic communication device for initiating communication between users. ADVANTAGE - The first time resources of users are saved by not having input additional commands to thereby arrive at the control options. Machine learning is a field of computer science that gives computers the ability to learn without being explicitly programmed. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a system for initiating communication between first and second users;(2) a method for initiating communication between first and second users; and(3) a computer program for initiating communication between first and second users. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the apparatus for initiating communication between users.First user (10)Interact (12)Equivalent block (110)Per block (120)Eye-tracking data (122)",,,
,,TAN B; LU M; MA X; KANG J,"Autostereoscopic three dimensional display platform for design works, has piece of software that converts original CAD design drawings to required three dimensional autostereoscopic view",NOVELTY - The autostereoscopic 3D display platform comprises a piece of software that converts the original CAD design drawings to the required 3D autostereoscopic view. An autostereoscopic 3D display displays the three dimensional design results. The workflow of the display platform is obtained from the data of software to autostereoscopic 3D display system. The parallax angle between two views is affected by the working distance of the display platform. An eye-tracking system is provided to monitor the position (3) of eye during whole working process. USE - Autostereoscopic three dimensional display platform for individual customers and designers to display CAD design as three dimensional floating virtual object. ADVANTAGE - The eyes positions are detected to ensure the viewer in the 3D viewing area. The display platform monitors the eyes positions of the individual user by a real-time eye-tracking system. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a 3D display platform to convert a CAD design to a 3D floating virtual object.LCD display screen (1)Optical element (2)Eye position (3),,,
10.3390/photonics6040106,2019,"Panke, Karola; Pladere, Tatjana; Velina, Mara; Svede, Aiga; Krumina, Gunta",Objective User Visual Experience Evaluation When Working with Virtual Pixel-Based 3D System and Real Voxel-Based 3D System,"Volumetric display shows promising implications for healthcare related applications as an innovative technology that creates real three-dimensional (3D) image by illuminating points in three-dimensional space to generate volumetric images without image separation. We used eccentric photorefractometry to objectively study ocular performance in a practical environment by evaluating near work-induced refraction shift, accommodative microfluctuations, and pupil size for 38 young adults after viewing anaglyph, and volumetric 3D content for prolonged time. The results of our study demonstrate that participants who performed relative depth estimation task on volumetric 3D content were less likely to experience task-induced myopic refraction shift. For both 3D content types, we observed pupil constriction, that is possibly related to visual fatigue. For anaglyph 3D pupil constriction, onset was observed significantly sooner, compared to volumetric 3D. Overall, sustained work with 3D content, and small disparities or the fully eliminated possibility of accommodation-vergence conflict, not only minimizes near work-induced myopic shift, but also provide beneficial accommodation relaxation that was demonstrated in this study as hypermetropic shift for nearly half of participants.",,,
10.1177/1729881417715984,2017,"Xu, De; Wang, Qingbin",A new vision measurement method based on active object gazing,"A new vision measurement system is developed with two cameras. One is fixed in pose to serve as a monitor camera. It finds and tracks objects in image space. The other is actively rotated to track the object in Cartesian space, working as an active object-gazing camera. The intrinsic parameters of the monitor camera are calibrated. The view angle corresponding to the object is calculated from the object's image coordinates and the camera's intrinsic parameters. The rotation angle of the object-gazing camera is measured with an encoder. The object's depth is computed with the rotation angle and the view angle. Then the object's three-dimensional position is obtained with its depth and normalized imaging coordinates. The error analysis is provided to assess the measurement accuracy. The experimental results verify the effectiveness of the proposed vision system and measurement method.",,,
,,RIJNDERS C E,"Object detection method for digitalizing images in digital video stream, involves entering extracted features of normalized frequency domain data in classifier to obtain data for object detection or visual saliency for video compression","NOVELTY - The detection method involves applying L-transformation to the frequency input of a sparse zone of the frequency domain content of each frame defining multiple images. Adaptations of the function that follows from the L-transformation are chosen separately with respect to X and Y axes, and then combined in a single value. Filtering kernels are then applied partially overlapping the sparse zones. A convolution is then performed between the transformed frequency data within each sparse zone and kernels. The convolution results are combined in single values. Each single value represents an extracted feature. The frequency domain data of the pair of sparse zones are normalized using only frequency domain data present in the sparse zones. The extracted features of the normalized frequency domain data are entered in a classifier to obtain the data for object detection or visual saliency to use for video compression. USE - Object detection method used for digitalizing images in digital video stream. Can be used for image processing for, e.g., human face tagging technology on social networks, recognition of hand gestures, for detection of pedestrians, cyclists and vehicles, human face detection for augmented reality and screens with three-dimensional (3D) effects, object recognition for augmented reality, head orientation or eye orientation tracking, object tracking for security systems, and gaze tracking. ADVANTAGE - Provides an object detection method configured to perform object detection through both the digital camera and a processor of the operated device or system, in connection with the classifier stored in a memory device and accessible by the operated device or system. Detects objects using features extracted in the frequency domain of the images or digital video stream of the images. Uses frequency domain that allows for computationally easy use of a sparse, small, portion of the frequency domain information for object detection. DESCRIPTION Of DRAWING(S) - The drawing shows a graphical diagram illustrating how only a sparse portion of the frequency domain information is required to capture the sinusoidal information in the frequency domain.",,,
,,TERRANO M; ERKELENS I; MACKENZIE K J,"Computer-based method for rendering augmented reality content, involves rendering augmented reality element based on predicted change in ability of user to visually process augmented reality element within display of head mounted display",NOVELTY - The method (1200) involves identifying (1210) eye tracking information for a user at an initial time by an augmented reality system comprising a head mounted display. A change is predicted (1220) in an ability of the user to visually process an augmented reality element at a future time based on a characteristic of the augmented reality element and using a machine learning model to analyze the eye tracking information for the user at the initial time within a display of the head mounted display. The augmented reality element is selectively rendered (1230) at the future time based on the predicted change in the ability of the user to visually process the augmented reality element within the display of the head mounted display. USE - Computer-based method for rendering augmented reality content based on prediction regarding user ability to visually process augmented reality content. ADVANTAGE - The system is able to reduce to energy consumption and heat generation of the head mounted display and associated processing device by avoiding unnecessary processing and display of the augmented reality element. The augmented reality system utilizes the identified eye tracking information with a machine learning model to predict a change in an ability of the user to visually process the augmented reality element. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an augmented reality system; and(2) a non-transitory computer-readable medium storing program for rendering augmented reality content based on prediction regarding user ability to visually process augmented reality content. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for rendering augmented reality content.Method for rendering augmented reality content (1200)Step for identifying eye tracking information for user at initial time (1210)Step for predicting change in ability of user to visually process augmented reality element at future time (1220)Step for rendering augmented reality element (1230),,,
,,ANTONY A; CHANDRAN D V; SHENOY A A; JOEL K,"User interfaced device used for interfacing the brain and eyes of the user, comprises an apparatus that is used for eye-tracking in three dimensional view, where the eye-tracking apparatus is placed near the display source",NOVELTY - The user interfaced device comprises an apparatus that is used for eye-tracking in three dimensional (3D) view. The eye-tracking apparatus is placed near the display source. The computational device uses the information collected by the eye-tracker to cancel the noise caused by the user blinking or his/her eye movements in electroencephalogram (EEG) waves. The computational device uses the data collected by the eye-tracker and EEG tracker for user recognition. USE - User interfaced device used for interfacing the brain and eyes of the user. ADVANTAGE - The accidental keyword input during typing or reading can be avoided by the computational device. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the user interfaced device.,,,
,,ZHOU X; JIANG J; LIN J; CHEN S,"Bidirectional LSTM and Itracker based eye tracking method, involves obtaining last eye tracking prediction by uniform connected layer, and obtaining final two-dimensional gaze vector by layer of last frame of forward unit and backward unit","NOVELTY - The method involves performing perspective transformation process by an original image. A human face reference point is extracted from an image center of a camera along fixed distance. An integral structure is divided into a static module and a time module. The static module is accessed with two branch CNN and uniform connection layer. A left eye and a right eye are input into a branch of a network. A LSTM structure is accessed with repetitive series of LSTM cells, where each LSTM cell comprises a forgotten gate, an input gate and an output gate. A last eye tracking prediction is obtained by the uniform connected layer. Final two-dimensional gaze vector is obtained by a layer of a last frame of a forward unit and a backward unit. USE - Bidirectional LSTM and Itracker based eye tracking method. ADVANTAGE - The method enables improving estimation precision of 3D eye tracking and reducing effect of interference factors. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a bidirectional LSTM and Itracker based eye tracking system. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a bidirectional LSTM and itracker based eye tracking system. '(Drawing includes non-English language text)'",,,
,,WINSTON R; TIGAR M,"Method for creating virtual reality (VR) interaction between users, involves stitching detected and recorded pupil image of user rendered by sensor so as to be superimposed where headset image removed from two-dimensional (2D) image of user",NOVELTY - The method involves creating a 2D image of user wearing a VR headset (690) using a camera device and positioned in the headset is sensors. The pupil movement of the user is tracked using the sensors. A headset image is removed from the created 2D image of the user wearing the headset. The detected and recorded pupil image of the user rendered is stitched by the sensors so as to be superimposed where the headset image was removed from the created 2D image of the user such that another user can view a VR image of the user as the user was not wearing the headset. The 2D image is created using a video camera. The camera device has a depth sensor. The sensors include infrared LED sensors. The sensors include micro video camera for recording aspects of a user's pupil. USE - Method for creating virtual reality (VR) interaction between two or more users. ADVANTAGE - The headset provides real-time image stitching using two separate video feeds or a single video feed in conjunction with a predetermined still-frame image cache. The eye-tracking is performed through a camera mounted inside a VR display device enabling real-time selection of cached images or blended images is derived from an initial setup phase used in an image stitching eye region of a users face is typically occluded due to the VR display device thus providing real-time 3D face tracking and reconstruction using a final. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an apparatus for creating VR interactions between two or more users. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of a computer device and mobile device.Computing system (600)Component (602)Memory (604)Storage device (606)VR headset (690),,,
10.3390/s21041381,2021,"Sulikowski, Piotr; Zdziebko, Tomasz; Coussement, Kristof; Dyczkowski, Krzysztof; Kluza, Krzysztof; Sachpazidu-Wojcicka, Karina",Gaze and Event Tracking for Evaluation of Recommendation-Driven Purchase,"Recommendation systems play an important role in e-commerce turnover by presenting personalized recommendations. Due to the vast amount of marketing content online, users are less susceptible to these suggestions. In addition to the accuracy of a recommendation, its presentation, layout, and other visual aspects can improve its effectiveness. This study evaluates the visual aspects of recommender interfaces. Vertical and horizontal recommendation layouts are tested, along with different visual intensity levels of item presentation, and conclusions obtained with a number of popular machine learning methods are discussed. Results from the implicit feedback study of the effectiveness of recommending interfaces for four major e-commerce websites are presented. Two different methods of observing user behavior were used, i.e., eye-tracking and document object model (DOM) implicit event tracking in the browser, which allowed collecting a large amount of data related to user activity and physical parameters of recommending interfaces. Results have been analyzed in order to compare the reliability and applicability of both methods. Observations made with eye tracking and event tracking led to similar results regarding recommendation interface evaluation. In general, vertical interfaces showed higher effectiveness compared to horizontal ones, with the first and second positions working best, and the worse performance of horizontal interfaces probably being connected with banner blindness. Neural networks provided the best modeling results of the recommendation-driven purchase (RDP) phenomenon.",,,
,,BHANDARI R; BHANDARI A,"System for assessing balance and posture of subject, has processor set to generate report pertaining to assessment of posture and stability of subject based on utilizing machine learning model to decipher normal pattern and abnormal pattern",NOVELTY - The system (100) has a memory (102). A processor (104) is communicatively coupled to the memory. A virtual reality or augmented reality headgear (108) is worn by the subject. A display (110) for providing multiple visual stimuli to the subject is provided. A gyroscope (112) and an inertial measurement unit (114) detects a head movement and a body sway of the subject in response to multiple visual stimuli provided to the subject on the display. An eye tracking unit (116) for tracking eye movements of the subject in response to multiple visual stimuli provided to the subject on the display is provided. The processor utilizes a machine learning model (118) to decipher normal patterns and abnormal patterns. The processor generates a report pertaining to an assessment of posture and stability of the subject based on utilizing the machine learning model (118) to decipher normal patterns and abnormal patterns. USE - System for assessing balance and posture of subject using virtual reality or augmented reality headgear. ADVANTAGE - The system of computerized dynamic posturography for effectively assessing the balance and posture of a patient or subject is provided. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for assessing balance and posture of subject using virtual reality or augmented reality headgear worn by subject. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the system for assessing balance and posture of subject.System for assessing balance and posture of subject (100)Memory (102)Processor (104)Virtual reality or augmented reality headgear (106)Display (110)Gyroscope (112)Inertial measurement unit (114)Eye tracking unit (116)Machine learning model (118),,,
10.1016/j.icte.2020.05.006,2020,"Kaisar, Shahriar",Developmental dyslexia detection using machine learning techniques : A survey,"Developmental dyslexia is a learning disability that occurs mostly in children during their early childhood. Dyslexic children face difficulties while reading, spelling and writing words despite having average or above-average intelligence. As a consequence, dyslexic children often suffer from negative feelings, such as low self-esteem, frustration, and anger. Therefore, early detection of dyslexia is very important to support dyslexic children right from the start. Researchers have proposed a wide range of techniques to detect developmental dyslexia, which includes game-based techniques, reading and writing tests, facial image capture and analysis, eye tracking, Magnetic reasoning imaging (MRI) and Electroencephalography (EEG) scans. This survey paper critically analyzes recent contributions in detecting dyslexia using machine learning techniques and identify potential opportunities for future research. (C) 2020 The Korean Institute of Communications and Information Sciences (KICS). Publishing services by Elsevier B.V.",,,
,,FRUEH C; KWATRA V; SUD A,"Method for determining pose of occluded face in image in coordinate system of camera that acquires image, involves storing three-dimensional (3-D) model of users face in database that is indexed by eye gaze direction",NOVELTY - The method involves capturing images (1505) of a user's face corresponding to gaze directions of user's eyes images using a camera. A 3-D model of the user's face is generated from the image. The 3-D model of the user's face is stored in a database that is indexed by eye gaze direction. The camera is a Red-Green-Blue-Depth (RGBD) camera that captures RGB values of pixels in image and depth values of the pixels that indicate distances between the RGBD camera and portion of the user's face represented by the pixels. USE - Method for determining a pose of a partially occluded face in an image in a coordinate system of a camera that acquires the image. ADVANTAGE - The face detector is applied to the 3-D face model and pixels to the eye region of the face that are eliminated by the pixels are occluded and consequently generate noise in the matching algorithm. The method improve robustness or noise reduction by combining multiple different images to reject outliers. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an apparatus for determining a pose of occluded face in an image. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of a method of determining a pose of a partially occluded face in an image in a coordinate system of a camera that acquires the image.Step for capturing image of a user's face (1505)Step for accessing reference model (1510)Step for selecting pose of response (1515)Step for matching unoccluded portion (1520)Step for determining face pose (1530),,,
,,TORNEUS D; SUNDBERG T; ARVIDSSON M; ANDERSSON J; CLAUSEN A; HAINZL R; KINGBAECK A; OLIN S; RYAN M; NUTTI B; TIDBECK C; BIEDERT R; BLOMQVIST N; RADELL D; THUNSTROEM R; KIMBUCK A; THUNSTROM R,"Augmented reality and virtual reality wearable apparatus e.g. virtual reality glasses for use with computing device, has processors determining one of location or orientation of sensor relative to holographic film, and changing parameter","NOVELTY - The apparatus has an eye tracking device comprising an image sensor (113) and an illuminator (111). Processors (120) activate the illuminator to illuminate a holographic film, and capture an image of a portion of the holographic film with the image sensor while the holographic film is illuminated. The processors determine a characteristic of the holographic film based on the image, determine one of a location or an orientation of the image sensor relative to the holographic film based on the characteristic, and change calibration parameter of the image sensor based on one of the location or the orientation of the image sensor relative to the holographic film. USE - Augmented reality (AR) and virtual reality (VR) wearable apparatus e.g. VR glasses and AR glasses for use during remote eye tracking for a computing device. Uses include but are not limited to a personal computer, a personal computer, a smart phone, and a vehicle-mounted device. ADVANTAGE - The apparatus automatically computes relative position and alignment of components by re-mapping the components to a three-dimensional (3D) coordinate system based on image data generated by the image sensor as relative positions and alignments are mapped on the same 3D coordinate system. The apparatus utilizes gaze-based interaction techniques so as to improve the processing accuracy based on a component calibration over time and/or based on a specific component configuration, where a user can accurately and intuitively manipulate a locked virtual object by moving user finger around the touch area on an external controller. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for changing a calibration parameter for an image sensor in an eye tracking device for an augmented reality and virtual reality wearable apparatus(2) a non-transitory machine-readable medium storing a set of instructions to perform a method for changing a calibration parameter for an image sensor in an eye tracking device for an augmented reality and virtual reality wearable apparatus. DESCRIPTION Of DRAWING(S) - The drawing shows a front view of an eye tracking system.Eye tracking system (100)Eye tracking module (110)Illuminator (111)Image sensor (113)Processors (120)",,,
10.3758/s13428-017-0969-4,2018,"Weber, Sascha; Schubert, Rebekka S.; Vogt, Stefan; Velichkovsky, Boris M.; Pannasch, Sebastian",Gaze3DFix: Detecting 3D fixations with an ellipsoidal bounding volume,"Nowadays, the use of eyetracking to determine 2-D gaze positions is common practice, and several approaches to the detection of 2-D fixations exist, but ready-to-use algorithms to determine eye movements in three dimensions are still missing. Here we present a dispersion-based algorithm with an ellipsoidal bounding volume that estimates 3D fixations. Therefore, 3D gaze points are obtained using a vector-based approach and are further processed with our algorithm. To evaluate the accuracy of our method, we performed experimental studies with real and virtual stimuli. We obtained good congruence between stimulus position and both the 3D gaze points and the 3D fixation locations within the tested range of 200-600 mm. The mean deviation of the 3D fixations from the stimulus positions was 17 mm for the real as well as for the virtual stimuli, with larger variances at increasing stimulus distances. The described algorithms are implemented in two dynamic linked libraries (Gaze3D.dll and Fixation3D.dll), and we provide a graphical user interface (Gaze3DFixGUI.exe) that is designed for importing 2-D binocular eyetracking data and calculating both 3D gaze points and 3D fixations using the libraries. The Gaze3DFix toolkit, including both libraries and the graphical user interface, is available as open-source software at https://github.com/applied-cognition-research/Gaze3DFix.",,,
10.1088/1674-1056/ab53d1,2019,"Xu, Bin; Li, Xue-Ling; Wang, Yuan-Qing",Wide color gamut switchable autostereoscopic 3D display based on directional quantum-dot backlight,"A switchable autostereoscopic 3-dimensional (3D) display device with wide color gamut is introduced in this paper. In conjunction with a novel directional quantum-dot (QD) backlight, the precise scanning control strategy, and the eye-tracking system, this spatial-sequential solution enables our autostereoscopic display to combine all the advantages of full resolution, wide color gamut, low crosstalk, and switchable 2D/3D. And also, we fabricated an autostereoscopic display prototype and demonstrated its performances effectively. The results indicate that our system can both break the limitation of viewing position and provide high-quality 3D images. We present two working modes in this system. In the spatial-sequential mode, the crosstalk is about 6%. In the time-multiplexed mode, the viewer should wear auxiliary and the crosstalk is about 1%, just next to that of a commercial 3D display (BENQ XL2707-B and View Sonic VX2268WM). Additionally, our system is also completely compatible with active shutter glasses and its 3D resolution is same as its 2D resolution. Because of the excellent properties of the QD material, the color gamut can be widely extended to 77.98% according to the ITU-R recommendation BT.2020 (Rec.2020).",,,
,,FERN X; HUANG W X; JANKOWSKI C R; LI Q; SAVARESE S; ZHANG Z,"Data annotation apparatus for use in machine learning assembly, has data integration portion configured to integrate biometrics data, data of stimulus, and data of first machine learning dataset to obtain second machine learning dataset","NOVELTY - The apparatus (100) has a stimulus generation portion for generating and presenting a stimulus based on first data from first machine learning dataset (1) to an agent (8). A biometrics reading portion is configured to measure a response of the agent to the stimulus and generate biometrics data based on the response. A data integration portion is configured to integrate the biometrics data, data of the stimulus, and data of the first machine learning dataset to obtain second machine learning dataset (2). USE - Data annotation apparatus for use in a machine learning assembly (claimed). ADVANTAGE - The apparatus automatically carries out biometrics-mediated data labeling, thus saving great amount of time for increasing efficiency and saving cost. DETAILED DESCRIPTION - The biometrics data comprise electroencephalography data, magneto encephalography data, functional MRI data, single-photon emission computed tomography data, ligand-based positron emission tomography data, near infrared spectroscopy data, diffusion-tensor imaging data, magnetic resonance spectroscopy data, regional cerebral blood flow data, transcranial magnetic stimulation data, eye tracking data, skin sweet level data, temperature data, ECG data, motion data, respiration rate data, facial coding data, pupil dilation data, and blood pressure data. INDEPENDENT CLAIMS are also included for the following:(1) a machine learning assembly(2) a machine learning method. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a data annotation apparatus in a machine learning assembly.Machine learning datasets (1, 2)Agent (8)Data annotation apparatus (100)Machine learning apparatus (200)Machine learning assembly (900)",,,
,2018,"Sears, Matthew; Alruwaythi, Omar; Goodrum, Paul",Visualizing Eye Tracking Convex Hull Areas: A Pilot Study for Understanding How Craft Workers Interpret 2D Construction Drawings,"Engineering deliverables to construction craft workers have remained largely unchanged for a century. Black and white, 2D paper drawings are the primary medium by which engineering designs are communicated, but a certain level of experience is required in order for a craft worker to efficiently extract necessary information from a drawing. This work presents a novel method of visualizing and analyzing eye tracking data through the use of convex hull areas. Convex hull areas are proposed as a metric for measuring the amount of information that a participant is processing from a drawing at a given instant in time.Eye tracking data was collected in a previous study where 20 construction craft workers were tasked with assembling a PVC pipe assembly from traditional 2D isometric pipe spool drawings. Demographic data and spatial cognition data were also collected from each participant. In the present work, craft worker spatial cognition and years of construction experience were both shown to correlate with a craft worker's average convex hull area. The authors developed software for producing animations of eye tracking data convex hull areas, but have only begun to assess the data produced from the convex hull analysis method. Average convex hull areas were analyzed in the present work, but several potential additional metrics are suggested. This study was severely limited by the small sample size of the previous study and further data collection is warranted to confirm the findings presented herein.",,,
,,SON M K; KIM H; HUN L S,User terminal e.g. smartphone with gaze matching function has face information detector that inputs image photographed through camera to pre-built machine learning and inputs image acquired while learning is completed into machine learning,"NOVELTY - The user terminal (100) has a database (110) that stores a display screen size of user terminal and location information of a camera lens mounted on user terminal. A receiving unit (120) receives an image captured through camera. A face information detector (130) obtains user face position information and eye position information. A control unit (140) estimates a viewpoint value and derives a corrected final viewpoint value from the estimated actual viewpoint value. An eye image generator (150) generates a pupil image with a changed viewpoint by applying final viewpoint value to image of pupil included in input image. An output unit (160) matches the pupil image of which viewpoint is changed to image and outputs corrected image on the display screen. The face information detector inputs image photographed through camera to pre-built machine learning and inputs image acquired while learning is completed into machine learning to detect the location of the user face and eyes. USE - User terminal such as smartphone, personal desktop assistant (PDA) and notebook computer with gaze matching function. ADVANTAGE - The user terminal corrects the gaze caused by the different positions of the display screen and the camera in software when taking a self photo or performing a video call. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an eye gaze synchronizing method using the user terminal. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a user terminal.User terminal (100)Database (110)Receiving unit (120)Face information detector (130)Control unit (140)Eye image generation unit (150)Output unit (160)",,,
10.1016/j.sigpro.2018.01.003,2018,"Zhang, Xiaoning; Xiao, Di; Li, Jianhua; Qi, Jinqing; Lu, Huchuan",Predicting human gaze with multi-level information,"Eye fixation models, which try to quantitatively predict human eye attended areas in visual fields, have received increasing interest in recent years. In this paper, a novel framework is proposed for the detection of eye fixations. First, a multi-channel detection module, which extracts information of color contrast, salient object proposals and center bias from input image, is conducted to introduce various useful information into the subsequent fixations detection. In salient object detection channel, we employ the multi-instance learning (MIL) algorithm to determine which object proposal can attract attention, which avoids the fuzzyness of positive sample selection. Second, an adaptive weighted fusion method achieved by deep learning framework is proposed to fuse the multi-level information (i.e., contrast, objective, center bias) together for the detection of fixations, so that the integration of information between each level becomes more scientific. Finally, the detection result is optimized by embedding semantic information. Experimental results show that the algorithm has achieved competitive results in MIT1003, MIT300 and Toronto120 dataset. (C) 2018 Elsevier B.V. All rights reserved.",,,
,2020,"Hegde, Srinidhi; Maurya, Jitender; Hebbalaguppe, Ramya; Kalkar, Aniruddha",SmartOverlays: A Visual Saliency Driven Label Placement for Intelligent Human-Computer Interfaces,"In augmented reality (AR), the computer generated labels assist in understanding a scene by addition of contextual information. However, naive label placement often results in clutter and occlusion impairing the effectiveness of AR visualization. For label placement, the main objectives to be satisfied are, non-occlusion to the scene of interest, the proximity of labels to the object, and, temporally coherent labels in a video/live feed. We present a novel method for the placement of labels corresponding to objects of interest in a video/live feed that satisfies the aforementioned objectives. Our proposed framework, SmartOverlays, first identifies the objects and generates corresponding labels using a YOLOv2 [28] in a video frame; at the same time, Saliency Attention Model (SAM) [7] learns eye fixation points that aid in predicting saliency maps; finally, computes Voronoi partitions of the video frame, choosing the centroids of objects as seed points, to place labels for satisfying the proximity constraints with the object of interest. In addition, our approach incorporates tracking the detected objects in a frame to facilitate temporal coherence between frames that enhances the readability of labels. We measure the effectiveness of SmartOverlays framework using three objective metrics: (a) Label Occlusion over Saliency (LOS), (b) temporal jitter metric to quantify jitter in the label placement, (c) computation time for label placement.",,,
10.3390/s19051172,2019,"Tokuoka, Mikihiro; Komiya, Naoki; Mizoguchi, Hiroshi; Egusa, Ryohei; Inagaki, Shigenori; Kusunoki, Fusako",Implementation and Evaluation of a Wide-Range Human-Sensing System Based on Cooperating Multiple Range Image Sensors,"A museum is an important place for science education for children. The learning method in the museum is reading exhibits and explanations. Museums are investing efforts to quantify interests using questionnaires and sensors to improve their exhibitions and explanations. Therefore, even in places where many people gather, such as in museums, it is necessary to quantify people's interest by sensing behavior of multiple people. However, this has not yet been realized. We aim to quantify the interest by sensing a wide range of human behavior for multiple people by coordinating multiple noncontact sensors. When coordinating multiple sensors, the coordinates and the time of each sensor differ. To solve these problems, coordinates were transformed using a simultaneous transformation matrix and time synchronization was performed using unified time. The effectiveness of this proposal was verified through experimental evaluation. Furthermore, we evaluated the actual museum content. In this paper, we describe the proposed method and the results of the evaluation experiment.",,,
10.1109/WACV48630.2021.00111,2021,"Min, Kyle; Corso, Jason J.",Integrating Human Gaze into Attention for Egocentric Activity Recognition,"It is well known that human gaze carries significant information about visual attention. However, there are three main difficulties in incorporating the gaze data in an attention mechanism of deep neural networks: (i) the gaze fixation points are likely to have measurement errors due to blinking and rapid eye movements; (ii) it is unclear when and how much the gaze data is correlated with visual attention; and (iii) gaze data is not always available in many real-world situations. In this work, we introduce an effective probabilistic approach to integrate human gaze into spatiotemporal attention for egocentric activity recognition. Specifically, we represent the locations of gaze fixation points as structured discrete latent variables to model their uncertainties. In addition, we model the distribution of gaze fixations using a variational method. The gaze distribution is learned during the training process so that the ground-truth annotations of gaze locations are no longer needed in testing situations since they are predicted from the learned gaze distribution. The predicted gaze locations are used to provide informative attentional cues to improve the recognition performance. Our method outperforms all the previous state-of-the-art approaches on EGTEA, which is a large-scale dataset for egocentric activity recognition provided with gaze measurements. We also perform an ablation study and qualitative analysis to demonstrate that our attention mechanism is effective.",,,
10.3390/s17071534,2017,"Kim, Ki Wan; Hong, Hyung Gil; Nam, Gi Pyo; Park, Kang Ryoung",A Study of Deep CNN-Based Classification of Open and Closed Eyes Using a Visible Light Camera Sensor,"The necessity for the classification of open and closed eyes is increasing in various fields, including analysis of eye fatigue in 3D TVs, analysis of the psychological states of test subjects, and eye status tracking-based driver drowsiness detection. Previous studies have used various methods to distinguish between open and closed eyes, such as classifiers based on the features obtained from image binarization, edge operators, or texture analysis. However, when it comes to eye images with different lighting conditions and resolutions, it can be difficult to find an optimal threshold for image binarization or optimal filters for edge and texture extraction. In order to address this issue, we propose a method to classify open and closed eye images with different conditions, acquired by a visible light camera, using a deep residual convolutional neural network. After conducting performance analysis on both self-collected and open databases, we have determined that the classification accuracy of the proposed method is superior to that of existing methods.",,,
10.1051/itmconf/20171502002,2017,"Borys, Magdalena; Barakate, Sara; Hachmoud, Karim; Plechawska-Wojcik, Malgorzata; Krukow, Pawel; Kaminski, Marek",Classification of user performance in the Ruff Figural Fluency Test based on eye-tracking features,"Cognitive assessment in neurological diseases represents a relevant topic due to its diagnostic significance in detecting disease, but also in assessing progress of the treatment. Computer-based tests provide objective and accurate cognitive skills and capacity measures. The Ruff Figural Fluency Test (RFFT) provides information about non-verbal capacity for initiation, planning, and divergent reasoning. The traditional paper form of the test was transformed into a computer application and examined. The RFFT was applied in an experiment performed among 70 male students to assess their cognitive performance in the laboratory environment. Each student was examined in three sequential series. Besides the students' performances measured by using in app keylogging, the eye-tracking data obtained by non-invasive video-based oculography were gathered, from which several features were extracted. Eye-tracking features combined with performance measures (a total number of designs and/or error ratio) were applied in machine learning classification. Various classification algorithms were applied, and their accuracy, specificity, sensitivity and performance were compared.",,,
,,RAUT P U; PATIL A B; TOMAR A; BHOSALE G G; SURI Y; MOAZ M A M,"Method for generating augmented reality or a virtual reality environment, involves receiving three dimensional spatial image data of real world environment from a first electromagnetic radiation sensor","NOVELTY - The method (500) involves receiving (510) 3-Dimensional (3D) spatial image data of a real world environment from a first electromagnetic radiation sensor. The eye tracking data pertaining is received (520) to movement of an eye of a user from a second electromagnetic radiation sensor. The hand tracking data pertaining is received (530) to movement of one or more hands of the user from a third electromagnetic radiation sensor. A 3D mesh pertaining is generated (540) to virtual reality or augmented reality objects, as a function of the spatial image data, eye tracking data and the hand tracking data. The 3D mesh display sources is displayed (550). USE - Method for generating an augmented reality or a virtual reality environment. ADVANTAGE - The MRD and the method offer a much larger range Field of view and higher quality holograms for relatively accurate augmented and virtual reality environments, that makes suitable for a gamut of applications such as medical sciences, defense training and automotive design. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a Mixed Reality Device (MRD) for generating an augmented reality or a virtual reality environment. DESCRIPTION Of DRAWING(S) - The drawing shows a flow chart of a method.Method (500)Receiving three Dimensional spatial image data of a real world environment (510)Receiving eye tracking data pertaining (520)Receiving hand tracking data pertaining (530)Generating a three dimensional mesh (540)Displaying the three dimensional mesh on display sources (550)",,,
10.1504/IJVD.2020.115057,2020,"Zhang, Zhaozhong; Velenis, Efstathios; Fotouhi, Abbas; Auger, Daniel J.; Cao, Dongpu",Driver distraction detection using machine learning algorithms: an experimental approach,"Driver distraction is the leading cause of accidents that contributes to 25% of all road crashes. In order to reduce the risks posed by distraction, the warning must be given to the driver once distraction is detected. According to the literature, no rankings of relevant features have been presented. In this study, the most relevant features in detecting driver distraction are identified in a closed testing environment. The relevant features are found to be the mean values of speed and lane deviation, maximum values of eye gaze in y direction, and head movement in x direction. After the relevant features have been identified, pre-processed data with relevant features are fed into decision tree classifiers to discriminate the data into normal and distracted driving. The results show that the detection accuracy of 78.4% using decision tree can be achieved. By eliminating unhelpful features, the time required to process data is reduced by around 40% to make the proposed technique suitable for real-time application.",,,
,,DIAO H; HUANG L,"Three-dimensional display device, has human eye tracking device for obtaining human eye space position, and three-dimensional processing device for rendering multi-viewpoint naked eye three-dimensional display screen corresponding to viewpoint of sub-pixels","NOVELTY - The utility model claims a naked eye type 3 D display technology, claims a human eye tracking device, comprising: a human eye tracker, comprising a first black and white camera configured to capture a first black and white image and a second black and white camera configured to capture a second black and white image; a human eye tracking image processor configured to identify the presence of the human eye based on at least one of the first black and white image and the second black and white image and determine the human eye space position based on the human eye identified in the first black and white image and the second black and white image. The device can realize respectively high precision determining the space position of the user eyes and high speed identifying human eyes, or real-time tracking human eyes. The utility model further claims a 3 D display device.",,,
,,SONG S K; LEE S M; CHO D H; SONG S; LEE S,"Robot service learning system for use in e.g. home, has machine learning unit for analyzing user interest and familiarity with respect to robot from user reaction data for indicating reaction of user through execution of reaction data","NOVELTY - The system (AA, 100) has a recognition data collecting unit (110) for collecting user recognition data representing user feelings. A reaction data selection instruction unit (130) selects robot reaction data in a robot reaction database according to user recognition data and instructing execution. A machine learning unit (150) analyzes user interest, familiarity and affinity with respect to a robot (10) from the user reaction data for indicating reaction of a user through execution of the robot reaction data to adjust selection probability of the robot reaction data, where the user recognition data includes facial expression, voice content, voice tone, motion of the user and the user reaction data includes the facial expression, voice content, implicit feedback data comprising pulse and eye gaze. USE - Robot service learning system for use in a home and an industrial field. ADVANTAGE - The system understands emotion and reaction of the user through interaction between the user and the robot, adjusts and learns behavioral probability of the robot suitable for the user and provides enhanced user-customized robot service. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a robot service learning method. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a robot service learning system. '(Drawing includes non-English language text)'Robot service learning system (AA, 100)Robot (10)Recognition data collecting unit (110)Reaction data selection instruction unit (130)Machine learning unit (150)",,,
10.1117/1.OE.59.10.102410,2020,"Du, Jingyan; Sang, Xinzhu; Yu, Xunbo; Gao, Xin; Gao, Chao; Yan, Binbin",Demonstration of a large viewing angle and high-resolution floating three-dimensional display based on the multichannel and multivariable correction algorithm,"Floating three-dimensional (3-D) display can provide a natural and realistic 3-D scene. Nowadays, because of stray light and aberration, most floating 3-D displays cannot realize a large viewing angle and high resolution simultaneously, and it directly deteriorates the viewing effect of the 3-D scene. A large viewing angle floating and high-resolution 3-D display based on multichannel and multivariable (MCMV) correction algorithm are presented. The optical system consists of a time-sequential autostereoscopic 3-D display with the directional backlight, a floating lens, and an eye tracker. The 3-D display with the directional backlight provides loss-free resolution for the system. To realize a large viewing angle floating 3-D display, MCMV correction algorithm based on eye tracking is proposed. The 3-D images can be reconstructed by the optical system within the viewing angle of 60 deg. The feasibility of the proposed display method is verified with the experimental results. (C) 2020 Society of Photo-Optical Instrumentation Engineers (SPIE)",,,
10.1109/TPAMI.2018.2871688,2019,"Zhang, Mengmi; Ma, Keng Teck; Lim, Joo Hwee; Zhao, Qi; Feng, Jiashi",Anticipating Where People will Look Using Adversarial Networks,"We introduce a new problem of gaze anticipation on future frames which extends the conventional gaze prediction problem to go beyond current frames. To solve this problem, we propose a new generative adversarial network based model, Deep Future Gaze (DFG), encompassing two pathways: DFG-P is to anticipate gaze prior maps conditioned on the input frame which provides task influences; DFG-G is to learn to model both semantic and motion information in future frame generation. DFG-P and DFG-G are then fused to anticipate future gazes. DFG-G consists of two networks: a generator and a discriminator. The generator uses a two-stream spatial-temporal convolution architecture (3D-CNN) for explicitly untangling the foreground and background to generate future frames. It then attaches another 3D-CNN for gaze anticipation based on these synthetic frames. The discriminator plays against the generator by distinguishing the synthetic frames of the generator from the real frames. Experimental results on the publicly available egocentric and third person video datasets show that DFG significantly outperforms all competitive baselines. We also demonstrate that DFG achieves better performance of gaze prediction on current frames in egocentric and third person videos than state-of-the-art methods.",,,
,,WANG G; YU W,"Field holographic display system, has human eye part and holographic projector connected with projection screen, and holographic projector whose conjugated window is connected with projection screen for covering position of human eye part","NOVELTY - The system has a processor (7) connected with a holographic projector (2). A human eye tracking unit (4) and a motion driving module (6) are electrically connected with each other. The processor sends data information to the holographic projector. The human eye tracking unit transmits locating information of a human eye part (8) to the processor. The processor controls the motion driving module according to the locating information of the human eye part. The human eye part and the holographic projector are connected with a projection screen (1) i.e. transmission projection screen. A conjugated window of the holographic projector is connected with the projection screen for covering position of the human eye part. USE - Field holographic display system. ADVANTAGE - The system utilizing non-parallel light as reference light source so as to form interference pattern formed by setting projection light of the reference light source and the holographic projector using phase modulation. The system avoids obtaining problem of distortion-free 3D image observed by the human eye part so as to display high quality and large size 3D image, thus satisfying requirements of a projection area as the projection screen and high quality and large size 3D diagram of the reference light source, and hence avoiding optical path configuration difficulties and simplifying light path arrangement, and is inexpensive. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a field holographic display system.Projection screen (1)Holographic projector (2)Human eye tracking unit (4)Motion driving module (6)Processor (7)Human eye part (8)",,,
,2019,"Bhattarai, Rasa; Phothisonothai, Montri",Eye-Tracking Based Visualizations and Metrics Analysis for Individual Eye Movement Patterns,"Uniqueness in the analysis pattern of objects by individual humans has a profound impact on the study of their visual learning and behavior. Eye movement patterns have been effectively emerging as a biometric based key for security systems, product recognition patterns, user identifications, as well as medical research purposes. The modern eye tracking systems are non-invasive and financially affordable. Therefore, in this paper, we proposed eye-tracking based visualizations and metrics analysis for individual eye movement patterns collected during any kinds of activities depending on the scope of the our experimental paradigms. Individuals can be aware of their own performances during certain task and improve upon their weak areas. The objective of the paper is to utilize the important visual metrics obtained from fixation, saccades and face recognition and use them to analyze for individual categorization. The obtained results shown that the specific features and patterns can be extracted the viewing aspect of individual subjects using naive Bayes classifier. We were successfully able to predict the individual eye movements with an accuracy of 90.22%.",,,
10.1109/ACCESS.2021.3054951,2021,"Yuan, Guoliang; Wang, Yafei; Peng, Jinjia; Fu, Xianping",A Novel Driving Behavior Learning and Visualization Method With Natural Gaze Prediction,"Driving behavior analysis is vital for the advanced driving assistance system, aiming to improve driving behavior and decrease traffic accidents. Most existing driving behavior learning methods focus on either vehicle sensor information or driver's attention information, and provide a classification result on the current time data samples. The visualization of driving behavior on time series data samples could give an understanding and review of the driver's continuous actions. However, there has been little progress in combining the multi-modal vehicle and driver information on driving behavior learning and visualization. A multi-information driving behavior learning and visualization method with natural gaze prediction is proposed in this paper, which automatically integrates driver's gaze direction estimated from face camera, and various vehicle sensor data collected from on-board diagnostics (OBD) system. To accurately estimate the eye gaze under large head movement, a novel head pose-free eye gaze prediction method without calibration is proposed based on global and local scale sparse encoding, which treats the direction mapping as small gaze region classification. To understand driving behavior more intuitively, the latent features that represent different driving behaviors are extracted by FastICA from the fused time series data, and mapped into RGB color space for distinguished visualization. Experimental results demonstrate the effectiveness of the proposed method, and show that the proposed method performs better than the compared methods.",,,
,,RATCLIFFL J; SUPIKOV A; AZUMA R; HUANG Q; LI T; RATCLIFF J J; SUPIKOV A M; AZUMA R T; RATCLIFF J,"Three-dimensional display device for 3D TV, has eye tracking device for tracking eyes, and light field processor for utilizing eye position information to convert color plus depth images or light field images into display images","NOVELTY - The device has an eye tracking device for tracking eyes. A rendering processor renders or processes color plus depth images or light field images. A light field processor utilizes eye position information to convert the rendered or processed color plus depth images or light field images into display images. The rendering processor renders the light field images based on the eye position information. The light field processor synthesizes the rendered or captured color plus depth images. A display backlight unit (314) guides backlight based on the eye position information. USE - Three-dimensional display device for a 3D TV, 3D cinema and 3D games. ADVANTAGE - The device is stereoscopically assembled in a user's brain to effectively re-create binocular viewing experience of 3D visual perception. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a three dimensional display light field imaging method(2) a tangible, non-transitory, machine-readable storage medium for storing set of instructions to perform an operation of a three-dimensional display device. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a three-dimensional display device. '(Drawing includes non-English language text)'User (302)Pupils (304)Display backlight unit (314)Computing device (322)Light field processing unit (324)",,,
,,LU Z,"Light field display system, has controller that is configured to calculate display content and display direction of each vector pixel in controller-controlled light panel according to acquired three dimensional (3D) display data","NOVELTY - The system has a rotating display screen subsystem that is a rotating structure formed by a light pole, a controller, a light pole fixing device and a motor. The light pole includes a first preset number of light boards. The display positioning device includes multiple lasers and a photosensitive sensor. The human eye tracking subsystem includes a human eye tracking camera and a position calculating unit. The controller is configured to determine a position of the light board controlled by the controller. The display content and display direction of each vector pixel in the controller-controlled light panel is calculated according to the acquired three dimensional (3D) display data and the spatial position and the line-of-sight direction of the eyes of the viewer. Each vector pixel is driven on the light board controlled by the controller to display the display content in the display direction to implement 3D image display. USE - Light field display system. ADVANTAGE - The light field display system is provided, so that multiple people can simultaneously view the naked eye 3D image, and the 3D image content seen by the viewers at different viewing positions is different, and the resolution and depth of field of the naked eye 3D image are improved, and the viewer perspective is increased. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of the light field display system. (Drawing includes non-English language text)",,,
,,ANDERSON G J,"Machine-readable medium for embedding human labeler influences in machine learning interfaces, has set of instructions for creating and training unified machine learning model based on features associated with classified sensor data","NOVELTY - The medium has a set of instructions for classifying sensor data with human labeler data by using a computing device (100), where the sensor data is obtained through a set of sensors. An unified machine learning model is created and trained based on features associated with the classified sensor data based on the human labeler data, where the features include human labeler influences obtained from the human labeler data in associated with the classified sensor data and the set of sensors includes one of a camera, a microphone, a touch sensor, a capacitor, a radio component, a radar component, a scanner and an accelerometer. USE - Machine-readable medium for embedding human labeler influences in machine learning interfaces in computing environments. ADVANTAGE - The medium enables performing basic operations by machine learning primitives based on machine learning algorithms without using a machine learning framework so as to create and optimize main computational logic associated with the machine learning algorithms by requiring developers of the machine learning algorithms and re-optimize and develop the computational logic as parallel processors. The medium enables determining latency in showing a decision distinguishing from conventional eye tracking techniques so as to repeatedly play a sample before classifying, or facilitate behaviors and variables by evaluating filtering logic. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a method for embedding human labeler influences in machine learning interfaces in computing environments(2) an apparatus for embedding human labeler influences in machine learning interfaces in computing environments. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a computing device employing a human labeler mechanism.Computing device (100)I/o sources (104)Operating system (106)Human labeler mechanism (110)Application processor (112)",,,
10.1109/TIP.2017.2722238,2017,"Le Meur, Olivier; Coutrot, Antoine; Liu, Zhi; Rama, Pia; Le Roch, Adrien; Helo, Andrea",Visual Attention Saccadic Models Learn to Emulate Gaze Patterns From Childhood to Adulthood,"How people look at visual information reveals fundamental information about themselves, their interests and their state of mind. While previous visual attention models output static 2D saliency maps, saccadic models aim to predict not only where observers look at but also how they move their eyes to explore the scene. In this paper, we demonstrate that saccadic models are a flexible framework that can be tailored to emulate observer's viewing tendencies. More specifically, we use fixation data from 101 observers split into five age groups (adults, 8-10 y.o., 6-8 y.o., 4-6 y.o., and 2 y.o.) to train our saccadic model for different stages of the development of human visual system. We show that the joint distribution of saccade amplitude and orientation is a visual signature specific to each age group, and can be used to generate age-dependent scan paths. Our age-dependent saccadic model does not only output human-like, age-specific visual scan paths, but also significantly outperforms other state-of-the-art saliency models. We demonstrate that the computational modeling of visual attention, through the use of saccadic model, can be efficiently adapted to emulate the gaze behavior of a specific group of observers.",,,
,,ALIABADI A; ROBERTS G D,"System for executing convolutional neural network (CNN), has output activation maps which are provided in interleaved output activation map layout comprising clusters output activation map pixels","NOVELTY - The system has a hardware processor to receive input activation maps (104) of convolutional layer. The processor reorders pixel values of input activation maps from basic input activation map layout into an interleaved input activation map layout comprising clusters of input activation map pixels. The processor determines output activation maps (112) of the convolutional layer from kernel tiles and clusters of input activation map pixels. The output activation maps are in an interleaved output activation map layout comprising clusters output activation map pixels. USE - System for executing convolutional neural network (CNN). Uses include but are not limited to augmented reality, mixed reality, virtual reality, machine learning, computer vision, facial recognition, eye tracking, object recognition, character, language, or speech analysis, computer games, etc. ADVANTAGE - The efficient computation of a convolution of an input activation map is enabled with a kernel in terms of the processing or mathematically aspect of convolutional layer. The power consumption of a computing device implementing a CNN is reduced, since the processor of the computing device is in an on state or a high frequency state for a shorter period of time. The convolutions can be performed in an improved or optimal fashion on vector processors. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of two-dimensional (2D) convolution.Convolution (100)Input activation map (104)Kernel (108)Output activation map (112)",,,
,,GONTKO L; CHRAPPA R; VARHANIK E,"Multi-functional three dimensional (3D) imaging device for computed tomography (CT) and magnetic resonance (MR) images, has auto-stereoscopic display that is provided with camera and driver are connected to personal computer","NOVELTY - The multi-functional 3D imaging device has motorized multi-positioning stand (2) with storage space on personal computer and auto-stereoscopic display (1) with cameras (3) for an eye tracking technology. The portion of the auto-stereoscopic display is a control touch layer and partially transparent mirror glass with a multi-touch foil that is attached to an auto-stereoscopic display and fastened by unit of motor or pneumatic hinge. The auto-stereoscopic display with cameras and the driver are connected to a personal computer. USE - Multi-functional three dimensional (3D) imaging device for computed tomography (CT) and magnetic resonance (MR) images with technology of parallax barrier or technology of pepper's ghost. ADVANTAGE - The images is controlled by multi-touch control, PC keyboard, joystick, mouse, leap motion-camera, myo-wrist band, manus-virtual reality (VR) gloves, or various virtual reality drivers as needed for given usage, where the physician as the viewer of those images can zoom, rotate, mark, select and cut the image. The multi-function 3D imaging device for CT and MR images consists of a motorized multi-positioning stand with storage space for PC and a modified auto-stereoscopic display. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of multi-functional 3D imaging device with cameras for use with the eye tracking technology.Auto-stereoscopic display (1)Motorized multi-positioning stand (2)Camera (3)",,,
,,LIN Y,"Eye tracking method in electronic device e.g. head mount display (HMD) for detecting gaze direction of user, involves measuring similarity value between estimated pupil region and pupil region of interest to optimize eye model","NOVELTY - The method (900) involves obtaining (S1) an eye model by a processing circuit. An image of an eye is captured (S2) by a camera. A pupil region of interest is detected (S3) in the image of the eye by the processing circuit. An estimated pupil region is calculated in the image of the eye based on the eye model by the processing circuit. A similarity value between the estimated pupil region and the pupil region of interest is measured to optimize the eye model, by the processing circuit. USE - Eye tracking method in electronic device (claimed) e.g. HMD such as standalone HMD and VIVE HMD (RTM: virtual reality headset) used in virtual reality (VR)/mixed reality (MR)/augmented reality (AR) system for detecting gaze direction of user. Can also be used in various applications such as virtual reality (VR) or augmented reality (AR) application, and VR/AR system to trace user's gazing direction to provide corresponding reaction and/or control in VR/AR environment. ADVANTAGE - The camera captures several images of eye of the user, such that the processing circuit analyzes several images to perform eye tracking. The accuracy of eye tracking is improved, since the calibration of the eye model is performed to meet several users' specific pupil shape. The processing circuit controls the video and/or audio content displayed by the display unit based on the gaze direction of the user, in order to provide smooth VR experience and/or friendly user interactive interface. The head position calibration is performed to optimize the eye model to increase the accuracy of the eye tracking. The accuracy of the eye model is valued, and the eye model is optimized correspondingly when the original eye model is failed due to the change of the position and/or angle of the HMD, by comparing the similarity between the estimated pupil region and the pupil region of interest. The processing circuit optimizes the eye model to minimize cost function according to the error values. The machine learning methods are applied to minimize the cost of errors between the estimated pupil region and the pupil region of interest to optimize the eye model. The eye tracking method with the head position calibration process is implemented to realize the eye tracker for the application in the VR, AR or mixed reality (MR) and increase the accuracy of eye tracking, which brings smoother user experience for the user wearing the HMD to interact with the object in the VR, AR or MR environment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an electronic device; and(2) a non-transitory computer readable storage medium storing eye tracking program. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the eye tracking method.Eye tracking method (900)Step for obtaining eye model (S1)Step for capturing image of eye, by camera (S2)Step for detecting pupil region of interest in the image of the eye (S3)Step for analyzing the pupil region of interest to obtain gaze vector of the pupil region of interest (S4)",,,
,,HEE D S; WOON Y J; HONG K Y; KYUN N H; KEUN A C; RIN S G; JIN H S; JOONG K S; SU K H; DO LEE J; RI J J; RYOUN N H; WOOK L D; HYUNE P J; JIN L D; TAE C S; GYEOM W S; WAN K T,"Method for processing three dimensional image i.e. polygon image of virtual reality contents at mobile device, involves fixing shader of peripheral object from target object with pre-set distance, and rendering target object based on light",NOVELTY - The method involves setting up first light for a moving object among multiple objects within three-dimensional (3D) image. Second light is set up to a pre-set target object among the objects within the 3D image. The 3D image is processed based on the first light for first photo-silver and shader of the moving object and the second light for second photo-silver and the target object. The shader of the peripheral object is fixed from the target object with pre-set distance. The moving objects are rendered within 3D image based on the first light. The target object is rendered based on the second light. The peripheral objects are rendered within the 3D image. The fixed type object among the objects is rendered within the 3D image to exclude effect of the first light. Brightness and shading effect is set to the moving object among the peripheral objects. USE - Method for processing a 3D image i.e. polygon image of virtual reality (VR) contents (claimed) at a mobile device. ADVANTAGE - The method enables processing 3D image and producing eye gaze derivative in the 3D image. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a computer-readable recording medium for storing a set of instructions to perform a 3D image processing method(2) a 3D image processing apparatus. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a 3D image processing method. '(Drawing includes non-English language text)',,,
,,KANG D; HEO J; WOO K D; GU H J; XU Z,"Method for facilitating tracking of eye of user, involves removing reflection area from input image, and outputting coordinates corresponding to eye of user from image in which reflection area is removed","NOVELTY - The method involves determining whether a reflection area (113) generated by an illumination is included in an input image. Reflection area is removed from the input image in response to determining that the reflection area is included in the input image. Coordinates are outputted corresponding to an eye of a user from an image in which the reflection area is removed. Reflection area is removed by processing the input image including the reflection area using a parameter learned through machine learning based on a database including the image in which the reflection area is removed. Reflection area is removed from the input image through image processing based on an illumination compensation algorithm. USE - Method for facilitating tracking of eye of user for use in viewpoint tracking-based autostereoscopic or glassless three-dimensional (3D) super-multiview (SMV) display application. ADVANTAGE - The method enables removing reflection area included in an input image through machine learning or image processing based on various illumination compensation algorithms, thus increasing accuracy and success rate of eye tracking or maintaining preset distance between the camera and the infrared light source. The method enables identifying eye, nose, lip, and a shape of a face from the images obtained by capturing face image of a user by the infrared camera. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a non-transitory computer-readable storage medium for storing set of instructions for facilitating tracking of eye of user(2) an apparatus for facilitating tracking of eye of user. DESCRIPTION Of DRAWING(S) - The drawing shows a photographic view of a pupil or a shape of an eye being occluded due to a reflection by an infrared light source.Images (110)Reflection area (113)Reflection areas (133,136)",,,
,,NI T; GU K,"Automatic three-dimensional stereoscopic image displaying method, involves calculating parameters of liquid crystal grating in display area, controlling voltage of display area, and displaying stereoscopic image in display area",NOVELTY - The method involves obtaining information of human eyes blink. A human eye-gaze point on a stereoscopic display device is obtained according to position information. Position information of an attention point in a display area is determined according to eye movement information and a predetermined calibration file. Parameters of a liquid crystal grating in the display area are calculated. Voltage of the display area is controlled through the liquid crystal grating parameter. A stereoscopic image is displayed in the display area. A human eye image is obtained. USE - Automatic three-dimensional stereoscopic image displaying method. ADVANTAGE - The method enables obtaining eye blink information by a viewer at a corresponding 3D image display area and calculating liquid crystal grating parameters of viewer eye information so as to improve better 3D display area effect. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an automatic three-dimensional (3D) stereoscopic image displaying device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating an automatic three-dimensional stereoscopic image displaying method. '(Drawing includes non-English language text)',,,
,,SUNG H K; JEONG S Y; HEE B Y,"Method for representing object of three-dimensional geographical spatial in e.g. actual surface facility, involves organizing camera information on space screen, and materializing sub object on geographical feature space","NOVELTY - The method involves producing data request spatial domain information in a region area information analysis phase using a minimum coordinate in camera position. Screen coordinate information is obtained toward a data request spatial domain of a user, where the screen coordinate information is converted into three-dimensional (3D) space coordinate information. Camera information is organized on a 3D space screen. A group object or a sub object is selectively materialized on geographical feature 3D space. USE - Method for representing an object of three-dimensional geographical spatial in an actual surface facility and underground facilities. ADVANTAGE - The method enables increasing information conformability efficient so as to prevent a space for existing an information offer field from being unnecessary, thus providing object information to secure eye gaze of a user into an object image and an information offer field. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a three-dimensional geographical spatial object. '(Drawing includes non-English language text)'User terminal (100)Camera (200)",,,
10.3758/s13428-017-0857-y,2018,"Faber, Myrthe; Bixler, Robert; D'Mello, Sidney K.",An automated behavioral measure of mind wandering during computerized reading,"Mind wandering is a ubiquitous phenomenon in which attention shifts from task-related to task-unrelated thoughts. The last decade has witnessed an explosion of interest in mind wandering, but research has been stymied by a lack of objective measures, leading to a near-exclusive reliance on self-reports. We addressed this issue by developing an eye-gaze-based, machine-learned model of mind wandering during computerized reading. Data were collected in a study in which 132 participants reported self-caught mind wandering while reading excerpts from a book on a computer screen. A remote Tobii TX300 or T60 eyetracker recorded their gaze during reading. The data were used to train supervised classification models to discriminate between mind wandering and normal reading in a manner that would generalize to new participants. We found that at the point of maximal agreement between the model-based and self-reported mind-wandering means (smallest difference between the group-level means: M (model) = .310, M (self) = .319), the participant-level mind-wandering proportional distributions were similar and were significantly correlated (r = .400). The model-based estimates were internally consistent (r = .751) and predicted text comprehension more strongly than did self-reported mind wandering (r (model) = -.374, r (self) = -.208). Our results also indicate that a robust strategy of probabilistically predicting mind wandering in cases with poor or missing gaze data led to improved performance on all metrics, as compared to simply discarding these data. Our findings demonstrate that an automated objective measure might be available for laboratory studies of mind wandering during reading, providing an appealing alternative or complement to self-reports.",,,
10.3390/s19081911,2019,"Elsahar, Yasmin; Hu, Sijung; Bouazza-Marouf, Kaddour; Kerr, David; Mansor, Annysa",Augmentative and Alternative Communication (AAC) Advances: A Review of Configurations for Individuals with a Speech Disability,"High-tech augmentative and alternative communication (AAC) methods are on a constant rise; however, the interaction between the user and the assistive technology is still challenged for an optimal user experience centered around the desired activity. This review presents a range of signal sensing and acquisition methods utilized in conjunction with the existing high-tech AAC platforms for individuals with a speech disability, including imaging methods, touch-enabled systems, mechanical and electro-mechanical access, breath-activated methods, and brain-computer interfaces (BCI). The listed AAC sensing modalities are compared in terms of ease of access, affordability, complexity, portability, and typical conversational speeds. A revelation of the associated AAC signal processing, encoding, and retrieval highlights the roles of machine learning (ML) and deep learning (DL) in the development of intelligent AAC solutions. The demands and the affordability of most systems hinder the scale of usage of high-tech AAC. Further research is indeed needed for the development of intelligent AAC applications reducing the associated costs and enhancing the portability of the solutions for a real user's environment. The consolidation of natural language processing with current solutions also needs to be further explored for the amelioration of the conversational speeds. The recommendations for prospective advances in coming high-tech AAC are addressed in terms of developments to support mobile health communicative applications.",,,
10.1371/journal.pone.0210858,2019,"Chen, Fei; Wang, Lan; Peng, Gang; Yan, Nan; Pan, Xiaojie",Development and evaluation of a 3-D virtual pronunciation tutor for children with autism spectrum disorders,"The deficit in speech sound production in some children with autism spectrum disorder (ASD) adds to their communication barriers. The 3-D virtual environments have been implemented to improve their communication abilities. However, there were no previous studies on the use of a 3-D virtual pronunciation tutor designed specifically to train pronunciation for children with ASD. To fill this research gap, the current study developed and evaluated a 3D virtual tutor which served as a multimodal and real-data-driven speech production tutor to present both places and manners of Mandarin articulation. Using an eye-tracking technique (RED 5 Eye Tracker), Experiment 1 objectively investigated children's gauged attention distribution online while learning with our computer-assisted 3-D virtual tutor in comparison to a real human face (HF) tutor. Eye-tracking results indicated most participants showed more interests in the visual speech cues of the 3-D tutor, and paid some degree of absolute attention to the additional visual speech information of both articulatory movements and airflow changes. To further compare treatment outcomes, training performance was evaluated in Experiment 2 with the ASD learners divided into two groups, with one group learning from the HF tutor and the other from the 3-D tutor (HF group vs. 3-D group). Both groups showed improvement with the help of computer-based training in the post-intervention test based on the calculation of a 5-point Likert scale. However, the 3-D group showed much higher gains in producing Mandarin stop and affricate consonants, and apical vowels. We conclude that our 3-D virtual imitation intervention system provides an effective approach of audiovisual pronunciation training for children with ASD.",,,
,,ZHAO W; ZHANG Z; WU H; LI S,"Artificial intelligence liquid glass for artificial reality (AR) display device, has eye tracking module that changes curvature radius of lens in lens screen as needed to realize focal length change and real-time display","NOVELTY - The glass has an eye tracking module (1) that tracks a pupil position and size, and sends to a data processing module (2) through intelligent analysis and processing to obtain the driving parameters. The eye tracking module autonomously changes the curvature radius of the lens in an integrated lens screen (5) as needed to realize the focal length change and real-time display. A reality capture camera (7) obtains the image in front of the glasses and in-depth information. The eye tracking module obtains the pupil diameter, pupil position, pupil center coordinates and other relevant data through the reflection and transmission of a half mirror lens (6). USE - Artificial intelligence liquid glass for AR display device. ADVANTAGE - The drive module drives according to the data, so that the radius of curvature of the lens in the integrated lens screen is changed, thus, realizing smart zoom and clearly imaging on the retina. The liquid glass has a myopia and hyperopia correction mode, a telephoto mode, and an AR adaptive adjustment mode. The liquid glass has analog three-dimensional (3D) glasses function, translation function, and navigation function on the basis of the real-time display. DESCRIPTION Of DRAWING(S) - The drawing shows a top view of the structure of an artificial intelligence liquid glass.Eye tracking module (1)Data processing module (2)Integrated lens screen (5)Half mirror lens (6)Reality capture camera (7)",,,
10.1109/WACV48630.2021.00371,2021,"Chen, Jingjing; Zhang, Jichao; Sangineto, Enver; Chen, Tao; Fan, Jiayuan; Sebe, Nicu",Coarse-to-Fine Gaze Redirection with Numerical and Pictorial Guidance,"Gaze redirection aims at manipulating the gaze of a given face image with respect to a desired direction (i.e., a reference angle) and it can be applied to many real life scenarios, such as video-conferencing or taking group photos. However, previous work on this topic mainly suffers of two limitations: (1) Low-quality image generation and (2) Low redirection precision. In this paper, we propose to alleviate these problems by means of a novel gaze redirection framework which exploits both a numerical and a pictorial direction guidance, jointly with a coarse-to-fine learning strategy. Specifically, the coarse branch learns the spatial transformation which warps input image according to desired gaze. On the other hand, the fine-grained branch consists of a generator network with conditional residual image learning and a multi-task discriminator. This second branch reduces the gap between the previously warped image and the ground-truth image and recovers finer texture details. Moreover, we propose a numerical and pictorial guidance module (NPG) which uses a pictorial gazemap description and numerical angles as an extra guide to further improve the precision of gaze redirection. Extensive experiments on a benchmark dataset show that the proposed method outperforms the state-of-the-art approaches in terms of both image quality and redirection precision. The code is available at https://github.com/jingjingchen777/CFGR",,,
10.1038/s41467-019-13285-0,2019,"Lio, Guillaume; Fadda, Roberta; Doneddu, Giuseppe; Duhamel, Jean-Rene; Sirigu, Angela",Digit-tracking as a new tactile interface for visual perception analysis,"Eye-tracking is a valuable tool in cognitive science for measuring how visual processing resources are allocated during scene exploration. However, eye-tracking technology is largely confined to laboratory-based settings, making it difficult to apply to large-scale studies. Here, we introduce a biologically-inspired solution that involves presenting, on a touch-sensitive interface, a Gaussian-blurred image that is locally unblurred by sliding a finger over the display. Thus, the user's finger movements provide a proxy for their eye movements and attention. We validated the method by showing strong correlations between attention maps obtained using finger-tracking vs. conventional optical eye-tracking. Using neural networks trained to predict empirically-derived attention maps, we established that identical high-level features hierarchically drive explorations with either method. Finally, the diagnostic value of digit-tracking was tested in autistic and brain-damaged patients. Rapid yet robust measures afforded by this method open the way to large scale applications in research and clinical settings.",,,
,2020,"Kaur, Harsimran; Manduchi, Roberto","EyeGAN: Gaze-Preserving, Mask-Mediated Eye Image Synthesis","Automatic synthesis of realistic eye images with prescribed gaze direction is important for multiple application domains. We introduce EyeGAN, an algorithm to generate eye images in the style of a desired target domain, that inherit annotations available in images from a source domain. EyeGAN takes in input ternary masks, which are used as domain-independent proxies for gaze direction. We evaluate EyeGAN against competing eye image synthesis algorithms by measuring a specific gaze consistency index. In addition, we present results from multiple experiments (involving eye region segmentation, pupil localization, and gaze direction estimation) showing that the use of EyeGAN-generated images with inherited annotations for network training leads to superior performances compared to other domain transfer algorithms.",,,
,2020,"Dias, Philipe A.; Malafronte, Damiano; Medeiros, Henry; Odone, Francesca",Gaze Estimation for Assisted Living Environments,"Effective assisted living environments must be able to perform inferences on how their occupants interact with one another as well as with surrounding objects. To accomplish this goal using a vision-based automated approach, multiple tasks such as pose estimation, object segmentation and gaze estimation must be addressed. Gaze direction provides some of the strongest indications of how a person interacts with the environment. In this paper, we propose a simple neural network regressor that estimates the gaze direction of individuals in a multi-camera assisted living scenario, relying only on the relative positions of facial keypoints collected from a single pose estimation model. To handle cases of keypoint occlusion, our model exploits a novel confidence gated unit in its input layer. In addition to the gaze direction, our model also outputs an estimation of its own prediction uncertainty. Experimental results on a public benchmark demonstrate that our approach performs on par with a complex, dataset-specific baseline, while its uncertainty predictions are highly correlated to the actual angular error of corresponding estimations. Finally, experiments on images from a real assisted living environment demonstrate that our model has a higher suitability for its final application.",,,
10.1007/978-3-030-31635-8_230,2020,"Adama, V. Sophie; Schindler, Benjamin; Schmid, Thomas",Using Time Domain and Pearson's Correlation to Predict Attention Focus in Autistic Spectrum Disorder from EEG P300 Components,"Patients with Autistic Spectrum Discorder are known to have deficits in interpreting others' intentions from gaze-direction or other social attention. Here, we use electroencephalography data recorded in virtual reality experiments with patients to predict one out of eight objects that was focused on. Correct labels for these objects were known from parallel eye-tracking measurements. We extracted features from the time domain and from Pearson's correlation and applied both statistical and neuro-inspired supervised machine learning algorithms. Using a multi-layer perceptron, we achieved 65.4% accuracy on the validation data set and 70.0% accuracy on the test data set.",,,
10.1016/j.optcom.2019.124714,2020,"Chen, Mingjun; Li, Xiaoke; Zhang, Hantao; Li, Kunyang; Liang, Weitang; Dai, Hanhong; Liang, Haowen; Zhou, Jianying",Quantitative measurement and reduction of flicker in directional backlight autostereoscopic displays,"The flicker in directional backlight autostereoscopic displays is studied and characterized. Two types of flicker, resulting from instant fluctuation and accumulated fluctuation, are recorded and analyzed. The introduced characterization algorithm is shown to present good agreement with the practical visual experience. Additionally, flicker reduction based on the evaluation scenario, utilizing an adaptive optimizing algorithm, in combination with retina motion prediction, is proposed. The measurement and control scenarios are shown to be highly effective to achieve a flicker-free autostereoscopic display.",,,
,,CHO Y; YOUN J; JOUNG M; KIM S,"Display device for vehicle, has gesture sensor sensing three-dimensional gesture of object through seat beams outputted to first direction and second direction, where gesture sensor is arranged in side of display main part","NOVELTY - The device (100) has a processor for providing a control signal according to three-dimensional (3D) gesture. A gesture sensor senses 3D gesture of an object through seat beams outputted to a first direction and a second direction, where the second direction is different from the first direction. The gesture sensor is arranged in a side of a display main part. A photo receiver receives one of the reflect beams reflected to the object. An optical output unit produces the other sheet beam reflected to the object, and is provided with multiple infrared light sources. USE - Display device for a vehicle (claimed) i.e. car. ADVANTAGE - The device increases correct recognition rate by recognizing the 3D gesture using the sheet beams. The sheet beams can cover a display plane entire region so as to avoid a blind spot of 3D gesture recognition. The device is convenient to use by providing different human machine interface according to the 3D gesture recognition. The device maintains eye gaze in front of the operator during operation of a vehicle, so that driving safety of the operator can be improved. DESCRIPTION Of DRAWING(S) - The drawing shows an enlarged perspective view of a vehicle. '(Drawing includes non-English language text)'Vehicle (1)Wheel (3FR, 3FL, 3RL)Steering input unit (21a)Display device (100)",,,
10.1117/12.2512539,2019,"Malla, Suneeta; Krupinski, Elizabeth; Mello-Thoms, Claudia",Missed cancer and visual search of mammograms: what feature based machine-learning can tell us that deep-convolution learning cannot,"Significant amount of effort has been invested in improving the quality of breast imaging modalities (for example, mammography) to increase the accuracy of breast cancer detection. Despite that, about 4-34% of cancers are still missed during mammographic examination of cancer of the breast [1]. This indicates the need to explore a) The features of the lesions that are missed, and b) Whether the features of missed cancers contribute to why some cancers are not 'looked at' (search error) whereas others are 'looked at' but still not reported. In this visual search study, we perform feature analysis of all lesions that were missed by at least one participating radiologist. We focus on features extracted by means of Grey Level Co-occurrence Matrix properties, textural properties using Gabor filters, statistical information extraction using 2nd and higher-order (3rd and 4th) spectral analysis and also spatial-temporal attributes of radiologists' visual search behaviour. We perform Analysis of Variance (ANOVA) on these features to explore the differences in features for cancers that were missed due to a) search, b) perception and c) decision making errors.Using these features, we trained Support Vector Machine, Gradient Boosting and stochastic gradient decent classifiers to determine the type of missed cancer (search, perception and decision making) We compared these feature-based models with a model trained using deep convolution neural network that learns features by itself. We determined whether deep learning or traditional machine learning performs best in this task.",,,
,,TOMORI Z; ZOLTAN T,"Method for interactive quantification of digitized three dimensional objects using eye tracking camera, involves analyzing particle by algorithm to verify property of marked particle in space","NOVELTY - The method involves selecting last level by gaze of operator from sequence of probe levels and focusing gaze on last level on which particle is visible. The level identification is confirmed by gaze fixation for certain period of time or by voice command or by release of mouse button pressed in phase or by eyewink. The particle is analyzed by algorithm to verify property of marked particle in three-dimensional (3D) space. The particle is marked with color mark, while mark position on levels between marked levels is determined by interpolation or finding representative point by analyzing data. USE - Method for interactive quantification of digitized three dimensional (3D) objects using eye tracking camera in combination with selective projection of series of sections into plane used in 3D analysis of microscopic images in biomedicine, when evaluating images from confocal microscope or from computer tomograph. ADVANTAGE - The camera tracks the operator's eyes and based on tracking identifies the coordinates of the observed screen space, thus the operator performs some operations much faster, only by gaze instead of by clicking with the mouse. The possibility of scaling and rotation of the object allows a detailed verification of correctness of mark assignment. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view illustrating combined interaction mouse/gaze.",,,
10.1145/3299874.3319492,2019,"Li, Hongjia; Liu, Ning; Ma, Xiaolong; Lin, Sheng; Ye, Shaokai; Zhang, Tianyun; Lin, Xue; Xu, Wenyao; Wang, Yanzhi",ADMM-based Weight Pruning for Real-Time Deep Learning Acceleration on Mobile Devices,"Deep learning solutions are being increasingly deployed in mobile applications, at least for the inference phase. Due to the large model size and computational requirements, model compression for deep neural networks (DNNs) becomes necessary, especially considering the real-time requirement in embedded systems. In this paper, we extend the prior work on systematic DNN weight pruning using ADMM (Alternating Direction Method of Multipliers). We integrate ADMM regularization with masked mapping/retraining, thereby guaranteeing solution feasibility and providing high solution quality. Besides superior performance on representative DNN benchmarks (e.g., AlexNet, ResNet), we focus on two new applications facial emotion detection and eye tracking, and develop a top-down framework of DNN training, model compression, and acceleration in mobile devices. Experimental results show that with negligible accuracy degradation, the proposed method can achieve significant storage/memory reduction and speedup in mobile devices.",,,
10.1145/3380877,2020,"Parikh, Saurin S.; Kalva, Hari",Feature Weighted Linguistics Classifier for Predicting Learning Difficulty Using Eye Tracking,"This article presents a new approach to predict learning difficulty in applications such as e-learning using eye movement and pupil response. We have developed 12 eye response features based on psycholinguistics, contextual information processing, anticipatory behavior analysis, recurrence fixation analysis, and pupillary response. A key aspect of the proposed approach is the temporal analysis of the feature response to the same concept. Results show that variations in eye response to the same concept over time are indicative of learning difficulty. A Feature Weighted Linguistics Classifier (FWLC) was developed to predict learning difficulty in real time. The proposed approach predicts learning difficulty with an accuracy of 90%.",,,
,,TRAN H; TRAN B,"Hearing system for e.g. hearing assistance, surveillance applications has microphone arrays that are coupled to eye tracking module and pick up sound from selected one or more microphones covering sound region of interest","NOVELTY - The hearing system (200) has a microphone array system (204) that includes microphone arrays (232-238) spaced apart positioned in regions in a room and each provided with at least three individual microphones, an eye tracking module (202) that detects a person in a sound region of interest within a room, and a processor that selects one or more microphones from the microphone arrays covering the sound region of interest. The microphone arrays are coupled to the eye tracking module and pick up sound from the selected one or more microphones covering the sound region of interest. One or more amplifiers are wirelessly coupled to each of the microphone arrays and to render sound from the sound region of interest detected by the selected one or more microphones to the person's ears to improve person to person communication. USE - Hearing system for e.g. hearing assistance, surveillance, or virtual/augmented reality applications to enhance audio communications with computers or humans. ADVANTAGE - The communication signals between external microphone array and the hearing aid include only separated sound signals that are emphasized based on location/context, thus improving sound at the user's ears while reducing a communication capacity. The set of machine learning algorithms enable the processor to learn from the previous choices of theme for an activity selected by the user and thus automatically selects themes based on the activity of user. Reduces computation since dynamic programming of a given portion of heart sound against most words produces poor dynamic programming scores rather quickly, enabling most words to be pruned after only a small percent of their comparison has been performed. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of the audio system.Hearing system (200)Eye tracking module (202)Microphone array system (204)Microphone arrays (232-238)",,,
10.3390/s18020458,2018,"Jimenez-Molina, Angel; Retamal, Cristian; Lira, Hernan",Using Psychophysiological Sensors to Assess Mental Workload During Web Browsing,"Knowledge of the mental workload induced by a Web page is essential for improving users' browsing experience. However, continuously assessing the mental workload during a browsing task is challenging. To address this issue, this paper leverages the correlation between stimuli and physiological responses, which are measured with high-frequency, non-invasive psychophysiological sensors during very short span windows. An experiment was conducted to identify levels of mental workload through the analysis of pupil dilation measured by an eye-tracking sensor. In addition, a method was developed to classify mental workload by appropriately combining different signals (electrodermal activity (EDA), electrocardiogram, photoplethysmo-graphy (PPG), electroencephalogram (EEG), temperature and pupil dilation) obtained with non-invasive psychophysiological sensors. The results show that the Web browsing task involves four levels of mental workload. Also, by combining all the sensors, the efficiency of the classification reaches 93.7%.",,,
10.1016/j.autcon.2018.05.006,2018,"Jeelani, Idris; Han, Kevin; Albert, Alex",Automating and scaling personalized safety training using eye-tracking data,"Research has shown that a large proportion of hazards remain unrecognized, which expose construction workers to unanticipated safety risks. Recent studies have also found that a strong correlation exists between viewing patterns of workers, captured using eye-tracking devices, and their hazard recognition performance. Therefore, it is important to analyze the viewing patterns of workers to gain a better understanding of their hazard recognition performance. From the training standpoint, scan paths and attention maps, generated using eye tracking technology, can be used effectively to provide personalized and focused feedback to workers. Such feedback is used to communicate the search process deficiency to workers in order to trigger self-reflection and subsequently improve their hazard recognition performance. This paper proposes a computer vision-based method that tracks workers on a construction site and automatically locates their fixation points, collected using a wearable eye-tracker, on a 3D point cloud. This data is then used to analyze their viewing behavior and compute their attention distribution. The presented case studies validate the proposed method.",,,
,,LIN Y,"Eye tracking method used in electronic device e.g. head mount display (HMD) for detecting gaze direction of user, involves calculating viewpoint of eye according to gaze vector based on eye model, and tracking motion of eye","NOVELTY - The method (900) involves capturing (S2) an image of an eye by a camera. A pupil region of interest is detected (S3) in the image of the eye by a processing circuit. The pupil region of interest is analyzed (S4) to obtain a gaze vector of the pupil region of interest by the processing circuit. A viewpoint of the eye is calculated (S5) according to the gaze vector based on an eye model by the processing circuit. The eye model comprises a matrix indicating relationship between the viewpoint of the eye and the gaze vector of the pupil region of interest. A motion of the eye is tracked (S6) based on the viewpoint calculated using the eye model by the processing circuit. USE - Eye tracking method in electronic device (claimed) e.g. HMD such as standalone HMD and VIVE HMD (RTM: virtual reality headset) used in virtual reality (VR)/mixed reality (MR)/augmented reality (AR) system for detecting gaze direction of user. Can also be used in various applications such as virtual reality (VR) or augmented reality (AR) application, and VR/AR system to trace user's gazing direction to provide corresponding reaction and/or control in VR/AR environment. ADVANTAGE - The image filtering can be performed to modify or enhance the image of the eye to emphasize the pupil features and/or remove other features, and reduce or eliminate the noise of the image. The accuracy of eye tracking is improved, since the calibration of the eye model is performed in operation to meet one or more users' specific pupil shape. The processing circuit can control the video and/or audio content displayed by the display unit based on the gaze direction of the user, in order to provide a smooth VR experience and/or a friendly user interactive interface. The head position calibration can be performed to optimize the eye model to increase the accuracy of the eye tracking. The processing circuit can be configured to optimize the eye model to minimize a cost function according to the error values. The various machine learning methods can be applied to minimize the cost of errors between the estimated pupil region and the pupil region of interest to optimize the eye model. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an electronic device; and(2) a non-transitory computer readable storage medium storing eye tracking programs. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the eye tracking method.Eye tracking method (900)Step for capturing the image of an eye (S2)Step for detecting the pupil region of interest (S3)Step for analyzing the pupil region of interest (S4)Step for calculating a viewpoint of the eye (S5)Step for tracking the motion of the eye (S6)",,,
10.1080/10447318.2021.1976509,2021,"Souchet, Alexis D.; Philippe, Stephanie; Lourdeaux, Domitile; Leroy, Laure",Measuring Visual Fatigue and Cognitive Load via Eye Tracking while Learning with Virtual Reality Head-Mounted Displays: A Review,"Virtual Reality Head-Mounted Displays (HMDs) reached the consumer market and are used for learning purposes. Risks regarding visual fatigue and high cognitive load arise while using HMDs. These risks could impact learning efficiency. Visual fatigue and cognitive load can be measured with eye tracking, a technique that is progressively implemented in HMDs. Thus, we investigate how to assess visual fatigue and cognitive load via eye tracking. We conducted this review based on five research questions. We first described visual fatigue and possible cognitive overload while learning with HMDs. The review indicates that visual fatigue can be measured with blinks and cognitive load with pupil diameter based on thirty-seven included papers. Yet, distinguishing visual fatigue from cognitive load with such measures is challenging due to possible links between them. Despite measure interpretation issues, eye tracking is promising for live assessment. More researches are needed to make data interpretation more robust and document human factor risks when learning with HMDs.",,,
10.18494/SAM.2020.2621,2020,"Shimoda, Koichi; Tanabe, Shun; Tobe, Yoshito",Investigation of Relationship Between Eye Gaze and Brain Waves towards Smart Sensing for E-learning,"In recent years, we have witnessed a new trend of learning called e-learning where learners can take courses using electronic devices with Internet connection. Although e-learning is convenient because it removes temporal and spatial limitations, it is difficult to know whether the learner is really paying attention to the learning materials. To address this problem, we tried to use electroencephalography (EEG) to investigate a learner's concentration in our previous study. However, our previous study relied on subjective evaluation, and there was no objective observation to relate the EEG signals to the learner's concentration. Given this background, we compared eye gaze and EEG results to find the appropriate position and frequency band of EEG in e-learning in this study. We compared the EEG result obtained during a period when the subjects were watching a video lecture and that obtained during a period when the subjects were not watching, and determined that the viewing state could be predicted from EEG logistic regression and a support vector machine (SVM). The results suggested that measuring beta and gamma waves and examining the parietal and occipital regions are both effective.",,,
,,RUSSELL A; HUIBERS A,"Device for video calling and video conferencing systems used in computer display systems, comprises a display, which has a curved auto stereoscopic panel, that is including a first subset of pixels and a second subset of pixels","NOVELTY - The device comprises a display (102), which has a curved auto stereoscopic panel, that is including a first subset of pixels and a second subset of pixels. An eye-tracking module to track a left-eye position and a right-eye position of a first user (105). An image capture device (110) to capture the image of the first user. A network interface to receive three-dimensional (3D) content from the first remote device. A processor (100) is used to generate the left-eye frame and the right-eye frame, which is based on the received 3D content of the left-eye position, and the right eye position. The left-eye frame, which displays at the first subset of pixels and the right-eye frame, which displays at the second subset of pixels. The 3D model of the first user is generated by selecting a pre-defined 3D model that most closely matches the captured image of the first user from the multiple pre-defined 3D models of the first user and provide the generated 3D model to the network interface. USE - Device for video calling and video conferencing systems used in computer display systems. ADVANTAGE - The device provides the video-based communication system, which supports the visual transfer of information that is allowing pictures, graphs, tables, and other imagery to efficiently communicated between users, and the workstation provides an immersive and responsive video environment to the user, which is enhancing the user experience with video-based communication. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for video calling and video conferencing systems used in computer display systems; and(2) a non-transitory readable storage medium for storing instructions used in computer display systems. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of the device.Processor (100)Display (102)User (105)Eye tracking module (108)Image capture device (110)",,,
10.1109/ICCVW.2019.00454,2019,"Damer, Naser; Boutros, Fadi; Kirchbuchner, Florian; Kuijper, Arjan",D-ID-Net: Two-Stage Domain and Identity Learning for Identity-Preserving Image Generation From Semantic Segmentation,"Training functionality-demanding AR/VR systems require accurate and robust gaze estimation and tracking solutions. Achieving such a performance requires the availability of diverse eye image data that might only be acquired by the means of image generation. Works addressing the generation of such images did not target realistic and identity-specific images, nor did they address the practical-relevant case of generation from semantic labels. Therefore, this work proposes a solution to generate realistic and identity-specific images that correspond to semantic labels, given samples of a specific identity. Our proposed solution consists of two stages. In the first stage, a network is trained to transform the semantic label into a corresponding eye image of a generic identity. The second stage is an identity-specific network that induces identity details on the generic eye image. The results of our D-ID-Net solutions shows a high degree of identity-preservation and similarity to the ground-truth images, with an RMSE of 7.235.",,,
10.1145/3283254.3283271,2018,"Bates, Tamas; Kober, Jens; Gienger, Michael",Head-tracked off-axis perspective projection improves gaze readability of 3D virtual avatars,"Virtual avatars have been employed in many contexts, from simple conversational agents to communicating the internal state and intentions of large robots when interacting with humans. Rarely, however, are they employed in scenarios which require non-verbal communication of spatial information or dynamic interaction from a variety of perspectives. When presented on a flat screen, many illusions and visual artifacts interfere with such applications, which leads to a strong preference for physically-actuated heads and faces.By adjusting the perspective projection used to render 3D avatars to match a viewer's physical perspective, they could provide a useful middle ground between typical 2D/3D avatar representations, which are often ambiguous in their spatial relationships, and physically-actuated heads/faces, which can be difficult to construct or impractical to use in some environments. A user study was conducted to determine to what extent a head-tracked perspective projection scheme was able to mitigate the issues in readability of a 3D avatar's expression or gaze target compared to use of a standard perspective projection. To the authors' knowledge, this is the first user study to perform such a comparison, and the results show not only an overall improvement in viewers' accuracy when attempting to follow the avatar's gaze, but a reduction in spatial biases in predictions made from oblique viewing angles.",,,
,,SULLIVAN M A; PRATT J H; STETTLER G L,"System for adjusting the display orientation based on user orientation, comprises memory that stores instructions, and processor executes instructions to perform operations, where operations involves receiving data pertaining to user","NOVELTY - The system comprises a memory that stores instructions. A processor executes the instructions to perform operations, where the operations involves receiving data pertaining to a user and a device (110), the data including an eye glaze plane of the user relative to the device and a position of a body portion of the user relative to a reference point. A content (112) is displayed in orientation associated with content displayed on the device is being adjusted. The operations further comprise determining whether the user has moved a threshold amount to cause a mismatch between the content display orientation and the eye gaze plane. The operations further comprise readjusting the content display orientation if the user has moved the threshold amount to cause the mismatch. USE - System for adjusting the display orientation based on user orientation. ADVANTAGE - The adjusting system effectively align the content displayed on the device, thus y allow for highly accurate and reliable for 3-D scenes, while minimizing distortions and errors. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for following:(1) a method for adjusting the display orientation based on user orientation; and(2) a non-transitory computer-readable device for adjusting the display orientation based on user orientation. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a tilted position that is displaying media content that is oriented based on the user's eye gaze plane and nose.Device (110)Content (112)Camera (115)Line (129)",,,
,,HYUN K A; SUK H L,"Method for modeling and recognizing driving action of automobile operator, involves dividing collected driving action data according to driving action of operator, and expressing divided data in grammar for recognizing driving action","NOVELTY - The method involves setting division of territory when an operator is stared. Driving action data is collected. The collected driving action data is divided according to driving action of the operator. The divided data is expressed in a grammar for recognizing the driving action. The driving action is recognized as an independent test data to measure and evaluate effect of a machine learning result (S21). Head position of the operator and coordinate of hollow are included in operator data. Environment, location and vehicle condition information are included in external data. USE - Method for modeling and recognizing driving action of a vehicle i.e. automobile, operator. ADVANTAGE - The method enables analyzing a state of a vehicle by utilizing an eye tracker with eye gaze data of a driving action of the operator, and determining temporal variation of the peripheral situation data so as to recognize the driving action of the analyzed operator by providing a warning to the operator, easily sequence analysis about the driving action to grading for Glasgow coma scale modeling and provide the result for the operator in a various mode. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for modeling and recognizing driving action of a vehicle operator. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for modeling and recognizing driving action of vehicle operator. '(Drawing includes non-English language text)'Step for recognizing driving action as independent test data to measure and evaluate effect of machine learning result (S21)Step for determining whether driving action of operator is safe driving action or not (S212)",,,
,,ORTIZ E S; GAO J F; LUNARDHI A D; BULUSU V S R; ORTIZ EGEA S,Near-Eye-Display system for sequentially presenting multiple of virtual stimuli to user in random or pseudo-random fashion comprises at least one display element configured to render computer generated images within a field of view of user,"NOVELTY - The Near-Eye-Display (NED) system comprises at least one display element configured to render computer generated images within a field of view of a user, at least one sensor (102) configured to generate eye tracking data associated with at least one eye of the user. The controller that is communicatively coupled to at least one display element and the at least one sensor, the controller is configured to perform a user-specific calibration protocol and a user-specific eye tracking protocol. The user-specific calibration protocol having operations of, causing at least one display element to sequentially present multiple of virtual stimuli. USE - Near-Eye-Display system for sequentially presenting multiple of virtual stimuli to a user in a random or pseudo-random fashion. ADVANTAGE - Highly sensitive and accurate in the detection of eye movements (e.g., the systems are sensitive enough to even accurately track saccadic eye movements). DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following :(1) computer-implemented method and(2) a eye tracking system. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of the system.Sensor (102)Pupil (106)Irises (108)3d Propagation (202)Corresponding Ellipse (204)",,,
,,DOUGLAS R E,"Method for performing precision localization within body on cross-sectional imaging examination, involves displaying digital object at corresponding main coordinate in image of sub three-dimensional dataset, and displaying imaging slice","NOVELTY - The method involves loading (400) a first 3D imaging dataset into an image processing workstation. A second 3D imaging dataset is loaded (401) into the image processing workstation, where the second 3D dataset comprises a voxelated dataset of the scanned volume at a second time point. A smart localization system is performed (402) and comprising determining a first coordinate of an image of a first 3D dataset, where the first coordinate is located at a sub-structure location by positioning a cursor, and utilizing an eye tracking system, and determining a corresponding first coordinate in a second 3D dataset, where the corresponding first coordinate is enclosed within the structure. A digital object is displayed (403) at the corresponding first coordinate in an image of the second 3D dataset, and an imaging slice of the second 3D dataset containing the sub-structure is displayed. USE - Method for performing precision localization within body on cross-sectional imaging examination used in medicine industry, military industry, and video games industry. ADVANTAGE - (1) The organ-based coordinate system can be established to improve localization by defining precise coordinates within an organ.(2) The augmented reality headset can be geo-registered with the organ-specific coordinate system, which enables precision localization of a small lesion within the liver.(3) The system can perform precision markup of lesions, enabling precision tracking of lesions and improved communication between radiologists. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) non-transitory computer readable storage medium storing for performing precision localization within body on cross-sectional imaging examination; and(2) apparatus for performing precision localization within body on cross-sectional imaging examination. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for performing precision localization within body on cross-sectional imaging examination.Step for loading a first 3D imaging dataset into an image processing workstation (400)Step for loading a second 3D imaging dataset into the image processing workstation (401)Step for performing smart localization system and comprising determining a first coordinate of an image of a first 3D dataset, where the first coordinate is located at a sub-structure location by positioning a cursor, and utilizing an eye tracking system (402)Step for displyaing a digital object at the corresponding first coordinate in an image of the second 3D dataset, and displaying an imaging slice of the second 3D dataset containing the sub-structure. (403)",,,
10.1002/lary.27447,2019,"Liao, David; Ishii, Masaru; Darrach, Halley M.; Bater, Kristin L.; Smith, Jane; Joseph, Andrew W.; Douglas, Raymond S.; Joseph, Shannon S.; Ishii, Lisa E.",Objectively Measuring Observer Attention in Severe Thyroid-Associated Orbitopathy: A 3D Study,"Objective Measure the attentional distraction of facial deformity related to severe thyroid-associated orbitopathy using three-dimensional (3D) images and eye-tracking technology. Methods Observers recruited at an academic tertiary referral center viewed 3D facial images of patients with severe thyroid-associated orbitopathy (TAO) and controls without TAO. An infrared eye-tracking monitor recorded their eye movements and fixations in real time. Multivariate Hotelling's analysis, followed by planned posthypothesis testing, was used to compare fixation durations for predefined regions of interest, including the eyes, nose, mouth, central triangle, and remaining face without the central triangle between severe TAO patients and controls. Results One hundred sixteen observers (mean age 26.4 years, 51% female) successfully completed the eye-tracking experiment. The majority of their attention was directed toward the central triangle (eyes, nose, mouth). On multivariate analysis, there were significant differences in the distribution of attention between control and severe TAO faces (T-2 = 49.37; F(5,922) = 9.8314, P < 0.0001). On planned posthypothesis testing, observers attended significantly more to the eyes (0.77 seconds, P < 0.0001, 95% confidence interval [CI], 0.51, 1.03 seconds) and less to the nose (-0.42 seconds, P < 0.0001, 95% CI, -0.23, -0.62 seconds) in severe TAO patients. There was no significant difference in time spent on the mouth, the total time spent on the central triangle, or time spent in the remaining face between the two groups. Conclusion Severe TAO distracted observer attention toward the eyes compared to control patients. These data lend insight into how TAO may alter observers' perceptions of these patients. Future studies should investigate how these changes in observer gaze patterns may reflect the social perception of TAO patients.",,,
,,SPOTTISWOODE B S; SIBILLE L; SIMCIC V,"Method for generating image findings, involves generating anatomical association for main image based on main classification and clinical characterization, and providing anatomical association for display","NOVELTY - The method involves receiving speech data (103) from a dictation system (102). A clinical characterization is determined based on the received speech data. The first image scan data for a first image is received. A feature extraction process is applied to the received first image scan data to identify feature of the first image. A trained machine learning model is applied to the feature of the first image, where the trained machine learning model is trained on historical anatomical associations generated for prior images. A first classification of the feature in the first image is determined based on the application of the trained machine learning model to the feature of the first image. An anatomical association for the first image is generated based on the first classification and the clinical characterization. The anatomical association for display is provided. USE - Method for generating image findings. ADVANTAGE - The annotation of measurements at a high anatomical granularity and/or the generation of structured reports for the providing of clinical staging support is allowed. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following(1) a non-transitory computer readable medium storing program for generating image findings; and(2) a system for generating image findings. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the nuclear imaging system.Dictation system (102)Speech data (103)Eye tracking system (104)Image display system (106)Clinical data (111)",,,
10.1109/ISM.2017.116,2017,"Liao, Wen-Hung; Chang, Chin-Wen; Wu, Yi-Chieh",Classification of Reading Patterns Based on Gaze Information,"Reading is one of the main paths to acquire knowledge, either done traditionally on paper media or practiced on electronic devices. Efficiency varies when different reading patterns are involved. It is the objective of this research to classify reading patterns from fixation data using machine learning techniques in an attempt to understand and evaluate the reading and learning process. In our experiment, a low-cost eye tracker is employed to record the eye movements during the reading process. A dispersion-based algorithm is implemented to identify fixation from the recorded data. Features pertaining to fixation including duration, path length, landing position and fixation direction are extracted for classification purposes.Five categories of reading pattern have been defined and investigated in this study, namely, speed reading, slow reading, in-depth reading, skim-and-skip, and keyword spotting. We have recruited thirty subjects to participate in our experiment. The participants are instructed to read different articles using specific styles designated by the experimenter in order to assign label to the collected data. Feature selection is achieved by analyzing the predictive results of cross-validation from the training data obtained from all subjects. The average classification accuracies in five random tests are 78.24%, 74.19%, 93.75%, 87.96%, and 96.20% respectively. Further improvements are accomplished by introducing an additional undecided class to address ambiguous reading patterns.",,,
,,YIN E; XIE L; DENG X; LIU X; DENG B; YAN Y,"Augmented reality based electroencephalogram auxiliary spectacle eye interaction self-calibration method, involves correcting fixation point by using electroencephalogram auxiliary module to realize blink parameter self-calibration","NOVELTY - The method involves collecting three-dimensional (3D) fixation point information in an eye picture. A depth neural network model is established corresponding to an augmented reality glass. Mapping relation between the eye picture and a gaze area is determined. A fixation point is corrected by using an electroencephalogram auxiliary module in a watch area to realize blink parameter self-calibration. A gaze point is generated. The eye picture and gazing point information are synchronously stored. Different batch samples are collected in different wearing modes. The eye picture is collected by using a camera e.g. gray scale camera, color camera and infrared camera. USE - Augmented reality based electroencephalogram auxiliary spectacle eye interaction self-calibration method. ADVANTAGE - The method enables accurately predicting eye gaze 3D point information, adjusting glasses wearing mode, realizing self-calibration of glasses eye interaction by using augmented reality in a convenient manner. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an augmented reality based electroencephalogram auxiliary spectacle eye interaction self-calibration system. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating an augmented reality based electroencephalogram auxiliary spectacle eye interaction self-calibration method. '(Drawing includes non-English language text)'",,,
10.1007/978-3-030-31635-8_110,2020,"D'Addio, Giovanni; Ricciardi, Carlo; Improta, Giovanni; Bifulco, Paolo; Cesarelli, Mario",Feasibility of Machine Learning in Predicting Features Related to Congenital Nystagmus,"Congenital nystagmus is an ocular-motor disease affecting people's visual acuity since their first years of life. Electrooculography is used to perform eye tracking in these patients, giving the possibility to extract a wide variety of parameters. The relationships among all these variables were analysed in the past and the aim of this paper is to perform a new analysis employing more recent techniques, those of machine learning. The electrooculography of 20 patients was recorded, signals were pre-processed, and some parameters were extracted through a custom-made software. Knime analytics platform was chosen in order to build predictive models using Random Forests and Logistic Regression Tree algorithms and some evaluation metrics were computed. The visual acuity and the variability of eye positioning were predicted employing five and six variables, respectively. In terms of coefficient of determination, visual acuity had values over 0.72 and variability of eye positioning over 0.70. Compared to the results obtained without machine learning algorithms during the past years, these values become more valuable. In conclusion, this approach showed its feasibility in detecting relationships among variables related to congenital nystagmus; it could be tested in order to find new and stronger relationships among these variables and be of support for clinicians.",,,
10.1016/j.patcog.2018.11.013,2019,"Gonzalez-Diaz, Ivan; Benois-Pineau, Jenny; Domenger, Jean-Philippe; Cattaert, Daniel; de Rugy, Aymar",Perceptually-guided deep neural networks for ego-action prediction: Object grasping,"We tackle the problem of predicting a grasping action in ego-centric video for the assistance to upper limb amputees. Our work is based on paradigms of neuroscience that state that human gaze expresses intention and anticipates actions. In our scenario, human gaze fixations are recorded by a glass-worn eye-tracker and then used to predict the grasping actions. We have studied two aspects of the problem: which object from a given taxonomy will be grasped, and when is the moment to trigger the grasping action. To recognize objects, we using gaze to guide Convolutional Neural Networks (CNN) to focus on an object-to-grasp area. However, the acquired sequence of fixations is noisy due to saccades toward distractors and visual fatigue, and gaze is not always reliably directed toward the object-of-interest. To deal with this challenge, we use video-level annotations indicating the object to be grasped and a weak loss in Deep CNNs. To detect a moment when a person will take an object we take advantage of the predictive power of Long-Short Term Memory networks to analyze gaze and visual dynamics. Results show that our method achieves better performance than other approaches on a real-life dataset. (C) 2018 Elsevier Ltd. All rights reserved.",,,
,2017,"Taran, Vladyslav; Alienin, Oleg; Stirenko, Sergii; Gordienko, Yuri; Rojbi, A.",Performance Evaluation of Distributed Computing Environments with Hadoop and Spark Frameworks,"Recently, due to rapid development of information and communication technologies, the data are created and consumed in the avalanche way. Distributed computing create preconditions for analyzing and processing such Big Data by distributing the computations among a number of compute nodes. In this work, performance of distributed computing environments on the basis of Hadoop and Spark frameworks is estimated for real and virtual versions of clusters. As a test task, we chose the classic use case of word counting in texts of various sizes. It was found that the running times grow very fast with the dataset size and faster than a power function even. As to the real and virtual versions of cluster implementations, this tendency is the similar for both Hadoop and Spark frameworks. Moreover, speedup values decrease significantly with the growth of dataset size, especially for virtual version of cluster configuration. The problem of growing data generated by IoT and multimodal (visual, sound, tactile, neuro and brain-computing, muscle and eye tracking, etc.) interaction channels is presented. In the context of this problem, the current observations as to the running times and speedup on Hadoop and Spark frameworks in real and virtual cluster configurations can be very useful for the proper scaling-up and efficient job management, especially for machine learning and Deep Learning applications, where Big Data are widely present.",,,
,,RODRIGUES DO NASCIMENTO G; DELOU C M; CAVALCANTI D N; CRESPO COELHO DA SILVA PI,"Computerized kit for early diagnosis of autism in infants from ocular tracking, has tablet comprising keyboard, webcam and software program, where processed static and moving image of infants is displayed for eye tracking","NOVELTY - The kit has a tablet comprising a keyboard, a webcam and a software program, where processed static and moving image of infants is displayed for eye tracking and evaluation of abnormal eye movements, and the static and moving images comprise human faces and/or objects for stimulus-response of infants. An auxiliary instrument comprises a software to track and analyze the ocular movements of infants for autism risk, where image processing includes structural analysis, motion analysis and object tracking, pattern recognition, camera calibration and 3D reconstruction. USE - Computerized kit for early diagnosis of autism in infants from ocular tracking. ADVANTAGE - The kit analyzes and tracks specific eye movements and provides feedback of deficit in social interaction in infants between 5 and 18 months of age for early diagnosis of autism. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a computerized kit. '(Drawing includes non-English language text)'",,,
10.1016/j.ress.2018.10.012,2020,"Zhang, Xiaoge; Mahadevan, Sankaran; Lau, Nathan; Weinger, Matthew B.",Multi-source information fusion to assess control room operator performance,"Control room operators respond to abnormal situations through a series of cognitively demanding activities, e.g., monitoring, detection, diagnosis, and response. However, variability among operators in terms of prior experience and current operational context affects their response to the malfunction. A machine learning framework was employed to integrate multiple data sources and develop an empirical model of operator performance in responding to malfunction events. A human-in-the-loop within-subjects experiment was performed using a high-fidelity Generic Pressurized Water Reactor simulator. The study recruited nine licensed operators in three-person crews completing ten scenarios, each incorporating two to four malfunction events. Individual operator performance was assessed using eye tracking technology and physiological recordings of skin conductance response and respiratory function. Expert-rated event management performance was the primary study outcome. These heterogeneous data sources were fused using an approach that integrated a support vector machine with bootstrap aggregation to develop a trained quantitative prediction model. While no single variable predicted operator performance, the fused model's predictions using independent verification data was very good (prediction accuracy of 75-83%). The proposed methodology offers a quantitative approach to evaluate the crew performance through fusing the heterogeneous data collected from experiment.",,,
,,KOO B; KO J; LEE W,"Head-mounted display device for determining a gaze point in real space for obtaining depth information by using electronic device, such as smart phone, comprises an eye tracking circuitry to obtain a direction of left and right eye of user","NOVELTY - The head-mounted display device comprises an eye tracking circuitry provided to obtain a direction of a left eye of a user and a direction of a right eye of the user with a depth sensor (150). Multiple processors (120) are provided to determine a gaze point based on the direction of the left eye of the user and the direction of the right eye of the user. A measurement parameter of the depth sensor is determined based on the gaze point. The depth sensor is provided to obtain depth information about multiple objects based on the determined measurement parameter. The processors obtain two-dimensional (2D) location information of the gaze point based on the direction of the left eye of the user and the direction of the right eye of the user. USE - Head-mounted display device for determining a gaze point in real space for obtaining depth information by using electronic device, such as smart phone, laptop computer, desktop, tablet PC and e-book terminal. ADVANTAGE - The electronic device can perform modeling on a required space with increasing modeling speed and reducing power consumption. The accuracy of the depth information can be improved. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for operating an electronic device; and(2) a non-transitory computer-readable recording medium having stored instructions for implementing the method for operating an electronic device. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of head-mounted display device with electronic device.Processors (120)Depth sensor (150)Eye tracking sensor (160)Light emitter (170)Sensor (180)",,,
,2019,"Xu, Mai; Jiang, Lai; Ding, Zhiguo",Machine-learning-based saliency detection and its video decoding application in wireless multimedia communications,,,,
,,KIM M J; LEE Y H; CHEONG C H,"Electronic device, has processor for producing multiple images corresponding to multiple view vectors related to spot of virtual three dimensional space phase, and communication circuit connected to processor","NOVELTY - The device has a processor (113) for producing multiple images corresponding to multiple view vectors related to a spot of virtual three dimensional (3D) space phase, where quality of an image among the images is set as quality higher than quality of a rest image and the image is synthesized. A communication circuit (112) is functionally connected to the processor, where eye gaze information of a user is obtained corresponding to an outer electronic device through the communication circuit. The communication circuit obtains choice input about a virtual 3D space inner side object. The processor is connected to a sensor (121). USE - Electronic device. Uses include but are not limited to a smartphone, a tablet personal computer (PC), a mobile phone, a video phone, an electronic book reader (e-book reader), a desk top PC, a laptop PC, a net book computer, a work station and a server. ADVANTAGE - The device performs rendering process of multiple images in an efficient manner so as to reduce video data size and turnaround time during image streaming. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an electronic device. '(Drawing includes non-English language text)'Server (110)Communication circuit (112)Processor (113)Sensor (121)Display unit (125)",,,
10.1016/j.patrec.2019.02.008,2019,"Zhao, Tongtong; Yan, Yuxiao; Peng, Jinjia; Mi, Zetian; Fu, Xianping",Guiding intelligent surveillance system by learning-by-synthesis gaze estimation,"We describe a novel learning-by-synthesis method for estimating the gaze direction of an automated intelligent surveillance system. Recently, the progress of integrated learning has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. However, due to the different distributions between the real and synthetic images, the desired performance cannot be achieved from the synthetic image learning as compared to the real images. In order to solve this problem, the previous method was to improve the authenticity of the composite image by learning the model. However, this kind of method had the disadvantage that the distortion was not improved and the level of authenticity was unstable. In order to solve this problem, we propose a new structure to improve the composite image. By referring to the idea of style transformation, we can effectively reduce the distortion of the image and minimize the need for actual data annotation. We estimate that this can produce highly realistic images, which we have demonstrated through qualitative and user research. We quantitatively evaluate the generated images by training the gaze estimation model. We use the refined synthetic dataset to show significant improvements compared with using the raw synthetic dataset. (C) 2019 Elsevier B.V. All rights reserved.",,,
10.1007/s00464-021-08569-w,2021,"Ezzat, Ahmed; Kogkas, Alexandros; Holt, Josephine; Thakkar, Rudrik; Darzi, Ara; Mylonas, George",An eye-tracking based robotic scrub nurse: proof of concept,"Background Within surgery, assistive robotic devices (ARD) have reported improved patient outcomes. ARD can offer the surgical team a third hand to perform wider tasks and more degrees of motion in comparison with conventional laparoscopy. We test an eye-tracking based robotic scrub nurse (RSN) in a simulated operating room based on a novel real-time framework for theatre-wide 3D gaze localization in a mobile fashion. Methods Surgeons performed segmental resection of pig colon and handsewn end-to-end anastomosis while wearing eye-tracking glasses (ETG) assisted by distributed RGB-D motion sensors. To select instruments, surgeons (ST) fixed their gaze on a screen, initiating the RSN to pick up and transfer the item. Comparison was made between the task with the assistance of a human scrub nurse (HSNt) versus the task with the assistance of robotic and human scrub nurse (R&HSNt). Task load (NASA-TLX), technology acceptance (Van der Laan's), metric data on performance and team communication were measured. Results Overall, 10 ST participated. NASA-TLX feedback for ST on HSNt vs R&HSNt usage revealed no significant difference in mental, physical or temporal demands and no change in task performance. ST reported significantly higher frustration score with R&HSNt. Van der Laan's scores showed positive usefulness and satisfaction scores in using the RSN. No significant difference in operating time was observed. Conclusions We report initial findings of our eye-tracking based RSN. This enables mobile, unrestricted hands-free human-robot interaction intra-operatively. Importantly, this platform is deemed non-inferior to HSNt and accepted by ST and HSN test users.",,,
10.1016/j.bpsc.2019.11.015,2020,"Washington, Peter; Park, Natalie; Srivastava, Parishkrita; Voss, Catalin; Kline, Aaron; Varma, Maya; Tariq, Qandeel; Kalantarian, Haik; Schwartz, Jessey; Patnaik, Ritik; Chrisman, Brianna; Stockham, Nathaniel; Paskov, Kelley; Haber, Nick; Wall, Dennis P.",Data-Driven Diagnostics and the Potential of Mobile Artificial Intelligence for Digital Therapeutic Phenotyping in Computational Psychiatry,"Data science and digital technologies have the potential to transform diagnostic classification. Digital technologies enable the collection of big data, and advances in machine learning and artificial intelligence enable scalable, rapid, and automated classification of medical conditions. In this review, we summarize and categorize various data-driven methods for diagnostic classification. In particular, we focus on autism as an example of a challenging disorder due to its highly heterogeneous nature. We begin by describing the frontier of data science methods for the neuropsychiatry of autism. We discuss early signs of autism as defined by existing pen-and-paper-based diagnostic instruments and describe data-driven feature selection techniques for determining the behaviors that are most salient for distinguishing children with autism from neurologically typical children. We then describe data-driven detection techniques, particularly computer vision and eye tracking, that provide a means of quantifying behavioral differences between cases and controls. We also describe methods of preserving the privacy of collected videos and prior efforts of incorporating humans in the diagnostic loop. Finally, we summarize existing digital therapeutic interventions that allow for data capture and longitudinal outcome tracking as the diagnosis moves along a positive trajectory. Digital phenotyping of autism is paving the way for quantitative psychiatry more broadly and will set the stage for more scalable, accessible, and precise diagnostic techniques in the field.",,,
,,KUWAHARA A; VEMBAR D S; SAKTHIVEL C; VENKATARAMAN R; INSKO B E; KALRA A S; LABBE H; APPU A R; SHAH A N; RAY J; OULD-AHMED-VALL E; SURTI P; RAMADOSS M,Non-transitory computer-readable storage medium used for data processing has set of commands executable by computing system to cause first graphics processor to receive eye tracking data,"NOVELTY - The non-transitory computer-readable storage medium has a set of commands executable by a computing system to process a graphics workload with a first graphics processor. The first graphics processor provides a command stream and retrieves vertex data of primitives associated with the graphics workload to determine lighting data associated with the graphics workload. The set of commands causes the first graphics processor to make available a distortion operation of the graphics workload to a second graphics processor and the distortion operation to be processed by the second graphics processor based upon a tracked head movement when executed by the computing system. The set of commands causes the first graphics processor to receive eye tracking data when executed by the computing system. USE - Non-transitory computer-readable storage medium used for data processing and graphics processing. ADVANTAGE - Reduces communication between processor and graphics processing unit (GPU) to ensure that GPU-biased pages are those which are required by GPU but not host processor and vice versa. Consumes less power by performing time warp operations on low precision compute engine instead of more fully featured three-dimensional (3D) pipeline. Provides separate images to left eye and right eye of user to facilitate rendering, generation and/or perception of 3D scenes. DESCRIPTION Of DRAWING(S) - The drawing shows the block diagram of a parallel processor.Parallel processor (200)Parallel processing unit (202)Scheduler (210)Memory interface (218)Parallel processor memory (222)",,,
10.1016/j.neucom.2019.09.085,2020,"Liang, Haoran; Jiang, Ming; Liang, Ronghua; Zhao, Qi",A structure-guided approach to the prediction of natural image saliency,"The structure of a scene provides global contextual information in directing gaze and complements local object information in saliency prediction. In this study, we explore how visual attention can be affected by scene structures, namely openness, depth and perspective. We first build an eye tracking dataset with 2500 natural scene images and collect gaze data via both eye tracking and mouse tracking. We make observations on scene layout properties and propose a set of scene structural features relating to visual attention. The set of complementary features are then integrated for saliency prediction. Our features are independent of and can work together with many computational modules, and this work demonstrates the use of Multiple kernel learning (MKL) as an example to integrate the features at low- and high-levels. Experimental results demonstrate that our model outperforms existing methods and our scene structural features can improve the performance of other saliency models in outdoor scenes. (C) 2019 Elsevier B.V. All rights reserved.",,,
,,LYREN P S,Method for use in wearable electronic glasses (WEG) such as handheld portable electronic devices with eye tracking for displaying virtual image of a real location involves tracking three-dimensional (3D) virtual image displayed on a WEG,"NOVELTY - The method involves tracking with wearable electronic glasses (WEG) worn on a head of a user, and is in the direction of gaze of the user. The three-dimensional (3D) virtual instructions (220) is displayed on a display of the WEG on how to operate an electronic device in response to in detecting the direction of gaze of the user at the electronic device visible through the display of the WEG. Audio instructions is played on how to operate the electronic device in response to detect the direction of gaze of the user is at the electronic device. The playing of the audio instructions how to operate the electronic device is discontinued when the direction of gaze of the user is no longer at the electronic device. The WEG simultaneously displays (230) the 3D virtual instructions and plays the audio instructions while the direction of gaze of the user is at the electronic device. USE - Method for use in wearable electronic glasses (claimed) such as handheld portable electronic devices with eye tracking for displaying a virtual image of a real location. ADVANTAGE - The real object increases the available space by reducing the distance between the virtual object and the real object. When the user moves his head or a position of the wearable electronic device changes, a size of the virtual object, a shape of the virtual object, or a distance between the virtual object and the real object changes to accommodate the movement of the head or wearable electronic device. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a non-transitory computer readable storage medium for the electronic device to execute the method; and(2) a wearable electronic glasses. DESCRIPTION Of DRAWING(S) - The drawing shows a flow-chart of a method for displaying a virtual image of a real location.Capturing images with a wearable device (200)Displaying images on wearable device (210)Receiving instructions for performing tasks (220)Displaying instructions received from remote device (230)",,,
,,HENRIKSON R; GROSSMAN T S; TROWBRIDGE S; BENKO H; WIGDOR D,"Method for predicting future positions and directions of input devices in three-dimensional spaces, involves predicting region of interest within three-dimensional space for user using regression model based on movement parameters","NOVELTY - The method (1100) involves tracking (1106) the movement parameters of multiple input devices of the user for a user performing a target acquisition movement within a 3D space. A region of interest is predicted within the 3D space for the user using a regression model based on the movement parameters, where the region of interest includes multiple targets in close proximity. An endpoint of the target acquisition movement is predicted within the region of interest using a pointer facilitation technique. Multiple input devices is comprised of an eye tracking input device, where each input device corresponds to a predefined input device type, and the movement parameters include gaze data from the eye tracking input device. USE - Method for predicting future positions and directions of input devices in 3D spaces used in electronic device (claimed) used in engineering design, medical surgery practice, military simulated practice, and video gaming. ADVANTAGE - The user tolerates wearing a lighter eyewear device and carrying or wearing the paired device for greater lengths of time than the user would tolerate wearing a heavy standalone eyewear device because weight carried in the neckband can be less invasive to a user than weight carried in the eyewear device, thus enabling an artificial reality environment to be incorporated more fully into a user's day-to-day activities. The battery power, computational resources, and/or additional features of the AR system is are by a paired device or shared between a paired device and an eyewear device, thus reducing the weight, heat profile, and form factor of the eyewear device overall while still retaining desired functionality. The device enables or enhances a user artificial reality experience in contexts and environments and/or in other contexts and environments. The device improves accuracy and effectiveness, while enabling cold start, gesture modelling, and offering a pathway for online personalization and performance improvements. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an electronic device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for predicting future positions and directions of input devices in 3D spaces.Method for predicting future positions and directions of input devices in 3D spaces (1100)Step for selecting a subset of templates from a library of templates (1104)Step for tracking the movement parameters of multiple input devices of the user for a user performing a target acquisition movement within a 3D space (1106)Step for comparing the tracked movement parameters of the one input device of the user to the movement parameters of the subset of the templates selected from the library of templates (1108)Step for predicting a target selection within the 3D space displayed on the display for the user based on comparsion (1110)",,,
,,FROY D,"Electronic gaming machine, has display device supported by housing, and processor for operating with binaural audio system to output 3D audio corresponding to virtual object based on 3D coordinates associated with virtual object","NOVELTY - The machine (100) has a display device (300) supported by a housing. A binaural audio system is supported by the housing. A memory device stores instructions. A processor executes the instructions to cause the display device to display a play of a game. The processor causes the display device to display a 3D image to a player (1) without requiring the player to wear 3D glasses, where the 3D image includes a virtual object associated with 3D coordinates. The processor operates with the binaural audio system to output 3D audio corresponding to the virtual object based on the 3D coordinates associated with the virtual object. USE - Electronic gaming machine. ADVANTAGE - The machine ensures that players wager and credits are preserved and minimizes potential disputes in event of malfunction on the machine. The machine provides dynamic sounds coupled with attractive multimedia images displayed on the display devices to provide an audio-visual representation or to display full-motion video with sound to attract players to the machine. The machine incorporates contactless haptic feedback to the player in conjunction with 3D audio synced with 3D gestures interaction to provide interactive virtual experience to the player. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for operating an electronic gaming machine. DESCRIPTION Of DRAWING(S) - The drawing shows a front perspective view of an electronic gaming machine.Player (1)Electronic gaming machine (100)Display device (300)Eye/head tracking zone (312)Eye tracking and/or head tracking camera (320)",,,
,,EL-GHOROURY H S; GRAZIOSI D B; ALPASLAN Z Y; GRAZIOSI D,"Method for forming e.g. near-eye augmented reality display, involves coupling output of eye and head tracking element to processor and encoder elements, providing image data to encoder element by processor element, and providing image data","NOVELTY - The method involves optically coupling an image display element to a near-eye display viewer's eyes with an optical element (206). An image processor element is electrically coupled to an encoder element (204). The encoder element is coupled to the image display element by embedding the image processor element and the encoder element within a near-eye display system (200a) within a vicinity of viewer's eyes. An eye and head tracking element (210) is optically coupled in a near-eye display (201) to sense a near-eye display viewer eye gaze direction and focus distance. An output of an eye and head tracking element is coupled to the image processor and encoder elements. Image data is provided to the encoder element by the image processor element. Compressed image data is provided to the near-eye display element by the encoder element. USE - Method for forming a near-eye display e.g. near-eye augmented reality (AR) display and virtual reality (VR) display, for a near-eye light field display system for use as a spatial light modulator (SLM) micro-display such as DLP and LCOS. Can also be used for use as a backlight source for an LCD. ADVANTAGE - The method enables reducing the near-eye display system latency, and adopting near-eye visual decompression methods for direct transfer and modulation of compressed image data so as to reduce the processing, memory and power consumption requirements of the near-eye system as the requirements for processing related to compression of the input image data at a processor is avoided. The method enables creating a near-eye display that satisfies stringent mobile device design requirements in compactness and power consumption and providing enhanced visual experience of two-dimensional (2D) or three-dimensional (3D) contents over a wide angular extent for users. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for forming a near-eye light field display system. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a near-eye display system with an embedded processor.Near-eye display system (200a)Near-eye display (201)Encoder element (204)Optical element (206)Eye and head tracking element (210)",,,
,,JENKINS C,"System for detecting potential attacks to building automation system, has processor that determines that attack on network occurred based on machine-learning algorithm output and updates graphical user interface based on attack occurrence","NOVELTY - The system has a first and second programmable logic controller (PLC) that are in communication by way of a network (504,522). A monitoring device receives a communication transmitted by way of the network over a period of time. A computing device (608) is comprised of a memory (612) comprising instructions that cause the processor (610) to perform acts comprised of receiving the communication from the monitoring device. A graphical user interface (GUI) (624) displayed on a display (616) comprises data indicative of communication traffic on the network and the communication traffic indicative of a potential attack on the network and the GUI comprises a directed graph. The directed graph includes a machine-learning algorithm that executes over the communication received from the monitoring device. An attack is determined on the network occurred based upon an output of the machine-learning algorithm. The GUI is updated based upon the determination that the attack on the network occurred. USE - System for detecting potential attacks to building automation system. ADVANTAGE - The volume of traffic is updated between each pair of devices in the updated listing of devices based on the additional communication data. The additional nodes or edges are representative of unexpected or unauthorized communication traffic. The natural user interface can rely on speech recognition, touch and stylus recognition, gesture recognition both on screen and adjacent to the screen, air gestures, head and eye tracking, voice and speech, vision, touch, gesture and machine intelligence. DESCRIPTION Of DRAWING(S) - The drawing shows a functional block diagram of a system that facilitates detecting potential attacks to a building automation system (BAS)Network (504,522)Computing device (608)Processor (610)Memory (612)Display (616)Graphical user interface (624)",,,
,,ZHAO F,"Energy-saving method for three dimensional display device, involves performing face detection within the monitoring range of three dimensional display device and acquiring bright pupil image and dark pupil image of face",NOVELTY - The energy-saving method involves performing face detection within the monitoring range of the 3D display device. The 3D display device is intended to be used through eye tracking. A bright pupil image and a dark pupil image of the face are acquired. The pupil image and the dark pupil image are differentially processed to obtain the image of the human eye pupil. The human eye pupil image is aligned with the face detection image. The eye pupil position of each face is traversed. The eye coordinates and interpupillary distance of the intended user are measured. The user is intended to perform location tracking. USE - Energy-saving method for a three dimensional display device. ADVANTAGE - Energy-saving system for three dimensional display device can be improved. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an energy-saving system for 3D display device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of energy-saving method for a three dimensional display device. (Drawing includes non-English language text).,,,
,,HAMMINGA D J S; BELIAEVA M,Apparatus for eye-tracking based text input has processor that locally refines three-dimensional scene based on features found between primary and secondary camera image streams combined with corresponding secondary camera rotation angle,"NOVELTY - The apparatus has a primary camera module mounted fixedly relative to the apparatus, and one or multiple secondary camera modules mounted to allow rotation around a first axis perpendicular to the primary camera optical axis and a second axis of which the direction vector is perpendicular to both the primary camera optical axis and the first rotation axis. One or more actuators rotate the secondary camera along the axis. The sensors measure secondary camera rotation angle for each of the rotation axis. A processor is coupled to actuators and sensors and used to control the secondary camera rotation. The processor iteratively constructs and locally refines a three-dimensional (3D) scene, based on corresponding features found between the primary camera image stream and any secondary camera image stream, combined with the corresponding secondary camera rotation angle. USE - Apparatus for eye-tracking based text input such as pressing keys on touchscreen of phone or voice recognition system. ADVANTAGE - Provides very compact physical form with new feedback system that overcomes accuracy issues associated with small form-factor solutions. Allows user to experience the correction as a natural and integral portion of the input session through the greatly enhanced speed of the feedback loop. The user is incentivized and trained continuously to provide clear microexpression, improving the system effectiveness during longer period of use. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for eye-tracking based text input. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the apparatus, the field-of-view for two cameras, and eyes as gazing upon a keyboard key.Screen (1)Keys (2)Eyes (4)Field-of-view (5)Narrower field-of-view (7)",,,
10.1007/s11277-018-5933-9,2019,"Noor, Shaheena; Uddin, Vali; Dur-e-Jabeen",Probabilistic Hierarchical Model Using First Person Vision for Scenario Recognition,"Smart homes are becoming a growing need to prepare for a comfortable life style for the elderly and make things easy for the caretakers of the future. One important component of these systems is to identify the human activities and scenarios. As the wireless technologies are becoming advanced, they are being used to provide a low-cost, non-intrusive and privacy-conscious solution to activity recognition. However, in more complicated environments, we need to identify scenarios with subtle cues e.g. eye gaze. These situations call for a complementary vision-based solution, and we present a robust scenario recognition system by following the objects seen in eye gaze trajectories. In this paper, we present a probabilistic hierarchical model for scenario recognition using the environmental elements like objects in the scene. We utilize the fact that any scenario can be divided into constituent tasks and activities recursively to the level of atomic actions and objects. Considering bottom-up, the scenario recognition problem can be hierarchically solved by identifying the objects seen and combining them together to form coarse-grained higher level activities. This is a novel contribution to be able to recognize complete scenarios only on the basis of objects seen. We performed experiments on standard datasets of Georgia Tech Egocentric Activities (GTEA-Gaze) and unconstrained videos collected in the Wild; and trained an Artificial Neural Network to get a precision of 73.84% and accuracy of 92.27%.",,,
,,HYUN K J; AHN D,"Interactive virtual reality system for head-mounted display terminal, has client system provided with university identity navigator part, where two-dimensional images are indicated according to indication of identity navigator part","NOVELTY - The system (1000) has a client system (100) provided with a university identity navigator part (105). The university identity navigator part determines selection according to time interval for controlling movement of a cursor based on eye gaze of a viewer. The university identity navigator part is provided with a video manager and a switch. The switch switches played video source, where multiple two-dimensional images are indicated according to indication of the university identity navigator part. The client system is characterized by a three-dimensional (3D) room part. USE - Interactive virtual reality (VR) system for a head-mounted display (HMD) terminal utilized in a university. ADVANTAGE - The system can be fixed in the HMD terminal so as to watch real-time casting with a VR environment. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an interactive VR system. '(Drawing includes non-English language text)'Client system (100)HMD renderer (101)Video player part (103)University identity navigator part (105)Interactive VR system (1000)",,,
10.3758/s13428-018-1133-5,2019,"Zemblys, Raimondas; Niehorster, Diederick C.; Holmqvist, Kenneth",gazeNet: End-to-end eye-movement event detection with deep neural networks,"Existing event detection algorithms for eye-movement data almost exclusively rely on thresholding one or more hand-crafted signal features, each computed from the stream of raw gaze data. Moreover, this thresholding is largely left for the end user. Here we present and develop gazeNet, a new framework for creating event detectors that do not require hand-crafted signal features or signal thresholding. It employs an end-to-end deep learning approach, which takes raw eye-tracking data as input and classifies it into fixations, saccades and post-saccadic oscillations. Our method thereby challenges an established tacit assumption that hand-crafted features are necessary in the design of event detection algorithms. The downside of the deep learning approach is that a large amount of training data is required. We therefore first develop a method to augment hand-coded data, so that we can strongly enlarge the data set used for training, minimizing the time spent on manual coding. Using this extended hand-coded data, we train a neural network that produces eye-movement event classification from raw eye-movement data without requiring any predefined feature extraction or post-processing steps. The resulting classification performance is at the level of expert human coders. Moreover, an evaluation of gazeNet on two other datasets showed that gazeNet generalized to data from different eye trackers and consistently outperformed several other event detection algorithms that we tested.",,,
10.1109/TIP.2020.3044209,2021,"Hsu, Wei-Yen; Chung, Chi-Jui",A Novel Eye Center Localization Method for Head Poses With Large Rotations,"Eye localization is undoubtedly crucial to acquiring large amounts of information. It not only helps people improve their understanding of others but is also a technology that enables machines to better understand humans. Although studies have reported satisfactory accuracy for frontal faces or head poses at limited angles, large head rotations generate numerous defects (e.g., disappearance of the eye), and existing methods are not effective enough to accurately localize eye centers. Therefore, this study makes three contributions to address these limitations. First, we propose a novel complete representation (CR) pipeline that can flexibly learn and generate two complete representations, namely the CR-center and CR-region, of the same identity. We also propose two novel eye center localization methods. This first method employs geometric transformation to estimate the rotational difference between two faces and an unknown-localization strategy for accurate transformation of the CR-center. The second method is based on image translation learning and uses the CR-region to train the generative adversarial network, which can then accurately generate and localize eye centers. Five image databases are employed to verify the proposed methods, and tests reveal that compared with existing methods, the proposed method can more accurately and robustly localize eye centers in challenging images, such as those showing considerable head rotation (both yaw rotation of -67.5 degrees to +67.5 degrees and roll rotation of +120 degrees to -120 degrees), complete occlusion of both eyes, poor illumination in addition to head rotation, head pose changes in the dark, and various gaze interaction.",,,
10.1109/ICCVW.2019.00542,2019,"Mequanint, Eyasu; Zhang, Shuai; Forutanpour, Bijan; Qi, Yingyong; Bi, Ning",Weakly-Supervised Degree of Eye-Closeness Estimation,"Following recent technological advances there is a growing interest in building non-intrusive methods that help us communicate with computing devices. In this regard, accurate information from eye is a promising input medium between a user and computing devices. In this paper we propose a method that captures the degree of eye closeness. Although many methods exist for detection of eyelid openness, they are inherently unable to satisfactorily perform in real world applications. Detailed eye state estimation is more important, in extracting meaningful information, than estimating whether eyes are open or closed. However, learning reliable eye state estimator requires accurate annotations which is cost prohibitive. In this work, we leverage synthetic face images which can be generated via computer graphics rendering techniques and automatically annotated with different levels of eye openness. These synthesized training data images, however, have a domain shift from real-world data. To alleviate this issue, we propose a weakly-supervised method which utilizes the accurate annotation from the synthetic data set, to learn accurate degree of eye openness, and the weakly labeled (open or closed) real world eye data set to control the domain shift. We introduce a data set of 1.3M synthetic face images with detail eye openness and eye gaze information, and 21k real-world images with open/closed annotation. The dataset will be released online upon acceptance. Extensive experiments validate the effectiveness of the proposed approach.",,,
10.1007/978-3-030-29384-0_18,2019,"Le, Khanh-Duy; Avellino, Ignacio; Fleury, Cedric; Fjeld, Morten; Kunz, Andreas",GazeLens: Guiding Attention to Improve Gaze Interpretation in Hub-Satellite Collaboration,"In hub-satellite collaboration using video, interpreting gaze direction is critical for communication between hub coworkers sitting around a table and their remote satellite colleague. However, 2D video distorts images and makes this interpretation inaccurate. We present GazeLens, a video conferencing system that improves hub coworkers' ability to interpret the satellite worker's gaze. A 360 degrees camera captures the hub coworkers and a ceiling camera captures artifacts on the hub table. The system combines these two video feeds in an interface. Lens widgets strategically guide the satellite worker's attention toward specific areas of her/his screen allow hub coworkers to clearly interpret her/his gaze direction. Our evaluation shows that GazeLens (1) increases hub coworkers' overall gaze interpretation accuracy by 25.8% in comparison to a conventional video conferencing system, (2) especially for physical artifacts on the hub table, and (3) improves hub coworkers' ability to distinguish between gazes toward people and artifacts. We discuss how screen space can be leveraged to improve gaze interpretation.",,,
10.1145/3192714.3192819,2018,"Yaneva, Victoria; Ha, Le An; Eraslan, Sukru; Yesilada, Yeliz; Mitkov, Ruslan",Detecting Autism Based on Eye-Tracking Data from Web Searching Tasks,"The ASD diagnosis requires a long, elaborate, and expensive procedure, which is subjective and is currently restricted to behavioural, historical, and parent-report information. In this paper, we present an alternative way for detecting the condition based on the atypical visual-attention patterns of people with autism. We collect gaze data from two different kinds of tasks related to processing of information from web pages: Browsing and Searching. The gaze data is then used to train a machine learning classifier whose aim is to distinguish between participants with autism and a control group of participants without autism. In addition, we explore the effects of the type of the task performed, different approaches to defining the areas of interest, gender, visual complexity of the web pages and whether or not an area of interest contained the correct answer to a searching task. Our best-performing classifier achieved 0.75 classification accuracy for a combination of selected web pages using all gaze features. These preliminary results show that the differences in the way people with autism process web content could be used for the future development of serious games for autism screening. The gaze data, R code, visual stimuli and task descriptions are made freely available for replication purposes.",,,
,2017,"Noronha, Bernardo; Dziemian, Sabine; Zito, Giuseppe A.; Konnaris, Charalambos; Faisal, A. Aldo","Wink to grasp - comparing Eye, Voice & EMG gesture control of grasp with soft-robotic gloves","The ability of robotic rehabilitation devices to support paralysed end-users is ultimately limited by the degree to which human-machine-interaction is designed to be effective and efficient in translating user intention into robotic action. Specifically, we evaluate the novel possibility of binocular eye-tracking technology to detect voluntary winks from involuntary blink commands, to establish winks as a novel low-latency control signal to trigger robotic action. By wearing binocular eye-tracking glasses we enable users to directly observe their environment or the actuator and trigger movement actions, without having to interact with a visual display unit or user interface. We compare our novel approach to two conventional approaches for controlling robotic devices based on electromyography (EMG) and speech-based human-computer interaction technology. We present an integrated software framework based on ROS that allows transparent integration of these multiple modalities with a robotic system. We use a soft-robotic SEM glove (Bioservo Technologies AB, Sweden) to evaluate how the 3 modalities support the performance and subjective experience of the end-user when movement assisted. All 3 modalities are evaluated in streaming, closed-loop control operation for grasping physical objects. We find that wink control shows the lowest error rate mean with lowest standard deviation of (0.23 +/- 0.07, mean +/- SEM) followed by speech control (0.35 +/- 0.13) and EMG gesture control (using the Myo armband by Thalamic Labs), with the highest mean and standard deviation (0.46 +/- 0.16). We conclude that with our novel own developed eye-tracking based approach to control assistive technologies is a well suited alternative to conventional approaches, especially when combined with 3D eye-tracking based robotic end-point control.",,,
,,YOKOKAWA Y,"Device for gesture-based user interfaces (UI) for augmented reality (AR) and virtual reality (VR) with gaze trigger, has processor that executes command responsive to emulated appendage being partially within three-dimensional (3D) object","NOVELTY - The device has a processor (24) that presents a 3D object on display (50). A player's appendage is imaged to render an emulated appendage, and the emulated appendage is provided with a gesture configuration as established by the player's appendage. The emulated appendage is different from the 3D object. The command is executed responsive to the emulated appendage being partially within the 3D object, and responsive to the gesture configuration correlating to a command. The command is not executed in responsive to the emulated appendage not being partially within the 3D object. The instructions are executable to present the 3D object responsive to reception of trigger. The trigger comprises eye tracking input signal and gesture of the appendage. USE - Device for gesture-based user interfaces (UI) for augmented reality (AR) and virtual reality (VR) with gaze trigger. ADVANTAGE - The movement of the video game controller or receiving a particular key sequence are detected to the controller used to identify a trigger gesture. The visual, audio, or tactile feedback or a combination can be generated at block to indicate that the command is executed. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an assembly for performing gesture-based UI for AR and VR with gaze trigger; and(2) a method for performing gesture-based UI for AR and VR with gaze trigger. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the system for performing gesture-based UI for AR and VR with gaze trigger.Primary display (14)Processor (24)Bluetooth (34)Display (50)Speaker (52)",,,
,,STUART B V; FARMER D; ZHANG T; DAS S; SHANBHAG S M; FONSEKA E,"Head-mounted system (HMD) for eye tracking latency enhancements has processors, which determine pose of eye of user based in portion on detected set of glint locations in second image and identified region of second image","NOVELTY - The head-mounted system has a camera to capture images of an eye (100) of a user, light source to illuminate the eye of the user, such that glints (115b) are represented in images of the eye of user, and processors to obtain a first image of the eye of the user from the camera and provide the first image as input to a machine learning model which has been trained to generate iris segmentation data and pupil segmentation data given an image of an eye. The processors obtain a second image of the eye of the user from the camera subsequent to the first image, detect, based on the iris segmentation data, a set of locations in the second image at which glints are represented, identify, based on the pupil segmentation data, a region of the second image at which the pupil of the eye of the user is represented, and determine a pose of the eye of the user based in portion on the detected set of glint locations in the second image and the identified region of the second image. USE - Head-mounted system for eye tracking latency enhancements. ADVANTAGE - Enables to identify the eye regions where glints are likely to occur and eye regions outside these regions do not need to be searched, which improves the accuracy, speed, and efficiency of the technique. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for: a method implemented by a head-mounted system of processors. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of an eye showing example eye features.Eye (100)Eyelid (110)Iris (112)Eye features (115)Iris features (115a)Glints (115b)",,,
,2020,"Lalanne, Clement; Rateaux, Maxence; Oudre, Laurent; Robert, Matthieu P.; Moreau, Thomas",Extraction of Nystagmus Patterns from Eye-Tracker Data with Convolutional Sparse Coding,"The analysis of the Nystagmus waveforms from eye-tracking records is crucial for the clinical interpretation of this pathological movement. A major issue to automatize this analysis is the presence of natural eye movements and eye blink artefacts that are mixed with the signal of interest.We propose a method based on Convolutional Dictionary Learning that is able to automatically highlight the Nystagmus waveforms, separating the natural motion from the pathological movements. We show on simulated signals that our method can indeed improve the pattern recovery rate and provide clinical examples to illustrate how this algorithm performs.",,,
,,ALONSO R E,"System for tracking eye of user through electronic display of e.g. personal computer, has processor determine third gaze location having third spatial coordinates based on gaze data and updated calibration settings","NOVELTY - The system has a processor (116) that determines a first gaze location having first spatial coordinates based on the gaze data and calibration settings associated with determining gaze locations. The processor alters a portion of the display (102) at or near the first gaze location. The processor determines a second gaze location having second spatial coordinates based on the gaze data and the calibration settings, after the portion of the display is altered. The processor performs comparison of the first gaze location to the second gaze location. The processor updates calibration settings based on the comparison. The processor determines a third gaze location having third spatial coordinates based on the gaze data and the updated calibration settings. USE - System for tracking eye of user through electronic display of electronic device e.g. personal computer. Uses include but are not limited to mobile device, tablet such as iPad (RTM: tablet computer developed by Apple), smart phone, personal digital assistant (PDA), Blu-ray player, gaming console, personal video recorder, set top box, DVD player, and digital video recorder. ADVANTAGE - The machine learning methods and/or techniques are utilized to train the eye tracking system to the user, thus increasing and/or maximizing accuracy of the determined gaze points as the iterations continue. The gaze data is advantageously used to optimize the media content provided to the users. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for tracking eye of user; and(2) a tangible machine-readable storage medium storing program for tracking eye of user. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view illustrating an eye tracking system.Display (102)Graphics and/or images (104)Electronic device (106)Screen (108)Processor (116)",,,
,,XIE L; QIN W; LU Y; DENG B; YAN Y; YIN E,"Non-calibration augmented reality glasses eye interacting method, involves predicting maximum probability three-dimensional gaze point observed by personnel within observation region after combining saliency detection models","NOVELTY - The method involves obtaining left and right eye images of person. Eye contour extraction process is performed through eye contour detection technology. An eye profile and eyeball profile information are mapped to a local enhanced three-dimensional observation region within augmented reality glasses view field by using a deep convolutional neural network mapping model. A maximum probability three-dimensional gaze point observed by personnel is predicted within a partial three-dimensional observation region after combining saliency detection models. The left and right eye images are obtained by camera shooting and combining lighting technology. USE - Non-calibration augmented reality glasses eye interacting method. ADVANTAGE - The method enables accurately predicting eye gaze 3D point information, adjusting a glasses wearing mode by a support personnel for convenient and rapid use, and providing an interactive mode of robust for augmented reality glasses. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a non-calibration augmented reality glasses eye interaction system. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a non-calibration augmented reality glasses eye interacting method. '(Drawing includes non-English language text)'",,,
,,CHEN J; LI K; ZHANG A; FAN H; CHEN C; ZHOU Y,"Augmented reality system based on naked-eye three dimensional display, has illumination light beam emitted by backlight module passes through lens array and image display layer reflected by half-mirror in viewing zone","NOVELTY - The system has a naked-eye three dimensional (3D) display terminal (10) includes a backlight module, a light source control module, a lens array and an image display layer (20). A tracking identification module acquires location information of a viewing zone. The light source control module obtains an image refresh synchronization signal from the image display layer and controls the backlight module to emit an illumination beam. The illumination light beam emitted by the backlight module passes through lens array and image display layer reflected by the half-mirror in a viewing zone. USE - Augmented reality system based on naked-eye 3D display. ADVANTAGE - The augmented reality system combines naked-eye stereoscopic display and augmented reality which reduces the sense of discomfort, blurring and flickering in the augmented reality system, increases the out-of-screen feeling and the layered feeling of the image or the interactive message in the viewing area can see a clear three-dimensional imaging blowing face realism. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic structural diagram of the augmented reality system based on naked eye 3D display.Naked-eye 3D display terminal (10)Image display layer (20)Semi-transflective lens (30)Human eye-tracking module (40)Fixing bracket (50)",,,
10.1007/978-981-13-8779-1_100,2020,"Liu, Zhiyao; Zhao, Qichao; Sun, Linwu; Luo, Yantong",Application of Virtual Human Factor Verification to Fire Accidents at the Main Control Room in Nuclear Power Plant,"The main control room (MCR) is the central of Human-Computer interaction (HCI) of nuclear power plants (NPPs), providing safety and comfortable working environment for operators. The present study built a 3-D virtual reality MCR and presented a fire accident in it. During the experiment, the physiological data including electrodermal activity (EDA) and electrocardiogram (ECG), and eye tracking data of the participants were measured. This method can quantitatively analyse and evaluate HCI, training, fitness and other human factors such as human errors or performance. The results reveal this quantitative evaluation method is valuable for improving HCI design and enhancing human performance.",,,
,2017,"Zhang, Cheng-lu; Shen, Rui-min",Automatic Detection of Mind Wandering Using Mobile Device In the Scenario of Online and Mobile Education,"Mind wandering is a state transition from task-related thinking to task-unrelated thinking and it has possible negative effects on learners' performance in online education. The detection of mind wandering can improve learners' performance by signaling, intervening and restoring attention to the process of learning. Traditional methods of detection of mind wandering involve specific devices such as eye gaze or physiologic sensors, tracking movements of eye ball or detecting electro-physiological activities of neurons or skin. This paper presents a new detection method using the camera of a mobile device to detect mind wandering when learners are tasked to watch a teaching video. Through the constant image capturing of the learner's face, his or her eye features can be captured and used to predict the learner's state of mind wandering if any, using methods of computer imagery and supervised machine learning. Through our experiment, we have attained an accuracy of 53%, which exceeded expectations, considering the hardware platform. The performance of the detection system based on the MCPP mechanism outperforms the detection system based on the traditional mechanism.",,,
,,CLEVELAND D; NORLOFF P L; NORLOFF M,"Eyetracking system for video eyetracking purposes, has time-of-flight (TOF) device that measures TOF-to-eye range and eye image to calculate spatial location, angular orientation, gazeline of eye in three-dimensional (3D) space","NOVELTY - The system has an eyetracking camera (410) that captures an image of an eye of a user (401). A light imaging and ranging TOF device measures a TOF-to-eye range from the TOF device to the eye. A processor (420) in communication with the eyetracking camera and the TOF device receives the measured TOF-to-eye range from the TOF device. The eyetracking camera image is received from the eyetracking camera. The eye image within the eyetracking camera image is identified. The TOF-to-eye range and the eye image are used to calculate a spatial location of the eye in a 3D space, an angular orientation of the eye in the 3D space, a gazeline (441) of the eye in the 3D space, and a spatial location of a gazepoint (442) of the eye in the 3D space. USE - Eyetracking system for video eyetracking purposes. ADVANTAGE - The system is able to precisely measure the camera-to-eye distance in video eyetracking by using a less expensive method and allow the eyetracker equipment to be miniaturized. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for eyetracking; and(2) a computer program product for eyetracking. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a video eye tracking system with TOF range detection using a single, broad AOI beam directed to a region including both eyes.User (401)Eyetracking camera (410)Processor (420)Gazeline (441)Gazepoint (442)",,,
10.1007/s10489-018-1331-y,2019,"El-Hussieny, Haitham; Ryu, Jee-Hwan",Inverse discounted-based LQR algorithm for learning human movement behaviors,"Recently, there has been an increasing interest towards understanding human movement behaviors. In this regard, one of the approaches is to retrieve the unknown underlying objective function that the human has to optimize while achieving a certain movement behavior. Existing research of behavioral understanding merely depends on predefined optimality criteria, where the minimum time, minimum variance or/and minimum effort are mainly adopted. These criteria are assumed to be constant, where the human is assumed to have the same preferences during the movement duration. However, in this paper, the optimality criteria underlying the kinematic characteristics of a certain human behavior are assumed to be exponentially discounted to account for the change in the human preferences that could happen while achieving this behavior. A new Inverse Discounted-based Linear Quadratic Regulator (ID-LQR) algorithm is developed in the light of Inverse Optimal Control (IOC) framework to find out the discounted cost function that could reproduce the measured human behavior perfectly. Meanwhile, an Incremental version of the ID-LQR algorithm is proposed to continuously refine the so far learned cost function in the case of sequentially presented demonstrations. The saccadic eye gaze movement is studied as an example to quantify both the proposed ID-LQR and Inverse ID-LQR approaches. Simulation results are encouraging and show that the saccadic trajectories generated by ID-LQR approach match the experimental data in many aspects, including position and velocity profiles of saccades. Moreover, when it is assessed by a subsequent set of scenarios, the incremental ID-LQR algorithm confirms its capability to generalize the so far retrieved cost function for the unseen saccadic demonstrations.",,,
10.3390/ijgi8060251,2019,"Snopkova, Dajana; Svedova, Hana; Kubicek, Petr; Stachon, Zdenek",Navigation in Indoor Environments: Does the Type of Visual Learning Stimulus Matter?,"This work addresses the impact of a geovisualization's level of realism on a user's experience in indoor navigation. The key part of the work is a user study in which participants navigated along a designated evacuation route previously learnt in a virtual tour or traditional 2D floor plan. The efficiency and effectiveness of completing the task was measured by the number of incorrect turns during navigation and completion time. The complexity of mental spatial representations that participants developed before and after navigating the route was also evaluated. The data was obtained using several qualitative and quantitative research methods (mobile eye tracking, structured interviews, sketching of cognitive maps, creation of navigation instructions, and additional questions to evaluate spatial orientation abilities). A total of 36 subjects (17 in the floor plan group and 19 in the virtual tour group) participated in the study. The results showed that the participants from both groups were able to finish the designated navigation route, but more detailed mental spatial representations were developed by the virtual tour group than the floor plan group. The participants in the virtual tour group created richer navigation instructions both before and after evacuation, mentioned more landmarks and could recall their characteristics. Visual landmark characteristics available in the virtual tour also seemed to support the correct decision-making.",,,
10.1016/j.ergon.2019.02.006,2019,"Guo, Fu; Li, Mingming; Hu, Mingcai; Li, Fengxiang; Lin, Bozhao",Distinguishing and quantifying the visual aesthetics of a product: An integrated approach of eye-tracking and EEG,"Although some methods of measuring visual aesthetics have been established, such as subjective reporting, feature calculating, and physiological assessing, designers still lack an integrated and quantified method in measuring the visual aesthetics of their products. This study aims to integrate eye-tracking metrics and EEG measurements to distinguish and quantify the visual aesthetics of a product. Thirty-two 3D prototypes of LED desk lamp with multiple views were designed to simulate an aesthetic appreciation flow. Eye-tracking and EEG signals were simultaneously recorded when participants were freely browsing each lamp. The evaluation of subjective visual aesthetics was conducted after each browsing. The results demonstrated that fixation time ratio and dwell time ratio significantly differed among the three clusters of visual aesthetic lamps. Meanwhile, average fixation duration only significantly differed between low and high aesthetic lamps and pupil size had no significant variation. Moreover, low aesthetic lamps evoked significantly weakened relative alpha power and enhanced relative gamma power. Thus, the eye-tracking metrics and the EEG measurements can distinguish the visual aesthetics of lamps. Regarding the results of quantification, the integrated multimodal physiological signals achieved an improved and reasonable accuracy. It seems beneficial to integrate multimodal physiological signals involved in different flows of visual aesthetic appreciation in quantifying the visual aesthetics of a product.Relevance to industry: As a premise of attracting consumers' attention, visual aesthetics has been identified as a crucial role in product design and marketing. Thus, thorough research on the variations of multimodal physiological signals involved in information retrieval and processing in appreciation flow can provide a distinction between product visual aesthetics. The quantification method can be utilized by designers in measuring the visual aesthetics of their products.",,,
10.1007/s00500-018-3670-3,2019,"Shi, Tianwei; Wang, Hong; Cui, Wenhua; Ren, Ling",Indoor space target searching based on EEG and EOG for UAV,"This paper puts forward a noninvasive electrooculography (EOG) and electroencephalogram (EEG)-based hybrid computer interface (HCI) system to implement the indoor target searching in three-dimensional (3D) space for a low-speed multi-rotor aircraft. The HCI system mainly consists of three subsystems, including the interface switching, decision and semi-autonomous navigation. The interface switching subsystem is accomplished by detecting the eyeblink EOG. The continuous wavelet transform is employed to indentify eyeblink features which are used to switch interfaces between horizontal and vertical motor imagery (MI) tasks. The average accuracy of the eyeblink feature detection reaches to 97.95%. The decision subsystem employs the joint regression (JR) model and spectral powers methods to extract the time and frequency domain features of MI tasks by analyzing the left- and right-hand MI EEG. Simultaneously, the support vector machine is applied to accomplish the MI tasks classification and final decision. The average classification accuracy of the HCI system reaches to 93.99%. The semi-autonomous navigation subsystem extracts the environmental features to avoid obstacles semi-automatically in 3D space and provide feasible directions for the decision subsystem. The actual indoor 3D space target searching experiments are put forward to verify the feasibility and adaptation performances of this proposed HCI system.",,,
10.1007/s10845-018-1458-z,2020,"Chen, Haiyong; Pang, Yue; Hu, Qidi; Liu, Kun",Solar cell surface defect inspection based on multispectral convolutional neural network,"Similar and indeterminate defect detection of solar cell surface with heterogeneous texture and complex background is a challenge of solar cell manufacturing. The traditional manufacturing process relies on human eye detection which requires a large number of workers without a stable and good detection effect. In order to solve the problem, a visual defect detection method based on multi-spectral deep convolutional neural network (CNN) is designed in this paper. Firstly, a selected CNN model is established. By adjusting the depth and width of the model, the influence of model depth and kernel size on the recognition result is evaluated. The optimal CNN model structure is selected. Secondly, the light spectrum features of solar cell color image are analyzed. It is found that a variety of defects exhibited different distinguishable characteristics in different spectral bands. Thus, a multi-spectral CNN model is constructed to enhance the discrimination ability of the model to distinguish between complex texture background features and defect features. Finally, some experimental results and K-fold cross validation show that the multi-spectral deep CNN model can effectively detect the solar cell surface defects with higher accuracy and greater adaptability. The accuracy of defect recognition reaches 94.30%. Applying such an algorithm can increase the efficiency of solar cell manufacturing and make the manufacturing process smarter.",,,
,2017,Dinh Thanh Nhan; Truong Quoc Bao; Truong Quoc Dinh,A Study on Warning System About Drowsy Status of Driver,"A warning system about drowsy status (fatigue or drowsiness) of the driver, helping to limit the traffic accidents caused by falling asleep behind the wheel by determining the status of the eyes combined with the head direction of driver. In this paper, we present a novel approach for determining the position of a human face and eyes in different directions of the face (turning left, turning right, tilt left, tilt right, looked-up, broke-down), using ASM model. Besides, we suggested an improved algorithm to detect and identify the status of the eyes to support the driver during driving. This algorithm consists of three main steps: (1) enhance the brightness of the image area contains two eyes using color spaces and median filter processing; (2) the image area contains two eye was extracted by a method of morphologies and image segmentation with dynamic threshold; (3) finally, the status of the eyes is determined by using SVM classifier based on HOG feature. Next, the direction of the driver's head is estimated by combining the 3D head model and algorithms posit to increase the effectiveness of the warning system. Our tested is performed with the data which was set by our (3778 photos of eyes of 9 people). Testing accuracy achieved is 97.86% and the average recognition time is 0.106s/image frame. Testing accuracy confirms the effectiveness and feasibility of the proposed system.",,,
,,SUMI N; ZHU S,"Display device for presenting auto-stereoscopic visual effect, has controller that is configured to define eye-to-eye line passing through position of viewer eyes, and generate image data of pixels to viewing position on eye-to-eye line","NOVELTY - The device (100) has a display module (110) that comprises several pixels (PA,PB). An optical modulator (120) is mounted on the display module and configured to modulate light emitted from the display module to corresponding directions. An eye tracking module (130) is configured to track positions of a viewer's eyes. A controller is configured to define an eye-to-eye line passing through the positions of the viewer's eyes. The image data of several pixels are generated according to multiple viewing positions (VPA,VPB) on the eye-to-eye line. The controller defines a center projection line on the display module corresponding to a center position (VC) of the viewer's eyes according to parameters related to the optical modulator and the display module. The controller derives modulator-to-pixel offsets for several pixels each according to a distance between a reference point of a pixel and a corresponding center projection line (CPL1,CPL2)of the center projection line. USE - Display device for presenting auto-stereoscopic visual effect. ADVANTAGE - The images for different viewing angles can be diffused to reduce the sharp change between different views. The display device uses the eye tracking module to track the positions of the viewer's eyes so as to improve the three-dimensional (3D) visual effects and allows the viewer to watch the display device casually without concerning about the limited viewing positions. The defects seen by the viewer is reduced and the visual quality is increased. The eye-to-eye line can help to imply the correct viewing positions of the pixels in the display module. The images presented by the display module according to the viewing position and the viewing direction of the viewer results in a wider view with better visual quality. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for operating display device. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the display device.Display device (100)Display module (110)Optical modulator (120)Eye tracking module (130)Center projection lines (CPL1,CPL2)Pixels (PA,PB)Center position (VC)Viewing positions (VPA,VPB)",,,
,2020,"Mitsuzum, Yu; Irie, Go; Kimura, Akisato; Nakazawa, Atsushi",A GENERATIVE SELF-ENSEMBLE APPROACH TO SIMULATED plus UNSUPERVISED LEARNING,"In this paper, we consider Simulated and Unsupervised (S+U) learning which is a problem of learning from labeled synthetic and unlabeled real images. After translating the synthetic images to real ones, existing S+U learning methods use only the labeled synthetic images for training a predictor (e.g., a regression function) and ignore the target real images, which may result in unsatisfactory prediction performance. Our approach utilizes both synthetic and real images to train the predictor. The main idea of ours is to involve a self-ensemble learning framework into S+U learning. More specifically, we require the prediction results for an unlabeled real image to be consistent between teacher and student predictors, even after some perturbations are added to the image. Furthermore, aiming at generating diverse perturbations along the underlying data manifold, we introduce one-to-many image translation between synthetic and real images. Evaluation experiments on an appearance-based gaze estimation task demonstrate that the proposed ideas can improve the prediction accuracy and our full method can outperform existing S+U learning methods.",,,
10.1109/LGRS.2019.2918955,2020,"Li, Xiaobin; Jiang, Bitao; Wang, Shengjin; Shen, Li; Fu, Yuze",A Human-Computer Fusion Framework for Aircraft Recognition in Remote Sensing Images,"Existing aircraft recognition methods usually regard recognition as an isolated classification problem, supposing that aircraft detection has finished. These methods use image slices each containing single aircraft as input, which is often not the case in practice. In order to recognize aircraft in remote sensing images that contain multiple objects and background, we propose a human-computer fusion framework that combines the advantages of human and computer. First, we propose candidate aircraft using human eye tracking, making use of the efficient and accurate search ability of human. Then, we propose a two-step recognition method, which simulates the recognition process of image analysts, to identify the types of candidate aircraft. In the first step, we recognize aircraft using fully connected features of a fine-tuned convolutional neural network (CNN). If it does not work, we go to the second step. In the second step, we use convolutional features extracted from the fine-tuned CNN for recognition. To improve the representational ability of convolutional features, we introduce the Fisher Vector encoding. Thorough experiments demonstrate that the proposed framework is effective, surpassing state-of-the-art methods on recognition performance.",,,
,,CHHIPA P C; DHIMAN R,"Method for providing augmented reality (AR) content in AR device, involves determining size of AR content associated with next appearing object, and displaying AR content of next appearing object based on determined size of AR content","NOVELTY - The method involves monitoring (2401) a viewing activity of a user with respect to an object. A viewing time of the AR content is determined (2402) associated with a next appearing object based on the viewing activity of the user. A size of the AR content is determined (2403) associated with the next appearing object based on the monitored viewing activity of the user. The AR content of the next appearing object is displayed (2404) based on the determined size of the AR content. A viewing parameter is detected comprising an eye gaze time and eye movement which is determined based on a head movement of the user or an eye ball movement of the user. USE - Method for providing augmented reality (AR) content in AR device such as wearable glasses, mobile device or AR server. ADVANTAGE - The artificial intelligence (AI) engine employs a reinforcement technique as a learning process, by which the prediction capabilities are improved based on a feedback of the past predicted values. The controller updates the minimum threshold view time and the maximum threshold view time based on observation category, historical viewing activity data, real-time viewing activity data and user-specific profile data. The detection module allows applying the image processing techniques and deep learning techniques to identify the objects in the captured image frame to identify the real objects or the augmented objects as present in the captured frame. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an apparatus for providing augmented reality content; and(2) a non-transitory computer readable storage medium storing instructions for providing augmented reality content. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for providing AR content for a real object based on an expected gaze time for next appearing AR content.Step for monitoring a viewing activity of a user with respect to object (2401)Step for determining a viewing time of the AR content associated with a next appearing object (2402)Step for determining a size of the AR content associated with the next appearing object (2403)Step for displaying the AR content of the next appearing object (2404)",,,
10.1109/TIM.2020.3020682,2021,"Hageman, Kristin N.; Chow, Margaret R.; Roberts, Dale C.; Della Santina, Charles C.",Low-Noise Magnetic Coil System for Recording 3-D Eye Movements,"Vestibular and oculomotor research often requires measurement of 3-D eye orientation and movement with high spatial and temporal precision and accuracy. We describe the design, implementation, validation, and use of a new magnetic coil system optimized for recording 3-D eye movements using small scleral coils in animals. Like older systems, the system design uses off-the-shelf components to drive three mutually orthogonal alternating magnetic fields at different frequencies. The scleral coil voltage induced by those fields is decomposed into three signals, each related to the coil's orientation relative to the axis of one field component. Unlike older systems based on analog demodulation and filtering, this system uses a field-programmable gate array (FPGA) to oversample each induced scleral coil voltage (at 25 M samples/s), demodulate in the digital domain, and average over 25 k samples per data point to generate 1-k samples/s output in real time. Noise floor is <0.036 degrees peak-to-peak and linearity error is <0.1 degrees during 345 degrees rotations in all three dimensions. This FPGA-based design, which is both reprogrammable and freely available upon request, delivers sufficient performance to record eye movements at high spatial and temporal precision and accuracy using coils small enough for use with small animals.",,,
10.1080/10400435.2016.1268218,2018,"Otoom, Mwaffaq; Alzubaidi, Mohammad A.",Ambient intelligence framework for real-time speech-to-sign translation,"Sign language can be used to facilitate communication with and between deaf or hard of hearing (Deaf/HH). With the advent of video streaming applications in smart TVs and mobile devices, it is now possible to use sign language to communicate over worldwide networks. In this article, we develop a prototype assistive device for real-time speech-to-sign translation. The proposed device aims at enabling Deaf/HH people to access and understand materials delivered in mobile streaming videos through the applications of pipelined and parallel processing for real-time translation, and the application of eye-tracking based user-satisfaction detection to support dynamic learning to improve speech-to-signing translation. We conduct two experiments to evaluate the performance and usability of the proposed assistive device. Nine deaf people participated in these experiments. Our real-time performance evaluation shows the addition of viewer's attention-based feedback reduced translation error rates by 16% (per the sign error rate [SER] metric) and increased translation accuracy by 5.4% (per the bilingual evaluation understudy [BLEU] metric) when compared to a non-real-time baseline system without these features. The usability study results indicate that our assistive device was also pleasant and satisfying to deaf users, and it may contribute to greater engagement of deaf people in day-to-day activities.",,,
10.1371/journal.pone.0247061,2021,"Lounis, Christophe; Peysakhovich, Vsevolod; Causse, Mickael",Visual scanning strategies in the cockpit are modulated by pilots' expertise: A flight simulator study,"During a flight, pilots must rigorously monitor their flight instruments since it is one of the critical activities that contribute to update their situation awareness. The monitoring is cognitively demanding, but is necessary for timely intervention in the event of a parameter deviation. Many studies have shown that a large part of commercial aviation accidents involved poor cockpit monitoring from the crew. Research in eye-tracking has developed numerous metrics to examine visual strategies in fields such as art viewing, sports, chess, reading, aviation, and space. In this article, we propose to use both basic and advanced eye metrics to study visual information acquisition, gaze dispersion, and gaze patterning among novices and pilots. The experiment involved a group of sixteen certified professional pilots and a group of sixteen novice during a manual landing task scenario performed in a flight simulator. The two groups landed three times with different levels of difficulty (manipulated via a double task paradigm). Compared to novices, professional pilots had a higher perceptual efficiency (more numerous and shorter dwells), a better distribution of attention, an ambient mode of visual attention, and more complex and elaborate visual scanning patterns. We classified pilot's profiles (novices-experts) by machine learning based on Cosine KNN (K-Nearest Neighbors) using transition matrices. Several eye metrics were also sensitive to the landing difficulty. Our results can benefit the aviation domain by helping to assess the monitoring performance of the crews, improve initial and recurrent training and ultimately reduce incidents, and accidents due to human error.",,,
10.1109/THMS.2020.2965429,2020,"Huang, Yifei; Cai, Minjie; Sato, Yoichi",An Ego-Vision System for Discovering Human Joint Attention,"Joint attention often happens during social interactions, in which individuals share focus on the same object. This article proposes an egocentric vision-based system (ego-vision system) that aims to discover the objects looked at jointly by a group of persons engaged in interactive activities. The proposed system relies on a collection of wearable eye-tracking cameras that provide an egocentric view of the interaction scenes as well as points-of-gaze measurement of each participant. Technically in our system, we develop a hierarchical conditional random field (CRF) based graphical model that can temporally localize joint attention periods and spatially segment objects of joint attention. By solving these two coupled tasks together in an iterative optimization procedure, we show that human joint attention can be reliably discovered from videos even with cluttered background and noisy gaze measurement. A new dataset of joint attention is collected and annotated for evaluating the two tasks of joint attention where two to four persons are involved. Experimental results demonstrate that our approach achieves state-of-the-art performance on both tasks of spatial segmentation and temporal localization of joint attention.",,,
10.3390/s19102377,2019,"Krol, Michal; Krol, Magdalena Ewa",A Novel Eye Movement Data Transformation Technique that Preserves Temporal Information: A Demonstration in a Face Processing Task,"Existing research has shown that human eye-movement data conveys rich information about underlying mental processes, and that the latter may be inferred from the former. However, most related studies rely on spatial information about which different areas of visual stimuli were looked at, without considering the order in which this occurred. Although powerful algorithms for making pairwise comparisons between eye-movement sequences (scanpaths) exist, the problem is how to compare two groups of scanpaths, e.g., those registered with vs. without an experimental manipulation in place, rather than individual scanpaths. Here, we propose that the problem might be solved by projecting a scanpath similarity matrix, obtained via a pairwise comparison algorithm, to a lower-dimensional space (the comparison and dimensionality-reduction techniques we use are ScanMatch and t-SNE). The resulting distributions of low-dimensional vectors representing individual scanpaths can be statistically compared. To assess if the differences result from temporal scanpath features, we propose to statistically compare the cross-validated accuracies of two classifiers predicting group membership: (1) based exclusively on spatial metrics; (2) based additionally on the obtained scanpath representation vectors. To illustrate, we compare autistic vs. typically-developing individuals looking at human faces during a lab experiment and find significant differences in temporal scanpath features.",,,
,,PICCIONELLI G A,"Method for operating immersive display to provide surround-type of representation in virtual reality, involves activating indicator in immersive display in response to controller when scope of coverage of camera is insufficient to support",NOVELTY - The method involves sharing a portion of the surround-type of representation of real-world visual information from multiple cameras where each of the cameras is coupled to a controller. A portion of the surround-type of representation of real-world visual information is displayed in each of multiple immersive displays where each of immersive displays is responsive to a controller. An indicator is activated in the immersive displays in response to the controller when the scope of coverage of cameras is insufficient to support the surround-type of representation of real-world visual information from cameras. USE - Method for operating immersive display e.g. three-dimensional (3D) display to provide surround-type of representation of real-world visual information in virtual reality (VR) or augmented reality (AR) environment. ADVANTAGE - The clear images are provided to the user without the use of prescriptive eyewear. The sufficient light is provided to illuminate the eyes of the user to enable eye tracking or eye condition evaluation operations or both eye tracking and eye condition evaluation. The orientation information is utilized to enable recording of sound by the microphones in coordination with visual images to provide a more realistic immersive experience for the user. The user can validate the immersive display warning. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a process for controlling the immersive display.Step for obtaining information (202)Step for determining object attributes (204)Step for identifying object (206)Step for identifying direction of view (210)Step for displaying object on display (212),,,
,,THUNSTROEM R; ROENNGREN D; CLAUSEN A,"System for presenting three-dimensional graphical items in e.g. gaming, on display device, has graphics processing device causing graphics to be displayed on display device, where graphics displayed on display device are modified","NOVELTY - The system has an eye tracking device (120) for determining a gaze point of a user on a display device (110). A graphics processing device (130) causes a graphics to be displayed on the display device, where the graphics displayed on the display device are modified such that the graphics in an area including the gaze point of the user with modified parameter relative to graphics outside an area and size of the area is determined based on a part of an amount of noise in the gaze point over time and auxiliary factor. USE - System for presenting three-dimensional (3D) graphical items in areas e.g. gaming, modeling and movies, on a display device for displaying information e.g. text, images and video, to a viewer. ADVANTAGE - The device reduces latency between outputs of information to a display device and enhances speed at which a user can move their eyes to perceive information and rendering quality of an entire display device when no viewers are detected so as to enhance system resource consumption and power consumption when display device is not a primary focus of the viewer. The device satisfies processing requirements if objects and or activity occurring in a periphery sufficiently cognizable to a user under foveated rendering conditions. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a system for modifying an image based on a user's gaze point.Gaze determination system (100)Display device (110)Eye tracking device (120)Graphics processing device (130)Processor (140)",,,
10.1109/JSEN.2018.2844252,2018,"Chen, Zhangjie; Wang, Ya; Liu, Hanwei",Unobtrusive Sensor-Based Occupancy Facing Direction Detection and Tracking Using Advanced Machine Learning Algorithms,"Facing direction detection plays a critical role in human-computer interaction and has a wide application in surveillance systems, driving awareness recognition, smart home appliances, computer games, and so on. Current detection methods are mainly focused on extracting specific patterns from user's optical images, which raises concerns on privacy invasion and these detection techniques do not usually work in a dark environment. To address these concerns, this paper proposes an activity recognition system guided by an unobtrusive sensor (ARGUS). By using a low pixel infrared thermopile array sensor, ARGUS is capable of identifying five facing directions (left 45 degrees/90 degrees, right 45 degrees /90 degrees, and front) through the support vector machine classifier. Also two feature extraction methods are compared. One is manually-defined and the other is based on a pre-trained convolutional neural network (CNN) model. The facing direction detection accuracy resulting from manually-defined features reaches 85.3%, 90.6%, and 85.2% at detection distance of 0.6, 1.2, and 1.8 m, respectively. The level of accuracy resulted from using pre-trained CNN features demonstrates a much more reliable performance (89.1%, 95.3%, and 95.1% at distance of 0.6, 1.2, and 1.8 m, respectively). In addition, ARGUS has been successfully applied for occupancy tracking with a root mean square error of 0.19 m.",,,
10.3390/ijgi6120407,2017,"Sasinka, Cenek; Morong, Kamil; Stachon, Zdenek",The Hypothesis Platform: An Online Tool for Experimental Research into Work with Maps and Behavior in Electronic Environments,"The article presents a testing platform named Hypothesis. The software was developed primarily for the purposes of experimental research in cartography and psychological diagnostics. Hypothesis is an event-logger application which can be used for the recording of events and their real-time processing, if needed. The platform allows for the application of Computerized Adaptive Testing. The modularity of the platform makes it possible to integrate various Processing.js-based applications for creation and presentation of rich graphic material, interactive animations, and tasks involving manipulation with 3D objects. The Manager Module allows not only the administration of user accounts and tests but also serves as a data export tool. Raw data is exported from the central database in text format and then converted in the selection module into a format suitable for statistical analysis. The platform has many functions e.g., the creation and administration of tasks with real-time interaction between several participants (multi-player function) and those where a single user completes several tests simultaneously (multi-task function). The platform may be useful e.g., for research in experimental economics or for studies involving collaborative tasks. In addition, connection of the platform to an eye-tracking system is also possible.",,,
,,KAKARAPARTHY S L; KUMAR V A; CONTRACTOR D; NAGAR S; DEY K; DWIVEDI U,Method for ordering digital teaching content involves receiving digital teaching content and any corresponding annotations and one or more content heuristic from a teacher,"NOVELTY - The method involves receiving digital teaching content and any corresponding annotations and one or more content heuristic from a teacher, the digital teaching content has at least one of text and one or more image. The student gaze of the digital teaching content is monitoried during a class by a multiple of students, and some of the digital teaching content is cognitively skipped during the class based on the monitoring and the one or more content heuristic. USE - Method for ordering digital teaching content. ADVANTAGE - Data structures as set forth herein can be updated by machine learning so that accuracy and reliability is iteratively improved over time without resource consuming rules intensive processing. Bright-pupil tracking creates greater iris/pupil contrast, allowing more robust eye-tracking with all iris pigmentation, and greatly reduces interference caused by eyelashes and other obscuring features. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following :(1) system for ordering digital teaching content and(2) a computer program product for ordering digital teaching content. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the methodClassroom (400)Lecturer (402)Class (404)Digital Teaching Content (406)Video camera (410)",,,
10.1016/j.im.2016.11.008,2017,"Ben Mimoun, Mohammed Slim; Poncin, Ingrid; Garnier, Marion",Animated conversational agents and e-consumer productivity: The roles of agents and individual characteristics,"This research presents two studies, in a lab (eye-tracking) and in a natural context, that highlight the effects of interacting with an animated conversational agents (ACA) on the objective and perceived econsumer productivity. Results from Study 1 specify that only objective e-consumer productivity depends on interaction with the ACA. Going further, Study 2 then reveals that individual characteristics affect perceived productivity, either independently from ACA use (for involvement or product familiarity) or in interaction with using the ACA (for Internet skills and need for interaction). These findings highlight the need to personalize websites that display an agent fitting user profiles. (C) 2016 Published by Elsevier B.V.",,,
10.1117/12.2590763,2021,"Tatsuya, Oshinomi; Masaya, Mitobe; Yuji, Sakamoto",Speckleless reconstructed image display using convergent light in CGH,"In Computer-Generated Hologram (CGH), random phases are often added to diffuse the object light. However, due to this effect, speckle noise, which is a deterioration in image quality, occurs. In this study, we generated a hologram that suppresses the influence of speckle by calculation so that the light wave converges to the viewpoint. The viewing zone is limited by this method, it can be applied to Head-Mounted Display (HMD) type CGH or Eye tracking type electronic holography. The effectiveness of the proposed method was confirmed by simulation and actual optical reproduction.",,,
,2019,"Azevedo, Roger; Biswas, Gautam","Measuring, Analyzing, and Modeling Multimodal Multichannel Data for Supporting Self-regulated Learning by Making Systems More Intelligent for All in the 21st Century","Learning with advanced learning technologies (ALTs) such as intelligent tutors, serious games, simulations, and immersive virtual environments, involves intricate and complex interactions among cognitive, metacognitive, motivational, affective, and social processes. Current psychological and educational research on learning with ALTs provides a wealth of empirical data indicating that learners of all ages have difficulty learning about complex topics in areas such as STEM. Learning with ALTs requires students to analyze the learning situation, set meaningful learning goals, determine which strategies to use, assess whether the strategies are effective in meeting the learning goal, and evaluate their emerging understanding of the topic. They also need to monitor and reflect on their understanding and modify their plans, goals, strategies, and effort in relation to contextual conditions (e.g., cognitive, motivational, and task conditions). We argue that understanding these processes necessitates the measurement, analyses, and modeling of multimodal multichannel data (e.g., log files, eye tracking, and physiological sensors) during learning and problem solving with ALTs.Understanding the complex nature of the temporally unfolding SRL processes is being addressed by emerging interdisciplinary research using online trace methods (e.g., log-files, eye-tracking, think-aloud protocols, physiological sensors, screen recording of human-machine interactions, classroom discourse). The use of these methods has been widely applauded by the research community. Despite these benefits of multimodal multichannel data, analyzing these data come with their own set of challenges that will be addressed by the participants of this workshop. They include the following: (1) temporal alignment of data sources based on different sampling rates; (2) complexity in dealing with noisy and messy data (e.g., missing data) with traditional and contemporary data mining and machine learning techniques; (3) accurate classification and tracking of the underlying cognitive, metacognitive, and affective processes; (4) assessment of the levels of accuracy in modeling complex underlying processes, and confidence in inferences based on current analytical methods; and (5) implications of multimodal analyses on instruction and learning (e.g., providing timely scaffolding needed to facilitate emotion regulation).",,,
10.1109/TPAMI.2020.2966453,2021,"Wang, Wenguan; Shen, Jianbing; Lu, Xiankai; Hoi, Steven C. H.; Ling, Haibin",Paying Attention to Video Object Pattern Understanding,"This paper conducts a systematic study on the role of visual attention in video object pattern understanding. By elaborately annotating three popular video segmentation datasets (DAVIS) with dynamic eye-tracking data in the unsupervised video object segmentation (UVOS) setting. For the first time, we quantitatively verified the high consistency of visual attention behavior among human observers, and found strong correlation between human attention and explicit primary object judgments during dynamic, task-driven viewing. Such novel observations provide an in-depth insight of the underlying rationale behind video object pattens. Inspired by these findings, we decouple UVOS into two sub-tasks: UVOS-driven Dynamic Visual Attention Prediction (DVAP) in spatiotemporal domain, and Attention-Guided Object Segmentation (AGOS) in spatial domain. Our UVOS solution enjoys three major advantages: 1) modular training without using expensive video segmentation annotations, instead, using more affordable dynamic fixation data to train the initial video attention module and using existing fixation-segmentation paired static/image data to train the subsequent segmentation module; 2) comprehensive foreground understanding through multi-source learning; and 3) additional interpretability from the biologically-inspired and assessable attention. Experiments on four popular benchmarks show that, even without using expensive video object mask annotations, our model achieves compelling performance compared with state-of-the-arts and enjoys fast processing speed (10 fps on a single GPU). Our collected eye-tracking data and algorithm implementations have been made publicly available athttps://github.com/wenguanwang/AGS.",,,
,,ROENNGREN D,"Method for rotating field of view represented by displayed image, involves displaying image representing field of view with display device based on first direction being same as second direction, where subject is centered in image","NOVELTY - The method (500) involves displaying a first image representing a first field of view with a display device. Gaze direction of a user toward the first image is determined (580) with an eye tracking device. Subject in the first image at which the gaze direction is directed is identified with a processing device, where the subject is in first direction from a center of the first image. Directional input in second direction is received at a handheld input device. A second image representing a second field of view is displayed with the display device based on the second direction being the same as the first direction, where the subject is centered in the second image. USE - Method for rotating a field of view represented by a displayed image in a TV, a monitor and a projection display device. ADVANTAGE - The method enables individually controlling illuminators in real-time based on current light conditions for each eye, so that performance of the eye tracking device can be improved compared to situation in which settings are based on one eye or average of both eyes. The method enables easily allowing a person to navigate a three dimensional (3D) virtual environment, without making it easy to target potential enemies/interactors for virtual combat or other interaction with players/characters to reduce necessity of calibration between the device and the user to operate the device, without user calibration in some instances due to extra data gathered by an image sensor. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a system for rotating a field of view represented by a displayed image(2) a non-transitory machine readable medium comprising a set of instructions for rotating a field of view represented by a displayed image. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for rotating a field of view represented by a displayed image.Method for rotating field of view represented by displayed image (500)Step for calibrating illuminators and image sensor (510)Step for receiving non-image information (520)Step for receiving image information from scene camera by control unit (530)Step for determining gaze direction of user toward first image with eye tracking device (580)",,,
,,ANDERSON F; GONG J; FITZMAURICE G,"Method for generating safety hazard alerts, involves receiving a first image frame captured by a first camera and a second image frame captured by a second camera, and determining an eye location based on the second image frame","NOVELTY - The generating method involves receiving a first image frame captured by a first camera and a second image frame captured by a second camera, where the first camera and the second camera are secured to a pair of safety glasses. A location of a hazard is determined based on the first image frame, and determined an eye location based on the second image frame. The process is performed to determine that the user has not noticed the hazard based on the location of the hazard and the eye location, and caused an alert to be generated responsive to determining the user has not noticed the hazard. The first image frame is processed using a machine learning model trained to identify hazardous objects. USE - Method for generating safety hazard alerts. ADVANTAGE - The method leverages eye gaze data to accurately distinguish between hazards that the user has and has not noticed, so as to avoid distracting the user with unnecessary alerts of hazards that the user has already noticed. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a computer-readable storage medium for storing instructions; and(2) a pair of safety glasses for automatically detecting and reporting safety hazards in a work environment. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart of a generating method.Receiving image frames captured by an eye-facing camera and an environmental camera (602)Detecting pupil locations within the image frames captured by the eye-facing camera (604)Converting the detected pupil locations to locations within the image frames captured by the environmental camera (606)Detecting a hazardous situation within the image frames captured the environmental camera (608)Causing alert to be raised (612)",,,
,,SHRIBERG E E; ARATOW M; ISLAM M; HARATI A; RUTOWSKI T; LIN D; LU Y; HAQUE F; ROGERS R D; HARATI N T A H,"Method for identifying whether subject is at risk with mental or physiological condition, involves outputting electronic report identifying whether subject is at risk with mental or physiological condition based on feature identified","NOVELTY - The method involves obtaining data from subject. The data is comprised of speech data and optionally associated visual data. The data is processed using models comprising a natural language processing (NLP) model, an acoustic model, or a visual model (2226) to yield processed data. The processed data is used to identify features indicative of mental or physiological condition. An electronic report identifying whether subject is at risk with mental or physiological condition is output based on features that are identified using processed data which risk is quantified in a form of a score having a confidence level provided in electronic report. USE - Method for identifying whether subject bis at risk with mental or physiological condition. ADVANTAGE - The context of the health screening or monitoring system improves performance metrics associated with machine learning algorithms used by the system. The signal quality is improved. The acoustic analysis of the audiovisual signal is used to improve sentiment analysis of words and phrases. The accuracy in application of composite mode is improved. The screening or monitoring accuracy is improved. The system renders improved and highly accurate classifications for a health condition. The accuracy of voice recognition is improved. The reliance on the analysis of a mouth region extraction is reduced. The reduced error rate or the accuracy is established relative to benchmark standards. The sharing of clinical data with reduced concerns with violation of privacy laws is allowed. The overall variance in the system is reduced, thus yielding a more precise prediction. The distinction between servers and clients facilitates human understanding of purpose of a given computer. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of the visual model logic of modeling computer system.Visual model (2226)Training facial cue model (4302)Eye/gaze model (4304)",,,
,2019,"Chen, Yuying; Liu, Congcong; Tai, Lei; Liu, Ming; Shi, Bertram E.",Gaze Training by Modulated Dropout Improves Imitation Learning,"Imitation learning by behavioral cloning is a prevalent method that has achieved some success in vision-based autonomous driving. The basic idea behind behavioral cloning is to have the neural network learn from observing a human expert's behavior. Typically, a convolutional neural network learns to predict the steering commands from raw driver-view images by mimicking the behaviors of human drivers. However, there are other cues, such as gaze behavior, available from human drivers that have yet to be exploited. Previous researches have shown that novice human learners can benefit from observing experts' gaze patterns. We present here that deep neural networks can also profit from this. We propose a method, gaze-modulated dropout, for integrating this gaze information into a deep driving network implicitly rather than as an additional input. Our experimental results demonstrate that gaze-modulated dropout enhances the generalization capability of the network to unseen scenes. Prediction error in steering commands is reduced by 23.5% compared to uniform dropout. Running closed loop in the simulator, the gaze-modulated dropout net increased the average distance travelled between infractions by 58.5%. Consistent with these results, the gaze-modulated dropout net shows lower model uncertainty.",,,
10.1007/s10207-018-0408-2,2019,"Joudaki, Zeinab; Thorpe, Julie; Martin, Miguel Vargas","Enhanced Tacit Secrets: System-assigned passwords you can't write down, but don't need to","We explore the feasibility of Tacit Secrets: system-assigned passwords that you can remember, but cannot write down or otherwise communicate. We design an approach to creating Tacit Secrets based on contextual cueing, an implicit learning method previously studied in the cognitive psychology literature. Our feasibility study indicates that our approach has strong security properties: resistance to brute-force attacks, online attacks, phishing attacks, some coercion attacks, and targeted impersonation attacks. It also offers protection against leaks from other verifiers as the secrets are system-assigned. Our approach also has some interesting usability properties, a high login success rate, and low false positive rates. We explore enhancements to our approach and find that incorporating eye-tracking data offers substantial improvements. We also explore the trade-offs of different configurations of our design and provide insight into valuable directions for future work.",,,
10.1109/CVPR42600.2020.00482,2020,"Tsiami, Antigoni; Koutras, Petros; Maragos, Petros",STAViS: Spatio-Temporal AudioVisual Saliency Network,"We introduce STAViS(1), a spatio-temporal audiovisual saliency network that combines spatio-temporal visual and auditory information in order to efficiently address the problem of saliency estimation in videos. Our approach employs a single network that combines visual saliency and auditory features and learns to appropriately localize sound sources and to fuse the two saliencies in order to obtain a final saliency map. The network has been designed, trained end-to-end, and evaluated on six different databases that contain audiovisual eye-tracking data of a large variety of videos. We compare our method against 8 different state-of-the-art visual saliency models. Evaluation results across databases indicate that our STAViS model outperforms our visual only variant as well as the other state-of-the-art models in the majority of cases. Also, the consistently good performance it achieves for all databases indicates that it is appropriate for estimating saliency in-the-wild. The code is available at https://github.com/atsiami/STAViS.",,,
,,WANG Q; LI K; YAO Y; SHI F,"Method for using scanner and eye tracker to determine position of gaze point in, involves solving vertical plane of line of sight and using vertical plane expression to solve coordinates of line of sight projection point","NOVELTY - The method involves choosing a suitable position and assuming a three-dimensional (3D) scanner, and placing (S10) positioning punctuation on the scanned object. The eye tracker is set up, an eye tracker coordinate system is established based on the eye tracker and the transition matrix, and the global coordinate system of the aircraft simulation cockpit are established (S20). The linear equation of the gaze direction is established (S30) according to the eye coordinates of the eye tracker and the coordinates of the gaze point on the virtual plane. The distance from the selected point in the point cloud to straight line of gaze direction is solved (S40). The distance from all points to eye is solved and coordinates of closest point is solved (S50). The vertical plane of line of sight is solved according to nearest point coordinates, the vertical plane expression is used to solve coordinates of line of sight projection point, and desired 3D fixation point coordinates are obtained (S60). USE - Method for using scanner and eye tracker such as head-mounted eye tracker and telemetry eye tracker to determine position of gaze point in 3D scene used in web page layout optimization, scene research, human-computer interaction, virtual reality, clinical medicine and other fields. ADVANTAGE - The accurate and fast tracking of the three-dimensional scene of the eye gaze is realized. The method avoids the shortcomings of the traditional method of complex solution and huge amount of calculation. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the method for using scanner and eye tracker to determine position of gaze point in three-dimensional scene. (Drawing includes non-English language text)Step for choosing a suitable position and assuming a 3D scanner, and placing positioning punctuation on the scanned object (S10)Step for setting up the eye tracker, establishing an eye tracker coordinate system based on the eye tracker and the transition matrix, and establishing the global coordinate system of the aircraft simulation cockpit (S20)Step for establishing the linear equation of the gaze direction according to the eye coordinates of the eye tracker and the coordinates of the gaze point on the virtual plane (S30)Step for solving the distance from the selected point in the point cloud to straight line of gaze direction (S40)Step for solving the distance from all points to eye is solved and coordinates of closest point (S50)Step for solving the vertical plane of line of sight according to nearest point coordinates, using the vertical plane expression to solve coordinates of line of sight projection point, and obtaining desired 3D fixation point coordinates (S60)",,,
,,ROSS J K; PRICE R K; JAIN P,"Three-dimensional imaging system for active illuminated imaging, has precision timing control block (TCON) configured to control frame timing of first imaging camera and frame timing of second imaging camera","NOVELTY - The system (100) has a first active imaging camera (102-1), a second active imaging camera (102-2), a TCON (104) in communication with the first active illuminated imaging camera and the second active illuminated IR imaging camera. The TCON is configured to control a frame timing of the first imaging camera and a frame timing of the second imaging camera with synchronization precision of less than 10 mu mseconds. Several memory devices are in communication with the first active imaging camera and the second imaging camera. The first active camera being an active three-dimensional (3D) imaging camera includes an illuminator. The second active imaging camera is being an infrared (IR) camera for eye tracking which includes the illuminator. The first active imaging camera and the second active IR imaging camera are being positioned in housing (101). USE - System for active illuminated imaging. ADVANTAGE - The systems for helping to reduce or eliminate interference between the different cameras. The camera can interfere with one another, limiting and/or preventing the accurate imaging and depth calculations of either camera. The interleaving of subframe exposures can allow for higher frame rates and less delay between each of the 3D imaging cameras, while limiting or preventing optical contamination of 3D imaging cameras. The shorter subframes can reduce the amount of blur in the image and improve the clarity of the image for image recognition and/or tracking purposes. The active portion of the second camera schedule and the inactive portion of the first camera schedule can be aligned temporally to interleave the integration and/or exposure of the first 3D imaging camera and the integration and/or exposure of the second 3D imaging camera. The optical contamination or interference between 3D imaging systems can be limited or prevented, when one of the two is an active 3D imaging system and the 3D imaging systems are sensitive to the same wavelength range. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for three-dimensional imaging; and(2) a method for active IR imaging. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic representation of the system for multiple active IR imaging sensors.Three-dimensional imaging system (100)Housing (101)First active imaging camera (102-1)Second active imaging camera (102-2)Timing control block (104)",,,
,,JEUNG S K,"System for experiencing HMD working virtual reality game contents of user using 3D cubic image display, has external culture impression providing unit arranged in window opening such that providing unit handles model at specific degrees","NOVELTY - The system has an experience part formed to progress different contents through a virtual reality (VR). The experience part is provided with an optical tracking motion sensing part (20). A control unit (70) is configured to include a wind generating unit (C), which is generated in a narration description area (B). A window opening is arranged to a scenario concept part (A). An external culture impression providing unit (D) is arranged in the window opening such that an experience providing unit (E) feels and handles a model at 360 degrees in an images provision impression part (F). USE - System for experiencing HMD working VR contents of a video game console using a 3D cubic image display based on a position sensor. ADVANTAGE - The system moves the user while freely applying movement to the user in a virtual world and direct imaginary realistic world, so that reality of the interface can be experienced with a display by using motion sensing and the user progressive direction at the virtual world is decided according to an eye gaze of the user, thus experiencing practical transfer on the surface of the virtual world. The system learns actually provides various experience in the VR service to the user by receiving the VR service through the interface between the virtual world and a real-device so as to provide the service with immersion and experience in the reality in a practical manner. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a system for experiencing HMD working VR contents of a video game console. '(Drawing includes non-English language text)'Scenario concept part (A)Narration description area (B)Wind generating unit (C)External culture impression providing unit (D)Experience providing unit (E)Images provision impression part (F)Optical tracking motion sensing part (20)Control unit (70)",,,
,2017,"Sudha, Ruba P.; Sudha, L.; Aruna, K. B.; Sureka, V; Jayanthi, K.",LEER DETECTION WITH WEBCAM IN A DESKTOP ENVIRONMENT,"This cardboard addresses the eye exhausting tracking botheration machine a reduced and included satisfactory web camera in a desktop domain, as against to tedious tracking procedures acute specific hardware, e.g., ambivalent high-resolution camera and self-contradicting on fire sources, as capable as a cumbersome plan handle. In the proposed strategy, we native hint the creature confront in a continuous video plan to extract the eye districts. At that point, we amalgamate intensity activity and curve spine to affirmation the iris centermost and propel the piecewise eye twist indicator to discover the eye corner. We acknowledge a sinusoidal model to mimic the 3-D curve shape.",,,
,,DUTU L; MATHE S; DUMITRU-GUZU M; LEMLEY J,"Neural network image processing apparatus for acquiring images from image sensor, feeds identified eye region to respective convolutional neural networks (CNNs) each configured to produce respective feature vector including numerical values","NOVELTY - The apparatus identifies a region of interest (ROI) containing a face region in the images, determines facial landmarks in a face region within a ROI, uses facial landmarks to transform the face region within the ROI into a face region having a given pose, and uses transformed landmarks within the transformed face region to identify a pair of eye regions (27L,27R) within the transformed face region. Each identified eye region is fed to respective CNNs (40L,40R) each configured to produce a respective feature vector (42) comprising numerical values. Each feature vector is fed to respective eyelid opening level neural networks (44L,44R) to obtain respective measures of eyelid opening for each eye region. The feature vectors are combined. The concatenated vector is fed to a gaze angle neural network (46) to generate gaze yaw and pitch values simultaneously with the eyelid opening values. The eyelid opening and gaze angle neural networks are jointly trained based on a common training set. USE - Neural network image processing apparatus arranged to acquire images from an image sensor. Can be used in dynamic platforms such as driver monitoring systems and handheld devices. ADVANTAGE - Provides an integrated network where the weights for the various layers are determined once in the same training process to provide eyelid and gaze estimation values i.e. each component (opening, gaze) of the network boosts the other as eyelid opening information to help the system learn more efficiently how to predict gaze, and vice-versa. There is no need to manually weight gaze angles calculated for separate eye regions, which reduces human intervention and favors a pure machine learning approach. DESCRIPTION Of DRAWING(S) - The drawing shows a a schematic view of each eye region fed to a respective CNN.Eye regions (27L,27R)CNNs (40L,40R)Feature vector (42)Eyelid opening level neural networks (44L,44R)Gaze angle neural network (46)",,,
,,ANTHONY R J L; CROWHURST S J; PRICE M J,"Method for collecting response data, involves producing advertisement based on expected user response to variables such that advertisement is created to elicit desired response from user of first type when presented to user","NOVELTY - The method involves collecting (1006) multiple response data for users of a first type in a controlled group as each of the users receive a presentation of digital content. The response data includes an electroencephalography (EEG) data, a functional magnetic resonance imaging (fMRI) data, a galvanic skin response (GSR) data, a heart rate data, a body temperature data, an eye tracking data, a face tracking data or a head tracking data. The response data is time correlated (1008) with variables of the digital content. An expected user response to the variables related to the digital content is determined based on the time correlation. An advertisement is produced based on the expected user response to the variables such that the advertisement is created to elicit a desired response from a user of the first type when presented to the user. USE - Method for collecting response data of user for aggregating by machine learning system and correlated with different variables for digital content such as advertisement. ADVANTAGE - By enabling the production of digital content according to the models such that the digital content can produce the predicted and repeatable user response from users of the particular user type, thus, increasing the likelihood of user engagement. The modified text are produced based on the configuration file output from the trained machine learning system to simplify the text and increase the size of the font, results in higher user engagement and reduced cognitive load on the user viewing the advertisement. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for collecting response data. DESCRIPTION Of DRAWING(S) - The drawing shows the flowchart illustrating the method for collecting response data.Step for determining user profile (1002)Step for presenting digital content to user (1004)Step for collecting multiple response data for users of first type in controlled group (1006)Step for correlating response data with variables of digital content (1008)Step for producing models (1012)",,,
10.1016/j.apergo.2020.103251,2021,"Wu, Chuhao; Cha, Jackie; Sulek, Jay; Sundaram, Chandru P.; Wachs, Juan; Proctor, Robert W.; Yu, Denny",Sensor-based indicators of performance changes between sessions during robotic surgery training,"Training of surgeons is essential for safe and effective use of robotic surgery, yet current assessment tools for learning progression are limited. The objective of this study was to measure changes in trainees' cognitive and behavioral states as they progressed in a robotic surgeon training curriculum at a medical institution. Seven surgical trainees in urology who had no formal robotic training experience participated in the simulation curriculum. They performed 12 robotic skills exercises with varying levels of difficulty repetitively in separate sessions. EEG (electroencephalogram) activity and eye movements were measured throughout to calculate three metrics: engagement index (indicator of task engagement), pupil diameter (indicator of mental workload) and gaze entropy (indicator of randomness in gaze pattern). Performance scores (completion of task goals) and mental workload ratings (NASA-Task Load Index) were collected after each exercise. Changes in performance scores between training sessions were calculated. Analysis of variance, repeated measures correlation, and machine learning classification were used to diagnose how cognitive and behavioral states associate with performance increases or decreases between sessions. The changes in performance were correlated with changes in engagement index (r(rm) = - .25, p < .001) and gaze entropy (r(rm) = - .37, p < .001). Changes in cognitive and behavioral states were able to predict training outcomes with 72.5% accuracy. Findings suggest that cognitive and behavioral metrics correlate with changes in performance between sessions. These measures can complement current feedback tools used by medical educators and learners for skills assessment in robotic surgery training.",,,
10.1016/j.apergo.2018.07.005,2018,"Chew, Jouh Yeong; Ohtomi, Koichi; Suzuki, Hiromasa",Glance behavior as design indices of in-vehicle visual support system: A study using crane simulators,"A prediction model is used to predict subjective responses of crane operators with respect to different designs of In-Vehicle Visual Support (IVVS). Selected gaze metrics are used as objective metrics to minimize prejudice, which is commonly caused by subjective measures. Experiments are carried out using crane simulator to measure glance behavior of novice operators and the 3D perspective projection method is used for autonomous mapping of gaze fixations to dynamic Area-of-Interests (AOIs). Subjective responses, such as operators' emotion and usability of IVVS, are evaluated using the Likert scale of the Semantic Differential method. Correlation between gaze metrics and subjective responses is established using multiple linear regression. Glance behavior exhibits a statistically significant difference when information on IVVS is perceived as useful to ease operation and reduce tension. Despite this, there are no significant signs of distraction. Glance behavior is found to be a reliable sub-conscious indicator of subjective response and distraction. More importantly, the proposed gaze metrics are found to be a good representation of glance behavior, such as randomness and distribution of attention. The methods and findings are useful to evaluate impact of IVVS, which is becoming more common in many other applications.",,,
10.1109/CVPR.2017.133,2017,"Su, Shan; Hong, Jung Pyo; Shi, Jianbo; Park, Hyun Soo",Predicting Behaviors of Basketball Players from First Person Videos,"This paper presents a method to predict the future movements (location and gaze direction) of basketball players as a whole from their first person videos. The predicted behaviors reflect an individual physical space that affords to take the next actions while conforming to social behaviors by engaging to joint attention. Our key innovation is to use the 3D reconstruction of multiple first person cameras to automatically annotate each other's visual semantics of social configurations.We leverage two learning signals uniquely embedded in first person videos. Individually, a first person video records the visual semantics of a spatial and social layout around a person that allows associating with past similar situations. Collectively, first person videos follow joint attention that can link the individuals to a group. We learn the egocentric visual semantics of group movements using a Siamese neural network to retrieve future trajectories. We consolidate the retrieved trajectories from all players by maximizing a measure of social compatibility-the gaze alignment towards joint attention predicted by their social formation, where the dynamics of joint attention is learned by a long-term recurrent convolutional network. This allows us to characterize which social configuration is more plausible and predict future group trajectories.",,,
10.3390/ijgi10020082,2021,"Rydvanskiy, Ruslan; Hedley, Nick",Mixed Reality Flood Visualizations: Reflections on Development and Usability of Current Systems,"Interest in and use of 3D visualizations for analysis and communication of flooding risks has been increasing. At the same time, an ecosystem of 3D user interfaces has also been emerging. Together, they offer exciting potential opportunities for flood visualization. In order to understand how we turn potential into real value, we need to develop better understandings of technical workflows, capabilities of the resulting systems, their usability, and implications for practice. Starting with existing geospatial datasets, we develop single user and collaborative visualization prototypes that leverage capabilities of the state-of-the art HoloLens 2 mixed reality system. By using the 3D displays, positional tracking, spatial mapping, and hand- and eye-tracking, we seek to unpack the capabilities of these tools for meaningful spatial data practice. We reflect on the user experience, hardware performance, and usability of these tools and discuss the implications of these technologies for flood risk management, and broader spatial planning practice.",,,
10.1109/ACCESS.2018.2879619,2018,"Dawood, Amina; Turner, Scott; Perepa, Prithvi",Affective Computational Model to Extract Nature Affective States of Students With Asperger Syndrome (AS) in Computer-Based Learning Environment,"This paper was inspired by looking at the central role of emotion in the learning process, its impact on students' performance; as well as the lack of affective computing models to detect and infer affective-cognitive states in real time for students with and without Asperger Syndrome (AS). This model overcomes gaps in other models that were designed for people with autism, which needed the use of sensors or physiological instrumentations to collect data. The model uses a webcam to capture students' affective-cognitive states of confidence, uncertainty, engagement, anxiety, and boredom. These states have a dominant effect on the learning process. The model was trained and tested on a natural-spontaneous affective dataset for students with and without AS, which was collected for this purpose. The dataset was collected in an uncontrolled environment and included variations in culture, ethnicity, gender, facial and hairstyle, head movement, talking, glasses, illumination changes, and background variation. The model structure used deep learning (DL) techniques like convolutional neural network and long short-term memory. The DL is the-state-of-art tool that used to reduce data dimensionality and capturing non-linear complex features from simpler representations. The affective model provides reliable results with accuracy 90.06%. This model is the first model to detected affective states for adult students with AS without physiological or wearable instruments. For the first time, the occlusions in this model, like hand over face or head were considered an important indicator for affective states like boredom, anxiety, and uncertainty. These occlusions have been ignored in most other affective models. The essential information channels in this model are facial expressions, head movement, and eye gaze. The model can serve as an aided-technology for tutors to monitor and detect the behaviors of all students at the same time and help in predicting negative affective states during learning process.",,,
10.1109/BIBE50027.2020.00141,2020,"Bao, Wenkai; Villarreal, Dario; Chiao, J-C",Vision-Based Autonomous Walking in a Lower-Limb Powered Exoskeleton,"Lower-limb powered exoskeletons have the potential to help patients who suffer from mobility impairments to regain their independence. Unlike traditional exoskeletons with predefined gait patterns, some recently developed exoskeletons are capable of planning gait patterns autonomously based on environmental information and human perception. However, patients may have difficulty navigating through unfamiliar environments due to the slow or miscommunication between the exoskeletons and users. Verbal or manual adjustments cannot continuously or adaptively manage the mechanical motions either. In this study, we propose and demonstrate an autonomous walking pattern generator based on shared visual information between human and a powered lower-limb exoskeleton. This control scheme is to understand the user's intention during walking and help the patient who has trouble walking to overcome obstacles. Human gazing positions in a 3-D environment are measured and evaluated via an eye tracker system to detect objects in the walking path. A model predictive controller (MPC) is formulated, which generates and modifies the footsteps and the center of mass trajectory of the powered exoskeleton in real time considering the visual feedback. Preliminary experiments are performed by having a human subject walking toward a target sign indicating a physical obstacle. Our results show the feasibility of integrating the walking pattern generator with visual feedbacks to achieve autonomous control for a lowerlimb powered exoskeleton.",,,
10.1007/s10956-017-9684-2,2017,"Herrington, Deborah G.; Sweeder, Ryan D.; VandenPlas, Jessica R.",Students' Independent Use of Screencasts and Simulations to Construct Understanding of Solubility Concepts,"As students increasingly use online chemistry animations and simulations, it is becoming more important to understand how students independently engage with such materials and to develop a set of best practices for students' use of these resources outside of the classroom. Most of the literature examining students' use of animations and simulations has focused on classroom use with some studies suggesting that better outcomes are obtained when students use simulations with minimal guidance while others indicate the need for appropriate scaffolding. This study examined differences with respect to (1) student understanding of the concept of dissolution of ionic and covalent compounds in water and (2) student use of electronic resources when students were asked to complete an assignment either by manipulating a simulation on their own or by watching a screencast in which an expert manipulated the same simulation. Comparison of students' pre- and posttest scores, answers to assignment questions, near-transfer follow-up questions, and eye-tracking analysis suggested that students who viewed the screencast gained a better understanding of the dissolving process, including interactions with water at the particulate level, particularly for covalent compounds. Additionally, the eye tracking indicated that there were significant differences in the ways that the different treatment groups (screencast or simulation) used the electronic resources.",,,
10.1109/ACCESS.2021.3070511,2021,"Morales, Aythami; Costela, Francisco M.; Woods, Russell L.",Saccade Landing Point Prediction Based on Fine-Grained Learning Method,"The landing point of a saccade defines the new fixation region, the new region of interest. We asked whether it was possible to predict the saccade landing point early in this very fast eye movement. This work proposes a new algorithm based on LSTM networks and a fine-grained loss function for saccade landing point prediction in real-world scenarios. Predicting the landing point is a critical milestone toward reducing the problems caused by display-update latency in gaze-contingent systems that make real-time changes in the display based on eye tracking. Saccadic eye movements are some of the fastest human neuro-motor activities with angular velocities of up to 1,000 degrees/s. We present a comprehensive analysis of the performance of our method using a database with almost 220,000 saccades from 75 participants captured during natural viewing of videos. We include a comparison with state-of-the-art saccade landing point prediction algorithms. The results obtained using our proposed method outperformed existing approaches with improvements of up to 50% error reduction. Finally, we analyzed some factors that affected prediction errors including duration, length, age, and user intrinsic characteristics.",,,
10.1109/ACCESS.2021.3074319,2021,"Ahmed, Ahmed Abdelmoamen; Echi, Mathias",Hawk-Eye: An AI-Powered Threat Detector for Intelligent Surveillance Cameras,"With recent advances in both AI and IoT capabilities, it is possible than ever to implement surveillance systems that can automatically identify people who might represent a potential security threat to the public in real-time. Imagine a surveillance camera system that can detect various on-body weapons, masked faces, suspicious objects and traffic. This system could transform surveillance cameras from passive sentries into active observers which would help in preventing a possible mass shooting in a school, stadium or mall. In this paper, we present a prototype implementation of such systems, Hawk-Eye, an AI-powered threat detector for smart surveillance cameras. Hawk-Eye can be deployed on centralized servers hosted in the cloud, as well as locally on the surveillance cameras at the network edge. Deploying AI-enabled surveillance applications at the edge enables the initial analysis of the captured images to take place on-site, which reduces the communication overheads and enables swift security actions. At the cloud side, we built a Mask R-CNN model that can detect suspicious objects in an image captured by a camera at the edge. The model can generate a high-quality segmentation mask for each object instance in the image, along with the confidence percentage and classification time. The camera side used a Raspberry Pi 3 device, Intel Neural Compute Stick 2 (NCS 2), and Logitech C920 webcam. At the camera side, we built a CNN model that can consume a stream of images directly from an on-site webcam, classify them, and displays the results to the user via a GUI-friendly interface. A motion detection module is developed to capture images automatically from the video when a new motion is detected. Finally, we evaluated our system using various performance metrics such as classification time and accuracy. Our experimental results showed an average overall prediction accuracy of 94% on our dataset.",,,
10.1109/ACCESS.2020.2978436,2020,"Guo, Zizheng; Pan, Yufan; Zhao, Guozhen; Zhang, Jun; Dong, Ni",Recognizing Hazard Perception in a Visual Blind Area Based on EEG Features,"Many potential hazards are encountered during daily driving in mixed traffic situations, and the anticipatory activity of a driver to a hazard is one of the key factors in many crashes. In a previous study using eye-tracking data, it was reliably recognized whether the eyes of a driver had become fixated or pursued hazard cues. A limitation of using eye-tracking data is that it cannot be identified whether the anticipatory activity of a driver to hazards has been activated. This study aimed to propose a method to recognize whether the psychological anticipation of a driver had been activated by a hazard cue using electroencephalogram (EEG) signals as input. Thirty-six drivers participated in a simulated driving task designed according to a standard psychological anticipatory study paradigm. Power spectral density (PSD) features were extracted from raw EEG data, and feature dimensions were reduced by principal component analysis (PCA). The results showed that when a driver detected a hazard cue, the alpha band immediately decreased, and the beta band increased approximately 300 ms after the cue appeared. Based on performance evaluation of the support vector machine (SVM), k-nearest neighbor (KNN) method, and linear discriminant analysis (LDA), SVM could detect the anticipatory activity of the driver to a potential hazard in a timely manner with an accuracy of 81%. The findings demonstrated that the hazard anticipatory activity of a driver could be recognized with EEG data as input.",,,
10.1371/journal.pone.0246739,2021,"Bafna, Tanya; Baekgaard, Per; Hansen, John Paulin",Mental fatigue prediction during eye-typing,"Mental fatigue is a common problem associated with neurological disorders. Until now, there has not been a method to assess mental fatigue on a continuous scale. Camera-based eye-typing is commonly used for communication by people with severe neurological disorders. We designed a working memory-based eye-typing experiment with 18 healthy participants, and obtained eye-tracking and typing performance data in addition to their subjective scores on perceived effort for every sentence typed and mental fatigue, to create a model of mental fatigue for eye-typing. The features of the model were the eye-based blink frequency, eye height and baseline-related pupil diameter. We predicted subjective ratings of mental fatigue on a six-point Likert scale, using random forest regression, with 22% lower mean absolute error than using simulations. When additionally including task difficulty (i.e. the difficulty of the sentences typed) as a feature, the variance explained by the model increased by 9%. This indicates that task difficulty plays an important role in modelling mental fatigue. The results demonstrate the feasibility of objective and non-intrusive measurement of fatigue on a continuous scale.",,,
10.1016/j.cobme.2016.12.002,2019,"Sapiro, Guillermo; Hashemi, Jordan; Dawson, Geraldine",Computer vision and behavioral phenotyping: an autism case study,"Despite significant recent advances in molecular genetics and neuroscience, behavioral ratings based on clinical observations are still the gold standard for screening, diagnosing, and assessing outcomes in neurodevelopmental disorders, including autism spectrum disorder. Such behavioral ratings are subjective, require significant clinician expertise and training, typically do not capture data from the children in their natural environments such as homes or schools, and are not scalable for large population screening, low-income communities, or longitudinal monitoring, all of which are critical for outcome evaluation in multisite studies and for understanding and evaluating symptoms in the general population. The development of computational approaches to standardized objective behavioral assessment is, thus, a significant unmet need in autism spectrum disorder in particular and developmental and neurodegenerative disorders in general. Here, we discuss how computer vision, and machine learning, can develop scalable low-cost mobile health methods for automatically and consistently assessing existing biomarkers, from eye tracking to movement patterns and affect, while also providing tools and big data for novel discovery.",,,
10.1111/bjet.13022,2020,"Lui, Michelle; McEwen, Rhonda; Mullally, Martha",Immersive virtual reality for supporting complex scientific knowledge: Augmenting our understanding with physiological monitoring,"Educators are recognizing the potential power of immersive virtual reality (IVR) to allow learners to experience previously intangible firsthand phenomena, such as atoms and molecules. In this study, an IVR simulation of a complex gene regulation system was co-designed with an undergraduate microbiology course instructor. The course, with 234 students, was taught using active learning strategies, including peer instruction and exposure to a two-dimensional computer simulation. Thirty-four students from the course participated in an interactive IVR experience using head-mounted displays. We assess students' conceptual understanding using tests, multimodal data collected during the IVR sessions (including video analysis in combination with physiological sensor data and eye-tracking data) as well as semi-structured interviews. We found that students who were seated while in IVR demonstrated significantly higher conceptual understanding of gene regulation at the end of the course and higher overall course outcomes, as compared to students who experienced the course as originally designed (control). However, students who experienced IVR in a standing position performed similarly to the control group. In addition, learning gain appears to be influenced by a combination of prior knowledge and how IVR is experienced (ie, sitting vs. standing). Learning implications for the connections between sensorimotor systems and cognition in IVR are discussed.",,,
10.1109/TLT.2020.3020499,2020,"Papamitsiou, Zacharoula; Pappas, Ilias O.; Sharma, Kshitij; Giannakos, Michail N.",Utilizing Multimodal Data Through fsQCA to Explain Engagement in Adaptive Learning,"Investigating and explaining the patterns of learners' engagement in adaptive learning conditions is a core issue towards improving the quality of personalized learning services. This article collects learner data from multiple sources during an adaptive learning activity, and employs a fuzzy set qualitative comparative analysis (fsQCA) approach to shed light to learners' engagement patterns, with respect to their learning performance. Specifically, this article measures and codes learners' engagement by fusing and compiling clickstreams (e.g., response time), physiological data (e.g., eye-tracking, electroencephalography, electrodermal activity), and survey data (e.g., goal-orientation) to determine what configurations of those data explain when learners can attain high or medium/low learning performance. For the evaluation of the approach, an empirical study with 32 undergraduates was conducted. The analysis revealed six configurations that explain learners' high performance and three that explain learners' medium/low performance, driven by engagement measures coming from the multimodal data. Since fsQCA explains the outcome of interest itself, rather than its variance, these findings advance our understanding on the combined effect of the multiple indicators of engagement on learners' performance. Limitations and potential implications of the findings are also discussed.",,,
10.1002/cav.1745,2018,"Krejtz, Krzysztof; Duchowski, Andrew; Zhou, Heng; Jorg, Sophie; Niedzielska, Anna",Perceptual evaluation of synthetic gaze jitter,"Eye movements are an essential part of non-verbal behavior. Non-player characters; as they occur in many games, communicate with the player through dialogue and non-verbal behavior and can have a strong influence on player experience or even on gameplay. In this paper, we evaluate a procedural model designed to synthesize the subtleties of eye motion. More specifically, our model adds microsaccadic jitter and pupil unrest both modeled by 1/f(alpha) or pink noise to the saccadic main sequence. In a series of perceptual two-alternative forced-choice experiments, we explore the perceived naturalness of different parameters of pink noise by comparing synthesized motions to rendered motion of recorded eye movements at extreme close shot and close shot distances. Our results show that, on average, animations based on a procedural model with pink noise were perceived and evaluated as highly natural, whereas data-driven motion without any jitter or with unfiltered jitter were consistently selected as the least natural in appearance.",,,
,,LOVTJAERN A; HOEGSTROEM J; PETERSSON R; SKOGOE M; WONG W,"Method for determining correspondence between gaze direction and environment around wearable device, involves determining gaze point on predefined image, and mapping gaze point onto predefined image in three-dimensional map","NOVELTY - The method (300) involves receiving a scene image from an outward facing image sensor (310). Gaze direction of a wearer of a wearable device is determined (320) with an eye tracking device at a point in time when the scene image is captured by an outward facing image sensor. Determination is made (340) to check whether a scene image includes a remaining portion of the predefined image based on the input parameter, where the remaining portion of the predefined image does not include the dynamic area. Position of the remaining portion in the scene image is determined. A gaze point on the predefined image is determined based on the gaze direction and the position. The gaze point is mapped onto the predefined image in a three-dimensional (3D) map. USE - Method for determining correspondence between a gaze direction and an environment around a wearable device. ADVANTAGE - The method enables mitigating burden of review process to easily and expediently correct incorrect mapped gaze, and determining parts of a video/image from an outward facing image sensor in an easy manner. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a system for determining a correspondence between a gaze direction and an environment around a wearable device(2) a non-transitory machine readable medium comprising a set of instructions for determining a correspondence between a gaze direction and an environment around a wearable device. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram illustrating a method for updating an input parameter to identify predefined images in a scene image based on user feedback.Method for determining correspondence between gaze direction and environment around wearable device (300)Step for receiving scene image from outward facing image sensor (310)Step for determining gaze direction of wearer of wearable device with eye tracking device at point in time when scene image is captured by outward facing image sensor (320)Step for determining whether scene image includes remaining portion of predefined image based on input parameter (340)",,,
,,FAABORG A J; CLEMENT M C; MCKENZIE C,"Method for dynamically switching and merging of e.g. hand/arm gestures, in immersive virtual reality system, involves shifting selection to virtual object of virtual objects, and maintaining selection of virtual object in response to input","NOVELTY - The method involves receiving a first input implementing a first input mode of input modes. A first virtual object of virtual objects are selected in response to the first input. A second input implementing a second input mode of the input modes is received. A priority value of the second input mode and a priority value of the first input mode are compared. Selection of the first virtual object is released, selection is shifted to a second virtual object of the virtual objects and selection of the first virtual object is maintained in response to the second input based on comparison. USE - Method for dynamically switching and merging of head, gesture and touch input i.e. physical interaction such as hand/arm gestures, head movement and head or eye directional gazes, in an immersive virtual reality system for generating three-dimensional (3D) immersive environment. ADVANTAGE - The method enables paring user manipulation of a handheld electronic device with head mounted display to allow the user to interact with 3D virtual immersive experience generated by the head mounted display. DETAILED DESCRIPTION - The input modes include a head gaze input mode, an eye gaze input mode, a point input mode, a reach input mode, and a gesture input mode. INDEPENDENT CLAIMS are also included for the following:(1) an apparatus for dynamically switching and merging of head, gesture and touch input in an immersive virtual reality system(2) a computer-readable storage medium comprising a set of instructions for dynamically switching and merging of head, gesture and touch input in an immersive virtual reality system. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for providing dynamic switching between user input modes in a virtual reality environment.Step for initiating immersive virtual experience (1310)Step for determining if first input is received using first input mode (1320)Step for setting an d maintaining list of objects (1330)Step for determining if dynamic receiving event is received (1340)Step for morphing of objects in list for interaction using second input mode (1350)",,,
,2018,"Ozturk, Mahiye Uluyagmur; Arman, Ayse Rodopman; Bulut, Gresa Carkaxhiu; Findik, Onur Tugce Poyraz; Yilmaz, Sultan Seval; Genc, Herdem Aslan; Yazgan, M. Yanki; Teker, Umut; Cataltepe, Zehra",Statistical Analysis and Multimodal Classification on Noisy Eye Tracker and Application Log Data of children with Autism and ADHD,"Emotion recognition behavior and performance may vary between people with major neurodevelopmental disorders such as Autism Spectrum Disorder (ASD), Attention Deficit Hyperactivity Disorder (ADHD) and control groups. It is crucial to identify these differences for early diagnosis and individual treatment purposes. This study represents a methodology by using statistical data analysis and machine learning to provide help to psychiatrists and therapists on the diagnosis and individualized treatment of participants with ASD and ADHD. In this paper we propose an emotion recognition experiment environment and collect eye tracker fixation data together with the application log data (APL). In order to detect the diagnosis of the participant we used classification algorithms with the Tomek links noise removing method. The highest classification accuracy results were reported as 86.36% for ASD vs. Control, 81.82% for ADHD vs. Control and 70.83% for ASD vs. ADHD. This study provides evidence that fixation and APL data have distinguishing features for the diagnosis of ASD and ADHD.",,,
10.3390/s19163465,2019,"Pongsakornsathien, Nichakorn; Lim, Yixiang; Gardi, Alessandro; Hilton, Samuel; Planke, Lars; Sabatini, Roberto; Kistan, Trevor; Ezer, Neta",Sensor Networks for Aerospace Human-Machine Systems,"Intelligent automation and trusted autonomy are being introduced in aerospace cyber-physical systems to support diverse tasks including data processing, decision-making, information sharing and mission execution. Due to the increasing level of integration/collaboration between humans and automation in these tasks, the operational performance of closed-loop human-machine systems can be enhanced when the machine monitors the operator's cognitive states and adapts to them in order to maximise the effectiveness of the Human-Machine Interfaces and Interactions (HMI2). Technological developments have led to neurophysiological observations becoming a reliable methodology to evaluate the human operator's states using a variety of wearable and remote sensors. The adoption of sensor networks can be seen as an evolution of this approach, as there are notable advantages if these sensors collect and exchange data in real-time, while their operation is controlled remotely and synchronised. This paper discusses recent advances in sensor networks for aerospace cyber-physical systems, focusing on Cognitive HMI2 (CHMI2) implementations. The key neurophysiological measurements used in this context and their relationship with the operator's cognitive states are discussed. Suitable data analysis techniques based on machine learning and statistical inference are also presented, as these techniques allow processing both neurophysiological and operational data to obtain accurate cognitive state estimations. Lastly, to support the development of sensor networks for CHMI2 applications, the paper addresses the performance characterisation of various state-of-the-art sensors and the propagation of measurement uncertainties through a machine learning-based inference engine. Results show that a proper sensor selection and integration can support the implementation of effective human-machine systems for various challenging aerospace applications, including Air Traffic Management (ATM), commercial airliner Single-Pilot Operations (SIPO), one-to-many Unmanned Aircraft Systems (UAS), and space operations management.",,,
,2019,"O'Connor, Patrick; Meekhof, Casey; McBride, Chad; Mei, Christopher; Bamji, Cyrus; Rohn, Dave; Strande, Hakon; Forrester, Justin; Fenton, Mike; Haraden, Ryan; Ozguner, Tolga; Perry, Travis",Custom Silicon and Sensors Developed for a 2nd Generation Mixed Reality User Interface,"Microsoft Hololens 2, like its predecessor, is an untethered holographic mixed reality (MR) headset that transforms the way we communicate, create, and explore. Hololens 2 advances MR ergonomics, intuitive interactions, and immersion. We describe the custom sensors and compute silicon developed to give hands-free user control of the headset and applications. With 3D Time of Flight (TOF) depth sensing, eye tracking and spatial array microphones, working with low power compute blocks aggregated in a custom ASIC, the hardware enables a comfortable, low latency user interface that sets the user free to focus on their work. We conclude with a look at how these building blocks can enable further innovation in the Intelligent Edge.",,,
,,POSTNIKOV A; BARNIDGE T J; ETHERINGTON T J; WENGER J C; TCHON J L,"Method for stereoscopic three dimensional (S3D) display of multiple layers of information to operator, involves displaying multiple objects based on transitioning, second placement and second order","NOVELTY - The method involves transitioning (1320) an object from a more proximal depth layer to a more distal depth layer and vice versa. A second placement and second order are determined (1322) based on a second rule set in the one or more display hierarchy rule sets associated with a second operator. Multiple objects based on the transitioning, second placement and second order are displayed (1324) in the S3D display of operational data in the vehicle. USE - Method for stereoscopic 3-D (S3D) display of multiple layers of information to operator. ADVANTAGE - The enhanced visual capabilities of the system are achieved by providing the operator eye tracking for the autostereoscopic presentation. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for stereoscopic 3-D (S3D) display of multiple layers of information to an operator. DESCRIPTION Of DRAWING(S) - The drawing shows a flow diagram of the method for displaying synthetic environments with stereoscopic avionics displays.Step for displaying the objects within the lowest level of the presentation hierarchy (1316)Step for displaying the objects within the intermediate levels of the presentation hierarchy (1318)Step for transitioning the object from the more proximal depth layer to the more distal depth layer (1320)Step for determining the second placement and second order (1322)Step for displaying the multiple objects (1324)",,,
,,HU Y; RAN M; HUO K,"Binocular identifying driver state detection system, has vehicle control device for controlling vehicle movement, where vehicle control device is provided with initial data storage unit for storing initial state information of driver","NOVELTY - The system has a binocular camera for detecting image information and depth information of a driver. A controller performs 3D modeling on the image information and the depth information for obtaining state information of the driver. A navigation system is connected with the vehicle control device. A control unit determines whether identity information is matched with initial identity information. The vehicle control device controls vehicle movement according to the state information. A vehicle control device is provided with an initial data storage unit for storing initial state information of the driver. USE - Binocular identifying driver state detection system. ADVANTAGE - The system has wide application range, and increases camera arrangement position of a possible area and integrates identity recognition, gesture recognition, eye tracking and fatigue driving. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a binocular identifying driver state detection method. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of a binocular identifying driver state detection system. '(Drawing includes non-English language text)'",,,
,,ALIABADI A; ROBERTS G D,"Method for efficient implementation of convolutional layer of convolutional neural network, involves determining output activation maps of convolutional layer based on kernels and input activation maps","NOVELTY - The method (1000) involves receiving (1004) a convolutional layer of a convolutional neural network. The convolutional layer comprises kernels in a kernel stack. The input activation maps of the convolutional layer are received (1012). The output activation maps of the convolutional layer are determined based on the kernels and the input activation maps. The output activation maps are in an interleaved output activation map layout. The input activation maps are in an interleaved input activation map layout. USE - Method for efficient implementation of convolutional layer of convolutional neural network on computing devices such as computer server, personal computer, tablet computer, mobile device or embedded device for augmented reality, mixed reality, virtual reality, machine learning, computer vision, facial recognition, eye tracking, object recognition, character, language or speech analysis, etc. ADVANTAGE - The method can reduce power consumption of the computing device implementing the convolutional neural network. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for efficient implementation of convolutional layer of convolutional neural network. DESCRIPTION Of DRAWING(S) - The drawing shows the flow diagram of the method for efficient implementation of convolutional layer of convolutional neural network.Method for efficient implementation of convolutional layer of convolutional neural network (1000)Step for receiving convolutional layer of convolutional neural network (1004)Step for receiving the input activation maps of the convolutional layer (1012)Step for reordering weight values of the input activation maps (1016)Step for computing output activation maps of the convolutional layer tile by tile (1020)",,,
10.1016/j.cognition.2019.03.003,2019,"Suzuki, Keisuke; Schwartzman, David J.; Augusto, Rafael; Seth, Anil K.",Sensorimotor contingency modulates breakthrough of virtual 3D objects during a breaking continuous flash suppression paradigm,"To investigate how embodied sensorimotor interactions shape subjective visual experience, we developed a novel combination of Virtual Reality (VR) and Augmented Reality (AR) within an adapted breaking continuous flash suppression (bCFS) paradigm. In a first experiment, participants manipulated novel virtual 3D objects, viewed through a head-mounted display, using three interlocking cogs. This setup allowed us to manipulate the sensorimotor contingencies governing interactions with virtual objects, while characterising the effects on subjective visual experience by measuring breakthrough times from bCFS. We contrasted the effects of the congruency (veridical versus reversed sensorimotor coupling) and contingency (live versus replayed interactions) using a motion discrimination task. The results showed that the contingency but not congruency of sensorimotor coupling affected breakthrough times, with live interactions displaying faster breakthrough times. In a second experiment, we investigated how the contingency of sensorimotor interactions affected object category discrimination within a more naturalistic setting, using a motion tracker that allowed object interactions with increased degrees of freedom. We again found that breakthrough times were faster for live compared to replayed interactions (contingency effect). Together, these data demonstrate that bCFS breakthrough times for unfamiliar 3D virtual objects are modulated by the contingency of the dynamic causal coupling between actions and their visual consequences, in line with theories of perception that emphasise the influence of sensorimotor contingencies on visual experience. The combination of VR/AR and motion tracking technologies with bCFS provides a novel methodology extending the use of binocular suppression paradigms into more dynamic and realistic sensorimotor environments.",,,
10.1080/0144929X.2018.1488993,2018,"Seo, Young-nam; Kim, Minkyung; Lee, Doohwang; Jung, Younbo",Attention to eSports advertisement: effects of ad animation and in-game dynamics on viewers' visual attention,"ESports refers to professional video gaming that is typically broadcasted with a live commentary on TV or the Internet. Despite the rapid growth of eSports industry and its potential for a valuable advertising platform, there has not been much discussion about effective advertising strategies for the placement of virtual ads in eSports. Based on previous research, one advertisement factor (i.e. ad animation) and one in-game factor (i.e. in-game dynamics) are identified and tested in an experiment (N=116) with a 2 (ad animation: static vs. animated ads)x2 (in-game dynamics: battle vs. non-battle scenes) mixed design. The results show that both ad animation and in-game dynamics have a significant impact on viewers' visual attention, in terms of fixation count and duration measured by an eye-tracking device. The results also show a significant interaction in which the impact of ad animation on fixation duration is more magnified when in-game play is less busy (i.e. non-battle scenes), compared to when it is busy (i.e. battle scenes). Theoretical implications based on the limited-capacity model, as well as practical implications for practitioners are discussed.",,,
,,ANGERMAYER L; MITTERHOEFER S; KEILWERT S; ANGEMAYER L,"Electronic wagering gaming apparatus for three-dimensional display for e.g. slot machine, in casino, has processor for determining nearest point on reference plane and distance from point on object mesh and displaying equalized scene","NOVELTY - The apparatus has a processor for determining a nearest point on a reference plane and distance from a point on an object mesh to a nearest point on the reference plane, defining a linear viewing ray connecting the reference viewpoint and the nearest point on the reference plane, and warping the object mesh by moving the point on the object mesh to a location on the viewing ray while maintaining the distance from the point on the object mesh to the nearest point on the reference plane. The processor displays an equalized three dimensional (3D) scene through a 3D display device. USE - Electronic wagering gaming apparatus for 3D display for an on-line gaming system or electronic gaming machine (EGM) such as slot machine, utilized with a computer device such as desktop computer and laptop, a mobile device e.g. tablet computer and smart phone, and a TV in a casino. ADVANTAGE - The apparatus utilizes stereoscopic displays to present different views of a displayed environment to a viewer's left and right eyes, thus providing a viewer with stereo parallax information about the environment. The apparatus utilizes auto-stereoscopic displays to utilize head-tracking and/or eye-tracking to locate a viewer's head and/or eyes and to adjust the display, so that views of the environment are continually directed to the viewer's eyes even as viewer's head moves. The apparatus utilizes dynamic viewing zones to improve a pixel resolution in each viewing zone by reducing or eliminating a number of wasted pixels. The apparatus automatically performs distortion compensation when a degree of tilt of a virtual game component is adjusted, so that designated front perspective view of a virtual game component is maintained in a view from a virtual camera while adjusting the tilt of the virtual game component. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for warping a virtual 3D object for distortion compensation in a wagering gaming system. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an EGM linked to a host system.EGM (20)Host system (41)Communications board (42)Game controller board (44)Displays (53)",,,
10.3389/fnbot.2021.598895,2021,"Arslan Aydin, Ulku; Kalkan, Sinan; Acarturk, Cengiz",Speech Driven Gaze in a Face-to-Face Interaction,"Gaze and language are major pillars in multimodal communication. Gaze is a non-verbal mechanism that conveys crucial social signals in face-to-face conversation. However, compared to language, gaze has been less studied as a communication modality. The purpose of the present study is 2-fold: (i) to investigate gaze direction (i.e., aversion and face gaze) and its relation to speech in a face-to-face interaction; and (ii) to propose a computational model for multimodal communication, which predicts gaze direction using high-level speech features. Twenty-eight pairs of participants participated in data collection. The experimental setting was a mock job interview. The eye movements were recorded for both participants. The speech data were annotated by ISO 24617-2 Standard for Dialogue Act Annotation, as well as manual tags based on previous social gaze studies. A comparative analysis was conducted by Convolutional Neural Network (CNN) models that employed specific architectures, namely, VGGNet and ResNet. The results showed that the frequency and the duration of gaze differ significantly depending on the role of participant. Moreover, the ResNet models achieve higher than 70% accuracy in predicting gaze direction.",,,
,,KIM H H; CHOI K S; KIM S S; HEO S J; HWANG S H,"Integrated helmet mounted display system for aircraft, has hour subtense part equipped in front region of helmet in pilot, where hour subtense part for indicating image, which is delivered from helmet control unit","NOVELTY - The system has a main control unit (100) mounted in an inner side of an aircraft. A helmet control unit (200) is equipped in a helmet that is mounted on an aircraft pilot head. The main control unit delivers a three-dimensional (3D) digital map image and a symbol data to the aircraft pilot. An image processing unit implements in an hour subtense part (300) to process all connected data in the helmet. The hour subtense part is equipped in a front region of the helmet in the pilot. The hour subtense part indicates the image, which is delivered from the helmet control unit. USE - Integrated helmet mounted display system for aircraft. ADVANTAGE - The system reflects trace result of the helmet through an organic coupling between each configuration so as to indicate various information corresponding to a pilot eye-gaze direction in the hour subtense part, so that operation safety and reliability can be increased. The system captures a season among aviation in a find of enemy plane along a downward direction or an upper direction without the aircraft cardinal number change so as to release a short distance missile launching, so that reduction of time and ability of capturing can be increased. The system transmits an image signal through electrical communication line so as to convert the optical signal, so that communication line is simplified and a traffic load can be reduced by performing massive data communication. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an integrated helmet mounted display system for aircraft. '(Drawing includes non-English language text)'Main control unit (100)Helmet control unit (200)Hour subtense part (300)Photographing unit (400)Tracking camera (500)",,,
,,BANKS M; CHOLEWIAK S; DRETTAKIS G; KOULIERIS G; LOVE G; NG R; SRINIVASAN P; NG Y; BANKS M S; CHOLEWIAK S A; LOVE G D,"Pseudo light-field display apparatus for use in head-mounted and desktop displays, has controller to adjust blur such that part of displayed image of stereoscopic display is in sharp focus and points near and far displayed image are blurred","NOVELTY - The display apparatus has a controller (126) configured to control blur rendered in the displayed image (128) on a stereoscopic display. As the user's first eye accommodates to different focal lens, the controller adjusts blur such that a part of the displayed image that should be in focus at the user's first eye will be in sharp focus and points nearer and farther in the stereoscopic display image will be appropriately blurred. The second eye of the user is maintained in focus with the stereoscopic display regardless of first eye changes in focus by changes in a first adjustable lens. USE - Pseudo light-field display apparatus for use in head-mounted displays and desktop displays. Can be used in e.g. virtual reality (VR) displays and augmented reality (AR) displays. ADVANTAGE - Provides a pseudo light-field display apparatus that allows the creation of correct focus cues with a display, a dynamic lens in front of each eye, and a method to measure the current focus of the eye or to estimate the current focus from the measurement of the gaze direction of each eye. Allows the creation of a display that supports focus cues with mostly commercially available and inexpensive equipment. Allows a practical display that supports focus cues, reduces visual discomfort, and improves visual performance with bright, non-flickering, and high-resolution imagery. Reduces major problems existing with current three-dimensional (3D) display technologies that do not support focus cues. Provides less expensive and more practical solution compared to current volumetric, multi-plane, and light-field displays. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a focus tracking display system;(2) an eye tracking display system;(3) a focus tracking display method;(4) an eye tracking display method; and(5) a pseudo light-field display method. DESCRIPTION Of DRAWING(S) - The drawing shows a top schematic view of a focus tracking display system.Display screen (102)Right and left half-silvered mirrors (108,110)Adjustable lenses (112,114)Controller (126)Displayed image (128)",,,
10.1364/OE.410374,2020,"Li, Zhenghan; Pandiyan, Vimal Prabhu; Maloney-Bertelli, Aiden; Jiang, Xiaoyun; Li, Xinyang; Sabesan, Ramkumar",Correcting intra-volume distortion for AO-OCT using 3D correlation based registration,"Adaptive optics (AO) based ophthalmic imagers, such as scanning laser ophthal-moscopes (SLO) and optical coherence tomography (OCT), are used to evaluate the structure and function of the retina with high contrast and resolution. Fixational eye movements during a raster-scanned image acquisition lead to intra-frame and intra-volume distortion, resulting in an inaccurate reproduction of the underlying retinal structure. For three-dimensional (3D) AO-OCT, segmentation-based and 3D correlation based registration methods have been applied to correct eye motion and achieve a high signal-to-noise ratio registered volume. This involves first selecting a reference volume, either manually or automatically, and registering the image/volume stream against the reference using correlation methods. However, even within the chosen reference volume, involuntary eye motion persists and affects the accuracy with which the 3D retinal structure is finally rendered. In this article, we introduced reference volume distortion correction for AO-OCT using 3D correlation based registration and demonstrate a significant improvement in registration performance via a few metrics. Conceptually, the general paradigm follows that developed previously for intra-frame distortion correction for 2D raster-scanned images, as in an AOSLO, but extended here across all three spatial dimensions via 3D correlation analyses. We performed a frequency analysis of eye motion traces before and after intra-volume correction and revealed how periodic artifacts in eye motion estimates are effectively reduced upon correction. Further, we quantified how the intra-volume distortions and periodic artifacts in the eye motion traces, in general, decrease with increasing AO-OCT acquisition speed. Overall, 3D correlation based registration with intra-volume correction significantly improved the visualization of retinal structure and estimation of fixational eye movements. (C) 2020 Optical Society of America under the terms of the OSA Open Access Publishing Agreement",,,
,,RABINOVICH A; BADRINARAYANAN V; RAJENDRAN S; LEE C; LEE C Y,"System for training multi-task neural network using meta-learning, has hardware processor that reduces or minimizes multi-task loss function based on estimated task weights, and outputs trained multi-task neural network","NOVELTY - The system has a multi-task neural network that determines output associated with the tasks. A meta-network (204) outputs the task weights associated with each of the tasks such that the meta-network associated with a meta-network loss function includes an expected loss based on the multi-task loss function. A hardware processor accesses training data associated with the reference task outputs for the tasks. The meta-network loss function is reduced or minimized to determine estimated task weights associated with each of the tasks. The multi-task loss function is reduced or minimized based on the estimated task weights and outputs a trained multi-task neural network. USE - System for training multi-task neural network using meta-learning. ADVANTAGE - The meta-learning approach can learn to adapt task loss balancing weights in the course of training to get improved performance on multiple tasks on real world datasets. The learning objective can increase or maximize the learning efficiency of all the tasks and their individual performance. The machine learning method enables robust and accurate solutions to a wide variety of problems, including eye image segmentation or eye tracking. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for using meta-learning and a meta-network to train a child neural network; and(2) a head mounted display system for training a neural network to learn a set of tasks. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram illustrating a meta-learning system with a meta-network and child network.Meta-learning system (200)Meta-network (204)Child network (208)",,,
10.1007/s11257-019-09244-5,2019,"Toker, Dereck; Conati, Cristina; Carenini, Giuseppe",Gaze analysis of user characteristics in magazine style narrative visualizations,"Previous research has shown that various user characteristics (e.g., cognitive abilities, personality traits, and learning abilities) can influence user experience during information visualization tasks. These findings have prompted researchers to investigate user-adaptive information visualizations that can help users by providing personalized support based on their specific needs. Whereas existing work has been mostly limited to tasks involving just visualizations, the aim of our research is to broaden this work to include scenarios where users process textual documents with embedded visualizations, i.e., Magazine Style Narrative Visualizations, or MSNVs for short. In this paper, we analyze eye tracking data collected from a user study with MSNVs to uncover processing behaviors that are negatively impacting user experience (i.e., time on task) for users with low abilities in these user characteristics. Our analysis leverages Linear Mixed-Effects Models to evaluate the relationships among user characteristics, gaze processing behaviors, and task performance. Our results identify several MSNV processing behaviors within the visualization that contribute to poor task performance for users with low reading proficiency. For instance, we identify that users with low reading proficiency transition significantly more often compared to their counterparts between relevant and non-relevant bars, and transition more often from bars to the labels. We present our findings as a step toward designing user-adaptive support mechanisms to alleviate these difficulties with MSNVs, and provide suggestions on how our results can be leveraged for creating a set of meaningful interventions for future evaluation (e.g., dynamically highlighting relevant bars and labels in the visualization to help users with low reading proficiency locate them more effectively).",,,
10.1016/j.foodres.2018.02.033,2019,"Siegrist, Michael; Ung, Chin-Yih; Zank, Markus; Marinello, Max; Kunz, Andreas; Hartmann, Christina; Menozzi, Marino",Consumers' food selection behaviors in three-dimensional (3D) virtualin reality,"Virtual reality (VR) can be a useful tool for conducting consumer behavior experiments. The aim of this research was to examine whether people standing in front of a supermarket shelf make similar decisions and process similar information as those in front of a shelf in a VR shop. In Study 1, participants were asked to select a cereal from among 33 commercially available types of cereals placed on a shelf. One group performed the task in front of a real shelf, while the other performed it in VR. Eye-tracking data were collected for both groups. No statistically significant differences were observed in the selection of the cereals by the two groups in the two conditions. Eye-tracking data only revealed few differences in the information-seeking behavior. In Study 2, results observed using real products were replicated in VR. Participants were asked to walk through a virtual supermarket and select either a healthy cereal (healthy condition) or a tasty cereal (hedonic condition). Results showed that participants in the healthy condition paid more attention to the nutrition information than those in the hedonic condition. The results of these two experiments suggest that a VR condition wherein participants can walk around and behave as in the real world is a useful tool for conducting experiments related to food decisions.",,,
,,WOO H,"Control method for glass type wearable XR device e.g. augmented reality (AR) glass, involves detecting specific object located in recognized specific direction among objects and enlarging and displaying detected specific object","NOVELTY - The method involves displaying (S2710) object located in an arbitrary first direction through a first camera. The user of the XR device located in second direction different from first direction is photographed (S2720) through second camera. The specific gesture and specific direction of user of the XR device is recognized based on data stored in memory. The specific object located in the recognized specific direction is detected (S2750) among objects and the detected specific object is enlarged and displayed (S2780). The first direction corresponds to direction viewed by user wearing the glass type XR device and the second direction corresponds to user's eyes direction around the XR device. USE - Control method for glass type wearable XR device (claimed) e.g. augmented reality (AR) glass. Uses include but not limited to head-mount display (HMD), head-up display (HUD) installed in vehicle, TV, mobile phone, smartphone, computer, wearable device, home appliance, digital signage, fixed robot, mobile robot, etc. ADVANTAGE - The desired actual/virtual objects are expanded and provided, by user while minimizing a user's action. The deep learning technique is used to provide solution that more accurately detects user's intention. The beamforming weight vector/precoding vector is applied to reduce the complexity of hardware implementation, increase performance using multiple antennas, flexibility of resource allocation, and ease of beam control by frequency. The data processing speed is improved and the power consumption is reduced, by minimizing unnecessary eye tracking technology. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a glass type wearable XR device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating the control method of AR glass type XR device. (Drawing includes non-English language text)Step for displaying object located in arbitrary first direction through first camera (S2710)Step for photographing user of XR device located in second direction different from first direction through second camera (S2720)Step for determining whether the pupil of the user stares in specific direction for predetermined time or more (S2740)Step for detecting specific object located in the recognized specific direction (S2750)Step for enlarging and displaying detected specific object (S2780)",,,
10.1007/s10489-018-1393-x,2019,"Kamimura, Ryotaro; Takeuchi, Haruhiko",Sparse semi-autoencoders to solve the vanishing information problem in multi-layered neural networks,"The present paper aims to propose a new neural network called sparse semi-autoencoder to overcome the vanishing information problem inherent to multi-layered neural networks. The vanishing information problem represents a natural tendency of multi-layered neural networks to lose information in input patterns as well as training errors, including also natural reduction in information due to constraints such as sparse regularization. To overcome this problem, two methods are proposed here, namely, input information enhancement by semi-autoencoders and the separation of error minimization and sparse regularization by soft pruning. First, we try to enhance information in input patterns to prevent the information from decreasing when going through multi-layers. The information enhancement is realized in a form of new architecture called semi-autoencoders, in which information in input patterns is forced to be given to all hidden layers to keep the original information in input patterns as much as possible. Second, information reduction by the sparse regularization is separated from a process of information acquisition as error minimization. The sparse regularization is usually applied in training autoencoders, and it has a natural tendency to decrease information by restricting the information capacity. This information reduction in terms of the penalties tends to eliminate even necessary and important information, because of the existence of many parameters to harmonize the penalties with error minimization. Thus, we introduce a new method of soft pruning, where information acquisition of error minimization and information reduction of sparse regularization are separately applied without a drastic change in connection weights, as is the case of the pruning methods. The two methods of information enhancement and soft pruning try jointly to keep the original information as much as possible and particularly to keep necessary and important information by enabling the making of a flexible compromise between information acquisition and reduction. The method was applied to the artificial data set, eye-tracking data set, and rebel forces participation data set. With the artificial data set, we demonstrated that the selectivity of connection weights increased by the soft pruning, giving sparse weights, and the final weights were naturally interpreted. Then, when it was applied to the real data set of eye tracking, it was confirmed that the present method outperformed the conventional methods, including the ensemble methods, in terms of generalization. In addition, for the eye-tracking data set, we could interpret the final results according to the conventional eye-tracking theory of choice process. Finally, the rebel data set showed the possibility of detailed interpretation of relations between inputs and outputs. However, it was also found that the method had the limitation that the selectivity by the soft pruning could not be increased.",,,
10.1109/VR50410.2021.00045,2021,"Wang, Miao; Ye, Zi-Ming; Shi, Jin-Chuan; Yang, Yang-Liang",Scene-Context-Aware Indoor Object Selection and Movement in VR,"Virtual reality (VR) applications such as interior design typically require accurate and efficient selection and movement of indoor objects. In this paper, we present an indoor object selection and movement approach by taking into account scene contexts such as object semantics and interrelations. This provides more intelligence and guidance to the interaction, and greatly enhances user experience. We evaluate our proposals by comparing them with traditional approaches in different interaction modes based on controller, head pose, and eye gaze. Extensive user studies on a variety of selection and movement tasks are conducted to validate the advantages of our approach. We demonstrate our findings via a furniture arrangement application.",,,
,,MOTTA R J; MILLER B D; RICK T; SRIKANTH M B,"Mixed reality display system for displaying stereoscopic scenes to users, involves controller for rendering frames for display by head-mounted display that include virtual content composited into captured views of users environment","NOVELTY - The system (10) has a controller comprising multiple processors. A head-mounted display (HMD) displays (100) a three-dimensional (3D) virtual view (102) to a user (190). Left and right displays display frames including left and right images to users eyes to provide the 3D virtual view to the user. A set of sensors (150) collects information about the user and the user's environment. Multiple cameras capture views of the user's environment. Multiple world mapping sensors (140) determines range information for objects in the user's environment. Multiple eye tracking sensors track position and movement of the user's eyes. The controller renders frames for display by the HMD that include virtual content (110) composited into the captured views of the user's environment. USE - Mixed reality display system for displaying stereoscopic scenes to users in a computing device. Uses include but are not limited to a desktop computer, a notebook, a laptop computer, a tablet device, a smartphone, a hand-held computing device and a game controller. ADVANTAGE - The system utilizes range information so as to adjust depth of real objects in the user's environment, so that objects are re-rendered in the display, thus avoiding the objects when moving about in the environment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a device for displaying stereoscopic scenes to users(2) a method for displaying stereoscopic scenes to users. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a mixed reality display system.Mixed reality display system (10)HMD display (100)Virtual view (102)Virtual content (110)World mapping sensors (140)Sensors (150)User (190)",,,
10.1186/s40708-020-00109-x,2020,"Rawnaque, Ferdousi Sabera; Rahman, Khandoker Mahmudur; Anwar, Syed Ferhat; Vaidyanathan, Ravi; Chau, Tom; Sarker, Farhana; Mamun, Khondaker Abdullah Al",Technological advancements and opportunities in Neuromarketing: a systematic review.,"Neuromarketing has become an academic and commercial area of interest, as the advancements in neural recording techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response of consumers to the marketing stimuli. This article presents the very first systematic review of the technological advancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a total of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic or empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both product and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band signals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha asymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional magnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to its low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, skin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical studies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component analysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and classification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) have performed with the highest average accuracy among other machine learning algorithms used in these literatures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarketing for making novel contributions.",,,
10.1016/j.engappai.2020.103609,2020,"Fu, Xianping; Yan, Yuxiao; Yan, Yang; Peng, Jinjia; Wang, Huibing",Purifying real images with an attention-guided style transfer network for gaze estimation,"Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. Image synthesis has been widely accepted as a cost effective way to learn models because it provides training sets that are large, diverse and accurately labeled. However, the realism of the synthetic image is not enough, this affects generalization on naturalistic test image. In an attempt to address this issue, previous methods learn a model to improve the realism of synthetic image. Different from previous methods, we take the first step towards purifying the real image to weaken the influence of light and convert the distribution of an outdoor naturalistic image through a real-time style transfer task to that of indoor synthetic image. In this paper, we first introduce the segmentation masks to construct Red, Green, and Blue-mask (RGB-mask) pairs as inputs, then we design an attention-guided style transfer network to learn style features separately from the attention and background regions, learn content features from full and attention regions. Moreover, we propose a novel region-level task-guided loss to restrain the features learnt from style and content. Experiments were performed using a mixed research (qualitative and quantitative) method to demonstrate the possibility of purifying real images in complex directions. We evaluate the proposed method on three public datasets, including Labeled pupils in the wild (LPW), Common Objects in COntext (COCO) and MPIIGaze. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results.",,,
10.1109/CVPRW.2019.00111,2019,"Zelinsky, Gregory; Yang, Zhibo; Huang, Lihan; Chen, Yupei; Ahn, Seoyoung; Wei, Zijun; Adeli, Hossein; Samaras, Dimitris; Minh Hoai",Benchmarking Gaze Prediction for Categorical Visual Search,"The prediction of human shifts of attention is a widely-studied question in both behavioral and computer vision, especially in the context of a free viewing task. However search behavior where the,fixation scanpaths are highly dependent on the viewer's goals, has received far less attention, even though visual search constitutes much of a person's everyday behavior One reason for this is the absence of real-world image datasets on which search models can be trained. In this paper we present a carefully created dataset for two target categories, microwaves and clocks, curated from the COCO2014 dataset. A total of 2183 images were presented to multiple participants, who were tasked to search for one of the two categories. This yields a total of 16184 validated fixations used, training, making our microwave-clock dataset currently one of the lamest datasets of eve fixations in categorical search. We also present a 40-image testing dataset, where images depict both a microwave and a clock target. Distinct fixation patterns emerged depending on whether participants searched for a microwave (n=30) or a clock (n=30) in the same images, meaning that models need to predict different search scanpaths from the same pixel inputs. We report the results of several state-of-the-art deep network models that were trained and evaluated on these datasets. Collectively, these datasets and our protocol for evaluation provide what we hope will be a useful test-bed for the development of new methods for predicting category-specific visual search behavior.",,,
,,KO D,"Wearable device e.g. goggle form wearable device for indicating three dimensional virtual reality image, has lens combined in frame and projecting generated image, where part of inner side of lens is coated with infrared cutting filter","NOVELTY - The device has a frame for forming an outer tube. A display (300) is equipped in the frame and formed to produce an image. A lens (200) is combined in the frame and projects the generated image. An infrared (IR) camera module (400) senses a position of pupil of a user and is connected to an inner side of the lens. A part of an inner side of the lens is coated with an IR cutting filter (600). A control unit produces constellation of the sensed pupil of the user through the IR camera module and is electrically connected to the IR camera module. A mirror (510, 520) reflects the generated image from the display to the inner side of the lens. A part of the mirror is coated with the IR cutting filter. The IR camera module is adjacent to a display section to face the mirror that is provided with first and second mirrors. USE - Wearable device e.g. glasses form wearable device and goggle form wearable device for indicating three dimensional (3D) virtual reality (VR) image and augmented reality (AR) image. ADVANTAGE - The device improves arrangement degree of freedom of the IR camera through the IR ray mirror, secures focal distance of the IR camera through the IR ray mirror and correctly senses the traced eye gaze of the user. The device prevents need of a separate lens for the focal distance of the IR camera so as to reduce unit cost of the product. The arrangement degree of freedom of the infrared camera is improved so as to improve degree of freedom of the exterior design of the device. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a wearable device.Lens (200)Display (300)IR camera module (400)Mirror (510, 520)IR cutting filter (600)",,,
,,LACEY P; MILLER S A; KRAMER N A; LUNDMARK D C,"Wearable system for performing transmodal filtering scheme, has processor that determines user interface operation on target virtual object based on one of first user input data, second user input data, third user input data, and vergence","NOVELTY - The system (200) has a first sensor configured to acquire first user input data in a first mode of input. A second sensor acquires second user input data in a second mode of input. A third sensor acquires third user input data in a third mode of input. A hardware processor is programmed to receive multiple inputs comprising the first user input data in the first mode of input, the second user input data in the second mode of input, and the third user input data in the third mode of input. The processor identifies a target virtual object from a set of candidate objects in a three-dimensional (3D) region around the wearable system, determines a user interface operation on the target virtual object based on one of the first user input data, the second user input data, the third user input data, and the vergence, and generates a transmodal input command that causes the user interface operation to be performed on the target virtual object. USE - Wearable system e.g. head mounted wearable system for performing transmodal filtering scheme. ADVANTAGE - The system can fuse together head pose, eye pose, and hand gesture inputs, thus allowing the user to look and point to select an object while using the head and eye gaze pose inputs to increase the accuracy of the hand gesture input. The wearable system can reduce or minimize latency by detecting when the user's hand is within view of the depth sensor, automatically switching the depth sensor to the appropriate gesture mode, and then giving feedback to the user when he or she can perform the gesture. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for performing transmodal filtering scheme. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a wearable system that can be configured to use the transmodal input fusion.Wearable system (200)User (210)Display element (220)Frame (230)Speaker (240)",,,
10.1109/CVPR.2019.00318,2019,"Wang, Wenguan; Song, Hongmei; Zhao, Shuyang; Shen, Jianbing; Zhao, Sanyuan; Hoi, Steven C. H.; Ling, Haibin",Learning Unsupervised Video Object Segmentation through Visual Attention,"This paper conducts a systematic study on the role of visual attention in the Unsupervised Video Object Segmentation (UVOS) task. By elaborately annotating three popular video segmentation datasets (DAVIS(16), Youtube-Objects and SegTrack(v2)) with dynamic eye-tracking data in the UVOS setting, for the first time, we quantitatively verified the high consistency of visual attention behavior among human observers, and found strong correlation between human attention and explicit primary object judgements during dynamic, task-driven viewing. Such novel observations provide an in-depth insight into the underlying rationale behind UVOS. Inspired by these findings, we decouple UVOS into two sub-tasks: UVOS-driven Dynamic Visual Attention Prediction (DVAP) in spatiotemporal domain, and Attention-Guided Object Segmentation (AGOS) in spatial domain. Our UVOS solution enjoys three major merits: 1) modular training without using expensive video segmentation annotations, instead, using more affordable dynamic fixation data to train the initial video attention module and using existing fixation-segmentation paired static/image data to train the subsequent segmentation module; 2) comprehensive foreground understanding through multi-source learning; and 3) additional interpretability from the biologically-inspired and assessable attention. Experiments on popular benchmarks show that, even without using expensive video object mask annotations, our model achieves compelling performance in comparison with state-of-the-arts.",,,
,,CHAI W; KANG L; LIU Y; WANG C,"Intelligent glasses based video sampling platform, has MCU for controlling camera module and recording module, and user terminal for remotely accessing server for obtaining and displaying video data and image data","NOVELTY - The platform has a camera module and a recording module that are connected with a data transmission module to transmit data. A MCU controls the camera module and the recording module. The camera module is provided with an eye tracking sensor. A 3D image processing module transmits image data to perform editing process. A video database stores video data and the image data after processing the video data and the image data by a video data editing module and the image processing module. A user terminal remotely accesses a server for obtaining and displaying the video data and the image data. USE - Intelligent glasses based video sampling platform. ADVANTAGE - The platform can collect video and image information and audio information in real-time operation process, allows a learner to view real-time visual angle of a remote unit during learning process through limit space of room, and realizes sharing of resource. The platform can automatically track point of interest, and automatically collect video image so as to reduce operation video collecting difficulties. DETAILED DESCRIPTION - The user terminal is a PC terminal or mobile phone terminal. DESCRIPTION Of DRAWING(S) - The drawing shows a block diagram of an intelligent glasses based video sampling platform. '(Drawing includes non-English language text)'",,,
,,TYLER C W; NICHOLAS S C; LIKOVA L T,"Method for tracking positions and angles of gaze of eyes in space from facial information, involves storing values of adjustable parameters including positions, angles of gaze, eyelid angles of closure and pupil sizes of eyes","NOVELTY - The method involves storing values of adjustable parameters including 3D positions, angles of gaze, eyelid angles of closure and pupil sizes of eyes under conditions providing minimum values of combined error values to form trajectories of the adjustable parameters over time such that the trajectories of eyeball positions, the angles of gaze, the eyelid angles of closure, and the pupil radii are tracked over time, where tracking is done without calibration and without a use of any specialized equipment except widely available consumer devices incorporating a camera. USE - Method for tracking 3D positions and angles of gaze of eyes in space from facial information in video or image sequences of a user face such as single frame images from a camera, film sequences from a film camera converted to digital format, digital image sequences from a visible light camera or other image acquisition device and digital image sequences digital image sequences acquired through infrared light of the electromagnetic spectrum captured by consumer devices such as video cameras, TV monitors, smartphones, tablet computers, laptop computers, desktop computers, gaming consoles, virtual reality devices, wearable electronics and inbuilt architectural displays (all claimed). ADVANTAGE - The method enables providing accurate assessment of a horizontal, vertical and oblique angles of rotation of the eyes even when parts of an iris and pupil are hidden by eyelids under extreme angles of gaze encountered in clinical disorders. The method enables utilizing fitting model to a binocular configuration of the eyes independent of movements of the head to allow accurate quantification, under the conditions of an ophthalmic examination from a video taken in ophthalmologist's office of both an ocular geometry and dynamics of the eye movements of patients with binocular coordination problems. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for tracking 3D positions and angles of gaze of eyes in space from facial information in video or other image sequences of an eye region of a face of an individual. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic diagram of a system for tracking a position and gaze angle of eyes relative to a camera in video or other image sequences.Eye-tracking system (100)Video camera (101)Subject (102)Cable (103)Computer (104)",,,
10.3390/s20102944,2020,"Olade, Ilesanmi; Fleming, Charles; Liang, Hai-Ning",BioMove: Biometric User Identification from Human Kinesiological Movements for Virtual Reality Systems,"Virtual reality (VR) has advanced rapidly and is used for many entertainment and business purposes. The need for secure, transparent and non-intrusive identification mechanisms is important to facilitate users' safe participation and secure experience. People are kinesiologically unique, having individual behavioral and movement characteristics, which can be leveraged and used in security sensitive VR applications to compensate for users' inability to detect potential observational attackers in the physical world. Additionally, such method of identification using a user's kinesiological data is valuable in common scenarios where multiple users simultaneously participate in a VR environment. In this paper, we present a user study (n = 15) where our participants performed a series of controlled tasks that require physical movements (such as grabbing, rotating and dropping) that could be decomposed into unique kinesiological patterns while we monitored and captured their hand, head and eye gaze data within the VR environment. We present an analysis of the data and show that these data can be used as a biometric discriminant of high confidence using machine learning classification methods such as kNN or SVM, thereby adding a layer of security in terms of identification or dynamically adapting the VR environment to the users' preferences. We also performed a whitebox penetration testing with 12 attackers, some of whom were physically similar to the participants. We could obtain an average identification confidence value of 0.98 from the actual participants' test data after the initial study and also a trained model classification accuracy of 98.6%. Penetration testing indicated all attackers resulted in confidence values of less than 50% (<50%), although physically similar attackers had higher confidence values. These findings can help the design and development of secure VR systems.",,,
10.1111/bjet.12854,2019,"Sharma, Kshitij; Papamitsiou, Zacharoula; Giannakos, Michail",Building pipelines for educational data using AI and multimodal analytics: A grey-box approach,"Students' on-task engagement during adaptive learning activities has a significant effect on their performance, and at the same time, how these activities influence students' behavior is reflected in their effort exertion. Capturing and explaining effortful (or effortless) behavior and aligning it with learning performance within contemporary adaptive learning environments, holds the promise to timely provide proactive and actionable feedback to students. Using sophisticated machine learning (ML) algorithms and rich learner data, facilitates inference-making about several behavioral aspects (including effortful behavior) and about predicting learning performance, in any learning context. Researchers have been using ML methods in a black-box approach, ie, as a tool where the input data is the learner data and the output is a given class from the chosen construct. This work proposes a methodological shift from the black-box approach to a grey-box approach that bridges the hypothesis/literature-driven (feature extraction) white-box approach with the computation/data-driven (feature fusion) black-box approach. This will allow us to utilize data features that are educationally and contextually meaningful. This paper aims to extend current methodological paradigms, and puts into practice the proposed approach in an adaptive self-assessment case study taking advantage of new, cutting-edge, interdisciplinary work on building pipelines for educational data, using innovative tools and techniques. Practitioner Notes What is already known about this topic Capturing and measuring learners' engagement and behavior using physiological data has been explored during the last years and exhibits great potential. Effortless behavioral patterns commonly exhibited by learners, such as cheating, guessing or gaming the system counterfeit the learning outcome. Multimodal data can accurately predict learning engagement, performance and processes. What this paper adds Generalizes a methodology for building machine learning pipelines for multimodal educational data, using a modularized approach, namely the grey-box approach. Showcases that fusion of eye-tracking, facial expressions and arousal data provide the best prediction of effort and performance in adaptive learning settings. Highlights the importance of fusing data from different channels to obtain the most suited combinations from the different multimodal data streams, to predict and explain effort and performance in terms of pervasiveness, mobility and ubiquity. Implications for practice and/or policy Learning analytics researchers shall be able to use an innovative methodological approach, namely the grey-box, to build machine learning pipelines from multimodal data, taking advantage of artificial intelligence capabilities in any educational context. Learning design professionals shall have the opportunity to fuse specific features of the multimodal data to drive the interpretation of learning outcomes in terms of physiological learner states. The constraints from the educational contexts (eg, ubiquity, low-cost) shall be catered using the modularized gray-box approach, which can also be used with standalone data sources.",,,
10.1007/s11517-017-1670-6,2017,"Pernek, Igor; Ferscha, Alois",A survey of context recognition in surgery,"With the introduction of operating rooms of the future context awareness has gained importance in the surgical environment. This paper organizes and reviews different approaches for recognition of context in surgery. Major electronic research databases were queried to obtain relevant publications submitted between the years 2010 and 2015. Three different types of context were identified: (i) the surgical workflow context, (ii) surgeon's cognitive and (iii) technical state context. A total of 52 relevant studies were identified and grouped based on the type of context detected and sensors used. Different approaches were summarized to provide recommendations for future research. There is still room for improvement in terms of methods used and evaluations performed. Machine learning should be used more extensively to uncover hidden relationships between different properties of the surgeon's state, particularly when performing cognitive context recognition. Furthermore, validation protocols should be improved by performing more evaluations in situ and with a higher number of unique participants. The paper also provides a structured outline of recent context recognition methods to facilitate development of new generation context-aware surgical support systems.",,,
10.1016/j.jneumeth.2019.06.001,2019,"Murphy, Aidan P.; Leopold, David A.",A parameterized digital 3D model of the Rhesus macaque face for investigating the visual processing of social cues,"Background: Rhesus macaques are the most popular model species for studying the neural basis of visual face processing and social interaction using intracranial methods. However, the challenge of creating realistic, dynamic, and parametric macaque face stimuli has limited the experimental control and ethological validity of existing approaches.New method: We performed statistical analyses of in vivo computed tomography data to generate an anatomically accurate, three-dimensional representation of Rhesus macaque cranio-facial morphology. The surface structures were further edited, rigged and textured by a professional digital artist with careful reference to photographs of macaque facial expression, colouration and pelage.Results: The model offers precise, continuous, parametric control of craniofacial shape, emotional expression, head orientation, eye gaze direction, and many other parameters that can be adjusted to render either static or dynamic high-resolution faces. Example single-unit responses to such stimuli in macaque inferotemporal cortex demonstrate the value of parametric control over facial appearance and behaviours.Comparison with existing method(s): The generation of such a high-dimensionality and systematically controlled stimulus set of conspecific faces, with accurate craniofacial modelling and professional finalization of facial details, is currently not achievable using existing methods.Conclusions: The results herald a new set of possibilities in adaptive sampling of a high-dimensional and socially meaningful feature space, thus opening the door to systematic testing of hypotheses about the abundant neural specialization for faces found in the primate.",,,
,,NIEDZIELA M M; ROSAZZA M R,"System for assessing marketability of product, has single benchmark data point associated with benchmark stimuli, and emotional term assigned to product based on emotional term value, where data point represents location of stimuli","NOVELTY - The system (100) has a data gathering system (108, 116) for sensing subject product data and benchmark data. A single benchmark data point is associated with benchmark stimuli, where the single benchmark data point represents a location of the respective benchmark stimuli on a 3D mood map. Emotional term value is calculated based on a single benchmark data point and the single benchmark data point, where the emotional term value is associated with an emotional term. The emotional term is assigned to a product based on the emotional term value. USE - System for assessing marketability of a product. ADVANTAGE - The system includes a computing device that is provided with a non-transitory computer readable medium for gathering and assessing various data and comprising instructions that are stored, when the instructions are executed by a processor. The system utilizes read data inputs such as facial muscle activity, heart rate changes, skin conductance changes, electrical charges across scalp, eye tracking and behavior analysis for psycho-physiological mood mapping in an effective manner. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for assessing a marketability of a product. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a system for assessing marketability of a product.Product marketability assessing system (100)Data gathering systems (108, 116)GSR sensors (120a, 120b)Cabling (130, 132)Computing devices (202a, 202b)",,,
10.1038/s41598-017-09424-6,2017,"Thomas, Victoria; Davidson, Matthew; Zakavi, Parisa; Tsuchiya, Naotsugu; van Boxtel, Jeroen","Simulated forward and backward self motion, based on realistic parameters, causes motion induced blindness","Motion Induced Blindness (MIB) is a well-established visual phenomenon whereby highly salient targets disappear when viewed against a moving background mask. No research has yet explored whether contracting and expanding optic flow can also trigger target disappearance. We explored MIB using mask speeds corresponding to driving at 35, 50, 65 and 80 km/h in simulated forward (expansion) and backward (contraction) motion as well as 2-D radial movement, random, and static mask motion types. Participants (n = 18) viewed MIB targets against masks with different movement types, speed, and target locations. To understand the relationship between saccades, pupil response and perceptual disappearance, we ran two additional eye-tracking experiments (n = 19). Target disappearance increased significantly with faster mask speeds and upper visual field target presentation. Simulated optic flow and 2-D radial movement caused comparable disappearance, and all moving masks caused significantly more disappearance than a static mask. Saccades could not entirely account for differences between conditions, suggesting that self-motion optic flow does cause MIB in an artificial setting. Pupil analyses implied that MIB disappearance induced by optic flow is not subjectively salient, potentially explaining why MIB is not noticed during driving. Potential implications of MIB for driving safety and Head-Up-Display (HUD) technologies are discussed.",,,
,,DWIBEDI D; BADRINARAYANAN V; RABINOVICH A; MALISIEWICZ T J,System for training cuboid detector has pooling layer and regressor layer which are both connected to second layer of convolutional layers and non-convolutional layers and non-transitory memory is configured to store executable instructions,"NOVELTY - The system has non-transitory memory which is configured to store executable instructions. Multiple hardware processors in communication with the non-transitory memory, the hardware processors programmed by the executable instructions to access multiple training images. The training images includes a first training image and generate a cuboid detector (200). The cuboid detector comprises multiple convolutional layers and non-convolutional layers of a first convolutional neural network (CNN). A region proposal network (RPN) is connected to a first layer of the convolutional layers and non-convolutional layers. The pooling layer and the regressor layer are both connected to a second layer of the convolutional layers and non-convolutional layers. A second difference between a reference representation of the cuboid and the determined representation of the cuboid are determined. The weights of the cuboid detector updated based on the first difference and the second difference. USE - System for training cuboid detector. ADVANTAGE - The system is sufficiently mathematically, computationally, or technically complex that application-specific hardware or one or more physical computing devices (utilizing appropriate specialized executable instructions) may be necessary to perform the functionality, for example, due to the volume or complexity of the calculations involved or to provide results substantially in real-time. The set of parameterization points to be geometrically informative and visually discriminative. Machine learning methods include a family of methods that can enable robust and accurate solutions to a wide variety of problems, including eye image segmentation and eye tracking. The cuboid detector can refine keypoints by pooling convolutional features iteratively, improving the accuracy of the keypoints detected. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for training a cuboid detector. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a cuboid detector.Cuboid detector (200)Pooling layer (212)Fully-connected layer (216)Convolutional feature map (228)Cuboid (232)",,,
10.1109/TPAMI.2019.2949414,2021,"Hasan, Irtiza; Setti, Francesco; Tsesmelis, Theodore; Belagiannis, Vasileios; Amin, Sikandar; Del Bue, Alessio; Cristani, Marco; Galasso, Fabio",Forecasting People Trajectories and Head Poses by Jointly Reasoning on Tracklets and Vislets,"In this article, we explore the correlation between people trajectories and their head orientations. We argue that people trajectory and head pose forecasting can be modelled as a joint problem. Recent approaches on trajectory forecasting leverage short-term trajectories (aka tracklets) of pedestrians to predict their future paths. In addition, sociological cues, such as expected destination or pedestrian interaction, are often combined with tracklets. In this article, we propose MiXing-LSTM (MX-LSTM) to capture the interplay between positions and head orientations (vislets) thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation. We additionally exploit the head orientations as a proxy for the visual attention, when modeling social interactions. MX-LSTM predicts future pedestrians location and head pose, increasing the standard capabilities of the current approaches on long-term trajectory forecasting. Compared to the state-of-the-art, our approach shows better performances on an extensive set of public benchmarks. MX-LSTM is particularly effective when people move slowly, i.e., the most challenging scenario for all other models. The proposed approach also allows for accurate predictions on a longer time horizon.",,,
10.1109/ACCESS.2020.2966628,2020,"Xia, Chen; Quan, Rong",Predicting Saccadic Eye Movements in Free Viewing of Webpages,"Attention modeling for webpages has emerged as a new research direction in computer vision. Despite an amount of research effort, most studies have focused on estimating webpage saliency to reveal the static location of human fixations. Without temporal information, existing models cannot interpret the dynamic properties of the actual attention process in free-viewing webpages. To solve this problem, we propose a webpage-based saccadic model in this study to model dynamic visual search behaviors of humans when they view webpages. In the first stage, we utilize the support vector machine to learn the mapping from multilevel saliency features to an initial probability of being fixated. In the second stage, we combine the mechanisms of spatial bias and inhibition of return with the estimation of the initial probability to iteratively predict a sequence of successive fixations for each webpage image. Experimental results on a benchmark eye-tracking data set for webpages have demonstrated that the proposed model outperforms the state-of-the-art saccadic methods.",,,
10.1016/j.aei.2021.101359,2021,"Kim, Namgyun; Kim, Jinwoo; Ahn, Changbum R.",Predicting workers' inattentiveness to struck-by hazards by monitoring biosignals during a construction task: A virtual reality experiment,"At construction workplaces, workers should be consistently attentive to approaching and nearby safety hazards. However, workers tend to allocate most of their attentional resources to a work task and often exhibit inattentive behaviors to hazards, which may lead to serious injuries and fatalities. Predicting construction workers' inattentiveness is thus critical to preventing accidents in construction workplaces. With the advent of biosensing technologies, the potential of using biosignals to predict human behaviors has been proven in various fields of study. However, to date there has been little discussion about utilizing biosignals to predict construction workers' inattentive behaviors. To this end, this study examines whether construction workers' inattentive behaviors can be predicted by assessing biosignal reactivity. A virtual road construction environment was created and used for an experiment to expose participants to a repeated struck-by hazard without risking actual injury. Participants' biosignals (i.e., electrodermal activity, pupil dilation, and saccadic eye movement) and physical engagement in inattentive behaviors were collected and analyzed. The results of statistical analyses revealed significant differences in biosignal reactivities between participants' attentive behaviors (i.e., paying attention to the hazard) and inattentive behaviors (i.e., ignoring the hazard). The outcomes of the machine learning-based behavior classification also indicate the usefulness of predicting inattentive behaviors by monitoring workers' biosignals during a construction task and provide a foundation for the utilization of biosignals in safety management to prevent accidents resulting from inattentive behaviors.",,,
10.1145/3241381,2020,"Todi, Kashyap; Jokinen, Jussi; Luyten, Kris; Oulasvirta, Antti",Individualising Graphical Layouts with Predictive Visual Search Models,"In domains where users are exposed to large variations in visuo-spatial features among designs, they often spend excess time searching for common elements (features) on an interface. This article contributes individualised predictive models of visual search, and a computational approach to restructure graphical layouts for an individual user such that features on a new, unvisited interface can be found quicker. It explores four technical principles inspired by the human visual system (HVS) to predict expected positions of features and create individualised layout templates: (I) the interface with highest frequency is chosen as the template; (II) the interface with highest predicted recall probability (serial position curve) is chosen as the template; (III) the most probable locations for features across interfaces are chosen (visual statistical learning) to generate the template; (IV) based on a generative cognitive model, the most likely visual search locations for features are chosen (visual sampling modelling) to generate the template. Given a history of previously seen interfaces, we restructure the spatial layout of a new (unseen) interface with the goal of making its features more easily tradable. The four HVS principles are implemented in Familiariser, a web browser that automatically restructures webpage layouts based on the visual history of the user. Evaluation of Familiariser (using visual statistical learning) with users provides first evidence that our approach reduces visual search time by over 10%, and number of eye-gaze fixations by over 20%, during web browsing tasks.",,,
10.1016/j.jss.2019.110434,2020,"Vrzakova, Hana; Begel, Andrew; Mehtatalo, Lauri; Bednarika, Roman",Affect Recognition in Code Review: An In-situ Biometric Study of Reviewer's Affect,"Code reviews are an important practice in software development that increases team productivity and improves product quality. They are also examples of remote, computer-mediated asynchronous communications which are prone to the loss of affective information. Prior research has focused on sentiment analysis in source codes, as positive affect has been linked to developer productivity. Although methods of sentiment analysis have advanced, challenges remain due to numerous domain-specific expressions, subtle nuance, and indications of sentiment. In this paper, we uncover the potential for 1) nonverbal behavioral signals such as conventional typing, and 2) indirect physiological measures (eye gaze, GSR, touch pressure) to reveal genuine affective states in in situ code review in a large software company.Nonverbal behavioral signals of 33 professional software developers were recorded unobtrusively while they worked on their daily code reviews. After analyzing these signals using Linear Mixed Effect Models, we observe that affect presented in the written comments is associated with prolonged typing duration. Using physiological features, a trained Random Forest classifier can predict post-task valence with 90.0% accuracy (F1-score = 0.937) and arousal with 83.9% accuracy (F1-score = 0.856). The results show promise for the creation of intelligent affect-aware interfaces for code review. (C) 2019 Elsevier Inc. All rights reserved.",,,
,,MOTTA R J; MILLER B D; RICK T; SRIKANTH M B; REICH T,"System for generating mixed reality views for users, has controller which is configured to render frames for displaying virtual content composite into captured views of user environment","NOVELTY - The system (10) has a controller and a head-mounted display (HMD) (100) configured to display a three dimensional (3D) virtual view (102) to a user (190). The HMD comprises left and right displays for displaying frames. Multiple sensors configured to collect information about the user and the user's environment. A controller is configured to render frames for displaying virtual content composite into the captured views of the user's environment based on the range information from a world mapping sensor (140) , position and movement of the user's eyes as tracked by the eye tracking sensors. USE - System for generating mixed reality views for users. ADVANTAGE - A range information is used in adjusting the depth of real objects in the environment when displayed. The sensors provide low latency monochrome imaging for tracking head position. The information collected by the user sensors used to adjust the rendering of images to be projected, and adjust the projection of the images by the projection system of the HMD. The user's detected hand and finger gestures used to determine interactions of the user with virtual content in the virtual space, gestures that manipulate virtual objects. The camera capture the high-quality, high-resolution video of the user's environment in real time for display. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for method of generating mixed reality views for users. DESCRIPTION Of DRAWING(S) - The drawing shows a perspective view of mixed reality system.System (10)Three dimensional virtual view (102)World mapping sensor (140)User sensor (150)User (190)",,,
,,ALVAREZ T L; DANTONIO-BERTAGNOLLI J; GIOIA R; SCHEIMAN M; SEQUEIRA M T; YARMOTHU C; DANTONIO-BERTAGNOLLI J V; YARAMOTHU C; DANTONIO B J,"Method for remediating visual symptoms in user with binocular dysfunction, involves controlling accommodative and proximal vergence stimulation of eyes of user through visual therapy video game","NOVELTY - The method involves rendering (1102) a visual therapy video game on one or more displays. The accommodative and proximal vergence stimulation of a user's eyes is controlled (1104) through the visual therapy video game. The visual therapy video game is rendered by a head mounted display with integrated eye tracking hardware and software. A preprogrammed portion of disparity vergence is stimulated. A feedback portion of disparity vergence is limited. A magnitude of asymmetrical stimulation is derived from a position of the left and right eyes. USE - Method for remediating visual symptoms in user with binocular dysfunction. ADVANTAGE - The method ensures accuracy of eye alignment, and correlating progress in the game to accurate eye movements. The mechanics of the eye remain the same where the eye position is used to improve vergence performance but the visual environment and 3D models of the game change so that the user does not perceive he/she is actually performing repetitive eye movements. The score allows players to unlock and purchase weapons and power ups, which make it easier for them to beat the game and accumulate a higher score. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) a method for treating binocular dysfunction; and(2) a system for remediating visual symptoms in a user with binocular dysfunction. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a method for remediating visual symptoms in a user with a binocular dysfunction.Step for rendering a visual therapy video game on one or more displays (1102)Step for controlling accommodative and proximal vergence stimulation of a user's eyes (1104)",,,
10.1109/TPAMI.2019.2956930,2021,"Sun, Wanjie; Chen, Zhenzhong; Wu, Feng",Visual Scanpath Prediction Using IOR-ROI Recurrent Mixture Density Network,"A visual scanpath represents the human eye movements when scanning the visual field for acquiring and receiving visual information. Predicting visual scanpaths when a certain stimulus is presented plays an important role in modeling overt human visual attention and search behavior. In this paper, we presented an 'Inhibition of Return - Region of Interest' (IOR-ROI) recurrent mixture density network based framework learning to produce human-like visual scanpaths under task-free viewing conditions. The proposed model simultaneously predicts a sequence of ordered fixation positions and their corresponding fixation durations. Our model integrates bottom-up features and semantic features extracted by convolutional neural networks. Then the integrated feature maps are fed into the IOR-ROI Long Short-Term Memory (LSTM) which is the core component of the proposed model. The IOR-ROI LSTM is a dual LSTM unit, i.e., the IOR-LSTM and the ROI-LSTM, capturing IOR dynamics and gaze shift behavior simultaneously. IOR-LSTM simulates the visual working memory to adaptively maintain and update visual information regarding previously fixated regions. ROI-LSTM is responsible for predicting the next possible ROIs given the spatially inhibited image feature maps on the feature-wise basis. Fixation duration is predicted by a regression neural network given the viewing history and image feature maps corresponding to currently fixated ROI. Considering the eye movement pattern variations among subjects, a mixture density network is adopted to model the next fixation distribution as Gaussian mixtures and the fixation duration is also modeled using Gaussian distribution. Our model is evaluated on the OSIE and MIT low resolution eye-tracking datasets and experimental results indicate that the proposed method can achieve superior performance in predicting visual scanpaths. The code will be publicly available on URL: https://github.com/sunwj/scanpath.",,,
10.1016/j.pneurobio.2020.101885,2020,"Franceschiello, Benedetta; Di Sopra, Lorenzo; Minier, Astrid; Ionta, Silvio; Zeugin, David; Notter, Michael P.; Bastiaansen, Jessica A. M.; Jorge, Joao; Yerly, Jerome; Stuber, Matthias; Murray, Micah M.",3-Dimensional magnetic resonance imaging of the freely moving human eye,"Eye motion is a major confound for magnetic resonance imaging (MRI) in neuroscience or ophthalmology. Currently, solutions toward eye stabilisation include participants fixating or administration of paralytics/anaesthetics. We developed a novel MRI protocol for acquiring 3-dimensional images while the eye freely moves. Eye motion serves as the basis for image reconstruction, rather than an impediment. We fully reconstruct videos of the moving eye and head. We quantitatively validate data quality with millimetre resolution in two ways for individual participants. First, eye position based on reconstructed images correlated with simultaneous eye-tracking. Second, the reconstructed images preserve anatomical properties; the eye's axial length measured from MRI images matched that obtained with ocular biometry. The technique operates on a standard clinical setup, without necessitating specialized hardware, facilitating wide deployment. In clinical practice, we anticipate that this may help reduce burdens on both patients and infrastructure, by integrating multiple varieties of assessments into a single comprehensive session. More generally, our protocol is a harbinger for removing the necessity of fixation, thereby opening new opportunities for ethologically-valid, naturalistic paradigms, the inclusion of populations typically unable to stably fixate, and increased translational research such as in awake animals whose eye movements constitute an accessible behavioural readout.",,,
10.1016/j.inffus.2017.02.006,2017,"Slanzi, Gino; Pizarro, Gaspar; Velasquez, Juan D.",Biometric information fusion for web user navigation and preferences analysis: An overview,"Throughout the years having knowledge of Web users' interests, navigational actions and preferences has gained importance due to the objectives of organizations and companies. Traditionally this field has been studied from the Web Mining perspective, particularly through the Web Usage Mining (WUM) concept, which consists of the application of machine learning techniques over data originated in the Web (Web data) for automatic extraction of behavioral patterns from Web users. WUM makes use of data sources that approximate users' behavior, such as weblogs or clickstreams among others; however these sources imply a considerable degree of subjectivity to interpret. For that reason, the application of biometric tools with the possibility of measuring actual responses to the stimuli presented via websites has become of interest in this field. Instead of doing separate analyses, information fusion (IF) tries to improve results by developing efficient methods for transforming information from different sources into a single representation, which then could be used to guide biometric data fusion to complement the traditional WUM studies and obtain better results. This paper presents a survey of Biometric IF applied to the WUM field, by first defining WUM and its main applications, later explaining how the Biometric IF could be applied and finally reviewing several studies that apply this concept to WUM. (C) 2017 Elsevier B.V. All rights reserved.",,,
10.1109/ACCESS.2019.2947692,2019,"Cheng, Qian; Wang, Wuhong; Jiang, Xiaobei; Hou, Shanyi; Qin, Yong",Assessment of Driver Mental Fatigue Using Facial Landmarks,"Driver fatigue is one of the causal factors for traffic accidents. Actions of facial units convey various information from our brains. This paper proposed a comprehensive approach to explore whether pattern of sequences of the drivers facial landmarks changes from the alert start to the fatigue state. A driving-simulator-based experiment was designed and conducted. Videos of 21 participants faces were recorded during the experiment, together with subjective and others assessment of the level of alertness. Sequences of eye aspect ratio (EAR) and mouth aspect ratio (MAR) were calculated based on facial landmarks. Totally 21 feature candidates including Percent eye-closure over a fixed time window (PERCLOS), blink rate, statistics of blink duration, closing speed, reopening speed and number of yawns were extracted. A mental fatigue assessment model is proposed after feature selection. Four machine learning algorithms were used to build the assessment model of fatigue. The best performance was achieved by logistic regression, with cross-validation accuracies of 83.7 and 85.4. This study may contribute to the development of driver fatigue monitoring system for automotive ergonomics.",,,
,,NA Y; CHEN C; LIU H; CHEN S,"System for eye gesture tracking used to determine gaze information of eye, has device that includes circuitry configured to obtain electrical signal representing measurement of optical signal reflected from eye by photodetector","NOVELTY - The system has a machine that includes a display including multiple tunable optical elements. A device includes circuitry configured to obtain an electrical signal that represents a measurement of an optical signal reflected from an eye (216A,216B) by a photodetector. A depth map of the eye is determined based on phase differences between a reference signal and the electrical signal generated by the photodetector. A gaze information that represents a gaze of the eye is determined based on the depth map. One or more processors are in communication with the machine and the device. The processors includes one or more storage devices storing instructions that are operable, when executed by the one or more processors, to cause the processors to perform the operations including receiving output data representing the gaze information from the device, and determining the gaze information representing the gaze of the eye in relation to the display of the machine. USE - System for eye gesture tracking used to determine gaze information of eye. ADVANTAGE - The real-time focusing based on the eye gesture tracking methods can reduce feelings of nausea by maintaining consistent depth perception between the user's eye and brain. The foveated focusing provides natural three dimensional (3D) effect through simple tunable optics. The efficiency of the eye gesture tracking is improved as the complexity of the interaction between the system and the user increases. The signal-to-noise ratio of the received reflected light pulses can be improved while maintaining the same power consumption for the eye gesture tracking device by reducing the duty cycle of the optical pulses by factor of N, and increasing the intensity of the optical pulses by factor of N at the same time. The device bandwidth is increased so that the duty cycle of the optical pulses can be decreased without distorting the pulse shape. The phase shifts can be helpful in eye gesture detection by reducing the difference in charge collection at each phase to enable more accurate depth map of the user's eye as differences in charge collection can impact the accuracy of the charge collection as whole. The selected portions of tunable elements of the second transparent lens can be tuned to provide enhanced focus of viewing of particular locations at the display of the computing device. The tunable optical elements of the transparent or the opaque screen can be configured to refocus light based on the determined gaze information of the user's eyes, thus providing solution to the inconsistencies that can arise between accommodation and vergence during certain viewing experiences. The optical emission power can be dynamically lowered to reduce the exposure of the eye to the optical emission given close distance between the eye and the eye tracking module. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of the cross-platform peripheral control system using eye gesture tracking.Headset (201)Accelerometer (211)Gyroscope (212)Eye-tracking Module (213)Eyes (216A,216B)",,,
,,GUR J,"Tracking scanning laser optics device for mounting in headgear, has calibration unit that operates with scanning optics to determine origin in two-dimensional space as reference point to identify location of portion within retina","NOVELTY - The device comprises a mounting unit that is integral with or configured for attachment to a headgear. An invisible light source is supported by the mounting unit for directing invisible light through a pupil of the user for scanning and imaging a portion of the retina. A visible light source is supported by the mounting unit for directing visible light through the pupil for writing on to the retina within a portion. A two-dimensional (2-D) scanning optics is supported by the mounting unit for scanning the portion of the retina with the invisible and visible light. An imaging device (18) is supported by the mounting unit for receiving the invisible light reflected by the portion of the retina and stores an image. A calibration unit is operative in conjunction with the 2-D scanning optics to determine an origin in 2-D space for serving as a reference point for identifying a location of the portion within the retina. USE - Tracking scanning laser optics device for mounting in headgear (claimed). ADVANTAGE - The device provides a highly compact eye tracking and scanning device that can be fitted or retro-fitted to regular eyeglasses. The device allows a patient to perform retinal scanning at home and convey the results to his or her physician. The device can be integrated with an automatic guidance system suitable for infantry soldiers that addresses defects of current target alignment. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a headgear;(2) a method for assisting a user wearing the headgear;(3) a method for estimating a range of a distant object in a line of sight of the user wearing the headgear;(4) a method for remotely directing one or more stationary soldiers each wearing headgear;(5) a method for providing an enlarged view of an object to a user wearing the headgear;(6) a method for accurate navigation of a user wearing the headgear;(7) a method for alerting a motorist wearing the headgear;(8) a method for controlling an augmented reality application by a user wearing the headgear; and(9) a method for remotely directing multiple mutually remote attendees at a teleconference when to speak, each remote attendee wearing headgear. DESCRIPTION Of DRAWING(S) - The drawing shows the schematic diagram of the device for compensating for rotation of the eye so as to maintain alignment between the device and the pupil opening.Spectacle frame (11)Imaging device (18)Beam splitter (28)Module (110)Reflector (113)",,,
,,NINAN A; MAMMEN N,"Method for adapting video images for wearable device for use in display application e.g. virtual reality (VR), involves causing display image to be rendered on device display of wearable device","NOVELTY - The method involves determining (402) a spatial direction of a wearable device. The spatial direction of the wearable device that represents an actual viewing direction of the wearable device is used (404) to select a set of multiple single-view images corresponding to a set of multiple viewing directions at the first time point, from a multi-view image comprising a single-view image. A display image is caused (406) to be rendered on a device display of the wearable device. The display image representing a single-view image as viewed from the actual viewing direction of the wearable device at the first time point is rendered on a device display of the wearable device. The display image is constructed based on a portion of the spatial direction of the wearable device and the single-view images corresponding to the viewing directions. USE - Method for adapting video images for wearable device for use in display application such as virtual reality (VR), augmented reality (AR), mixed reality (MR), telepresence, telemedicine, three-dimensional (3D) video, omnidirectional video, etc. ADVANTAGE - The human vision system is relatively not sensitive to spatial acuity outside of a focus region, and used to minimize network bandwidth requirements, to meet stringent delay requirements, and in the meantime to support a seamless viewing experience that is relatively free from visual artifacts, physiological discomforts, etc. Each of the relatively high spatial resolution images covers the focus region in accordance with an eye gaze direction of the viewer in real time or in near real time. The relative low quality image for the field of view supported by the respective imager and the relatively high quality image for the focus region of the respective eye of the viewer in line with the gaze direction of each of the left and right eyes of the viewer is superimposed, combined to generate a single overall display image to be rendered by the respective imager. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:(1) an apparatus for adapting video images for wearable device;(2) a system for adapting video images for wearable device;(3) a non-transitory computer readable storage medium storing program for adapting video images for wearable device; and(4) a computing device for adapting video images for wearable device. DESCRIPTION Of DRAWING(S) - The drawing shows a flowchart illustrating a process for video images for wearable device.Step for determining spatial direction of wearable device (402)Step for using spatial direction that represents actual viewing direction of wearable device (404)Step for causing display image to be rendered on display device (406)",,,
,2018,"Hermund, Anders; Klint, Lars Simon; Bundgaard, Ture Slot; Bjornson-Langen, Rune Noel Meedom Meldgaard","The Perception of Architectural Space in Reality, in Virtual Reality, and through Plan and Section Drawings A case study of the perception of architectural atmosphere","This paper presents the findings from a comparative study of an architectural space communicated as the space itself and its two different representations, i.e. a virtual reality model and traditional plan and section drawings. Using eye tracking technology in combination with qualitative questionnaires, a case study of an architectural space is investigated in physical reality, a virtual reality 3D BIM model, and finally through representation of the space in plan and section drawings. In this study, the virtual reality scenario seems closer to reality than the experience of the same space experienced through plan and section drawings. There is an overall higher correlation of both the conscious reflections and the less conscious behaviour between the real physical architectural space and the virtual reality space, than there is between the real space and the space communicated through plan and section drawings. We can conclude that the scenario with the best overall size estimations, compared to the actual measures, is the virtual reality scenario. The paper further discusses the future applications of virtual reality in architecture.",,,
,,LIU X; FU Y; LU L,Light field camera has controller that is configured to control multiple different voltage levels applied to liquid crystal layer during time period and individually adjust during time period based on multiple different voltage levels,"NOVELTY - The light field camera (100) has an angle sensitive pixel (ASP) array (115) that is configured to capture multiple light field frames. A liquid crystal layer (120) is configured to receive the light that entered the light field camera, and selectively propagate a portion of the received light toward the array of pixels based on multiple different voltage levels applied to the liquid crystal layer and multiple angles of incidence of the received light with the liquid crystal layer while applying the different voltage levels. A controller (130) is configured to control different voltage levels applied to the liquid crystal layer during a time period, and individually adjust based on different voltage levels. Each liquid crystal cell of the liquid crystal layer control amounts of light at the angles are propagated through the liquid crystal layer and captured by the array of pixels as light field frames, such that the array of pixels captures the light field frames during the time period. USE - Light field camera used in virtual reality (VR), augmented reality (AR) and mixed reality (MR) system, for example, three-dimensional (3D) or depth estimation of objects in a scene, and for eye tracking. ADVANTAGE - The scanning of the liquid crystal layer for different angle information can be faster compared to the time multiplexed operation. The magnification and focusing of the image light by the optics block allows the electronic display to be physically smaller, weigh less and consume less power than larger displays. The inertial measurement unit (IMU) provides the sampled measurement signals to the console, which interprets the data to reduce error. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method of utilizing a light field camera. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a light field camera including an ASP array.Light field camera (100)Image-forming optical module (110)ASP array (115)Liquid crystal layer (120)Controller (130)",,,
,,BENNET R; GESELOWITZ L; ZHANG W; POULOS A G; BEVIS J; PIMMEL K P; FAJT N G,"Method for displaying holographic keyboard image and holographic hand image representing user's hand, involves displaying holographic keyboard image, and maintaining keyboard image spatially separated by virtual distance from hand image","NOVELTY - The method involves determining that a user's hand is spaced by initial actual distance from a capture device. A holographic keyboard image spatially separated by virtual distance from the holographic hand image is displayed. Determination is made that the user's hand moves to an updated actual distance from the capture device. The holographic keyboard image spatially separated by the virtual distance from the holographic hand image is maintained in response to determining that the user's hand moves to the updated actual distance from the capture device. USE - Method for displaying a holographic keyboard image and a holographic hand image representing a user's hand in virtual reality and mixed reality display systems. ADVANTAGE - The method enables a light guide to enable a user to perceive a 3D holographic image located within a physical environment that the user is viewing, while allowing the user to view physical objects in the physical environment. The method enables the user to change the location and/or orientation of his body and/or hands, while maintaining the holographic keyboard image at a consistent distance under the holographic hand image. The method enables the keyboard interface program that facilitates a consistent user interaction experience by allowing the user's hand to drift or move, while maintaining constant interaction angle between the virtual hand plane and the virtual keyboard plane of the holographic keyboard image. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:(1) a keyboard interface system for displaying a holographic keyboard image and a holographic hand image representing a user's hand(2) a head-mounted display device for displaying a holographic keyboard image and a holographic hand image representing a user's hand. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic view of a keyboard interface system.Transparent display (68)Eye-tracking system (72)Optical sensor system (76)Position sensor system (80)Microphone system (84)",,,
10.1371/journal.pone.0236939,2020,"Billing, Erik; Belpaeme, Tony; Cai, Haibin; Cao, Hoang-Long; Ciocan, Anamaria; Costescu, Cristina; David, Daniel; Homewood, Robert; Garcia, Daniel Hernandez; Esteban, Pablo Gomez; Liu, Honghai; Nair, Vipul; Matu, Silviu; Mazel, Alexandre; Selescu, Mihaela; Senft, Emmanuel; Thill, Serge; Vanderborght, Bram; Vernon, David; Ziemke, Tom",The DREAM Dataset: Supporting a data-driven study of autism spectrum disorder and robot enhanced therapy,"We present a dataset of behavioral data recorded from 61 children diagnosed with Autism Spectrum Disorder (ASD). The data was collected during a large-scale evaluation of Robot Enhanced Therapy (RET). The dataset covers over 3000 therapy sessions and more than 300 hours of therapy. Half of the children interacted with the social robot NAO supervised by a therapist. The other half, constituting a control group, interacted directly with a therapist. Both groups followed the Applied Behavior Analysis (ABA) protocol. Each session was recorded with three RGB cameras and two RGBD (Kinect) cameras, providing detailed information of children's behavior during therapy. This public release of the dataset comprises body motion, head position and orientation, and eye gaze variables, all specified as 3D data in a joint frame of reference. In addition, metadata including participant age, gender, and autism diagnosis (ADOS) variables are included. We release this data with the hope of supporting further data-driven studies towards improved therapy methods as well as a better understanding of ASD in general.",,,
10.1109/CVPR.2017.370,2017,"Gorji, Siavash; Clark, James J.",Attentional Push: A Deep Convolutional Network for Augmenting Image Salience with Shared Attention Modeling in Social Scenes,"We present a novel visual attention tracking technique based on Shared Attention modeling. By considering the viewer as a participant in the activity occurring in the scene, our model learns the loci of attention of the scene actors and use it to augment image salience. We go beyond image salience and instead of only computing the power of image regions to pull attention, we also consider the strength with which the scene actors push attention to the region in question, thus the term Attentional Push. We present a convolutional neural network (CNN) which augments standard saliency models with Attentional Push. Our model contains two pathways: an Attentional Push pathway which learns the gaze location of the scene actors and a saliency pathway. These are followed by a shallow augmented saliency CNN which combines them and generates the augmented saliency. For training, we use transfer learning to initialize and train the Attentional Push CNN by minimizing the classification error of following the actors' gaze location on a 2-D grid using a large-scale gaze-following dataset. The Attentional Push CNN is then fine-tuned along with the augmented saliency CNN to minimize the Euclidean distance between the augmented saliency and ground truth fixations using an eye-tracking dataset, annotated with the head and the gaze location of the scene actors. We evaluate our model on three challenging eye fixation datasets, SALICON, iSUN and CAT2000, and illustrate significant improvements in predicting viewers' fixations in social scenes.",,,
,2017,"Abromavicius, Vytautas; Serackis, Arturas",Stereoscopic Focus Moment Identification Based on Pupil Dynamics Measures,"Consumption time of stereoscopic content is Increasing with the new technologies being introduced to the market, visual comfort in viewing stereoscopic images and videos is currently a subject of very high Importance. Different stereoscopic viewing techniques, e.g. head mounted displays, stereoscopic monitors and projectors and large cinema screens requires individual setups for comfort Image perception. The aim of the investigation, presented in this paper is to indicate measurable features of the eye activity which may correlate with visual discomfort during stereoscopic perception. In addition, a technique for focus moment identification was proposed that is based on real-time measures of eye activity. 120 stereoscopic images of publicly available dataset which had subjective evaluation of visual discomfort was shown until participants indicated they have had focused on shown stereoscopic content. Pupillometric and gaze data of subjects were collected using eye tracker. Measured data shows a distinct constriction of pupil diameter, when subjects were exposed to the stereoscopic content, followed by a dilation, half of Initial pupil size, until subjects indicated the time of focus. Based on these findings afore mentioned focus moment identification technique was proposed. Also, our findings of pupil dilation agrees with existing research where relationship was found between cognitive activity and pupil dilation.",,,
10.1145/3375462.3375498,2020,"Sharma, Kshitij; Papamitsiou, Zacharoula; Olsen, Jennifer K.; Giannakos, Michail",Predicting learners' effortful behaviour in adaptive assessment using multimodal data,"Many factors influence learners' performance on an activity beyond the knowledge required. Learners' on-task effort has been acknowledged for strongly relating to their educational outcomes, reflecting how actively they are engaged in that activity. However, effort is not directly observable. Multimodal data can provide additional insights into the learning processes and may allow for effort estimation. This paper presents an approach for the classification of effort in an adaptive assessment context. Specifically, the behaviour of 32 students was captured during an adaptive self-assessment activity, using logs and physiological data (i.e., eye-tracking, EEG, wristband and facial expressions). We applied k-means to the multimodal data to cluster students' behavioural patterns. Next, we predicted students' effort to complete the upcoming task, based on the discovered behavioural patterns using a combination of Hidden Markov Models (HMMs) and the Viterbi algorithm. We also compared the results with other state-of-the-art classification algorithms (SVM, Random Forest). Our findings provide evidence that HMMs can encode the relationship between effort and behaviour (captured by the multimodal data) in a more efficient way than the other methods. Foremost, a practical implication of the approach is that the derived HMMs also pinpoint the moments to provide preventive/prescriptive feedback to the learners in real-time, by building-upon the relationship between behavioural patterns and the effort the learners are putting in.",,,
10.1088/1757-899X/364/1/012039,2018,"Cantoni, Virginio; Dondi, Piercarlo; Lombardi, Luca; Nugrahaningsih, Nahumi; Porta, Marco; Setti, Alessandra",A Multi-Sensory Approach to Cultural Heritage: The Battle of Pavia Exhibition,"In the last years, several museums and exhibits have adopted new kinds of interactive installations that present artworks in more attractive ways, especially for young visitors. At the same time, new communication technologies have been introduced to allow vision and motion impaired people to visit arts centers. In this work, we present the multisensory solutions we have implemented for the Battle of Pavia Exhibition, a collateral event of Milan Expo 2015. The installation combined different interaction methods to achieve two main goals: providing visitors with engaging experiences and allowing blind and partially sighted people to appreciate the exposed artworks. The used technologies include gesture communication, gaze-based interaction, 3D character reconstruction, virtual avatars, and 3D tactile images. This work can be also viewed in the context of digital humanities for cultural heritage. To the best of our knowledge, this is the first exhibit to gather such a high number of interactive technologies in a single installation. The positive response from visitors is a great spur to continue our research in this direction.",,,
10.1111/exsy.12398,2020,"Abiyev, Rahib H.; Arslan, Murat",Head mouse control system for people with disabilities,"In this paper, a human-machine interface for disabled people with spinal cord injuries is proposed. The designed human-machine interface is an assistive system that uses head movements and blinking for mouse control. In the proposed system, by moving one's head, the user moves the mouse pointer to the required coordinates and then blinks to send commands. The considered head mouse control is based on image processing including facial recognition, in particular, the recognition of the eyes, mouth, and nose. The proposed recognition system is based on the convolutional neural network, which uses the low-quality images that are captured by a computer's camera. The convolutional neural network (CNN) includes convolutional layers, a pooling layer, and a fully connected network. The CNN transforms the head movements to the actual coordinates of the mouse. The designed system allows people with disabilities to control a mouse pointer with head movements and to control mouse buttons with blinks. The results of the experiments demonstrate that this system is robust and accurate. This invention allows people with disabilities to freely control mouse cursors and mouse buttons without wearing any equipment.",,,
,,HEILAND P; MAAS H; SHAW P; TOROK M,"Computer system for monitoring operator of baggage inspection system, has memory which stores inspection data defining visual engagement behavior in association with image identifier which identifies displayed image and tagging data","NOVELTY - The computer system has monitoring unit, such as camera, which captures visual engagement behavior of first operator (106a) engaged in inspecting displayed image to generate inspection outcome which causes physical articles to be tagged for second inspection procedure based on identifying item of interest in image, and computer memory which stores inspection data defining visual engagement behavior in association with image identifier which identifies displayed image and tagging data which indicates whether article is tagged for further inspection. USE - Computer system for monitoring operator of baggage inspection system (claimed) when inspecting captured and displayed images of sequentilly physical articles. ADVANTAGE - Helps to determine highly objectively by automated manner if and how an operator is accomplishing the task of inspecting scanned images of baggage items. The capability to determine real time data, performance values and consensus decision for operators that are certified and in charge of inspecting baggage allows training in real environment without overhead effort, to measure uncertified operators and consequently to determine and improve their individual performance until a certification can be issued. Assists the operator in preventing false positives and consequently improve the efficiency of the inspection process. Gives some sort of guidance to second operator who has to do physical inspection of article of baggage and helps to increase efficiency of that part of the process. With an increased amount of images of items to be tagged in a whole set of appearances within scanned image, and respective metadata about underlying visual inspection process, an automated visual inspection process becomes more and more precise. Helps to improve both database and machine learning application of image recognition as well as quality and performance of first operator. A particular ensemble of operators with needed performance levels can be assigned purposely to inspection belt in relation to its security requirements, without delay and overhead. Systems are in first instance suited to support more-eyes-principle and thus are in particular suited to enhance and improve the quality of inspection process. The likelihood of missing items to be tagged according to a security level can be reduced, i.e. the rate of false negatives can be minimized if not almost excluded. With the direct comparison of performance of multiple operators, the system allows for further evaluation and in particular for evaluation of performance of each single operator, more or less in real-time. DETAILED DESCRIPTION - The inspection data indicates whether operator looked at the image, or indicates how long the operator looked at the image. The monitoring unit performs eye tracking of operator's gaze trajectory. An INDEPENDENT CLAIM is also included for an automated inspection system. DESCRIPTION Of DRAWING(S) - The drawing shows a schematic block diagram of a cloud-based baggage inspection system.Server (100)Training module (102)Inspection module (104)First operator (106as)Data aggregation server (110)",,,
,2020,"Cai, Jiannan; Yang, Liu; Zhang, Yuxi; Cai, Hubo",Estimating the Visual Attention of Construction Workers from Head Pose Using Convolutional Neural Network-Based Multi-Task Learning,"The visual attention of construction workers is an important indicator to assess their situational awareness and infer their intention for reducing construction injuries and improving construction site safety. The eye-tracking technology has been adopted in several studies to directly measure the gaze direction and determine workers' visual attention. However, eye-trackers are expensive and wearing them may disturb normal operations. Considering the increasing use of surveillance videos and the availability of construction images, it is of great potential to estimate workers' visual attention from imagery data, which, however, has not been well exploited by existing studies. This paper presents a convolutional neural network (CNN)-based multi-task learning framework to estimate the visual attention of construction workers from head pose using low-resolution images. Visual attention is approximated by head yaw and pitch orientation. The problem is formulated as a multi-task image classification problem, where the first task is head yaw classification, and the second task is head pitch classification. A CNN-based multi-task learning framework is designed to jointly learn two tasks, with shared layers capturing the commonality between tasks, and task-specific layers modeling the uniqueness of individual tasks. Compared to traditional single-task learning mechanism that trains different classifiers for each task, the proposed approach leverages the commonality of relevant tasks and captures the shared representation, which can significantly improve the efficiency and performance. The results suggest the proposed multi-learning framework can achieve an accuracy of 76.5% for head yaw estimation and 88.7% for head pitch estimation, better than the performance obtained using conventional single task learning.",,,
10.3414/ME16-02-0006,2017,"Rosa, Pedro J.; Gamito, Pedro; Oliveira, Jorge; Morais, Diogo; Pavlovic, Matthew; Smyth, Olivia; Maia, Ines; Gomes, Tiago",Eye Movement Analysis and Cognitive Assessment,"Background: An adequate behavioral response depends on attentional and mnesic processes. When these basic cognitive functions are impaired, the use of non-immersive Virtual Reality Applications (VRAs) can be a reliable technique for assessing the level of impairment. However, most non-immersive VRAs use indirect measures to make inferences about visual attention and mnesic processes (e.g., time to task completion, error rate).Objectives: To examine whether the eye movement analysis through eye tracking (ET) can be a reliable method to probe more effectively where and how attention is deployed and how it is linked with visual working memory during comparative visual search tasks (CVSTs) in non-immersive VRAs.Methods: The eye movements of 50 healthy participants were continuously recorded while CVSTs, selected from a set of cognitive tasks in the Systemic Lisbon Battery (SLB). Then a VRA designed to assess of cognitive impairments were randomly presented.Results: The total fixation duration, the number of visits in the areas of interest and in the interstimulus space, along with the total execution time was significantly different as a function of the Mini Mental State Examination (MMSE) scores.Conclusions: The present study demonstrates that CVSTs in SLB, when combined with ET, can be a reliable and unobtrusive method for assessing cognitive abilities in healthy individuals, opening it to potential use in clinical samples.",,,
10.1109/TIP.2019.2945857,2020,"Che, Zhaohui; Borji, Ali; Zhai, Guangtao; Min, Xiongkuo; Guo, Guodong; Le Callet, Patrick",How is Gaze Influenced by Image Transformations? Dataset and Model,"Data size is the bottleneck for developing deep saliency models, because collecting eye-movement data is very time-consuming and expensive. Most of current studies on human attention and saliency modeling have used high-quality stereotype stimuli. In real world, however, captured images undergo various types of transformations. Can we use these transformations to augment existing saliency datasets? Here, we first create a novel saliency dataset including fixations of 10 observers over 1900 images degraded by 19 types of transformations. Second, by analyzing eye movements, we find that observers look at different locations over transformed versus original images. Third, we utilize the new data over transformed images, called data augmentation transformation (DAT), to train deep saliency models. We find that label-preserving DATs with negligible impact on human gaze boost saliency prediction, whereas some other DATs that severely impact human gaze degrade the performance. These label-preserving valid augmentation transformations provide a solution to enlarge existing saliency datasets. Finally, we introduce a novel saliency model based on generative adversarial networks (dubbed GazeGAN). A modified U-Net is utilized as the generator of the GazeGAN, which combines classic skip connection with a novel center-surround connection (CSC) module. Our proposed CSC module mitigates trivial artifacts while emphasizing semantic salient regions, and increases model nonlinearity, thus demonstrating better robustness against transformations. Extensive experiments and comparisons indicate that GazeGAN achieves state-of-the-art performance over multiple datasets. We also provide a comprehensive comparison of 22 saliency models on various transformed scenes, which contributes a new robustness benchmark to saliency community. Our code and dataset are available at: https://github.com/CZHQuality/Sal-CFS-GAN.",,,
10.1109/JSEN.2019.2962339,2020,"Horng, Gwo-Jiun; Lin, Jia-Yi",Using Multimodal Bio-Signals for Prediction of Physiological Cognitive State Under Free-Living Conditions,"Drowsiness is a common human physiological response. Research suggests that insufficient sleep results in low energy levels, various negative physiological and psychological effects on the body, and abnormal cognitive functioning. Studies have predominantly focused on driving while drowsy, using brainwave measurement and facial detection techniques to address this topic, whereas few have discussed the physiological prediction of drowsiness. In addition to driving, working conditions and environments as well as daily activities also correspond to the risk of accidents occurring when people are drowsy. This study designed an experiment consisting of five tests in which a brainwave sensor, eye tracker, heart rate sensor, and galvanic skin response sensor were used to record physiological changes in participants. The data indicated the binary outcomes of patients either being or not being in a state of drowsiness or sleepiness. During various states of drowsiness or sleepiness, brain wave activity, eye movement, heart rate, and GSR were measured, and differences in these physiological responses were analyzed. A classification model was also used to predict a participant's state of drowsiness or sleepiness. Data on physiological characteristics included eye movement and the heart rate value, which was calculated during various states of drowsiness or sleepiness to obtain a value. Brain wave and GSR signals were converted through software development kit (SDK) programming at the sensor end. Subsequently, the data were processed through an artificial neural network (ANN), back propagation network, and support vector machine (SVM). In the experiment, the SVM hyperparameters were adjusted, the ANN model was added to the Adam optimization model, and overfitting was avoided to ensure that the results were comprehensive. According to the experiment results, the use of SVM yields the optimal classification performance, reaching an accuracy of 89.1%; 90% of participants were also categorized more accurately through SVM than ANN.",,,
,2020,"Xia, Jiangyue; Tian, Jingqi; Xing, Jiankai; Cheng, Jiawen; Zhang, Jun; Wen, Jiangtao; Li, Zhengguang; Lou, Jian",SOCIAL DATA ASSISTED MULTI-MODAL VIDEO ANALYSIS FOR SALIENCY DETECTION,"Video saliency should be taken into consideration to facilitate optimization of the end-to-end video production, delivery and consumption ecosystem to improve user experience at lowered cost. Although recent studies have significantly increased the accuracy of saliency prediction, the approaches are mostly video-centric, without considering any prior bias that viewers may have with regard to the video contents. In this paper, we propose a novel learning-based multi-modal method for optimizing user-oriented video analysis. In particular, we generate a face-popularity mask using face recognition results and popularity information obtained from social media, and combine it with conventional content-only saliency analysis to produce multi-modal popularity-motion features. A convolutional long short-term memory (ConvL-STM) network discovers temporal correlation of human attention across frames. Experiments show that our method outperforms the state-of-the-art video saliency prediction approaches in representing human viewing preferences in real world applications, and demonstrate the necessity as well as the potential for integrating user bias information into attention detection.",,,
,,KIM Y J; AHN B H; YANG J E; JUNG I H; YU I S; LIM J H; CHOI B P; LEE J E; KIM Y; AN B; LIANG Z; ZHENG R; RYU I S; LIM J; CUI B; AHN B; CHOL B; LI Z,"Electronic devices such as smart glass, head mount device (HMD), has processor displays first partial image obtained by image processing for region of interest using first attribute information and second partial image","NOVELTY - The electronic device has an eye-tracking unit that is configured to obtain a gaze information of a user. A processor is configured to determine a region of interest (510) of an image to be displayed on the display, by using the user's gaze information. The processor collects context information of the electronic device. The processor determines first attribute information relating to image processing for the region of interest of the image and second attribute information relating to image processing for a remaining region other than the region of interest of the image, based on the context information. The processor displays, on the display, a first partial image obtained by the image processing for the region of interest using the first attribute information and a second partial image obtained by the image processing for the remaining region using the second attribute information. USE - Electronic devices such as smart glass, head mount device (HMD), smart phone, tablet personal computer (PC), mobile phone, video telephone, electronic book reader, desktop PC, laptop PC, net book computer, workstation, server, personal digital assistant (PDA), portable multimedia player (PMP), MP3 player, mobile medical device, camera, or wearable device e.g. watch, ring, bracelet, ankle bracelet, glass, contact lens, cloth-integrated types such as electronic clothe, body-attached types e.g. skin pads or tattoos, or implantable types e.g. implantable circuit, home appliance, such as television (TV), digital versatile disc (DVD) player, audio, refrigerator, air conditioner, cleaner, oven, microwave oven, washing machine, air cleaner, set-top box, home automation control panel, security control panel, TV box e.g. dictionary, electronic key, camcorder, electronic picture frame, medical device e.g. various portable medical measurement device such as blood glucose monitoring device, heartbeat measuring device, blood pressure measuring device, body temperature measuring device, magnetic resonance angiography (MRA) device, magnetic resonance imaging (MRI) device, computed tomography (CT) devices, medical scanners, and ultrasonic devices, electronic equipment for vessels e.g. navigation system and gyrocompass, avionic, security device, head unit for vehicle, industrial or home robots, automatic teller machines (ATM), points of sales device (POS), or internet of thing (IoT) device e.g. light bulb, sensor, electric or gas meter, sprinkler devices, fire alarm, thermostat, street lamp, toaster, exercise equipment, hot water tank, heater, boiler, electronic board, electronic signature receiving device, projector, or various measuring instrument e.g. water meter, electricity meter, gas meter, or wave meter. ADVANTAGE - In the foveated rendering mode, the HMD can reduce the processor load by reducing the computational complexity for the image data processing. The correction unit can correct the rendered image to prevent image distortion caused by the lens unit. The processor can determine the complexity of the image based on the computational complexity needed to render the image. The processor is configured to analyze scene information of the image to be displayed, and determine scene complexity of the image, based on the analysis result. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a image output method performed in an electronic device configured to output at least part of a three dimensional (3D) image. DESCRIPTION Of DRAWING(S) - The drawing shows an application of foveated rendering to an image.Output image (501)First layer (510)Peripheral region (520)",,,
10.1177/0040517520944248,2021,"Shi, Min; Ming, Hou; Liu, Yaning; Mao, Tianlu; Zhu, Dengming; Wang, Zhaoqi; Zhang, Fan",Saliency-dependent adaptive remeshing for cloth simulation,"We propose a method for simulating cloth with meshes dynamically refined according to visual saliency. It is a common belief that it is preferable for the regions of an image being viewed to have more details than others. For a certain scene, a low-resolution cloth mesh is first simulated and rendered into images in the preview stage. Pixel saliency values of these images are predicted according to a pre-trained saliency prediction model. These pixel saliencies are then translated to a vertex saliency of the corresponding meshes. Vertex saliency, together with camera positions and a number of geometric features of surfaces, guides the dynamic remeshing for simulation in the production stage. To build the saliency prediction model, images extracted from various videos of clothing scenes were used as training data. Participants were asked to watch these videos and their eye motion was tracked. A saliency map is generated from the eye motion data for each extracted video frame image. Image feature vectors and map labels are sent to a Support Vector Machine for training to obtain a saliency prediction model. Our method greatly reduces the number of vertices and faces in the clothing model, and generates a speed-up of more than 3 x for scenes with single dressed character, while for multi-character scenes the speed-up is increased to more than 5x. The proposed technique can work together with view-dependency for offline simulation.",,,
10.1111/bjet.12989,2020,"Ahn, Byunghoon Tony; Harley, Jason M.",Facial expressions when learning with a Queer History App: Application of the Control Value Theory of Achievement Emotions,"Learning analytics (LA) incorporates analyzing cognitive, social and emotional processes in learning scenarios to make informed decisions regarding instructional design and delivery. Research has highlighted important roles that emotions play in learning. We have extended this field of research by exploring the role of emotions in a relatively uncommon learning scenario: learning about queer history with a multimedia mobile app. Specifically, we used an automatic facial recognition software (FaceReader 7) to measure learners' discrete emotions and a counter-balanced multiple-choice quiz to assess learning. We also used an eye tracker (EyeLink 1000) to identify the emotions learners experienced while they read specific content, as opposed to the emotions they experienced over the course of the entire learning session. A total of 33 out of 57 of the learners' data were eligible to be analyzed. Results revealed that learners expressed more negative-activating emotions (ie, anger, anxiety) and negative-deactivating emotions (ie, sadness) than positive-activating emotions (ie, happiness). Learners with an angry emotion profile had the highest learning gains. The importance of examining typically undesirable emotions in learning, such as anger, is discussed using the control-value theory of achievement emotions. Further, this study describes a multimodal methodology to integrate behavioral trace data into learning analytics research. Practitioner Notes What is already known about this topic Multimodal analytics have increasingly gained traction, accompanied by more advanced methodologies and tools. Emotions play a critical role in learning, impacting learners' cognitive, motivational and regulatory processes. The Control Value Theory of Achievement Emotion predicts positive-activating emotions (eg, enjoyment) should lead to better performance. What this paper adds Application of Control Value Theory of Achievement Emotion to a less studied subject domain (ie, history) and issues (ie, LGBTQ rights). Preliminary evidence that negative-activating emotions (eg, anger) can facilitate learning in certain contexts, and that emotions may play different roles depending on the subject matter and domain. Insights into methods of aligning facial recognition data from facial expression analysis software with eye tracking data dealing with dynamic content. Insights into frequency and the types of emotions elicited in learning scenarios dealing with sensitive topics. Implications for practice and/or policy Educators should be aware of different types of emotions, and their roles in learning scenarios. Educators should critically evaluate whether emotions with positive valence always have a positive impact on learning, and vice versa. Learners may not always behaviorally express emotions through facial expressions, including when gazing at sections of learning material directly connected to assessment; therefore, it is helpful to supplement granular with larger timeframe analyses to examine emotional profiles.",,,
10.1016/j.bbe.2018.08.001,2018,"Abromavicius, Vytautas; Serackis, Arturas",Eye and EEG activity markers for visual comfort level of images,"Depth perception by binocular cues is based on the matching of image features from one retina with corresponding elements from the second retina. However, high disparities are related to the higher visual discomfort levels and may cause the eye fatigue during extended stereoscopic perception time. The goal of the investigation was to find a set of measurable features for stereoscopic image visual comfort level prediction. The investigation involved gaze, pupillometric and EEG data from 28 subjects who evaluated visual comfort level of 120 stereoscopic images. Six different time frame windows were used to analyze four measured features: the number of focus points; the dynamics of pupil size; disparity level at the focus points; the activity of EEG bands at the frontal lobe. A significant difference was found in all investigated stereoscopic image groups. 2-s and 5-s pre-DPI window showed best results for the selected feature sets. The higher disparity at the focus points, lower number of focus points are related to the lower levels of visual comfort. However, features such as the number of focus points, the pupil size and the disparity level for the images with lowest visual comfort scores showed similar results to the images scored as comfortable'' or very comfortable''. (C) 2018 Nalecz Institute of Biocybernetics and Biomedical Engineering of the Polish Academy of Sciences. Published by Elsevier B.V. All rights reserved.",,,
