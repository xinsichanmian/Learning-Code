作者,作者 ID,标题,年份,来源出版物名称,卷,期,论文编号,起始页码,结束页码,页码计数,施引文献,DOI,链接,归属机构,带归属机构的作者,摘要,作者关键字,索引关键字,文献类型,出版阶段,开放获取,来源出版物,EID
"Kim J.-H., Jeong J.-W.","57205720618;57205722835;","Multi-view multi-modal head-gaze estimation for advanced indoor user interaction",2022,"Computers, Materials and Continua","70","3",,"5107","5132",,,"10.32604/cmc.2022.021107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117047706&doi=10.32604%2fcmc.2022.021107&partnerID=40&md5=304c891eee8c6e2f5b4d7f28298b5081","Department of Computer Engineering, Kumoh National Institute of Technology, Gumi, 39177, South Korea; Department of Data Science, Seoul National University of Science and Technology, Seoul, 01811, South Korea","Kim, J.-H., Department of Computer Engineering, Kumoh National Institute of Technology, Gumi, 39177, South Korea; Jeong, J.-W., Department of Data Science, Seoul National University of Science and Technology, Seoul, 01811, South Korea","Gaze estimation is one of the most promising technologies for supporting indoor monitoring and interaction systems. However, previous gaze estimation techniques generally work only in a controlled laboratory environment because they require a number of high-resolution eye images. This makes them unsuitable for welfare and healthcare facilities with the following challenging characteristics: 1) users’ continuous movements, 2) various lighting conditions, and 3) a limited amount of available data. To address these issues, we introduce a multi-view multi-modal head-gaze estimation system that translates the user’s head orientation into the gaze direction. The proposed system captures the user using multiple cameras with depth and infrared modalities to train more robust gaze estimators under the aforementioned conditions. To this end, we implemented a deep learning pipeline that can handle different types and combinations of data. The proposed system was evaluated using the data collected from 10 volunteer participants to analyze how the use of single/multiple cameras and modalities affect the performance of head-gaze estimators. Through various experiments, we found that 1) an infrared-modality provides more useful features than a depth-modality, 2) multi-view multi-modal approaches provide better accuracy than single-view single-modal approaches, and 3) the proposed estimators achieve a high inference efficiency that can be used in real-time applications. © 2022 Tech Science Press. All rights reserved.","Deep learning; Head-gaze estimation; Human-computer interaction; Indoor monitoring","Cameras; Deep learning; Modal analysis; Deep learning; Gaze estimation; Head-gaze estimation; Indoor monitoring; Interaction systems; Monitoring system; Multi-modal; Multi-views; Multiple cameras; User interaction; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85117047706
"Karargyris A., Kashyap S., Lourentzou I., Wu J.T., Sharma A., Tong M., Abedin S., Beymer D., Mukherjee V., Krupinski E.A., Moradi M.","26654835300;36350862900;55953611100;57200497494;57201754607;57221148455;57219693679;6602270956;57207271026;26643320200;57208604625;","Creation and validation of a chest X-ray dataset with eye-tracking and report dictation for AI development",2021,"Scientific Data","8","1","92","","",,5,"10.1038/s41597-021-00863-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103416457&doi=10.1038%2fs41597-021-00863-5&partnerID=40&md5=5d655e760bb8c61d6f5706e5df8fff9b","IBM Research, Almaden Research Center, San Jose, CA  95120, United States; Department of Computer Science, Virginia Tech, Blacksburg, VA  24061, United States; Department of Radiology and Imaging Sciences, Emory University, Atlanta, GA  30322, United States","Karargyris, A., IBM Research, Almaden Research Center, San Jose, CA  95120, United States; Kashyap, S., IBM Research, Almaden Research Center, San Jose, CA  95120, United States; Lourentzou, I., IBM Research, Almaden Research Center, San Jose, CA  95120, United States, Department of Computer Science, Virginia Tech, Blacksburg, VA  24061, United States; Wu, J.T., IBM Research, Almaden Research Center, San Jose, CA  95120, United States; Sharma, A., IBM Research, Almaden Research Center, San Jose, CA  95120, United States; Tong, M., IBM Research, Almaden Research Center, San Jose, CA  95120, United States; Abedin, S., IBM Research, Almaden Research Center, San Jose, CA  95120, United States; Beymer, D., IBM Research, Almaden Research Center, San Jose, CA  95120, United States; Mukherjee, V., IBM Research, Almaden Research Center, San Jose, CA  95120, United States; Krupinski, E.A., Department of Radiology and Imaging Sciences, Emory University, Atlanta, GA  30322, United States; Moradi, M., IBM Research, Almaden Research Center, San Jose, CA  95120, United States","We developed a rich dataset of Chest X-Ray (CXR) images to assist investigators in artificial intelligence. The data were collected using an eye-tracking system while a radiologist reviewed and reported on 1,083 CXR images. The dataset contains the following aligned data: CXR image, transcribed radiology report text, radiologist’s dictation audio and eye gaze coordinates data. We hope this dataset can contribute to various areas of research particularly towards explainable and multimodal deep learning/machine learning methods. Furthermore, investigators in disease classification and localization, automated radiology report generation, and human-machine interaction can benefit from these data. We report deep learning experiments that utilize the attention maps produced by the eye gaze dataset to show the potential utility of this dataset. © 2021, The Author(s).",,"diagnostic imaging; human; radiography; thorax; Deep Learning; Humans; Radiography; Thorax",Data Paper,"Final","",Scopus,2-s2.0-85103416457
"Chen S., Zhang Y., Yin B., Wang B.","57257208000;55951477400;8616230700;56768300100;","TRFH: towards real-time face detection and head pose estimation",2021,"Pattern Analysis and Applications","24","4",,"1745","1755",,,"10.1007/s10044-021-01026-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114816789&doi=10.1007%2fs10044-021-01026-3&partnerID=40&md5=350b36780739b0f827e50fca16f52082","Beijing University of Technology, Beijing, China","Chen, S., Beijing University of Technology, Beijing, China; Zhang, Y., Beijing University of Technology, Beijing, China; Yin, B., Beijing University of Technology, Beijing, China; Wang, B., Beijing University of Technology, Beijing, China","Nowadays, face detection and head pose estimation have a lot of application such as face recognition, aiding in gaze estimation and modeling attention. For these two tasks, it is usually to design two different models. However, the head pose estimation model often depends on the region of interest (ROI) detected in advance, which means that a serial face detector is needed. Even the lightest face detector will slow down the whole forward inference time and cannot achieve real-time performance when detecting the head pose of multiple people. We can see that both face detection and head pose estimation need face features, so a shared face feature map can be used between them. In this paper, a multi-task learning model is proposed that can solve both problems simultaneously. We directly detect the location of the center point of the bounding box of face; at this location, we calculate the size of the bounding box of face and the head attitude. We evaluate our model’s performance on the AFLW. The proposed model has great competitiveness with the multi-stage face attribute analysis model, and our model can achieve real-time performance. © 2021, The Author(s).","Anchor Free; Face detection; Head pose; Multi-task",,Article,"Final","",Scopus,2-s2.0-85114816789
"Masud M., Hossain M.S., Alhumyani H., Alshamrani S.S., Cheikhrouhou O., Ibrahim S., Muhammad G., Rashed A.E.E., Gupta B.B.","17338820600;24066717900;55586872500;56825932900;35112771500;55425859100;56605566900;57282828000;34770593700;","Pre-Trained Convolutional Neural Networks for Breast Cancer Detection Using Ultrasound Images",2021,"ACM Transactions on Internet Technology","21","4","3418355","","",,2,"10.1145/3418355","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112603216&doi=10.1145%2f3418355&partnerID=40&md5=8a202d3ebd97e8739575276ba7119e7d","Taif University, P. O. Box 11099, Taif, 21944, Saudi Arabia; Research Pervasive and Mobile Computing, And Department of Software Engineering, College of Computer and Information Sciences, Riyadh, 11543, Saudi Arabia; King Saud University, Riyadh, Saudi Arabia; National Institute of Technology and Asia University, Taiwan; Taif University and Cairo University","Masud, M., Taif University, P. O. Box 11099, Taif, 21944, Saudi Arabia; Hossain, M.S., Taif University, P. O. Box 11099, Taif, 21944, Saudi Arabia; Alhumyani, H., Research Pervasive and Mobile Computing, And Department of Software Engineering, College of Computer and Information Sciences, Riyadh, 11543, Saudi Arabia; Alshamrani, S.S., King Saud University, Riyadh, Saudi Arabia; Cheikhrouhou, O., Research Pervasive and Mobile Computing, And Department of Software Engineering, College of Computer and Information Sciences, Riyadh, 11543, Saudi Arabia; Ibrahim, S., National Institute of Technology and Asia University, Taiwan; Muhammad, G., National Institute of Technology and Asia University, Taiwan; Rashed, A.E.E., Taif University and Cairo University; Gupta, B.B., Taif University, P. O. Box 11099, Taif, 21944, Saudi Arabia","Volunteer computing based data processing is a new trend in healthcare applications. Researchers are now leveraging volunteer computing power to train deep learning networks consisting of billions of parameters. Breast cancer is the second most common cause of death in women among cancers. The early detection of cancer may diminish the death risk of patients. Since the diagnosis of breast cancer manually takes lengthy time and there is a scarcity of detection systems, development of an automatic diagnosis system is needed for early detection of cancer. Machine learning models are now widely used for cancer detection and prediction research for improving the successive therapy of patients. Considering this need, this study implements pre-Trained convolutional neural network based models for detecting breast cancer using ultrasound images. In particular, we tuned the pre-Trained models for extracting key features from ultrasound images and included a classifier on the top layer. We measured accuracy of seven popular state-of-The-Art pre-Trained models using different optimizers and hyper-parameters through fivefold cross validation. Moreover, we consider Grad-CAM and occlusion mapping techniques to examine how well the models extract key features from the ultrasound images to detect cancers. We observe that after fine tuning, DenseNet201 and ResNet50 show 100% accuracy with Adam and RMSprop optimizers. VGG16 shows 100% accuracy using the Stochastic Gradient Descent optimizer. We also develop a custom convolutional neural network model with a smaller number of layers compared to large layers in the pre-Trained models. The model also shows 100% accuracy using the Adam optimizer in classifying healthy and breast cancer patients. It is our belief that the model will assist healthcare experts with improved and faster patient screening and pave a way to further breast cancer research. © 2021 Association for Computing Machinery.","breast cancer; convolutional neural network; Deep learning; gaze detection","Convolution; Convolutional neural networks; Data handling; Deep learning; Diagnosis; Gradient methods; Medical imaging; Multilayer neural networks; Optimization; Patient treatment; Stochastic systems; Ultrasonic applications; Breast Cancer; Breast cancer detection; Convolutional neural network; Deep learning; Gaze detection; Health care application; Key feature; Optimizers; Ultrasound images; Volunteer computing; Diseases",Article,"Final","",Scopus,2-s2.0-85112603216
"Ansari M.F., Kasprowski P., Obetkal M.","57221608762;8940684200;57279844500;","Gaze tracking using an unmodified web camera and convolutional neural network",2021,"Applied Sciences (Switzerland)","11","19","9068","","",,,"10.3390/app11199068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116006986&doi=10.3390%2fapp11199068&partnerID=40&md5=18fcbeeec0e7eebe010141c9d3cc9c29","Department of Applied Informatics, Silesian University of Technology, Gliwice, 44-100, Poland","Ansari, M.F., Department of Applied Informatics, Silesian University of Technology, Gliwice, 44-100, Poland; Kasprowski, P., Department of Applied Informatics, Silesian University of Technology, Gliwice, 44-100, Poland; Obetkal, M., Department of Applied Informatics, Silesian University of Technology, Gliwice, 44-100, Poland","Gaze estimation plays a significant role in understating human behavior and in human– computer interaction. Currently, there are many methods accessible for gaze estimation. However, most approaches need additional hardware for data acquisition which adds an extra cost to gaze tracking. The classic gaze tracking approaches usually require systematic prior knowledge or expertise for practical operations. Moreover, they are fundamentally based on the characteristics of the eye region, utilizing infrared light and iris glint to track the gaze point. It requires high-quality images with particular environmental conditions and another light source. Recent studies on appearance-based gaze estimation have demonstrated the capability of neural networks, especially convolutional neural networks (CNN), to decode gaze information present in eye images and achieved significantly simplified gaze estimation. In this paper, a gaze estimation method that utilizes a CNN for gaze estimation that can be applied to various platforms without additional hardware is presented. An easy and fast data collection method is used for collecting face and eyes images from an unmodified desktop camera. The proposed method registered good results; it proves that it is possible to predict the gaze with reasonable accuracy without any additional tools. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural network; Gaze tracking; Human computer interaction",,Article,"Final","",Scopus,2-s2.0-85116006986
"Toyosaka Y., Okita T.","55857955900;57196004948;","Activity Knowledge Graph Recognition by Eye Gaze: Identification of Distant Object in Eye Sight for Watch Activity",2021,"UbiComp/ISWC 2021 - Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers",,,,"334","339",,,"10.1145/3460418.3479351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115984916&doi=10.1145%2f3460418.3479351&partnerID=40&md5=94f23a2fc50ca2a93eeef6f2e0303141","Kyushu Institute of Technology, Iizuka, Fukuoka, Japan","Toyosaka, Y., Kyushu Institute of Technology, Iizuka, Fukuoka, Japan; Okita, T., Kyushu Institute of Technology, Iizuka, Fukuoka, Japan","From the side of upper-level applications which require planning the actions in robot or those which need to search the whole log of activities in smart home, the action predicate expressions in the form of knowledge graphs may play an important role. The sequence of activities alone, which can be supplied by the conventional activity recognition systems, may not be sufficient for those applications. The subject of the particular activity is crucial information in most of the cases, and the object of the particular activity is often necessary to identify the characteristics. From this perspective, we have investigated the activities recognized by activity recognition systems, trying to identify their hidden elements which play the role of the subject and the object of the activities, i.e. activity knowledge graph. If we focus on these hidden elements, they are categorized in two: (1) person (subject)-person (object) interaction, and (2) person (subject)-object (object) interactions. Depending on the class of activities, these two are sometimes faced great difficulties: The hidden elements for walk, pick-up, open, and drink are quite easy but those for look-At, see, watch, and throw are difficult. The source of difficulties arises from the fact that the object (object) is not contacted from the person (subject). In this paper we have developed a method which identifies non-contacted object by the direction of the eye gaze of the person (subject) in the category of watch (activity). Using ""Watching TV""data by Stair lab, the proposed system achieved 85% in accuracy. © 2021 ACM.","action localization; gaze estimation; interaction; object detection","Automation; Object detection; Robot programming; Watches; Action localization; Activity recognition; Eye-gaze; Gaze estimation; Hidden elements; Interaction; Knowledge graphs; Localisation; Object interactions; Recognition systems; Knowledge graph",Conference Paper,"Final","",Scopus,2-s2.0-85115984916
"Moon S., Zhang C., Park S., Zhang H., Kim W.-S., Ko J.H.","57277193900;57193807497;57277843000;57216240708;55492044000;56921245400;","A Sub-Milliwatt and Sub-Millisecond 3-D Gaze Estimator for Ultra Low-Power AR Applications",2021,"UbiComp/ISWC 2021 - Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers",,,,"481","485",,,"10.1145/3460418.3479360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115950725&doi=10.1145%2f3460418.3479360&partnerID=40&md5=ac898f026459de09d3608817f5b2c9bd","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Sait China Lab, Samsung Research, China-Beijing (SRC-B), Beijing, China; Samsung Advanced Institute of Technology (SAIT), Suwon, South Korea; College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea","Moon, S., Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Zhang, C., Sait China Lab, Samsung Research, China-Beijing (SRC-B), Beijing, China; Park, S., Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Zhang, H., Sait China Lab, Samsung Research, China-Beijing (SRC-B), Beijing, China; Kim, W.-S., Samsung Advanced Institute of Technology (SAIT), Suwon, South Korea; Ko, J.H., College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea","The critical factors of real-Time gaze tracking are high accuracy and user-friendliness such as low latency and no run-Time calibration. Existing gaze estimator hardware designs are based on 2D regression algorithms as 2D methods have a simple computation process. However, they require multiple run-Time calibration steps, and are vulnerable to head motions. On the other hand, the 3D model-based method can maintain better accuracy than the 2D method without run-Time calibration steps, and is robust to head motions. In this paper, we aim to design the first 3D model-based gaze estimator hardware that consumes less than 1mW power and 1ms latency per frame. The simulation results based on the hardware synthesis show that the proposed design requires 172μW and 0.5ms per frame, while maintaining less than 0.9° error. © 2021 ACM.","Gaze Tracker;Gaze Vector Estimation;Smart Glasses;ASIC","3D modeling; Application specific integrated circuits; Eye tracking; Integrated circuit design; 3D models; 3d-modeling; Gaze tracker; Gaze tracker;; Gaze vector estimation;; Head motion; Runtimes; Smart glass; Smart glass;; Vector estimation; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-85115950725
"Araluce J., Bergasa L.M., Ocaña M., López-Guillén E., Revenga P.A., Felipe Arango J., Pérez O.","57204909678;6602297767;56410080500;57204784175;12790320200;57262405500;57215137907;","Gaze focalization system for driving applications using openface 2.0 toolkit with NARMAX algorithm in accidental scenarios",2021,"Sensors","21","18","6262","","",,,"10.3390/s21186262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115051391&doi=10.3390%2fs21186262&partnerID=40&md5=9e690bf7336c0b2a0ae9ed5d6a403f08","Electronics Department, University of Alcalá, Alcalá de Henares, 28801, Spain","Araluce, J., Electronics Department, University of Alcalá, Alcalá de Henares, 28801, Spain; Bergasa, L.M., Electronics Department, University of Alcalá, Alcalá de Henares, 28801, Spain; Ocaña, M., Electronics Department, University of Alcalá, Alcalá de Henares, 28801, Spain; López-Guillén, E., Electronics Department, University of Alcalá, Alcalá de Henares, 28801, Spain; Revenga, P.A., Electronics Department, University of Alcalá, Alcalá de Henares, 28801, Spain; Felipe Arango, J., Electronics Department, University of Alcalá, Alcalá de Henares, 28801, Spain; Pérez, O., Electronics Department, University of Alcalá, Alcalá de Henares, 28801, Spain","Monitoring driver attention using the gaze estimation is a typical approach used on road scenes. This indicator is of great importance for safe driving, specially on Level 3 and Level 4 automation systems, where the take over request control strategy could be based on the driver’s gaze estimation. Nowadays, gaze estimation techniques used in the state-of-the-art are intrusive and costly, and these two aspects are limiting the usage of these techniques on real vehicles. To test this kind of application, there are some databases focused on critical situations in simulation, but they do not show real accidents because of the complexity and the danger to record them. Within this context, this paper presents a low-cost and non-intrusive camera-based gaze mapping system integrating the open-source state-of-the-art OpenFace 2.0 Toolkit to visualize the driver focalization on a database composed of recorded real traffic scenes through a heat map using NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) to establish the correspondence between the OpenFace 2.0 parameters and the screen region the user is looking at. This proposal is an improvement of our previous work, which was based on a linear approximation using a projection matrix. The proposal has been validated using the recent and challenging public database DADA2000, which has 2000 video sequences with annotated driving scenarios based on real accidents. We compare our proposal with our previous one and with an expensive desktop-mounted eye-tracker, obtaining on par results. We proved that this method can be used to record driver attention databases. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Accidental scenarios; Computer vision; Driver focalization; Gaze estimation; Heat map; NARMAX","Accidents; Automation; Automobile testing; Behavioral research; Database systems; Open systems; Automation systems; Control strategies; Driver attention; Linear approximations; Nonlinear autoregressive moving average models; Projection matrix; Public database; State of the art; Eye tracking",Article,"Final","",Scopus,2-s2.0-85115051391
"Das S., Ghosh I.D., Chattopadhyay A.","55605761984;12760477800;57225350566;","An efficient deep sclera recognition framework with novel sclera segmentation, vessel extraction and gaze detection",2021,"Signal Processing: Image Communication","97",,"116349","","",,,"10.1016/j.image.2021.116349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107796193&doi=10.1016%2fj.image.2021.116349&partnerID=40&md5=ce85c8ae9ded790974e692ea5f30a428","University of Engineering and Management Newtown, Kolkata, West Bengal  700156, India; Barrackpore Rastraguru Surendranath College Barrackpore, Kolkata, West Bengal  700120, India","Das, S., University of Engineering and Management Newtown, Kolkata, West Bengal  700156, India; Ghosh, I.D., Barrackpore Rastraguru Surendranath College Barrackpore, Kolkata, West Bengal  700120, India; Chattopadhyay, A., University of Engineering and Management Newtown, Kolkata, West Bengal  700156, India","Sclera recognition is a promising ocular biometric modality because of contact-less, gaze-independent image acquisition in visible light. Moreover, it is unaffected even if the subjects are wearing contact lenses in eyes. However, it is a difficult task because several steps are required, each of which must be performed accurately and efficiently. In this work, sclera recognition is performed in the following steps, namely, segmentation of sclera region, extraction of sclera vasculature pattern, detection of gaze direction and finally comparison of two vasculature patterns for matching and recognition. The proposed segmentation model DSeg is based on well-known deep learning model UNet and reduces model complexity by creating a Knowledge Base of sclera and non-sclera colors. DSeg is a lightweight and environment-friendly model, which outperforms UNet in terms of speed, efficiency and accuracy. Two rule-based unsupervised vessel extraction methods require prior sclera segmentation and exhibit competing recognition performance to a supervised deep model for vessel extraction, which does not require prior sclera segmentation. A novel deep recognition model is proposed which compares two vessel structures taking into account their affine-transformation, and produces a single Boolean output to decide whether the structures match or not. The model does not require post logic in the matching process. The model is further improved to detect errors in prediction. We achieve best recognition rates with low false-acceptance-rates for two sets of training and validation, using the publicly available dataset SBVPI and the best achieved AUC score is 0.98. © 2021 Elsevier B.V.","DeepR; Gaze detection; Sclera biometric; Sclera recognition; Sclera segmentation; Vasculature segmentation","Deep learning; Extraction; Knowledge based systems; Affine transformations; Environment friendly; False acceptance rate; Matching process; Recognition models; Segmentation models; Vessel extraction; Vessel structure; Pattern recognition",Article,"Final","",Scopus,2-s2.0-85107796193
"Kobayashi K., Komuro T., Kagawa K., Kawahito S.","57199997893;7005736117;7101606653;7005154064;","Transmission of correct gaze direction in video conferencing using screen-embedded cameras",2021,"Multimedia Tools and Applications","80","21-23",,"31509","31526",,,"10.1007/s11042-020-09758-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091144236&doi=10.1007%2fs11042-020-09758-w&partnerID=40&md5=145227c45b3ea2cfe8e5fcf42b538044","Saitama University, Shimo-okubo 255, Sakura-ku, Saitama, Japan; Shizuoka University, Johoku 3-5-1, Naka-ku, Hamamatsu, Japan","Kobayashi, K., Saitama University, Shimo-okubo 255, Sakura-ku, Saitama, Japan; Komuro, T., Saitama University, Shimo-okubo 255, Sakura-ku, Saitama, Japan; Kagawa, K., Shizuoka University, Johoku 3-5-1, Naka-ku, Hamamatsu, Japan; Kawahito, S., Shizuoka University, Johoku 3-5-1, Naka-ku, Hamamatsu, Japan","In this paper, we propose a new video conferencing system that presents correct gaze directions of a remote user by switching among images obtained from multiple cameras embedded in a screen according to a local user’s position. Our proposed method reproduces a situation like that in which the remote user is in the same space as the local user. The position of the remote user to be displayed on the screen is determined so that the positional relationship between the users is reproduced. The system selects one of the embedded cameras whose viewing direction towards the remote user is the closest to the local user’s viewing direction to the remote user’s image on the screen. As a result of quantitative evaluation, we confirmed that, in comparison with the case using a single camera, the accuracy of gaze estimation was improved by switching among the cameras according to the position of the local user. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Camera selection; Gaze preservation; Teleconferencing","Cameras; Transmissions; Gaze direction; Gaze estimation; Multiple cameras; Positional relationship; Quantitative evaluation; Single cameras; Video conferencing system; Viewing directions; Video conferencing",Article,"Final","",Scopus,2-s2.0-85091144236
[无可用作者姓名],[无可用的作者 ID],"ACM SIGGRAPH 2021 Courses, SIGGRAPH 2021",2021,"ACM SIGGRAPH 2021 Courses, SIGGRAPH 2021",,,,"","",2220,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113735861&partnerID=40&md5=70a1b227bb611d5f7543a9018a1383b4",,"","The proceedings contain 17 papers. The topics discussed include: an interactive introduction to WEBGL; user interfaces for high-dimensional design problems; least squares for programmers; an introduction to deep learning on meshes; contact and friction simulation for computer graphics; differentiable computer graphics in TensorFlow; color management with Opencolorio V2; SIGGRAPH’21 course notes new techniques in interactive character animation; and gaze-aware displays and interaction.",,,Conference Review,"Final","",Scopus,2-s2.0-85113735861
"Hao Q., Tao Y., Cao J., Tang M., Cheng Y., Zhou D., Ning Y., Bao C., Cui H.","7102508877;57201557439;56518662900;57215963872;7404914681;57215194568;57211678730;57226456780;57216751377;","Retina-like imaging and its applications: A brief review",2021,"Applied Sciences (Switzerland)","11","15","7058","","",,,"10.3390/app11157058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111617189&doi=10.3390%2fapp11157058&partnerID=40&md5=a1b3d0a2e49d3af11ef88965f7eb881e","Key Laboratory of Biomimetic Robots and Systems, Ministry of Education, Beijing Institute of Technology, Beijing, 100081, China; Yangtze Delta Region Academy, Beijing Institute of Technology, Jiaxing, 314003, China; System Engineering Institute, Academy of Army Research, Chinese People’s Liberation Army, Beijing, 100039, China","Hao, Q., Key Laboratory of Biomimetic Robots and Systems, Ministry of Education, Beijing Institute of Technology, Beijing, 100081, China, Yangtze Delta Region Academy, Beijing Institute of Technology, Jiaxing, 314003, China; Tao, Y., Key Laboratory of Biomimetic Robots and Systems, Ministry of Education, Beijing Institute of Technology, Beijing, 100081, China, System Engineering Institute, Academy of Army Research, Chinese People’s Liberation Army, Beijing, 100039, China; Cao, J., Key Laboratory of Biomimetic Robots and Systems, Ministry of Education, Beijing Institute of Technology, Beijing, 100081, China, Yangtze Delta Region Academy, Beijing Institute of Technology, Jiaxing, 314003, China; Tang, M., Key Laboratory of Biomimetic Robots and Systems, Ministry of Education, Beijing Institute of Technology, Beijing, 100081, China; Cheng, Y., Key Laboratory of Biomimetic Robots and Systems, Ministry of Education, Beijing Institute of Technology, Beijing, 100081, China, Yangtze Delta Region Academy, Beijing Institute of Technology, Jiaxing, 314003, China; Zhou, D., Key Laboratory of Biomimetic Robots and Systems, Ministry of Education, Beijing Institute of Technology, Beijing, 100081, China; Ning, Y., Key Laboratory of Biomimetic Robots and Systems, Ministry of Education, Beijing Institute of Technology, Beijing, 100081, China; Bao, C., Key Laboratory of Biomimetic Robots and Systems, Ministry of Education, Beijing Institute of Technology, Beijing, 100081, China, Yangtze Delta Region Academy, Beijing Institute of Technology, Jiaxing, 314003, China; Cui, H., Key Laboratory of Biomimetic Robots and Systems, Ministry of Education, Beijing Institute of Technology, Beijing, 100081, China","The properties of the human eye retina, including space-variant resolution and gaze characters, provide many advantages for numerous applications that simultaneously require a large field of view, high resolution, and real-time performance. Therefore, retina-like mechanisms and sensors have received considerable attention in recent years. This paper provides a review of stateof-the-art retina-like imaging techniques and applications. First, we introduce the principle and implementing methods, including software and hardware, and describe the comparisons between them. Then, we present typical applications combined with retina-like imaging, including threedimensional acquisition and reconstruction, target tracking, deep learning, and ghost imaging. Finally, the challenges and outlook are discussed to further study for practical use. The results are beneficial for better understanding retina-like imaging. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Ghost imaging; Retina-like imaging; Three-dimensional reconstruction",,Review,"Final","",Scopus,2-s2.0-85111617189
"Wang H., Wang J., Bai K., Sun Y.","57208759829;55972836500;57219715800;57222614160;","Centered multi-task generative adversarial network for small object detection",2021,"Sensors","21","15","5194","","",,1,"10.3390/s21155194","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111604704&doi=10.3390%2fs21155194&partnerID=40&md5=f005a27e142c92e560825f477addc314","School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, 100081, China","Wang, H., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, 100081, China; Wang, J., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, 100081, China; Bai, K., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, 100081, China; Sun, Y., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, 100081, China","Despite the breakthroughs in accuracy and efficiency of object detection using deep neural networks, the performance of small object detection is far from satisfactory. Gaze estimation has developed significantly due to the development of visual sensors. Combining object detection with gaze estimation can significantly improve the performance of small object detection. This paper presents a centered multi-task generative adversarial network (CMTGAN), which combines small object detection and gaze estimation. To achieve this, we propose a generative adversarial network (GAN) capable of image super-resolution and two-stage small object detection. We exploit a generator in CMTGAN for image super-resolution and a discriminator for object detection. We introduce an artificial texture loss into the generator to retain the original feature of small objects. We also use a centered mask in the generator to make the network focus on the central part of images where small objects are more likely to appear in our method. We propose a discriminator with detection loss for two-stage small object detection, which can be adapted to other GANs for object detection. Compared with existing interpolation methods, the super-resolution images generated by CMTGAN are more explicit and contain more information. Experiments show that our method exhibits a better detection performance than mainstream methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Generative adversarial network; Image super-resolution; Two-stage small object detection","Deep neural networks; Object recognition; Optical resolving power; Textures; Adversarial networks; Detection loss; Detection performance; Gaze estimation; Image super resolutions; Interpolation method; Small object detection; Super resolution; Object detection; adaptation; human; image processing; methodology; Adaptation, Physiological; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Research Design",Article,"Final","",Scopus,2-s2.0-85111604704
"Garde G., Larumbe-Bergera A., Bossavit B., Porta S., Cabeza R., Villanueva A.","57215963483;57210106737;36730794700;7005292345;36763933900;7101612861;","Low-cost eye tracking calibration: A knowledge-based study†",2021,"Sensors","21","15","5109","","",,,"10.3390/s21155109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111278274&doi=10.3390%2fs21155109&partnerID=40&md5=3ad91a130121ac4257e854d6b109d0dd","Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain; Trinity College Dublin, School of Computer Science and Statistics, The University of Dublin, College Green, Dublin 2, D02 PN40, Ireland","Garde, G., Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain; Larumbe-Bergera, A., Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain; Bossavit, B., Trinity College Dublin, School of Computer Science and Statistics, The University of Dublin, College Green, Dublin 2, D02 PN40, Ireland; Porta, S., Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain; Cabeza, R., Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain; Villanueva, A., Department of Electrical, Electronic and Communications Engineering, Arrosadia Campus, Public University of Navarre, Pamplona, 31006, Spain","Subject calibration has been demonstrated to improve the accuracy in high-performance eye trackers. However, the true weight of calibration in off-the-shelf eye tracking solutions is still not addressed. In this work, a theoretical framework to measure the effects of calibration in deep learning-based gaze estimation is proposed for low-resolution systems. To this end, features extracted from the synthetic U2Eyes dataset are used in a fully connected network in order to isolate the effect of specific user’s features, such as kappa angles. Then, the impact of system calibration in a real setup employing I2Head dataset images is studied. The obtained results show accuracy improvements over 50%, probing that calibration is a key process also in low-resolution gaze estimation scenarios. Furthermore, we show that after calibration accuracy values close to those obtained by high-resolution systems, in the range of 0.7◦, could be theoretically obtained if a careful selection of image features was performed, demonstrating significant room for improvement for off-the-shelf eye tracking systems. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Calibration; Gaze-estimation; Low-resolution; Theoretical analysis","Calibration; Costs; Deep learning; Image enhancement; Knowledge based systems; Accuracy Improvement; Calibration accuracy; Eye tracking systems; Fully connected networks; High-resolution systems; Low cost eye tracking; System calibration; Theoretical framework; Eye tracking; calibration; eye fixation; Calibration; Eye-Tracking Technology; Fixation, Ocular",Article,"Final","",Scopus,2-s2.0-85111278274
"Min-Allah N., Jan F., Alrashed S.","24725148500;55247968500;14026381800;","Pupil detection schemes in human eye: a review",2021,"Multimedia Systems","27","4",,"753","777",,2,"10.1007/s00530-021-00806-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106312065&doi=10.1007%2fs00530-021-00806-5&partnerID=40&md5=fac0c0fd05f4653f80672999130452bd","Department of Computer Science, College of Computer Science and Information Technology, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia; Management Information Systems Department, College of Applied Studies and Community Service, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia","Min-Allah, N., Department of Computer Science, College of Computer Science and Information Technology, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia; Jan, F., Department of Computer Science, College of Computer Science and Information Technology, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia; Alrashed, S., Management Information Systems Department, College of Applied Studies and Community Service, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia","Pupil detection in a human eyeimage or video plays a key role in many applications such as eye-tracking, diabetic retinopathy screening, smart homes, iris recognition, etc. Literature reveals pupil detection faces many complications including light reflections, cataract disease, pupil constriction/dilation moments, contact lenses, eyebrows, eyelashes, hair strips, and closed eye. To cope with these challenges, research community has been struggling to devise resilient pupil localization schemes for the image/video data collected using the near-infrared (NIR) or visible spectrum (VS) illumination. This study presents a critical review of numerous pupil detection schemes taken from standard sources. This review includes pupil localization schemes based on machine learning, histogram/thresholding, Integro-differential operator (IDO), Hough transform and among others. The probable pros and cons of each scheme are highlighted. Finally, this study offers recommendations for designing a robust pupil detection system. As scope of pupil detection is very broader, therefore this review would be a great source of information for the relevant research community. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Biocybernetics; Deep learning; Gaze detection; Pupil detection; Smart cities; Smart systems; Super resolution","Automation; Biometrics; Diagnosis; Eye protection; Hough transforms; Infrared devices; Intelligent buildings; Mathematical operators; Diabetic retinopathy screening; Integro differential operator; Iris recognition; Pupil detection; Pupil localization; Research communities; Standard sources; Visible spectra; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85106312065
"Yang L., Dong K., Ding Y., Brighton J., Zhan Z., Zhao Y.","57219401199;57197782440;56597197500;15041679200;24337106100;57196243255;","Recognition of visual-related non-driving activities using a dual-camera monitoring system",2021,"Pattern Recognition","116",,"107955","","",,2,"10.1016/j.patcog.2021.107955","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103662190&doi=10.1016%2fj.patcog.2021.107955&partnerID=40&md5=d1569cea4cd1fea72f277c98c5e53118","School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, United Kingdom; Chongqing Automotive Collaborative Innovation Centre, Chongqing University, No.174 Shazheng St., Shapingba District, Chongqing  400044, China; Key Laboratory of Dynamics and Control of Flight Vehicle, Ministry of Education, School of Aerospace Engineering, Beijing Institute of Technology, Beijing, 100081, China","Yang, L., School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, United Kingdom; Dong, K., Chongqing Automotive Collaborative Innovation Centre, Chongqing University, No.174 Shazheng St., Shapingba District, Chongqing  400044, China; Ding, Y., Key Laboratory of Dynamics and Control of Flight Vehicle, Ministry of Education, School of Aerospace Engineering, Beijing Institute of Technology, Beijing, 100081, China; Brighton, J., School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, United Kingdom; Zhan, Z., Chongqing Automotive Collaborative Innovation Centre, Chongqing University, No.174 Shazheng St., Shapingba District, Chongqing  400044, China; Zhao, Y., School of Aerospace, Transport and Manufacturing, Cranfield University, Cranfield, United Kingdom","For a Level 3 automated vehicle, according to the SAE International Automation Levels definition (J3016), the identification of non-driving activities (NDAs) that the driver is engaging with is of great importance in the design of an intelligent take-over interface. Much of the existing literature focuses on the driver take-over strategy with associated Human-Machine Interaction design. This paper proposes a dual-camera based framework to identify and track NDAs that require visual attention. This is achieved by mapping the driver's gaze using a nonlinear system identification approach, on the object scene, recognised by a deep learning algorithm. A novel gaze-based region of interest (ROI) selection module is introduced and contributes about a 30% improvement in average success rate and about a 60% reduction in average processing time compared to the results without this module. This framework has been successfully demonstrated to identify five types of NDA required visual attention with an average success rate of 86.18%. The outcome of this research could be applicable to the identification of other NDAs and the tracking of NDAs within a certain time window could potentially be used to evaluate the driver's attention level for both automated and human-driving vehicles. © 2021","activities identification; Computer vision; Driver behaviour; Level 3 automation; Non-driving related task","Behavioral research; Cameras; Computer vision; Deep learning; Image segmentation; Automated vehicles; Automation levels; Cameras monitoring; Driver's behavior; Dual cameras; Human machine interaction; Level 3 automation; Monitoring system; Non-driving related task; Visual Attention; Automation",Article,"Final","",Scopus,2-s2.0-85103662190
"Yang B., Huang J., Sun M., Huo J., Li X., Xiong C.","57226054935;8621146500;57299165700;57207206228;57221840704;57211738191;","Head-free, Human Gaze-driven Assistive Robotic System for Reaching and Grasping",2021,"Chinese Control Conference, CCC","2021-July",,,"4138","4143",,,"10.23919/CCC52363.2021.9549800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117325986&doi=10.23919%2fCCC52363.2021.9549800&partnerID=40&md5=c915ef7753eba709d3185f0def88d93d","Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Huazhong University of Science and Technology, Sch. of Mech. Sci. and Eng. and the State Key Lab. of Digital Manufacturing Equipment and Technology, Wuhan, 430074, China","Yang, B., Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Huang, J., Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Sun, M., Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Huo, J., Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Li, X., Huazhong University of Science and Technology, Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; Xiong, C., Huazhong University of Science and Technology, Sch. of Mech. Sci. and Eng. and the State Key Lab. of Digital Manufacturing Equipment and Technology, Wuhan, 430074, China","Patients with limb dysfunction have limited mobility, which prevents them from performing daily activities. We have developed an assistive robot system with an intuitive head free gaze interface. The system consists of multiple modules, including 3D gaze estimation, head free coordinate transformation, intention recognition, and robot trajectory planning. The robotic assistive system obtains clues from the user's gaze to decode their intentions and implement actions. This allows the user only needs to look at the objects to make the robot system reach, grasp, and bring them to the user. The 3D gaze estimation is evaluated with 5 subjects, showing an overall accuracy of 5.53±1.2 cm. The integrated system's experimental results show that the success rate is 96% in the implementation of automatic trajectory planning, and the success rate is 92% in the implementation of fixation-based trajectory planning. Finally, the results and work required to improve the system are discussed. © 2021 Technical Committee on Control Theory, Chinese Association of Automation.","3D gaze estimation; Assistive robot; Gaze-based control; Intention estimation","Robotics; Robots; Trajectories; 3d gaze estimation; Assistive robotics; Assistive robots; Daily activity; Gaze estimation; Gaze-based control; Intention estimation; Robotic systems; Robots system; Trajectory Planning; Robot programming",Conference Paper,"Final","",Scopus,2-s2.0-85117325986
"Wu Y., Liang H., Hou X., Shen L.","57214135007;57222402814;57194454615;7401704647;","GazeFlow: Gaze Redirection with Normalizing Flows",2021,"Proceedings of the International Joint Conference on Neural Networks","2021-July",,,"","",,,"10.1109/IJCNN52387.2021.9533913","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116420910&doi=10.1109%2fIJCNN52387.2021.9533913&partnerID=40&md5=0d65ed98103ec2341ed61c5c01ab0d21","Shenzhen University, Computer Vision Institute, College of Computer Science and Software Engineering, China","Wu, Y., Shenzhen University, Computer Vision Institute, College of Computer Science and Software Engineering, China; Liang, H., Shenzhen University, Computer Vision Institute, College of Computer Science and Software Engineering, China; Hou, X., Shenzhen University, Computer Vision Institute, College of Computer Science and Software Engineering, China; Shen, L., Shenzhen University, Computer Vision Institute, College of Computer Science and Software Engineering, China","Gaze estimation often requires a large scale datasets with well annotated gaze information to train the estimator. However, such a dataset requires costive annotation and is usually very difficult to collect. Therefore, a number of gaze redirection approaches have been proposed to address such a problem. However, existing methods lack the ability to precisely synthesize images with target gaze and head pose in complex lighting scenes. As a powerful technique to model the distribution of given data, normalizing flows have the ability to generate photo-realistic images and provide flexible latent space manipulation. In this work, we present a novel flow-based generative model, GazeFlow11The code will be made available at https://github.com/CVI-SZU/GazeFlow, for gaze redirection. The visual results of gaze redirection show that the quality of eye images synthesized by GazeFlow is significantly higher than that of other approaches like Deep Warp and PRGAN. Our approach has also been applied to augment the training data to improve the accuracy of gaze estimators and significant improvement has been achieved for both within dataset and cross dataset experiments. © 2021 IEEE.",,"Computer vision; Eye images; Flow based; Gaze estimation; Generative model; Head pose; Large-scale datasets; Photorealistic images; Space manipulation; Synthesised; Training data; Large dataset",Conference Paper,"Final","",Scopus,2-s2.0-85116420910
"Canuto C., Moreno P., Samatelo J., Vassallo R., Santos-Victor J.","57207460501;22433465300;35753476500;7004279899;7003525618;","Action anticipation for collaborative environments: The impact of contextual information and uncertainty-based prediction",2021,"Neurocomputing","444",,,"301","318",,1,"10.1016/j.neucom.2020.07.135","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098185104&doi=10.1016%2fj.neucom.2020.07.135&partnerID=40&md5=d8856228f804bfc9df83b507c6d27d22","Room 20, CT-II, Department of Electrical Engineering, Federal University Espírito Santo, Av. Fernando Ferrari, 514, Goiabeiras, Vitória – ES, 29075-910, Brazil; Floor 7, North Tower, Institute for Systems and Robotics, Instituto Superior Técnico, Universidade de Lisboa, Av. Rovisco Pais, 1, Lisbon, 1049-001, Portugal","Canuto, C., Room 20, CT-II, Department of Electrical Engineering, Federal University Espírito Santo, Av. Fernando Ferrari, 514, Goiabeiras, Vitória – ES, 29075-910, Brazil; Moreno, P., Floor 7, North Tower, Institute for Systems and Robotics, Instituto Superior Técnico, Universidade de Lisboa, Av. Rovisco Pais, 1, Lisbon, 1049-001, Portugal; Samatelo, J., Room 20, CT-II, Department of Electrical Engineering, Federal University Espírito Santo, Av. Fernando Ferrari, 514, Goiabeiras, Vitória – ES, 29075-910, Brazil; Vassallo, R., Room 20, CT-II, Department of Electrical Engineering, Federal University Espírito Santo, Av. Fernando Ferrari, 514, Goiabeiras, Vitória – ES, 29075-910, Brazil; Santos-Victor, J., Floor 7, North Tower, Institute for Systems and Robotics, Instituto Superior Técnico, Universidade de Lisboa, Av. Rovisco Pais, 1, Lisbon, 1049-001, Portugal","To interact with humans in collaborative environments, machines need to be able to predict (i.e., anticipate) future events, and execute actions in a timely manner. However, the observation of the human limb movements may not be sufficient to anticipate their actions unambiguously. In this work, we consider two additional sources of information (i.e., context) over time, gaze, movement and object information, and study how these additional contextual cues improve the action anticipation performance. We address action anticipation as a classification task, where the model takes the available information as the input and predicts the most likely action. We propose to use the uncertainty about each prediction as an online decision-making criterion for action anticipation. Uncertainty is modeled as a stochastic process applied to a time-based neural network architecture, which improves the conventional class-likelihood (i.e., deterministic) criterion. The main contributions of this paper are fourfold: (i) We propose a novel and effective decision-making criterion that can be used to anticipate actions even in situations of high ambiguity; (ii) we propose a deep architecture that outperforms previous results in the action anticipation task when using the Acticipate collaborative dataset; (iii) we show that contextual information is important to disambiguate the interpretation of similar actions; and (iv) we also provide a formal description of three existing performance metrics that can be easily used to evaluate action anticipation models. Our results on the Acticipate dataset showed the importance of contextual information and the uncertainty criterion for action anticipation. We achieve an average accuracy of 98.75% in the anticipation task using only an average of 25% of observations. Also, considering that a good anticipation model should perform well in the action recognition task, we achieve an average accuracy of 100% in action recognition on the Acticipate dataset, when the entire observation set is used. © 2020 Elsevier B.V.","Action anticipation; Bayesian deep learning; Context information; Early action prediction; Uncertainty","Decision making; Forecasting; Network architecture; Random processes; Stochastic systems; Action anticipations; Classification tasks; Collaborative environments; Contextual information; Decision making criteria; On-line decision makings; Performance metrics; Sources of informations; Classification (of information); anticipation; article; decision making; deep learning; gaze; human; human experiment; prediction; stochastic model; uncertainty",Article,"Final","",Scopus,2-s2.0-85098185104
"Kocejko T.","24824292600;","Using deep learning to increase accuracy of gaze controlled prosthetic arm",2021,"International Conference on Human System Interaction, HSI","2021-July",,,"","",,,"10.1109/HSI52170.2021.9538710","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116064473&doi=10.1109%2fHSI52170.2021.9538710&partnerID=40&md5=cb80d4c7530832cff2d0376be9ca5394","Gdansk University of Technology, Department of Biomedical Engineering, Gdansk, Poland","Kocejko, T., Gdansk University of Technology, Department of Biomedical Engineering, Gdansk, Poland","This paper presents how neural networks can be utilized to improve the accuracy of reach and grab functionality of hybrid prosthetic arm with eye tracing interface. The LSTM based Autoencoder was introduced to overcome the problem of lack of accuracy of the gaze tracking modality in this hybrid interface. The gaze based interaction strongly depends on the eye tracking hardware. In this paper it was presented how the overall the accuracy can be slightly improved by software solution. The cloud of points related to possible final positions of the arm was created to train Autoencoder. The trained model was next used to improve the position provided by the eye tracker. Using the LSTM based Autoencoder resulted in nearly 3% improvement of the overall accuracy. © 2021 IEEE.","deep filter; gaze tracking; HCI; human computer interaction; prosthetic arm","Human computer interaction; Long short-term memory; Prosthetics; Auto encoders; Cloud of point; Deep filter; Eye-tracking; Gaze-based interaction; Gaze-tracking; Hybrid interface; Neural-networks; Prosthetic arm; Software solution; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116064473
"Palmero C., Sharma A., Behrendt K., Krishnakumar K., Komogortsev O.V., Talathi S.S.","57188829002;57219762538;57193013839;57219761370;6506328653;57224997923;","Openeds2020 challenge on gaze tracking for vr: Dataset and results",2021,"Sensors","21","14","4769","","",,,"10.3390/s21144769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109573720&doi=10.3390%2fs21144769&partnerID=40&md5=e4c512bb1b3a4d2607ae7716cabe7fbd","Department of Mathematics and Informatics, Universitat de Barcelona, Barcelona, 08007, Spain; Computer Vision Center, Campus UAB, Bellaterra08193, Spain; Eye Tracking Department, Facebook Reality Labs Research, Redmond, WA  98052, United States; Facebook Reality Labs, Menlo Park, CA  94025, United States; Department of Computer Science, Texas State University, San Marcos, TX  78666, United States","Palmero, C., Department of Mathematics and Informatics, Universitat de Barcelona, Barcelona, 08007, Spain, Computer Vision Center, Campus UAB, Bellaterra08193, Spain; Sharma, A., Eye Tracking Department, Facebook Reality Labs Research, Redmond, WA  98052, United States; Behrendt, K., Facebook Reality Labs, Menlo Park, CA  94025, United States; Krishnakumar, K., Facebook Reality Labs, Menlo Park, CA  94025, United States; Komogortsev, O.V., Eye Tracking Department, Facebook Reality Labs Research, Redmond, WA  98052, United States, Department of Computer Science, Texas State University, San Marcos, TX  78666, United States; Talathi, S.S., Eye Tracking Department, Facebook Reality Labs Research, Redmond, WA  98052, United States","This paper summarizes the OpenEDS 2020 Challenge dataset, the proposed baselines, and results obtained by the top three winners of each competition: (1) Gaze prediction Challenge, with the goal of predicting the gaze vector 1 to 5 frames into the future based on a sequence of previous eye images, and (2) Sparse Temporal Semantic Segmentation Challenge, with the goal of using temporal information to propagate semantic eye labels to contiguous eye image frames. Both competitions were based on the OpenEDS2020 dataset, a novel dataset of eye-image sequences captured at a frame rate of 100 Hz under controlled illumination, using a virtual-reality head-mounted display with two synchronized eye-facing cameras. The dataset, which we make publicly available for the research community, consists of 87 subjects performing several gaze-elicited tasks, and is divided into 2 subsets, one for each competition task. The proposed baselines, based on deep learning approaches, obtained an average angular error of 5.37 degrees for gaze prediction, and a mean intersection over union score (mIoU) of 84.1% for semantic segmentation. The winning solutions were able to outperform the baselines, obtaining up to 3.17 degrees for the former task and 95.2% mIoU for the latter. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Gaze estimation; Gaze prediction; Semantic segmentation; Video oculography; Virtual reality","Deep learning; Forecasting; Helmet mounted displays; Image segmentation; Semantics; Virtual reality; Angular errors; Frame rate; Gaze tracking; Head mounted displays; Learning approach; Research communities; Semantic segmentation; Temporal information; Eye tracking; human; photography; semantics; virtual reality; Eye-Tracking Technology; Humans; Photography; Semantics; Smart Glasses; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85109573720
"Luan X., Zhang B., Liu D., Liu X., Tong X., Li K.","57259651100;57259750700;57259691800;55981047600;57203925203;57204189178;","A Lightweight Heatmap-based Eye Tracking System",2021,"Proceedings - International Conference on Computer Communications and Networks, ICCCN","2021-July",,,"","",,,"10.1109/ICCCN52240.2021.9522300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114965179&doi=10.1109%2fICCCN52240.2021.9522300&partnerID=40&md5=34def4ed1eee9fcf330a4fbe96a20add","College of Intelligence and Computing, Tianjin University, Tianjin Key Laboratory of Advanced Networking (TANK), China","Luan, X., College of Intelligence and Computing, Tianjin University, Tianjin Key Laboratory of Advanced Networking (TANK), China; Zhang, B., College of Intelligence and Computing, Tianjin University, Tianjin Key Laboratory of Advanced Networking (TANK), China; Liu, D., College of Intelligence and Computing, Tianjin University, Tianjin Key Laboratory of Advanced Networking (TANK), China; Liu, X., College of Intelligence and Computing, Tianjin University, Tianjin Key Laboratory of Advanced Networking (TANK), China; Tong, X., College of Intelligence and Computing, Tianjin University, Tianjin Key Laboratory of Advanced Networking (TANK), China; Li, K., College of Intelligence and Computing, Tianjin University, Tianjin Key Laboratory of Advanced Networking (TANK), China","Eye tracking is playing an important role in many applications including human-computer interaction and behavior study. However, the existing approaches have at least one of the following limitations: (i) dedicated devices such as infrared camera and eye-tracker are required; (ii) complex calibration process is involved; (iii) substantial computing resources are consumed; (iv) users suffer from the risk of privacy leakage. To address the above limitations, we propose a H eatmap-based E ye T racking (HETrack) system. One of the key challenges in our system is to design a lightweight model for fine-grained tracking when the computing resources of device is limited. Also, it is necessary to protect user privacy in such a system. To address the above challenging issues, the proposed system consists of the following processes. First, when users randomly look at the screen of the device, HETrack obtains the raw image containing facial information. Then, we design a neural network model and train it with federated learning. The model can map the image to heatmap that implies the possibility of the user's gaze position on the screen. Finally, HETrack can intercept the real-time video stream into frames, and employ the trained model to generate the heatmap of current frame for gaze estimation. We implement HETrack based on a Commercial-Off-The-Shelf (COTS) camera and conduct extensive experiments to evaluate its performance. Our HETrack system only requires once calibration; whereas, the state-of-the-art work proposed by Google requires 35 times calibration on average. Unlike previous approaches that transmit raw image data to a central server, in our HETrack system, only parameters are transmitted, thereby well protecting the user's privacy. Experimental results demonstrate that the average distance error of estimated gaze point is 3cm, which is compatible with the state-of-the-art methods. © 2021 IEEE.","Eye tracking; Federated learning; Heatmap","Calibration; Cameras; Commercial off-the-shelf; Computer networks; Human computer interaction; Privacy by design; Calibration process; Computing resource; Eye tracking systems; Infra-red cameras; Neural network model; Real-time video streams; State of the art; State-of-the-art methods; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85114965179
"Spiller M., Liu Y.-H., Hossain M.Z., Gedeon T., Geissler J., Nürnberger A.","57211096099;26662786100;57212814547;24400830200;57221331539;14027288100;","Predicting Visual Search Task Success from Eye Gaze Data as a Basis for User-Adaptive Information Visualization Systems",2021,"ACM Transactions on Interactive Intelligent Systems","11","2","14","","",,1,"10.1145/3446638","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110737317&doi=10.1145%2f3446638&partnerID=40&md5=b89aa198b939f06d2316f727665dd46f","INKA-Innovation Laboratory for Image Guided Therapy, Health Campus Immunology Infectiology and Inflammation (GC-I3), Otto-von-Guericke-University, Leipziger Straße 44, Magdeburg, Germany; Oslo Metropolitan University, Pilestredet 48, Oslo, 0167, Norway; The Australi, Canberra, ACT  2600, Australia; Otto von Guericke University, Universitätsplatz 2, Magdeburg, 39106, Germany","Spiller, M., INKA-Innovation Laboratory for Image Guided Therapy, Health Campus Immunology Infectiology and Inflammation (GC-I3), Otto-von-Guericke-University, Leipziger Straße 44, Magdeburg, Germany; Liu, Y.-H., Oslo Metropolitan University, Pilestredet 48, Oslo, 0167, Norway; Hossain, M.Z., The Australi, Canberra, ACT  2600, Australia; Gedeon, T., The Australi, Canberra, ACT  2600, Australia; Geissler, J., Otto von Guericke University, Universitätsplatz 2, Magdeburg, 39106, Germany; Nürnberger, A., Otto von Guericke University, Universitätsplatz 2, Magdeburg, 39106, Germany","Information visualizations are an efficient means to support the users in understanding large amounts of complex, interconnected data; user comprehension, however, depends on individual factors such as their cognitive abilities. The research literature provides evidence that user-adaptive information visualizations positively impact the users' performance in visualization tasks. This study attempts to contribute toward the development of a computational model to predict the users' success in visual search tasks from eye gaze data and thereby drive such user-adaptive systems. State-of-the-art deep learning models for time series classification have been trained on sequential eye gaze data obtained from 40 study participants' interaction with a circular and an organizational graph. The results suggest that such models yield higher accuracy than a baseline classifier and previously used models for this purpose. In particular, a Multivariate Long Short Term Memory Fully Convolutional Network shows encouraging performance for its use in online user-adaptive systems. Given this finding, such a computational model can infer the users' need for support during interaction with a graph and trigger appropriate interventions in user-adaptive information visualization systems. This facilitates the design of such systems since further interaction data like mouse clicks is not required. © 2021 Association for Computing Machinery.","Eye tracking; individual differences; time series classification; user-adaptation","Adaptive systems; Computation theory; Computational methods; Convolutional neural networks; Data visualization; Deep learning; Digital storage; Information analysis; Information systems; Mammals; Online systems; Visualization; Cognitive ability; Computational model; Convolutional networks; Individual factors; Information visualization; State of the art; Time series classifications; User-adaptive systems; Search engines",Article,"Final","",Scopus,2-s2.0-85110737317
"Golard A., Talathi S.S.","57222990964;57224997923;","Ultrasound for gaze estimation—a modeling and empirical study",2021,"Sensors","21","13","4502","","",,,"10.3390/s21134502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108896359&doi=10.3390%2fs21134502&partnerID=40&md5=287ee868e89aece27cd880904cb69569","Facebook Reality Labs, Redmond, WA  98052, United States","Golard, A., Facebook Reality Labs, Redmond, WA  98052, United States; Talathi, S.S., Facebook Reality Labs, Redmond, WA  98052, United States","Most eye tracking methods are light-based. As such, they can suffer from ambient light changes when used outdoors, especially for use cases where eye trackers are embedded in Augmented Reality glasses. It has been recently suggested that ultrasound could provide a low power, fast, light-insensitive alternative to camera-based sensors for eye tracking. Here, we report on our work on modeling ultrasound sensor integration into a glasses form factor AR device to evaluate the feasibility of estimating eye-gaze in various configurations. Next, we designed a benchtop experimental setup to collect empirical data on time of flight and amplitude signals for reflected ultrasound waves for a range of gaze angles of a model eye. We used this data as input for a low-complexity gradient-boosted tree machine learning regression model and demonstrate that we can effectively estimate gaze (gaze RMSE error of 0.965 ± 0.178 degrees with an adjusted R2 score of 90.2 ± 4.6). © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","CMUT; Comsol Modeling; Eye tracking; Gaze estimation; Gradient Boosted Regression Trees; Machine Learning; Ultrasound","Augmented reality; Glass; Regression analysis; Trees (mathematics); Ultrasonics; Camera based Sensors; Empirical data; Empirical studies; Eye tracking methods; Gaze estimation; Regression model; Ultrasound sensors; Ultrasound waves; Eye tracking; echography; eye fixation; eye movement; machine learning; Augmented Reality; Eye Movements; Fixation, Ocular; Machine Learning; Ultrasonography",Article,"Final","",Scopus,2-s2.0-85108896359
"Oki T., Kizawa S.","56970664800;57274824000;","Evaluating visual impressions based on gaze analysis and deep learning: A case study of attractiveness evaluation of streets in densely built-up wooden residential area",2021,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2021",,"887","894",,,"10.5194/isprs-archives-XLIII-B3-2021-887-2021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115828084&doi=10.5194%2fisprs-archives-XLIII-B3-2021-887-2021&partnerID=40&md5=0da265ee673419ec59e854872ed2ec7c","Tokyo Institute of Technology, 2-12-1-M1-27 Ookayama, Meguro-ku Tokyo, 152-8550, Japan","Oki, T., Tokyo Institute of Technology, 2-12-1-M1-27 Ookayama, Meguro-ku Tokyo, 152-8550, Japan; Kizawa, S., Tokyo Institute of Technology, 2-12-1-M1-27 Ookayama, Meguro-ku Tokyo, 152-8550, Japan","This paper examines the possibility of impression evaluation based on gaze analysis of subjects and deep learning, using an example of evaluating street attractiveness in densely built-up wooden residential areas. Firstly, the relationship between the subjects' gazing tendency and their evaluation of street image attractiveness is analysed by measuring the subjects' gaze with an eye tracker. Next, we construct a model that can estimate an attractiveness evaluation result using convolutional neural networks (CNNs), combined with the method of gradient-weighted class activation mapping (Grad-CAM) - these in in visualizing which street components can contribute to evaluating attractiveness. Finally, we discuss the similarity between the subjects' gaze tendencies and activation heatmaps created by Grad-CAM. © 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.","Attractiveness; Convolutional Neural Network; Densely Built-up Wooden Residential Area; Gaze Analysis; Google Street View; Grad-CAM; Questionnaire; Semantic Segmentation","Cams; Chemical activation; Convolution; Convolutional neural networks; Deep learning; Eye tracking; Housing; Semantics; Activation mapping; Attractiveness; Convolutional neural network; Densely build-up wooden residential area; Gaze analysis; Google street view; Google+; Gradient-weighted class activation mapping; Questionnaire; Residential areas; Semantic segmentation; Semantic Segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85115828084
"Bernard V., Wannous H., Vandeborre J.-P.","57246531000;23391125300;6507497277;","Eye-Gaze Estimation using a Deep Capsule-based Regression Network",2021,"Proceedings - International Workshop on Content-Based Multimedia Indexing","2021-June",,"9461895","","",,,"10.1109/CBMI50038.2021.9461895","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114272153&doi=10.1109%2fCBMI50038.2021.9461895&partnerID=40&md5=942efad11eb6d145fb374314c2638e34","Univ. Lille, Cnrs Umr 9189 - CRIStAL, Imt Lille Douai, Lille, F-59000, France","Bernard, V., Univ. Lille, Cnrs Umr 9189 - CRIStAL, Imt Lille Douai, Lille, F-59000, France; Wannous, H., Univ. Lille, Cnrs Umr 9189 - CRIStAL, Imt Lille Douai, Lille, F-59000, France; Vandeborre, J.-P., Univ. Lille, Cnrs Umr 9189 - CRIStAL, Imt Lille Douai, Lille, F-59000, France","Eye-gaze information is used in a variety of user platforms, such as driver monitoring systems and head-mounted interfaces. In order to estimate human eye-gaze, many solutions have been proposed, using different devices and techniques. However, achieving such estimation using only cheap devices like RGB cameras would enable gaze interactions on mobile devices and therefore generalise this kind of interaction. It could also enable behavior studies based on gaze and made on every day devices. We propose in this paper a new method for eye-gaze estimation using a new deep learning architecture based on the Capsule Neural Network. Capsule Networks have shown great results so far on classification tasks, but only a few works use them for regression tasks.By taking advantage of the Capsule Network architecture and its ability to reconstruct images, we are able to recreate simplified eye images and then estimate human gaze from them. Experiments are performed on two representative datasets for the task of eye-gaze estimation. Encouraging results are obtained for both the estimation and the reconstruction. © 2021 IEEE.",,"Deep learning; Indexing (of information); Behavior studies; Classification tasks; Driver monitoring system; Eye images; Gaze interaction; Human eye; Learning architectures; RGB cameras; Network architecture",Conference Paper,"Final","",Scopus,2-s2.0-85114272153
"Sharma K., Papavlasopoulou S., Lee-Cultura S., Giannakos M.","55903734200;57063398000;57203855458;36462343600;","Information flow and children's emotions during collaborative coding: A causal analysis",2021,"Proceedings of Interaction Design and Children, IDC 2021",,,,"350","362",,,"10.1145/3459990.3460731","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110188729&doi=10.1145%2f3459990.3460731&partnerID=40&md5=a49f48a6104059202e0ee1067e5f2f2d","Idi Ntnu, Norway; Idi Norwegian University of Science and Technology, Norway; Department of Computer Science, Norwegian University of Science and Technology, Norway","Sharma, K., Idi Ntnu, Norway; Papavlasopoulou, S., Idi Ntnu, Norway; Lee-Cultura, S., Idi Norwegian University of Science and Technology, Norway; Giannakos, M., Department of Computer Science, Norwegian University of Science and Technology, Norway","This paper investigates the relation between children's joint gaze and emotions with the information flow of the screen from a causal point of view, in the context of collaborative coding. We employ Granger's definition of causality to extend the knowledge we have about children's collaborative activities from correlational methods. We organised a coding workshop with 50 children (10 dyads and 10 triads; 13-16 years old). While the children were coding collaboratively, their facial video and the screen were recorded. From the screen recording we computed the information flow; and from the facial video we computed children's emotions (e.g., frustration and boredom) and estimated their gaze. The gaze estimation was used to compute the joint visual attention (JVA) of the team. Our results show that for high performing teams JVA drives the information flow; while for low performing teams we observe causal relation between emotions and information flow. In particular for the low performing teams, frustration and boredom drive the information flow and the information flow then drives children's confusion. These results extend the understanding of the socio-cognitive processes underlying collaborative performance, which is primarily correlational in nature, with the causal relations between measurements. These novel results have the potential to guide the design of learning tools that scaffold children's learning and collaboration. © 2021 ACM.","affective states; causal modelling; Collaborative learning; Computational thinking; emotions; Granger causality; Joint visual attention","Causal analysis; Causal relations; Collaborative activities; Collaborative coding; Collaborative performance; Gaze estimation; Information flows; Visual Attention; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85110188729
[无可用作者姓名],[无可用的作者 ID],"WebSci 2021 - Proceedings of the 13th ACM Web Science Conference",2021,"ACM International Conference Proceeding Series",,,,"","",337,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109035421&partnerID=40&md5=3323049e1f2ae2df28670a8cc4ea55a5",,"","The proceedings contain 34 papers. The topics discussed include: the future of web and society; the coded gaze: algorithmic bias, facial recognition and beyond: how research can change the law and influence people; directions in digital government; the influence of search engine optimization on Google’s results: a multi-dimensional approach for detecting SEO; measuring digital literacy with eye tracking: an examination of skills and performance based on user gaze; towards a novel benchmark for automatic generation of ClaimReview markup; automatically selecting striking images for social cards; limiting tags fosters efficiency; on the conditions for integrating deep learning into the study of visual politics; and improving reactions to rejection in crowdsourcing through self-reflection.",,,Conference Review,"Final","",Scopus,2-s2.0-85109035421
"Barz M., Sonntag D.","57189847803;12241487800;","Automatic visual attention detection for mobile eye tracking using pre-trained computer vision models and human gaze",2021,"Sensors","21","12","4143","","",,,"10.3390/s21124143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107880702&doi=10.3390%2fs21124143&partnerID=40&md5=25527013262da014bdcb9b6f7ccc8042","Interactive Machine Learning Department, German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany; Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany","Barz, M., Interactive Machine Learning Department, German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany, Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany; Sonntag, D., Interactive Machine Learning Department, German Research Center for Artificial Intelligence (DFKI), Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany, Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany","Processing visual stimuli in a scene is essential for the human brain to make situation-aware decisions. These stimuli, which are prevalent subjects of diagnostic eye tracking studies, are commonly encoded as rectangular areas of interest (AOIs) per frame. Because it is a tedious manual annotation task, the automatic detection and annotation of visual attention to AOIs can accelerate and objectify eye tracking research, in particular for mobile eye tracking with egocentric video feeds. In this work, we implement two methods to automatically detect visual attention to AOIs using pre-trained deep learning models for image classification and object detection. Furthermore, we develop an evaluation framework based on the VISUS dataset and well-known performance metrics from the field of activity recognition. We systematically evaluate our methods within this framework, discuss potentials and limitations, and propose ways to improve the performance of future automatic visual attention detection methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Area of interest; Computer vision; Eye tracking; Eye tracking data analysis; Visual attention","Behavioral research; Computer vision; Deep learning; Object detection; Petroleum reservoir evaluation; Activity recognition; Automatic Detection; Evaluation framework; Eye-tracking studies; Manual annotation; Mobile eye-tracking; Performance metrics; Processing visual stimulus; Eye tracking; computer; eye movement; human; vision; Computers; Eye Movements; Eye-Tracking Technology; Humans; Vision, Ocular",Article,"Final","",Scopus,2-s2.0-85107880702
"Murthy L.R.D., Biswas P.","57205505055;14007579800;","Appearance-based gaze estimation using attention and difference mechanism",2021,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,,,"3137","3146",,,"10.1109/CVPRW53098.2021.00351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116069594&doi=10.1109%2fCVPRW53098.2021.00351&partnerID=40&md5=7eae6a6cbeba8f98d12c54f82a5d8a04","I3D Lab, CPDM, Indian Institute of Science, Bangalore, India","Murthy, L.R.D., I3D Lab, CPDM, Indian Institute of Science, Bangalore, India; Biswas, P., I3D Lab, CPDM, Indian Institute of Science, Bangalore, India","Appearance-based gaze estimation problem received wide attention over the past few years. Even though model-based approaches existed earlier, availability of large datasets and novel deep learning techniques made appearance-based methods achieve superior accuracy than model-based approaches. In this paper, we proposed two novel techniques to improve gaze estimation accuracy. Our first approach, I2D-Net uses a difference layer to eliminate any common features from left and right eyes of a participant that are not pertinent to gaze estimation task. Our second approach, AGE-Net adapted the idea of attention-mechanism and assigns weights to the features extracted from eye images. I2D-Net performed on par with the existing state-of-the-art approaches while AGE-Net reported state-of-the-art accuracy of 4.09 and 7.44 error on MPI-IGaze and RT-Gene datasets respectively. We performed ablation studies to understand the effectiveness of the proposed approaches followed by analysis of gaze error distribution with respect to various factors of MPIIGaze dataset. © 2021 IEEE.",,"Computer vision; Deep learning; Appearance based; Appearance-based methods; Attention mechanisms; Common features; Estimation problem; Gaze estimation; Large datasets; Learning techniques; Model based approach; Novel techniques; Large dataset",Conference Paper,"Final","",Scopus,2-s2.0-85116069594
"Siegfried R., Odobez J.-M.","57195685304;57203103085;","Visual focus of attention estimation in 3D scene with an arbitrary number of targets",2021,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,,,"3147","3155",,,"10.1109/CVPRW53098.2021.00352","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115987696&doi=10.1109%2fCVPRW53098.2021.00352&partnerID=40&md5=bd428ddb4dbf8194258c6b5d6bf7ba90","Idiap Research Institute, Martigny, Switzerland; Ecole Polytechnique Fédérale de Lausanne, Switzerland","Siegfried, R., Idiap Research Institute, Martigny, Switzerland, Ecole Polytechnique Fédérale de Lausanne, Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, Switzerland, Ecole Polytechnique Fédérale de Lausanne, Switzerland","Visual Focus of Attention (VFOA) estimation in conversation is challenging as it relies on difficult to estimate information (gaze) combined with scene features like target positions and other contextual information (speaking status) allowing to disambiguate situations. Previous VFOA models fusing all these features are usually trained for a specific setup and using a fixed number of interacting people, and should be retrained to be applied to another one, which limits their usability. To address these limitations, we propose a novel deep learning method that encodes all input features as a fixed number of 2D maps, which makes the input more naturally processed by a convolutional neural network, provides scene normalization, and allows to consider an arbitrary number of targets. Experiments performed on two publicly available datasets demonstrate that the proposed method can be trained in a cross-dataset fashion without loss in VFOA accuracy compared to intra-dataset training. © 2021 IEEE.",,"Computer vision; Convolutional neural networks; 3D scenes; Arbitrary number; Attention estimations; Attention model; Contextual information; Fixed numbers; Input features; Learning methods; Target position; Visual focus of attentions; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85115987696
"Tomas H., Reyes M., Dionido R., Ty M., Mirando J., Casimiro J., Atienza R., Guinto R.","57224588134;57224592630;57203033848;57224567882;57224575706;57191608798;8220749100;57224588386;","GOO: A dataset for gaze object prediction in retail environments",2021,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,,,"3119","3127",,,"10.1109/CVPRW53098.2021.00349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113681490&doi=10.1109%2fCVPRW53098.2021.00349&partnerID=40&md5=9e89f61d544136c76f2cc52a55271f6c","University of the Philippines; Samsung RandD Institute Philippines","Tomas, H., University of the Philippines; Reyes, M., University of the Philippines; Dionido, R., University of the Philippines; Ty, M., University of the Philippines; Mirando, J., University of the Philippines; Casimiro, J., University of the Philippines; Atienza, R., University of the Philippines; Guinto, R., Samsung RandD Institute Philippines","One of the most fundamental and information-laden actions humans do is to look at objects. However, a survey of current works reveals that existing gaze-related datasets annotate only the pixel being looked at, and not the boundaries of a specific object of interest. This lack of object an-notation presents an opportunity for further advancing gaze estimation research. To this end, we present a challenging new task called gaze object prediction, where the goal is to predict a bounding box for a person's gazed-at object. To train and evaluate gaze networks on this task, we present the Gaze On Objects (GOO) dataset. GOO is composed of a large set of synthetic images (GOO-Synth) supplemented by a smaller subset of real images (GOO-Real) of people looking at objects in a retail environment. Our work establishes extensive baselines on GOO by re-implementing and evaluating selected state-of-the-art models on the task of gaze following and domain adaptation. Code is available1 on github. © 2021 IEEE.",,"Computer vision; 'current; ART model; Bounding-box; Domain adaptation; Gaze estimation; Real images; State of the art; Synthetic images; Forecasting",Conference Paper,"Final","",Scopus,2-s2.0-85113681490
"Gatoula P., Dimas G., Iakovidis D.K., Koulaouzidis A.","57226190947;57195485922;6603967427;14627591700;","Enhanced CNN-Based gaze estimation on wireless capsule endoscopy images",2021,"Proceedings - IEEE Symposium on Computer-Based Medical Systems","2021-June",,"9474669","189","195",,,"10.1109/CBMS52027.2021.00070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110907745&doi=10.1109%2fCBMS52027.2021.00070&partnerID=40&md5=a6a4c9abd023a734c508eb1778260169","University of Thessaly, Dept. of Computer Science and Biomedical Informatics, Lamia, Greece; Pomeranian Medical University, Dept. of Social Medicine and Public Health, Szczecin, Poland","Gatoula, P., University of Thessaly, Dept. of Computer Science and Biomedical Informatics, Lamia, Greece; Dimas, G., University of Thessaly, Dept. of Computer Science and Biomedical Informatics, Lamia, Greece; Iakovidis, D.K., University of Thessaly, Dept. of Computer Science and Biomedical Informatics, Lamia, Greece; Koulaouzidis, A., Pomeranian Medical University, Dept. of Social Medicine and Public Health, Szczecin, Poland","Wireless capsule endoscopy (WCE) is a modality used for the non-invasive examination of the gastrointestinal (GI) tract. Physicians diagnose pathologies in images derived from Capsule Endoscopy (CE) using specific gaze patterns to observe pathologically related visual cues. Lately, deep learning has advanced in the domain of human eye-fixation estimation in natural images. However, the potentials of predicting the eye related patterns, such as eye fixations, in medical images has not been thoroughly investigated. In this work, we propose a CNN auto-encoder model, that is capable of predicting saliency maps estimating the gaze-patterns, in terms of eye-fixations, of physicians in CE images. The proposed model outperforms other approaches for visual saliency estimation based on physicians' eye fixation by providing an AUC-J of 0.726 among CE images depicting various pathological and normal cases. © 2021 IEEE.","Deep Learning; Visual Saliency; Wireless Capsule Endoscopy","Deep learning; Endoscopy; Medical imaging; Capsule endoscopy; Eye fixations; Gastrointestinal tract; Gaze estimation; Natural images; Visual saliency; Wireless capsule endoscopy; Wireless capsule endoscopy image; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85110907745
"Gu S., Wang L., He L., He X., Wang J.","36449028500;55080150600;57224909652;57188805587;57224988983;","Gaze Estimation via a Differential Eyes’ Appearances Network with a Reference Grid",2021,"Engineering","7","6",,"777","786",,1,"10.1016/j.eng.2020.08.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108090130&doi=10.1016%2fj.eng.2020.08.027&partnerID=40&md5=7a0c4690d5036f94a45536d6b2703569","Chengdu Aeronautic Polytechnic, Chengdu, 610100, China; Department of Production Engineering, KTH Royal Institute of Technology, Stockholm, 10044, Sweden","Gu, S., Chengdu Aeronautic Polytechnic, Chengdu, 610100, China; Wang, L., Department of Production Engineering, KTH Royal Institute of Technology, Stockholm, 10044, Sweden; He, L., Chengdu Aeronautic Polytechnic, Chengdu, 610100, China; He, X., Chengdu Aeronautic Polytechnic, Chengdu, 610100, China; Wang, J., Chengdu Aeronautic Polytechnic, Chengdu, 610100, China","A person's eye gaze can effectively express that person's intentions. Thus, gaze estimation is an important approach in intelligent manufacturing to analyze a person's intentions. Many gaze estimation methods regress the direction of the gaze by analyzing images of the eyes, also known as eye patches. However, it is very difficult to construct a person-independent model that can estimate an accurate gaze direction for every person due to individual differences. In this paper, we hypothesize that the difference in the appearance of each of a person's eyes is related to the difference in the corresponding gaze directions. Based on this hypothesis, a differential eyes’ appearances network (DEANet) is trained on public datasets to predict the gaze differences of pairwise eye patches belonging to the same individual. Our proposed DEANet is based on a Siamese neural network (SNNet) framework which has two identical branches. A multi-stream architecture is fed into each branch of the SNNet. Both branches of the DEANet that share the same weights extract the features of the patches; then the features are concatenated to obtain the difference of the gaze directions. Once the differential gaze model is trained, a new person's gaze direction can be estimated when a few calibrated eye patches for that person are provided. Because person-specific calibrated eye patches are involved in the testing stage, the estimation accuracy is improved. Furthermore, the problem of requiring a large amount of data when training a person-specific model is effectively avoided. A reference grid strategy is also proposed in order to select a few references as some of the DEANet's inputs directly based on the estimation values, further thereby improving the estimation accuracy. Experiments on public datasets show that our proposed approach outperforms the state-of-the-art methods. © 2021 THE AUTHORS","Cross-person evaluations; Differential gaze; Gaze estimation; Human–robot collaboration; Siamese neural network","Industrial engineering; Gaze direction; Gaze estimation; Individual Differences; Intelligent Manufacturing; Multi-stream architecture; Person-independent; Reference grids; State-of-the-art methods; Engineering",Article,"Final","",Scopus,2-s2.0-85108090130
"Ahmed M., Laskar R.H.","57206975401;23397200000;","Evaluation of accurate iris center and eye corner localization method in a facial image for gaze estimation",2021,"Multimedia Systems","27","3",,"429","448",,,"10.1007/s00530-020-00744-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099597525&doi=10.1007%2fs00530-020-00744-8&partnerID=40&md5=65eb3865911770c349f42b862d9699ee","Department of Electronics and Communication Engineering, National Institute of Technology Silchar, Assam, 788010, India","Ahmed, M., Department of Electronics and Communication Engineering, National Institute of Technology Silchar, Assam, 788010, India; Laskar, R.H., Department of Electronics and Communication Engineering, National Institute of Technology Silchar, Assam, 788010, India","Accurate estimation of eye-related information is important for many applications such as gaze estimation, face alignment, driver drowsiness detection, etc. Earlier works fail to estimate eye information in low-resolution images captured by a regular camera or webcam. This paper is aimed at developing an Iris Center (IC) and Eye Corner (EC) localization method in low-resolution facial images with an application of gaze estimation. A three-stage method is proposed for IC and EC localization. In the first stage, a circular gradient-intensity-based operator is proposed for rough ICs estimation and a CNN model is used in the second stage to find true ICs. In the third stage, Explicit Shape Regression (ESR) method is used for EC localization where initialization is done taking the ICs as a reference point to the mean eye contour shape model. The proposed IC localization method is evaluated on BioID and Gi4E database and it shows better accuracy compare to some of the state-of-the-art methods. This method further evaluated for gaze estimation based on IC and EC which does not require any prior calibrations unlike earlier infrared illumination-based gaze trackers. Here, the experiment for gaze estimation is performed in our proposed NITSGoP database that prepared under indoor conditions with complex background and uneven illuminations. The experimental results suggest that the proposed method can be used for gaze estimation with better accuracy both in still images and videos. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.","Cascaded regression; Eye corner detection; Eye verification; Gaze estimation; Image gradient; Iris center detection","Accurate estimation; Complex background; Infrared illumination; Localization method; Low resolution images; State-of-the-art methods; Three-stage method; Uneven illuminations; Integrated circuits",Article,"Final","",Scopus,2-s2.0-85099597525
[无可用作者姓名],[无可用的作者 ID],"Proceedings of the 12th Augmented Human International Conference, AH 2021",2021,"ACM International Conference Proceeding Series",,,,"","",80,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107984217&partnerID=40&md5=a77de61d51f3b9446d6931e2b813a100",,"","The proceedings contain 14 papers. The topics discussed include: I know you are looking to me: enabling eye-gaze communication between small children and parents with visual impairments; a multimodal tracking approach for augmented reality applications; deep learning for neuromarketing; classification of user preference using EEG Signals; design for long-term memory augmentation in personal knowledge management applications; PalmBeat: a kinesthetic way to feel groove with music; xBalloon: animated objects with balloon plastic actuator; PenShaft: enabling pen shaft detection and interaction for touchscreens; manipulating virtual objects in augmented reality using a new ball shaped input device; and xLimb: wearable robot arm with storable and extendable mechanisms.",,,Conference Review,"Final","",Scopus,2-s2.0-85107984217
"Rundo F., Leotta R., Battiato S.","55336120800;57221644663;6603989025;","Real-Time Deep Neuro-Vision Embedded Processing System for Saliency-based Car Driving Safety Monitoring",2021,"2021 4th International Conference on Circuits, Systems and Simulation, ICCSS 2021",,,"9464177","218","224",,,"10.1109/ICCSS51193.2021.9464177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113702181&doi=10.1109%2fICCSS51193.2021.9464177&partnerID=40&md5=2c0aab0974f1139ad7b2af9254f41b2d","STMicroelectronics, ADG Central RandD, Catania, Italy","Rundo, F., STMicroelectronics, ADG Central RandD, Catania, Italy; Leotta, R., STMicroelectronics, ADG Central RandD, Catania, Italy; Battiato, S., STMicroelectronics, ADG Central RandD, Catania, Italy","Recently, much interest has been aroused by scientific community regarding visual saliency-based applications. The proposed approach contributes to the progressive growth of knowledge of video saliency applications in the automotive field. Through the visual saliency detection, the car driver assistance systems (ADAS i.e. Advanced Driver Assistance Systems) are able to process the driving observation scene selectively. Specifically, it has been observed that the drivers along the driving route, focuses his/her gaze on some objects rather than others. This is determined by the perceptual activity of the brain which through the visual saliency determine the focused scene. We propose a driving safety assessment pipeline which combines a near-real time drowsiness car driver monitoring system driven by a visual saliency detection applied to the acquired driving scene. The proposed approach includes ad-hoc 3D pre-trained Semantic Segmentation Deep Network combined with ad-hoc 1D temporal Deep Dilated Convolutional Neural Network. This architecture was developed for the embedded platform based on STA1295 Accordo5 core (ARM A7 Dual-Cores) embedding an hardware graphics accelerator. The proposed system embeds a bio-sensor which will be placed on the steering wheel of the car having the target to collect the driver's Photoplethysmography (PPG) signal and which will result in a control of driver attention level. The so collected PPG time-series will be classified by the mentioned 1D Temporal Deep Convolutional Network which provides an assessment of the driver attention level. A final analyzer block verifies if the car driver attention level is adequate for the saliency-based scene classification. The performed tests confirmed the effectiveness of the overall proposed pipeline. © 2021 IEEE.","D-CNN; Deep learning; Deep-LSTM; Drowsiness; PPG (PhotoPlethySmography)","Automobile drivers; Automobile safety devices; Brain; Convolution; Convolutional neural networks; Deep neural networks; Object recognition; Pipelines; Semantics; Visualization; Convolutional networks; Embedded processing; Graphics accelerators; Photoplethysmography (PPG); Scene classification; Scientific community; Semantic segmentation; Visual saliency detections; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85113702181
"Gowroju S., Aarti, Kumar S.","57221499478;55904388800;57234304400;","Robust Pupil Segmentation using UNET and Morphological Image Processing",2021,"2021 International Mobile, Intelligent, and Ubiquitous Computing Conference, MIUCC 2021",,,"9447658","105","109",,,"10.1109/MIUCC52538.2021.9447658","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111361906&doi=10.1109%2fMIUCC52538.2021.9447658&partnerID=40&md5=4a95173b023de7b370926a566550ad58","Lovely Professional University, Computer Science Engineering, Punjab, India; Sreyas Institute of Engineering and Technology, Electronics Communication Engineering, Hyderabad, India","Gowroju, S., Lovely Professional University, Computer Science Engineering, Punjab, India; Aarti, Lovely Professional University, Computer Science Engineering, Punjab, India; Kumar, S., Sreyas Institute of Engineering and Technology, Electronics Communication Engineering, Hyderabad, India","The current development in image processing towards biometrics systems has opened much research on realtime applications. The deep learning algorithms are added many expectations to the researchers. The main challenges of these applications are vulnerability towards training time, detection accuracy, and accurate segmentation. In addition to this, the visual noise among various biometric systems is the main challenge. In this paper, we deployed the CNN model using modified UNet to perform the segmentation. The proposed method uses noisy images from the MMU (Multi Media University Iris database) dataset. The acquired colored eye images from the dataset exhibit specular reflections, eye gaze, off-angle images with less resolution, and occlusions caused by eyelids and eyelashes. The focus of our work is mainly to perform accurate segmentation in less training time. Compared the existing methods that uses UNet architecture, with the proposed method, we achieved an accuracy of 91.7%. © 2021 IEEE.","Accuracy; Morphological Processing; Pupil Segmentation; UNET","Biometrics; Deep learning; Learning algorithms; Physical addresses; Ubiquitous computing; Biometric systems; Detection accuracy; Morphological image processing; Pupil segmentation; Real-time application; Specular reflections; Training time; Visual noise; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85111361906
"Lei Y.","57224569585;","Eye Tracking Calibration on Mobile Devices",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169260",,"4","","",,,"10.1145/3450341.3457989","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107974459&doi=10.1145%2f3450341.3457989&partnerID=40&md5=42144bbc89c7ab4c8e47095895164c45","School of Computer Science University of St Andrews, United Kingdom","Lei, Y., School of Computer Science University of St Andrews, United Kingdom","Eye tracking has been widely used in psychology, human-computer interaction and many other fields. Recently, eye tracking based on off-The-shelf cameras has produced promising results, compared to the traditional eye tracking devices. This presents an opportunity to introduce eye tracking on mobile devices. However, eye tracking on mobile devices face many challenges, including occlusion of faces and unstable and changing distance between face and camera. This research project aims to obtain stable and accurate calibration of front-camera based eye tracking in dynamic contexts through the construction of real-world eye-movement datasets, the introduction of novel context-Awareness models and improved gaze estimation methods that can be adapted to partial faces. © 2021 ACM.","context awareness; eye tracking; gaze estimation","Calibration; Cameras; Eye movements; Human computer interaction; Camera-based; Context- awareness; Dynamic contexts; Eye tracking devices; Gaze estimation; Partial faces; Real-world; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107974459
"Eskildsen A.Mø., Witzner Hansen D.","57217096874;57224477246;","Analysis of iris obfuscation: Generalising eye information processes for privacy studies in eye tracking.",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169256",,,"","",,,"10.1145/3448017.3457385","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107667898&doi=10.1145%2f3448017.3457385&partnerID=40&md5=8e47374753a251475ea9fb9cf060f399","Eye Information Laboratory It, University of Copenhagen, Denmark","Eskildsen, A.Mø., Eye Information Laboratory It, University of Copenhagen, Denmark; Witzner Hansen, D., Eye Information Laboratory It, University of Copenhagen, Denmark","We present a framework to model and evaluate obfuscation methods for removing sensitive information in eye-tracking. The focus is on preventing iris-pattern identification. Candidate methods have to be effective at removing information while retaining high utility for gaze estimation. We propose several obfuscation methods that drastically outperform existing ones. A stochastic grid-search is used to determine optimal method parameters and evaluate the model framework. Precise obfuscation and gaze effects are measured for selected parameters. Two attack scenarios are considered and evaluated. We show that large datasets are susceptible to probabilistic attacks, even with seemingly effective obfuscation methods. However, additional data is needed to more accurately access the probabilistic security. © 2021 ACM.","eye-tracking; obfuscation; privacy","Large dataset; Stochastic models; Stochastic systems; Additional datum; Attack scenarios; Gaze estimation; Information process; Model framework; Optimal methods; Probabilistic securities; Sensitive informations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107667898
"Seida K., Takemura K.","57224457179;8575290600;","Eye Gaze Estimation using Imperceptible Marker Presented on High-Speed Display",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"7","","",,,"10.1145/3448018.3458000","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107611141&doi=10.1145%2f3448018.3458000&partnerID=40&md5=ac853505e30beeb2f667c346ffbe23ac","Tokai University, Japan","Seida, K., Tokai University, Japan; Takemura, K., Tokai University, Japan","Advanced eye-tracking methods require a dedicated display equipped with near-infrared LEDs (light-emitting diodes). However, this requirement hinders the widespread adoption of such methods. Additionally, some glints may pass undetected when a large display is employed. To avoid these problems, we propose eye gaze estimation using imperceptible markers presented on a commercially available high-speed display. The marker reference points reflected on the cornea are extracted instead of glints, and the point-of-gaze can be estimated using the cross-ratio method. The accuracy of the estimated point-of-gaze was approximately 1.64 degrees, as verified from experimental evaluations of the estimation using a high-speed display. © 2021 ACM.","Cross-ratio method; High-speed display; Imperceptible marker","Infrared devices; Cross-ratios; Experimental evaluation; Eye tracking methods; High speed displays; Large displays; Near-infrared leds; Point of gaze; Reference points; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107611141
"Emery K.J., Zannoli M., Warren J., Xiao L., Talathi S.S.","57191309772;50263543600;57223647367;57208440163;57224997923;","OpenNEEDS: A Dataset of Gaze, Head, Hand, and Scene Signals during Exploration in Open-Ended VR Environments",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"11","","",,1,"10.1145/3448018.3457996","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107605103&doi=10.1145%2f3448018.3457996&partnerID=40&md5=545b29a7a6100c8ef4aa6ea3a4d4e4cf","The Institute for Neuroscience, The University of Nevada, Reno, United States; Facebook Reality Labs, United States; Facebook Reality Labs Research, United States","Emery, K.J., The Institute for Neuroscience, The University of Nevada, Reno, United States; Zannoli, M., Facebook Reality Labs, United States; Warren, J., Facebook Reality Labs Research, United States; Xiao, L., The Institute for Neuroscience, The University of Nevada, Reno, United States; Talathi, S.S., The Institute for Neuroscience, The University of Nevada, Reno, United States","We present OpenNEEDS, the first large-scale, high frame rate, comprehensive, and open-source dataset of Non-Eye (head, hand, and scene) and Eye (3D gaze vectors) data captured for 44 participants as they freely explored two virtual environments with many potential tasks (i.e., reading, drawing, shooting, object manipulation, etc.). With this dataset, we aim to enable research on the relationship between head, hand, scene, and gaze spatiotemporal statistics and its applications to gaze estimation. To demonstrate the power of OpenNEEDS, we show that gaze estimation models using individual non-eye sensors and an early fusion model combining all non-eye sensors outperform all baseline gaze estimation models considered, suggesting the possibility of considering non-eye sensors in the design of robust eye trackers. We anticipate that this dataset will support research progress in many areas and applications such as gaze estimation and prediction, sensor fusion, human-computer interaction, intent prediction, perceptuo-motor control, and machine learning. © 2021 ACM.","datasets; eye tracking; gaze estimation; virtual reality","Human computer interaction; Large dataset; Statistics; 3D gaze vectors; Gaze estimation; High frame rate; ITS applications; Motor control; Object manipulation; Sensor fusion; Spatio-temporal statistics; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107605103
"Chaudhary A.K., Gyawali P.K., Wang L., Pelz J.B.","57210103548;57194830715;15043491500;7007018556;","Semi-Supervised Learning for Eye Image Segmentation",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"8","","",,,"10.1145/3448018.3458009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107601224&doi=10.1145%2f3448018.3458009&partnerID=40&md5=76741e35192356bc8ae2697c2dda89be","Rochester Institute of Technology, United States; Computing and Information Sciences Rochester, Institute of Technology, United States","Chaudhary, A.K., Rochester Institute of Technology, United States; Gyawali, P.K., Computing and Information Sciences Rochester, Institute of Technology, United States; Wang, L., Rochester Institute of Technology, United States; Pelz, J.B., Rochester Institute of Technology, United States","Recent advances in appearance-based models have shown improved eye tracking performance in difficult scenarios like occlusion due to eyelashes, eyelids or camera placement, and environmental reflections on the cornea and glasses. The key reason for the improvement is the accurate and robust identification of eye parts (pupil, iris, and sclera regions). The improved accuracy often comes at the cost of labeling an enormous dataset, which is complex and time-consuming. This work presents two semi-supervised learning frameworks to identify eye-parts by taking advantage of unlabeled images where labeled datasets are scarce. With these frameworks, leveraging the domain-specific augmentation and novel spatially varying transformations for image segmentation, we show improved performance on various test cases with limited labeled samples. For instance, for a model trained on just 4 and 48 labeled images, these frameworks improved by at least 4.7% and 0.4% respectively, in segmentation performance over the baseline model, which is trained only with the labeled dataset. © 2021 ACM.","AR/VR; eye-segmentation; eye-tracking; gaze-tracking; segmentation; semi-supervised learning","Image enhancement; Image segmentation; Semi-supervised learning; Appearance-based models; Baseline models; Camera placement; Domain specific; Labeled dataset; Labeled datasets; Robust identification; Segmentation performance; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107601224
"Kastrati A., Plomecka M.B., Wattenhofer R., Langer N.","57222152052;57195755873;6701529043;35766494700;","Using Deep Learning to Classify Saccade Direction from Brain Activity",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"28","","",,,"10.1145/3448018.3458014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107581882&doi=10.1145%2f3448018.3458014&partnerID=40&md5=cc8b0692b172394b4cb27a6849e75bf0","Department of Electrical Engineering and Eth, Zurich, Switzerland; Department of Psychology, Methods of Plasticity Research and University, Zurich, Switzerland; Eth Zurich and Department of Electrical Engineering, Switzerland","Kastrati, A., Department of Electrical Engineering and Eth, Zurich, Switzerland; Plomecka, M.B., Department of Psychology, Methods of Plasticity Research and University, Zurich, Switzerland; Wattenhofer, R., Eth Zurich and Department of Electrical Engineering, Switzerland; Langer, N., Department of Psychology, Methods of Plasticity Research and University, Zurich, Switzerland","We present first insights into our project that aims to develop an Electroencephalography (EEG) based Eye-Tracker. Our approach is tested and validated on a large dataset of simultaneously recorded EEG and infrared video-based Eye-Tracking, serving as ground truth. We compared several state-of-the-art neural network architectures for time series classification: InceptionTime, EEGNet, and investigated other architectures such as convolutional neural networks (CNN) with Xception modules and Pyramidal CNN. We prepared and tested these architectures with our rich dataset and obtained a remarkable accuracy of the left/right saccades direction classification (94.8 %) for the InceptionTime network, after hyperparameter tuning. © 2021 Owner/Author.","gaze detection; neural networks; simultaneous Electroencephalography and Eye-tracking; time-series classification","Brain; Classification (of information); Convolutional neural networks; Electroencephalography; Electrophysiology; Eye movements; Eye tracking; Large dataset; Network architecture; Brain activity; Eye trackers; Ground truth; Hyper-parameter; Infrared video; State of the art; Time series classifications; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85107581882
"Kübler T.C., Fuhl W., Wagner E., Kasneci E.","55701951700;56770084800;57194609927;56059892600;","55 Rides: Attention annotated head and gaze data during naturalistic driving",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"17","","",,,"10.1145/3448018.3457993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107577538&doi=10.1145%2f3448018.3457993&partnerID=40&md5=c53b5831baf62f054f8cad7c939d3925","University of Tübingen, Germany; Eberhard Karls Universität Tübingen Wilhelm Schickard Institut, Germany","Kübler, T.C., University of Tübingen, Germany; Fuhl, W., Eberhard Karls Universität Tübingen Wilhelm Schickard Institut, Germany; Wagner, E., University of Tübingen, Germany; Kasneci, E., University of Tübingen, Germany","Trained eye patterns are essential for safe driving. Whether for exploration of the surrounding traffic or to make sure that a lane is clear through a shoulder check - quick and effective perception is the key to driving safety. Surprisingly though, free and open access data on gaze behavior during driving are yet extremely sparse. The environment inside a vehicle is challenging for eye-tracking technology due to rapidly changing illumination conditions, such as exiting a tunnel to brightest sunlight, proper calibration and safety. So far, available data exhibits environments that likely influence the viewing behavior, sometimes dramatically (e.g., driving simulators without mirrors, limited field of view). We propose crowd-sourced eye-tracking data collected during real-world driving using NIR-cameras and illuminators that were placed within the driver's cabin. We analyze this data using a deep learning appearance-based gaze estimation, with raw videos not being part of the data set due to legal restrictions. Our data set contains four different drivers in their habitual cars and 55 rides of an average of 30 minutes length. At least three human raters rated each ride continuously with regard to driver attention and vigilance level on a ten-point scale. From the recorded videos we extracted drivers' head and eye movements as well as eye opening angle. For this data, we apply a normalization with respect to different placement of the driver monitoring camera and demonstrate a baseline for driver attention monitoring based on eye gaze and head movement features. © 2021 ACM.","datasets; driver attention; gaze detection; neural networks","Behavioral research; Cameras; Deep learning; Eye movements; Driver attention; Driver monitoring; Driving simulator; Eye tracking technologies; Illumination conditions; Legal restriction; Real-world drivings; Vigilance levels; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107577538
[无可用作者姓名],[无可用的作者 ID],"Proceedings - ETRA 2021: ACM Symposium on Eye Tracking Research and Applications, Short Papers Proceedings",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,,"","",240,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107539659&partnerID=40&md5=2c5188b175b4eb8e700f07b89a252b9e",,"","The proceedings contain 37 papers. The topics discussed include: eye tracking analysis of code layout, crowding and dyslexia - an open data set; synchronization of spontaneous eyeblink during formula car driving; towards gaze-based prediction of the intent to interact in virtual reality; pupillary response reflects vocabulary comprehension; understanding game roles and strategy using a mixed methods approach; understanding urban devotion through the eyes of an observer; repetition effects in task-driven eye movement analyses after longer time-spans; eye gaze estimation using imperceptible marker presented on high-speed display; and modelling of blink-related eyelid-induced shunting on the electrooculogram.",,,Conference Review,"Final","",Scopus,2-s2.0-85107539659
"Chaudhary A.K., Pelz J.B.","57210103548;7007018556;","Enhancing the precision of remote eye-tracking using iris velocity estimation",2021,"Eye Tracking Research and Applications Symposium (ETRA)","PartF169257",,"12","","",,,"10.1145/3448018.3458010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107530667&doi=10.1145%2f3448018.3458010&partnerID=40&md5=e3fe704bb6fdd7a0b021ccd31b54c659","Rochester Institute of Technology, United States; Center for Imaging Science Rochester, Institute of Technology, United States","Chaudhary, A.K., Rochester Institute of Technology, United States; Pelz, J.B., Center for Imaging Science Rochester, Institute of Technology, United States","Most of the previous work on eye-tracking has focused on positional information of the eye features. Recent advances in camera technology such as high-resolution and event cameras allow consideration of the velocity estimate for eye tracking. Some previous work on velocity-based estimates has demonstrated high-precision gaze estimation by tracking the motion of iris features on high-resolution images rather than by exploiting pupil edges. While these methods provide high precision, the bottleneck for velocity-based methods are temporal drift and the inability to track across blinks. In this work, we present a new theoretical methodology (It) to address these issues by optimally combining low-temporal frequency components of the pupil edges with the high-temporal frequency components from the iris textures. We show improved precision with this method while fixating a series of small targets and following a smoothly moving target. Further, we demonstrate the capability to reliably identify microsaccades between targets separated by 0.2°. © 2021 Owner/Author.","gaze estimation; iris features; microsaccades; smooth pursuit; velocity-based estimation","Cameras; Motion tracking; Textures; Velocity; Camera technology; Gaze estimation; High resolution; High resolution image; High temporal frequency; Positional information; Temporal frequency; Velocity estimation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85107530667
"Ashwinkumar J.S., Kumaran H.S., Sivakarthikeyan U., Rajesh K.P.B.V., Lavanya R.","57238080200;57238949000;57219121023;57238252500;55994222100;","Deep Learning based Approach for Facilitating Online Proctoring using Transfer Learning",2021,"2021 5th International Conference on Computer, Communication, and Signal Processing, ICCCSP 2021",,,"9465530","306","312",,,"10.1109/ICCCSP52374.2021.9465530","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113854494&doi=10.1109%2fICCCSP52374.2021.9465530&partnerID=40&md5=6acb3b1ef876c5707d616ce53dba0730","Amrita Vishwa Vidyapeetham, Amrita School of Engineering, Department of Electronics and Communication Engineering, Coimbatore, India","Ashwinkumar, J.S., Amrita Vishwa Vidyapeetham, Amrita School of Engineering, Department of Electronics and Communication Engineering, Coimbatore, India; Kumaran, H.S., Amrita Vishwa Vidyapeetham, Amrita School of Engineering, Department of Electronics and Communication Engineering, Coimbatore, India; Sivakarthikeyan, U., Amrita Vishwa Vidyapeetham, Amrita School of Engineering, Department of Electronics and Communication Engineering, Coimbatore, India; Rajesh, K.P.B.V., Amrita Vishwa Vidyapeetham, Amrita School of Engineering, Department of Electronics and Communication Engineering, Coimbatore, India; Lavanya, R., Amrita Vishwa Vidyapeetham, Amrita School of Engineering, Department of Electronics and Communication Engineering, Coimbatore, India","This paper aims at developing an algorithm which helps to ensure the reliability of online examinations. The proposed algorithm provides an automated approach to facilitate online proctoring which alleviates the cumbersome nature of its manual counterpart. It makes use of transfer learning to realize deep learning and it combines three models namely-YOLO (for fraudulent object and multi-person detection), MPGazeII (for abnormal gaze detection) and VGG16 (for Face recognition) and combines the results of all the individual anomaly detection algorithms to ultimately predict whether the examinee has been engaging in malpractice so that necessary action could be taken. The existing algorithms make use of huge amounts of processing for localization as well as tedious feature extraction to realize online proctoring. The proposed algorithm used Deep Learning to overcome the drawbacks of existing online Proctoring algorithms such that no hand-crafted feature extraction is involved. © 2021 IEEE.","Decision fusion; Deep Learning; GazeML; Neural Networks; Remote online proctoring; VGG-16; YOLO","Anomaly detection; E-learning; Electronic assessment; Extraction; Face recognition; Feature extraction; Learning algorithms; Object detection; Transfer learning; Anomaly-detection algorithms; Automated approach; Gaze detection; Learning-based approach; On-line examinations; Person detection; Three models; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85113854494
"Heck M., Edinger J., Becker C.","57205548572;56178122500;55683104700;","Conditioning Gaze-Contingent Systems for the Real World: Insights from a Field Study in the Fast Food Industry",2021,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,,"10.1145/3411763.3451658","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105819571&doi=10.1145%2f3411763.3451658&partnerID=40&md5=3a377116513e849c5396beafc9e8333b","University of Mannheim, Germany; University of Hamburg, Germany; Information Systems Ii, University of Mannheim, Germany","Heck, M., University of Mannheim, Germany; Edinger, J., University of Hamburg, Germany; Becker, C., Information Systems Ii, University of Mannheim, Germany","Eye tracking can be used to infer what is relevant to a user, and adapt the content and appearance of an application to support the user in their current task. A prerequisite for integrating such adaptive user interfaces into public terminals is robust gaze estimation. Commercial eye trackers are highly accurate, but require prior person-specific calibration and a relatively stable head position. In this paper, we collect data from 26 authentic customers of a fast food restaurant while interacting with a total of 120 products on a self-order terminal. From our observations during the experiment and a qualitative analysis of the collected gaze data, we derive best practice approaches regarding the integration of eye tracking software into self-service systems. We evaluate several implicit calibration strategies that derive the user's true focus of attention either from the context of the user interface, or from their interaction with the system. Our results show that the original gaze estimates can be visibly improved by taking into account both contextual and interaction-based information. © 2021 ACM.","eye tracker calibration; eye tracking; gaze interfaces","Calibration; Human engineering; User interfaces; Adaptive user interface; Fast-food industries; Fast-food restaurants; Focus of Attention; Gaze estimation; Public terminals; Qualitative analysis; Self-service systems; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85105819571
"Karakaya M.","35223530800;","Iris-ocular-periocular: Toward more accurate biometrics for off-angle images",2021,"Journal of Electronic Imaging","30","3","033035","","",,,"10.1117/1.JEI.30.3.033035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109025003&doi=10.1117%2f1.JEI.30.3.033035&partnerID=40&md5=bdb126c3bbcd9ca317bf1ded7f63a3f0","Kennesaw State University, Department of Computer Science, Marietta, GA, United States","Karakaya, M., Kennesaw State University, Department of Computer Science, Marietta, GA, United States","Iris is one of the most well-known biometrics; it is a nonintrusive and contactless authentication technique with high accuracy, enhanced security, and unique distinctiveness. However, its dependence on image quality and its frontal image acquisition requirement limit its recognition performance and hinder its potential use in standoff applications. Standoff biometric systems require a less controlled environment than traditional systems, so their captured images will likely be nonideal, including off-angle. We present convolutional neural network (CNN)-based deep learning frameworks to improve the recognition performance of iris, ocular, and periocular biometric modalities for off-angle images. Our contribution is fourfold: first, the performances of popular AlexNet, GoogLeNet, and ResNet50 architectures are presented for off-angle biometrics. Second, we study the effect of the gaze angle difference between training and testing images on iris, ocular, and periocular recognitions. Third, we investigate the network behavior for untrained gaze angles and the information fusion capability of CNN networks at multiple off-angle images. Finally, deep learning-based results are compared with a traditional iris recognition algorithm using the gallery approach. Our results with off-angle images ranging from -50 deg to 50 deg in gaze angle show that the proposed methods improve the recognition performance of iris, ocular, and periocular recognition. © The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.","biometrics; convolutional neural networks; iris recognition; ocular; off-angle; periocular","Biometrics; Convolutional neural networks; Deep learning; Authentication techniques; Biometric systems; Controlled environment; Iris recognition algorithm; Learning frameworks; Periocular recognition; Traditional systems; Training and testing; Image enhancement",Article,"Final","",Scopus,2-s2.0-85109025003
"Hu Z., Bulling A., Li S., Wang G.","57208101391;6505807414;56002421500;7407150270;","FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments",2021,"IEEE Transactions on Visualization and Computer Graphics","27","5","9382883","2681","2690",,1,"10.1109/TVCG.2021.3067779","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103251105&doi=10.1109%2fTVCG.2021.3067779&partnerID=40&md5=216cf8756fad33afadd5c6a0b28ffb3f","Peking University, China; University of Stuttgart, Germany","Hu, Z., Peking University, China; Bulling, A., University of Stuttgart, Germany; Li, S., Peking University, China; Wang, G., Peking University, China","Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction. However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications. We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments. Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities. Based on this analysis, we propose FixationNet - a novel learning-based model to forecast users' eye fixations in the near future in VR. We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93° to 2.35°) in free-viewing and of 15.1% (from 2.05° to 1.74°) in task-oriented situations. As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research. © 1995-2012 IEEE.","convolutional neural network; deep learning; Fixation forecasting; task-oriented attention; virtual reality; visual search","Behavioral research; Eye tracking; Comprehensive analysis; Gaze-based interaction; Gaze-contingent; Human visual attention; Immersive virtual reality; Learning Based Models; State of the art; Viewing conditions; Virtual reality",Article,"Final","",Scopus,2-s2.0-85103251105
"Liaqat S., Wu C., Duggirala P.R., Cheung S.-C.S., Chuah C.-N., Ozonoff S., Young G.","57222324379;57210639542;57222178800;34869344500;7004251298;7003652751;57225681104;","Predicting ASD diagnosis in children with synthetic and image-based eye gaze data",2021,"Signal Processing: Image Communication","94",,"116198","","",,4,"10.1016/j.image.2021.116198","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099252040&doi=10.1016%2fj.image.2021.116198&partnerID=40&md5=96a086cb9c784a893cb6b5ee17beefd0","University of Kentucky, United States; University of California, Davis, United States","Liaqat, S., University of Kentucky, United States; Wu, C., University of California, Davis, United States; Duggirala, P.R., University of California, Davis, United States; Cheung, S.-C.S., University of Kentucky, United States, University of California, Davis, United States; Chuah, C.-N., University of California, Davis, United States; Ozonoff, S., University of California, Davis, United States; Young, G., University of California, Davis, United States","As early intervention is highly effective for young children with autism spectrum disorder (ASD), it is imperative to make accurate diagnosis as early as possible. ASD has often been associated with atypical visual attention and eye gaze data can be collected at a very early age. An automatic screening tool based on eye gaze data that could identify ASD risk offers the opportunity for intervention before the full set of symptoms is present. In this paper, we propose two machine learning methods, synthetic saccade approach and image based approach, to automatically classify ASD given children's eye gaze data collected from free-viewing tasks of natural images. The first approach uses a generative model of synthetic saccade patterns to represent the baseline scan-path from a typical non-ASD individual and combines it with the real scan-path as well as other auxiliary data as inputs to a deep learning classifier. The second approach adopts a more holistic image-based approach by feeding the input image and a sequence of fixation maps into a convolutional or recurrent neural network. Using a publicly-accessible collection of children's gaze data, our experiments indicate that the ASD prediction accuracy reaches 67.23% accuracy on the validation dataset and 62.13% accuracy on the test dataset. © 2021 Elsevier B.V.","Autism spectrum disorders; Deep learning; Eye gaze data","Behavioral research; Eye movements; Learning systems; Statistical tests; Automatic screening; Auxiliary data; Early intervention; Generative model; Learning classifiers; Prediction accuracy; Publicly accessible; Visual Attention; Recurrent neural networks",Article,"Final","",Scopus,2-s2.0-85099252040
"Wright R., Ellis T.J., Makris D.","57221287885;7202590048;35616446000;","Measuring inferred gaze direction to support analysis of people in a meeting",2021,"Expert Systems with Applications","169",,"114398","","",,,"10.1016/j.eswa.2020.114398","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098684285&doi=10.1016%2fj.eswa.2020.114398&partnerID=40&md5=608d1804633f7bbae19fd6b898d14d6c","Faculty of Science, Engineering and Computing, Kingston University, Penrhyn Road Campus, Kingston upon Thames, KT1 2EE England, United Kingdom","Wright, R., Faculty of Science, Engineering and Computing, Kingston University, Penrhyn Road Campus, Kingston upon Thames, KT1 2EE England, United Kingdom; Ellis, T.J., Faculty of Science, Engineering and Computing, Kingston University, Penrhyn Road Campus, Kingston upon Thames, KT1 2EE England, United Kingdom; Makris, D., Faculty of Science, Engineering and Computing, Kingston University, Penrhyn Road Campus, Kingston upon Thames, KT1 2EE England, United Kingdom","This paper introduces a method to infer gaze direction and the point of focus of participants involved in a collaborative activity, such as a meeting. It uses a single depth-based sensor placed overhead to capture the meeting, which has the benefit of avoiding occlusion and is unobtrusive, minimising possible changes in behaviour that might arise if people are aware of the sensor. The inferred gaze direction of each participant is estimated in the horizontal plane from the orientation of the head (yaw), derived from a segmentation of the depth image to generate an outline of the head. A common focus of attention is inferred by intersecting the gaze directions of each participant. Performance evaluation using a depth camera to record a meeting achieved a head detection performance of 99.6% and a valid gaze detection of 96.9%. © 2020 Elsevier Ltd","Depth capture; Gaze estimation; Gaze intersection; Group common fixation point; Group meeting analysis; Visual focus of attention","Information systems; Mathematical models; Collaborative activities; Depth camera; Depth image; Focus of Attention; Gaze detection; Gaze direction; Head detection; Support analysis; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85098684285
"Hasan I., Setti F., Tsesmelis T., Belagiannis V., Amin S., Del Bue A., Cristani M., Galasso F.","57200394534;35191068200;55973117400;35483155200;55365733000;12752759400;8244336900;23396411100;","Forecasting People Trajectories and Head Poses by Jointly Reasoning on Tracklets and Vislets",2021,"IEEE Transactions on Pattern Analysis and Machine Intelligence","43","4","8883081","1267","1278",,3,"10.1109/TPAMI.2019.2949414","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102321703&doi=10.1109%2fTPAMI.2019.2949414&partnerID=40&md5=80c701fc472edebb5ceff37bb852a74d","Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Department of Computer Science, University of Verona, Verona, Italy; Italian Institute of Technology, Genova, Italy; Ulm University, Ulm, Germany; Osram GmbH, Munich, Germany","Hasan, I., Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Setti, F., Department of Computer Science, University of Verona, Verona, Italy; Tsesmelis, T., Italian Institute of Technology, Genova, Italy; Belagiannis, V., Ulm University, Ulm, Germany; Amin, S., Osram GmbH, Munich, Germany; Del Bue, A., Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Cristani, M., Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Galasso, F., Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates","In this article, we explore the correlation between people trajectories and their head orientations. We argue that people trajectory and head pose forecasting can be modelled as a joint problem. Recent approaches on trajectory forecasting leverage short-term trajectories (aka tracklets) of pedestrians to predict their future paths. In addition, sociological cues, such as expected destination or pedestrian interaction, are often combined with tracklets. In this article, we propose MiXing-LSTM (MX-LSTM) to capture the interplay between positions and head orientations (vislets) thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation. We additionally exploit the head orientations as a proxy for the visual attention, when modeling social interactions. MX-LSTM predicts future pedestrians location and head pose, increasing the standard capabilities of the current approaches on long-term trajectory forecasting. Compared to the state-of-the-art, our approach shows better performances on an extensive set of public benchmarks. MX-LSTM is particularly effective when people move slowly, i.e., the most challenging scenario for all other models. The proposed approach also allows for accurate predictions on a longer time horizon. © 1979-2012 IEEE.","gaze estimation; head pose estimation; LSTM; RNN; trajectory forecasting; visual attention","Behavioral research; Covariance matrix; Forecasting; Optimization; Trajectories; Accurate prediction; Covariance matrices; Long-term trajectories; Social interactions; State of the art; Time horizons; Unconstrained optimization; Visual Attention; Long short-term memory; article; back propagation; controlled study; covariance; forecasting; head position; human; human experiment; pedestrian; prediction; reasoning; social interaction; visual attention",Article,"Final","",Scopus,2-s2.0-85102321703
"Kim H., Ohmura Y., Kuniyoshi Y.","57217147980;7006439860;7005371269;","Gaze-Based Dual Resolution Deep Imitation Learning for High-Precision Dexterous Robot Manipulation",2021,"IEEE Robotics and Automation Letters","6","2","9354902","1630","1637",,,"10.1109/LRA.2021.3059619","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101294313&doi=10.1109%2fLRA.2021.3059619&partnerID=40&md5=34d777602459df6b922df5016a82b044","Laboratory for Intelligent Systems and Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan","Kim, H., Laboratory for Intelligent Systems and Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Ohmura, Y., Laboratory for Intelligent Systems and Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Kuniyoshi, Y., Laboratory for Intelligent Systems and Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan","A high-precision manipulation task, such as needle threading, is challenging. Physiological studies have proposed connecting low-resolution peripheral vision and fast movement to transport the hand into the vicinity of an object, and using high-resolution foveated vision to achieve the accurate homing of the hand to the object. The results of this study demonstrate that a deep imitation learning based method, inspired by the gaze-based dual resolution visuomotor control system in humans, can solve the needle threading task. First, we recorded the gaze movements of a human operator who was teleoperating a robot. Then, we used only a high-resolution image around the gaze to precisely control the thread position when it was close to the target. We used a low-resolution peripheral image to reach the vicinity of the target. The experimental results obtained in this study demonstrate that the proposed method enables precise manipulation tasks using a general-purpose robot manipulator and improves computational efficiency. © 2016 IEEE.","bioinspired robot learning; deep learning in grasping and manipulation; failure detection and recovery; Imitation learning; telerobotics and teleoperation","Agricultural robots; Computational efficiency; Industrial manipulators; Man machine systems; Manipulators; Needles; Robot applications; Robots; Dexterous robots; Dual resolutions; High resolution image; Imitation learning; Manipulation task; Peripheral vision; Precise manipulation; Robot manipulator; Deep learning",Article,"Final","",Scopus,2-s2.0-85101294313
"Liu J., Chi J., Hu W., Wang Z.","57217315767;8702376200;57220187912;55880036500;","3D Model-Based Gaze Tracking Via Iris Features with a Single Camera and a Single Light Source",2021,"IEEE Transactions on Human-Machine Systems","51","2","9270574","75","86",,,"10.1109/THMS.2020.3035176","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097143340&doi=10.1109%2fTHMS.2020.3035176&partnerID=40&md5=9194f4490d10e5d50f8646e17082fbf1","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, 100083, China","Liu, J., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Chi, J., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Hu, W., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Wang, Z., School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, 100083, China","Traditional 3D gaze estimation methods are usually based on the models of pupil refraction and corneal reflection. These methods typically rely on multiple light sources. The 3D gaze can be estimated using single-camera-single-light-source systems only when certain user-dependent eye parameters are available a priori, which is rarely the case. This article proposes a 3D gaze estimation method which works based on iris features using a single camera and a single light source. User-dependent eye parameters involving the iris radius and the cornea radius are user-calibrated. The 3D line-of-sight is estimated from the optical axis and the positional relationship between the optical axis and the visual axis, and then optimized using a binocular stereo vision model. The feasibility and robustness of the proposed method are assessed by simulations and practical experiments. The system configuration required by the method is simpler than that required by the state-of-the-art methods, which shows significant potential value, especially with regard to mobile device applications. © 2013 IEEE.","3D gaze estimation; iris radius; kappa angle; single-camera-single-light-source","3D modeling; Cameras; Light sources; Stereo image processing; Stereo vision; Binocular stereo vision; Corneal reflection; Light-source systems; Mobile device applications; Multiple light source; Positional relationship; State-of-the-art methods; System configurations; Eye tracking",Article,"Final","",Scopus,2-s2.0-85097143340
"Fan X., Wang F., Song D., Lu Y., Liu J.","56460611200;36065073900;57205425730;57205388984;8969748600;","GazMon: Eye Gazing Enabled Driving Behavior Monitoring and Prediction",2021,"IEEE Transactions on Mobile Computing","20","4","8944165","1420","1433",,,"10.1109/TMC.2019.2962764","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077339876&doi=10.1109%2fTMC.2019.2962764&partnerID=40&md5=389edf986a1fb1d070c0b0970f39e17c","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; Department of Computer and Information Science, University of Mississippi, University, MS, United States; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC  V6T 1Z4, Canada; Shenzhen Jiangxing Intelligence Inc., Shenzhen, Guangdong, 518057, China","Fan, X., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China, Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC  V6T 1Z4, Canada, Shenzhen Jiangxing Intelligence Inc., Shenzhen, Guangdong, 518057, China; Wang, F., Department of Computer and Information Science, University of Mississippi, University, MS, United States; Song, D., School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; Lu, Y., School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; Liu, J., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China, School of Computing Science, Simon Fraser University, Burnaby, BC, Canada","Automobiles have become one of the necessities of modern life, but also introduced numerous traffic accidents that threaten drivers and other road users. Most state-of-the-art safety systems are passively triggered, reacting to dangerous road conditions or driving maneuvers only after they happen and are observed, which greatly limits the last chances for collision avoidances. Timely tracking and predicting the driving maneuvers calls for a more direct interface beyond the traditional steering wheel/brake/gas pedal. In this paper, we argue that a driver's eyes are the interface, as it is the first and the essential window that gathers external information during driving. Our experiments suggest that a driver's gaze patterns appear prior to and correlate with the driving maneuvers for driving maneuver prediction. We accordingly present GazMon, an active driving maneuver monitoring and prediction framework for driving assistance applications. GazMon extracts the gaze information through a front-camera and analyzes the facial features, including facial landmarks, head pose, and iris centers, through a carefully constructed deep learning architecture. Both our on-road experiments and driving simulator based evaluations demonstrate the superiority of our GazMon on predicting driving maneuvers as well as other distracted behaviors. It is readily deployable using RGB cameras and allows reuse of existing smartphones towards more safely driving. © 2002-2012 IEEE.","deep learning; driving assistant; Gaze; mobile computing","Accidents; Cameras; Deep learning; Mobile computing; Roads and streets; Driving assistance; Driving Assistant; Driving behavior; Driving simulator; External informations; Gaze; Learning architectures; State of the art; Forecasting",Article,"Final","",Scopus,2-s2.0-85077339876
"Jeong J.E., Choi Y.S.","57223140112;56055020700;","Depth-enhanced gaze following method",2021,"Proceedings of the ACM Symposium on Applied Computing",,,,"1090","1093",,,"10.1145/3412841.3442107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105011621&doi=10.1145%2f3412841.3442107&partnerID=40&md5=2d819f6055085afeb059d1cb4a470c32","Hanyang University, Seoul, South Korea","Jeong, J.E., Hanyang University, Seoul, South Korea; Choi, Y.S., Hanyang University, Seoul, South Korea","Gaze following is the task of detecting the point of attention of where a third person gaze is staring in a single image. Existing studies have made some modifications to architectures or have additionally learned the gaze angle, and have achieved notable performances. However, when a complex scene is given, the methods generally predict incorrect locations because of the lack of depth information in an RGB image. In this paper, we propose a novel three-stage deep neural networks algorithm to tackle such challenging scenes using a depth map. We achieve state-of-the-art performance on the GazeFollow dataset and examine possibilities for the research of depth information in image interpretation. Moreover, a qualitative comparison shows that our method works stably and accurately for complex scenes similar to those found in real-world photographs. © 2021 Owner/Author.","gaze estimation; gaze following; salient region detections; visual attention","Complex networks; Complex scenes; Depth information; Gaze angles; Image interpretation; Neural networks algorithms; RGB images; Single images; State-of-the-art performance; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85105011621
"Gapi K.T., Magbitang R.M.G., Villaverde J.F.","57266871300;57267728100;57195431920;","Classification of Attentiveness on Virtual Classrooms using Deep Learning for Computer Vision",2021,"ACM International Conference Proceeding Series",,,,"34","39",,,"10.1145/3460238.3460244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115384175&doi=10.1145%2f3460238.3460244&partnerID=40&md5=b084081ced16c4bd382d740cf3e64a30","School of Electrical, Electronics and Computer Engineering, Mapua University, Philippines","Gapi, K.T., School of Electrical, Electronics and Computer Engineering, Mapua University, Philippines; Magbitang, R.M.G., School of Electrical, Electronics and Computer Engineering, Mapua University, Philippines; Villaverde, J.F., School of Electrical, Electronics and Computer Engineering, Mapua University, Philippines","Nowadays, virtual classrooms are highly encouraged due to the COVID-19 pandemic. This could be a disadvantage because some students might not really be engaged with this kind of setup. This study presents a system for classifying level of attentiveness on virtual classrooms using deep learning for computer vision. The study confined in the development of the technology for classifying attentiveness itself, the integration of the system to virtual classrooms is not included in the scope. The criteria for the classification include the prediction of droopy corners of mouth facial cue, hanging eyelid facial cue, eye state, and eye gaze. The software of the system used the combinations of Convolutional Neural Network (CNN) models, Dlib, and OpenCV library. After evaluation, the system was able to successfully classify attentiveness of three classes with an overall accuracy of 83.33%. © 2021 ACM.","Computer vision; Convolutional Neural Network models; Deep learning; Dlib library; Facial cue; OpenCV","Computer aided instruction; Convolution; Convolutional neural networks; Deep learning; Digital libraries; E-learning; Convolutional neural network; Convolutional neural network model; Deep learning; Dlib library; Eye-gaze; Facial cue; Neural network model; Opencv; Overall accuracies; Virtual Classroom; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85115384175
"Chen J., Li Q., Ling H., Ren D., Duan P.","57196106177;57221683066;7202062619;57221174347;57221174689;","Audiovisual saliency prediction via deep learning",2021,"Neurocomputing","428",,,"248","258",,1,"10.1016/j.neucom.2020.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098474959&doi=10.1016%2fj.neucom.2020.12.011&partnerID=40&md5=ed76e852d5bb2b3896ae5fdb13adb5fe","School of Computer Science and Technology, Huazhong University of Science and Technology, China; School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, China; Department of Information Engineering, Hubei Urban Construction Vocational and Technological College, China","Chen, J., School of Computer Science and Technology, Huazhong University of Science and Technology, China; Li, Q., School of Computer Science and Technology, Huazhong University of Science and Technology, China; Ling, H., School of Computer Science and Technology, Huazhong University of Science and Technology, China; Ren, D., School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, China; Duan, P., Department of Information Engineering, Hubei Urban Construction Vocational and Technological College, China","Neuroscience study verifies that synchronized audiovisual stimuli would make a stronger response of visual perception than an independent stimulus. Many researches show that audio signals would affect human gaze behavior in the viewing of natural video scenes. Thus in this paper, we propose a multi-sensory framework of audio and visual signals for video saliency prediction. It mainly includes four modules: auditory feature extraction, visual feature extraction, semantic interaction between auditory feature and visual feature, and feature fusion. With the inputs of audio and visual signals, we present a network architecture of deep learning to undertake the tasks of these four modules. It is an end-to-end architecture that could interact the semantics from its learned features of audio and visual stimuli. The numerical and visual results show our method achieves a significant improvement over eleven recent saliency models that are regardless of the audio stimuli, even some of them are state-of-the-art deep learning models. © 2020 Elsevier B.V.","Audiovisual saliency; Deep learning; Semantic interaction; Visual attention","Audiovisual; Behavioral research; Extraction; Feature extraction; Network architecture; Numerical methods; Semantics; Audio-visual stimulus; Auditory feature; Semantic interactions; State of the art; Video saliencies; Visual feature extraction; Visual perception; Visual stimulus; Deep learning; article; deep learning; feature extraction; human; human experiment; prediction; semantics; videorecording; visual attention",Article,"Final","",Scopus,2-s2.0-85098474959
[无可用作者姓名],[无可用的作者 ID],"ICIAI 2021- 5th International Conference on Innovation in Artificial Intelligence",2021,"ACM International Conference Proceeding Series","PartF171546",,,"","",252,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114614782&partnerID=40&md5=419b64410da93e72101e4b69cee1f1f9",,"","The proceedings contain 39 papers. The topics discussed include: a domain data pattern randomization based deep reinforcement learning method for sim-to-real transfer; scale adaptive and lightweight super-resolution with a selective hierarchical residual network; deep multi-scale recursive residual attention network for spectral super resolution; multiple biases-incorporated latent factorization of tensors for dynamic network link prediction; optical flow estimation with foreground attention guided network; Chinese description of videos incorporating multimodal features and attention mechanism; infinite von mises-fisher mixture model and its application to gene expression data clustering; reversible data hiding in encrypted images based on bit-plane rearrangement and Huffman coding; and face shows your intention: visual search based on full-face gaze estimation with channel-spatial attention.",,,Conference Review,"Final","",Scopus,2-s2.0-85114614782
"Liu S., Zhou X.-D., Jiang X., Wu H., Shi Y.","57221491395;57191432161;57251548800;57221706335;57213417833;","Face Shows Your Intention: Visual Search Based on Full-face Gaze Estimation with Channel-spatial Attention",2021,"ACM International Conference Proceeding Series","PartF171546",,,"76","81",,,"10.1145/3461353.3461362","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114557535&doi=10.1145%2f3461353.3461362&partnerID=40&md5=43f48a076d704969d9a690f351f65ff5","Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Chongqing University of Education, China","Liu, S., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Zhou, X.-D., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Jiang, X., Chongqing University of Education, China; Wu, H., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Shi, Y., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China","Visual search is the process that humans use visual perception to recognize targets of interest among multitudinous objects, which is a challenging research topic in computer vision. In contrast to previous works that take the overt gaze signal as input to predict the target of visual search with computational models, we proposed a visual search network based on full-face gaze estimation with channel-spatial mechanism, which can directly predict the user's search objects from the full-face images without extra obtaining the prohibitive intermediate gaze data. We seamlessly integrate the gaze information generated by the full-face gaze estimation module and the semantic information of the scene image into the visual search network that can directly infer the user's search intention. We demonstrate the effectiveness of our method for visual search task in real-world settings, and illustrate that directions for future research on full-face based human visual cognition. © 2021 Association for Computing Machinery. All rights reserved.","Attention mechanism; Eye tracking; Gaze estimation; Machine learning; Visual search","Semantics; Computational model; Real world setting; Search intentions; Semantic information; Spatial attention; Spatial mechanism; Targets of interest; Visual perception; Artificial intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85114557535
"Arslan Aydin Ü., Kalkan S., Acartürk C.","57222505565;23027462100;25652940800;","Speech Driven Gaze in a Face-to-Face Interaction",2021,"Frontiers in Neurorobotics","15",,"598895","","",,,"10.3389/fnbot.2021.598895","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102960088&doi=10.3389%2ffnbot.2021.598895&partnerID=40&md5=0fa104a7b72319af1944ff42f1bf0d2d","Cognitive Science Department, Middle East Technical University, Ankara, Turkey; Computer Engineering Department, Middle East Technical University, Ankara, Turkey; Cyber Security Department, Middle East Technical University, Ankara, Turkey","Arslan Aydin, Ü., Cognitive Science Department, Middle East Technical University, Ankara, Turkey; Kalkan, S., Computer Engineering Department, Middle East Technical University, Ankara, Turkey; Acartürk, C., Cognitive Science Department, Middle East Technical University, Ankara, Turkey, Cyber Security Department, Middle East Technical University, Ankara, Turkey","Gaze and language are major pillars in multimodal communication. Gaze is a non-verbal mechanism that conveys crucial social signals in face-to-face conversation. However, compared to language, gaze has been less studied as a communication modality. The purpose of the present study is 2-fold: (i) to investigate gaze direction (i.e., aversion and face gaze) and its relation to speech in a face-to-face interaction; and (ii) to propose a computational model for multimodal communication, which predicts gaze direction using high-level speech features. Twenty-eight pairs of participants participated in data collection. The experimental setting was a mock job interview. The eye movements were recorded for both participants. The speech data were annotated by ISO 24617-2 Standard for Dialogue Act Annotation, as well as manual tags based on previous social gaze studies. A comparative analysis was conducted by Convolutional Neural Network (CNN) models that employed specific architectures, namely, VGGNet and ResNet. The results showed that the frequency and the duration of gaze differ significantly depending on the role of participant. Moreover, the ResNet models achieve higher than 70% accuracy in predicting gaze direction. © Copyright © 2021 Arslan Aydin, Kalkan and Acartürk.","deep learning; face-to-face interaction; gaze analysis; multimodal communication; speech annotation","Convolutional neural networks; Eye movements; Communication modalities; Comparative analysis; Computational model; Data collection; Face-to-face conversation; Face-to-face interaction; Multimodal communications; Speech features; Speech communication; adult; article; aversion; clinical article; computer model; convolutional neural network; deep learning; eye movement; female; gaze; human; human experiment; job interview; male; residual neural network; speech",Article,"Final","",Scopus,2-s2.0-85102960088
"Kapp S., Barz M., Mukhametov S., Sonntag D., Kuhn J.","57195483291;57189847803;57211779287;12241487800;55984409000;","Arett: Augmented reality eye tracking toolkit for head mounted displays",2021,"Sensors","21","6","2234","","",,5,"10.3390/s21062234","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102790558&doi=10.3390%2fs21062234&partnerID=40&md5=44063cb55782a4842be02ef315ce4c64","Department of Physics, Technische Universität Kaiserslautern, Erwin-Schrödinger-Str. 46, Kaiserslautern, 67663, Germany; German Research Center for Artificial Intelligence (DFKI), Interactive Machine Learning Department, Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany; Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany","Kapp, S., Department of Physics, Technische Universität Kaiserslautern, Erwin-Schrödinger-Str. 46, Kaiserslautern, 67663, Germany; Barz, M., German Research Center for Artificial Intelligence (DFKI), Interactive Machine Learning Department, Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany, Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany; Mukhametov, S., Department of Physics, Technische Universität Kaiserslautern, Erwin-Schrödinger-Str. 46, Kaiserslautern, 67663, Germany; Sonntag, D., German Research Center for Artificial Intelligence (DFKI), Interactive Machine Learning Department, Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, Saarbrücken, 66123, Germany, Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, Oldenburg, 26129, Germany; Kuhn, J., Department of Physics, Technische Universität Kaiserslautern, Erwin-Schrödinger-Str. 46, Kaiserslautern, 67663, Germany","Currently an increasing number of head mounted displays (HMD) for virtual and augmented reality (VR/AR) are equipped with integrated eye trackers. Use cases of these integrated eye trackers include rendering optimization and gaze-based user interaction. In addition, visual attention in VR and AR is interesting for applied research based on eye tracking in cognitive or educational sciences for example. While some research toolkits for VR already exist, only a few target AR scenarios. In this work, we present an open-source eye tracking toolkit for reliable gaze data acquisition in AR based on Unity 3D and the Microsoft HoloLens 2, as well as an R package for seamless data analysis. Furthermore, we evaluate the spatial accuracy and precision of the integrated eye tracker for fixation targets with different distances and angles to the user (n = 21). On average, we found that gaze estimates are reported with an angular accuracy of 0.83 degrees and a precision of 0.27 degrees while the user is resting, which is on par with state-of-the-art mobile eye trackers. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Accuracy; Augmented reality; Eye tracking; Precision; Toolkit","Augmented reality; Behavioral research; Data acquisition; Helmet mounted displays; Educational science; Head mounted displays; Rendering optimizations; Spatial accuracy; State of the art; User interaction; Virtual and augmented reality; Visual Attention; Eye tracking; virtual reality; Augmented Reality; Eye-Tracking Technology; Smart Glasses; Virtual Reality",Article,"Final","",Scopus,2-s2.0-85102790558
"Emery K.J., Zannoli M., Xiao L., Warren J., Talathi S.S.","57191309772;50263543600;57208440163;57223647367;57224997923;","Estimating gaze from head and hand pose and scene images for open-ended exploration in VR Environments",2021,"Proceedings - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2021",,,"9419188","554","555",,,"10.1109/VRW52623.2021.00159","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105969306&doi=10.1109%2fVRW52623.2021.00159&partnerID=40&md5=ef12b054c3d59e2eeaf19f2edb1de259","University of Nevada, Reno, Facebook Reality Labs; Facebook Reality Labs","Emery, K.J., University of Nevada, Reno, Facebook Reality Labs; Zannoli, M., Facebook Reality Labs; Xiao, L., Facebook Reality Labs; Warren, J., Facebook Reality Labs; Talathi, S.S., Facebook Reality Labs","The widespread utility of eye tracking technology has created a growing demand for more consistent and reliable eye-tracking systems, and there is a need for new and accessible approaches that can enhance the accuracy of eye-tracking data. Previous studies have offered evidence for associations between certain non-eye signals and gaze such as a strong coordination between head motion and gaze shifts. e.g. [3] , hand and eye spatiotemporal statistics, e.g. [7] , and gaze behavior and scene content, e.g. [2]. Previous studies have also shown how various combinations of eye, head, scene, and hand signals can be leveraged for applications such as gaze estimation [5] , [10] , prediction [8] , and classification [6]. Though these previous approaches provide support for the idea that non-eye sensors (i.e. head, hand, and scene) are useful for estimating gaze, they have not yet fully addressed how these signals individually and in combination contribute to gaze estimation. © 2021 IEEE.","Eye tracking; Gaze estimation; Non eye sensors; Virtual reality","Abstracting; Statistics; User interfaces; Virtual reality; Eye tracking systems; Eye tracking technologies; Gaze behavior; Gaze estimation; Gaze shifts; Growing demand; Scene image; Spatio-temporal statistics; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85105969306
"Hu Z.","57208101391;","[DC] Eye fixation forecasting in task-oriented virtual reality",2021,"Proceedings - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2021",,,"9419161","707","708",,,"10.1109/VRW52623.2021.00236","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105966888&doi=10.1109%2fVRW52623.2021.00236&partnerID=40&md5=f1c5e70c67e30187665a531cc1408f81","Peking University, China","Hu, Z., Peking University, China","In immersive virtual reality (VR), users' visual attention is crucial for many important applications, including VR content design, gaze-based interaction, and gaze-contingent rendering. Especially, information on users' future eye fixations is key for intelligent user interfaces and has significant relevance for many areas, such as visual attention enhancement, dynamic event triggering, and human-computer interaction. However, previous works typically focused on free-viewing conditions and paid less attention to task-oriented attention. This paper aims at forecasting users' eye fixations in task-oriented virtual reality. To this end, a VR eye tracking dataset that corresponds to different users performing a visual search task in immersive virtual environments is built. A comprehensive analysis of users' eye fixations is performed based on the collected data. The analysis reveals that eye fixations are correlated with users' historical gaze positions, task-related objects, saliency information of the VR content, and head rotation velocities. Based on this analysis, a novel learning-based model is proposed to forecast users' eye fixations in the near future in immersive virtual environments. © 2021 IEEE.","Convolutional neural network; Eye tracking; Deep learning; Fixation forecasting; Virtual reality; Visual attention; Visual search","Abstracting; Behavioral research; Eye tracking; Forecasting; Human computer interaction; User interfaces; Comprehensive analysis; Gaze-based interaction; Immersive virtual environments; Immersive virtual reality; Intelligent User Interfaces; Learning Based Models; Viewing conditions; Visual Attention; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85105966888
"Wu C., Liaqat S., Helvaci H., Chcung S.-C.S., Chuah C.-N., Ozonoff S., Young G.","57210639542;57222324379;57223108984;57223111892;7004251298;7003652751;57225681104;","Machine learning based autism spectrum disorder detection from videos",2021,"2020 IEEE International Conference on E-Health Networking, Application and Services, HEALTHCOM 2020",,,"9398924","","",,,"10.1109/HEALTHCOM49281.2021.9398924","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104844350&doi=10.1109%2fHEALTHCOM49281.2021.9398924&partnerID=40&md5=9aa3f2ad5c3ed08690b8e4678de7b4d0","University of California, Department of Computer Science, Davis, CA, United States; University of Kentucky, Department of Electrical and Computer Engineering, Lexington, KY, United States; University of California, Department of Electrical and Computer Engineering, Davis, CA, United States; UC Davis MIND Institute, University of California, Davis, CA, United States","Wu, C., University of California, Department of Computer Science, Davis, CA, United States; Liaqat, S., University of Kentucky, Department of Electrical and Computer Engineering, Lexington, KY, United States; Helvaci, H., University of Kentucky, Department of Electrical and Computer Engineering, Lexington, KY, United States; Chcung, S.-C.S., University of Kentucky, Department of Electrical and Computer Engineering, Lexington, KY, United States, University of California, Department of Electrical and Computer Engineering, Davis, CA, United States; Chuah, C.-N., University of California, Department of Electrical and Computer Engineering, Davis, CA, United States; Ozonoff, S., UC Davis MIND Institute, University of California, Davis, CA, United States; Young, G., UC Davis MIND Institute, University of California, Davis, CA, United States","Early diagnosis of Autism Spectrum Disorder (ASD) is crucial for best outcomes to interventions. In this paper, we present a machine learning (ML) approach to ASD diagnosis based on identifying specific behaviors from videos of infants of ages 6 through 36 months. The behaviors of interest include directed gaze towards faces or objects of interest, positive affect, and vocalization. The dataset consists of 2000 videos of 3-minute duration with these behaviors manually coded by expert raters. Moreover, the dataset has statistical features including duration and frequency of the above mentioned behaviors in the video collection as well as independent ASD diagnosis by clinicians. We tackle the ML problem in a two-stage approach. Firstly, we develop deep learning models for automatic identification of clinically relevant behaviors exhibited by infants in a one-on-one interaction setting with parents or expert clinicians. We report baseline results of behavior classification using two methods: (1) image based model (2) facial behavior features based model. We achieve 70% accuracy for smile, 68% accuracy for look face, 67% for look object and 53% accuracy for vocalization. Secondly, we focus on ASD diagnosis prediction by applying a feature selection process to identify the most significant statistical behavioral features and a over and under sampling process to mitigate the class imbalance, followed by developing a baseline ML classifier to achieve an accuracy of 82% for ASD diagnosis. © 2021 IEEE.","Autism Spectrum Disorder; Facial Keypoint Detection; Human Behavior Detection; Machine Learning","Automation; Deep learning; Diagnosis; Diseases; Sampling; Turing machines; Autism spectrum disorders; Baseline results; Behavior classification; Behavioral features; Image-based modeling; Statistical features; Two stage approach; Video collections; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85104844350
"Kim H., Kwon S., Lee S.","57215855670;54415852000;57211724385;","Nra-net—neg-region attention network for salient object detection with gaze tracking",2021,"Sensors","21","5","1753","1","18",,1,"10.3390/s21051753","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101902574&doi=10.3390%2fs21051753&partnerID=40&md5=2dafbf3080812ab1a40cea74aaebf731","Department of Plasma Bio Display, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea; Department of Smart Convergence, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea; Ingenium College of Liberal Arts, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea","Kim, H., Department of Plasma Bio Display, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea; Kwon, S., Department of Smart Convergence, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea; Lee, S., Ingenium College of Liberal Arts, Kwangwoon University, 20 Kwangwoon-ro, Nowon-gu, Seoul, 01897, South Korea","In this paper, we propose a detection method for salient objects whose eyes are focused on gaze tracking; this method does not require a device in a single image. A network was constructed using Neg-Region Attention (NRA), which predicts objects with a concentrated line of sight using deep learning techniques. The existing deep learning-based method has an autoencoder structure, which causes feature loss during the encoding process of compressing and extracting features from the image and the decoding process of expanding and restoring. As a result, a feature loss occurs in the area of the object from the detection results, or another area is detected as an object. The proposed method, that is, NRA, can be used for reducing feature loss and emphasizing object areas with encoders. After separating positive and negative regions using the exponential linear unit activation function, converted attention was performed for each region. The attention method provided without using the backbone network emphasized the object area and suppressed the background area. In the experimental results, the proposed method showed higher detection results than the conventional methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Autoencoder; Convolutional neural network; Deep learning; Gaze tracking; Image process-ing; Salient object detection","Deep learning; Eye tracking; Learning systems; Object tracking; Signal encoding; Activation functions; Back-bone network; Conventional methods; Detection methods; Extracting features; Learning techniques; Learning-based methods; Salient object detection; Object detection; article; attention network; autoencoder; convolutional neural network; deep learning; gaze; human; human experiment; image processing; Eye-Tracking Technology; Neural Networks, Computer",Article,"Final","",Scopus,2-s2.0-85101902574
"Liu G., Yu Y., Mora K.A.F., Odobez J.-M.","56420692700;57188644020;55336423200;57203103085;","A Differential Approach for Gaze Estimation",2021,"IEEE Transactions on Pattern Analysis and Machine Intelligence","43","3","8920005","1092","1099",,9,"10.1109/TPAMI.2019.2957373","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100828326&doi=10.1109%2fTPAMI.2019.2957373&partnerID=40&md5=7d68db05be9c1d7d51f379f94f4dc159","Idiap Research Institute, Martigny, Switzerland; Eyeware Tech Sa, Martigny, Switzerland","Liu, G., Idiap Research Institute, Martigny, Switzerland; Yu, Y., Idiap Research Institute, Martigny, Switzerland; Mora, K.A.F., Eyeware Tech Sa, Martigny, Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, Switzerland","Most non-invasive gaze estimation methods regress gaze directions directly from a single face or eye image. However, due to important variabilities in eye shapes and inner eye structures amongst individuals, universal models obtain limited accuracies and their output usually exhibit high variance as well as subject dependent biases. Thus, increasing accuracy is usually done through calibration, allowing gaze predictions for a subject to be mapped to her actual gaze. In this article, we introduce a novel approach, which works by directly training a differential convolutional neural network to predict gaze differences between two eye input images of the same subject. Then, given a set of subject specific calibration images, we can use the inferred differences to predict the gaze direction of a novel eye sample. The assumption is that by comparing eye images of the same user, annoyance factors (alignment, eyelid closing, illumination perturbations) which usually plague single image prediction methods can be much reduced, allowing better prediction altogether. Furthermore, the differential network itself can be adapted via finetuning to make predictions consistent with the available user reference pairs. Experiments on 3 public datasets validate our approach which constantly outperforms state-of-the-art methods even when using only one calibration sample or those relying on subject specific gaze adaptation. © 1979-2012 IEEE.","differential network; gaze calibration; Gaze estimation","Calibration; Convolutional neural networks; Calibration samples; Differential approach; Differential network; Gaze direction; Gaze estimation; State-of-the-art methods; Subject-specific; Universal model; Forecasting",Article,"Final","",Scopus,2-s2.0-85100828326
"Pan Y., Mitchell K.","55549956700;7202163186;","Improving VIP viewer gaze estimation and engagement using adaptive dynamic anamorphosis",2021,"International Journal of Human Computer Studies","147",,"102563","","",,,"10.1016/j.ijhcs.2020.102563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096183137&doi=10.1016%2fj.ijhcs.2020.102563&partnerID=40&md5=1452cd94a94e501cc3a9c516c00e8799","Disney Research, Los Angeles, United States; Edinburgh Napier University, 10 Colinton Road, Edinburgh, EH10 5DT, United Kingdom","Pan, Y., Disney Research, Los Angeles, United States; Mitchell, K., Disney Research, Los Angeles, United States, Edinburgh Napier University, 10 Colinton Road, Edinburgh, EH10 5DT, United Kingdom","Anamorphosis for 2D displays can provide viewer centric perspective viewing, enabling 3D appearance, eye contact and engagement, by adapting dynamically in real time to a single moving viewer's viewpoint, but at the cost of distorted viewing for other viewers. We present a method for constructing non-linear projections as a combination of anamorphic rendering of selective objects whilst reverting to normal perspective rendering of the rest of the scene. Our study defines a scene consisting of five characters, with one of these characters selectively rendered in anamorphic perspective. We conducted an evaluation experiment and demonstrate that the tracked viewer centric imagery for the selected character results in an improved gaze and engagement estimation. Critically, this is performed without sacrificing the other viewers’ viewing experience. In addition, we present findings on the perception of gaze direction for regularly viewed characters located off-center to the origin, where perceived gaze shifts from being aligned to misalignment increasingly as the distance between viewer and character increases. Finally, we discuss different viewpoints and the spatial relationship between objects. © 2020","Display; Dynamic anamorphosis; Gaze","Image enhancement; Adaptive dynamics; Evaluation experiments; Eye contact; Gaze direction; Gaze estimation; Gaze shifts; Nonlinear projections; Spatial relationships; Rendering (computer graphics)",Article,"Final","",Scopus,2-s2.0-85096183137
"Oh K., Oh I.-S., Le V.N.T., Lee D.-W.","53880261600;15119932900;57215932775;56819445600;","Deep Anatomical Context Feature Learning for Cephalometric Landmark Detection",2021,"IEEE Journal of Biomedical and Health Informatics","25","3","9117151","806","817",,4,"10.1109/JBHI.2020.3002582","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092450204&doi=10.1109%2fJBHI.2020.3002582&partnerID=40&md5=0d3f54bc52a277db6f9c8db9d1f60cb3","Division of Computer Science and Engineering Department, Jeonbuk National University, Jeonju, 561-712, South Korea; Department of Pediatric Dentistry, Jeonbuk National University, Research Institute of Clinical Medicine, Jeonbuk National University, Biomedical Research Institute of Jeonbuk National University Hospital, Jeonjusi, 561-712, South Korea","Oh, K., Division of Computer Science and Engineering Department, Jeonbuk National University, Jeonju, 561-712, South Korea; Oh, I.-S., Division of Computer Science and Engineering Department, Jeonbuk National University, Jeonju, 561-712, South Korea; Le, V.N.T., Department of Pediatric Dentistry, Jeonbuk National University, Research Institute of Clinical Medicine, Jeonbuk National University, Biomedical Research Institute of Jeonbuk National University Hospital, Jeonjusi, 561-712, South Korea; Lee, D.-W., Department of Pediatric Dentistry, Jeonbuk National University, Research Institute of Clinical Medicine, Jeonbuk National University, Biomedical Research Institute of Jeonbuk National University Hospital, Jeonjusi, 561-712, South Korea","In the past decade, anatomical context features have been widely used for cephalometric landmark detection and significant progress is still being made. However, most existing methods rely on handcrafted graphical models rather than incorporating anatomical context during training, leading to suboptimal performance. In this study, we present a novel framework that allows a Convolutional Neural Network (CNN) to learn richer anatomical context features during training. Our key idea consists of the Local Feature Perturbator (LFP) and the Anatomical Context loss (AC loss). When training the CNN, the LFP perturbs a cephalometric image based on prior anatomical distribution, forcing the CNN to gaze relevant features more globally. Then AC loss helps the CNN to learn the anatomical context based on spatial relationships between the landmarks. The experimental results demonstrate that the proposed framework makes the CNN learn richer anatomical representation, leading to increased performance. In the performance comparisons, the proposed scheme outperforms state-of-the-art methods on the ISBI 2015 Cephalometric X-ray Image Analysis Challenge. © 2013 IEEE.","Cephalometric Landmark Detection; Context Feature Learning; Fully Convolutional Network","Convolutional neural networks; Deep learning; Anatomical distribution; Context features; Landmark detection; Performance comparison; Relevant features; Spatial relationships; State-of-the-art methods; Sub-optimal performance; Feature extraction; article; convolutional neural network; gaze; human; human experiment; image analysis; learning; X ray; cephalometry; radiography; Cephalometry; Humans; Neural Networks, Computer; Radiography",Article,"Final","",Scopus,2-s2.0-85092450204
"Parthasarathy S., Sundaram S.","57200948577;15766372600;","Detecting Expressions with Multimodal Transformers",2021,"2021 IEEE Spoken Language Technology Workshop, SLT 2021 - Proceedings",,,"9383573","636","643",,,"10.1109/SLT48900.2021.9383573","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103978992&doi=10.1109%2fSLT48900.2021.9383573&partnerID=40&md5=bb1a41bf4a67d25ce03e3e7cef042984","Amazon","Parthasarathy, S., Amazon; Sundaram, S., Amazon","Developing machine learning algorithms to understand person-to-person engagement can result in natural user experiences for communal devices such as Amazon Alexa. Among other cues such as voice activity and gaze, a person's audio-visual expression that includes tone of the voice and facial expression serves as an implicit signal of engagement between parties in a dialog. This study investigates deep-learning algorithms for audio-visual detection of user's expression. We first implement an audio-visual baseline model with recurrent layers that shows competitive results compared to current state of the art. Next, we propose the transformer architecture with encoder layers that better integrate audio-visual features for expressions tracking. Performance on the Aff-Wild2 database shows that the proposed methods perform better than baseline architecture with recurrent layers with absolute gains approximately 2% for arousal and valence descriptors. Further, multimodal architectures show significant improvements over models trained on single modalities with gains of up to 3.6%. Ablation studies show the significance of the visual modality for the expression detection on the Aff-Wild2 database. © 2021 IEEE.","computational paralinguistics; expression detection; human-computer interaction","Deep learning; User experience; Audio-visual features; Base-line architecture; Expression detections; Facial Expressions; Multimodal architectures; Recurrent layers; State of the art; Visual modalities; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85103978992
"Ou W.-L., Kuo T.-L., Chang C.-C., Fan C.-P.","55786494400;57216125874;57221691595;7402656929;","Deep-learning-based pupil center detection and tracking technology for visible-light wearable gaze tracking devices",2021,"Applied Sciences (Switzerland)","11","2","851","1","21",,1,"10.3390/app11020851","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099837702&doi=10.3390%2fapp11020851&partnerID=40&md5=13bd2196fd9991fadfa0781320ebc0e9","Department of Electrical Engineering, National Chung Hsing University, 145 Xingda Rd., South Dist., Taichung City, 402, Taiwan","Ou, W.-L., Department of Electrical Engineering, National Chung Hsing University, 145 Xingda Rd., South Dist., Taichung City, 402, Taiwan; Kuo, T.-L., Department of Electrical Engineering, National Chung Hsing University, 145 Xingda Rd., South Dist., Taichung City, 402, Taiwan; Chang, C.-C., Department of Electrical Engineering, National Chung Hsing University, 145 Xingda Rd., South Dist., Taichung City, 402, Taiwan; Fan, C.-P., Department of Electrical Engineering, National Chung Hsing University, 145 Xingda Rd., South Dist., Taichung City, 402, Taiwan","In this study, for the application of visible-light wearable eye trackers, a pupil tracking methodology based on deep-learning technology is developed. By applying deep-learning object detection technology based on the You Only Look Once (YOLO) model, the proposed pupil tracking method can effectively estimate and predict the center of the pupil in the visible-light mode. By using the developed YOLOv3-tiny-based model to test the pupil tracking performance, the detection accuracy is as high as 80%, and the recall rate is close to 83%. In addition, the average visible-light pupil tracking errors of the proposed YOLO-based deep-learning design are smaller than 2 pixels for the training mode and 5 pixels for the cross-person test, which are much smaller than those of the previous ellipse fitting design without using deep-learning technology under the same visible-light conditions. After the combination of calibration process, the average gaze tracking errors by the proposed YOLOv3-tiny-based pupil tracking models are smaller than 2.9 and 3.5 degrees at the training and testing modes, respectively, and the proposed visible-light wearable gaze tracking system performs up to 20 frames per second (FPS) on the GPU-based software embedded platform. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep-learning; Gaze tracker; Pupil tracking; Visible-light; Wearable eye tracker; YOLOv3-tiny",,Article,"Final","",Scopus,2-s2.0-85099837702
"Li R., Ma H., Wang R., Ding J.","57296544600;56406620700;57203525600;57296896900;","Device-Adaptive 2D Gaze Estimation: A Multi-Point Differential Framework",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12889 LNCS",,,"485","497",,,"10.1007/978-3-030-87358-5_39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117124096&doi=10.1007%2f978-3-030-87358-5_39&partnerID=40&md5=b34f2238396f037f29952634d0adde4d","Tsinghua University, Beijing, 100084, China; University of Science and Technology Beijing, Beijing, 100083, China","Li, R., Tsinghua University, Beijing, 100084, China; Ma, H., University of Science and Technology Beijing, Beijing, 100083, China; Wang, R., University of Science and Technology Beijing, Beijing, 100083, China; Ding, J., University of Science and Technology Beijing, Beijing, 100083, China","Eye tracking system on mobile devices is important for many interactive applications. However, since models are usually customized with limited types of devices and new devices have totally different physical parameters, it is hard to generalize over unseen devices. In this paper, we present a device-adaptive 2D gaze estimation algorithm based on differential prediction. We reformulate the gaze estimation as a relative position prediction problem between the input image and calibration images, which skips the estimation for camera parameters and makes models easily generalize over devices. To tackle the new challenge, this work proposes a framework which jointly trains a differential prediction module and an aggregation module for ensembling the predictions from multiple calibration points. Experiments show that the framework outperforms baseline models constantly on open datasets with only 3–5 calibration points. © 2021, Springer Nature Switzerland AG.","Adaptive gaze estimation; Differential prediction; Neural networks","Calibration; Computer vision; Eye tracking; Adaptive gaze estimation; Calibration points; Differential prediction; Eye tracking systems; Gaze estimation; Interactive applications; Multi-points; Neural-networks; New devices; System on mobile devices; Forecasting",Conference Paper,"Final","",Scopus,2-s2.0-85117124096
"Gao H., Zhan Y., Ma F., Chen Z.","57226395759;14030938900;57204794764;57296893400;","Eye Movement Event Detection Based onPath Signature",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12889 LNCS",,,"820","830",,,"10.1007/978-3-030-87358-5_67","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117081678&doi=10.1007%2f978-3-030-87358-5_67&partnerID=40&md5=de4c35ab98d00318f1256c8219480c46","School of Computer Science and Technology, Guangdong University of Technology, 100 Waihuanxi Road, Higher Education Megacenter, Guangzhou, 510006, China","Gao, H., School of Computer Science and Technology, Guangdong University of Technology, 100 Waihuanxi Road, Higher Education Megacenter, Guangzhou, 510006, China; Zhan, Y., School of Computer Science and Technology, Guangdong University of Technology, 100 Waihuanxi Road, Higher Education Megacenter, Guangzhou, 510006, China; Ma, F., School of Computer Science and Technology, Guangdong University of Technology, 100 Waihuanxi Road, Higher Education Megacenter, Guangzhou, 510006, China; Chen, Z., School of Computer Science and Technology, Guangdong University of Technology, 100 Waihuanxi Road, Higher Education Megacenter, Guangzhou, 510006, China","Eye movement event detection is a demanding technique in cognitive behavior analysis and HCI. Since an eye movement trajectory is a natural path, we try to introduce path signature (PS) to better explore eye movement events; PS is a feature that can highly summarize path information. For this, a multi-input network (MINN) combining 1D-CNN and bidirectional long short-term memory (BiLSTM) is constructed to classify gaze samples as fixation, saccade, smooth pursuit or noise. MINN requires two inputs of local features and global features respectively. The local features include the speed and direction of the gaze trajectory and the global features are the PS’s of the gaze trajectory. Experiments on GazeCom, the biggest eye movement event detection dataset, show that our approach with PS outperforms the state-of-the-art methods that do not use PS. © 2021, Springer Nature Switzerland AG.","Deep learning; Eye movement event detection; Path signature","Deep learning; Motion analysis; Trajectories; Behavior analysis; Cognitive behavior; Deep learning; Events detection; Eye movement event detection; Global feature; Local feature; Movement trajectories; Multiinput; Path signature; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85117081678
"Cao S., Zhao X., Qin B., Li J., Xiang Z.","57219797003;55352149700;57221152419;57218474951;57296381600;","A Monocular Reflection-Free Head-Mounted 3D Eye Tracking System",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12890 LNCS",,,"659","672",,,"10.1007/978-3-030-87361-5_54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117076445&doi=10.1007%2f978-3-030-87361-5_54&partnerID=40&md5=4368d46cfaf8759e17022a665fa88524","School of Computer Science, Northwestern Polytechnical University, Xi’an, 710129, China; School of Software, Northwestern Polytechnical University, Xi’an, 710129, China; Ningbo Institute of Northwestern Polytechnical University, Ningbo, 315103, China","Cao, S., School of Computer Science, Northwestern Polytechnical University, Xi’an, 710129, China; Zhao, X., School of Computer Science, Northwestern Polytechnical University, Xi’an, 710129, China, Ningbo Institute of Northwestern Polytechnical University, Ningbo, 315103, China; Qin, B., School of Computer Science, Northwestern Polytechnical University, Xi’an, 710129, China; Li, J., School of Computer Science, Northwestern Polytechnical University, Xi’an, 710129, China; Xiang, Z., School of Software, Northwestern Polytechnical University, Xi’an, 710129, China","Head-mounted eye tracking has significant potential for gaze baesd application such as consumer attention monitoring, human-computer interaction, or virtual reality (VR). Existing methods, however, either use pupil center-corneal reflection (PCCR) vectors as gaze directions or require complex hardware setups and use average physiological parameters of the eye to obtain gaze directions. In view of this situation, we propose a novel method which uses only a single camera to obtain gaze direction by fitting a 3D eye model based on the motion trajectory of pupil contour. Then a 3D to 2D mapping model is proposed based on the fitting model, so the complex structure of hardware and the use of average parameters for the eyes are avoided. The experimental results show that the method can improve the gaze accuracy and simplify the hardware structure. © 2021, Springer Nature Switzerland AG.","3D gaze estimation; Head-mounted device; Mapping model; Pupil contour; Single camera","3D modeling; Cameras; Computer hardware; Eye movements; Eye tracking; Human computer interaction; Mapping; Physiological models; 3d gaze estimation; Eye tracking systems; Free-head; Gaze direction; Gaze estimation; Head-mounted device; Head-mounted eye tracking; Mapping modeling; Pupil contour; Single cameras; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85117076445
"Saab K., Hooper S.M., Sohoni N.S., Parmar J., Pogatchnik B., Wu S., Dunnmon J.A., Zhang H.R., Rubin D., Ré C.","57193841679;57216149541;57202708705;57285170100;36176344300;57286547400;36767226100;57285862700;7202307112;10739281400;","Observational Supervision for Medical Image Classification Using Gaze Data",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12902 LNCS",,,"603","614",,,"10.1007/978-3-030-87196-3_56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116417040&doi=10.1007%2f978-3-030-87196-3_56&partnerID=40&md5=f7971d1e96ed98855e586d37ca72908f","Department of Electrical Engineering, Stanford University, Stanford, United States; Institute for Computational and Mathematical Engineering, Stanford University, Stanford, United States; Department of Computer Science, Stanford University, Stanford, United States; Department of Radiology, Stanford University, Stanford, United States; Khoury College of Computer Sciences, Northeastern University, Boston, United States; Department of Biomedical Data Science, Stanford University, Stanford, United States","Saab, K., Department of Electrical Engineering, Stanford University, Stanford, United States; Hooper, S.M., Department of Electrical Engineering, Stanford University, Stanford, United States; Sohoni, N.S., Institute for Computational and Mathematical Engineering, Stanford University, Stanford, United States; Parmar, J., Department of Computer Science, Stanford University, Stanford, United States; Pogatchnik, B., Department of Radiology, Stanford University, Stanford, United States; Wu, S., Department of Computer Science, Stanford University, Stanford, United States; Dunnmon, J.A., Department of Computer Science, Stanford University, Stanford, United States; Zhang, H.R., Khoury College of Computer Sciences, Northeastern University, Boston, United States; Rubin, D., Department of Biomedical Data Science, Stanford University, Stanford, United States; Ré, C., Department of Computer Science, Stanford University, Stanford, United States","Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinician’s workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines. © 2021, Springer Nature Switzerland AG.","Eye tracking; Medical image diagnosis; Weak supervision","Computer aided instruction; Deep learning; Image classification; Linearization; Magnetic resonance imaging; Medical imaging; Classification tasks; Eye-tracking; Labeled dataset; Learning models; Medical image classification; Medical image diagnosis; Performance; Precision point; Weak supervision; Work-flows; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85116417040
"Amadori P.V., Fischer T., Wang R., Demiris Y.","56703112800;57190126084;57189039099;6506125343;","Predicting Secondary Task Performance: A Directly Actionable Metric for Cognitive Overload Detection",2021,"IEEE Transactions on Cognitive and Developmental Systems",,,,"","",,,"10.1109/TCDS.2021.3114162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115685349&doi=10.1109%2fTCDS.2021.3114162&partnerID=40&md5=6977027aed41eb30e44b70baa33f49f4","Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K. (e-mail: pierluigi.amadori@gmail.com); Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.","Amadori, P.V., Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K. (e-mail: pierluigi.amadori@gmail.com); Fischer, T., Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.; Wang, R., Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.; Demiris, Y., Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.","In this paper, we address cognitive overload detection from unobtrusive physiological signals for users in dual-tasking scenarios. Anticipating cognitive overload is a pivotal challenge in interactive cognitive systems and could lead to safer shared-control between users and assistance systems. Our framework builds on the assumption that decision mistakes on the cognitive secondary task of dual-tasking users correspond to cognitive overload events, wherein the cognitive resources required to perform the task exceed the ones available to the users. We propose DecNet, an end-to-end sequence-to-sequence deep learning model that infers in real-time the likelihood of user mistakes on the secondary task, i.e., the practical impact of cognitive overload, from eye-gaze and head-pose data. We train and test DecNet on a dataset collected in a simulated driving setup from a cohort of 20 users on two dual-tasking decision-making scenarios, with either visual or auditory decision stimuli. DecNet anticipates cognitive overload events in both scenarios and can perform in time-constrained scenarios, anticipating cognitive overload events up to 2s before they occur. We show that DecNet&#x2019;s performance gap between audio and visual scenarios is consistent with user perceived difficulty. This suggests that single modality stimulation induces higher cognitive load on users, hindering their decision-making abilities. Crown","Cognitive Workload; Data models; Decision Anticipation; Feature extraction; Load modeling; Simulated Driving.; Solid modeling; Task analysis; User Monitoring; Vehicles; Visualization","Cognitive systems; Deep learning; Job analysis; Statistical tests; Cognitive overload; Cognitive workloads; Decision anticipation; Features extraction; Load modeling; Simulated driving; Simulated driving.; Solid modelling; Task analysis; User monitoring; Decision making",Article,"Article in Press","",Scopus,2-s2.0-85115685349
"Chang Y., He C., Zhao Y., Luy T., Gu N.","57224667373;57262788100;56451804400;57263506500;57262788200;","High-Frame-Rate Eye-Tracking Framework for Mobile Devices",2021,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2021-June",,,"1445","1449",,,"10.1109/ICASSP39728.2021.9414624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115165423&doi=10.1109%2fICASSP39728.2021.9414624&partnerID=40&md5=fd5062d0ee1fa069488c090a888a438f","School of Computer Science, Fudan University; Shanghai Key Laboratory of Data Science, Fudan University","Chang, Y., School of Computer Science, Fudan University; He, C., School of Computer Science, Fudan University; Zhao, Y., School of Computer Science, Fudan University; Luy, T., Shanghai Key Laboratory of Data Science, Fudan University; Gu, N., School of Computer Science, Fudan University","Gaze-on-screen tracking, an appearance-based eye-tracking task, has drawn significant interest in recent years. While learning-based high-precision eye-tracking methods have been designed in the past, the complex pre-training and high computation in neural network-based deep models restrict their applicability in mobile devices. Moreover, as the display frame rate of mobile devices has steadily increased to 120 fps, high-frame-rate eye tracking becomes increasingly challenging. In this work, we tackle the tracking efficiency challenge and introduce GazeHFR, a biologic-inspired eyetracking model specialized for mobile devices, offering both high accuracy and efficiency. Specifically, GazeHFR classifies the eye movement into two distinct phases, i.e., saccade and smooth pursuit, and leverages inter-frame motion information combined with lightweight learning models tailored to each movement phase to deliver high-efficient eye tracking without affecting accuracy. Compared to prior art, Gaze- HFR achieves approximately 7x speedup and 15% accuracy improvement on mobile devices. ©2021 IEEE.","Applications of machine learning; Biomedical video analysis; Gaze estimation; Mobile imaging","Display devices; Efficiency; Eye movements; Learning systems; Motion tracking; Accuracy Improvement; Appearance based; Eye tracking methods; High frame rate; High-precision; Learning models; Motion information; Smooth pursuit; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85115165423
"Stember J.N., Celik H., Gutman D., Swinburne N., Young R., Eskreis-Winkler S., Holodny A., Jambawalikar S., Wood B.J., Chang P.D., Krupinski E., Bagci U.","18538253000;57212691442;57222346959;55481511100;57232771700;51963422000;7004827467;6507408536;7401873523;57192687394;26643320200;57225322215;","Integrating eye tracking and speech recognition accurately annotates mr brain images for deep learning: Proof of principle",2021,"Radiology: Artificial Intelligence","3","1","e200047","","",,2,"10.1148/ryai.2020200047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113654183&doi=10.1148%2fryai.2020200047&partnerID=40&md5=29a50f1dc39340c03a8d08b8499b12d3","Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; The National Institutes of Health Clinical Center, BethesdaMD, United States; Department of Radiology, Columbia University Medical Center, New York, NY, United States; Department of Radiology, University of California–Irvine, Irvine, CA, United States; Department of Radiology & Imaging Sciences, Emory University, Atlanta, GA, United States; Center for Research in Computer Vision, University of Central Florida, Orlando, FLA, United States","Stember, J.N., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Celik, H., The National Institutes of Health Clinical Center, BethesdaMD, United States; Gutman, D., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Swinburne, N., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Young, R., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Eskreis-Winkler, S., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Holodny, A., Department of Radiology, Memorial Sloan-Kettering Cancer Center, 1275 York Ave, New York, NY  10065, United States; Jambawalikar, S., Department of Radiology, Columbia University Medical Center, New York, NY, United States; Wood, B.J., The National Institutes of Health Clinical Center, BethesdaMD, United States; Chang, P.D., Department of Radiology, University of California–Irvine, Irvine, CA, United States; Krupinski, E., Department of Radiology & Imaging Sciences, Emory University, Atlanta, GA, United States; Bagci, U., Center for Research in Computer Vision, University of Central Florida, Orlando, FLA, United States","Purpose: To generate and assess an algorithm combining eye tracking and speech recognition to extract brain lesion location labels automatically for deep learning (DL). Materials and Methods: In this retrospective study, 700 two-dimensional brain tumor MRI scans from the Brain Tumor Segmentation database were clinically interpreted. For each image, a single radiologist dictated a standard phrase describing the lesion into a microphone, simulating clinical interpretation. Eye-tracking data were recorded simultaneously. Using speech recognition, gaze points corresponding to each lesion were obtained. Lesion locations were used to train a keypoint detection convolutional neural network to find new lesions. A network was trained to localize lesions for an independent test set of 85 images. The statistical measure to evaluate our method was percent accuracy. Results: Eye tracking with speech recognition was 92% accurate in labeling lesion locations from the training dataset, thereby demonstrating that fully simulated interpretation can yield reliable tumor location labels. These labels became those that were used to train the DL network. The detection network trained on these labels predicted lesion location of a separate testing set with 85% accuracy. Conclusion: The DL network was able to locate brain tumors on the basis of training data that were labeled automatically from simulated clinical image interpretation. © RSNA, 2020.",,"Article; back propagation; brain damage; brain tumor; convolutional neural network; deep learning; human; image analysis; image segmentation; nuclear magnetic resonance imaging; prediction; retrospective study; speech discrimination",Article,"Final","",Scopus,2-s2.0-85113654183
"Khellat-Kihel S., Sun Z., Tistarelli M.","56582558200;8081773300;7003853982;","An Hybrid Attention-Based System for the Prediction of Facial Attributes",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12339 LNCS",,,"116","127",,,"10.1007/978-3-030-82427-3_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113290786&doi=10.1007%2f978-3-030-82427-3_9&partnerID=40&md5=3673bb5c51f215750be2bf1ecb6542c3","Computer Vision Laboratory, University of Sassari, Viale Italia 39, Sassari, 07100, Italy; Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Room 1605, Intelligence Bulding, 95 Zhongguancun East Road, Beijing, 100190, China; Computer Vision Laboratory, Department of Biomedical Sciences and Information Technology, University of Sassari, Viale S. Pietro 43/b, Sassari, 07100, Italy","Khellat-Kihel, S., Computer Vision Laboratory, University of Sassari, Viale Italia 39, Sassari, 07100, Italy; Sun, Z., Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Room 1605, Intelligence Bulding, 95 Zhongguancun East Road, Beijing, 100190, China; Tistarelli, M., Computer Vision Laboratory, Department of Biomedical Sciences and Information Technology, University of Sassari, Viale S. Pietro 43/b, Sassari, 07100, Italy","Recent research on face analysis has demonstrated the richness of information embedded in feature vectors extracted from a deep convolutional neural network. Even though deep learning achieved a very high performance on several challenging visual tasks, such as determining the identity, age, gender and race, it still lacks a well grounded theory which allows to properly understand the processes taking place inside the network layers. Therefore, most of the underlying processes are unknown and not easy to control. On the other hand, the human visual system follows a well understood process in analyzing a scene or an object, such as a face. The direction of the eye gaze is repeatedly directed, through purposively planned saccadic movements, towards salient regions to capture several details. In this paper we propose to capitalize on the knowledge of the saccadic human visual processes to design a system to predict facial attributes embedding a biologically-inspired network architecture, the HMAX. The architecture is tailored to predict attributes with different textural information and conveying different semantic meaning, such as attributes related and unrelated to the subject’s identity. Salient points on the face are extracted from the outputs of the S2 layer of the HMAX architecture and fed to a local texture characterization module based on LBP (Local Binary Pattern). The resulting feature vector is used to perform a binary classification on a set of pre-defined visual attributes. The devised system allows to distill a very informative, yet robust, representation of the imaged faces, allowing to obtain high performance but with a much simpler architecture as compared to a deep convolutional neural network. Several experiments performed on publicly available, challenging, large datasets demonstrate the validity of the proposed approach. © 2021, The Author(s).",,"Biomimetics; Brain; Computation theory; Computer architecture; Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Eye movements; Forecasting; Large dataset; Network layers; Semantics; Textures; Binary classification; Biologically inspired networks; Human Visual System; Local binary patterns; Recent researches; Salient regions; Textural information; Visual attributes; Network architecture",Conference Paper,"Final","",Scopus,2-s2.0-85113290786
"Xu Y., Zhang Z., Gao S.","57192081433;57204289144;35224747100;","Spherical DNNs and Their Applications in 360<formula><tex>$^\circ$</tex></formula> Images and Videos",2021,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,,,"","",,,"10.1109/TPAMI.2021.3100259","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112654967&doi=10.1109%2fTPAMI.2021.3100259&partnerID=40&md5=cbbc4fa5a4d96220edd0af2944da363d","Institute of High Performance Computing (IHPC), A*STAR, 54759 Singapore, Singapore, Singapore, (e-mail: xuyy2@shanghaitech.edu.cn); School of Information Science and Techology, ShanghaiTech University, 387433 Shanghai, Shanghai, China, (e-mail: zhangzh@shanghaitech.edu.cn); ShanghaiTech University, ShanghaiTech University, Shanghai, Shanghai, China, (e-mail: gaoshh@shanghaitech.edu.cn)","Xu, Y., Institute of High Performance Computing (IHPC), A*STAR, 54759 Singapore, Singapore, Singapore, (e-mail: xuyy2@shanghaitech.edu.cn); Zhang, Z., School of Information Science and Techology, ShanghaiTech University, 387433 Shanghai, Shanghai, China, (e-mail: zhangzh@shanghaitech.edu.cn); Gao, S., ShanghaiTech University, ShanghaiTech University, Shanghai, Shanghai, China, (e-mail: gaoshh@shanghaitech.edu.cn)","Spherical images or videos, as typical non-Euclidean data, are usually stored in the form of 2D panoramas obtained through an equirectangular projection, which is neither equal area nor conformal. The distortion caused by the projection limits the performance of vanilla Deep Neural Networks (DNNs) designed for traditional Euclidean data. In this paper, we design a novel Spherical Deep Neural Network (DNN) to deal with the distortion caused by the equirectangular projection. Specifically, we customize a set of components, including a spherical convolution, a spherical pooling, a spherical ConvLSTM cell and a spherical MSE loss, as the replacements of their counterparts in vanilla DNNs for spherical data. The core idea is to change the identical behavior of the conventional operations in vanilla DNNs across different feature patches so that they will be adjusted to the distortion caused by the variance of sampling rate among different feature patches. We demonstrate the effectiveness of our Spherical DNNs for saliency detection and gaze estimation in <formula><tex>$360^\circ$</tex></formula> videos. To facilitate the study of the 360 video saliency detection, we further construct a large-scale <formula><tex>$360^\circ$</tex></formula> video saliency detection dataset. Comprehensive experiments validate the effectiveness of our proposed Spherical DNNs for spherical handwritten digit classification and sport classification, saliency detection and gaze tracking in <formula><tex>$360^\circ$</tex></formula> videos. IEEE","360&#x00B0; Videos; Convolution; Distortion; Feature extraction; Gaze Prediction; Kernel; Saliency Detection; Saliency detection; Spherical Deep Neural Networks; Task analysis; Videos","Character recognition; Deep neural networks; Eye tracking; Large dataset; Neural networks; Gaze estimation; Gaze tracking; Handwritten digit classification; Non-Euclidean; Saliency detection; Sampling rates; Spherical images; Video saliencies; Spheres",Article,"Article in Press","",Scopus,2-s2.0-85112654967
"Jha S., Marzban M.F., Hu T., Mahmoud M.H., Al-Dhahir N., Busso C.","57193014012;57188584808;57221705332;57222034344;7006800187;35742852700;","The Multimodal Driver Monitoring Database: A Naturalistic Corpus to Study Driver Attention",2021,"IEEE Transactions on Intelligent Transportation Systems",,,,"","",,,"10.1109/TITS.2021.3095462","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112610245&doi=10.1109%2fTITS.2021.3095462&partnerID=40&md5=49bebc55c38b192240caa007c89f5aa5","Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA (e-mail: busso@utdallas.edu).","Jha, S., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Marzban, M.F., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Hu, T., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Mahmoud, M.H., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Al-Dhahir, N., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Busso, C., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA (e-mail: busso@utdallas.edu).","A smart vehicle should be able to monitor the actions and behaviors of the human driver to provide critical warnings or intervene when necessary. Recent advancements in deep learning and computer vision have shown great promise in monitoring human behavior and activities. While these algorithms work well in a controlled environment, naturalistic driving conditions add new challenges such as illumination variations, occlusions, and extreme head poses. A vast amount of in-domain data is required to train models that provide high performance in predicting driving related tasks to effectively monitor driver actions and behaviors. Toward building the required infrastructure, this paper presents the multimodal driver monitoring (MDM) dataset, which was collected with 59 subjects that were recorded performing various tasks. We use the Fi-Cap device that continuously tracks the head movement of the driver using fiducial markers, providing frame-based annotations to train head pose algorithms in naturalistic driving conditions. We ask the driver to look at predetermined gaze locations to obtain accurate correlation between the driver's facial image and visual attention. We also collect data when the driver performs common secondary activities such as navigation using a smart phone and operating the in-car infotainment system. All of the driver's activities are recorded with high definition RGB cameras and a time-of-flight depth camera. We also record the controller area network-bus (CAN-Bus), extracting important information. These high quality recordings serve as the ideal resource to train various efficient algorithms for monitoring the driver, providing further advancements in the field of in-vehicle safety systems. CCBYNCND","driver monitoring dataset.; driver visual attention; In-vehicle safety","Cameras; Control system synthesis; Deep learning; Driver training; Patient monitoring; Safety engineering; Smartphones; Car infotainment; Controlled environment; Controller-area-network bus; Driver monitoring; Driving conditions; High-quality recordings; Illumination variation; Vehicle safety systems; Behavioral research",Article,"Article in Press","",Scopus,2-s2.0-85112610245
"Savochkina E., Lee L.H., Drukker L., Papageorghiou A.T., Noble J.A.","57226651419;57217029182;36241434600;6603569987;57226264208;","First Trimester Gaze Pattern Estimation Using Stochastic Augmentation Policy Search for Single Frame Saliency Prediction",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12722 LNCS",,,"361","374",,,"10.1007/978-3-030-80432-9_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112210242&doi=10.1007%2f978-3-030-80432-9_28&partnerID=40&md5=72246c163f65073924e746a627f904af","Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Nuffield Department of Women’s and Reproductive Health, University of Oxford, Oxford, United Kingdom","Savochkina, E., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Lee, L.H., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Drukker, L., Nuffield Department of Women’s and Reproductive Health, University of Oxford, Oxford, United Kingdom; Papageorghiou, A.T., Nuffield Department of Women’s and Reproductive Health, University of Oxford, Oxford, United Kingdom; Noble, J.A., Department of Engineering Science, University of Oxford, Oxford, United Kingdom","While performing an ultrasound (US) scan, sonographers direct their gaze at regions of interest to verify that the correct plane is acquired and to interpret the acquisition frame. Predicting sonographer gaze on US videos is useful for identification of spatio-temporal patterns that are important for US scanning. This paper investigates utilizing sonographer gaze, in the form of gaze-tracking data, in a multi-modal imaging deep learning framework to assist the analysis of the first trimester fetal ultrasound scan. Specifically, we propose an encoder-decoder convolutional neural network with skip connections to predict the visual gaze for each frame using 115 first trimester ultrasound videos; 29,250 video frames for training, 7,290 for validation and 9,126 for testing. We find that the dataset of our size benefits from automated data augmentation, which in turn, alleviates model overfitting and reduces structural variation imbalance of US anatomical views between the training and test datasets. Specifically, we employ a stochastic augmentation policy search method to improve segmentation performance. Using the learnt policies, our models outperform the baseline: KLD, SIM, NSS and CC (2.16, 0.27, 4.34 and 0.39 versus 3.17, 0.21, 2.92 and 0.28). © 2021, Springer Nature Switzerland AG.","Data augmentation; Fetal ultrasound; First trimester; Gaze tracking; Single frame saliency prediction; U-Net","Convolutional neural networks; Deep learning; Forecasting; Image analysis; Image understanding; Medical imaging; Statistical tests; Stochastic systems; Ultrasonics; Fetal ultrasound; Learning frameworks; Multi-modal imaging; Regions of interest; Segmentation performance; Spatiotemporal patterns; Structural variations; Ultrasound videos; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85112210242
"Chakraborty P., Ahmed S., Yousuf M.A., Azad A., Alyami S.A., Moni M.A.","57216817907;57225877868;57188663159;36087132900;57190760454;35119094400;","A Human-Robot Interaction System Calculating Visual Focus of Human's Attention Level",2021,"IEEE Access","9",,"9462086","93409","93421",,3,"10.1109/ACCESS.2021.3091642","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112114960&doi=10.1109%2fACCESS.2021.3091642&partnerID=40&md5=42c7e47f1cd68060b7d6b4891ef7acf5","Department of Computer Science and Engineering, Comilla University, Cumilla, Bangladesh; Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh; School of Biotechnology and Biomolecular Sciences, University of New South Wales Sydney (UNSW Sydney), Sydney, NSW, Australia; Department of Mathematics and Statistics, Faculty of Science, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, Saudi Arabia; WHO Collaborating Centre on EHealth, UNSW Digital Health, University of New South Wales Sydney (UNSW Sydney), Sydney, NSW, Australia","Chakraborty, P., Department of Computer Science and Engineering, Comilla University, Cumilla, Bangladesh; Ahmed, S., Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh; Yousuf, M.A., Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh; Azad, A., School of Biotechnology and Biomolecular Sciences, University of New South Wales Sydney (UNSW Sydney), Sydney, NSW, Australia; Alyami, S.A., Department of Mathematics and Statistics, Faculty of Science, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, Saudi Arabia; Moni, M.A., WHO Collaborating Centre on EHealth, UNSW Digital Health, University of New South Wales Sydney (UNSW Sydney), Sydney, NSW, Australia","Attention is the mental awareness of human on a particular object or a piece of information. The level of attention indicates how intense the focus is on an object or an instance. In this study, several types of human attention level have been observed. After introducing image segmentation and detection technique for facial features, eyeball movement and gaze estimation were measured. Eye movement were assessed using the video data, and a total of 10197 data instances were manually labelled for the attention level. Then Artificial Neural Network (ANN) and Recurrent Neural Network-Long Short Term Memory (LSTM) based Deep learning (DL) architectures have been proposed for analysing the data. Next, the trained DL model has been implanted into a robotic system that is capable of detecting various features; ultimately leading to the calculation of visual attention for reading, browsing, and writing purposes. This system is capable of checking the attention level of the participants and also can detect if participants are present or not. Based on a certain level of visual focus of attention (VFOA), this system interacts with the person, generates awareness and establishes verbal or visual communication with that person. The proposed ML techniques have achieved almost 99.24% validation accuracy and 99.43% test accuracy. It is also shown in the comparative study that, since the dataset volumes are limited, ANN is more suitable for attention level calculation than RNN-LSTM. We hope that the implemented robotic structure manifests the real-world implication of the proposed method. © 2013 IEEE.","ANN; attention level; concentration; Human-robot interaction; RNN-LSTM; visual focus of attention","Behavioral research; Deep learning; Eye movements; Feature extraction; Image segmentation; Long short-term memory; Robotics; Visual communication; Attention level; Comparative studies; Eyeball movements; Gaze estimation; Human attention; Robotic structures; Visual Attention; Visual focus of attentions; Social robots",Article,"Final","",Scopus,2-s2.0-85112114960
"Sakya G., Singh S., Mishra S., Tiwari S.M.","55786972300;57226390360;57226389199;57226381894;","Intelligent Invigilation Using Video Surveillance",2021,"Lecture Notes in Networks and Systems","197 LNNS",,,"401","411",,,"10.1007/978-981-16-0980-0_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111379750&doi=10.1007%2f978-981-16-0980-0_37&partnerID=40&md5=9dc68c916d7b6cda2654377f8e8676d7","JSS Academy of Technical Education, Noida, India","Sakya, G., JSS Academy of Technical Education, Noida, India; Singh, S., JSS Academy of Technical Education, Noida, India; Mishra, S., JSS Academy of Technical Education, Noida, India; Tiwari, S.M., JSS Academy of Technical Education, Noida, India","At the examination center, examinees daily face difficulties in finding their seating position that consumes a lot of time. Also in COVID-19 pandemic, we need to maintain social distancing while taking examination offline. So, this paper proposes system that can help examinees in knowing their seat number in their respective examination hall with the help of online Web portal. It can help in eliminating the crowd at the notice board and can also save their time. With the help of an admit card generated to which a barcode be attached, an examinee can scan that code each day and know their seating location as per a daily basis. Another system is also proposed for face recognition that can help the examiner in identifying the identity of candidates that can also alert them if they are not found in the database using deep learning. Within the examination hall, the orientation of the head and movement of the mouth of an examinee provide us the clue of suspicious behaviors. Nowadays, most of the suspicious behaviors monitoring procedures are done manually that involves a lot of invigilators in each examination hall. In the pandemic situation, we proposed an intelligent invigilation using video surveillance that can autonomously detect and track examinee’s eye gaze, head orientation, and mouth movement to robustly detect their cheating activities. Algorithms which are implemented independently include eigen-face, fisherface, and linear binary pattern histograms. With the help of webcam installed at each examinee’s desk, if any suspicious activity is detected, the system will generate an alarm indicating such behaviors. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Deep learning; Faces recognition; Intelligent invigilation; Video surveillance; Web development",,Conference Paper,"Final","",Scopus,2-s2.0-85111379750
"Gite S., Pradhan B., Alamri A., Kotecha K.","56656365900;12753037900;57215408871;6506676097;","ADMT: Advanced driver's movement tracking system using spatio-temporal interest points and maneuver anticipation using deep neural networks",2021,"IEEE Access","9",,"9478887","99312","99326",,2,"10.1109/ACCESS.2021.3096032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111162872&doi=10.1109%2fACCESS.2021.3096032&partnerID=40&md5=09edc144bbb551bbd59b0d2ee4598179","Department of Computer Science and Information Technology, Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, 412115, India; Symbiosis Centre of Applied A.I. (SCAAI), Symbiosis International, Deemed University, Pune, 412115, India; Centre for Advanced Modelling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and I.T., University of Technology Sydney, Ultimo, NSW  2007, Australia; Earth Observation Center, Institute of Climate Change, Universiti Kebangsaan Malaysia, Selangor, Bangi, 43600, Malaysia; Department of Geology and Geophysics, College of Science, King Saud University, Riyadh, 11451, Saudi Arabia","Gite, S., Department of Computer Science and Information Technology, Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, 412115, India, Symbiosis Centre of Applied A.I. (SCAAI), Symbiosis International, Deemed University, Pune, 412115, India; Pradhan, B., Centre for Advanced Modelling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and I.T., University of Technology Sydney, Ultimo, NSW  2007, Australia, Earth Observation Center, Institute of Climate Change, Universiti Kebangsaan Malaysia, Selangor, Bangi, 43600, Malaysia; Alamri, A., Department of Geology and Geophysics, College of Science, King Saud University, Riyadh, 11451, Saudi Arabia; Kotecha, K., Department of Computer Science and Information Technology, Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, 412115, India, Symbiosis Centre of Applied A.I. (SCAAI), Symbiosis International, Deemed University, Pune, 412115, India","Assistive driving is a complex engineering problem and is influenced by several factors such as the sporadic nature of the quality of the environment, the response of the driver, and the standard of the roads on which the vehicle is being driven. The authors track the driver's anticipation based on his head movements using Spatio-Temporal Interest Point (STIP) extraction and enhance the anticipation of action accuracy well before using the RNN-LSTM framework. This research tackles a fundamental problem of lane change assistance by developing a novel model called Advanced Driver's Movement Tracking (ADMT). ADMT uses customized convolution-based deep learning networks by using Recurrent Convolutional Neural Network (RCNN). STIP with eye gaze extraction and RCNN performed in ADMT on brain4cars dataset for driver movement tracking. Its performance is compared with the traditional machine learning and deep learning models, namely Support Vector Machines (SVM), Hidden Markov Model (HMM), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and provided an increment of almost 12% in the prediction accuracy and 44% in the anticipation time. Furthermore, ADMT systems outperformed all of the models in terms of both the accuracy of the system and the previously mentioned time of anticipation that is discussed at length in the paper. Thus it assists the driver with additional anticipation time to access the typical reaction time for better preparedness to respond to undesired future behavior. The driver is then assured of a safe and assisted driving experience with the proposed system. © 2013 IEEE.","Advanced driver movement tracking system; Deep neural networks; Eye gaze tracking; RCNN; Spatio-temporal interest points","Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Extraction; Eye movements; Hidden Markov models; Learning systems; Motion analysis; Support vector machines; Tracking (position); Assisted drivings; Complex engineering problems; Learning models; Learning network; Movement tracking systems; Prediction accuracy; Recurrent neural network (RNN); Spatio-temporal interest points; Long short-term memory",Article,"Final","",Scopus,2-s2.0-85111162872
"Gamage V., Ennis C., Ross R.","57206663847;26424948000;8843354800;","Learned Dynamics Models and Online Planning for Model-Based Animation Agents",2021,"Smart Innovation, Systems and Technologies","241",,,"27","37",,,"10.1007/978-981-16-2994-5_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111089741&doi=10.1007%2f978-981-16-2994-5_3&partnerID=40&md5=5727c831696206691e6f5624f43a1cdb","School of Computer Science, Technological University Dublin, Dublin, Ireland","Gamage, V., School of Computer Science, Technological University Dublin, Dublin, Ireland; Ennis, C., School of Computer Science, Technological University Dublin, Dublin, Ireland; Ross, R., School of Computer Science, Technological University Dublin, Dublin, Ireland","Deep Reinforcement Learning (RL) has resulted in impressive results when applied in creating virtual character animation control agents capable of responsive behaviour. However, current state-of-the-art methods are heavily dependant on physics-driven feedback to learn character behaviours and are not transferable to portraying behaviour such as social interactions and gestures. In this paper, we present a novel approach to data-driven character animation; we introduce model-based RL animation control agents that learn character dynamics models that are applicable to a range of behaviours. Animation tasks are expressed as meta-objectives, and online planning is used to generate animation within a beta-distribution parameterised space that substantially improves agent efficiency. Purely through self-exploration and learned dynamics, agents created within our framework are able to output animations to successfully complete gaze and pointing tasks robustly while maintaining smoothness of motion, using minimal training epochs. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",,"Animation; Deep learning; Dynamics; Reinforcement learning; Animation control; Beta distributions; Character animation; Responsive behaviour; Smoothness of motions; Social interactions; State-of-the-art methods; Virtual character; Multi agent systems",Conference Paper,"Final","",Scopus,2-s2.0-85111089741
"Chakraborty S., Krishna R., Ding Y., Ray B.","57193152367;57197678498;57221142654;24492560400;","Deep Learning based Vulnerability Detection: Are We There Yet",2021,"IEEE Transactions on Software Engineering",,,,"","",,,"10.1109/TSE.2021.3087402","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111011696&doi=10.1109%2fTSE.2021.3087402&partnerID=40&md5=7ed8e9b6182392aa420a50a1a516ac16","Computer Science, Columbia University, 5798 New York, New York, United States, 10027-6902 (e-mail: saikatc@cs.columbia.edu); Computer Science, Columbia University, 5798 New York, New York, United States, (e-mail: i.m.ralk@gmail.com); Computer Science, Columbia University, 5798 New York, New York, United States, (e-mail: yangruibo.ding@columbia.edu); Computer Science, Columbia University, New York, New York, United States, (e-mail: rayb@cs.columbia.edu)","Chakraborty, S., Computer Science, Columbia University, 5798 New York, New York, United States, 10027-6902 (e-mail: saikatc@cs.columbia.edu); Krishna, R., Computer Science, Columbia University, 5798 New York, New York, United States, (e-mail: i.m.ralk@gmail.com); Ding, Y., Computer Science, Columbia University, 5798 New York, New York, United States, (e-mail: yangruibo.ding@columbia.edu); Ray, B., Computer Science, Columbia University, New York, New York, United States, (e-mail: rayb@cs.columbia.edu)","Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95% at detecting vulnerabilities. In this paper, we ask, ""how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario"". To our surprise, we find that their performance drops by more than 50%. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline up to 33.57% boost in precision and 128.38% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: https://git.io/Jf6IA. IEEE","Data models; datasets; gaze detection; Neural networks; neural networks; Predictive models; Security; Testing; text tagging; Training; Training data","Drops; Forecasting; Automated detection; Data duplication; Empirical findings; Prediction research; Prediction systems; Software security; Software vulnerabilities; Vulnerability detection; Deep learning",Article,"Article in Press","",Scopus,2-s2.0-85111011696
"Kaur H., Manduchi R.","57217714958;7004297978;","Subject guided eye image synthesis with application to gaze redirection",2021,"Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021",,,,"11","20",,,"10.1109/WACV48630.2021.00006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109784647&doi=10.1109%2fWACV48630.2021.00006&partnerID=40&md5=f5865a1385e7d9e8ab737bd985d48618","University of California, Santa Cruz, United States","Kaur, H., University of California, Santa Cruz, United States; Manduchi, R., University of California, Santa Cruz, United States","We propose a method for synthesizing eye images from segmentation masks with a desired style. The style encompasses attributes such as skin color, texture, iris color, and personal identity. Our approach generates an eye image that is consistent with a given segmentation mask and has the attributes of the input style image. We apply our method to data augmentation as well as to gaze redirection. The previous techniques of synthesizing real eye images from synthetic eye images for data augmentation lacked control over the generated attributes. We demonstrate the effectiveness of the proposed method in synthesizing realistic eye images with given characteristics corresponding to the synthetic labels for data augmentation, which is further useful for various tasks such as gaze estimation, eye image segmentation, pupil detection, etc. We also show how our approach can be applied to gaze redirection using only synthetic gaze labels, improving the previous state of the art results. The main contributions of our paper are i) a novel approach for Style-Based eye image generation from segmentation mask; ii) the use of this approach for gaze-redirection without the need for gaze annotated real eye images © 2021 IEEE.",,"Computer vision; Image enhancement; Textures; Color textures; Data augmentation; Eye images; Gaze estimation; Images segmentations; Images synthesis; Personal identity; Pupils detection; Segmentation masks; Skin colour; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85109784647
"Hong P.-H., Wang Y.","57219339163;36715604600;","Real-Time Driver’s Focus of Attention Extraction and Prediction using Deep Learning",2021,"International Journal of Advanced Computer Science and Applications","12","6",,"1","10",,,"10.14569/IJACSA.2021.0120601","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109182953&doi=10.14569%2fIJACSA.2021.0120601&partnerID=40&md5=36d40e4eb15c4fd7abbba081be97c1c8","Department of Computer Science, Texas A&M University-Commerce Commerce, Texas, 75428, United States","Hong, P.-H., Department of Computer Science, Texas A&M University-Commerce Commerce, Texas, 75428, United States; Wang, Y., Department of Computer Science, Texas A&M University-Commerce Commerce, Texas, 75428, United States","Driving is one of the most common activities in our modern lives. Every day, millions drive to and from their schools or workplaces. Even though this activity seems simple and everyone knows how to drive on roads, it actually requires drivers’ complete attention to keep their eyes on the road and surrounding cars for safe driving. However, most of the research focused on either keeping improving the configurations of active safety systems with high-cost components like Lidar, night vision cameras, and radar sensor array, or finding the optimal way of fusing and interpreting sensor information without considering the impact of drivers’ continuous attention and focus. We notice that effective safety technologies and systems are greatly affected by drivers’ attention and focus. In this paper, we design, implement and evaluate DFaep, a deep learning network for automatically examining, estimating, and predicting driver’s focus of attention in a real-time manner with dual low-cost dash cameras for driver-centric and car-centric views. Based on the raw stream data captured by the dash cameras during driving, we first detect the driver’s face and eye and generate augmented face images to extract facial features and enable real-time head movement tracking. We then parse the driver’s attention behaviors and gaze focus together with the road scene data captured by one front-facing dash camera faced towards the roads. Facial features, augmented face images, and gaze focus data are then inputted to our deep learning network for modeling drivers’ driving and attention behaviors. Experiments are then conducted on the large dataset, DR(eye)VE, and our own dataset under realistic driving conditions. The findings of this study indicated that the distribution of driver’s attention and focus is highly skewed. Results show that DFaep can quickly detect and predict the driver’s attention and focus, and the average accuracy of prediction is 99.38%. This will provide a basis and feasible solution with a computational learnt model for capturing and understanding driver’s attention and focus to help avoid fatal collisions and eliminate the probability of potential unsafe driving behavior in the future. © 2021","attention; deep learning; deep neural network; Driving; interesting zones; models",,Article,"Final","",Scopus,2-s2.0-85109182953
"Amer S.G., Kamh S.A., Elshahed M.A., Ramadan R.A.","57225110585;6602245911;56494900800;15058265600;","Wheelchair Control System based Eye Gaze",2021,"International Journal of Advanced Computer Science and Applications","12","6",,"895","900",,,"10.14569/IJACSA.2021.01206104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109153735&doi=10.14569%2fIJACSA.2021.01206104&partnerID=40&md5=add1e75daee5009f355b7d29ff8199b5","Department of Physics, Faculty of Women, Ain Shames University, Cairo, Egypt; Computer Engineering Department, Faculty of Engineering, Cairo University, Cairo, Egypt","Amer, S.G., Department of Physics, Faculty of Women, Ain Shames University, Cairo, Egypt; Kamh, S.A., Department of Physics, Faculty of Women, Ain Shames University, Cairo, Egypt; Elshahed, M.A., Department of Physics, Faculty of Women, Ain Shames University, Cairo, Egypt; Ramadan, R.A., Computer Engineering Department, Faculty of Engineering, Cairo University, Cairo, Egypt","The inability to control the limbs is the main reason that affects the daily activities of the disabled which causes social restrictions and isolation. More studies were performed to help disabilities for easy communication with the outside world and others. Various techniques are designed to help the disabled in carrying out daily activities easily. Among these technologies is the Smart Wheelchair. This research aims to develop a smart eye-controlled wheelchair whose movement depends on eye movement tracking. The proposed Wheelchair is simple in design and easy to use with low cost compared with previous Wheelchairs. The eye movement was detected through a camera fixed on the chair. The user’s gaze direction is obtained from the captured image after some processing and analysis. The order is sent to the Arduino Uno board which controls the wheelchair movement. The Wheelchair performance was checked using different volunteers and its accuracy reached 94.4% with a very short response time compared with the other existing chairs. © 2021. All Rights Reserved.","deep learning; Dilip; facial landmarks points; gaze ratio; numpy",,Article,"Final","",Scopus,2-s2.0-85109153735
"Srivastava A., Sangwan K.S., Dhiraj","57211272077;6701355439;56009552500;","Real-Time Driver Drowsiness Detection Using GRU with CNN Features",2021,"Communications in Computer and Information Science","1378 CCIS",,,"501","513",,,"10.1007/978-981-16-1103-2_42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107521142&doi=10.1007%2f978-981-16-1103-2_42&partnerID=40&md5=f5fc91dbcc278a766af1c4000c480600","Birla-Institute of Technology and Science (BITS, Pilani), Pilani Campus, Pilani, Rajasthan, India; CSIR - Central Electronics Engineering Research Institute (CSIR-CEERI), Pilani, India","Srivastava, A., Birla-Institute of Technology and Science (BITS, Pilani), Pilani Campus, Pilani, Rajasthan, India; Sangwan, K.S., Birla-Institute of Technology and Science (BITS, Pilani), Pilani Campus, Pilani, Rajasthan, India; Dhiraj, CSIR - Central Electronics Engineering Research Institute (CSIR-CEERI), Pilani, India","There are many visual characteristics linked with drowsiness like eye closure duration, blinking frequency, gaze, pose, and yawning. In this paper, we propose a robust deep learning approach to detect a driver’s drowsiness. We start by extracting the mouth region from incoming frames of the video stream using Dlib’s frontal face detector and a custom dlib landmark detector. Then, a deep convolutional neural network (DCNN) is used to extract deep features. Lastly, yawning is detected using a yawn detector consisting of 1D-DepthWise Separable-CNN and Gated Recurrent Unit (GRU) which learns the mapping or patterns from temporal information of the sequence of features extracted from frames and predicts yawning/not yawning. Yawn detector was able to reach training and validation accuracy of 99.99% and 99.97% respectively. We were able to achieve an inference speed of ~30FPS on our host machine on live video recording and ~23 FPS on an embedded board for the same. To check the robustness of our model, testing was done on the YawDD test set where we were able to detect yawning successfully. On the other hand, while testing NTHU data many false positive was observed. Thus, our approach can be effectively used for real-time driver drowsiness detection on an embedded platform. © 2021, Springer Nature Singapore Pte Ltd.","Convolutional neural networks (CNN); Drowsiness detection; Gated Recurrent Unit (GRU); Recurrent neural networks (RNN)","Computer vision; Convolutional neural networks; Deep neural networks; Recurrent neural networks; Video recording; Driver drowsiness; Embedded boards; Embedded platforms; Eye closure; False positive; Frontal faces; Learning approach; Temporal information; Feature extraction",Conference Paper,"Final","",Scopus,2-s2.0-85107521142
"Imaoka H., Hashimoto H., Takahashi K., Ebihara A.F., Liu J., Hayasaka A., Morishita Y., Sakurai K.","7003746327;57214231120;57224105998;57219621812;34976887400;7004539705;36617695400;7402174170;","The future of biometrics technology: From face recognition to related applications",2021,"APSIPA Transactions on Signal and Information Processing",,,,"","",,,"10.1017/ATSIP.2021.8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106991714&doi=10.1017%2fATSIP.2021.8&partnerID=40&md5=04f8a862d8e13f41f5ffb857bc7b9a3c","NEC Corporation, Tokyo, Minato-ku, Japan","Imaoka, H., NEC Corporation, Tokyo, Minato-ku, Japan; Hashimoto, H., NEC Corporation, Tokyo, Minato-ku, Japan; Takahashi, K., NEC Corporation, Tokyo, Minato-ku, Japan; Ebihara, A.F., NEC Corporation, Tokyo, Minato-ku, Japan; Liu, J., NEC Corporation, Tokyo, Minato-ku, Japan; Hayasaka, A., NEC Corporation, Tokyo, Minato-ku, Japan; Morishita, Y., NEC Corporation, Tokyo, Minato-ku, Japan; Sakurai, K., NEC Corporation, Tokyo, Minato-ku, Japan","Biometric recognition technologies have become more important in the modern society due to their convenience with the recent informatization and the dissemination of network services. Among such technologies, face recognition is one of the most convenient and practical because it enables authentication from a distance without requiring any authentication operations manually. As far as we know, face recognition is susceptible to the changes in the appearance of faces due to aging, the surrounding lighting, and posture. There were a number of technical challenges that need to be resolved. Recently, remarkable progress has been made thanks to the advent of deep learning methods. In this position paper, we provide an overview of face recognition technology and introduce its related applications, including face presentation attack detection, gaze estimation, person re-identification and image data mining. We also discuss the research challenges that still need to be addressed and resolved. Copyright © The Author(s), 2021 published by Cambridge University Press in association with Asia Pacific Signal and Information Processing Association.","Biometrics; Deep Learning; Face Recognition","Authentication; Biometrics; Data mining; Deep learning; Learning systems; Pattern recognition systems; Biometric recognition technology; Biometrics technology; Face recognition technologies; Image data minings; Learning methods; Person re identifications; Research challenges; Technical challenges; Face recognition",Article,"Article in Press","",Scopus,2-s2.0-85106991714
"Nagamatsu T., Hiroe M., Arai H.","23398000100;57202892342;57224004474;","Extending the measurement angle of a gaze estimation method using an eye model expressed by a revolution about the optical axis of the eye",2021,"IEICE Transactions on Information and Systems","E104.D","5",,"729","738",,,"10.1587/transinf.2020EDP7072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106705057&doi=10.1587%2ftransinf.2020EDP7072&partnerID=40&md5=6a7d22411b47d3945f834cff92dbe781","Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658–0022, Japan","Nagamatsu, T., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658–0022, Japan; Hiroe, M., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658–0022, Japan; Arai, H., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658–0022, Japan","SUMMARY An eye model expressed by a revolution about the optical axis of the eye is one of the most accurate models for use in a 3D gaze estimation method. The measurement range of the previous gaze estimation method that uses two cameras based on the eye model is limited by the larger of the two angles between the gaze and the optical axes of two cameras. The previous method cannot calculate the gaze when exceeding a certain limit of the rotation angle of the eye. In this paper, we show the characteristics of reflections on the surface of the eye from two light sources, when the eye rotates. Then, we propose a method that extends the rotation angle of the eye for a 3D gaze estimation based on this model. The proposed method uses reflections that were not used in the previous method. We developed an experimental gaze tracking system for a wide projector screen and experimentally validated the proposed method with 20 participants. The result shows that the proposed method can measure the gaze of more number of people with increased accuracy compared with the previous method. Copyright © 2021 The Institute of Electronics, Information and Communication Engineers","Aspherical eye model; Eye tracking; Gaze tracking","Cameras; Light sources; Eye model; Gaze estimation; Gaze tracking system; Measurement range; Number of peoples; Optical axes; Optical axis; Rotation angles; Eye tracking",Article,"Final","",Scopus,2-s2.0-85106705057
"Hu T., Jha S., Busso C.","57221705332;57193014012;35742852700;","Temporal Head Pose Estimation From Point Cloud in Naturalistic Driving Conditions",2021,"IEEE Transactions on Intelligent Transportation Systems",,,,"","",,1,"10.1109/TITS.2021.3075350","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105844898&doi=10.1109%2fTITS.2021.3075350&partnerID=40&md5=a1fc0876bcf359bbb6fa098ab8746d25","Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA.; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA (e-mail: busso@utdallas.edu).","Hu, T., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA.; Jha, S., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA.; Busso, C., Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA (e-mail: busso@utdallas.edu).","Head pose estimation is an important problem as it facilitates tasks such as gaze estimation and attention modeling. In the automotive context, head pose provides crucial information about the driver's mental state, including drowsiness, distraction and attention. It can also be used for interaction with in-vehicle infotainment systems. While computer vision algorithms using RGB cameras are reliable in controlled environments, head pose estimation is a challenging problem in the car due to sudden illumination changes, occlusions and large head rotations that are common in a vehicle. These issues can be partially alleviated by using depth cameras. Head rotation trajectories are continuous with important temporal dependencies. Our study leverages this observation, proposing a novel temporal deep learning model for head pose estimation from point cloud. The approach extracts discriminative feature representation directly from point cloud data, leveraging the 3D spatial structure of the face. The frame-based representations are then combined with bidirectional long short term memory (BLSTM) layers. We train this model on the newly collected multimodal driver monitoring (MDM) dataset, achieving better results compared to non-temporal algorithms using point cloud data, and state-of-the-art models using RGB images. We further show quantitatively and qualitatively that incorporating temporal information provides large improvements not only in accuracy, but also in the smoothness of the predictions. CCBYNCND","Cameras; deep learning; Deep learning; Driver head pose estimation; Feature extraction; Magnetic heads; point cloud; Pose estimation; temporal modeling.; Three-dimensional displays; Vehicles","Cameras; Driver training; 3D spatial structure; Computer vision algorithms; Controlled environment; Discriminative features; Head Pose Estimation; Illumination changes; Temporal information; Vehicle infotainment; Deep learning",Article,"Article in Press","",Scopus,2-s2.0-85105844898
"Zhuang Y., Zhang Y., Zhao H.","57223040112;57209691865;16320268200;","Appearance-based gaze estimation using separable convolution neural networks",2021,"IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)",,,"9390807","609","612",,,"10.1109/IAEAC50856.2021.9390807","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104620141&doi=10.1109%2fIAEAC50856.2021.9390807&partnerID=40&md5=2f2da48739995189086935afa77c0b95","North China University of Technology, Information Institute, Beijing, China","Zhuang, Y., North China University of Technology, Information Institute, Beijing, China; Zhang, Y., North China University of Technology, Information Institute, Beijing, China; Zhao, H., North China University of Technology, Information Institute, Beijing, China","Gaze estimation is one of the current important research contents of computer vision. For the current situation where the gaze estimation neural network has a large amount of parameters but the accuracy is not greatly improved and the head pose is difficult to handle, this paper proposes a simplified gaze estimation network model SLeNet based on the LeNet neural network. The deep separable convolution in the Xception network is used to reduce the amount of parameters in the convolution part and improve the computational performance of the network model. The method of splicing head posture features is retained, but another branch neural network is designed to learn head posture based on eye image and mouth corner information, and no additional module is required to obtain head posture separately. The improved network model is used to compare experiments with the original network and VGG-16 on the MPIIGaze dataset. The results show that the improved SLeNet network model performs better on the MPIIGaze dataset than LeNet and VGG-16 and has fewer parameters. © 2021 IEEE.","depth separable convolution; gaze estimation; head pose; neural network","Convolution; Appearance based; Computational performance; Convolution neural network; Current situation; Gaze estimation; Head posture; Large amounts; Network modeling; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85104620141
"Garde G., Larumbe-Bergera A., Porta S., Cabeza R., Villanueva A.","57215963483;57210106737;7005292345;36763933900;7101612861;","Synthetic Gaze Data Augmentation for Improved User Calibration",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12663 LNCS",,,"377","389",,1,"10.1007/978-3-030-68796-0_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104281273&doi=10.1007%2f978-3-030-68796-0_27&partnerID=40&md5=e404f7e913eebea8d638b9a00a0de72f","Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain","Garde, G., Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain; Larumbe-Bergera, A., Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain; Porta, S., Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain; Cabeza, R., Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain; Villanueva, A., Public University of Navarra (UPNA), Calle Arrosadía, Pamplona, 31006, Spain","In this paper, we focus on the calibration possibilitiesó of a deep learning based gaze estimation process applying transfer learning, comparing its performance when using a general dataset versus when using a gaze specific dataset in the pretrained model. Subject calibration has demonstrated to improve gaze accuracy in high performance eye trackers. Hence, we wonder about the potential of a deep learning gaze estimation model for subject calibration employing fine-tuning procedures. A pretrained Resnet-18 network, which has great performance in many computer vision tasks, is fine-tuned using user’s specific data in a few shot adaptive gaze estimation approach. We study the impact of pretraining a model with a synthetic dataset, U2Eyes, before addressing the gaze estimation calibration in a real dataset, I2Head. The results of the work show that the success of the individual calibration largely depends on the balance between fine-tuning and the standard supervised learning procedures and that using a gaze specific dataset to pretrain the model improves the accuracy when few images are available for calibration. This paper shows that calibration is feasible in low resolution scenarios providing outstanding accuracies below 1.5 ∘ of error. © 2021, Springer Nature Switzerland AG.","Calibration; Gaze estimation; Transfer learning","Deep learning; Image enhancement; Learning systems; Pattern recognition; Transfer learning; Data augmentation; Eye trackers; Fine tuning; Gaze estimation; Individual calibrations; Low resolution; Pre-training; User calibration; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-85104281273
"Golard A., Talathi S.S.","57222990964;57224997923;","Ultrasound for Gaze Estimation",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12663 LNCS",,,"369","376",,,"10.1007/978-3-030-68796-0_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104277085&doi=10.1007%2f978-3-030-68796-0_26&partnerID=40&md5=0cb0652f9e6014dbb3b879276e1455b4","Facebook Reality Labs, Redmond, WA  98052, United States","Golard, A., Facebook Reality Labs, Redmond, WA  98052, United States; Talathi, S.S., Facebook Reality Labs, Redmond, WA  98052, United States","Most eye tracking methods are light-based. As such they can suffer from ambient light changes when used outdoors. It has been suggested that ultrasound could provide a low power, fast, light-insensitive alternative to camera based sensors for eye tracking. We designed a bench top experimental setup to investigate the utility of ultrasound for eye tracking, and collected time of flight and amplitude data for a range of gaze angles of a model eye. We used this data as input for a machine learning model and demonstrate that we can effectively estimate gaze (gaze RMSE error of 1.021 ± 0.189 ∘ with an adjusted R2 score of 89.92 ± 4.9). © 2021, Springer Nature Switzerland AG.","CMUT; Eye tracking; Machine learning; Ultrasound","Pattern recognition; Turing machines; Ultrasonics; Ambient light; Camera based Sensors; Eye tracking methods; Gaze estimation; Low Power; Machine learning models; Range of gaze; Time of flight; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85104277085
"Guo Z., Yuan Z., Zhang C., Chi W., Ling Y., Zhang S.","57221249235;7401477128;57216944490;55340588700;56030248400;57216951195;","Domain Adaptation Gaze Estimation by Embedding with Prediction Consistency",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12626 LNCS",,,"292","307",,,"10.1007/978-3-030-69541-5_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103356084&doi=10.1007%2f978-3-030-69541-5_18&partnerID=40&md5=a6edfa12d6e871b51f13f97a0d224033","Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Tencent Robotics X, Beijing, China","Guo, Z., Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Yuan, Z., Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Zhang, C., Tencent Robotics X, Beijing, China; Chi, W., Tencent Robotics X, Beijing, China; Ling, Y., Tencent Robotics X, Beijing, China; Zhang, S., Tencent Robotics X, Beijing, China","Gaze is the essential manifestation of human attention. In recent years, a series of work has achieved high accuracy in gaze estimation. However, the inter-personal difference limits the reduction of the subject-independent gaze estimation error. This paper proposes an unsupervised method for domain adaptation gaze estimation to eliminate the impact of inter-personal diversity. In domain adaption, we design an embedding representation with prediction consistency to ensure that linear relationships between gaze directions in different domains remain consistent on gaze space and embedding space. Specifically, we employ source gaze to form a locally linear representation in the gaze space for each target domain prediction. Then the same linear combinations are applied in the embedding space to generate hypothesis embedding for the target domain sample, remaining prediction consistency. The deviation between the target and source domain is reduced by approximating the predicted and hypothesis embedding for the target domain sample. Guided by the proposed strategy, we design Domain Adaptation Gaze Estimation Network(DAGEN), which learns embedding with prediction consistency and achieves state-of-the-art results on both the MPIIGaze and the EYEDIAP datasets. © 2021, Springer Nature Switzerland AG.",,"Computer vision; Forecasting; Different domains; Domain adaptation; Domain adaptions; Linear combinations; Linear relationships; Linear representation; State of the art; Unsupervised method; Embeddings",Conference Paper,"Final","",Scopus,2-s2.0-85103356084
"Wang X., Zhang J., Zhang H., Zhao S., Liu H.","57219425011;57214900779;57222558640;57222556510;57218402201;","Vision-based Gaze Estimation: A Review",2021,"IEEE Transactions on Cognitive and Developmental Systems",,,,"","",,,"10.1109/TCDS.2021.3066465","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103172218&doi=10.1109%2fTCDS.2021.3066465&partnerID=40&md5=2a953fb3a71db63522a4fcb7982a7621","school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China. (e-mail: 57652761@qq.com); School of Computer Science and Engineering, Tianjin University of Technology, Tianjin 300384, China.; school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China.; School of Computing, University of Portsmouth, Portsmouth PO13HE, U.K.","Wang, X., school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China. (e-mail: 57652761@qq.com); Zhang, J., School of Computer Science and Engineering, Tianjin University of Technology, Tianjin 300384, China.; Zhang, H., school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China.; Zhao, S., School of Computing, University of Portsmouth, Portsmouth PO13HE, U.K.; Liu, H., school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China.","Eye gaze is an important natural behavior in social interaction as it delivers complex exchanges between observer and observed, by building up the geometric constraints and relation of the exchanges. These inter-person exchanges can be modeled based on gaze direction estimated using computer vision. Despite significant progresses in vision-based gaze estimation in last 10 years, it is still nontrivial since the accuracy of gaze estimation is significantly affected by such intrinsic factors as head pose variance, individual bias between optical axis and visual axis, eye blink, occlusion and image blur, degrade gaze features, lead to inaccurate gaze-involved human social interaction analysis. This paper aims to review and discuss existing methods addressing above-mentioned problems, gaze involved applications and datasets against the state-of-the-arts in vision-based gaze estimation. It also points out future research directions and challenges of gaze estimation in terms of meta learning, causal inference, disentangled representation, and social gaze behaviour for unconstrained gaze estimation. IEEE","3D Gaze Estimation; Computer Vision.; Estimation; Faces; Feature extraction; Head pose; Iris; Optical axis; Solid modeling; Three-dimensional displays; Visual axis; Visualization","Causal inferences; Future research directions; Gaze estimation; Geometric constraint; Human social interactions; Intrinsic factors; Social interactions; State of the art; Arts computing",Article,"Article in Press","",Scopus,2-s2.0-85103172218
"Aakur S.N., Bagavathi A.","57202367984;56879080800;","Unsupervised gaze prediction in egocentric videos by energy-based surprise modeling",2021,"VISIGRAPP 2021 - Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","5",,,"935","942",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102970100&partnerID=40&md5=ebe861f5bce936c29f251ce5e2e7517c","Department of Computer Science, Oklahoma State University, Stillwater, OK  74078, United States","Aakur, S.N., Department of Computer Science, Oklahoma State University, Stillwater, OK  74078, United States; Bagavathi, A., Department of Computer Science, Oklahoma State University, Stillwater, OK  74078, United States","Egocentric perception has grown rapidly with the advent of immersive computing devices. Human gaze prediction is an important problem in analyzing egocentric videos and has primarily been tackled through either saliency-based modeling or highly supervised learning. We quantitatively analyze the generalization capabilities of supervised, deep learning models on the egocentric gaze prediction task on unseen, out-of-domain data. We find that their performance is highly dependent on the training data and is restricted to the domains specified in the training annotations. In this work, we tackle the problem of jointly predicting human gaze points and temporal segmentation of egocentric videos without using any training data. We introduce an unsupervised computational model that draws inspiration from cognitive psychology models of event perception. We use Grenander’s pattern theory formalism to represent spatial-temporal features and model surprise as a mechanism to predict gaze fixation points. Extensive evaluation on two publicly available datasets - GTEA and GTEA+ datasets-shows that the proposed model can significantly outperform all unsupervised baselines and some supervised gaze prediction baselines. Finally, we show that the model can also temporally segment egocentric videos with a performance comparable to more complex, fully supervised deep learning baselines. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Egocentric vision; Pattern theory; Temporal event segmentation; Unsupervised gaze prediction","Computation theory; Computer graphics; Computer vision; Deep learning; Forecasting; Cognitive psychology; Computational model; Computing devices; Generalization capability; Learning models; Prediction tasks; Spatial-temporal features; Temporal segmentations; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85102970100
"Mellouk W., Handouzi W.","57219662091;56028804900;","Convolutional Neural Network for Identifying Human Emotions with Different Head Poses",2021,"Lecture Notes in Networks and Systems","183",,,"785","796",,,"10.1007/978-3-030-66840-2_59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102621011&doi=10.1007%2f978-3-030-66840-2_59&partnerID=40&md5=3337ed66cf351177b604e94b2f1c1a3d","Laboratoire D’automatique de Tlemcen LAT, Tlemcen University, Tlemcen, Algeria","Mellouk, W., Laboratoire D’automatique de Tlemcen LAT, Tlemcen University, Tlemcen, Algeria; Handouzi, W., Laboratoire D’automatique de Tlemcen LAT, Tlemcen University, Tlemcen, Algeria","Automatic facial emotion recognition is intriguing, emerging research area, provided many advantages in different fields. In recent years, artificial intelligence has enjoyed enormous success thanks to powerful deep learning architectures, capable of interpreting and classifying after training with several data. Researchers are now using this technique to code facial expressions, to a get better emotion classification. The objective of our study is to achieve better precision in classifying the seven basic emotions and to overcome several challenges such as different ages, sexes, races, head poses, and gazes. In this work, we propose deep convolutional neural networks CNN evaluated on the RaFD database. In the first, we studied emotions with frontal head pose then we passed on the studying through different head poses (front, left, and right). A 98% of accuracy with 5% loss validation obtained on frontal faces and 96.55% of accuracy with 11% of loss validation on three head poses. Our method achieved competitive results with the state of the art methods trained and tested with the same database. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","CNN; Facial expression; Head pose",,Conference Paper,"Final","",Scopus,2-s2.0-85102621011
"Krishnan S., Amudha J., Tejwani S.","57222997680;35766448700;55988859900;","Gaze Fusion-Deep Neural Network Model for Glaucoma Detection",2021,"Communications in Computer and Information Science","1366",,,"42","53",,,"10.1007/978-981-16-0419-5_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102276918&doi=10.1007%2f978-981-16-0419-5_4&partnerID=40&md5=883e0113ea30be750a0f2d3e809c7f1f","Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bangalore, India; Narayana Nethralaya, Bommasandra, Bengaluru, India","Krishnan, S., Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bangalore, India; Amudha, J., Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bangalore, India; Tejwani, S., Narayana Nethralaya, Bommasandra, Bengaluru, India","The proposed system, Gaze Fusion - Deep Neural Network Model (GFDM) has utilized transfer learning approach to discriminate subject’s eye tracking data in the form of fusion map into two classes: glaucoma and normal. We have fed eye tracking data in the form of fusion maps of different participants to Deep Neural Network (DNN) model which is pretrained with ImageNet weights. The experimental results of the GFDM show that fusion map dissimilar to pretrained model’s dataset can give better understanding of glaucoma. The model also show the part of the screen where participants has the difficulty in viewing. GFDM has compared with traditional machine learning models such as Support Vector Classifier, Decision Tree classifier and ensemble classifier and shown that the proposed model outperforms other classifiers. The model has Area Under ROC Curve (AUC) score 0.75. The average sensitivity of correctly identifying glaucoma patients is 100% with specificity value 83%. © 2021, Springer Nature Singapore Pte Ltd.","Deep neural network; Eye tracking; Fusion map; Glaucoma; Transfer learning","Decision trees; Deep learning; Deep neural networks; Eye tracking; Heuristic algorithms; Learning algorithms; Learning systems; Ophthalmology; Transfer learning; Area under roc curve (AUC); Average sensitivities; Decision tree classifiers; Ensemble classifiers; Glaucoma detection; Machine learning models; Neural network model; Support vector classifiers; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85102276918
[无可用作者姓名],[无可用的作者 ID],"2nd Symposium on Machine Learning and Metaheuristics Algorithms, and Applications, SoMMA 2020",2021,"Communications in Computer and Information Science","1366",,,"","",246,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102250189&partnerID=40&md5=ed9852bd2e4c60db2671cc09f4a7d01f",,"","The proceedings contain 19 papers. The special focus in this conference is on 2nd Symposium on Machine Learning and Metaheuristics Algorithms, and Applications, SoMMA 2020. The topics include: Exam Seating Allocation to Prevent Malpractice Using Genetic Multi-optimization Algorithm; Big Data: Does BIG Matter for Your Business?; modelling Energy Consumption of Domestic Households via Supervised and Unsupervised Learning: A Case Study; machine Learning and Soft Computing Techniques for Combustion System Diagnostics and Monitoring: A Survey; Traffic Sign Classification Using ODENet; Analysis of UNSW-NB15 Dataset Using Machine Learning Classifiers; concept Drift Detection in Phishing Using Autoencoders; detection of Obfuscated Mobile Malware with Machine Learning and Deep Learning Models; CybSecMLC: A Comparative Analysis on Cyber Security Intrusion Detection Using Machine Learning Classifiers; smart Security and Surveillance System in Laboratories Using Machine Learning; Deep Neural Networks with Multi-class SVM for Recognition of Cross-Spectral Iris Images; gaze Fusion-Deep Neural Network Model for Glaucoma Detection; deep Learning Based Stable and Unstable Candle Flame Detection; emotion Recognition from Facial Expressions Using Siamese Network; activity Modeling of Individuals in Domestic Households Using Fuzzy Logic; Stock Price Prediction Using Machine Learning and LSTM-Based Deep Learning Models; an Improved Salp Swarm Algorithm Based on Adaptive β -Hill Climbing for Stock Market Prediction.",,,Conference Review,"Final","",Scopus,2-s2.0-85102250189
"Gupta P.","57207922785;","MERASTC: Micro-expression Recognition using Effective Feature Encodings and 2D Convolutional Neural network",2021,"IEEE Transactions on Affective Computing",,,,"","",,,"10.1109/TAFFC.2021.3061967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101847104&doi=10.1109%2fTAFFC.2021.3061967&partnerID=40&md5=ab17068e067189cdd1eec13b2d7a0b29","Computer Science and Engineering, IIT Indore, 226957 Indore, Madhya Pradesh, India, 452017 (e-mail: puneet@iiti.ac.in)","Gupta, P., Computer Science and Engineering, IIT Indore, 226957 Indore, Madhya Pradesh, India, 452017 (e-mail: puneet@iiti.ac.in)","Facial micro-expression (ME) can disclose genuine and concealed human feelings. It makes MEs extensively useful in real-world applications pertaining to affective computing and psychology. Unfortunately, they are induced by subtle facial movements for a short duration of time, which makes the ME recognition, a highly challenging problem even for human beings. In automatic ME recognition, the well-known features encode either incomplete or redundant information, and there is a lack of sufficient training data. The proposed method, Micro-Expression Recognition by Analysing Spatial and Temporal Characteristics, MERASTC mitigates these issues for improving the ME recognition. It compactly encodes the subtle deformations using action units (AUs), landmarks, gaze, and appearance features of all the video frames while preserving most of the relevant ME information. Furthermore, it improves the efficacy by introducing a novel neutral face normalization for ME and initiating the utilization of gaze features in deep learning based ME recognition. The features are provided to the 2D convolutional neural network that jointly analyses the spatial and temporal behavior for correct ME classification. Experimental results on the publicly available datasets indicate that the proposed method exhibits better performance than the well-known methods. IEEE","Action units; Deep Learning; Gaze feature; Micro-expression Recognition; Spatiotemporal CNN",,Article,"Article in Press","",Scopus,2-s2.0-85101847104
"Zhang L., Su G., Yin J., Li Y., Lin Q., Zhang X., Shao L.","35231925400;57222182040;8249720800;55945677900;57220176427;35232030000;55643855000;","Bioinspired Scene Classification by Deep Active Learning With Remote Sensing Applications",2021,"IEEE Transactions on Cybernetics",,,,"","",,,"10.1109/TCYB.2020.2981480","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101805922&doi=10.1109%2fTCYB.2020.2981480&partnerID=40&md5=4e3bb3d71607c3396da304e1935fc669","College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou, China.; College of Computer Sciences, Zhejiang University, Hangzhou 310027, China.; College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou, China; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE.","Zhang, L., College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou, China.; Su, G., College of Computer Sciences, Zhejiang University, Hangzhou 310027, China.; Yin, J., College of Computer Sciences, Zhejiang University, Hangzhou 310027, China.; Li, Y., College of Computer Sciences, Zhejiang University, Hangzhou 310027, China.; Lin, Q., College of Computer Sciences, Zhejiang University, Hangzhou 310027, China.; Zhang, X., College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou, China; Shao, L., Inception Institute of Artificial Intelligence, Abu Dhabi, UAE.","Accurately classifying sceneries with different spatial configurations is an indispensable technique in computer vision and intelligent systems, for example, scene parsing, robot motion planning, and autonomous driving. Remarkable performance has been achieved by the deep recognition models in the past decade. As far as we know, however, these deep architectures are incapable of explicitly encoding the human visual perception, that is, the sequence of gaze movements and the subsequent cognitive processes. In this article, a biologically inspired deep model is proposed for scene classification, where the human gaze behaviors are robustly discovered and represented by a unified deep active learning (UDAL) framework. More specifically, to characterize objects' components with varied sizes, an objectness measure is employed to decompose each scenery into a set of semantically aware object patches. To represent each region at a low level, a local-global feature fusion scheme is developed which optimally integrates multimodal features by automatically calculating each feature's weight. To mimic the human visual perception of various sceneries, we develop the UDAL that hierarchically represents the human gaze behavior by recognizing semantically important regions within the scenery. Importantly, UDAL combines the semantically salient region detection and the deep gaze shifting path (GSP) representation learning into a principled framework, where only the partial semantic tags are required. Meanwhile, by incorporating the sparsity penalty, the contaminated/redundant low-level regional features can be intelligently avoided. Finally, the learned deep GSP features from the entire scene images are integrated to form an image kernel machine, which is subsequently fed into a kernel SVM to classify different sceneries. Experimental evaluations on six well-known scenery sets (including remote sensing images) have shown the competitiveness of our approach. IEEE","Active learning; bioinspired; contaminated; gaze behavior; machine learning; multimodal; remote sensing","Behavioral research; Biomimetics; Computer vision; Formal languages; Intelligent systems; Remote sensing; Robot programming; Semantics; Support vector machines; Vision; Biologically inspired; Experimental evaluation; Human visual perception; Remote sensing applications; Remote sensing images; Robot motion planning; Salient region detections; Spatial configuration; Deep learning",Article,"Article in Press","",Scopus,2-s2.0-85101805922
"Ceccacci S., Generosi A., Cimini G., Faggiano S., Giraldi L., Mengoni M.","55546505300;57201216842;57222172235;57220105724;57191833336;22634930400;","Facial coding as a mean to enable continuous monitoring of student’s behavior in e-Learning",2021,"CEUR Workshop Proceedings","2817",,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101784233&partnerID=40&md5=83c0294087b1f28d339a89775ae9f6dd","Università Politecnica delle Marche, Via Brecce Bianche 12, Ancona, 60131, Italy; Emoj srl, via Ferruccio Fioretti 10/B, Ancona, 60131, Italy","Ceccacci, S., Università Politecnica delle Marche, Via Brecce Bianche 12, Ancona, 60131, Italy; Generosi, A., Università Politecnica delle Marche, Via Brecce Bianche 12, Ancona, 60131, Italy; Cimini, G., Emoj srl, via Ferruccio Fioretti 10/B, Ancona, 60131, Italy; Faggiano, S., Emoj srl, via Ferruccio Fioretti 10/B, Ancona, 60131, Italy; Giraldi, L., Emoj srl, via Ferruccio Fioretti 10/B, Ancona, 60131, Italy; Mengoni, M., Università Politecnica delle Marche, Via Brecce Bianche 12, Ancona, 60131, Italy","This paper introduces an e-learning platform for the management of courses based on MOOCs, able to continuously monitoring student’s behavior through facial coding techniques, with a low computational effort client-side, and to provide useful insight for the instructor. The system exploits the most recent developments in Deep Learning and Computer Vision for Affective Computing, in compliance with the European GDPR. Taking as input the video capture by the webcam of the device used to attend the course, it: (1) performs continuous student’s authentication based on face recognition, (2) monitors the student’s level of attention through head orientation tracking and gaze detection analysis, (3) estimates student’s emotion during the course attendance. The paper describes the overall system design and reports the results of a preliminary survey, which involved a total of 14 subjects, aimed at investigating user acceptance, in terms of intention to continue using such a system. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Affective Computing; Deep Learning; E-leaning; Facial Coding; Facial Recognition","Computer aided instruction; Deep learning; E-learning; Face recognition; Privacy by design; Affective Computing; Coding techniques; Computational effort; Continuous monitoring; E-learning platforms; Gaze detection; Orientation tracking; User acceptance; Students",Conference Paper,"Final","",Scopus,2-s2.0-85101784233
[无可用作者姓名],[无可用的作者 ID],"27th International Conference on MultiMedia Modeling, MMM 2021",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12572 LNCS",,,"","",496,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101769138&partnerID=40&md5=75bc9269790025ec2c6d91e880dfac2e",,"","The proceedings contain 50 papers. The special focus in this conference is on MultiMedia Modeling. The topics include: Unsupervised Gaze: Exploration of Geometric Constraints for 3D Gaze Estimation; Median-Pooling Grad-CAM: An Efficient Inference Level Visual Explanation for CNN Networks in Remote Sensing Image Classification; multi-granularity Recurrent Attention Graph Neural Network for Few-Shot Learning; EEG Emotion Recognition Based on Channel Attention for E-Healthcare Applications; the MovieWall: A New Interface for Browsing Large Video Collections; keystroke Dynamics as Part of Lifelogging; HTAD: A Home-Tasks Activities Dataset with Wrist-Accelerometer and Audio Features; MNR-Air: An Economic and Dynamic Crowdsourcing Mechanism to Collect Personal Lifelog and Surrounding Environment Dataset. A Case Study in Ho Chi Minh City, Vietnam; kvasir-Instrument: Diagnostic and Therapeutic Tool Segmentation Dataset in Gastrointestinal Endoscopy; tropical Cyclones Tracking Based on Satellite Cloud Images: Database and Comprehensive Study; catMeows: A Publicly-Available Dataset of Cat Vocalizations; search and Explore Strategies for Interactive Analysis of Real-Life Image Collections with Unknown and Unique Categories; graph-Based Indexing and Retrieval of Lifelog Data; on Fusion of Learned and Designed Features for Video Data Analytics; XQM: Interactive Learning on Mobile Phones; a Multimodal Tensor-Based Late Fusion Approach for Satellite Image Search in Sentinel 2 Images; canopy Height Estimation from Spaceborne Imagery Using Convolutional Encoder-Decoder; implementation of a Random Forest Classifier to Examine Wildfire Predictive Modelling in Greece Using Diachronically Collected Fire Occurrence and Fire Mapping Data; mobile eHealth Platform for Home Monitoring of Bipolar Disorder; multimodal Sensor Data Analysis for Detection of Risk Situations of Fragile People in @home Environments.",,,Conference Review,"Final","",Scopus,2-s2.0-85101769138
[无可用作者姓名],[无可用的作者 ID],"27th International Conference on MultiMedia Modeling, MMM 2021",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12573 LNCS",,,"","",496,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101650562&partnerID=40&md5=dc48e538b3f8c6c7f934698a7dfd3e44",,"","The proceedings contain 50 papers. The special focus in this conference is on MultiMedia Modeling. The topics include: Unsupervised Gaze: Exploration of Geometric Constraints for 3D Gaze Estimation; Median-Pooling Grad-CAM: An Efficient Inference Level Visual Explanation for CNN Networks in Remote Sensing Image Classification; multi-granularity Recurrent Attention Graph Neural Network for Few-Shot Learning; EEG Emotion Recognition Based on Channel Attention for E-Healthcare Applications; the MovieWall: A New Interface for Browsing Large Video Collections; keystroke Dynamics as Part of Lifelogging; HTAD: A Home-Tasks Activities Dataset with Wrist-Accelerometer and Audio Features; MNR-Air: An Economic and Dynamic Crowdsourcing Mechanism to Collect Personal Lifelog and Surrounding Environment Dataset. A Case Study in Ho Chi Minh City, Vietnam; kvasir-Instrument: Diagnostic and Therapeutic Tool Segmentation Dataset in Gastrointestinal Endoscopy; tropical Cyclones Tracking Based on Satellite Cloud Images: Database and Comprehensive Study; catMeows: A Publicly-Available Dataset of Cat Vocalizations; search and Explore Strategies for Interactive Analysis of Real-Life Image Collections with Unknown and Unique Categories; graph-Based Indexing and Retrieval of Lifelog Data; on Fusion of Learned and Designed Features for Video Data Analytics; XQM: Interactive Learning on Mobile Phones; a Multimodal Tensor-Based Late Fusion Approach for Satellite Image Search in Sentinel 2 Images; canopy Height Estimation from Spaceborne Imagery Using Convolutional Encoder-Decoder; implementation of a Random Forest Classifier to Examine Wildfire Predictive Modelling in Greece Using Diachronically Collected Fire Occurrence and Fire Mapping Data; mobile eHealth Platform for Home Monitoring of Bipolar Disorder; multimodal Sensor Data Analysis for Detection of Risk Situations of Fragile People in @home Environments.",,,Conference Review,"Final","",Scopus,2-s2.0-85101650562
"Lu Y., Wang Y., Xin Y., Wu D., Lu G.","57212483262;57221296847;57215128937;57215128978;55872294200;","Unsupervised Gaze: Exploration of Geometric Constraints for 3D Gaze Estimation",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12573 LNCS",,,"121","133",,,"10.1007/978-3-030-67835-7_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101602875&doi=10.1007%2f978-3-030-67835-7_11&partnerID=40&md5=d14dd2caf1b26a176b82c14b1f79c6b3","Intelligent Vision and Sensing Lab, Rochester Institute of Technology, Rochester, United States; Tecent Deep Sea Lab, Shenzhen, China","Lu, Y., Intelligent Vision and Sensing Lab, Rochester Institute of Technology, Rochester, United States; Wang, Y., Intelligent Vision and Sensing Lab, Rochester Institute of Technology, Rochester, United States; Xin, Y., Tecent Deep Sea Lab, Shenzhen, China; Wu, D., Tecent Deep Sea Lab, Shenzhen, China; Lu, G., Intelligent Vision and Sensing Lab, Rochester Institute of Technology, Rochester, United States","Eye gaze estimation can provide critical evidence for people attention, which has extensive applications on cognitive science and computer vision areas, such as human behavior analysis and fake user identification. Existing typical methods mostly place the eye-tracking sensors directly in front of the eyeballs, which is hard to be utilized in the wild. And recent learning-based methods require prior ground truth annotations of gaze vector for training. In this paper, we propose an unsupervised learning-based method for estimating the eye gaze in 3D space. Building on top of the existing unsupervised approach to regress shape parameters and initialize the depth, we propose to apply geometric spectral photometric consistency constraint and spatial consistency constraints across multiple views in video sequences to refine the initial depth values on the detected iris landmark. We demonstrate that our method is able to learn gaze vector in the wild scenes more robust without ground truth gaze annotations or 3D supervision, and show our system leads to a competitive performance compared with existing supervised methods. © 2021, Springer Nature Switzerland AG.","3D gaze estimation; Geometric constraints; Unsupervised learning","Behavioral research; Learning systems; Competitive performance; Consistency constraints; Eye-tracking sensors; Geometric constraint; Human behavior analysis; Learning-based methods; Unsupervised approaches; User identification; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85101602875
"Zhang L., Zhang X., Xu M., Shao L.","35231925400;35232030000;55703591000;55643855000;","Massive-Scale Aerial Photo Categorization by Cross-Resolution Visual Perception Enhancement",2021,"IEEE Transactions on Neural Networks and Learning Systems",,,,"","",,,"10.1109/TNNLS.2021.3055548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100933709&doi=10.1109%2fTNNLS.2021.3055548&partnerID=40&md5=28c2444de3939af39b1af8406754e51c","College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou 325035, China.; College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou 325035, China (e-mail: zhangxiaoqinnan@gmail.com); Center for Interdisciplinary Information Science Research, Zhengzhou University, Zhengzhou 450000, China.; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE.","Zhang, L., College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou 325035, China.; Zhang, X., College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou 325035, China (e-mail: zhangxiaoqinnan@gmail.com); Xu, M., Center for Interdisciplinary Information Science Research, Zhengzhou University, Zhengzhou 450000, China.; Shao, L., Inception Institute of Artificial Intelligence, Abu Dhabi, UAE.","Categorizing aerial photographs with varied weather/lighting conditions and sophisticated geomorphic factors is a key module in autonomous navigation, environmental evaluation, and so on. Previous image recognizers cannot fulfill this task due to three challenges: 1) localizing visually/semantically salient regions within each aerial photograph in a weakly annotated context due to the unaffordable human resources required for pixel-level annotation; 2) aerial photographs are generally with multiple informative attributes (e.g., clarity and reflectivity), and we have to encode them for better aerial photograph modeling; and 3) designing a cross-domain knowledge transferal module to enhance aerial photograph perception since multiresolution aerial photographs are taken asynchronistically and are mutually complementary. To handle the above problems, we propose to optimize aerial photograph's feature learning by leveraging the low-resolution spatial composition to enhance the deep learning of perceptual features with a high resolution. More specifically, we first extract many BING-based object patches (Cheng et al., 2014) from each aerial photograph. A weakly supervised ranking algorithm selects a few semantically salient ones by seamlessly incorporating multiple aerial photograph attributes. Toward an interpretable aerial photograph recognizer indicative to human visual perception, we construct a gaze shifting path (GSP) by linking the top-ranking object patches and, subsequently, derive the deep GSP feature. Finally, a cross-domain multilabel SVM is formulated to categorize each aerial photograph. It leverages the global feature from low-resolution counterparts to optimize the deep GSP feature from a high-resolution aerial photograph. Comparative results on our compiled million-scale aerial photograph set have demonstrated the competitiveness of our approach. Besides, the eye-tracking experiment has shown that our ranking-based GSPs are over 92&#x0025; consistent with the real human gaze shifting sequences. IEEE","Aerial photograph; cross domain; Image recognition; Kernel; machine learning; perception enhancement; ranking; Semantics; Support vector machines; Training; transfer learning; Visual perception; Visualization","Air navigation; Antennas; Deep learning; Eye tracking; Image enhancement; Photographic equipment; Vision; Aerial Photographs; Autonomous navigation; Environmental evaluation; Human visual perception; Informative attributes; Perceptual feature; Spatial composition; Visual perception; Aerial photography",Article,"Article in Press","",Scopus,2-s2.0-85100933709
"Amadori P.V., Fischer T., Demiris Y.","56703112800;57190126084;6506125343;","HammerDrive: A Task-Aware Driving Visual Attention Model",2021,"IEEE Transactions on Intelligent Transportation Systems",,,,"","",,,"10.1109/TITS.2021.3055120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100869852&doi=10.1109%2fTITS.2021.3055120&partnerID=40&md5=265f423a47285f84b96887e03323c2d0","Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K. (e-mail: p.amadori@imperial.ac.uk); Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K..","Amadori, P.V., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K. (e-mail: p.amadori@imperial.ac.uk); Fischer, T., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K..; Demiris, Y., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K..","We introduce HammerDrive, a novel architecture for task-aware visual attention prediction in driving. The proposed architecture is learnable from data and can reliably infer the current focus of attention of the driver in real-time, while only requiring limited and easy-to-access telemetry data from the vehicle. We build the proposed architecture on two core concepts: 1) driving can be modeled as a collection of sub-tasks (maneuvers), and 2) each sub-task affects the way a driver allocates visual attention resources, i.e., their eye gaze fixation. HammerDrive comprises two networks: a hierarchical monitoring network of forward-inverse model pairs for sub-task recognition and an ensemble network of task-dependent convolutional neural network modules for visual attention modeling. We assess the ability of HammerDrive to infer driver visual attention on data we collected from 20 experienced drivers in a virtual reality-based driving simulator experiment. We evaluate the accuracy of our monitoring network for sub-task recognition and show that it is an effective and light-weight network for reliable real-time tracking of driving maneuvers with above 90&#x0025; accuracy. Our results show that HammerDrive outperforms a comparable state-of-the-art deep learning model for visual attention prediction on numerous metrics with ~13&#x0025; improvement for both Kullback-Leibler divergence and similarity, and demonstrate that task-awareness is beneficial for driver visual attention prediction. Crown","Advanced driver-assistance systems; Computational modeling; Computer architecture; HAMMER.; Predictive models; Real-time systems; simulated driving; Task analysis; task recognition; Vehicles; visual attention; Visualization","Convolutional neural networks; Deep learning; Forecasting; Inverse problems; Network architecture; Virtual reality; Driving simulator; Focus of Attention; Kullback Leibler divergence; Monitoring network; Novel architecture; Proposed architectures; Real time tracking; Visual attention model; Behavioral research",Article,"Article in Press","",Scopus,2-s2.0-85100869852
"Li Y., Liu M., Rehg J.","56938415500;57204285999;7004835775;","In the Eye of the Beholder: Gaze and Actions in First Person Video",2021,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,,,"","",,,"10.1109/TPAMI.2021.3051319","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099727750&doi=10.1109%2fTPAMI.2021.3051319&partnerID=40&md5=50ae5b38139dad1037839cb4f826966b","Biostatistics / Computer Sciences, University of Wisconsin-Madison, 5228 Madison, Wisconsin, United States, 53706 (e-mail: yin.li@wisc.edu); IRIM, Georgia Institute of Technology, 1372 Atlanta, Georgia, United States, 30332-0002 (e-mail: mliu328@gatech.edu); School of Interactive Computing, Georgia Institute of Technology, 1372 Atlanta, Georgia, United States, (e-mail: rehg@gatech.edu)","Li, Y., Biostatistics / Computer Sciences, University of Wisconsin-Madison, 5228 Madison, Wisconsin, United States, 53706 (e-mail: yin.li@wisc.edu); Liu, M., IRIM, Georgia Institute of Technology, 1372 Atlanta, Georgia, United States, 30332-0002 (e-mail: mliu328@gatech.edu); Rehg, J., School of Interactive Computing, Georgia Institute of Technology, 1372 Atlanta, Georgia, United States, (e-mail: rehg@gatech.edu)","We address the task of jointly determining what a person is doing and where they are looking based on the analysis of video captured by a headworn camera. To facilitate our research, we first introduce the EGTEA Gaze+ dataset. Our dataset comes with videos, gaze tracking data, hand masks and action annotations, thereby providing the most comprehensive benchmark for First Person Vision (FPV). Moving beyond the dataset, we propose a novel deep model for joint gaze estimation and action recognition in FPV. Our method describes the participant&#x0027;s gaze as a probabilistic variable and models its distribution using stochastic units in a deep network. We further sample from these stochastic units, generating an attention map to guide the aggregation of visual features for action recognition. Our method is evaluated on our EGTEA Gaze+ dataset and achieves a performance level that exceeds the state-of-the-art by a significant margin. More importantly, we demonstrate that our model can be applied to larger scale FPV dataset---EPIC-Kitchens even without using gaze, offering new state-of-the-art results on FPV action recognition. IEEE","Action recognition; Benchmark testing; Cameras; Convolution; deep probabilistic models; first person vision; gaze estimation; Gaze tracking; Stochastic processes; Three-dimensional displays; video analysis; Visualization","Probability distributions; Stochastic models; Stochastic systems; Action recognition; First-person visions; Gaze estimation; Gaze tracking; Headworn cameras; Performance level; State of the art; Visual feature; Eye tracking",Article,"Article in Press","",Scopus,2-s2.0-85099727750
"Wagner B., Taffner F., Karaca S., Karge L.","57221606594;55962543000;57221614890;57221605109;","Vision Based Detection of Driver Cell Phone Usage and Food Consumption",2021,"IEEE Transactions on Intelligent Transportation Systems",,,,"","",,,"10.1109/TITS.2020.3043145","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099548028&doi=10.1109%2fTITS.2020.3043145&partnerID=40&md5=b59cd52dd1c07209c127d5c8f72bc138","ARRK Engineering, 80807 Munich, Germany (e-mail: benjamin.wagner@arrk-engineering.com); ARRK Engineering, 80807 Munich, Germany.","Wagner, B., ARRK Engineering, 80807 Munich, Germany (e-mail: benjamin.wagner@arrk-engineering.com); Taffner, F., ARRK Engineering, 80807 Munich, Germany.; Karaca, S., ARRK Engineering, 80807 Munich, Germany.; Karge, L., ARRK Engineering, 80807 Munich, Germany.","Distracted driving is a problem which yearly causes a large amount of road traffic crashes with high rates of fatalities and injured persons. Recently, car manufacturers started to integrate driver monitoring systems to detect visual distraction. This paper proposes a method to extend such systems by driver posture classification to detect driver cell phone usage and food consumption. Such an extension can be beneficial since systems that focus on the detection of visual distraction mainly rely on head pose and gaze information. Thus, distraction caused by cell phone usage or food consumption can not be detected by these systems when the driver is looking to the road ahead. To robustly detect those types of manual and cognitive distraction, different deep learning models were trained and evaluated based on a new image dataset which was captured by two infrared cameras to ensure that a large range of head angles can be covered by the system. Separate Convolutional Neural Networks (CNNs) were trained and evaluated for the dataset of the left and the right camera to optimize the classification accuracy. The trained CNNs revealed a competitive test accuracy of 92.88&#x0025; and 90.36&#x0025; for the left and the right camera, respectively. In inference mode, the models achieve a frame rate of 44Hz and 28Hz for the left and the right camera, respectively. The combination of the classification output of both networks revealed a test accuracy of 92.54&#x0025;. IEEE","deep learning; Driver monitoring; driver posture classification.","Automobile manufacture; Cameras; Cellular telephones; Convolutional neural networks; Deep learning; Food supply; Large dataset; Roads and streets; Cell phone usages; Classification accuracy; Cognitive distractions; Driver monitoring system; Posture classification; Road traffic crashes; Vision-based detection; Visual distractions; Classification (of information)",Article,"Article in Press","",Scopus,2-s2.0-85099548028
[无可用作者姓名],[无可用的作者 ID],"2nd International Conference on Trends in Computational and Cognitive Engineering, TCCE 2020",2021,"Advances in Intelligent Systems and Computing","1309",,,"","",720,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098254375&partnerID=40&md5=ffa074da92628e9d8ae1af45822b1f38",,"","The proceedings contain 58 papers. The special focus in this conference is on Trends in Computational and Cognitive Engineering. The topics include: A machine learning approach to predict events by analyzing bengali facebook posts; gaze movement’s entropy analysis to detect workload levels; a comparative study among segmentation techniques for skin disease detection systems; thermomechanism: Snake pit membrane; sentiment analysis on bangla text using long short-term memory (lstm) recurrent neural network; comparative analysis of different classifiers on eeg signals for predicting epileptic seizure; anomaly detection in electroencephalography signal using deep learning model; an effective leukemia prediction technique using supervised machine learning classification algorithm; quantitative analysis of deep cnns for multilingual handwritten digit recognition; deep cnn-supported ensemble cadx architecture to diagnose malaria by medical image; building a non-ionic, non-electronic, non-algorithmic artificial brain: Cortex and connectome interaction in a humanoid bot subject (hbs); detection of ovarian malignancy from combination of ca125 in blood and tvus using machine learning; auditory attention state decoding for the quiet and hypothetical environment: A comparison between blstm and svm; em signal processing in bio-living system; 6g access network for intelligent internet of healthcare things: Opportunity, challenges, and research directions; towards a blockchain-based supply chain management for e-agro business system; normalized approach to find optimal number of topics in latent dirichlet allocation (lda); towards developing a real-time hand gesture controlled wheelchair; a novel deep learning approach to predict air quality index; performance analysis of machine learning approaches in software complexity prediction; virtual heritage of the saith gumbad mosque, bangladesh.",,,Conference Review,"Final","",Scopus,2-s2.0-85098254375
"Araluce J., Bergasa L.M., Gómez-Huélamo C., Barea R., López-Guillén E., Arango F., Pérez-Gil Ó.","57204909678;6602297767;57204909395;56802238100;57204784175;57215127305;57215137907;","Integrating OpenFace 2.0 Toolkit for Driver Attention Estimation in Challenging Accidental Scenarios",2021,"Advances in Intelligent Systems and Computing","1285",,,"274","288",,1,"10.1007/978-3-030-62579-5_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097397904&doi=10.1007%2f978-3-030-62579-5_19&partnerID=40&md5=fd78088009e440249708b8ac4b41bf43","Electronics Department, University of Alcalá (UAH), Alcalá de Henares, Spain","Araluce, J., Electronics Department, University of Alcalá (UAH), Alcalá de Henares, Spain; Bergasa, L.M., Electronics Department, University of Alcalá (UAH), Alcalá de Henares, Spain; Gómez-Huélamo, C., Electronics Department, University of Alcalá (UAH), Alcalá de Henares, Spain; Barea, R., Electronics Department, University of Alcalá (UAH), Alcalá de Henares, Spain; López-Guillén, E., Electronics Department, University of Alcalá (UAH), Alcalá de Henares, Spain; Arango, F., Electronics Department, University of Alcalá (UAH), Alcalá de Henares, Spain; Pérez-Gil, Ó., Electronics Department, University of Alcalá (UAH), Alcalá de Henares, Spain","Gaze estimation is a typical approach to monitor the driver attention on the road scene. This indicator is of great importance in safe driving and in the design of the takeover control strategy for a Level 3 and Level 4 automation system. Nowadays, most of eye gaze tracking techniques are intrusive and costly, which limits their applicability over real vehicles. On the other hand, current databases used for gaze validation face the driver attention task focused on critical situations in simulation but they do not encounter actual accidents. This paper presents a low-cost and non-intrusive camera-based gaze mapping system integrating the open-source state-of-the art OpenFace 2.0 Toolkit[3] to visualise the driver attention simulation on prerecorded real traffic scenes through a heat map. The proposal has been validated by using the recent and challenging public dataset DADA2000[9] which has 2000 video sequences with annotated driving scenarios based on real accidents. We compare our system with an expensive desktop-mounted eye-tracker, obtaining on par results and showing it is a good tool for driver attention monitoring able to be used in the design of take over systems and driving scenarios awareness systems for automated vehicles. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.","Accidental scenarios; Computer vision; Driver attention; Gaze estimation; Heat map","Accidents; Automation; Eye tracking; Open systems; Automated vehicles; Automation systems; Awareness systems; Control strategies; Driver attention; Eye gaze tracking; State of the art; Video sequences; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85097397904
[无可用作者姓名],[无可用的作者 ID],"21st International Workshop of Physical Agents, WAF 2020",2021,"Advances in Intelligent Systems and Computing","1285",,,"","",360,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097385190&partnerID=40&md5=af2ae7b56ebe036715ad4f9a321e199b",,"","The proceedings contain 24 papers. The special focus in this conference is on Physical Agents. The topics include: Defining Adaptive Proxemic Zones for Activity-Aware Navigation; can a Social Robot Learn to Gesticulate Just by Observing Humans?; Evaluation of a Multi-speaker System for Socially Assistive HRI in Real Scenarios; graph Neural Networks for Human-Aware Social Navigation; a Toolkit to Generate Social Navigation Datasets; JBCA: Designing an Adaptative Continuous Authentication Architecture; optimization of a Robotics Gaze Control System; Open-Loop Sidescan Sonar Mosaic and ANN Velocity Estimation; 360$$^{\circ }$$ Real-Time 3D Multi-object Detection and Tracking for Autonomous Vehicle Navigation; Towards Fine-Tuning of VQA Models in Public Datasets; integrating OpenFace 2.0 Toolkit for Driver Attention Estimation in Challenging Accidental Scenarios; lifelong Object Localization in Robotic Applications; embedded Deep Learning Solution for Person Identification and Following with a Robot; survival Loss: A Neuron Death Regularizer; reinforcement Learning Experiments and Benchmark for Solving Robotic Reaching Tasks; age and Gender Recognition from Speech Using Deep Neural Networks; monocular 3D Hand Pose Estimation for Teleoperating Low-Cost Actuators; estimation of Customer Activity Patterns in Open Malls by Means of Combining Localization and Process Mining Techniques; Train Here, Drive There: Simulating Real-World Use Cases with Fully-Autonomous Driving Architecture in CARLA Simulator; DQN-Based Deep Reinforcement Learning for Autonomous Driving; concept Drift Detection and Adaptation for Robotics and Mobile Devices in Federated and Continual Settings; qoS Metrics-in-the-Loop for Better Robot Navigation.",,,Conference Review,"Final","",Scopus,2-s2.0-85097385190
"Naqvi R.A., Hussain D., Loh W.-K.","55975847900;57196185189;7102037423;","Artificial intelligence-based semantic segmentation of ocular regions for biometrics and healthcare applications",2021,"Computers, Materials and Continua","66","1",,"715","732",,4,"10.32604/cmc.2020.013249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096491110&doi=10.32604%2fcmc.2020.013249&partnerID=40&md5=db5d16f300603be75615fef21ab7d8e3","Department of Unmanned Vehicle Engineering, Sejong University, Seoul, 05006, South Korea; School of Computational Science, Korea Institute for Advanced Study (KIAS), Seoul, 02455, South Korea; Department of Software, Gachon University, Seongnam, 13120, South Korea","Naqvi, R.A., Department of Unmanned Vehicle Engineering, Sejong University, Seoul, 05006, South Korea; Hussain, D., School of Computational Science, Korea Institute for Advanced Study (KIAS), Seoul, 02455, South Korea; Loh, W.-K., Department of Software, Gachon University, Seongnam, 13120, South Korea","Multiple ocular region segmentation plays an important role in different applications such as biometrics, liveness detection, healthcare, and gaze estimation. Typically, segmentation techniques focus on a single region of the eye at a time. Despite the number of obvious advantages, very limited research has focused on multiple regions of the eye. Similarly, accurate segmentation of multiple eye regions is necessary in challenging scenarios involving blur, ghost effects low resolution, off-angles, and unusual glints. Currently, the available segmentation methods cannot address these constraints. In this paper, to address the accurate segmentation of multiple eye regions in unconstrainted scenarios, a lightweight outer residual encoder-decoder network suitable for various sensor images is proposed. The proposed method can determine the true boundaries of the eye regions from inferior-quality images using the high-frequency information flow from the outer residual encoder-decoder deep convolutional neural network (called ORED-Net). Moreover, the proposed ORED-Net model does not improve the performance based on the complexity, number of parameters or network depth. The proposed network is considerably lighter than previous state-of-theart models. Comprehensive experiments were performed, and optimal performance was achieved using SBVPI and UBIRIS.v2 datasets containing images of the eye region. The simulation results obtained using the proposed OREDNet, with the mean intersection over union score (mIoU) of 89.25 and 85.12 on the challenging SBVPI and UBIRIS.v2 datasets, respectively. © 2020 Tech Science Press. All rights reserved.","Biometric for healthcare; Deep learning; Ocular regions; Semantic segmentation; Sensors","Biometrics; Convolutional neural networks; Decoding; Deep neural networks; Health care; Semantics; Signal encoding; Health care application; High-frequency informations; Liveness detection; Optimal performance; Region segmentation; Segmentation methods; Segmentation techniques; Semantic segmentation; Image segmentation",Article,"Final","",Scopus,2-s2.0-85096491110
"Wang Z., Liu J., He K., Xu Q., Xu X., Liu H.","57205198429;57211670942;55309024200;55320764800;56566828900;57218402201;","Screening Early Children with Autism Spectrum Disorder via Response-to-Name Protocol",2021,"IEEE Transactions on Industrial Informatics","17","1","8928567","587","595",,3,"10.1109/TII.2019.2958106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091717659&doi=10.1109%2fTII.2019.2958106&partnerID=40&md5=a928e16ba46c63407eefdabae95be346","State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Child Health Care, Children's Hospital of Fudan University, Shanghai, China; School of Computing, University of Portsmouth, Portsmouth, PO1 3HE, United Kingdom","Wang, Z., State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Liu, J., State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; He, K., State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Xu, Q., Department of Child Health Care, Children's Hospital of Fudan University, Shanghai, China; Xu, X., Department of Child Health Care, Children's Hospital of Fudan University, Shanghai, China; Liu, H., State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China, School of Computing, University of Portsmouth, Portsmouth, PO1 3HE, United Kingdom","Incidence of children with autism spectrum disorder (ASD) has increased with an average rate of 1% worldwide. Clinical ASD screening, especially for children screening is a laborious and skilled task; however, there is no objective and effective method automating ASD children screening. Analyzing children ASD characteristics in predefined motion behavior protocols is attempted to provide automatic solutions to children ASD screening. A novel protocol, response to name (RTN), is proposed in this article for ASD clinical validation and diagnosis. The RTN method is jointly designed with clinical partners, and novel gaze estimation is developed for validating ASD characteristic behavior. Seventeen subjects including ten adults and seven children (five ASD subjects and two healthy subjects) have participated the experiment. The experiment results show that the proposed RTN system achieves an average classification score of 92.7% fully demonstrating that the principle of motion protocol based ASD screening has the potential to have early ASD screening automated. © 2005-2012 IEEE.","Autism spectrum disorder (ASD); early screening; gaze estimation; multisensor fusion; response to name (RTN)","Electrical engineering; Industry; Average rate; Children with autisms; Clinical validations; Gaze estimation; Healthy subjects; Motion behavior; Diseases",Article,"Final","",Scopus,2-s2.0-85091717659
"Lai Q., Khan S., Nie Y., Sun H., Shen J., Shao L.","57200229356;56415451600;25624852500;57203293326;13605783600;55643855000;","Understanding More about Human and Machine Attention in Deep Neural Networks",2021,"IEEE Transactions on Multimedia","23",,"9133499","2086","2099",,1,"10.1109/TMM.2020.3007321","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090962478&doi=10.1109%2fTMM.2020.3007321&partnerID=40&md5=c67fd680519244530e562c418ed4f8d2","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, 999077, Hong Kong; Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates; School of Computer Science and Engineering, South China University of Technology, Guangzhou, 510006, China; University of Electronic Science and Technology of China, Chengdu, 610051, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates","Lai, Q., Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, 999077, Hong Kong; Khan, S., Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Nie, Y., School of Computer Science and Engineering, South China University of Technology, Guangzhou, 510006, China; Sun, H., University of Electronic Science and Technology of China, Chengdu, 610051, China; Shen, J., Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Shao, L., Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates","Human visual system can selectively attend to parts of a scene for quick perception, a biological mechanism known as Human attention. Inspired by this, recent deep learning models encode attention mechanisms to focus on the most task-relevant parts of the input signal for further processing, which is called Machine/Neural/Artificial attention. Understanding the relation between human and machine attention is important for interpreting and designing neural networks. Many works claim that the attention mechanism offers an extra dimension of interpretability by explaining where the neural networks look. However, recent studies demonstrate that artificial attention maps do not always coincide with common intuition. In view of these conflicting evidence, here we make a systematic study on using artificial attention and human attention in neural network design. With three example computer vision tasks (i.e., salient object segmentation, video action recognition, and fine-grained image classification), diverse representative backbones (i.e., AlexNet, VGGNet, ResNet) and famous architectures (i.e., Two-stream, FCN), corresponding real human gaze data, and systematically conducted large-scale quantitative studies, we quantify the consistency between artificial attention and human visual attention and offer novel insights into existing artificial attention mechanisms by giving preliminary answers to several key questions related to human and artificial attention mechanisms. Overall results demonstrate that human attention can benchmark the meaningful 'ground-truth' in attention-driven tasks, where the more the artificial attention is close to human attention, the better the performance; for higher-level vision tasks, it is case-by-case. It would be advisable for attention-driven tasks to explicitly force a better alignment between artificial and human attention to boost the performance; such alignment would also improve the network explainability for higher-level computer vision tasks. © 1999-2012 IEEE.","artificial attention; Attention mechanism; deep learning; human attention","Alignment; Computer hardware description languages; Computer networks; Computer vision; Data streams; Deep neural networks; Image segmentation; Large scale systems; Action recognition; Attention mechanisms; Conflicting evidence; Extra dimensions; Network backbones; Neural network designs; Quantitative study; Selective attention; Neural networks",Article,"Final","",Scopus,2-s2.0-85090962478
"Modi N., Singh J.","57212387785;26422494700;","A review of various state of art eye gaze estimation techniques",2021,"Advances in Intelligent Systems and Computing","1086",,,"501","510",,,"10.1007/978-981-15-1275-9_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087532490&doi=10.1007%2f978-981-15-1275-9_41&partnerID=40&md5=ce3c8ffb21753a4b9e6139046ca74045","Department of Computer Science and Engineering, Chitkara University Institute of Engineering and Technology, Chitkara University, Patiala, Punjab, India; Department of Computer Applications, Chitkara University Institute of Engineering and Technology, Chitkara University, Patiala, Punjab, India","Modi, N., Department of Computer Science and Engineering, Chitkara University Institute of Engineering and Technology, Chitkara University, Patiala, Punjab, India; Singh, J., Department of Computer Applications, Chitkara University Institute of Engineering and Technology, Chitkara University, Patiala, Punjab, India","Eye gaze tracking is a technique to track an individual’s focus of attention. This paper provides a review on eye gaze trackers (EGTs) classified on the basis of intrusive and non-intrusive techniques. According to the numerous applications of EGTs in human–computer interface, neuroscience, psychology and in advertising and marketing, this paper brings forward a deep insight into recent and future advancements in the field of eye gaze tracking. Finally, comparative analyses on various EGT techniques along with its applications in various fields are discussed. © Springer Nature Singapore Pte Ltd. 2021.","Computer vision; Eye gaze; Eye gaze trackers (EGTs); Intrusive techniques; Video oculography","Artificial intelligence; Marketing; Comparative analysis; Eye gaze trackers; Eye gaze tracking; Eye-gaze; Focus of Attention; ITS applications; Non-intrusive techniques; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85087532490
"Fang W., Zhang K.","55435889400;57226713170;","Real-time object detection of retail products for eye tracking",2020,"2020 8th International Conference on Orange Technology, ICOT 2020",,,"9468806","","",,,"10.1109/ICOT51877.2020.9468806","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112435021&doi=10.1109%2fICOT51877.2020.9468806&partnerID=40&md5=7e73ede7f03d41d6713bf3c0a2b059e9","Suzhou Institute of Trade and Commerce, Information Technology Department, Suzhou, China","Fang, W., Suzhou Institute of Trade and Commerce, Information Technology Department, Suzhou, China; Zhang, K., Suzhou Institute of Trade and Commerce, Information Technology Department, Suzhou, China","Object detection is one important task in automatically analyzing eye tracking video data. This paper presents a real-time object detection method of retail products based on deep learning for eye tracking system. In the proposed approach, an eye tracking based Convolutional Neural Networks is constructed to obtain the feature of original images. Then, a weighted bounding box selection strategy based on gaze location is used for object detection. Besides, parameters are adjusted according to gaze location of eye tracking. Experimental results show that our method can achieve better accuracy for eye tracking than other existing methods in the detection of retail products. © 2020 IEEE.","Convolutional Neural Networks; Eye Tracking; Gaze Location; Object Detection","Citrus fruits; Convolutional neural networks; Deep learning; Object detection; Object recognition; Object tracking; Sales; Bounding box; Eye tracking systems; Object detection method; Original images; Real time; Video data; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85112435021
"Prava Roy A., Kumar Koley S., Garain U.","57208867238;57222121533;6602234041;","Eyes speak out Mind: Deep models for Gaze-based Analysis of Bilingual and Monolingual Reading",2020,"2020 IEEE 17th India Council International Conference, INDICON 2020",,,"9342182","","",,,"10.1109/INDICON49873.2020.9342182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101518207&doi=10.1109%2fINDICON49873.2020.9342182&partnerID=40&md5=f8ef81c87410b13ffc01f671c6bed412","National Inst. of Technology, Dept. of Ece, Durgapur, India; Indian Statistical Institute, Comp. Vis. Patt. Recog. (CVPR) Unit, Kolkata, India; Indian Statistical Institute, Cvpr Unit and Centre for Aiml (CAIML), Kolkata, India","Prava Roy, A., National Inst. of Technology, Dept. of Ece, Durgapur, India; Kumar Koley, S., Indian Statistical Institute, Comp. Vis. Patt. Recog. (CVPR) Unit, Kolkata, India; Garain, U., Indian Statistical Institute, Cvpr Unit and Centre for Aiml (CAIML), Kolkata, India","This paper presents an approach that attempts to explore effects of bilingualism in reading by analysis of eye-gaze data. As readers are more comfortable in reading first language (L1) compared to second language (L2), their eyes move differently in these two cases. Being motivated by this hypothesis, a deep learning model is developed to predict whether a reader is reading first or second language by analyzing the reading behavior given by the eye-tracking data. Exposure to different languages simultaneously may influence the proficiency in native language reading and this matter is also investigated by using deep learning model which can differentiate monolingual readers from bilingual readers analyzing their eye movement during reading in their respective native language. Experiments are conducted on GECO Corpus [1] which provides eye-tracking data of 19 readers reading a novel in L1 and L2 along with eye-tracking data of several monolingual readers reading same text in their native language. Experimental results show that the proposed models are quite efficient in capturing the differences in eye gaze pattern for reading L1 and L2 for a reader as well as in identifying monolingual and bilingual readers by processing eye-gaze data. © 2020 IEEE.","Bilingual; Deep Learning; Eye-tracking Data; Feature attribution; First Language; Monolingual; Reading Behavior; Second Language","Data handling; Deep learning; Eye movements; Learning systems; Eye-gaze; Learning models; Native language; Second language; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85101518207
"Melesse D., Khalil M., Kagabo E., Ning T., Huang K.","57207925565;57207939901;57221800260;7101956756;41761666000;","Appearance-Based Gaze Tracking through Supervised Machine Learning",2020,"International Conference on Signal Processing Proceedings, ICSP","2020-December",,"9321075","467","471",,,"10.1109/ICSP48669.2020.9321075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100262989&doi=10.1109%2fICSP48669.2020.9321075&partnerID=40&md5=afb7af6471358038badbf941ae4e9a41","Trinity College, Department of Engineering, Hartford, CT, United States","Melesse, D., Trinity College, Department of Engineering, Hartford, CT, United States; Khalil, M., Trinity College, Department of Engineering, Hartford, CT, United States; Kagabo, E., Trinity College, Department of Engineering, Hartford, CT, United States; Ning, T., Trinity College, Department of Engineering, Hartford, CT, United States; Huang, K., Trinity College, Department of Engineering, Hartford, CT, United States","Applications that use human gaze have become increasingly more popular in the domain of human-computer interfaces, and advances in eye gaze tracking technology over the past few decades have led to the development of promising gaze estimation techniques. In this paper, a low-cost, in-house video camera-based gaze tracking system was developed, trained and evaluated. Seminal gaze detection methods constrained the application space to indoor conditions, and in most cases techniques required intrusive hardware. More modern gaze detection techniques try to eliminate the use of any additional hardware to reduce monetary cost as well as undue burden to the user, all the while maintaining accuracy of detection. In this work, image acquisition was achieved using a low-cost USB web camera mounted at a fixed position on the viewing screen or laptop. In order to determine the point of gaze, the Viola Jones face detection algorithm is used to extract facial features from the image frame. The gaze is then calculated using image processing techniques to extract gaze features, namely related to the image position of the pupil. Thousands of images are classified and labeled to form an in-house database. A multi-class Support Vector Machine (SVM) was trained and tested on this data set to distinguish point of gaze from input face image. Cross validation was used to train the model. Confusion matrices, accuracy, precision, and recall are used to evaluate the performance of the classification model. Evaluation of the proposed appearance-based technique using two different kernel functions is also assessed in detail. 2020 IEEE.","automatic gaze tracking; face detection; human computer interface; support vector machine","Computer hardware; Costs; Face recognition; Support vector machines; Video cameras; Classification models; Confusion matrices; Face detection algorithm; Gaze tracking system; Human computer interfaces; Image processing technique; Multi-class support vector machines; Supervised machine learning; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85100262989
[无可用作者姓名],[无可用的作者 ID],"BDSIC 2020 - 2020 2nd International Conference on Big-Data Service and Intelligent Computation",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"","",75,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117540488&partnerID=40&md5=529d13400207d8861fa9a19ed35c6ef2",,"","The proceedings contain 11 papers. The topics discussed include: LSB substitution image steganography based on randomized pixel selection and one-time pad encryption; Alzheimer’s disease classification using structural MRI based on convolutional neural networks; ultra-fast mini license plate recognition system based-on vision processing unit; driver’s gaze zone estimation method: a four-channel convolutional neural network model; driver hand detection using squeeze-and-excitation YOLOv4 network; analysis the influence of Guilin tourism economic income on the per capita disposable income of urban residents based on GIS technology; feedback knowledge graph for recommendation; research on the relationship between precision marketing and company development ability; stock volume prediction based on polarity of tweets, news, and historical data using deep learning; and an evolutionary game analysis on patients’ value creation behavior in online healthcare community.",,,Conference Review,"Final","",Scopus,2-s2.0-85117540488
"Seneviratne I.K., Perera B.A.S.D., Fernando R.S.D., Siriwardana L.K.B., Rajapaksha U.U.S.K.","57221960473;57221962584;57221966495;57221950231;57204419051;","Student and lecturer performance enhancement system using artificial intelligence",2020,"Proceedings of the 3rd International Conference on Intelligent Sustainable Systems, ICISS 2020",,,"9315981","88","93",,,"10.1109/ICISS49785.2020.9315981","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100813463&doi=10.1109%2fICISS49785.2020.9315981&partnerID=40&md5=7395b6ee676fde70a266862040ed545e","Sri Lanka Institute of Information Technology, Dept of Software Engineering, Malabe, Sri Lanka","Seneviratne, I.K., Sri Lanka Institute of Information Technology, Dept of Software Engineering, Malabe, Sri Lanka; Perera, B.A.S.D., Sri Lanka Institute of Information Technology, Dept of Software Engineering, Malabe, Sri Lanka; Fernando, R.S.D., Sri Lanka Institute of Information Technology, Dept of Software Engineering, Malabe, Sri Lanka; Siriwardana, L.K.B., Sri Lanka Institute of Information Technology, Dept of Software Engineering, Malabe, Sri Lanka; Rajapaksha, U.U.S.K., Sri Lanka Institute of Information Technology, Dept of Software Engineering, Malabe, Sri Lanka","The proposed research work develops a system to enhance the performance of university students and lecturers by providing an excellent statistical insight. Already existing research works have attempted to solve independent classroom challenges that are related to measuring the student attention and marking student attendance but the existing research works have not combined theimportant aspects into one system. Hence, the proposed research wor has been carried out on various main aspects such as attendance register, monitoring student behavior as well as lecturer performance and lecture summarization. The system will incorporate tools and technologies in the different domains of artificial intelligence, machine learning, and natural language processing. After implementing and testing the proposed method it has been concluded that the student activity recognition process has been performed much better than the other emotion and gaze components by providing 94.5% results. The proposed system can determine the lecturer's physical activities and the quality of the lecture content with a reasonable accuracy. The summarized lecture has showed 70% similarity to actual lecture content and student attendance by using Face Recognition was marked with 83% accuracy. This research concludes that the automation of major classroom activities will impact the students and lecturers positively. Also, this system yields valuable results and increases the productivity of higher education institutions in the future. © 2020 IEEE.","Artificial intelligence; Computer vision; Deep learning; Natural language processing","Artificial intelligence; Face recognition; Natural language processing systems; Activity recognition; Higher education institutions; NAtural language processing; Performance enhancements; Reasonable accuracy; Student attendances; Tools and technologies; University students; Students",Conference Paper,"Final","",Scopus,2-s2.0-85100813463
"Zhang Y., Yang X., Ma Z.","57219185145;55683790100;57219766050;","Driver's Gaze Zone Estimation Method: A Four-channel Convolutional Neural Network Model",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"20","24",,,"10.1145/3440054.3440058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100661674&doi=10.1145%2f3440054.3440058&partnerID=40&md5=ce2974e73c0d52d726a184348e421734","The University of Jinan, China; The University of Jinan","Zhang, Y., The University of Jinan, China; Yang, X., The University of Jinan, China; Ma, Z., The University of Jinan","Driver's gaze has become an important indicator to analysis driving state. By estimating the gaze zone of drivers, we can further judge their fatigue state and even predict their driving intention in the next step. In this paper, we propose a four-channel gaze estimation model based on Convolutional Neural Network (CNN), which is used to estimate the gaze zones of the driver. In the proposed method, the images of the right eye, the left eye, the face, and the head are used as the input data of the multi-channel CNN. Then, the features of different channels are fused to estimate the gaze zone. Finally, we compared our method with several existing methods, and the experimental results show that the accuracy of our method is 96%. © 2020 ACM.","convolution neural network; deep learning; driver gaze zone estimation; eye tracking; Gaze estimation","Big data; Convolution; Intelligent computing; Driving state; Estimation methods; Four-channel; Gaze estimation; Input datas; Multi channel; Convolutional neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85100661674
"Luo K., Jia X., Xiao H., Liu D., Peng L., Qiu J., Han P.","35109480500;57221052559;15754768500;55508444800;57200375469;38661707600;35336999400;","A new gaze estimation method based on homography transformation derived from geometric relationship",2020,"Applied Sciences (Switzerland)","10","24","9079","1","18",,,"10.3390/app10249079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098073140&doi=10.3390%2fapp10249079&partnerID=40&md5=158aed67e4ed5bf6e5a6c37c7916078b","Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Physics and Telecommunication Engineering, South China Normal University, Guangzhou, 510006, China; Guangdong-Hong Kong Joint Laboratory of Quantum Matter, South China Normal University, Guangzhou, 510006, China; Guangdong Provincial Engineering Research Center for Optoelectronic Instrument, South China Normal University, Guangzhou, 510006, China; SCNU Qingyuan Institute of Science and Technology Innovation Co., Ltd., Qingyuan, 511517, China","Luo, K., Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Physics and Telecommunication Engineering, South China Normal University, Guangzhou, 510006, China, Guangdong-Hong Kong Joint Laboratory of Quantum Matter, South China Normal University, Guangzhou, 510006, China, Guangdong Provincial Engineering Research Center for Optoelectronic Instrument, South China Normal University, Guangzhou, 510006, China, SCNU Qingyuan Institute of Science and Technology Innovation Co., Ltd., Qingyuan, 511517, China; Jia, X., Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Physics and Telecommunication Engineering, South China Normal University, Guangzhou, 510006, China, Guangdong-Hong Kong Joint Laboratory of Quantum Matter, South China Normal University, Guangzhou, 510006, China, Guangdong Provincial Engineering Research Center for Optoelectronic Instrument, South China Normal University, Guangzhou, 510006, China, SCNU Qingyuan Institute of Science and Technology Innovation Co., Ltd., Qingyuan, 511517, China; Xiao, H., Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Physics and Telecommunication Engineering, South China Normal University, Guangzhou, 510006, China; Liu, D., Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Physics and Telecommunication Engineering, South China Normal University, Guangzhou, 510006, China, Guangdong-Hong Kong Joint Laboratory of Quantum Matter, South China Normal University, Guangzhou, 510006, China, Guangdong Provincial Engineering Research Center for Optoelectronic Instrument, South China Normal University, Guangzhou, 510006, China, SCNU Qingyuan Institute of Science and Technology Innovation Co., Ltd., Qingyuan, 511517, China; Peng, L., Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Physics and Telecommunication Engineering, South China Normal University, Guangzhou, 510006, China, Guangdong-Hong Kong Joint Laboratory of Quantum Matter, South China Normal University, Guangzhou, 510006, China, Guangdong Provincial Engineering Research Center for Optoelectronic Instrument, South China Normal University, Guangzhou, 510006, China, SCNU Qingyuan Institute of Science and Technology Innovation Co., Ltd., Qingyuan, 511517, China; Qiu, J., Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Physics and Telecommunication Engineering, South China Normal University, Guangzhou, 510006, China, Guangdong-Hong Kong Joint Laboratory of Quantum Matter, South China Normal University, Guangzhou, 510006, China, Guangdong Provincial Engineering Research Center for Optoelectronic Instrument, South China Normal University, Guangzhou, 510006, China, SCNU Qingyuan Institute of Science and Technology Innovation Co., Ltd., Qingyuan, 511517, China; Han, P., Guangdong Provincial Key Laboratory of Quantum Engineering and Quantum Materials, School of Physics and Telecommunication Engineering, South China Normal University, Guangzhou, 510006, China, Guangdong-Hong Kong Joint Laboratory of Quantum Matter, South China Normal University, Guangzhou, 510006, China, Guangdong Provincial Engineering Research Center for Optoelectronic Instrument, South China Normal University, Guangzhou, 510006, China, SCNU Qingyuan Institute of Science and Technology Innovation Co., Ltd., Qingyuan, 511517, China","In recent years, the gaze estimation system, as a new type of human-computer interaction technology, has received extensive attention. The gaze estimation model is one of the main research contents of the system. The quality of the model will directly affect the accuracy of the entire gaze estimation system. To achieve higher accuracy even with simple devices, this paper proposes an improved mapping equation model based on homography transformation. In the process of experiment, the model mainly uses the “Zhang Zhengyou calibration method” to obtain the internal and external parameters of the camera to correct the distortion of the camera, and uses the LM(Levenberg-Marquardt) algorithm to solve the unknown parameters contained in the mapping equation. After all the parameters of the equation are determined, the gaze point is calculated. Different comparative experiments are designed to verify the experimental accuracy and fitting effect of this mapping equation. The results show that the method can achieve high experimental accuracy, and the basic accuracy is kept within 0.6◦ . The overall trend shows that the mapping method based on homography transformation has higher experimental accuracy, better fitting effect and stronger stability. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Gaze estimation; Homography transformation; Mapping equation",,Article,"Final","",Scopus,2-s2.0-85098073140
"Khairdoost N., Shirpour M., Bauer M.A., Beauchemin S.S.","36967924700;56403167000;7401779344;6701791427;","Real-Time Driver Maneuver Prediction Using LSTM",2020,"IEEE Transactions on Intelligent Vehicles","5","4","9121758","714","724",,5,"10.1109/TIV.2020.3003889","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087483768&doi=10.1109%2fTIV.2020.3003889&partnerID=40&md5=9caea192771eff7a741a9a573611b5cb","Department of Computer Science, Western University, London, ON  N6A 5B7, Canada","Khairdoost, N., Department of Computer Science, Western University, London, ON  N6A 5B7, Canada; Shirpour, M., Department of Computer Science, Western University, London, ON  N6A 5B7, Canada; Bauer, M.A., Department of Computer Science, Western University, London, ON  N6A 5B7, Canada; Beauchemin, S.S., Department of Computer Science, Western University, London, ON  N6A 5B7, Canada","Driver maneuver prediction is of great importance in designing a modern Advanced Driver Assistance System (ADAS). Such predictions can improve driving safety by alerting the driver to the danger of unsafe or risky traffic situations. In this research, we developed a model to predict driver maneuvers, including left/right lane changes, left/right turns and driving straight forward 3.6 seconds on average before they occur in real time. For this, we propose a deep learning method based on Long Short-Term Memory (LSTM) which utilizes data on the driver's gaze and head position as well as vehicle dynamics data. We applied our approach on real data collected during drives in an urban environment in an instrumented vehicle. In comparison with previous IOHMM techniques that predicted three maneuvers including left/right turns and driving straight, our prediction model is able to anticipate two more maneuvers. In addition to this, our experimental results show that our model using identical dataset improved F1 score by 4% and increased to 84%. © 2016 IEEE.",,"Automobile drivers; Deep learning; Digital storage; Forecasting; Learning systems; Long short-term memory; Driving safety; Head position; Instrumented vehicle; Learning methods; Prediction model; Traffic situations; Urban environments; Vehicle dynamics; Advanced driver assistance systems",Article,"Final","",Scopus,2-s2.0-85087483768
"Sharma K., Giannakos M., Dillenbourg P.","55903734200;36462343600;8912010400;","Eye-tracking and artificial intelligence to enhance motivation and learning",2020,"Smart Learning Environments","7","1","13","","",,6,"10.1186/s40561-020-00122-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084034865&doi=10.1186%2fs40561-020-00122-x&partnerID=40&md5=eb86fcffa7c05cc87ef11df26336cfa5","Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway; Department of Computer Science, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","Sharma, K., Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway; Giannakos, M., Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway; Dillenbourg, P., Department of Computer Science, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","The interaction with the various learners in a Massive Open Online Course (MOOC) is often complex. Contemporary MOOC learning analytics relate with click-streams, keystrokes and other user-input variables. Such variables however, do not always capture users’ learning and behavior (e.g., passive video watching). In this paper, we present a study with 40 students who watched a MOOC lecture while their eye-movements were being recorded. We then proposed a method to define stimuli-based gaze variables that can be used for any kind of stimulus. The proposed stimuli-based gaze variables indicate students’ content-coverage (in space and time) and reading processes (area of interest based variables) and attention (i.e., with-me-ness), at the perceptual (following teacher’s deictic acts) and conceptual levels (following teacher discourse). In our experiment, we identified a significant mediation effect of the content coverage, reading patterns and the two levels of with-me-ness on the relation between students’ motivation and their learning performance. Such variables enable common measurements for the different kind of stimuli present in distinct MOOCs. Our long-term goal is to create student profiles based on their performance and learning strategy using stimuli-based gaze variables and to provide students gaze-aware feedback to improve overall learning process. One key ingredient in the process of achieving a high level of adaptation in providing gaze-aware feedback to the students is to use Artificial Intelligence (AI) algorithms for prediction of student performance from their behaviour. In this contribution, we also present a method combining state-of-the-art AI technique with the eye-tracking data to predict student performance. The results show that the student performance can be predicted with an error of less than 5%. © 2020, The Author(s).","Deep learning; Eye-tracking; Learning; Massive open online courses; MOOCs; Motivation; Multimodal analytics; Video based learning",,Article,"Final","",Scopus,2-s2.0-85084034865
"Lan G., Heit B., Scargill T., Gorlatova M.","57189242558;57220747706;57219272275;24775927000;","GazeGraph: Graph-based few-shot cognitive context sensing from human visual behavior",2020,"SenSys 2020 - Proceedings of the 2020 18th ACM Conference on Embedded Networked Sensor Systems",,,,"422","435",,1,"10.1145/3384419.3430774","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097524234&doi=10.1145%2f3384419.3430774&partnerID=40&md5=49ede4803830b1b298c151601c622781","Duke University, Durham, NC, United States","Lan, G., Duke University, Durham, NC, United States; Heit, B., Duke University, Durham, NC, United States; Scargill, T., Duke University, Durham, NC, United States; Gorlatova, M., Duke University, Durham, NC, United States","In this work, we present GazeGraph, a system that leverages human gazes as the sensing modality for cognitive context sensing. GazeGraph is a generalized framework that is compatible with different eye trackers and supports various gaze-based sensing applications. It ensures high sensing performance in the presence of heterogeneity of human visual behavior, and enables quick system adaptation to unseen sensing scenarios with few-shot instances. To achieve these capabilities, we introduce the spatial-temporal gaze graphs and the deep learning-based representation learning method to extract powerful and generalized features from the eye movements for context sensing. Furthermore, we develop a few-shot gaze graph learning module that adapts the 'learning to learn' concept from meta-learning to enable quick system adaptation in a data-efficient manner. Our evaluation demonstrates that GazeGraph outperforms the existing solutions in recognition accuracy by 45% on average over three datasets. Moreover, in few-shot learning scenarios, GazeGraph outperforms the transfer learning-based approach by 19% to 30%, while reducing the system adaptation time by 80%. © 2020 ACM.","cognitive context sensing; eye tracking; few-shot learning","Behavioral research; Deep learning; Embedded systems; Eye movements; Graphic methods; Transfer learning; Learning scenarios; Learning to learn; Learning-based approach; Recognition accuracy; Sensing applications; Sensing modalities; Sensing performance; Spatial temporals; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85097524234
"Abdeltawab A., Ahmad A.","57204789984;55364133600;","Classification of motor imagery EEG signals using machine learning",2020,"2020 IEEE 10th International Conference on System Engineering and Technology, ICSET 2020 - Proceedings",,,"09265364","196","201",,,"10.1109/ICSET51301.2020.9265364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098271611&doi=10.1109%2fICSET51301.2020.9265364&partnerID=40&md5=3b0eb61dd9c02e2018e038530de3e530","School of Electrical Engineering, Faculty of Engineering, Universiti Teknologi Malaysia (UTM), Johor, Malaysia","Abdeltawab, A., School of Electrical Engineering, Faculty of Engineering, Universiti Teknologi Malaysia (UTM), Johor, Malaysia; Ahmad, A., School of Electrical Engineering, Faculty of Engineering, Universiti Teknologi Malaysia (UTM), Johor, Malaysia","Brain Computer Interface (BCI) is a term that was first introduced by Jacques Vidal in the 1970s when he created a system that can determine the human eye gaze direction, making the system able to determine the direction a person want to go or move something to using scalp-recorded visual evoked potential (VEP) over the visual cortex. Ever since that time, many researchers where captivated by the huge potential and list of possibilities that can be achieved if simply a digital machine can interpret human thoughts. In this work, we explore electroencephalography (EEG) signal classification, specifically for motor imagery (MI) tasks. Classification of MI tasks can be carried out by using machine learning and deep learning models, yet there is a trade between accuracy and computation time that needs to be maintained. The objective is to create a machine learning model that can be optimized for real-time classification while having a relatively acceptable classification accuracy. The proposed model relies on common spatial patter (CSP) for feature extraction as well as linear discriminant analysis (LDA) for classification. With simple pre-processing stage and a proper selection of data for training the model proved to have a balanced accuracy of +80% while maintaining small run-time (milliseconds) that is opted for real-time classifications. © 2020 IEEE","Common Spatial Pattern (CSP); EEG; Linear Discriminant Analysis (LDA); Machine learning; Motor Imagery Classification","Biomedical signal processing; Brain computer interface; Deep learning; Discriminant analysis; Electroencephalography; Electrophysiology; Eye movements; Image classification; Systems engineering; Turing machines; Classification accuracy; Computation time; Digital machines; Linear discriminant analysis; Machine learning models; Motor imagery eeg signals; Signal classification; Visual evoked potential; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85098271611
"Wu J., Lu M., Lin Y., Zhang X.","57222004771;57212964869;56909063600;16032700200;","Scanpaths Generation for Target Search Based on Deep Learning",2020,"Proceedings - 2020 Chinese Automation Congress, CAC 2020",,,"9327358","1443","1448",,,"10.1109/CAC51589.2020.9327358","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100940810&doi=10.1109%2fCAC51589.2020.9327358&partnerID=40&md5=fb873f7ed40b2f0b2e72580dce99c87a","National Engineering Laboratory for Visual Information Processing and Applications, Xi'an, Shaanxi, 710049, China; School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, China; School of Foreign Studies, Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, China","Wu, J., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an, Shaanxi, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, China; Lu, M., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an, Shaanxi, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, China; Lin, Y., School of Foreign Studies, Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, China; Zhang, X., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an, Shaanxi, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, China","Scanpath is a sequence of gaze fixations changing with time when browsing something, which records eyes' movement dynamically. Accurate prediction of scanpaths can help computers better predict which area human pay attention to and drive the development of next-generation systems that can understand human behavior and needs. Therefore, this paper introduces an algorithm of scanpaths generation for target search. Based on one-shot learning network, the algorithm extracts the target map with task information, and then imports it into a visual model of the superior colliculus to predict the task-drive scanpaths. The results are similar to the gaze behavior of human eyes in shape, direction and length. At the same time, the algorithm also has the ability to predict the scanpaths based on unseen categories. © 2020 IEEE.","Computer Vision; Deep Learning; Scanpaths Generation; Target Search","Behavioral research; Eye movements; Forecasting; Learning systems; Accurate prediction; Gaze behavior; Human behaviors; Next generation systems; One-shot learning; Superior colliculus; Target search; Task information; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85100940810
"Bublea A., Caleanu C.D.","57221596015;14049898400;","Deep Learning based Eye Gaze Tracking for Automotive Applications: An Auto-Keras Approach",2020,"2020 14th International Symposium on Electronics and Telecommunications, ISETC 2020 - Conference Proceedings",,,"9301091","","",,1,"10.1109/ISETC50328.2020.9301091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099600654&doi=10.1109%2fISETC50328.2020.9301091&partnerID=40&md5=069911eeca07718f176b34859e8823bc","Politehnica University Timioara, Department of Applied Electronics Faculty of Electronics, Telecommunications and Information Technologies, Timisoara, Romania","Bublea, A., Politehnica University Timioara, Department of Applied Electronics Faculty of Electronics, Telecommunications and Information Technologies, Timisoara, Romania; Caleanu, C.D., Politehnica University Timioara, Department of Applied Electronics Faculty of Electronics, Telecommunications and Information Technologies, Timisoara, Romania","We propose a deep neural network-based gaze sensing method in which the design of the neural architecture is performed automatically, through a network architecture search algorithm called Auto-Keras. First, the neural model is generated using the Columbia Gaze Data Set. Then, the performance of the solution is estimated on an online scenario and proves the generalization ability of our model. In comparison to a geometrical approach, which uses dlib facial landmarks, filtering and morphological operators for gaze estimation, the proposed method provides superior results and certain advantages. © 2020 IEEE.","automotive; deep learning; eye gaze tracking","Deep neural networks; Eye tracking; Network architecture; Automotive applications; Eye gaze tracking; Generalization ability; Geometrical approaches; Morphological operator; Neural architectures; Neural modeling; Search Algorithms; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85099600654
"Gowroju S., Aarti, Kumar S.","57221499478;55904388800;57234304400;","Robust Deep Learning Technique: U-Net Architecture for Pupil Segmentation",2020,"11th Annual IEEE Information Technology, Electronics and Mobile Communication Conference, IEMCON 2020",,,"9284947","609","613",,1,"10.1109/IEMCON51383.2020.9284947","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099305717&doi=10.1109%2fIEMCON51383.2020.9284947&partnerID=40&md5=180d7846e6b5c2fadeb1b5341c87fdd1","Lovely Professional University, Computer Science Engineering, Punjab, India; Electronics Communication Engineering, Sreyas Institute of Engineering and Technology, Hyderabad, India","Gowroju, S., Lovely Professional University, Computer Science Engineering, Punjab, India; Aarti, Lovely Professional University, Computer Science Engineering, Punjab, India; Kumar, S., Electronics Communication Engineering, Sreyas Institute of Engineering and Technology, Hyderabad, India","In many of the iris biometric applications plays a major role in tracking the gaze, detecting fatigue, and predicting the age of a person, etc.That were built for human-computer interaction and security applications such as border control applications or criminal tracking applications. In this paper, we proposed a novel CNN U-Net based model to perform the accurate segmentation of pupil. We experimented on the CASIA database and generated an accuracy of 90% in segmentation. We considered various parameters such as Accuracy, Loss, and Mean Square Error (MSE) to predict the efficiency of the model. The proposed system performed the segmentation of pupil from 512\times 512 images with MSE of 1.24. © 2020 IEEE.","CNN; Pupil; Segmentation; U-Net; Val-Accuracy; Val_Loss","Human computer interaction; Image segmentation; Mean square error; Mobile telecommunication systems; Border control application; Casia database; Iris biometrics; Learning techniques; NET architecture; Pupil segmentation; Security application; Tracking application; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85099305717
"Yvinec E., Dapogny A., Bailly K.","57219733813;56916151100;25633701200;","DeeSCo: Deep heterogeneous ensemble with Stochastic Combinatory loss for gaze estimation",2020,"Proceedings - 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2020",,,"9320196","146","152",,,"10.1109/FG47880.2020.00039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101476670&doi=10.1109%2fFG47880.2020.00039&partnerID=40&md5=b0e15487eb82e3f0f70685084c83a657","Datakalab, 114 Boulevard Malesherbes, Paris, 75017, France","Yvinec, E., Datakalab, 114 Boulevard Malesherbes, Paris, 75017, France; Dapogny, A., Datakalab, 114 Boulevard Malesherbes, Paris, 75017, France; Bailly, K., Datakalab, 114 Boulevard Malesherbes, Paris, 75017, France","From medical research to gaming applications, gaze estimation is becoming a valuable tool. While there exists a number of hardware-based solutions, recent deep learningbased approaches, coupled with the availability of large-scale databases, have allowed to provide a precise gaze estimate using only consumer sensors. However, there remains a number of questions, regarding the problem formulation, architectural choices and learning paradigms for designing gaze estimation systems in order to bridge the gap between geometry-based systems involving specific hardware and approaches using consumer sensors only. In this paper, we introduce a deep, end-toend trainable ensemble of heatmap-based weak predictors for 2D/3D gaze estimation. We show that, through heterogeneous architectural design of these weak predictors, we can improve the decorrelation between the latter predictors to design more robust deep ensemble models. Furthermore, we propose a stochastic combinatory loss that consists in randomly sampling combinations of weak predictors at train time. This allows to train better individual weak predictors, with lower correlation between them. This, in turns, allows to significantly enhance the performance of the deep ensemble. We show that our Deep heterogeneous ensemble with Stochastic Combinatory loss (DeeSCo) outperforms state-of-the-art approaches for 2D/3D gaze estimation on multiple datasets. © 2020 IEEE.","deep learning; Ensemble methods; Gaze estimation","Bridges; Gesture recognition; Gaming applications; Heterogeneous ensembles; Large-scale database; Learning paradigms; Learning-based approach; Multiple data sets; Problem formulation; State-of-the-art approach; Stochastic systems",Conference Paper,"Final","",Scopus,2-s2.0-85101476670
"Zhang Y., Yang S., Xiao J., Shan S., Chen X.","57218472407;57215371844;57215329571;22235341500;57215374943;","Can We Read Speech beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition",2020,"Proceedings - 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2020",,,"9320240","356","363",,6,"10.1109/FG47880.2020.00134","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095322216&doi=10.1109%2fFG47880.2020.00134&partnerID=40&md5=6d68c4d894ca93958da9fdc186cc6b7c","Institute of Computing Technology, CAS, Key Laboratory of Intelligent Information Processing, Chinese Academy of Sciences (CAS), Beijing, 100190, China","Zhang, Y., Institute of Computing Technology, CAS, Key Laboratory of Intelligent Information Processing, Chinese Academy of Sciences (CAS), Beijing, 100190, China; Yang, S., Institute of Computing Technology, CAS, Key Laboratory of Intelligent Information Processing, Chinese Academy of Sciences (CAS), Beijing, 100190, China; Xiao, J., Institute of Computing Technology, CAS, Key Laboratory of Intelligent Information Processing, Chinese Academy of Sciences (CAS), Beijing, 100190, China; Shan, S., Institute of Computing Technology, CAS, Key Laboratory of Intelligent Information Processing, Chinese Academy of Sciences (CAS), Beijing, 100190, China; Chen, X., Institute of Computing Technology, CAS, Key Laboratory of Intelligent Information Processing, Chinese Academy of Sciences (CAS), Beijing, 100190, China","Recent advances in deep learning have heightened interest among researchers in the field of visual speech recognition (VSR). Currently, most existing methods equate VSR with automatic lip reading, which attempts to recognise speech by analysing lip motion. However, human experience and psychological studies suggest that we do not always fix our gaze at each other's lips during a face-to-face conversation, but rather scan the whole face repetitively. This inspires us to revisit a fundamental yet somehow overlooked problem: can VSR models benefit from reading extraoral facial regions, i.e. beyond the lips? In this paper, we perform a comprehensive study on the evaluation of the effects of different facial regions with state-of-the-art VSR models, including the mouth, the whole face, the upper face, and even the cheeks. Experiments are conducted on both word-level and sentence-level benchmarks with different characteristics. We find that despite the complex variations of the data, incorporating information from extraoral facial regions, even the upper face, consistently benefits VSR performance. Furthermore, we introduce a simple yet effective method based on Cutout to learn more discriminative features for face-based VSR, hoping to maximise the utility of information encoded in different facial regions. Our experiments show obvious improvements over existing state-of-the-art methods that use only the lip region as inputs, a result we believe would probably provide the VSR community with some new and exciting insights. © 2020 IEEE.","visual speech recognition","Deep learning; Gesture recognition; Discriminative features; Face-to-face conversation; Roi selections; Sentence level; State of the art; State-of-the-art methods; Utility of information; Visual speech recognition; Speech recognition",Conference Paper,"Final","",Scopus,2-s2.0-85095322216
"Batliner M., Hess S., Ehrlich-Adám C., Lohmeyer Q., Meboldt M.","57204901071;57200655398;57219417447;43461718100;22835485400;","Automated areas of interest analysis for usability studies of tangible screen-based user interfaces using mobile eye tracking",2020,"Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM","34","4",,"505","514",,1,"10.1017/S0890060420000372","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092604652&doi=10.1017%2fS0890060420000372&partnerID=40&md5=8deadaeb109470a96a6eacf07a852f45","Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland","Batliner, M., Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland; Hess, S., Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland; Ehrlich-Adám, C., Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland; Lohmeyer, Q., Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland; Meboldt, M., Product Development Group Zurich, ETH Zurich, Leonhardstr. 21, Zürich, 8092, Switzerland","The user's gaze can provide important information for human-machine interaction, but the analysis of manual gaze data is extremely time-consuming, inhibiting wide adoption in usability studies. Existing methods for automated areas of interest (AOI) analysis cannot be applied to tangible products with a screen-based user interface (UI), which have become ubiquitous in everyday life. The objective of this paper is to present and evaluate a method to automatically map the user's gaze to dynamic AOIs on tangible screen-based UIs based on computer vision and deep learning. This paper presents an algorithm for automated Dynamic AOI Mapping (aDAM), which allows the automated mapping of gaze data recorded with mobile eye tracking to the predefined AOIs on tangible screen-based UIs. The evaluation of the algorithm is performed using two medical devices, which represent two extreme examples of tangible screen-based UIs. The different elements of aDAM are examined for accuracy and robustness, as well as the time saved compared to manual mapping. The break-even point for an analyst's effort for aDAM compared to manual analysis is found to be 8.9 min gaze data time. The accuracy and robustness of both the automated gaze mapping and the screen matching indicate that aDAM can be applied to a wide range of products. aDAM allows, for the first time, automated AOI analysis of tangible screen-based UIs with AOIs that dynamically change over time. The algorithm requires some additional initial input for the setup and training, but analyzed gaze data duration and effort is only determined by computation time and does not require any additional manual work thereafter. The efficiency of the approach has the potential for a broader adoption of mobile eye tracking in usability testing for the development of new products and may contribute to a more data-driven usability engineering process in the future. Copyright © The Author(s), 2020. Published by Cambridge University Press.","Computer vision; convolutional neural networks; mobile eye tracking; usability testing","Automation; Deep learning; Mapping; Usability engineering; User interfaces; Automated mapping; Computation time; Human machine interaction; Mobile eye-tracking; Tangible product; Usability engineering process; Usability studies; Usability testing; Eye tracking",Article,"Final","",Scopus,2-s2.0-85092604652
"Wang Y., Cai W., Gu T., Shao W.","57117129900;16232517100;7102898974;57208885593;","Your eyes reveal your secrets: An eye movement based password inference on smartphone",2020,"IEEE Transactions on Mobile Computing","19","11","8798759","2714","2730",,2,"10.1109/TMC.2019.2934690","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092487692&doi=10.1109%2fTMC.2019.2934690&partnerID=40&md5=118925f8405056b5d6adc77f87aae9e2","Northwestern Polytechnical University, Xi'an, 710129, China; RMIT University, Melbourne, VIC  3000, Australia","Wang, Y., Northwestern Polytechnical University, Xi'an, 710129, China; Cai, W., Northwestern Polytechnical University, Xi'an, 710129, China; Gu, T., RMIT University, Melbourne, VIC  3000, Australia; Shao, W., RMIT University, Melbourne, VIC  3000, Australia","The widespread use of smartphones has brought great convenience to our daily lives, while at the same time we have been increasingly exposed to security threats. Keystroke security is essential to user privacy protection. In this paper, we present GazeRevealer, a novel side-channel based keystroke inference framework to infer sensitive inputs on smartphone from video recordings of victim's eye patterns captured from smartphone front camera. We observe that eye movements typically follow the keystrokes typing on the number-only soft keyboard during password input. By exploiting eye movement patterns, we are able to infer the passwords being entered. We propose a novel algorithm to extract sensitive eye images from video streams, and classify these images with Support Vector Classification. We also propose a novel classification enhancement algorithm to further improve classification accuracy. Compared with prior keystroke detection approaches, GazeRevealer does not require any external auxiliary devices, and it only relies on smartphone front camera. We evaluate the performance of GazeRevealer on several smartphones under different real-life usage scenarios. The results show that GazeRevealer achieves an inference rate of 77.89 percent for single key number and an inference rate of 84.38 percent for 6-digit password in the ideal case. © 2002-2012 IEEE.","gaze estimation; Keystroke inference; mobile security","Authentication; Cameras; Data privacy; Image classification; Mobile security; Smartphones; Video recording; Auxiliary device; Classification accuracy; Detection approach; Enhancement algorithms; Eye movement patterns; Security threats; Support vector classification; Usage scenarios; Eye movements",Article,"Final","",Scopus,2-s2.0-85092487692
"Yang Y., Jiao S., He J., Xia B., Li J., Xiao R.","57201058186;8424707000;55978290700;57216916727;57221572847;57216916207;","Image retrieval via learning content-based deep quality model towards big data",2020,"Future Generation Computer Systems","112",,,"243","249",,2,"10.1016/j.future.2020.05.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085270091&doi=10.1016%2fj.future.2020.05.016&partnerID=40&md5=6dc65a4a794ed55c6869c4168d6f3c14","Chang'an University National Engineering Laboratory for Highway Maintenance Equipment, Xi'an, 710100, China; Yan'an University College of Mathematics and Computer Science, Yan'an, 716000, China","Yang, Y., Chang'an University National Engineering Laboratory for Highway Maintenance Equipment, Xi'an, 710100, China, Yan'an University College of Mathematics and Computer Science, Yan'an, 716000, China; Jiao, S., Chang'an University National Engineering Laboratory for Highway Maintenance Equipment, Xi'an, 710100, China; He, J., Yan'an University College of Mathematics and Computer Science, Yan'an, 716000, China; Xia, B., Yan'an University College of Mathematics and Computer Science, Yan'an, 716000, China; Li, J., Chang'an University National Engineering Laboratory for Highway Maintenance Equipment, Xi'an, 710100, China; Xiao, R., Chang'an University National Engineering Laboratory for Highway Maintenance Equipment, Xi'an, 710100, China","Image retrieval aims to search specific image from large-scale datasets. Traditional text-based and content-based image retrieval approaches have shown competitive performance. However, both of which are limited by semantic gap, i.e., they cannot reflect human perception of images. To narrow semantic gap in image retrieval, this paper proposes a deep neural network (DNN) based image retrieval method, where saliency map is derived to form human gaze shifting paths by constraint metrics. More specifically, we first design a DNN-based image saliency prediction. Subsequently, we leverage image quality assessment (IQA) algorithm to select high-quality salient regions, which will be concatenated in sequence by using proposed constraint metrics to mimic human visual perception. Afterwards, we leverage the CNN-based architecture for deep representation acquisition of each images, where spatial structure among salient regions can be well preserved. Subsequently, based on the quality score of the query image, a series of candidate images whose quality scores are similar to that of the query image are derived. Finally, we engineer a ranking distance metric to refine the candidate images to achieve image retrieval. Extend experiments demonstrate that our method outperforms several state-of-the-art algorithms. © 2020 Elsevier B.V.","Big data; CNN; DNN; Image retrieval; Quality assessment","Content based retrieval; Deep learning; Deep neural networks; Large dataset; Quality control; Semantics; CNN-based architecture; Competitive performance; Content based image retrieval; Human visual perception; Image quality assessment (IQA); Large-scale datasets; Spatial structure; State-of-the-art algorithms; Search engines",Article,"Final","",Scopus,2-s2.0-85085270091
"Liu S., Liu D., Wu H.","57221491395;57218528195;57221706335;","Gaze estimation with multi-scale channel and spatial attention",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"3437438","303","309",,,"10.1145/3436369.3437438","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099880091&doi=10.1145%2f3436369.3437438&partnerID=40&md5=87c63e8d4dc7823cccacdb790b3ead52","Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; School of Microelectronics and Communication Engineering, Chongqing University, China","Liu, S., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Liu, D., School of Microelectronics and Communication Engineering, Chongqing University, China; Wu, H., Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China","Gaze estimation is well established as a significant research topic in computer vision given its importance for different applications. Recent studies demonstrate that other regions of the face beyond the two eyes contain valuable information for gaze estimation. Motivated by these works, we propose a novel and powerful deep convolutional network with multi-scale channel and spatial attention, which only takes the full-face image as input without additional modules to detect the eyes and estimate the head pose, to handle the gaze estimation task. It uses multi-scale channel and spatial information to adaptively select and increase important features and suppress some unnecessary facial regions which may not contribute to estimate gaze. By rigorously evaluating our module, we show that our method significantly outperforms the state-of-the-art for 3D gaze estimation on multiple public datasets. © 2020 ACM.","Appearance-based gaze estimation; Attention mechanism; Eye tracking; Machine learning","Convolutional neural networks; Convolutional networks; Facial regions; Gaze estimation; Important features; Research topics; Spatial attention; Spatial informations; State of the art; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85099880091
"Chauhan H., Prasad A., Shukla J.","57221469442;57221461581;57027180600;","Engagement analysis of ADHD students using visual cues from eye tracker",2020,"ICMI 2020 Companion - Companion Publication of the 2020 International Conference on Multimodal Interaction",,,,"27","31",,1,"10.1145/3395035.3425256","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099266318&doi=10.1145%2f3395035.3425256&partnerID=40&md5=24c0ddcc341826e3487d9762025d4d6e","Indraprastha Institution of Information Technology Delhi, HMI Lab, New Delhi, India","Chauhan, H., Indraprastha Institution of Information Technology Delhi, HMI Lab, New Delhi, India; Prasad, A., Indraprastha Institution of Information Technology Delhi, HMI Lab, New Delhi, India; Shukla, J., Indraprastha Institution of Information Technology Delhi, HMI Lab, New Delhi, India","In this paper, we focus on finding the correlation between visual attention and engagement of ADHD students in one-on-one sessions with specialized educators using visual cues and eye-tracking data. Our goal is to investigate the extent to which observations of eye-gaze, posture, emotion and other physiological signals can be used to model the cognitive state of subjects and to explore the integration of multiple sensor modalities to improve the reliability of detection of human displays of awareness and emotion in the context of ADHD affected children. This is a novel problem since no previous studies have aimed to identify markers of attentiveness in the context of students affected with ADHD. The experiment has been designed to collect data in a controlled environment and later on can be used to generate Machine Learning models to assist real-world educators. Additionally, we propose a novel approach for AOI (Area of Interest) detection for eye-tracking analysis in dynamic scenarios using existing deep learning-based saliency prediction and fixation prediction models. We aim to use the processed data to extract the features from a subject's eye-movement patterns and use Machine Learning models to classify the attention levels. © 2020 ACM.","Eye tracking; Gaze detection; Machine learning; Saliency prediction","Behavioral research; Deep learning; Eye movements; Interactive computer systems; Learning systems; Physiological models; Predictive analytics; Students; Controlled environment; Dynamic scenarios; Engagement analysis; Eye movement patterns; Eye-tracking analysis; Machine learning models; Multiple sensors; Physiological signals; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85099266318
"Liu M., Fu Li Y., Liu H.","57217199438;57222350602;57224917932;","3D gaze estimation for head-mounted devices based on visual saliency",2020,"IEEE International Conference on Intelligent Robots and Systems",,,"9341755","10611","10616",,,"10.1109/IROS45743.2020.9341755","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102408756&doi=10.1109%2fIROS45743.2020.9341755&partnerID=40&md5=30db035319a4768cba781c58b7413817","City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Central China Normal University, National Engineering Research Center for E-Learning, Wuhan, China","Liu, M., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Fu Li, Y., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Liu, H., Central China Normal University, National Engineering Research Center for E-Learning, Wuhan, China","Compared with the maturity of 2D gaze tracking technology, 3D gaze tracking has gradually become a research hotspot in recent years. The head-mounted gaze tracker has shown great potential for gaze estimation in 3D space due to its appealing flexibility and portability. The general challenge for 3D gaze tracking algorithms is that calibration is necessary before the usage, and calibration targets cannot be easily applied in some situations or might be blocked by moving human and objects. Besides, the accuracy on depth direction has always come to be a crucial problem. Regarding the issues mentioned above, a 3D gaze estimation with auto-calibration method is proposed in this study. We use an RGBD camera as the scene camera to acquire the accurate 3D structure of the environment. The automatic calibration is achieved by uniting gaze vectors with saliency maps of the scene which aligned depth information. Finally, we determine the 3D gaze point through a point cloud generated from the RGBD camera. The experiment result demonstrates that our proposed method achieves 4.34 of average angle error in the field from 0.5m to 3m and the average depth error is 23.22mm, which is sufficient for 3D gaze estimation in the real scene. © 2020 IEEE.",,"Agricultural robots; Calibration; Cameras; Intelligent robots; Object tracking; Auto-calibration method; Automatic calibration; Average angle; Calibration targets; Depth information; Gaze estimation; Rgb-d cameras; Visual saliency; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85102408756
"Weber D., Santini T., Zell A., Kasneci E.","57217014046;54881866000;7003785870;56059892600;","Distilling location proposals of unknown objects through gaze information for human-robot interaction",2020,"IEEE International Conference on Intelligent Robots and Systems",,,"9340893","11086","11093",,,"10.1109/IROS45743.2020.9340893","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102404452&doi=10.1109%2fIROS45743.2020.9340893&partnerID=40&md5=8033b0eb4a87c9bd3a699ff89ada11a7","University of Tübingen, Tübingen, 72076, Germany","Weber, D., University of Tübingen, Tübingen, 72076, Germany; Santini, T., University of Tübingen, Tübingen, 72076, Germany; Zell, A., University of Tübingen, Tübingen, 72076, Germany; Kasneci, E., University of Tübingen, Tübingen, 72076, Germany","Successful and meaningful human-robot interaction requires robots to have knowledge about the interaction context - e.g., which objects should be interacted with. Unfortunately, the corpora of interactive objects is - for all practical purposes - infinite. This fact hinders the deployment of robots with pre-trained object-detection neural networks other than in pre-defined scenarios. A more flexible alternative to pre-training is to let a human teach the robot about new objects after deployment. However, doing so manually presents significant usability issues as the user must manipulate the object and communicate the object's boundaries to the robot. In this work, we propose streamlining this process by using automatic object location proposal methods in combination with human gaze to distill pertinent object location proposals. Experiments show that the proposed method 1) increased the precision by a factor of approximately 21 compared to location proposal alone, 2) is able to locate objects sufficiently similar to a state-of-the-art pre-trained deep-learning method (FCOS) without any training, and 3) detected objects that were completely missed by FCOS. Furthermore, the method is able to locate objects for which FCOS was not trained on, which are undetectable for FCOS by definition. © 2020 IEEE.",,"Agricultural robots; Deep learning; Educational robots; Intelligent robots; Learning systems; Location; Man machine systems; Object detection; Object recognition; Interaction context; Interactive objects; Learning methods; Object location; Pre-training; State of the art; Unknown objects; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85102404452
"Olawale O.P., Dimililer K.","57220811417;14624921500;","Individual Eye Gaze Prediction with the Effect of Image Enhancement Using Deep Neural Networks",2020,"4th International Symposium on Multidisciplinary Studies and Innovative Technologies, ISMSIT 2020 - Proceedings",,,"9254786","","",,1,"10.1109/ISMSIT50672.2020.9254786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097667477&doi=10.1109%2fISMSIT50672.2020.9254786&partnerID=40&md5=3ca49e16582cd84676f17a4fe8d34ad4","Near East University, Faculty of Engineering, Research Center for AI and IoT, Software Engineering Department, via Mersin 10, Nicosia Cyprus, Turkey; Near East University, Faculty of Engineering, Applied Artificial Intelligence Research Center, Electrical and Electronic Engineering Department, via Mersin 10, Nicosia, Turkey","Olawale, O.P., Near East University, Faculty of Engineering, Research Center for AI and IoT, Software Engineering Department, via Mersin 10, Nicosia Cyprus, Turkey; Dimililer, K., Near East University, Faculty of Engineering, Applied Artificial Intelligence Research Center, Electrical and Electronic Engineering Department, via Mersin 10, Nicosia, Turkey","The prediction of individual eye gaze is a research topic that has gained the interest of researchers with its wide range of applications because neural networks majorly increase the rate of accuracy of individual gaze. In this research work, MPIIGaze dataset has been employed for the prediction of individual gaze and the direction of individual gaze was grouped into down view, left view, right view and lastly centre view. A CNN model was used to train and validate a random selection of images. Firstly, the ordinary images were trained and validated, after which image enhancement processing technique was applied. With the image brightness enhancement technique, a higher rate of gaze prediction accuracy was achieved. Hence, it can be deduced that image enhancement has proved its purpose by providing image interpretation with better quality. © 2020 IEEE.","deep learning; Gaze detection; gaze direction; image enhancement; individual eyes",,Conference Paper,"Final","",Scopus,2-s2.0-85097667477
"Aftab A.R., Von Der Beeck M., Feld M.","57211750560;6603266701;24831443100;","You Have a Point There: Object Selection Inside an Automobile Using Gaze, Head Pose and Finger Pointing",2020,"ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction",,,,"595","603",,1,"10.1145/3382507.3418836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096692814&doi=10.1145%2f3382507.3418836&partnerID=40&md5=c38b2506e2c3bb15eee65ca254ee0ecd","University of Saarland, Saarbrücken, Germany; Bmw Group, Munich, Germany; German Research Center for Artificial Intelligence, Saarbrücken, Germany","Aftab, A.R., University of Saarland, Saarbrücken, Germany, Bmw Group, Munich, Germany, German Research Center for Artificial Intelligence, Saarbrücken, Germany; Von Der Beeck, M., Bmw Group, Munich, Germany; Feld, M., German Research Center for Artificial Intelligence, Saarbrücken, Germany","Sophisticated user interaction in the automotive industry is a fast emerging topic. Mid-air gestures and speech already have numerous applications for driver-car interaction. Additionally, multimodal approaches are being developed to leverage the use of multiple sensors for added advantages. In this paper, we propose a fast and practical multimodal fusion method based on machine learning for the selection of various control modules in an automotive vehicle. The modalities taken into account are gaze, head pose and finger pointing gesture. Speech is used only as a trigger for fusion. Single modality has previously been used numerous times for recognition of the user's pointing direction. We, however, demonstrate how multiple inputs can be fused together to enhance the recognition performance. Furthermore, we compare different deep neural network architectures against conventional Machine Learning methods, namely Support Vector Regression and Random Forests, and show the enhancements in the pointing direction accuracy using deep learning. The results suggest a great potential for the use of multimodal inputs that can be applied to more use cases in the vehicle. © 2020 ACM.","cnn; data fusion; late fusion; neural networks; rnn; svr","Decision trees; Deep neural networks; Interactive computer systems; Learning systems; Network architecture; Support vector regression; Automotive vehicle; Conventional machines; Driver-car interaction; Multi-modal approach; Multi-modal fusion; Multimodal inputs; Multiple sensors; User interaction; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85096692814
"Yu Z., Huang X., Zhang X., Shen H., Li Q., Deng W., Tang J., Yang Y., Ye J.","57220059228;57211979633;57218452564;57202803726;57221243465;8905974100;55713937200;57221524455;7403237682;","A Multi-Modal Approach for Driver Gaze Prediction to Remove Identity Bias",2020,"ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction",,,,"768","776",,,"10.1145/3382507.3417961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096677530&doi=10.1145%2f3382507.3417961&partnerID=40&md5=8c3d340a52efddc895a3f5cb7d06d323","Beijing University of Posts and Telecommunications, DiDi Chuxing, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; DiDi Chuxing, Beijing, China","Yu, Z., Beijing University of Posts and Telecommunications, DiDi Chuxing, Beijing, China; Huang, X., Beijing University of Posts and Telecommunications, Beijing, China; Zhang, X., DiDi Chuxing, Beijing, China; Shen, H., DiDi Chuxing, Beijing, China; Li, Q., DiDi Chuxing, Beijing, China; Deng, W., Beijing University of Posts and Telecommunications, Beijing, China; Tang, J., DiDi Chuxing, Beijing, China; Yang, Y., DiDi Chuxing, Beijing, China; Ye, J., DiDi Chuxing, Beijing, China","Driver gaze prediction is an important task in Advanced Driver Assistance System (ADAS). Although the Convolutional Neural Network (CNN) can greatly improve the recognition ability, there are still several unsolved problems due to the challenge of illumination, pose and camera placement. To solve these difficulties, we propose an effective multi-model fusion method for driver gaze estimation. Rich appearance representations, i.e. holistic and eyes regions, and geometric representations, i.e. landmarks and Delaunay angles, are separately learned to predict the gaze, followed by a score-level fusion system. Moreover, pseudo-3D appearance supervision and identity-adaptive geometric normalization are proposed to further enhance the prediction accuracy. Finally, the proposed method achieves state-of-the-art accuracy of 82.5288% on the test data, which ranks 1st at the EmotiW2020 driver gaze prediction sub-challenge. © 2020 ACM.","appearance representation; driver gaze prediction; geometric representation; identity bias; model fusion","Automobile drivers; Convolutional neural networks; Forecasting; Interactive computer systems; Geometric representation; Multi-modal approach; Multi-model fusion; Prediction accuracy; Recognition abilities; Score-level fusion; State of the art; Unsolved problems; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85096677530
"Stappen L., Rizos G., Schuller B.","57210120403;57194224622;6603767415;","X-AWARE: ConteXt-AWARE Human-Environment Attention Fusion for Driver Gaze Prediction in the Wild",2020,"ICMI 2020 - Proceedings of the 2020 International Conference on Multimodal Interaction",,,,"858","867",,2,"10.1145/3382507.3417967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096659210&doi=10.1145%2f3382507.3417967&partnerID=40&md5=80b9660cab9367c61c270f5360bc1719","University of Augsburg, Chair of Embedded Intelligence for Health Care and Wellbeing, Augsburg, Germany; Imperial College London, Group on Language, Audio, and Music, London, United Kingdom","Stappen, L., University of Augsburg, Chair of Embedded Intelligence for Health Care and Wellbeing, Augsburg, Germany; Rizos, G., Imperial College London, Group on Language, Audio, and Music, London, United Kingdom; Schuller, B., Imperial College London, Group on Language, Audio, and Music, London, United Kingdom","Reliable systems for automatic estimation of the driver's gaze are crucial for reducing the number of traffic fatalities and for many emerging research areas aimed at developing intelligent vehicle-passenger systems. Gaze estimation is a challenging task, especially in environments with varying illumination and reflection properties. Furthermore, there is wide diversity with respect to the appearance of drivers' faces, both in terms of occlusions (e.g. vision aids) and cultural/ethnic backgrounds. For this reason, analysing the face along with contextual information - for example, the vehicle cabin environment - adds another, less subjective signal towards the design of robust systems for passenger gaze estimation. In this paper, we present an integrated approach to jointly model different features for this task. In particular, to improve the fusion of the visually captured environment with the driver's face, we have developed a contextual attention mechanism, X-AWARE, attached directly to the output convolutional layers of InceptionResNetV2 networks. In order to showcase the effectiveness of our approach, we use the Driver Gaze in the Wild dataset, recently released as part of the Eighth Emotion Recognition in the Wild Challenge (EmotiW) challenge. Our best model outperforms the baseline by an absolute of 15.03% in accuracy on the validation set, and improves the previously best reported result by an absolute of 8.72% on the test set. © 2020 ACM.","attention fusion; context aware; gaze detection; in the wild","Interactive computer systems; Network layers; Attention mechanisms; Automatic estimation; Contextual information; Emotion recognition; Human environment; Integrated approach; Reflection properties; Traffic fatalities; Vision aids",Conference Paper,"Final","",Scopus,2-s2.0-85096659210
"Xia Y., Liang B.","57216709662;56643806300;","Gaze Estimation Based on Deep Learning Method",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"3425003","","",,1,"10.1145/3424978.3425003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094900908&doi=10.1145%2f3424978.3425003&partnerID=40&md5=1105a91d7d328980fdd84477d6af5529","Institute of Medical Technology, Peking University, Beijing, China; Department of Biostatistics, Peking University, Beijing, China","Xia, Y., Institute of Medical Technology, Peking University, Beijing, China; Liang, B., Department of Biostatistics, Peking University, Beijing, China","Many mature methods of gaze estimation are available in various scenarios. Relying on additional hardware or platforms with professional equipment to tackle intensive computation tasks is a prominent problem of traditional methods, which usually involves high costs and is relatively tedious. Besides, the implementation of traditional gaze estimation method is typically complex. Traditional gaze estimation approaches require systematic prior knowledge or expertise for practical operations, and the gaze is estimated through the representation of pupil and iris, so high-quality images shot in special environments are required. This paper proposes a data-driven method for gaze estimation. It can be applied to various mobile platforms with deep learning methods instead of additional hardware devices or systematic prior knowledge. When collecting gaze data set, the paper designs a set of automatic and fast data collection mechanism on the mobile platform. Beyond that, the paper proposes an annotation method on collected gaze dataset that improves the predicted accuracy. The results demonstrate that the deep learning method performs well and can satisfy the task need of different applications. © 2020 ACM.","Annotation method; Deep learning; Gaze estimation","Computer hardware; Data acquisition; Drilling platforms; Learning systems; Annotation methods; Computation tasks; Data collection mechanism; Data-driven methods; Hardware devices; High quality images; Learning methods; Professional equipment; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85094900908
"Sipele O., Ledezma A., Sanchis A.","57194946206;6507061932;7004052541;","Integration model of multi-agent architectures for data fusion-based active driving system",2020,"Human Factors in Intelligent Vehicles",,,,"125","141",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106599519&partnerID=40&md5=491846884ffb5ae339bb761c7bb2d51d","Departamento de Informática, Universidad Carlos III, de Madrid, Spain","Sipele, O., Departamento de Informática, Universidad Carlos III, de Madrid, Spain; Ledezma, A., Departamento de Informática, Universidad Carlos III, de Madrid, Spain; Sanchis, A., Departamento de Informática, Universidad Carlos III, de Madrid, Spain","Most of the time, a pipeline of Extract-Transform-Load (ETL) processes composes a model that constitutes the core of new active safety systems approaches. Nowadays, the inspection of driver behavior during the performance of dynamic driving tasks is included in reasoning models to enhance the ergonomics of active safety systems. Moreover, aspects concerning high automation level such as Take-Over Request (TOR) requires monitoring techniques for driving behavior analysis as well as surrounding prediction to determine the driver’s vigilance level. Nonetheless, the integration into Human-In-the-Loop (HITL) driving simulators to assess new models’ performance in a controlled environment becomes an arduous task. This paper presents an architecture model to integrate new active safety systems into the HITL driving simulators, indispensable for the assessment of complex systems whose inputs come from heterogeneous data sources. A mediation engine orchestrates the distributed multiagent architecture, gathering data from information providers and feeding the required inputs for all pipeline components which compose the data fusion based ADAS. It deployed a support system whose reasoning process merges the driver’s face orientation and gaze estimation with driving scene analysis in the STISIM driving simulator. Ten drivers participated in an experimental process based on performing a driving task with sudden and unexpected events to assess the system performance and to measure aspects of human factors. © 2020 River Publishers.",,,Book Chapter,"Final","",Scopus,2-s2.0-85106599519
"Otsu K., Seo M., Kitajima T., Chen Y.-W.","57221536630;35280835900;56810615800;56036268200;","Automatic generation of eye gaze corrected video using recursive conditional generative adversarial networks",2020,"2020 IEEE 9th Global Conference on Consumer Electronics, GCCE 2020",,,"9291784","674","677",,,"10.1109/GCCE50665.2020.9291784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099408812&doi=10.1109%2fGCCE50665.2020.9291784&partnerID=40&md5=9a59eee985d358a7f60cdb44968c0567","Ritsumeikan University, Graduate School of Information and Engineering, Shiga, Japan; Osaka Institute of Technology, Osaka, Japan; Samsung Japan Research Institute, Kanagawa, Japan","Otsu, K., Ritsumeikan University, Graduate School of Information and Engineering, Shiga, Japan; Seo, M., Osaka Institute of Technology, Osaka, Japan; Kitajima, T., Samsung Japan Research Institute, Kanagawa, Japan; Chen, Y.-W., Ritsumeikan University, Graduate School of Information and Engineering, Shiga, Japan","Eye contact plays an important role in conversations. However, it is difficult to maintain eye contact while using current popular video calling systems due to the different positions of the camera and display. To solve this problem, we introduce convolutional long short-term memory, which captures the features of frames up to the previous instant, into a deep learning generation model - conditional generative adversarial networks (GANs), which is recursive GANs - to generate eye gaze corrected video. By extending this network, the generator generates an image that takes the previous frame into account and the discriminator identifies the image by considering the previous frame. Thus, we aimed to achieve a consistent video conversion. © 2020 IEEE.","Convolutional LSTM; deep learning; face; generative adversarial networks; image transformation; Video","Electronics engineering; Electronics industry; Adversarial networks; Automatic Generation; Eye contact; Eye-gaze; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85099408812
"Liu M., Li Y.F., Liu H.","57217199438;8589964900;57224917932;","Towards Robust Auto-calibration for Head-mounted Gaze Tracking Systems",2020,"2020 IEEE International Conference on Mechatronics and Automation, ICMA 2020",,,"9233571","588","593",,1,"10.1109/ICMA49215.2020.9233571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096604606&doi=10.1109%2fICMA49215.2020.9233571&partnerID=40&md5=c69806bebb22c81f28b42a425b6808aa","City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Central China Normal University, National Engineering Research Center for E-Learning, Wuhan, Hubei Province, China","Liu, M., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Li, Y.F., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Liu, H., Central China Normal University, National Engineering Research Center for E-Learning, Wuhan, Hubei Province, China","Removing explicit user calibration is indeed an appealing goal for gaze tracking systems. In this paper, a novel auto-calibration method is proposed to achieve the 3D point of regard (PoR) prediction for the head-mounted gaze tracker. Our method chooses an RGBD sensor as the scene camera to capture 3D structures of the environment and treats salient regions as possible 3D calibration targets. In order to improve efficiency, the bag of words (BoW) algorithm is applied to calculate scene images' similarity and eliminate redundant maps. After elimination, the translation relationship between eye cameras and the scene camera can be determined by uniting calibration targets with gaze vectors, and 3D gaze points are obtained by transformed gaze vectors and the point cloud of environment. The experiment results indicate that our method achieves effective performance on 3D gaze estimation for head-mounted gaze trackers, which can promote engineering applications of human-computer interaction technology in many areas. © 2020 IEEE.","3D gaze estimation; auto-calibration; head-mounted gaze tracker; human-computer interaction","Calibration; Cameras; Human computer interaction; Image enhancement; Auto calibration; Auto-calibration method; Calibration targets; Effective performance; Engineering applications; Gaze tracking system; Point of regards; User calibration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85096604606
"Zhao Z., Li S., Kosaki T.","57220033636;16202805500;7004216768;","Estimating a Driver's Gaze Point by a Remote Spherical Camera",2020,"2020 IEEE International Conference on Mechatronics and Automation, ICMA 2020",,,"9233793","599","604",,,"10.1109/ICMA49215.2020.9233793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096533484&doi=10.1109%2fICMA49215.2020.9233793&partnerID=40&md5=fbf5d0a63ca020730bc25375a72d9670","Hiroshima City Universityd, Department of Systems Engineering, Hiroshima, Japan","Zhao, Z., Hiroshima City Universityd, Department of Systems Engineering, Hiroshima, Japan; Li, S., Hiroshima City Universityd, Department of Systems Engineering, Hiroshima, Japan; Kosaki, T., Hiroshima City Universityd, Department of Systems Engineering, Hiroshima, Japan","In this paper we propose a novel method of detecting a driver's gaze point by a single remote spherical camera. A spherical camera is fixed at the upside of the windshield of a vehicle so that it can observes outside scenes and an inside driver simultaneously. We propose a simplified model of a driver's gaze point detection under this setting-up by approximating the gaze direction vector as that starting from the viewpoint of the spherical camera by omitting the distance of the driver from the spherical camera. Furthermore, two algorithms are proposed and evaluated: one is gaze point estimation based on the geometric model using OpenFace [1], and the other is to use a neural network, which uses the gaze vector and head pose parameters obtained from OpenFace as input of the neural network, to estimate the gaze point in the spherical image. As shown in the experimental results, using a neural network to compensate the measuring errors of OpenFace can achieves a much better result. © 2020 IEEE.","Gaze Estimation; Spherical Camera","Cameras; Spheres; Gaze direction; Gaze point; Gaze point detections; Gaze point estimations; Geometric modeling; Head pose; Measuring errors; Spherical images; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85096533484
"Bermejo C., Chatzopoulos D., Hui P.","57192959451;56890943900;14029922900;","EyeShopper: Estimating Shoppers' Gaze using CCTV Cameras",2020,"MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia",,,,"2765","2774",,1,"10.1145/3394171.3413683","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106964693&doi=10.1145%2f3394171.3413683&partnerID=40&md5=31cc50e3f51eaf7b26e45acb4712dc30","The Hong Kong University of Science and Technology, Hong Kong; University of Helsinki, Finland","Bermejo, C., The Hong Kong University of Science and Technology, Hong Kong; Chatzopoulos, D., The Hong Kong University of Science and Technology, Hong Kong; Hui, P., The Hong Kong University of Science and Technology, Hong Kong, University of Helsinki, Finland","Recent advances in machine and deep learning allow for enhanced retail analytics by applying object detection techniques. However, existing approaches either require laborious installation processes to function or lack precision when the customers turn their back in the installed cameras. In this paper, we present EyeShopper, an innovative system that tracks the gaze of shoppers when facing away from the camera and provides insights about their behavior in physical stores. EyeShopper is readily deployable in existing surveillance systems and robust against low-resolution video inputs. At the same time, its accuracy is comparable to state-of-the-art gaze estimation frameworks that require high-resolution and continuous video inputs to function. Furthermore, EyeShopper is more robust than state-of-the-art gaze tracking techniques for back head images. Extensive evaluation with different real video datasets and a synthetic dataset we produced shows that EyeShopper estimates with high accuracy the gaze of customers. © 2020 ACM.","camera; convolutional neural networks; gaze estimator; indoor location; retail stores","Cameras; Deep learning; Object detection; Sales; Security systems; Gaze estimation; High resolution; Innovative systems; Low resolution video; Physical stores; State of the art; Surveillance systems; Video datasets; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85106964693
"Zhang J., Chen J., Tang H., Wang W., Yan Y., Sangineto E., Sebe N.","57204979191;57221654731;57208238003;57201865855;56431699600;6506128346;57204924633;","Dual In-painting Model for Unsupervised Gaze Correction and Animation in the Wild",2020,"MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia",,,,"1588","1596",,3,"10.1145/3394171.3413981","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106685325&doi=10.1145%2f3394171.3413981&partnerID=40&md5=8c4ba823064afc2fe208bfc85d66d0df","University of Trento, Trento, Italy; Shandong University, Jinan, China; École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Texas State University, San Marcos, TX, United States; Huawei Research Ireland","Zhang, J., University of Trento, Trento, Italy; Chen, J., Shandong University, Jinan, China; Tang, H., University of Trento, Trento, Italy; Wang, W., École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Yan, Y., Texas State University, San Marcos, TX, United States; Sangineto, E., University of Trento, Trento, Italy; Sebe, N., University of Trento, Trento, Italy, Huawei Research Ireland","We address the problem of unsupervised gaze correction in the wild, presenting a solution that works without the need of precise annotations of the gaze angle and the head pose. We created a new dataset called CelebAGaze consisting of two domains X, Y, where the eyes are either staring at the camera or somewhere else. Our method consists of three novel modules: the Gaze Correction module(GCM), the Gaze Animation module(GAM), and the Pretrained Autoencoder module (PAM). Specifically, GCM and GAM separately train a dual in-painting network using data from the domain X for gaze correction and data from the domain Y for gaze animation. Additionally, a Synthesis-As-Training method is proposed when training GAM to encourage the features encoded from the eye region to be correlated with the angle information, resulting in gaze animation achieved by interpolation in the latent space. To further preserve the identity information e.g., eye shape, iris color, we propose the PAM with an Autoencoder, which is based on Self-Supervised mirror learning where the bottleneck features are angle-invariant and which works as an extra input to the dual in-painting models. Extensive experiments validate the effectiveness of the proposed method for gaze correction and gaze animation in the wild and demonstrate the superiority of our approach in producing more compelling results than state-of-the-art baselines. Our code, the pretrained models and supplementary results are available at:https://github.com/zhangqianhui/GazeAnimation. © 2020 ACM.","deep learning; gaze animation; gaze correction; generative adversarial networks; image translation","Animation; Angle information; Animation modules; Auto encoders; Bottleneck features; Eye regions; Identity information; State of the art; Training methods; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85106685325
"Fu E.Y., Yang Z., Leong H.V., Ngai G., Do C.-W., Chan L.","57189356098;57221875576;7005127948;8915594400;57218301448;36983793800;","Exploiting Active Learning in Novel Refractive Error Detection with Smartphones",2020,"MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia",,,,"2775","2783",,,"10.1145/3394171.3413748","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106680632&doi=10.1145%2f3394171.3413748&partnerID=40&md5=7d0e399ff13c8de6b70a427be31e4754","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; The Hong Kong Polytechnic University, School of Optometry, Hong Kong","Fu, E.Y., Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Yang, Z., Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Leong, H.V., Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Ngai, G., Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Do, C.-W., The Hong Kong Polytechnic University, School of Optometry, Hong Kong; Chan, L., The Hong Kong Polytechnic University, School of Optometry, Hong Kong","Refractive errors, such as myopia and astigmatism, can lead to severe visual impairment if not detected and corrected in time. Traditional methods of refractive error diagnosis rely on well-trained optometrists operating expensive and importable devices, constraining the vision screening process. Advance in smartphone camera has enabled novel low-cost ubiquitous vision screening to detect refractive error or ametropia through eye image processing, based on the principle of photorefraction. However, contemporary smartphone-based methods rely heavily on hand-crafted features and sufficiency of well-labeled data. To address these challenges, this paper exploits active learning methods with a set of Convolutional Neural Network features encoding information of human eyes from pre-trained gaze estimation model. This enables more effective training on refractive error detection models with less labeled data. Our experimental results demonstrate the encouraging effectiveness of our active learning approach. The new set of features is able to attain screening accuracy of more than 80% with mean absolute error less than 0.66, meeting the expectation of optometrists for 0.5 to 1. The proposed active learning also requires significantly fewer training samples of 18% in achieving satisfactory performance. © 2020 ACM.","active learning; computer-aided diagnosis; mobile heathcare; vision screening","Convolutional neural networks; Error detection; Image processing; Labeled data; Ophthalmology; Smartphones; Active learning methods; Detection models; Encoding informations; Mean absolute error; Photo refractions; Smart-phone cameras; Vision screening; Visual impairment; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85106680632
"Lollett C., Hayashi H., Kamezaki M., Sugano S.","57221329332;57203992527;24923346900;7102227917;","A Robust Driver's Gaze Zone Classification using a Single Camera for Self-occlusions and Non-aligned Head and Eyes Direction Driving Situations",2020,"IEEE Transactions on Systems, Man, and Cybernetics: Systems","2020-October",,"9283470","4302","4308",,,"10.1109/SMC42975.2020.9283470","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098847449&doi=10.1109%2fSMC42975.2020.9283470&partnerID=40&md5=9d491ec1d4124ef5d923d6c937259718","Waseda University, Graduate School of Creative Science and Engineering, Department of Modern Mechanical Engineering, 17 Kikui-cho, Shinjuku-ku, Tokyo, 162-0044, Japan; Waseda University, Research Institute for Science and Engineering (RISE), 17 Kikui-cho, Shinjuku-ku, Tokyo, 162-0044, Japan; Waseda University, Department of Modern Mechanical Engineering, 3-4-1 Okubo, Shinjuku-ku, Tokyo, 169-8555, Japan","Lollett, C., Waseda University, Graduate School of Creative Science and Engineering, Department of Modern Mechanical Engineering, 17 Kikui-cho, Shinjuku-ku, Tokyo, 162-0044, Japan; Hayashi, H., Waseda University, Graduate School of Creative Science and Engineering, Department of Modern Mechanical Engineering, 17 Kikui-cho, Shinjuku-ku, Tokyo, 162-0044, Japan; Kamezaki, M., Waseda University, Research Institute for Science and Engineering (RISE), 17 Kikui-cho, Shinjuku-ku, Tokyo, 162-0044, Japan; Sugano, S., Waseda University, Department of Modern Mechanical Engineering, 3-4-1 Okubo, Shinjuku-ku, Tokyo, 169-8555, Japan","Distracted driving is one of the most common causes of traffic accidents around the world. Recognizing the driver's gaze direction during a maneuver could be an essential step for avoiding the matter mentioned above. Thus, we propose a gaze zone classification system that serves as a base of supporting systems for driver's situation awareness. However, the challenge is to estimate the driver's gaze inside not ideal scenarios, specifically in this work, scenarios where may occur self-occlusions or non-aligned head and eyes direction of the driver. Firstly, towards solving miss classifications during self-occlusions scenarios, we designed a novel protocol where a 3D full facial geometry reconstruction of the driver from a single 2D image is made using the state-of-the-art method PRNet. To solve the miss classification when the driver's head and eyes direction are not aligned, eyes and head information are extracted. After this, based on a mix of different data pre-processing and deep learning methods, we achieved a robust classifier in situations where self-occlusions or non-aligned head and eyes direction of the driver occur. Our results from the experiments explicitly measure and show that the proposed method can make an accurate classification for the two before-mentioned problems. Moreover, we demonstrate that our model generalizes new drivers while being a portable and extensible system, making it easy-adaptable for various automobiles. © 2020 IEEE.",,"Classification (of information); Data handling; Deep learning; Geometry; Data preprocessing; Driving situations; Extensible systems; Learning methods; Situation awareness; State-of-the-art methods; Supporting systems; Zone classifications; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85098847449
"Kabra A., Agrawal C., Pallab Jyoti Dutta H., Bhuyan M.K., Laskar R.H.","57208397359;57221399508;57221389145;56024582900;23397200000;","Vision Based Communicator",2020,"Proceedings of 2020 IEEE Applied Signal Processing Conference, ASPCON 2020",,,"9276664","293","297",,,"10.1109/ASPCON49795.2020.9276664","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099065803&doi=10.1109%2fASPCON49795.2020.9276664&partnerID=40&md5=fe5eb52aadc8585bce21c8539356aef0","IIT Guwahati, Department of Electronics and Electrical Engineering, Assam, 781039, India; IIT Guwahati, Department of Electronics and Electrical Engineering, Assam, 781039, India; IIT Guwahati, Department of Electronics and Electrical Engineering, Assam, 781039, India; IIT Guwahati, Department of Electronics and Electrical Engineering, Assam, 781039, India; NIT Silchar, Department of Electronics and Communication Engineering, Assam, 781039, India","Kabra, A., IIT Guwahati, Department of Electronics and Electrical Engineering, Assam, 781039, India; Agrawal, C., IIT Guwahati, Department of Electronics and Electrical Engineering, Assam, 781039, India; Pallab Jyoti Dutta, H., IIT Guwahati, Department of Electronics and Electrical Engineering, Assam, 781039, India; Bhuyan, M.K., IIT Guwahati, Department of Electronics and Electrical Engineering, Assam, 781039, India; Laskar, R.H., NIT Silchar, Department of Electronics and Communication Engineering, Assam, 781039, India","This work provides a vision-based human-computer interface. The interface senses and interprets eye-blinks as controls along with gaze detection methods. We used image processing methods like haar-like features for automatic face detection, followed by eye tracking and blink detection based on landmarks. The gaze detection mechanism helps in easy controlling of the mouse pointer which can help people with disabilities to communicate effectively. A heatmap is also generated so that in future developments, this data can be used to design a more efficient layout for the controls. Given the necessity to move towards online solutions, such data can cater to the needs of a variety of sectors. © 2020 IEEE.","Communicator; Computer Vision; Gaze estimation; Gaze Heatmap","Eye tracking; Mammals; Processing; Automatic face detection; Blink detections; Gaze detection; Haar-like features; Human computer interfaces; Image processing - methods; Mouse pointers; People with disabilities; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85099065803
"Alfaroby E. M., Wibirama S., Ardiyanto I.","57221293375;26654457700;36069000500;","Accuracy Improvement of Object Selection in Gaze Gesture Application using Deep Learning",2020,"ICITEE 2020 - Proceedings of the 12th International Conference on Information Technology and Electrical Engineering",,,"9271771","307","311",,,"10.1109/ICITEE49829.2020.9271771","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098691974&doi=10.1109%2fICITEE49829.2020.9271771&partnerID=40&md5=87133a2a10d7b49530c07a6224d0b0ac","Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia","Alfaroby E., M., Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia; Wibirama, S., Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia; Ardiyanto, I., Universitas Gadjah Mada, Department of Electrical and Information Engineering, Yogyakarta, 55281, Indonesia","Gaze-based interaction is a crucial research area. Gaze gesture provides faster interaction between a user and a computer application because people naturally look at the object of interest before taking any other actions. Spontaneous gaze-gesture-based application uses gaze-gesture as an input modality without performing any calibration. The conventional eye tracking systems have a problem with low accuracy. In general, data captured by eye tracker contains errors and noise within gaze position signal. The errors and noise affect the performance of object selection in gaze gesture based application that controls digital contents on the display using smooth-pursuit eye movement. The conventional object selection method suffers from low accuracy (<80%). In this paper, we addressed this accuracy problem with a novel approach using deep learning. We exploited deep learning power to recognize the pattern of eye-gaze data. Long Short Term Memory (LSTM) is a deep learning architecture based on recurrent neural network (RNN). We used LSTM to perform object selection task. The dataset consisted of 34 participants taken from previous study of object selection technique of gaze gesture-based application. Our experimental results show that the proposed method achieved 96.17% of accuracy. In future, our result may be used as a guidance for developing gaze gesture application. © 2020 IEEE.","Deep Learning; Eye Tracking; Gaze Gesture; LSTM","Eye movements; Eye tracking; Long short-term memory; Accuracy Improvement; Accuracy problems; Eye tracking systems; Gaze-based interaction; Input modalities; Learning architectures; Recurrent neural network (RNN); Smooth pursuit eye movement; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85098691974
"Tan T., Montague E., Furst J., Raicu D.","57218310586;26030605800;7006869722;6602800828;","Robust Physician Gaze Prediction Using a Deep Learning Approach",2020,"Proceedings - IEEE 20th International Conference on Bioinformatics and Bioengineering, BIBE 2020",,,"9288056","993","998",,,"10.1109/BIBE50027.2020.00168","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099584611&doi=10.1109%2fBIBE50027.2020.00168&partnerID=40&md5=08a4de747d896cf78c12954179c131fd","DePaul University, College of Computing and Digital Media, Chicago, United States","Tan, T., DePaul University, College of Computing and Digital Media, Chicago, United States; Montague, E., DePaul University, College of Computing and Digital Media, Chicago, United States; Furst, J., DePaul University, College of Computing and Digital Media, Chicago, United States; Raicu, D., DePaul University, College of Computing and Digital Media, Chicago, United States","The patient-physician relationship is an integral part of primary care visits. To build a better relationship, understanding the communication between patient and physician is the key. This study focused on analyzing the gaze, one of the most important non-verbal behaviors found to influence patient outcomes. Gaze analysis often needs a manual rating process which might be time-consuming, costly, and unreliable. This research aimed to support automated analysis of physician-patient interaction using a deep convolutional neural network with transfer learning to a build robust model for physician gaze prediction. Utilizing only 3 minutes of 15 videos capturing 3 physicians interacting with different patients in a clinical setting, the model achieved over 98% accuracy for train, test, and validation sets. By visualizing the convolutional layers and comparing sample frames from different interactions, results highlighted several patterns shared across frames predicted correctly from both seen and unseen video sequences. The proposed work has the potential to informed the future design of technologies used to capture the clinical interaction and provide real-time feedback for physicians, which will contribute to the improvement of care quality. © 2020 IEEE.","automatic labeling; deep learning; physician gaze; primary care visits","Bioinformatics; Convolution; Convolutional neural networks; Deep neural networks; Transfer learning; Automated analysis; Clinical settings; Learning approach; Nonverbal behavior; Patient interaction; Real-time feedback; Robust modeling; Validation sets; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85099584611
"Mitsuzum Y., Irie G., Kimura A., Nakazawa A.","57221269355;35173001300;7402712235;35807510800;","A Generative Self-Ensemble Approach to Simulated+Unsupervised Learning",2020,"Proceedings - International Conference on Image Processing, ICIP","2020-October",,"9191100","2151","2155",,,"10.1109/ICIP40778.2020.9191100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098668992&doi=10.1109%2fICIP40778.2020.9191100&partnerID=40&md5=0b6ebce36ac553b05d14d030e173ee8c","Ntt Corporation; Kyoto University, Japan","Mitsuzum, Y., Ntt Corporation; Irie, G., Ntt Corporation; Kimura, A., Ntt Corporation; Nakazawa, A., Kyoto University, Japan","In this paper, we consider Simulated and Unsupervised (S+U) learning which is a problem of learning from labeled synthetic and unlabeled real images. After translating the synthetic images to real ones, existing S+U learning methods use only the labeled synthetic images for training a predictor (e.g., a regression function) and ignore the target real images, which may result in unsatisfactory prediction performance. Our approach utilizes both synthetic and real images to train the predictor. The main idea of ours is to involve a self-ensemble learning framework into S+U learning. More specifically, we require the prediction results for an unlabeled real image to be consistent between 'teacher' and 'student' predictors, even after some perturbations are added to the image. Furthermore, aiming at generating diverse perturbations along the underlying data manifold, we introduce one-to-many image translation between synthetic and real images. Evaluation experiments on an appearance-based gaze estimation task demonstrate that the proposed ideas can improve the prediction accuracy and our full method can outperform existing S+U learning methods. © 2020 IEEE.","Image-to-Image Translation; SemiSupervised Learning; Simulated+Unsupervised Learning","Forecasting; Learning systems; Ensemble approaches; Ensemble learning; Evaluation experiments; Image translation; Prediction accuracy; Prediction performance; Regression function; Synthetic images; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85098668992
"Zanlorensi L.A., Proenca H., Menotti D.","57204585920;14016540600;22835431000;","Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks for Attribute Normalization",2020,"Proceedings - International Conference on Image Processing, ICIP","2020-October",,"9191251","1361","1365",,1,"10.1109/ICIP40778.2020.9191251","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098634004&doi=10.1109%2fICIP40778.2020.9191251&partnerID=40&md5=fe280b30665994fe6307c7be59aa36ac","Federal University of Paraná, Department of Informatics, Curitiba, Brazil; University of Beira Interior, Department of Informatics, Covilhã, Portugal","Zanlorensi, L.A., Federal University of Paraná, Department of Informatics, Curitiba, Brazil; Proenca, H., University of Beira Interior, Department of Informatics, Covilhã, Portugal; Menotti, D., Federal University of Paraná, Department of Informatics, Curitiba, Brazil","Ocular biometric systems working in unconstrained environments usually face the problem of small within-class compactness caused by the multiple factors that jointly degrade the quality of the obtained data. In this work, we propose an attribute normalization strategy based on deep learning generative frameworks, that reduces the variability of the samples used in pairwise comparisons, without reducing their discriminability. The proposed method can be seen as a preprocessing step that contributes for data regularization and improves the recognition accuracy, being fully agnostic to the recognition strategy used. As proof of concept, we consider the 'eyeglasses' and 'gaze' factors, comparing the levels of performance of five different recognition methods with/without using the proposed normalization strategy. Also, we introduce a new dataset for unconstrained periocular recognition, composed of images acquired by mobile devices, particularly suited to perceive the impact of 'wearing eyeglasses' in recognition effectiveness. Our experiments were performed in two different datasets, and support the usefulness of our attribute normalization scheme to improve the recognition performance. © 2020 IEEE.","Attribute editing; Biometrics; Image normalization; Periocular recognition","Biometrics; Eyeglasses; Image processing; Normalization strategies; Pair-wise comparison; Periocular recognition; Pre-processing step; Recognition accuracy; Recognition methods; Recognition strategies; Unconstrained environments; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85098634004
"Aresta G., Ferreira C., Pedrosa J., Araujo T., Rebelo J., Negrao E., Morgado M., Alves F., Cunha A., Ramos I., Campilho A.","57190949419;56204792400;57159246600;57190949608;57204512245;57212081287;57219445643;57219444857;7103392597;57191864412;57215190972;","Automatic Lung Nodule Detection Combined with Gaze Information Improves Radiologists' Screening Performance",2020,"IEEE Journal of Biomedical and Health Informatics","24","10","9007735","2894","2901",,3,"10.1109/JBHI.2020.2976150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092750056&doi=10.1109%2fJBHI.2020.2976150&partnerID=40&md5=c12e6ad9d618e413006fc4bf2c719240","Faculty of Engineering of University of Porto (FEUP), Porto, 4099-002, Portugal; Feup, Inesc Tec, Porto, Portugal; Inesc Tec, Porto, Portugal; Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Inesc Tec, University of Trás-os-Montes e Alto Douro (UTAD), Vila Real, 5001-801, Portugal","Aresta, G., Faculty of Engineering of University of Porto (FEUP), Porto, 4099-002, Portugal; Ferreira, C., Feup, Inesc Tec, Porto, Portugal; Pedrosa, J., Inesc Tec, Porto, Portugal; Araujo, T., Feup, Inesc Tec, Porto, Portugal; Rebelo, J., Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Negrao, E., Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Morgado, M., Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Alves, F., Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Cunha, A., Inesc Tec, University of Trás-os-Montes e Alto Douro (UTAD), Vila Real, 5001-801, Portugal; Ramos, I., Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Campilho, A., Inesc Tec, Porto, Portugal","Early diagnosis of lung cancer via computed tomography can significantly reduce the morbidity and mortality rates associated with the pathology. However, searching lung nodules is a high complexity task, which affects the success of screening programs. Whilst computer-aided detection systems can be used as second observers, they may bias radiologists and introduce significant time overheads. With this in mind, this study assesses the potential of using gaze information for integrating automatic detection systems in the clinical practice. For that purpose, 4 radiologists were asked to annotate 20 scans from a public dataset while being monitored by an eye tracker device, and an automatic lung nodule detection system was developed. Our results show that radiologists follow a similar search routine and tend to have lower fixation periods in regions where finding errors occur. The overall detection sensitivity of the specialists was 0.67\pm 0.07, whereas the system achieved 0.69. Combining the annotations of one radiologist with the automatic system significantly improves the detection performance to similar levels of two annotators. Filtering automatic detection candidates only for low fixation regions still significantly improves the detection sensitivity without increasing the number of false-positives. © 2013 IEEE.","clinical environment; computer-aided diagnosis; deep learning; eye-tracking; Lung cancer","Biological organs; Computerized tomography; Diagnosis; Diseases; Automatic Detection; Automatic detection systems; Clinical practices; Computer aided detection systems; Detection performance; Detection sensitivity; Lung nodule detection; Screening performance; Eye tracking; accuracy; algorithm; Article; artifact; cancer screening; clinical practice; communication skill; computer assisted tomography; computer simulation; deep learning; echography; electroencephalography; false positive result; filtration; gait; gaze; human; human experiment; image segmentation; joint function; lung nodule; lung volume; morbidity; mortality; quality of life; radiologist; signal noise ratio; thorax radiography; training; computer assisted diagnosis; diagnostic imaging; eye fixation; lung tumor; physiology; procedures; radiologist; x-ray computed tomography; Deep Learning; Eye-Tracking Technology; Fixation, Ocular; Humans; Lung Neoplasms; Radiographic Image Interpretation, Computer-Assisted; Radiologists; Tomography, X-Ray Computed",Article,"Final","",Scopus,2-s2.0-85092750056
"Yang L., Dong K., Dmitruk A.J., Brighton J., Zhao Y.","57219401199;57197782440;57219402237;15041679200;57196243255;","A Dual-Cameras-Based Driver Gaze Mapping System with an Application on Non-Driving Activities Monitoring",2020,"IEEE Transactions on Intelligent Transportation Systems","21","10","8836623","4318","4327",,9,"10.1109/TITS.2019.2939676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092566879&doi=10.1109%2fTITS.2019.2939676&partnerID=40&md5=3ba9884c19a269f56fc23439a89545e7","School of Aerospace Transport and Manufacturing, Cranfield University, Bedfordshire, MK43 0AL, United Kingdom; Chongqing Automotive Collaborative Innovation Centre, Chongqing University, Chongqing, 400044, China","Yang, L., School of Aerospace Transport and Manufacturing, Cranfield University, Bedfordshire, MK43 0AL, United Kingdom; Dong, K., Chongqing Automotive Collaborative Innovation Centre, Chongqing University, Chongqing, 400044, China; Dmitruk, A.J., School of Aerospace Transport and Manufacturing, Cranfield University, Bedfordshire, MK43 0AL, United Kingdom; Brighton, J., School of Aerospace Transport and Manufacturing, Cranfield University, Bedfordshire, MK43 0AL, United Kingdom; Zhao, Y., School of Aerospace Transport and Manufacturing, Cranfield University, Bedfordshire, MK43 0AL, United Kingdom","Characterisation of the driver's non-driving activities (NDAs) is of great importance to the design of the take-over control strategy in Level 3 automation. Gaze estimation is a typical approach to monitor the driver's behaviour since the eye gaze is normally engaged with the human activities. However, current eye gaze tracking techniques are either costly or intrusive which limits their applicability in vehicles. This paper proposes a low-cost and non-intrusive dual-cameras based gaze mapping system that visualises the driver's gaze using a heat map. The challenges introduced by complex head movement during NDAs and camera distortion are addressed by proposing a nonlinear polynomial model to establish the relationship between the face features and eye gaze on the simulated driver's view. The Root Mean Square Error of this system in the in-vehicle experiment for the X and Y direction is 7.80±5.99 pixel and 4.64±3.47 pixel respectively with the image resolution of 1440 × 1080 pixels. This system is successfully demonstrated to evaluate three NDAs with visual attention. This technique, acting as a generic tool to monitor driver's visual attention, will have wide applications on NDA characterisation for intelligent design of take over strategy and driving environment awareness for current and future automated vehicles. © 2000-2011 IEEE.","camera mapping; Driver attention evaluation; heat map; Level 3 automation; system identification","Automobile drivers; Cameras; Eye movements; Eye tracking; Image resolution; Mapping; Mean square error; Pixels; Vehicles; Automated vehicles; Control strategies; Driving environment; Eye gaze tracking; Intelligent designs; Nonlinear polynomials; Root mean square errors; Vehicle experiment; Behavioral research",Article,"Final","",Scopus,2-s2.0-85092566879
"Nguyen M.T., Siritanawan P., Kotani K.","57211425376;36186132800;56038824900;","Saliency detection in human crowd images of different density levels using attention mechanism",2020,"Signal Processing: Image Communication","88",,"115976","","",,2,"10.1016/j.image.2020.115976","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089547567&doi=10.1016%2fj.image.2020.115976&partnerID=40&md5=13dfa6ecd3d705d5b641e18bc8538937","School of Information Science, Japan Advanced Institute of Science and Technology, Nomi, Ishikawa, 923-1292, Japan","Nguyen, M.T., School of Information Science, Japan Advanced Institute of Science and Technology, Nomi, Ishikawa, 923-1292, Japan; Siritanawan, P., School of Information Science, Japan Advanced Institute of Science and Technology, Nomi, Ishikawa, 923-1292, Japan; Kotani, K., School of Information Science, Japan Advanced Institute of Science and Technology, Nomi, Ishikawa, 923-1292, Japan","The human visual system has the ability to rapidly identify and redirect attention to important visual information in high complexity scenes such as the human crowd. Saliency prediction in the human crowd scene is the process using computer vision techniques to imitate the human visual system, predicting which areas in a human crowd scene may attract human attention. However, it is a challenging task to identify which factors may attract human attention due to the high complexity of the human crowd scene. In this work, we propose Multiscale DenseNet — Dilated and Attention (MSDense-DAt), a convolutional neural network (CNN) using self-attention to integrate the result of knowledge-driven gaze in the human visual system to identify salient areas in the human crowd scene. Our method combines various state-of-the-art deep learning architectures to deal with the high complexity in human crowd image, such as multiscale DenseNet for multiscale deep features extraction, self-attention, and dilated convolution. Then the effectiveness of each component in our CNN architecture is evaluated by comparing different components combinations. Finally, the proposed method is further evaluated in different crowd density levels to appraise the effect of crowd density on model performance. © 2020 Elsevier B.V.","Attention mechanism; Deep neural network; Human crowd; Saliency","Complex networks; Convolution; Convolutional neural networks; Deep learning; Network architecture; Attention mechanisms; Computer vision techniques; Different densities; Features extraction; Human Visual System; Learning architectures; Saliency detection; Visual information; Image processing",Article,"Final","",Scopus,2-s2.0-85089547567
"Shi P., Billeter M., Eisemann E.","55858760100;23092768600;35304781200;","SalientGaze: Saliency-based gaze correction in virtual reality",2020,"Computers and Graphics (Pergamon)","91",,,"83","94",,2,"10.1016/j.cag.2020.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088662850&doi=10.1016%2fj.cag.2020.06.007&partnerID=40&md5=89fbc20657a13b144a09e2373fbbfca1","Computer Graphics and Visualization Group, Delft University of Technology, Delft, 2628 XE, Netherlands; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, 410073, China","Shi, P., Computer Graphics and Visualization Group, Delft University of Technology, Delft, 2628 XE, Netherlands, Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, 410073, China; Billeter, M., Computer Graphics and Visualization Group, Delft University of Technology, Delft, 2628 XE, Netherlands; Eisemann, E., Computer Graphics and Visualization Group, Delft University of Technology, Delft, 2628 XE, Netherlands","Eye-tracking with gaze estimation is a key element in many applications, ranging from foveated rendering and user interaction to behavioural analysis and usage metrics. For virtual reality, eye-tracking typically relies on near-eye cameras that are mounted in the VR headset. Such methods usually involve an initial calibration to create a mapping from eye features to a gaze position. However, the accuracy based on the initial calibration degrades when the position of the headset relative to the users’ head changes; this is especially noticeable when users readjust the headset for comfort or even completely remove it for a short while. We show that a correction of such shifts can be achieved via 2D drift vectors in eye space. Our method estimates these drifts by extracting salient cues from the shown virtual environment to determine potential gaze directions. Our solution can compensate for HMD shifts, even those arising from taking off the headset, which enables us to eliminate reinitialization steps. © 2020 Elsevier Ltd","Drift estimation; Eye-tracking; Headsets shifts; Saliency; Stereo; Virtual reality","Behavioral research; Calibration; Eye tracking; Vector spaces; Eye camera; Gaze direction; Gaze estimation; Key elements; Reinitialization; User interaction; Virtual reality",Article,"Final","",Scopus,2-s2.0-85088662850
"Boutros F., Damer N., Raja K., Ramachandra R., Kirchbuchner F., Kuijper A.","57205379838;50861109400;57188866050;57190835798;57031859600;56131137100;","On benchmarking iris recognition within a head-mounted display for AR/VR applications",2020,"IJCB 2020 - IEEE/IAPR International Joint Conference on Biometrics",,,"9304919","","",,4,"10.1109/IJCB48548.2020.9304919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099719694&doi=10.1109%2fIJCB48548.2020.9304919&partnerID=40&md5=7cec54f18d42f7d2a183113d643747ad","Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; The Norwegian Colour and Visual Computing Laboratory, NTNU, Gjovik, Norway; Norwegian Biometrics Laboratory, NTNU, Gjovik, Norway","Boutros, F., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Damer, N., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Raja, K., The Norwegian Colour and Visual Computing Laboratory, NTNU, Gjovik, Norway, Norwegian Biometrics Laboratory, NTNU, Gjovik, Norway; Ramachandra, R., Norwegian Biometrics Laboratory, NTNU, Gjovik, Norway; Kirchbuchner, F., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Kuijper, A., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany","Augmented and virtual reality is being deployed in different fields of applications. Such applications might involve accessing or processing critical and sensitive information, which requires strict and continuous access control. Given that Head-Mounted Displays (HMD) developed for such applications commonly contains internal cameras for gaze tracking purposes, we evaluate the suitability of such setup for verifying the users through iris recognition. In this work, we first evaluate a set of iris recognition algorithms suitable for HMD devices by investigating three well-established handcrafted feature extraction approaches, and to complement it, we also present the analysis using four deep learning models. While taking into consideration the minimalistic hardware requirements of stand-alone HMD, we employ and adapt a recently developed miniature segmentation model (EyeMMS) for segmenting the iris. Further, to account for non-ideal and non-collaborative capture of iris, we define a new iris quality metric that we termed as Iris Mask Ratio (IMR) to quantify the iris recognition performance. Motivated by the performance of iris recognition, we also propose the continuous authentication of users in a non-collaborative capture setting in HMD. Through the experiments on a publicly available OpenEDS dataset, we show that performance with EER = 5% can be achieved using deep learning methods in a general setting, along with high accuracy for continuous user authentication. © 2020 IEEE.",,"Authentication; Benchmarking; Biometrics; Deep learning; Eye tracking; Learning systems; Street traffic control; Augmented and virtual realities; Continuous authentications; Head mounted displays; Iris recognition algorithm; Quality metrices; Segmentation models; Sensitive informations; User authentication; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85099719694
"Lee K.F., Chen Y.L., Yu C.W., Wu C.H.","56413130800;35322122400;54379325600;57203205774;","The Eye Tracking and Gaze Estimation System by Low Cost Wearable Devices",2020,"2020 IEEE International Conference on Consumer Electronics - Taiwan, ICCE-Taiwan 2020",,,"9258009","","",,,"10.1109/ICCE-Taiwan49838.2020.9258009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098465558&doi=10.1109%2fICCE-Taiwan49838.2020.9258009&partnerID=40&md5=21208bd82f557dc7aab55340bdbd1fcd","National Taipei University of Technology, Taipei, Taiwan","Lee, K.F., National Taipei University of Technology, Taipei, Taiwan; Chen, Y.L., National Taipei University of Technology, Taipei, Taiwan; Yu, C.W., National Taipei University of Technology, Taipei, Taiwan; Wu, C.H., National Taipei University of Technology, Taipei, Taiwan","This study develop a wearable eye tracking and gaze estimation low cost devices. This devices use infrared camera and design by integration of elastic mechanism adaptable. The use of cheap endoscope camera for mobile and 3D print technique for building up the devices, which results in be a low cost solution. This device can effectively extract and estimate pupil ellipse from few camera-captured samples of an eye and compute the corresponding 3D eye model. And this device use multiple point's calibration method to solve the related polynomial formula for future angle-to-gaze mapping. This wearable device is a low-cost which can be used for virtual reality and auxiliary equipment. © 2020 IEEE.",,"3D modeling; 3D printers; Auxiliary equipment; Cameras; Cost estimating; Wearable technology; Calibration method; Elastic mechanism; Gaze estimation; Infra-red cameras; Low-cost devices; Low-cost solution; Multiple points; Wearable devices; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85098465558
"Moroto Y., Maeda K., Ogawa T., Haseyama M.","57205502977;56537475300;35332753900;56238860900;","Estimation of Person-Specific Visual Attention via Selection of Similar Persons",2020,"2020 IEEE International Conference on Consumer Electronics - Taiwan, ICCE-Taiwan 2020",,,"9258340","","",,,"10.1109/ICCE-Taiwan49838.2020.9258340","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098459201&doi=10.1109%2fICCE-Taiwan49838.2020.9258340&partnerID=40&md5=976e96cb81e3cf4526fc04838e6c94a3","Graduate School of Information Science and Technology, Hokkaido University, Sapporo, Hokkaido, 060-0814, Japan; Office of Institutional Research, Hokkaido University, Sapporo, Hokkaido, 060-0808, Japan; Hokkaido University, Faculty of Information Science and Technology, Sapporo, Hokkaido, 060-0814, Japan","Moroto, Y., Graduate School of Information Science and Technology, Hokkaido University, Sapporo, Hokkaido, 060-0814, Japan; Maeda, K., Office of Institutional Research, Hokkaido University, Sapporo, Hokkaido, 060-0808, Japan; Ogawa, T., Hokkaido University, Faculty of Information Science and Technology, Sapporo, Hokkaido, 060-0814, Japan; Haseyama, M., Graduate School of Information Science and Technology, Hokkaido University, Sapporo, Hokkaido, 060-0814, Japan","This paper presents a method for estimation of person-specific visual attention based on estimated similar persons' visual attention. For improving the estimation performance of person-specific visual attention, the proposed method uses the dataset including the large number of images and corresponding gaze data of many persons not including the target person and trains an estimation model based on deep learning. By using the estimated visual attention of similar persons for the target image, the proposed method estimates the visual attention of the target person with the small amount of gaze data. Experimental results show that the proposed method is effective for estimation of person-specific visual attention. © 2020 IEEE.",,"Deep learning; Image enhancement; Large dataset; Estimation models; Estimation performance; Target images; Visual Attention; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85098459201
"Amadori P.V., Fischer T., Wang R., Demiris Y.","56703112800;57190126084;57189039099;6506125343;","Decision Anticipation for Driving Assistance Systems",2020,"2020 IEEE 23rd International Conference on Intelligent Transportation Systems, ITSC 2020",,,"9294216","","",,2,"10.1109/ITSC45102.2020.9294216","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099667660&doi=10.1109%2fITSC45102.2020.9294216&partnerID=40&md5=0167e13a074b83f53d015c6c28c6e443","Imperial College London, Personal Robotics Laboratory, Department of Electrical and Electronic EngineeringSW7 2BT, United Kingdom","Amadori, P.V., Imperial College London, Personal Robotics Laboratory, Department of Electrical and Electronic EngineeringSW7 2BT, United Kingdom; Fischer, T., Imperial College London, Personal Robotics Laboratory, Department of Electrical and Electronic EngineeringSW7 2BT, United Kingdom; Wang, R., Imperial College London, Personal Robotics Laboratory, Department of Electrical and Electronic EngineeringSW7 2BT, United Kingdom; Demiris, Y., Imperial College London, Personal Robotics Laboratory, Department of Electrical and Electronic EngineeringSW7 2BT, United Kingdom","Anticipating the correctness of imminent driver decisions is a crucial challenge in advanced driving assistance systems and has the potential to lead to more reliable and safer human-robot interactions. In this paper, we address the task of decision correctness prediction in a driver-in-the-loop simulated environment using unobtrusive physiological signals, namely, eye gaze and head pose. We introduce a sequence-to-sequence based deep learning model to infer the driver's likelihood of making correct/wrong decisions based on the corresponding cognitive state. We provide extensive experimental studies over multiple baseline classification models on an eye gaze pattern and head pose dataset collected from simulated driving. Our results show strong correlates between the physiological data and decision correctness, and that the proposed sequential model reliably predicts decision correctness from the driver with 80% precision and 72% recall. We also demonstrate that our sequential model performs well in scenarios where early anticipation of correctness is critical, with accurate predictions up to two seconds before a decision is performed. © 2020 IEEE.",,"Behavioral research; Classification (of information); Deep learning; Intelligent systems; Man machine systems; Physiological models; Physiology; Social robots; Accurate prediction; Classification models; Driving assistance systems; Physiological data; Physiological signals; Sequential model; Simulated driving; Simulated environment; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85099667660
"Jalilian E., Karakaya M., Uhl A.","57195436690;35223530800;7005841206;","End-to-end Off-angle Iris Recognition Using CNN Based Iris Segmentation",2020,"BIOSIG 2020 - Proceedings of the 19th International Conference of the Biometrics Special Interest Group",,,"9210983","","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094919586&partnerID=40&md5=de7a4009c9b21b8720ebc7d23f244969","University of Salzburg, Department of Computer Science, Salzburg, Austria; Kennesaw State University, Department of Computer ScienceGA, United States","Jalilian, E., University of Salzburg, Department of Computer Science, Salzburg, Austria; Karakaya, M., Kennesaw State University, Department of Computer ScienceGA, United States; Uhl, A., University of Salzburg, Department of Computer Science, Salzburg, Austria","While deep learning techniques are increasingly becoming a tool of choice for iris segmentation, yet there is no comprehensive recognition framework dedicated for off-angle iris recognition using such modules. In this work, we investigate the effect of different gaze-angles on the CNN based off-angle iris segmentations, and their recognition performance, introducing an improvement scheme to compensate for some segmentation degradations caused by the off-angle distortions. Also, we propose an off-angle parameterization algorithm to re-project the off-angle images back to frontal view. Taking benefit of these, we further investigate if: (i) improving the segmentation outputs and/or correcting the iris images before or after the segmentation, can compensate for off-angle distortions, or (ii) the generalization capability of the network can be improved, by training it on iris images of different gaze-angles. In each experimental step, segmentation accuracy and the recognition performance are evaluated, and the results are analyzed and compared. © 2020 German Computer Association (Gesellschaft für Informatik e.V.).","CNN; Convolutional neural network; Iris parameterization; Off-angle iris recognition; Off-angle iris segmentation","Biometrics; Deep learning; Image enhancement; End to end; Gaze angles; Generalization capability; Iris images; Iris recognition; Iris segmentation; Learning techniques; Segmentation accuracy; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85094919586
"Shimauchi T., Sakurai K., Tate L., Tamura H.","57219557177;56909413100;57219560650;35600682500;","Gaze-based vehicle driving evaluation of system with an actual vehicle at an intersection with a traffic light",2020,"Electronics (Switzerland)","9","9","1408","1","15",,,"10.3390/electronics9091408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093910250&doi=10.3390%2felectronics9091408&partnerID=40&md5=ff33d91890b044bbd383fa12f771fb94","Faculty of Engineering, University of Miyazaki, 1-1, Gakuen Kibanadai-nishi, Miyazaki-shi, 889-2192, Japan","Shimauchi, T., Faculty of Engineering, University of Miyazaki, 1-1, Gakuen Kibanadai-nishi, Miyazaki-shi, 889-2192, Japan; Sakurai, K., Faculty of Engineering, University of Miyazaki, 1-1, Gakuen Kibanadai-nishi, Miyazaki-shi, 889-2192, Japan; Tate, L., Faculty of Engineering, University of Miyazaki, 1-1, Gakuen Kibanadai-nishi, Miyazaki-shi, 889-2192, Japan; Tamura, H., Faculty of Engineering, University of Miyazaki, 1-1, Gakuen Kibanadai-nishi, Miyazaki-shi, 889-2192, Japan","Due to the population aging in Japan, more elderly people are retaining their driver’s licenses and the increase in the number of car accidents by elderly drivers is a social problem. To address this problem, an objective data-based method to evaluate whether elderly drivers can continue driving is needed. In this paper, we propose a car driving evaluation system based on gaze as calculated by eye and head angles. We used an eye tracking device (TalkEye Lite) made by the Takei Scientific Instruments Cooperation. For our image processing technique, we propose a gaze fixation condition using deep learning (YOLOv2-tiny). By using an eye tracking device and the proposed gaze fixation condition, we built a system where drivers could be evaluated during actual car operation. We describe our system in this paper. In order to evaluate our proposed method, we conducted experiments from November 2017 to November 2018 where elderly people were evaluated by our system while driving an actual car. The subjects were 22 general drivers (two were 80–89 years old, four were 70–79 years old, six were 60–69 years old, three were 50–59 years old, five were 40–49 years old and two were 30–39 years old). We compared the subjects’ gaze information with the subjective evaluation by a professional driving instructor. As a result, we confirm that the subjects’ gaze information is related to the subjective evaluation by the instructor. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Car driving evaluation system; Eye tracking device; Gaze fixation; Subjective evaluation; YOLOv2-tiny",,Article,"Final","",Scopus,2-s2.0-85093910250
"Ishrat M., Abrol P.","57192108561;26632764700;","Image complexity analysis with scanpath identification using remote gaze estimation model",2020,"Multimedia Tools and Applications","79","33-34",,"24393","24412",,,"10.1007/s11042-020-09117-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086727986&doi=10.1007%2fs11042-020-09117-9&partnerID=40&md5=99cec889798685f8b58d42d2abc04571","Department of Computer Science & IT, University of Jammu (J&K), Jammu, India","Ishrat, M., Department of Computer Science & IT, University of Jammu (J&K), Jammu, India; Abrol, P., Department of Computer Science & IT, University of Jammu (J&K), Jammu, India","Analysis of gaze points has been a vital tool for understanding varied human behavioral pattern and underlying psychological processing. Gaze points are analyzed generally in terms of two events of fixations and saccades that are collectively termed as scanpath. Scanpath could potentially establish correlation between visual scenery and human cognitive tendencies. Scanpath has been analyzed for different domains that include visual perception, usability, memory, visual search or low level attributes like color, illumination and edges in an image. Visual search is one prominent area that examines scanpath of subjects while a target object is searched in a given set of images. Visual search explores behavioral tendencies of subjects with respect to image complexity. Complexity of an image is governed by spatial, frequency and color information present in the image. Scanpath based image complexity analysis determines human visual behavior that could lead to development of interactive and intelligent systems. There are several sophisticated eye tracking devices and associated algorithms for recording and classification of scanpath. However, in the present scenario when the chances of viral infections (COVID-19) from known and unknown sources are high, it is very important that the contact less methods and models be designed. In addition, even though the devices acquire and process eye movement data with fair accuracy but are intrusive and costly. The objective of current research work is to establish the complexity of the given set of images while target objects are searched and to present analysis of gaze search pattern. To achieve these objectives a remote gaze estimation and analysis model has been proposed for scanpath identification and analysis. The model is an alternate option for gaze point tracking and scanpath analysis that is non intrusive and low cost. The gaze points are tracked remotely as against sophisticated wearable eye tracking devices available in the market. The model employs easily available softwares and hardware devices. In the current work, complexity is derived on the basis of analysis of fixation and saccade gaze points. Based on the results generated by the proposed model, influence on subjects due to external stimuli is studied. The set of images chosen, act as external stimuli for the subjects during visual search. In order to statistically analyze scanpath for different subjects, certain scanpath parameters have been identified. The model maps and classifies eye movement gaze points into fixations and saccades and generates data for identified parameters. For eye detection and subsequent iris detection voila jones and circular hough transform (CHT) algorithms have been used. Identification by dispersion threshold (I-DT) is implemented for scanpath identification. The algorithms are customized for better iris and scanpath detection. Algorithms are developed for gaze screen mapping and classification of fixations and saccades. The experimentation has been carried on different subjects. Variations during visual search have been observed and analyzed. The present model requires no contact of human subject with any equipment including eye tracking devices, screen or computing devices. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","CHT; Fixation; Gaze; Saccade; Visual search; Voila Jones","Behavioral research; Eye movements; Eye protection; Hough transforms; Image analysis; Intelligent systems; Behavioral patterns; Circular Hough transforms; Eye movement datum; Eye tracking devices; Identified parameter; Image Complexity Analysis; Low-level attributes; Remote gaze estimation; Eye tracking",Article,"Final","",Scopus,2-s2.0-85086727986
"Jia D.","57221328143;","An analysis of driver cognitive distraction",2020,"Proceedings - 2020 International Conference on Computing and Data Science, CDS 2020",,,"9275937","334","337",,,"10.1109/CDS49703.2020.00071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098852145&doi=10.1109%2fCDS49703.2020.00071&partnerID=40&md5=531220a792d28f0ba21ad455b21226d2","College of Computer Science, Sichuan University, Chengdu, China","Jia, D., College of Computer Science, Sichuan University, Chengdu, China","Driver distraction detection is a very important problem today. There are a large number of accidents occur every year. In this paper, we give some example of a variety of driver distraction detection methods and present an organized review. Our discussion focuses on the characteristics of driver distraction detection methods and the comparison of advantages and disadvantages, so that readers can quickly understand the mainstream methods in this field. We also propose some opinions on the field of distraction detection. © 2020 IEEE","Driver distraction detection; Gaze estimation; Head pose; Visual focus","Computer programming; Computer science; Cognitive distractions; Detection methods; Driver distractions; Data Science",Conference Paper,"Final","",Scopus,2-s2.0-85098852145
"Minegishi T., Osawa H.","57219854449;24484018400;","Do You Move Unconsciously? Accuracy of Social Distance and Line of Sight between a Virtual Robot Head and Humans",2020,"29th IEEE International Conference on Robot and Human Interactive Communication, RO-MAN 2020",,,"9223506","503","508",,,"10.1109/RO-MAN47096.2020.9223506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095744966&doi=10.1109%2fRO-MAN47096.2020.9223506&partnerID=40&md5=65b1b3747d3f0d1e11be1985cc89e2d7","University of Tsukuba","Minegishi, T., University of Tsukuba; Osawa, H., University of Tsukuba","In this paper, we examine the effectiveness of the social distance between a virtual agent and users, and the gaze instruction using a display that can be viewed stereoscopically without using any wearable devices. An actual robot cannot always maintain an appropriate interpersonal distance, through nonverbal gestures owing to its limited range of motion. Because large movements may harm humans, the nonverbal gestures of robots are limited in the real world. In this work, 14 participants were asked how far they wanted to move from a robot posing as a museum guide agent. They were also asked to identify the point at which they felt the agent was gazing. There was a significant distance between the initial position and the position to which the participants moved in the first task under two-dimensional (2D) and three-dimensional (3D) scenarios. The participants moved a significant distance in the first task. In the gaze estimation task, however, the error between the 3D and 2D evaluations was significantly lesser, at a point far from the agent. © 2020 IEEE.",,"Agricultural robots; Display devices; Stereo image processing; Wearable technology; Gaze estimation; Range of motions; Social distance; Threedimensional (3-d); Two Dimensional (2 D); Virtual agent; Virtual robots; Wearable devices; Social robots",Conference Paper,"Final","",Scopus,2-s2.0-85095744966
"Aivaliotis P.-E., Grivokostopoulou F., Perikos I., Daramouskas I., Hatziligeroudis I.","57221474593;54412205200;27667764000;57207760262;25924984600;","Eye Gaze Analysis of Students in Educational Systems",2020,"11th International Conference on Information, Intelligence, Systems and Applications, IISA 2020",,,"9284374","","",,,"10.1109/IISA50023.2020.9284374","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099188196&doi=10.1109%2fIISA50023.2020.9284374&partnerID=40&md5=de1597d3eb866d829d1ebc98e54e95b9","University of Patras, Department of Computer Engineering and Informatics, Patras, Greece","Aivaliotis, P.-E., University of Patras, Department of Computer Engineering and Informatics, Patras, Greece; Grivokostopoulou, F., University of Patras, Department of Computer Engineering and Informatics, Patras, Greece; Perikos, I., University of Patras, Department of Computer Engineering and Informatics, Patras, Greece; Daramouskas, I., University of Patras, Department of Computer Engineering and Informatics, Patras, Greece; Hatziligeroudis, I., University of Patras, Department of Computer Engineering and Informatics, Patras, Greece","Eye gaze provides indicative information about the status and the behavior of a person and can be very assistive in human-computer interaction. Eye-gaze analysis is very helpful in a variety of applications in order to understand the interest of the users, their behavior or even to unveil distractions. However, the accurate eye-gaze estimation is a very challenging process. In this paper, we present an eye gaze estimation work that relies on convolutional neural networks which imitate the LeNet's architecture. They analyze eye gaze and provide a 2D vector that concerns the coordinates of the specific pixel inside the 2D screen's space, in which the user is looking at. Also, a system capable of working under various real-world conditions such as light, angle and distance differentiations was designed and developed. An evaluation study was performed and the results are quite promising pointing out that the system is scalable and accurate in estimating the eye gaze of the users. © 2020 IEEE.","Convolutional Neural Networks; Deep Learning; Eye Gaze estimation; Human-Computer Interaction","Convolutional neural networks; Vector spaces; Assistive; Educational systems; Evaluation study; Eye-gaze; Real-world; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85099188196
"Dahmani M., Chowdhury M.E.H., Khandakar A., Rahman T., Al-Jayyousi K., Hefny A., Kiranyaz S.","57214653802;8964151000;36053012700;57216883687;57218098738;57214813088;7801632948;","An intelligent and low-cost eye-tracking system for motorized wheelchair control",2020,"Sensors (Switzerland)","20","14","3936","1","27",,7,"10.3390/s20143936","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087912991&doi=10.3390%2fs20143936&partnerID=40&md5=78582708851ec19bcd9e47bb9e1a1b23","School of Engineering, University of Maryland, College Park, MD  20742, United States; Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar; Department of Biomedical Physics and Technology, University of Dhaka, Dhaka, 1000, Bangladesh","Dahmani, M., School of Engineering, University of Maryland, College Park, MD  20742, United States; Chowdhury, M.E.H., Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar; Khandakar, A., Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar; Rahman, T., Department of Biomedical Physics and Technology, University of Dhaka, Dhaka, 1000, Bangladesh; Al-Jayyousi, K., Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar; Hefny, A., Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar; Kiranyaz, S., Department of Electrical Engineering, College of Engineering, Qatar University, Doha, 2713, Qatar","In the 34 developed and 156 developing countries, there are ~132 million disabled people who need a wheelchair, constituting 1.86% of the world population. Moreover, there are millions of people suffering from diseases related to motor disabilities, which cause inability to produce controlled movement in any of the limbs or even head. This paper proposes a system to aid people with motor disabilities by restoring their ability to move effectively and effortlessly without having to rely on others utilizing an eye-controlled electric wheelchair. The system input is images of the user’s eye that are processed to estimate the gaze direction and the wheelchair was moved accordingly. To accomplish such a feat, four user-specific methods were developed, implemented, and tested; all of which were based on a benchmark database created by the authors. The first three techniques were automatic, employ correlation, and were variants of template matching, whereas the last one uses convolutional neural networks (CNNs). Different metrics to quantitatively evaluate the performance of each algorithm in terms of accuracy and latency were computed and overall comparison is presented. CNN exhibited the best performance (i.e., 99.3% classification accuracy), and thus it was the model of choice for the gaze estimator, which commands the wheelchair motion. The system was evaluated carefully on eight subjects achieving 99% accuracy in changing illumination conditions outdoor and indoor. This required modifying a motorized wheelchair to adapt it to the predictions output by the gaze estimation algorithm. The wheelchair control can bypass any decision made by the gaze estimator and immediately halt its motion with the help of an array of proximity sensors, if the measured distance goes below a well-defined safety margin. This work not only empowers any immobile wheelchair user, but also provides low-cost tools for the organization assisting wheelchair users. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks (CNNs); Eye tracking; Machine learning; Motorized wheelchair; Ultrasonic proximity sensors","Convolutional neural networks; Costs; Developing countries; Eye movements; Template matching; Wheelchairs; Benchmark database; Classification accuracy; Electric wheelchair; Illumination conditions; Low cost eye tracking; Motor disability; Motorized wheelchairs; Wheelchair control; Eye tracking; algorithm; disabled person; human; wheelchair; Algorithms; Disabled Persons; Eye-Tracking Technology; Humans; Neural Networks, Computer; Wheelchairs",Article,"Final","",Scopus,2-s2.0-85087912991
"Dari S., Kadrileev N., Hullermeier E.","57211024441;57219548736;6701552637;","A Neural Network-Based Driver Gaze Classification System with Vehicle Signals",2020,"Proceedings of the International Joint Conference on Neural Networks",,,"9207709","","",,2,"10.1109/IJCNN48605.2020.9207709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093868851&doi=10.1109%2fIJCNN48605.2020.9207709&partnerID=40&md5=f538277641862acf6ee248b9d085dd55","BMW Goup, Research and Development, Munich, Germany; Technical University Munich, Department of Electrical Engineering, Munich, Germany; Technical University Munich, Department of Computer Science, Munich, Germany","Dari, S., BMW Goup, Research and Development, Munich, Germany; Kadrileev, N., Technical University Munich, Department of Electrical Engineering, Munich, Germany; Hullermeier, E., Technical University Munich, Department of Computer Science, Munich, Germany","Driver monitoring can play an essential part in avoiding accidents by warning the driver and shifting the driver's attention to the traffic scenery in time during critical situations. This may apply for the different levels of automated driving, for take-over requests as well as for driving in manual mode. A great proxy for this purpose has always been the driver's gazing direction. The aim of this work is to introduce a robust gaze detection system. In this regard, we make several contributions that are novel in the area of gaze detection systems. In particular, we propose a deep learning approach to predict gaze regions, which is based on informative features such as eye landmarks and head pose angles of the driver. Moreover, we introduce different post-processing techniques that improve the accuracy by exploiting temporal information from videos and the availability of other vehicle signals. Last but not least, we confirm our method with a leave-one-driver-out cross-validation. Unlike previous studies, we do not use gazes to predict maneuver changes, but we consider the human-computer-interaction aspect and use vehicle signals to improve the performance of the estimation. The proposed system is able to achieve an accuracy of 92.3% outperforming earlier landmark-based gaze estimators. © 2020 IEEE.","Autonomous Driving; Driver distraction; Driver Gaze Estimation; Driver Monitoring; Driver State Recognition; Naturalistic Driving Study","Behavioral research; Deep learning; Human computer interaction; Vehicles; Automated driving; Classification system; Cross validation; Driver monitoring; Gaze detection; Learning approach; Post-processing techniques; Temporal information; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85093868851
"Riad Saboundji R., Adrian Rill R.","57222514796;57191287116;","Predicting Human Errors from Gaze and Cursor Movements",2020,"Proceedings of the International Joint Conference on Neural Networks",,,"9207189","","",,,"10.1109/IJCNN48605.2020.9207189","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093849077&doi=10.1109%2fIJCNN48605.2020.9207189&partnerID=40&md5=f3db94bf08711a83fd24202293b7e95e","Eotvos Lorand University, Faculty of Informatics, Budapest, Hungary; Babes-Bolyai University, Faculty of Mathematics and Computer Science, Cluj-Napoca, Romania","Riad Saboundji, R., Eotvos Lorand University, Faculty of Informatics, Budapest, Hungary; Adrian Rill, R., Babes-Bolyai University, Faculty of Mathematics and Computer Science, Cluj-Napoca, Romania","Intelligent interfaces are increasingly integrated into diverse technological areas. In complex high-risk environments, where humans represent a crucial part of the system and their attention is often divided between simultaneous activities, imminent human errors may have serious consequences. Enhancing interfaces with predictive capabilities promotes the safe and reliable operation of such systems. In this work, we employ a data-driven approach to predict human errors in a special divided attention task involving timing constraints and requiring focused concentration and frequent shifts of attention. We performed a longitudinal study with 10 subjects, and constructed time series from the experimental data using gaze movement and mouse cursor motion features in order to classify successful and failed actions. We evaluate classical machine learning algorithms, compare them with a more traditional temporal modeling approach and a deep learning based LSTM model. Employing a leave-one- subject-out cross-validation procedure we achieve a classification accuracy of up to 86%, with LSTM presenting the highest performance. Furthermore, we investigate the trade-off between evaluation metrics and anticipation window, i.e. the time remaining until the correct action can still be performed. We conclude that prediction is feasible and accuracy and F1-score increases, despite the training dataset becoming greatly imbalanced. Investigating the anticipation window allows to understand how far in advance human errors need to be predicted in order to initiate preventive measures. Our efforts have implications for the design of predictive interfaces involving decision making under time pressure in dynamic divided attention environments. © 2020 IEEE.","anticipation window; gaze tracking; human error; LSTM; predictive interface; time series","Decision making; Deep learning; Economic and social effects; Errors; Forecasting; Learning algorithms; Long short-term memory; Mammals; Classification accuracy; Data-driven approach; Evaluation metrics; High risk environment; Intelligent interface; Longitudinal study; Predictive capabilities; Preventive measures; Time and motion study",Conference Paper,"Final","",Scopus,2-s2.0-85093849077
"Cazzato D., Carcagnì P., Cimarelli C., Voos H., Distante C., Leo M.","55866556300;23003296600;57211170210;6603280387;55884135100;7006471658;","Ocular biometrics recognition by analyzing human exploration during video observations",2020,"Applied Sciences (Switzerland)","10","13","4548","","",,,"10.3390/app10134548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087826465&doi=10.3390%2fapp10134548&partnerID=40&md5=c4f51f18ba320ac3ed14a1d8523cd481","Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, 29, Avenue J. F. Kennedy, Luxembourg, 1855, Luxembourg; Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Via Monteroni snc, Lecce, 73100, Italy","Cazzato, D., Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, 29, Avenue J. F. Kennedy, Luxembourg, 1855, Luxembourg; Carcagnì, P., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Via Monteroni snc, Lecce, 73100, Italy; Cimarelli, C., Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, 29, Avenue J. F. Kennedy, Luxembourg, 1855, Luxembourg; Voos, H., Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, 29, Avenue J. F. Kennedy, Luxembourg, 1855, Luxembourg; Distante, C., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Via Monteroni snc, Lecce, 73100, Italy; Leo, M., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Via Monteroni snc, Lecce, 73100, Italy","Soft biometrics provide information about the individual but without the distinctiveness and permanence able to discriminate between any two individuals. Since the gaze represents one of the most investigated human traits, works evaluating the feasibility of considering it as a possible additional soft biometric trait have been recently appeared in the literature. Unfortunately, there is a lack of systematic studies on clinically approved stimuli to provide evidence of the correlation between exploratory paths and individual identities in ""natural"" scenarios (without calibration, imposed constraints, wearable tools). To overcome these drawbacks, this paper analyzes gaze patterns by using a computer vision based pipeline in order to prove the correlation between visual exploration and user identity. This correlation is robustly computed in a free exploration scenario, not biased by wearable devices nor constrained to a prior personalized calibration. Provided stimuli have been designed by clinical experts and then they allow better analysis of human exploration behaviors. In addition, the paper introduces a novel public dataset that provides, for the first time, images framing the faces of the involved subjects instead of only their gaze tracks. © 2020 by the authors.","Gaze estimation; Human attention recognition; Public gaze dataset; Soft biometrics",,Article,"Final","",Scopus,2-s2.0-85087826465
"Kim H., Ohmura Y., Kuniyoshi Y.","57217147980;7006439860;7005371269;","Using Human Gaze to Improve Robustness against Irrelevant Objects in Robot Manipulation Tasks",2020,"IEEE Robotics and Automation Letters","5","3","9103290","4415","4422",,1,"10.1109/LRA.2020.2998410","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086446787&doi=10.1109%2fLRA.2020.2998410&partnerID=40&md5=69b51097c9866b61f4d0f71c21f95757","Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, University of Tokyo, Tokyo, Japan","Kim, H., Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, University of Tokyo, Tokyo, Japan; Ohmura, Y., Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, University of Tokyo, Tokyo, Japan; Kuniyoshi, Y., Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, University of Tokyo, Tokyo, Japan","Deep imitation learning enables the learning of complex visuomotor skills from raw pixel inputs. However, this approach suffers from the problem of overfitting to the training images. The neural network can easily be distracted by task-irrelevant objects. In this letter, we use the human gaze measured by a head-mounted eye tracking device to discard task-irrelevant visual distractions. We propose a mixture density network-based behavior cloning method that learns to imitate the human gaze. The model predicts gaze positions from raw pixel images and crops images around the predicted gazes. Only these cropped images are used to compute the output action. This cropping procedure can remove visual distractions because the gaze is rarely fixated on task-irrelevant objects. This robustness against irrelevant objects can improve the manipulation performance of robots in scenarios where task-irrelevant objects are present. We evaluated our model on four manipulation tasks designed to test the robustness of the model to irrelevant objects. The results indicate that the proposed model can predict the locations of task-relevant objects from gaze positions, is robust to task-irrelevant objects, and exhibits impressive manipulation performance especially in multi-object handling. © 2016 IEEE.","computer vision for automation; Deep learning in grasping and manipulation; learning from demonstration; telerobotics and teleoperation; visual servoing","Agricultural robots; Clone cells; Deep learning; Pixels; Head-mounted eye tracking; Imitation learning; Manipulation task; Mixture density; Robot manipulation; Task relevant; Training image; Visual distractions; Eye tracking",Article,"Final","",Scopus,2-s2.0-85086446787
"Cha X., Yang X., Zhang Y., Feng Z., Xu T., Fan X.","57214798390;55683790100;57219185145;7403443516;56683649700;57197729874;","Eye Tracking in Driving Environment Based on Multichannel Convolutional Neural Network",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"141","144",,,"10.1145/3408127.3408179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091591611&doi=10.1145%2f3408127.3408179&partnerID=40&md5=0cb1e0445980a08f9b7f5b983c4561a9","School of Information Science and Engineering, University of Jinan, Jinan, China; Shandong Provincial Key Laboratory of Network Based Intelligent Computing, Jinan, China","Cha, X., School of Information Science and Engineering, University of Jinan, Jinan, China; Yang, X., School of Information Science and Engineering, University of Jinan, Jinan, China; Zhang, Y., School of Information Science and Engineering, University of Jinan, Jinan, China; Feng, Z., Shandong Provincial Key Laboratory of Network Based Intelligent Computing, Jinan, China; Xu, T., School of Information Science and Engineering, University of Jinan, Jinan, China; Fan, X., School of Information Science and Engineering, University of Jinan, Jinan, China","Gaze is the most important way for human to obtain information from the outside world, and it is the most direct and significant cue to analysis human behavior and intention. In driving environment, eye tracking is usually applied to model driver's fixations and gaze allocations, which is important in advanced driver assistance system (ADAS). In this paper, we have proposed a new eye tracking method in driving environment, which is based on multichannel convolutional neural network. Firstly, we establish the dataset for driver's eye tracking, which includes the left eye region image, the right eye region image and the face region image. After that, the multi-channel convolutional neural network is training using the dataset. Finally, the driver's gaze zone will be estimated using the pre-trained network. Experimental results show that the accuracy of the proposed method is 94.60% for seven gaze zone estimation, and it can be used in ADAS to analysis the driver's behavior and detect driver distraction. © 2020 ACM.","advanced driver assistance system; Deep learning; driver distraction; eye tracking; gaze zone estimation","Advanced driver assistance systems; Automobile drivers; Behavioral research; Convolution; Convolutional neural networks; Digital signal processing; Driver distractions; Driver's behavior; Driving environment; Eye tracking methods; Face regions; Human behaviors; Model drivers; Multichannel; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85091591611
"Song H., Yang M., Li T., Chen S.","57219178937;57210466465;57210468511;24491760700;","Fixation Points Estimation Based on Binocular Stereo Vision",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"71","74",,,"10.1145/3408127.3408198","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091575432&doi=10.1145%2f3408127.3408198&partnerID=40&md5=2e058f149a077d566bccdde7ae813d58","School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China","Song, H., School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; Yang, M., School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; Li, T., School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China; Chen, S., School of Computer Science and Engineering, Tianjin University of Technology, Tianjin, China","This paper proposes a method of fixation points estimation combining the two-dimension mapping model with three-dimension stereo vision. The purpose of our study is to design an easy-to-use non-contact gaze tracking system under natural light. The method of two-dimension mapping is easy to achieve, however, it needs the user to keep the head being fixed. It could achieve higher estimation accuracy though it is still not easy for users to use the algorithm. To solve this problem, we have introduced the binocular cameras to calculate the pose of head and then add the related result into the result of 2D mapping to compensate the movement of head. The average error angles of gaze estimation in the case head fixed and head moving are 5.1°and 8.5°, respectively. This proposed method is easy to achieve and the experiment device is easy to mounted. © 2020 ACM.","binocular cameras; estimation of fixation points; Gaze estimation; Iris center location; Stereo imaging","Digital signal processing; Eye tracking; Mapping; Stereo image processing; Average errors; Binocular camera; Binocular stereo vision; Fixation point; Gaze estimation; Gaze tracking system; Three dimensions; Two-dimension; Stereo vision",Conference Paper,"Final","",Scopus,2-s2.0-85091575432
"John B., Liu A., Xia L., Koppal S., Jain E.","57205639875;57210118926;23011050500;14039289000;36715118000;","Let It Snow: Adding pixel noise to protect the user's identity",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3390512","","",,1,"10.1145/3379157.3390512","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086309897&doi=10.1145%2f3379157.3390512&partnerID=40&md5=63f421a4f6c8bfe191fb53ba866ab847","University of Florida, United States; Rensselaer Polytechnic Institute","John, B., University of Florida, United States; Liu, A., Rensselaer Polytechnic Institute; Xia, L., Rensselaer Polytechnic Institute; Koppal, S., University of Florida, United States; Jain, E., University of Florida, United States","Optical eye trackers record images of the eye to estimate the gaze direction. These images contain the iris of the user. While useful for authentication, these images can be used for a spoofing attack if stolen. We propose to use pixel noise to break the iris signature while retaining gaze estimation. In this paper, we present an algorithm to add ""snow"" to the eye image and evaluate the privacy-utility tradeoff for the choice of noise parameter. © 2020 ACM.","Eye Tracking; Iris Recognition; Privacy","Pixels; Snow; Eye images; Eye trackers; Gaze direction; Gaze estimation; Noise parameters; Pixel noise; Spoofing attacks; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086309897
"Saad A., Elkafrawy D.H., Abdennadher S., Schneegass S.","57205427603;57217114539;55970641400;55317907700;","Are They Actually Looking? Identifying Smartphones Shoulder Surfing Through Gaze Estimation",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3391422","","",,,"10.1145/3379157.3391422","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086304946&doi=10.1145%2f3379157.3391422&partnerID=40&md5=0ea245fc3e7630a48cb0d55077d500e5","University of Duisburg, Essen, Germany; German University, Cairo, Egypt","Saad, A., University of Duisburg, Essen, Germany; Elkafrawy, D.H., German University, Cairo, Egypt; Abdennadher, S., German University, Cairo, Egypt; Schneegass, S., University of Duisburg, Essen, Germany","Mobile devices have evolved to be a crucial part of our everyday lives. However, they are subject to different types of user-centered attacks such as shoulder surfing attacks. Previous work focused on notifying the user with a potential shoulder surfer if an extra face is detected. Although it is a successful approach, it eliminated the possibility that the alleged attacker is just standing and not looking at the user's device. In this work, we investigate estimating the gaze of potential attackers in order to verify if they are indeed looking at the user's phone. © 2020 Owner/Author.","Gaze Estimation; Shoulder Surfing; Usable Security","Gaze estimation; Shoulder surfing; User-centered; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086304946
"Kim J.-H., Jeong J.-W.","57205720618;57205722835;","Gaze Estimation in the Dark with Generative Adversarial Networks",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3391654","","",,,"10.1145/3379157.3391654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086303847&doi=10.1145%2f3379157.3391654&partnerID=40&md5=5647fe5a5546555b6c69055260bf2e2b","Kumoh National Institute of Technology, South Korea","Kim, J.-H., Kumoh National Institute of Technology, South Korea; Jeong, J.-W., Kumoh National Institute of Technology, South Korea","In this paper, we propose to utilize generative adversarial networks (GANs) to achieve successful gaze estimation in interactive multimedia environments with low light conditions such as a digital museum or exhibition hall. The proposed approach utilizes a GAN to enhance user images captured under low-light conditions, thereby recovering missing information for gaze estimation. The recovered images are fed into the CNN architecture to estimate the direction of user gaze. The preliminary experimental results on the modified MPIIGaze dataset demonstrated an average performance improvement of 6.6 under various low light conditions, which is a promising step for further research. © 2020 ACM.","deep learning; GAN; Gaze estimation; low-light environment","Exhibition buildings; Image enhancement; Interactive computer systems; Multimedia systems; Adversarial networks; Digital museums; Exhibition halls; Gaze estimation; Interactive multimedia; Low light conditions; Missing information; User images; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086303847
"Nair N., Chaudhary A.K., Kothari R.S., Diaz G.J., Pelz J.B., Bailey R.","57215969974;57210103548;56533410500;55436582600;7007018556;16641965200;","RIT-Eyes: Realistically rendered eye images for eye-tracking applications",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3391990","","",,1,"10.1145/3379157.3391990","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086303721&doi=10.1145%2f3379157.3391990&partnerID=40&md5=3dedebcff3d7d8764fa8501b66aa21bd","Rochester Institute of Technology, Rochester, NY, United States","Nair, N., Rochester Institute of Technology, Rochester, NY, United States; Chaudhary, A.K., Rochester Institute of Technology, Rochester, NY, United States; Kothari, R.S., Rochester Institute of Technology, Rochester, NY, United States; Diaz, G.J., Rochester Institute of Technology, Rochester, NY, United States; Pelz, J.B., Rochester Institute of Technology, Rochester, NY, United States; Bailey, R., Rochester Institute of Technology, Rochester, NY, United States","Convolutional neural network-based solutions for video oculography require large quantities of accurately labeled eye images acquired under a wide range of image quality, surrounding environmental reflections, feature occlusion, and varying gaze orientations. Manually annotating such a dataset is challenging, time-consuming, and error-prone. To alleviate these limitations, this work introduces an improved eye image rendering pipeline designed in Blender. RIT-Eyes provides access to realistic eye imagery with error-free annotations in 2D and 3D which can be used for developing gaze estimation algorithms. Furthermore, RIT-Eyes is capable of generating novel temporal sequences with realistic blinks and mimicking eye and head movements derived from publicly available datasets. © 2020 Owner/Author.","computer graphics; Datasets; eyetracking; rendering; segmentation","Convolutional neural networks; Eye movements; Image enhancement; Rendering (computer graphics); Error prones; Eye images; Gaze estimation; Head movements; Temporal sequences; Video oculography; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086303721
"Li Y., Zhan Y., Yang Z.","57205654781;14030938900;57203791621;","Evaluation of appearance-based eye tracking calibration data selection",2020,"Proceedings of 2020 IEEE International Conference on Artificial Intelligence and Computer Applications, ICAICA 2020",,,"9181854","222","224",,1,"10.1109/ICAICA50127.2020.9181854","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092146609&doi=10.1109%2fICAICA50127.2020.9181854&partnerID=40&md5=37d95106606ab91f001382339f7866c9","School of Computers, Guangdong University of Technology, Guangzhou, China","Li, Y., School of Computers, Guangdong University of Technology, Guangzhou, China; Zhan, Y., School of Computers, Guangdong University of Technology, Guangzhou, China; Yang, Z., School of Computers, Guangdong University of Technology, Guangzhou, China","Eye tracking is a valuable topic in computer vision. Appearance-based eye tracking is a promising research direction in recent years. Convolutional neural networks (CNN) had been used in gaze estimation, which cover the significant variability in eye appearance caused by unconstrained head motion. With computation capability of consumer devices rapidly evolving, accurate and efficient appearance-based eye tracking has the potential for multipurpose applications. Person-independent networks have limit in improving gaze estimation accuracy. Person-specific network with calibration is more effective than person-independent approaches. Unlike classical eye tracking methods, appearance-based eye tracking has not a clear way to calibration. Our goal is to analyze the impact of calibration data selection and calibration target distribution on person-specific gaze estimation accuracy. We trained person-independent network and use SVR to calibration. We choose two kind of typical distribution targets to evaluation. Use different distribution targets to calibration achieves different accuracy. © 2020 IEEE.","calibration; eye tracking; gaze; human computer interaction","Calibration; Convolutional neural networks; Data reduction; Appearance based; Calibration data; Calibration targets; Consumer devices; Different distributions; Eye tracking methods; Gaze estimation; Person-independent; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85092146609
"Mahanama B., Jayawardana Y., Jayarathna S.","57206891362;57206898577;36052654200;","Gaze-Net: Appearance-based gaze estimation using capsule networks",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"","",,,"10.1145/3396339.3396393","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117542848&doi=10.1145%2f3396339.3396393&partnerID=40&md5=b2c2c782cf2a196a741cf99897d3df9a","Department of Computer Science, Old Dominion University, Norfolk, VA  23529, United States","Mahanama, B., Department of Computer Science, Old Dominion University, Norfolk, VA  23529, United States; Jayawardana, Y., Department of Computer Science, Old Dominion University, Norfolk, VA  23529, United States; Jayarathna, S., Department of Computer Science, Old Dominion University, Norfolk, VA  23529, United States","Recent studies on appearance based gaze estimation indicate the ability of Neural Networks to decode gaze information from facial images encompassing pose information. In this paper, we propose Gaze-Net: A capsule network capable of decoding, representing, and estimating gaze information from ocular region images. We evaluate our proposed system using two publicly available datasets, MPIIGaze (200,000+ images in the wild) and Columbia Gaze (5000+ images of users with 21 gaze directions observed at 5 camera angles/positions). Our model achieves a Mean Absolute Error (MAE) of 2.84° for Combined angle error estimate within dataset for MPIIGaze dataset. Further, model achieves a MAE of 10.04° for across dataset gaze estimation error for Columbia gaze dataset. Through transfer learning, the error is reduced to 5.9°. The results show this approach is promising with implications towards using commodity webcams to develop low-cost multi-user gaze tracking systems. © 2020 Association for Computing Machinery.","Capsule networks; Deep learning; Gaze estimation; Gaze tracking; Transfer learning","Decoding; Errors; Transfer learning; Appearance based; Camera angles; Facial images; Gaze direction; Gaze estimation; Gaze tracking system; Mean absolute error; Pose information; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85117542848
"Zhou X., Lin J., Zhang Z., Shao Z., Chen S., Liu H.","55743240400;57210574380;57191632915;55921729700;57192606393;54958434200;","Improved itracker combined with bidirectional long short-term memory for 3D gaze estimation using appearance cues",2020,"Neurocomputing","390",,,"217","225",,2,"10.1016/j.neucom.2019.04.099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074493405&doi=10.1016%2fj.neucom.2019.04.099&partnerID=40&md5=0eb3cdfe3754fcc13e8cbf14b40f297b","Zhejiang University of Technology, Hangzhou, 210023, China; Quzhou University, Quzhou, 324000, China; Tianjin University of Technology, Tianjin, 300384, China; University of Portsmouth, Portsmouth, United Kingdom","Zhou, X., Zhejiang University of Technology, Hangzhou, 210023, China, Quzhou University, Quzhou, 324000, China; Lin, J., Zhejiang University of Technology, Hangzhou, 210023, China; Zhang, Z., Zhejiang University of Technology, Hangzhou, 210023, China; Shao, Z., Zhejiang University of Technology, Hangzhou, 210023, China; Chen, S., Zhejiang University of Technology, Hangzhou, 210023, China, Tianjin University of Technology, Tianjin, 300384, China; Liu, H., University of Portsmouth, Portsmouth, United Kingdom","Gaze is an important non-verbal cue for speculating human's attention, which has been widely employed in many human–computer interaction-based applications. In this paper, we propose an improved Itracker to predict the subject's gaze for a single image frame, as well as employ a many-to-one bidirectional Long Short-Term Memory (bi-LSTM) to fit the temporal information between frames to estimate gaze for video sequence. For single image frame gaze estimation, we improve the conventional Itracker by removing the face-grid and reducing one network branch via concatenating the two-eye region images. Experimental results show that our improved Itracker obtains 11.6% significant improvement over the state-of-the-art methods on MPIIGaze dataset and has robust estimation accuracy for different image resolutions under the premise of greatly reducing network complexity. For video sequence gaze estimation, by employing the bi-LSTM to fit the temporal information between frames, experimental results on EyeDiap dataset further demonstrate 3% accuracy improvement. © 2019","CNN; Gaze estimation; LSTM; RNN","Brain; Human computer interaction; Image enhancement; Image resolution; Video recording; Accuracy Improvement; Computer interaction; Gaze estimation; LSTM; Network complexity; Robust estimation; State-of-the-art methods; Temporal information; Long short-term memory; article; gaze; human; human experiment; short term memory; videorecording",Article,"Final","",Scopus,2-s2.0-85074493405
"Li J., Zhong Y., Han J., Ouyang G., Li X., Liu H.","56272795400;57205359088;57051695600;7005498402;57192497946;54958434200;","Classifying ASD children with LSTM based on raw videos",2020,"Neurocomputing","390",,,"226","238",,9,"10.1016/j.neucom.2019.05.106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074363799&doi=10.1016%2fj.neucom.2019.05.106&partnerID=40&md5=db9933d6d92b05cf6e69f760315ab3fa","School of Information Engineering, Nanchang University, Nanchang, 330031, China; State Key Laboratory of Cognitive Neuroscience and Learning & IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, 100875, China; State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China","Li, J., School of Information Engineering, Nanchang University, Nanchang, 330031, China; Zhong, Y., School of Information Engineering, Nanchang University, Nanchang, 330031, China; Han, J., State Key Laboratory of Cognitive Neuroscience and Learning & IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, 100875, China; Ouyang, G., State Key Laboratory of Cognitive Neuroscience and Learning & IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, 100875, China; Li, X., State Key Laboratory of Cognitive Neuroscience and Learning & IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, 100875, China; Liu, H., State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China","Autism spectrum disorder (ASD) is a serious neurodevelopmental disorder that impairs a child's ability to communicate and interact with others. Usually, recognizing a child with ASD needs the diagnosis by professional doctors. However, it is not only expensive and time-consuming, but also the results are influenced by subjective factors, such as the experience of a doctor. Recently, some methods which identify ASD based on biomarkers have been developed, but there are rarely works specific to raw video data. This paper is the first attempt to help diagnose the children with ASD in raw video data using a deep learning technique. Firstly, in order to investigate different gaze patterns between ASD children and typically developing (TD) children, we track the eye movement in each video by the tracking-learning-detection method. Secondly, we divide these tracking trajectories into two components: (i) the length; and (ii) the angle. Afterwards, we calculate an accumulative histogram for each component. Finally, we adopt three-layer Long Short-Term Memory (LSTM) network for classification. Experimental results on our extended dataset (Ext-Dataset) containing 272 videos captured from 136 ASD children and 136 TD children show the LSTM network outperforms the traditional machine learning methods, e.g., Support Vector Machine, with the improvement of accuracy by 6.2% (from 86.4% to 92.6%). Especially, for ASD, we obtain the sensitivity (the true positive rate, TPR) of 91.9% and the specificity (the true negative rate, TNR) of 93.4%, which demonstrates the effectiveness of our method. © 2019","Accumulative histogram; Autism spectrum disorder; Deep learning; Long Short-Term Memory (LSTM); Tracking-learning-detection (TLD)","Brain; Deep learning; Diseases; Eye movements; Graphic methods; Support vector machines; Video recording; Accumulative histogram; Autism spectrum disorders; Detection methods; Learning techniques; Machine learning methods; Tracking trajectory; True negative rates; True positive rates; Long short-term memory; Article; autism; child; deep learning; eye tracking; feature extraction; gaze; human; long short term memory; major clinical study; memory; priority journal; sensitivity and specificity; support vector machine",Article,"Final","",Scopus,2-s2.0-85074363799
"Fu X., Yan Y., Yan Y., Peng J., Wang H.","7402204912;57204644440;57214760249;57192708626;55986542800;","Purifying real images with an attention-guided style transfer network for gaze estimation",2020,"Engineering Applications of Artificial Intelligence","91",,"103609","","",,,"10.1016/j.engappai.2020.103609","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082627543&doi=10.1016%2fj.engappai.2020.103609&partnerID=40&md5=adfa39ee6b781789143fc0e1e46db9e8","Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Pengcheng Laboratory, Shenzhen, 518055, China","Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China, Pengcheng Laboratory, Shenzhen, 518055, China; Yan, Y., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Yan, Y., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Peng, J., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Wang, H., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. Image synthesis has been widely accepted as a cost effective way to learn models because it provides training sets that are large, diverse and accurately labeled. However, the realism of the synthetic image is not enough, this affects generalization on naturalistic test image. In an attempt to address this issue, previous methods learn a model to improve the realism of synthetic image. Different from previous methods, we take the first step towards purifying the real image to weaken the influence of light and convert the distribution of an outdoor naturalistic image through a real-time style transfer task to that of indoor synthetic image. In this paper, we first introduce the segmentation masks to construct Red, Green, and Blue-mask (RGB-mask) pairs as inputs, then we design an attention-guided style transfer network to learn style features separately from the attention and background regions, learn content features from full and attention regions. Moreover, we propose a novel region-level task-guided loss to restrain the features learnt from style and content. Experiments were performed using a mixed research (qualitative and quantitative) method to demonstrate the possibility of purifying real images in complex directions. We evaluate the proposed method on three public datasets, including Labeled pupils in the wild (LPW), Common Objects in COntext (COCO) and MPIIGaze. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results. © 2020 Elsevier Ltd","Attention-guided style transfer network; Gaze estimation; Learning-by-synthesis; Style transfer","Cost effectiveness; Gallium compounds; Transfer learning; Background region; Complex directions; Gaze estimation; Material resources; Segmentation masks; Style and contents; Style transfer; Transfer network; Image enhancement",Article,"Final","",Scopus,2-s2.0-85082627543
"Morimoto C.H., Coutinho F.L., Hansen D.W.","7102275798;22233254400;15063910800;","Screen-Light Decomposition Framework for Point-of-Gaze Estimation Using a Single Uncalibrated Camera and Multiple Light Sources",2020,"Journal of Mathematical Imaging and Vision","62","4",,"585","605",,1,"10.1007/s10851-020-00947-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079709699&doi=10.1007%2fs10851-020-00947-8&partnerID=40&md5=3b9d26ba4449b858eb50f1802de46566","University of São Paulo, São Paulo, Brazil; IT University, Copenhagen, Denmark","Morimoto, C.H., University of São Paulo, São Paulo, Brazil; Coutinho, F.L., University of São Paulo, São Paulo, Brazil; Hansen, D.W., IT University, Copenhagen, Denmark","The use of a single uncalibrated camera is desirable for eye tracking to reduce the overall complexity and cost of the system. Quite often, at least one external light source is used to enhance image quality and generate a corneal reflection used as a reference point to estimate the point-of-gaze (PoG). Though the use of more than one light source has shown to enhance accuracy and robustness to head motion, it is unlikely that all corneal reflections appear in the eye images during natural eye movements. In this paper, we introduce the Screen-Light Decomposition (SLD) framework as a generalized model for PoG estimation using a single uncalibrated camera and a variable number of light sources. SLD synthesizes existing uncalibrated video-based eye trackers and can be used as a modeling tool to compare and design eye trackers. We have used the framework to design a novel eye-tracking technique, called SAGE, for single normalized space adaptive gaze estimation, that can gracefully degrade the gaze tracker performance when one or more corneal reflections are not detected, even during the calibration procedure. Results from an user experiment are presented to demonstrate its improved performance over other designs. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Adaptive; Calibration; Estimation; eye; Framework; Normalized; Space; Tracking","Calibration; Cameras; Estimation; Eye movements; Image enhancement; Light sources; Surface discharges; Adaptive; Calibration procedure; Framework; Generalized models; Multiple light source; Normalized; Space; Un-calibrated camera; Eye tracking",Article,"Final","",Scopus,2-s2.0-85079709699
"Erickson A., Norouzi N., Kim K., LaViola J.J., Jr., Bruder G., Welch G.F.","57212551904;57204712431;56159628700;6602792780;23391698600;35572266400;","Effects of Depth Information on Visual Target Identification Task Performance in Shared Gaze Environments",2020,"IEEE Transactions on Visualization and Computer Graphics","26","5","8998348","1934","1944",,2,"10.1109/TVCG.2020.2973054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079619388&doi=10.1109%2fTVCG.2020.2973054&partnerID=40&md5=36d37536c7c731fba2bfbb35eb3b162b","University of Central Florida, United States","Erickson, A., University of Central Florida, United States; Norouzi, N., University of Central Florida, United States; Kim, K., University of Central Florida, United States; LaViola, J.J., Jr., University of Central Florida, United States; Bruder, G., University of Central Florida, United States; Welch, G.F., University of Central Florida, United States","Human gaze awareness is important for social and collaborative interactions. Recent technological advances in augmented reality (AR) displays and sensors provide us with the means to extend collaborative spaces with real-time dynamic AR indicators of one's gaze, for example via three-dimensional cursors or rays emanating from a partner's head. However, such gaze cues are only as useful as the quality of the underlying gaze estimation and the accuracy of the display mechanism. Depending on the type of the visualization, and the characteristics of the errors, AR gaze cues could either enhance or interfere with collaborations. In this paper, we present two human-subject studies in which we investigate the influence of angular and depth errors, target distance, and the type of gaze visualization on participants' performance and subjective evaluation during a collaborative task with a virtual human partner, where participants identified targets within a dynamically walking crowd. First, our results show that there is a significant difference in performance for the two gaze visualizations ray and cursor in conditions with simulated angular and depth errors: the ray visualization provided significantly faster response times and fewer errors compared to the cursor visualization. Second, our results show that under optimal conditions, among four different gaze visualization methods, a ray without depth information provides the worst performance and is rated lowest, while a combination of a ray and cursor with depth information is rated highest. We discuss the subjective and objective performance thresholds and provide guidelines for practitioners in this field. © 2020 IEEE.","Augmented Reality; Depth Error; Gaze Visualization; Performance Measures; Shared Gaze","Augmented reality; Errors; Flow visualization; Interactive computer systems; Job analysis; Real time systems; Three dimensional computer graphics; Three dimensional displays; Virtual reality; Visualization; Collaboration; Depth errors; Gaze tracking; Performance measure; Shared gazes; Task analysis; Eye tracking; adolescent; adult; computer graphics; eye fixation; female; human; male; physiology; task performance; young adult; Adolescent; Adult; Augmented Reality; Computer Graphics; Eye-Tracking Technology; Female; Fixation, Ocular; Humans; Male; Task Performance and Analysis; Young Adult",Article,"Final","",Scopus,2-s2.0-85079619388
"Sattar H., Fritz M., Bulling A.","57142145400;14035495500;6505807414;","Deep gaze pooling: Inferring and visually decoding search intents from human gaze fixations",2020,"Neurocomputing","387",,,"369","382",,3,"10.1016/j.neucom.2020.01.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078159922&doi=10.1016%2fj.neucom.2020.01.028&partnerID=40&md5=21e31e745d23f92eaa3ab52685999d3e","Max-Planck-Institut für Informatik, Saarland Informatics Campus, Campus E1 4Saarbrücken  66123, Germany; CISPA Helmholtz Center for Information Security, Saarland Informatics Campus, Stuhlsatzenhaus 5Saarbrücken  66123, Germany; Institute for Visualisation and Interactive Systems, University of Stuttgart, Pfaffenwaldring 5a, Stuttgart, 70569, Germany","Sattar, H., Max-Planck-Institut für Informatik, Saarland Informatics Campus, Campus E1 4Saarbrücken  66123, Germany; Fritz, M., CISPA Helmholtz Center for Information Security, Saarland Informatics Campus, Stuhlsatzenhaus 5Saarbrücken  66123, Germany; Bulling, A., Institute for Visualisation and Interactive Systems, University of Stuttgart, Pfaffenwaldring 5a, Stuttgart, 70569, Germany","Predicting the target of visual search from human eye fixations (gaze) is a difficult problem with many applications, e.g. in human-computer interaction. While previous work has focused on predicting specific search target instances, we propose the first approach to predict categories and attributes of search intents from gaze data and to visually reconstruct plausible targets. However, state-of-the-art models for categorical recognition, in general, require large amounts of training data, which is prohibitive for gaze data. To address this challenge, we further propose a novel Gaze Pooling Layer that combines gaze information with visual representations from Deep Learning approaches. Our scheme incorporates both spatial and temporal aspects of human gaze behavior as well as the appearance of the fixated locations. We propose an experimental setup and novel dataset and demonstrate the effectiveness of our method for gaze-based search target prediction and reconstruction. We highlight several practical advantages of our approach, such as compatibility with existing architectures, no need for gaze training data, and robustness to noise from common gaze sources. © 2020 Elsevier B.V.","Deep learning; Gaze pooling; Mental image; Visual search; Visual search target prediction; Visual search target reconstruction","Behavioral research; Data visualization; Forecasting; Human computer interaction; Existing architectures; Gaze pooling; Learning approach; Mental images; Robustness to noise; Target prediction; Visual representations; Visual search; Deep learning; article; deep learning; gaze; human; human experiment; noise; prediction",Article,"Final","",Scopus,2-s2.0-85078159922
"Kim J.-H., Jeong J.-W.","57205720618;57205722835;","A preliminary study on performance evaluation of multi-view multi-modal gaze estimation under challenging conditions",2020,"Conference on Human Factors in Computing Systems - Proceedings",,,"3382856","","",,1,"10.1145/3334480.3382856","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090036435&doi=10.1145%2f3334480.3382856&partnerID=40&md5=dd0b8416b7fc58fe17f11ab050c9f875","Kumoh National Institute of Technology, Gumi, South Korea","Kim, J.-H., Kumoh National Institute of Technology, Gumi, South Korea; Jeong, J.-W., Kumoh National Institute of Technology, Gumi, South Korea","In this paper, we address gaze estimation under practical and challenging conditions. Multi-view and multi-modal learning have been considered useful for various complex tasks; however, an in-depth analysis or a large-scale dataset on multi-view, multi-modal gaze estimation under a long-distance setup with a low illumination is still very limited. To address these limitations, first, we construct a dataset of images captured under challenging conditions. And we propose a simple deep learning architecture that can handle multi-view multi-modal data for gaze estimation. Finally, we conduct a performance evaluation of the proposed network with the constructed dataset to understand the effects of multiple views of a user and multi-modality (RGB, depth, and infrared). We report various findings from our preliminary experimental results and expect this would be helpful for gaze estimation studies to deal with challenging conditions. © 2020 Owner/Author.","Deep neural networks; Gaze estimation; Multi-modal interaction; Multi-view learning","Deep learning; Human engineering; Large dataset; Gaze estimation; In-depth analysis; Large-scale dataset; Learning architectures; Low illuminations; Multi-modal data; Multi-modal learning; Multiple views; Modal analysis",Conference Paper,"Final","",Scopus,2-s2.0-85090036435
"Xu L., Chi J.","57218290275;8702376200;","3D eye model-based gaze tracking system with a consumer depth camera",2020,"Proceedings - 2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2020",,,"9131201","293","297",,,"10.1109/AEMCSE50948.2020.00070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088635925&doi=10.1109%2fAEMCSE50948.2020.00070&partnerID=40&md5=89ea83507a80dd0583b4bc727aec88fd","University of Science and Technology, China","Xu, L., University of Science and Technology, China; Chi, J., University of Science and Technology, China","Most existing gaze tracking systems are high-cost, intrusive and difficult to calibrate, and some rely on the infrared illuminant. However, such systems may not work outdoor and meet real-time requirements. This paper proposes a non-intrusive system based on the 3D eyeball model, which does not need the exact infrared illuminant and complicated calibration process and allows the natural movement of the head. In the proposed system, Kinect is used to track the iris center and face model of the person, and the 3D information is easy to obtain. At the same time, point cloud registration algorithm is applied based on feature points in the face model sequence to obtain accurate head pose estimation results. In this paper, a personal calibration process is also proposed to obtain the gaze model parameters for different users, such as the eyeball center and angle kappa. The proposed method has good adaptability to the change of illuminant and head movement. In the actual operating environment, the system speed reaches 30 fps, which can meet the requirements of real-time control. © 2020 IEEE.","Eye model-based; Gaze estimation; Personal calibration; Rgb-d camera","3D modeling; Calibration; Real time control; Software engineering; Calibration process; Gaze tracking system; Head Pose Estimation; Model parameters; Natural movements; Operating environment; Point cloud registration; Real time requirement; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85088635925
"Bace M., Staal S., Bulling A.","44461063200;57194012922;6505807414;","How Far Are We from Quantifying Visual Attention in Mobile HCI?",2020,"IEEE Pervasive Computing","19","2","9052000","46","55",,1,"10.1109/MPRV.2020.2967736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083007338&doi=10.1109%2fMPRV.2020.2967736&partnerID=40&md5=467028a65d442f4f0e07e587111d7a41","Department of Computer Science, ETH Zurich, Switzerland; Institute for Visualisation and Interactive SystemsUniversity of Stuttgart, Germany","Bace, M., Department of Computer Science, ETH Zurich, Switzerland; Staal, S., Department of Computer Science, ETH Zurich, Switzerland; Bulling, A., Institute for Visualisation and Interactive SystemsUniversity of Stuttgart, Germany","With an ever-increasing number of mobile devices competing for attention, quantifying when, how often, or for how long users look at their devices has emerged as a key challenge in mobile human-computer interaction. Encouraged by recent advances in automatic eye contact detection using machine learning and device-integrated cameras, we provide a fundamental investigation into the feasibility of quantifying overt visual attention during everyday mobile interactions. In this article, we discuss the main challenges and sources of error associated with sensing visual attention on mobile devices in the wild, including the impact of face and eye visibility, the importance of robust head poses estimation, and the need for accurate gaze estimation. Our analysis informs future research on this emerging topic and underlines the potential of eye contact detection for exciting new applications toward next-generation pervasive attentive user interfaces. © 2002-2012 IEEE.",,"Human computer interaction; User interfaces; Attentive user interfaces; Eye visibilities; Integrated cameras; Mobile human computer interaction; Mobile interaction; New applications; Overt visual attentions; Visual Attention; Behavioral research",Article,"Final","",Scopus,2-s2.0-85083007338
"Lee K.-F., Chen Y.-L., Yu C.-W., Chin K.-Y., Wu C.-H.","56413130800;35322122400;54379325600;25824572800;57203205774;","Gaze tracking and point estimation using low-cost head-mounted devices",2020,"Sensors (Switzerland)","20","7","1917","","",,5,"10.3390/s20071917","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082732579&doi=10.3390%2fs20071917&partnerID=40&md5=952798313432eb8c4b42adf49c4f90fa","Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, 10608, Taiwan; Department of Digital Humanities and Information Applications, Aletheia University, New Taipei City, Taiwan","Lee, K.-F., Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, 10608, Taiwan; Chen, Y.-L., Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, 10608, Taiwan; Yu, C.-W., Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, 10608, Taiwan; Chin, K.-Y., Department of Digital Humanities and Information Applications, Aletheia University, New Taipei City, Taiwan; Wu, C.-H., Department of Computer Science and Information Engineering, National Taipei University of Technology, Taipei, 10608, Taiwan","In this study, a head-mounted device was developed to track the gaze of the eyes and estimate the gaze point on the user's visual plane. To provide a cost-effective vision tracking solution, this head-mounted device is combined with a sized endoscope camera, infrared light, and mobile phone; the devices are also implemented via 3D printing to reduce costs. Based on the proposed image pre-processing techniques, the system can efficiently extract and estimate the pupil ellipse from the camera module. A 3D eye model was also developed to effectively locate eye gaze points from extracted eye images. In the experimental results, average accuracy, precision, and recall rates of the proposed system can achieve an average of over 97%, which can demonstrate the efficiency of the proposed system. This study can be widely used in the Internet of Things, virtual reality, assistive devices, and human-computer interaction applications. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Eye tracking; Gaze estimation; Head-mounted; Mobile devices; Wearable devices","3D modeling; 3D printers; Cameras; Cost effectiveness; Costs; Human computer interaction; Image processing; Assistive devices; Camera modules; Cost effective; Gaze tracking; Image preprocessing; Infrared light; Point estimation; Vision tracking; Eye tracking; algorithm; eye fixation; eye movement; human; image processing; infrared radiation; mobile phone; photography; physiology; procedures; three dimensional printing; Algorithms; Cell Phone; Eye Movements; Eye-Tracking Technology; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Infrared Rays; Photography; Printing, Three-Dimensional",Article,"Final","",Scopus,2-s2.0-85082732579
"Chen Y., Liu C., Shi B.E., Liu M.","57192879444;57194613111;7402547071;55512872300;","Robot Navigation in Crowds by Graph Convolutional Networks with Attention Learned from Human Gaze",2020,"IEEE Robotics and Automation Letters","5","2","8990034","2754","2761",,12,"10.1109/LRA.2020.2972868","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080896788&doi=10.1109%2fLRA.2020.2972868&partnerID=40&md5=e9297ba514b7245ac8362cdbdc990cc1","Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong","Chen, Y., Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Liu, C., Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Shi, B.E., Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Liu, M., Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, Hong Kong","Safe and efficient crowd navigation for mobile robot is a crucial yet challenging task. Previous work has shown the power of deep reinforcement learning frameworks to train efficient policies. However, their performance deteriorates when the crowd size grows. We suggest that this can be addressed by enabling the network to identify and pay attention to the humans in the crowd that are most critical to navigation. We propose a novel network utilizing a graph representation to learn the policy. We first train a graph convolutional network based on human gaze data that accurately predicts human attention to different agents in the crowd as they perform a navigation task based on a top down view of the environment. We incorporate the learned attention into a graph-based reinforcement learning architecture. The proposed attention mechanism enables the assignment of meaningful weightings to the neighbors of the robot, and has the additional benefit of interpretability. Experiments on real-world dense pedestrian datasets with various crowd sizes demonstrate that our model outperforms state-of-art methods, increasing task completion rate by 18.4% and decreasing navigation time by 16.4%. © 2020 IEEE.","Autonomous vehicle navigation; deep learning in robotics and automation; social human-robot interaction","Arts computing; Convolution; Deep learning; Graphic methods; Navigation; Navigation systems; Reinforcement learning; Social robots; Attention mechanisms; Autonomous vehicle navigation; Convolutional networks; Graph representation; Interpretability; Navigation tasks; Social human-robot interactions; State-of-art methods; Mobile robots",Article,"Final","",Scopus,2-s2.0-85080896788
"Bloem A.C., Barakova E., Hootsmans I.M., Opheij L.M., Toebosch R.H.A., Kerzel M., Barros P.","57216354066;35614959800;57216335668;57216355692;57216331178;41861835400;55734354000;","Improving emotional expression recognition of robots using regions of interest from human data",2020,"ACM/IEEE International Conference on Human-Robot Interaction",,,,"142","144",,1,"10.1145/3371382.3378359","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083200505&doi=10.1145%2f3371382.3378359&partnerID=40&md5=39d7102b3c1d6813a260673b8e54e68b","Industrial Design Eindhoven, University of Technology, Eindhoven, Netherlands; Knowledge Technology, Departmentof Informatics, University of Hamburg, Hamburg, Germany; Cognitive Architecture for Collaborative Technologies (CONTACT), Unit Istituto Italiano di Tecnologia, Genova, Italy","Bloem, A.C., Industrial Design Eindhoven, University of Technology, Eindhoven, Netherlands; Barakova, E., Industrial Design Eindhoven, University of Technology, Eindhoven, Netherlands; Hootsmans, I.M., Industrial Design Eindhoven, University of Technology, Eindhoven, Netherlands; Opheij, L.M., Industrial Design Eindhoven, University of Technology, Eindhoven, Netherlands; Toebosch, R.H.A., Industrial Design Eindhoven, University of Technology, Eindhoven, Netherlands; Kerzel, M., Knowledge Technology, Departmentof Informatics, University of Hamburg, Hamburg, Germany; Barros, P., Cognitive Architecture for Collaborative Technologies (CONTACT), Unit Istituto Italiano di Tecnologia, Genova, Italy","This paper is the first step of an attempt to equip social robots with emotion recognition capabilities comparable to those of humans. Most of the recent deep learning solutions for facial expression recognition under-perform when deployed in Human-Robot-Interaction scenarios, although they are capable of breaking records on the most varied benchmarks on facial expression recognition. The main reason for that we believe is that they are using techniques that are developed for recognition of static pictures, while in real-life scenarios, we infer emotions from intervals of expression. Utilising on the feature of CNN to form regions of interests that are similar to human gaze patterns, we use recordings from human-gaze patterns to train such a network to infer facial emotions from 3 seconds video footage of humans expressing 6 basic emotions. © 2020 ACM.","Attention maps; Emotion recognition; HRI; Neural networks","Agricultural robots; Deep learning; Man machine systems; Social robots; Basic emotions; Emotion recognition; Emotional expressions; Facial emotions; Facial expression recognition; Human data; Regions of interest; Video footage; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85083200505
"Aydin A.S., Feiz S., Ashok V., Ramakrishnan I.V.","55749193700;57209393850;55633359800;7003899264;","SaIL",2020,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,"111","115",,2,"10.1145/3377325.3377540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082472741&doi=10.1145%2f3377325.3377540&partnerID=40&md5=a8f49613d8283d72cdbd62fc5fc66c8a","Stony Brook University, United States; Old Dominion University","Aydin, A.S., Stony Brook University, United States; Feiz, S., Stony Brook University, United States; Ashok, V., Old Dominion University; Ramakrishnan, I.V., Stony Brook University, United States","Navigating webpages with screen readers is a challenge even with recent improvements in screen reader technologies and the increased adoption of web standards for accessibility, namely ARIA. ARIA landmarks, an important aspect of ARIA, lets screen reader users access different sections of the webpage quickly, by enabling them to skip over blocks of irrelevant or redundant content. However, these landmarks are sporadically and inconsistently used by web developers, and in many cases, even absent in numerous web pages. Therefore, we propose SaIL, a scalable approach that automatically detects the important sections of a web page, and then injects ARIA landmarks into the corresponding HTML markup to facilitate quick access to these sections. The central concept underlying SaIL is visual saliency, which is determined using a state-of-the-art deep learning model that was trained on gaze-tracking data collected from sighted users in the context of web browsing. We present the findings of a pilot study that demonstrated the potential of SaIL in reducing both the time and effort spent in navigating webpages with screen readers. © ACM.","landmarks; screen reader; WAI-ARIA; web accessibility","Deep learning; Eye tracking; Websites; landmarks; Learning models; Redundant content; Scalable approach; Screen readers; State of the art; WAI-ARIA; Web accessibility; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85082472741
"Sarowar Dhrubo Z.Z., Islam Hridoy M.A., Jamal L., Sarker S., Shidujaman M.","57222470229;57222469596;26431906900;56644176700;55490645100;","Development of a Sign Language for Total Paralysis and Interpretation using Deep Learning",2020,"Proceedings of International Conference on Image Processing and Robotics, ICIPRoB 2020",,,"9367362","","",,,"10.1109/ICIP48927.2020.9367362","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102740736&doi=10.1109%2fICIP48927.2020.9367362&partnerID=40&md5=11a903190f1ec8b2d213858f8050dcd7","University of Dhaka, Department of Robotics and Mechatronics Engineering, Dhaka, Bangladesh; Tsinghua University, Academy of Arts and Design, X-Studio, Dept. of Information Art and Design, Beijing, China","Sarowar Dhrubo, Z.Z., University of Dhaka, Department of Robotics and Mechatronics Engineering, Dhaka, Bangladesh; Islam Hridoy, M.A., University of Dhaka, Department of Robotics and Mechatronics Engineering, Dhaka, Bangladesh; Jamal, L., University of Dhaka, Department of Robotics and Mechatronics Engineering, Dhaka, Bangladesh; Sarker, S., University of Dhaka, Department of Robotics and Mechatronics Engineering, Dhaka, Bangladesh; Shidujaman, M., Tsinghua University, Academy of Arts and Design, X-Studio, Dept. of Information Art and Design, Beijing, China","People with total paralysis suffer not just from physical disability but also from the agony of the inability to communicate and express their feelings. Researches have long tried to solve this problem with various techniques such as collecting the patient's message directly from the brain using Brain-Computer interface and determining the gaze of the patient on a screen that contains letters and symbols. The first approach proved to be expensive and less accurate. The second one needs very fine calculation of the pupil center and even if done correctly the drifting of the human eye makes it hard to get a better accuracy. Again, all of these methods are dependent on electronic devices. In this paper, we proposed a solution that does not require the help of any electronic devices by developing a sign language for eye movements. We also proposed an approach to automatically interpret the patient's sign using Convolutional Neural Network and achieved great accuracy on the training data. This suggests that our network can be used to provide a better communication method for paralyzed patients. © 2020 IEEE.","Convolutional Neural Network; Deep Learning; Eye Sign; Quadriplegia; Sign Language","Agricultural robots; Brain computer interface; Convolutional neural networks; Eye movements; Image processing; Medical computing; Robotics; Thermoelectric equipment; Communication method; Electronic device; Human eye; Paralyzed patients; Physical disability; Pupil centers; Sign language; Training data; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85102740736
"Sung Lee B., Phattharaphon R., Yean S., Liu J., Shakya M.","57219806388;57219799341;57192383054;57209506625;57219806375;","Euclidean Distance based Loss Function for Eye-Gaze Estimation",2020,"2020 IEEE Sensors Applications Symposium, SAS 2020 - Proceedings",,,"9220051","","",,,"10.1109/SAS48726.2020.9220051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095610813&doi=10.1109%2fSAS48726.2020.9220051&partnerID=40&md5=4c577c3d25820b770b8e51365ae0aaf1","Nanyang Technological University, School of Computer Science and Engineering, Singapore, Singapore; Kasetsart University, Department of Computer Engineering, Thailand","Sung Lee, B., Nanyang Technological University, School of Computer Science and Engineering, Singapore, Singapore; Phattharaphon, R., Kasetsart University, Department of Computer Engineering, Thailand; Yean, S., Nanyang Technological University, School of Computer Science and Engineering, Singapore, Singapore; Liu, J., Nanyang Technological University, School of Computer Science and Engineering, Singapore, Singapore; Shakya, M., Nanyang Technological University, School of Computer Science and Engineering, Singapore, Singapore","The Loss function is an integral component in a Neural network. It affects the performance of CNN network in its classification. In this paper, we propose a Euclidean distance based Loss function for the CNN model, in an eye-gaze memory card game. We compared the Euclidean distance loss function with the well-known cross-entropy loss function. The performance parameters used in our comparison are prediction accuracy and average Euclidean distance prediction error. The results show that cross-entropy has better prediction accuracy. However, the Euclidean distance loss function provides a better average Euclidean distance prediction error resulting in better user experience. This is because the wrongly predicted eye gaze cards are near to the user intended card. In the case of cross-entropy, the predicted card error is quite evenly spread across the screen. © 2020 IEEE.","Convolutional Neural Network; Cross-entropy; Euclidean distance; Eye Gaze; Loss function","Entropy; Errors; User experience; CNN network; Cross entropy; Euclidean distance; Integral components; Loss functions; Performance parameters; Prediction accuracy; Prediction errors; Forecasting",Conference Paper,"Final","",Scopus,2-s2.0-85095610813
"Rabba S., Kyan M., Gao L., Quddus A., Zandi A.S., Guan L.","57196347194;6506086351;56285851900;57212781508;57212855259;55679853000;","Discriminative Robust Head-Pose and Gaze Estimation Using Kernel-DMCCA Features Fusion",2020,"International Journal of Semantic Computing","14","1",,"107","135",,,"10.1142/S1793351X20500014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092735859&doi=10.1142%2fS1793351X20500014&partnerID=40&md5=1485abdf640e640b8002630ae1647989","Electrical and Computer Engineering, Ryerson University, 350 Victoria Street, Toronto, ON  M5B 2K3, Canada; Electrical and Computer Engineering, York University, 4700 Keele Street, Toronto, ON  M3J 1P3, Canada; Alcohol Countermeasure Systems, ACS Corporation, 60 International Blvd, Etobicoke, ON  M9W 6J2, Canada","Rabba, S., Electrical and Computer Engineering, Ryerson University, 350 Victoria Street, Toronto, ON  M5B 2K3, Canada; Kyan, M., Electrical and Computer Engineering, York University, 4700 Keele Street, Toronto, ON  M3J 1P3, Canada; Gao, L., Electrical and Computer Engineering, Ryerson University, 350 Victoria Street, Toronto, ON  M5B 2K3, Canada; Quddus, A., Alcohol Countermeasure Systems, ACS Corporation, 60 International Blvd, Etobicoke, ON  M9W 6J2, Canada; Zandi, A.S., Alcohol Countermeasure Systems, ACS Corporation, 60 International Blvd, Etobicoke, ON  M9W 6J2, Canada; Guan, L., Electrical and Computer Engineering, Ryerson University, 350 Victoria Street, Toronto, ON  M5B 2K3, Canada","There remain outstanding challenges for improving accuracy of multi-feature information for head-pose and gaze estimation. The proposed framework employs discriminative analysis for head-pose and gaze estimation using kernel discriminative multiple canonical correlation analysis (K-DMCCA). The feature extraction component of the framework includes spatial indexing, statistical and geometrical elements. Head-pose and gaze estimation is constructed by feature aggregation and transforming features into a higher dimensional space using K-DMCCA for accurate estimation. The two main contributions are: Enhancing fusion performance through the use of kernel-based DMCCA, and by introducing an improved iris region descriptor based on quadtree. The overall approach is also inclusive of statistical and geometrical indexing that are calibration free (does not require any subsequent adjustment). We validate the robustness of the proposed framework across a wide variety of datasets, which consist of different modalities (RGB and Depth), constraints (wide range of head-poses, not only frontal), quality (accurately labelled for validation), occlusion (due to glasses, hair bang, facial hair) and illumination. Our method achieved an accurate head-pose and gaze estimation of 4.8 using Cave, 4.6 using MPII, 5.1 using ACS, 5.9 using EYEDIAP, 4.3 using OSLO and 4.6 using UULM datasets. © 2020 World Scientific Publishing Company.","Gaze; head-pose; kernel-DMCCA; pupil; quadtree","Computer programming; Computer science; Accurate estimation; Calibration free; Canonical correlation analysis; Feature aggregation; Features fusions; Fusion performance; Higher-dimensional; Spatial indexing; Indexing (of information)",Conference Paper,"Final","",Scopus,2-s2.0-85092735859
"Dias P.A., Malafronte D., Medeiros H., Odone F.","57188569230;55295947000;24470208900;6602231198;","Gaze estimation for assisted living environments",2020,"Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020",,,"9093439","279","288",,3,"10.1109/WACV45572.2020.9093439","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085505029&doi=10.1109%2fWACV45572.2020.9093439&partnerID=40&md5=f012cd592d8593fa3d372e7bd61626f3","Marquette University (EECE), United States; Italian Institute of Technology, Italy; University of Genova (MaLGa-DIBRIS), Italy","Dias, P.A., Marquette University (EECE), United States; Malafronte, D., Italian Institute of Technology, Italy; Medeiros, H., Marquette University (EECE), United States; Odone, F., University of Genova (MaLGa-DIBRIS), Italy","Effective assisted living environments must be able to perform inferences on how their occupants interact with one another as well as with surrounding objects. To accomplish this goal using a vision-based automated approach, multiple tasks such as pose estimation, object segmentation and gaze estimation must be addressed. Gaze direction provides some of the strongest indications of how a person interacts with the environment. In this paper, we propose a simple neural network regressor that estimates the gaze direction of individuals in a multi-camera assisted living scenario, relying only on the relative positions of facial keypoints collected from a single pose estimation model. To handle cases of keypoint occlusion, our model exploits a novel confidence gated unit in its input layer. In addition to the gaze direction, our model also outputs an estimation of its own prediction uncertainty. Experimental results on a public benchmark demonstrate that our approach performs on par with a complex, dataset-specific baseline, while its uncertainty predictions are highly correlated to the actual angular error of corresponding estimations. Finally, experiments on images from a real assisted living environment demonstrate that our model has a higher suitability for its final application. © 2020 IEEE.",,"Computer vision; Image segmentation; Automated approach; Gaze estimation; Highly-correlated; Living environment; Object segmentation; Pose estimation; Prediction uncertainty; Relative positions; Assisted living",Conference Paper,"Final","",Scopus,2-s2.0-85085505029
"Wang Z., Zhao J., Lu C., Huang H., Yang F., Li L., Guo Y.","57195938983;57191896599;57217766799;57216948641;57216772514;57216955746;55642210700;","Learning to detect head movement in unconstrained remote gaze estimation in the wild",2020,"Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020",,,"9093476","3432","3441",,1,"10.1109/WACV45572.2020.9093476","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085499134&doi=10.1109%2fWACV45572.2020.9093476&partnerID=40&md5=d075fa34c89664023d7b09cdf430f9da","Columbia University, United States; XPENG Motors, China; Institute of North Electronic Equipment, Beijing, China","Wang, Z., Columbia University, United States, XPENG Motors, China; Zhao, J., Institute of North Electronic Equipment, Beijing, China; Lu, C., XPENG Motors, China; Huang, H., XPENG Motors, China; Yang, F., XPENG Motors, China; Li, L., XPENG Motors, China; Guo, Y., XPENG Motors, China","Unconstrained remote gaze estimation remains challenging mostly due to its vulnerability to the large variability in head-pose. Prior solutions struggle to maintain reliable accuracy in unconstrained remote gaze tracking. Among them, appearance-based solutions demonstrate tremendous potential in improving gaze accuracy. However, existing works still suffer from head movement and are not robust enough to handle real-world scenarios. Especially most of them study gaze estimation under controlled scenarios where the collected datasets often cover limited ranges of both head-pose and gaze which introduces further bias. In this paper, we propose novel end-to-end appearance-based gaze estimation methods that could more robustly incorporate different levels of head-pose representations into gaze estimation. Our method could generalize to real-world scenarios with low image quality, different lightings and scenarios where direct head-pose information is not available. To better demonstrate the advantage of our methods, we further propose a new benchmark dataset with the most rich distribution of head-gaze combination reflecting real-world scenarios. Extensive evaluations on several public datasets and our own dataset demonstrate that our method consistently outperforms the state-of-the-art by a significant margin. © 2020 IEEE.",,"Computer vision; Appearance based; Benchmark datasets; Gaze estimation; Gaze tracking; Head movements; Real-world scenario; Remote gaze estimation; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085499134
"Chen Z., Shi B.E.","56808413900;7402547071;","Offset calibration for appearance-based gaze estimation via gaze decomposition",2020,"Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020",,,"9093419","259","268",,3,"10.1109/WACV45572.2020.9093419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085491803&doi=10.1109%2fWACV45572.2020.9093419&partnerID=40&md5=3a8ca439f935368aecfed3a21e3063ce","Hong Kong University of Science and Technology, Hong Kong","Chen, Z., Hong Kong University of Science and Technology, Hong Kong; Shi, B.E., Hong Kong University of Science and Technology, Hong Kong","Appearance-based gaze estimation provides relatively unconstrained gaze tracking. However, subject-independent models achieve limited accuracy partly due to individual variations. To improve estimation, we propose a gaze decomposition method that enables low complexity calibration, i.e., using calibration data collected when subjects view only one or a few gaze targets and the number of images per gaze target is small. Lowering the complexity of calibration makes it more convenient and less timeconsuming for the user, and more widely applicable. Motivated by our finding that the inter-subject squared bias exceeds the intra-subject variance for a subject-independent estimator, we decompose the gaze estimate into the sum of a subject-independent term estimated from the input image by a deep convolutional network and a subject-dependent bias term. During training, both the weights of the deep network and the bias terms are estimated. During testing, if no calibration data is available, we can set the bias term to zero. Otherwise, the bias term can be estimated from images of the subject gazing at known gaze targets. Experimental results on three datasets show that without calibration, our method outperforms state-of-the-art by at least 6.3%. For low complexity calibration sets, our method outperforms other calibration methods. More complex calibration algorithms do not outperform our method until the size of the calibration set is excessively large. Even then, the gains obtained by alternatives are small, e.g., only 0.1° lower error for 64 gaze targets. Source code is available at https://github.com/czk32611/Gaze-Decomposition. © 2020 IEEE.",,"Calibration; Complex networks; Computer vision; Image enhancement; Appearance based; Calibration algorithm; Calibration method; Convolutional networks; Decomposition methods; Independent model; Offset calibration; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085491803
"Heo H., Lee M., Kim S., Hwang Y.","57204140010;57216933665;25652751100;7402311392;","Gaze+Gesture Interface: Considering Social Acceptability",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VRW 2020",,,"9090522","691","692",,,"10.1109/VRW50115.2020.00196","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085377665&doi=10.1109%2fVRW50115.2020.00196&partnerID=40&md5=87ec1e4194839d2367bd32a2e31b84e7","Korea Electronics Technology Institute, South Korea","Heo, H., Korea Electronics Technology Institute, South Korea; Lee, M., Korea Electronics Technology Institute, South Korea; Kim, S., Korea Electronics Technology Institute, South Korea; Hwang, Y., Korea Electronics Technology Institute, South Korea","In public places like cafes, the usage of smart glasses with interfaces including controller, touchpad, voice, or mid-air gesture not only receives a lot of interest from people around him or her but also cannot protect the privacy of individuals. If smart glasses become more advanced and more popular than regular glasses in the future, socially acceptable user interfaces can be required. In this paper, we propose a user interface on HoloLens by using gaze tracking instead of head tracking for navigation, and unobtrusive gesture based on deep learning instead of mid-air gestures for selection/manipulation, that is more socially acceptable than the existing user interfaces on smart glasses. A study was conducted to investigate social acceptability from the users' perspective, and the results showed the advantages of the proposed method to improve social acceptability. © 2020 IEEE.","Gestrual input; Human computer interaction (HCI); Human-centered computing; Interaction techniques","Air navigation; Deep learning; Eye tracking; Glass; Virtual reality; Gaze tracking; Gesture interfaces; Glasses In; Head tracking; Public places; Smart glass; Social acceptability; Touchpad; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85085377665
"Onuki Y., Kudo K., Kumazawa I.","57194029718;57203743861;55943452700;","Removal of the Infrared Light Reflection of Eyeglass Using Multi-Channel CycleGAN Applied for the Gaze Estimation Images",2020,"Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VRW 2020",,,"9090544","591","592",,,"10.1109/VRW50115.2020.00146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085365484&doi=10.1109%2fVRW50115.2020.00146&partnerID=40&md5=4334679e9f8405f0b97e5b9e55809f56","Tokyo Institute of Technology, Japan","Onuki, Y., Tokyo Institute of Technology, Japan; Kudo, K., Tokyo Institute of Technology, Japan; Kumazawa, I., Tokyo Institute of Technology, Japan","In virtual reality (VR) environments, the importance of the eye gaze estimation is rapidly increasing. The geometrical model base method is commonly used by equipping infrared (IR) light sources and cameras inside of the head mount displays (HMDs). Some HMDs enable users to wear with spectacles on, and, in this case, IR light reflections of the eyeglass often causes serious obstruction to detect those of the corneal. In this study, we propose the multi-channel CycleGAN to generate the eye images with no eyeglass from those with eyeglass. Proposed method has 4 channels input, which consists of three normal eye images at different time points and an image in blinking, in order to distinguish stationary and moving reflections in images. Proposal achieved to selectively remove the eyeglass reflections and keep the corneal reflections alive in sequential gaze estimation images. © 2020 IEEE.","Computing methodologies; Human computer interaction; Human-centered computing; Interaction paradigms; Machine learning; Machine learning algorithm; Virtual reality","Light reflection; Light sources; User interfaces; Corneal reflection; Eye images; Gaze estimation; Geometrical modeling; Head-mount displays; Infrared light; Multi channel; Time points; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85085365484
"Lai H.-Y., Saavedra-Pena G., Sodini C.G., Sze V., Heldt T.","57193616001;57204676182;7006166884;23053117700;6602161089;","Measuring Saccade Latency Using Smartphone Cameras",2020,"IEEE Journal of Biomedical and Health Informatics","24","3","8703178","885","897",,,"10.1109/JBHI.2019.2913846","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081940899&doi=10.1109%2fJBHI.2019.2913846&partnerID=40&md5=311bd08bd382d6cccd2dbc59e91367fc","Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, United States; Department of Electrical Engineering and Computer Science, Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, MA, United States","Lai, H.-Y., Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, United States; Saavedra-Pena, G., Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, United States; Sodini, C.G., Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, United States; Sze, V., Department of Electrical Engineering and Computer Science, Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, MA, United States; Heldt, T., Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, United States","Objective: Accurate quantification of neurodegenerative disease progression is an ongoing challenge that complicates efforts to understand and treat these conditions. Clinical studies have shown that eye movement features may serve as objective biomarkers to support diagnosis and tracking of disease progression. Here, we demonstrate that saccade latency - an eye movement measure of reaction time - can be measured robustly outside of the clinical environment with a smartphone camera. Methods: To enable tracking of saccade latency in large cohorts of patients and control subjects, we combined a deep convolutional neural network for gaze estimation with a model-based approach for saccade onset determination that provides automated signal-quality quantification and artifact rejection. Results: Simultaneous recordings with a smartphone and a high-speed camera resulted in negligible differences in saccade latency distributions. Furthermore, we demonstrated that the constraint of chinrest support can be removed when recording healthy subjects. Repeat smartphone-based measurements of saccade latency in 11 self-reported healthy subjects resulted in an intraclass correlation coefficient of 0.76, showing our approach has good to excellent test-retest reliability. Additionally, we conducted more than 19 000 saccade latency measurements in 29 self-reported healthy subjects and observed significant intra- and inter-subject variability, which highlights the importance of individualized tracking. Lastly, we showed that with around 65 measurements we can estimate mean saccade latency to within less-than-10-ms precision, which takes within 4 min with our setup. Conclusion and Significance: By enabling repeat measurements of saccade latency and its distribution in individual subjects, our framework opens the possibility of quantifying patient state on a finer timescale in a broader population than previously possible. © 2013 IEEE.","convolutional neural networks; Eye tracking; health monitoring; mobile imaging; saccade latency","Convolution; Convolutional neural networks; Deep neural networks; Diagnosis; Eye tracking; High speed cameras; Neurodegenerative diseases; Smartphones; Accurate quantifications; Clinical environments; Eye-movement measures; Health monitoring; Intraclass correlation coefficients; Mobile imaging; Simultaneous recording; Test-retest reliability; Eye movements; biological marker; adult; algorithm; Article; automation; bootstrapping; clinical article; controlled study; degenerative disease; disease exacerbation; escape latency; eye movement; eye position; eye tracking; eye tracking algorithm; female; gaze; head movement; human; image quality; male; normal human; reaction time; saccade latency; sample size; stimulus response; test retest reliability; videorecording; visual stimulation; devices; image processing; middle aged; physiology; saccadic eye movement; smartphone; young adult; Adult; Algorithms; Eye-Tracking Technology; Female; Humans; Image Processing, Computer-Assisted; Male; Middle Aged; Neural Networks, Computer; Saccades; Smartphone; Young Adult",Article,"Final","",Scopus,2-s2.0-85081940899
"Hoshino K., Noguchi Y., Nakai Y.","7402671353;57216151087;57216146908;","Gaze Estimation with Easy Calibration Method",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"102","106",,,"10.1145/3385209.3385232","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086141725&doi=10.1145%2f3385209.3385232&partnerID=40&md5=f05da9fa80fb962ea0f021e9d30d5e63","University of Tsukuba, 1-1-1 Tennodai, Tsukuba, Japan","Hoshino, K., University of Tsukuba, 1-1-1 Tennodai, Tsukuba, Japan; Noguchi, Y., University of Tsukuba, 1-1-1 Tennodai, Tsukuba, Japan; Nakai, Y., University of Tsukuba, 1-1-1 Tennodai, Tsukuba, Japan","In the corneal reflection method for gaze estimation, the user is asked to look at multiple calibration points with known lines of sight, and the central pupil point is captured for each vision angle. Therefore, the center of the eye in the images can be acquired for each known eye rotation angle. However, because the spherical center of the eye is captured in two-dimensional images and because measurement errors are, of course, included, in order to map eye rotation angles from two-dimensional images, normally, the approximate solution is calculated by using the least squares method in the overdetermined system linear equation. Normally, six coefficients are calculated from nine points of regard for calibration. This study proposes a new gaze estimation method that uses fewer calibration points while maintaining accuracy equivalent to using nine points. © 2020 ACM.","Calibration; Corneal reflection method; Gaze estimation; Lines of sight; Overdetermined system","Calibration; Approximate solution; Calibration method; Calibration points; Corneal reflection; Least squares methods; Multiple calibration; Overdetermined systems; Two dimensional images; Least squares approximations",Conference Paper,"Final","",Scopus,2-s2.0-85086141725
[无可用作者姓名],[无可用的作者 ID],"Proceedings ETRA 2020 Full Papers - ACM Symposium on Eye Tracking Research and Applications",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",220,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086264265&partnerID=40&md5=4679967efc1ac92f6e35eb8e7826e058",,"","The proceedings contain 23 papers. The topics discussed include: visual search target inference in natural interaction settings with machine learning; combining gaze estimation and optical flowfor pursuits interaction; analyzing gaze behavior using object detection and unsupervised clustering; a minhash approach for fast scanpath classification; label likelihood maximization: adapting iris segmentation models using domain adaptation; anticipating averted gaze in dyadic interactions; a survey of digital eye strainin gaze-based interactive systems; bubble gaze cursor + bubble gaze lens: applying area cursor technique to eye-gaze interface; and eye gaze controlled robotic arm for persons with severe speech and motor impairment.",,,Conference Review,"Final","",Scopus,2-s2.0-85086264265
"Barz M., Stauden S., Sonntag D.","57189847803;57204112887;12241487800;","Visual search target inference in natural interaction settings with machine learning",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,4,"10.1145/3379155.3391314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086181657&doi=10.1145%2f3379155.3391314&partnerID=40&md5=622a8dda11e08153ba8c12d3c3b46711","German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Saarland University, Saarbrücken, Germany; Saarbrücken Graduate School of Computer Science, Germany","Barz, M., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany, Saarbrücken Graduate School of Computer Science, Germany; Stauden, S., Saarland University, Saarbrücken, Germany; Sonntag, D., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany","Visual search is a perceptual task in which humans aim at identifying a search target object such as a traffic sign among other objects. Search target inference subsumes computational methods for predicting this target by tracking and analyzing overt behavioral cues of that person, e.g., the human gaze and fixated visual stimuli. We present a generic approach to inferring search targets in natural scenes by predicting the class of the surrounding image segment. Our method encodes visual search sequences as histograms of fixated segment classes determined by SegNet, a deep learning image segmentation model for natural scenes. We compare our sequence encoding and model training (SVM) to a recent baseline from the literature for predicting the target segment. Also, we use a new search target inference dataset. The results show that, first, our new segmentation-based sequence encoding outperforms the method from the literature, and second, that it enables target inference in natural settings. © 2020 ACM.","Machine Learning; Mobile Eyetracking; Search Target Inference; Visual Attention","Deep learning; Encoding (symbols); Eye tracking; Forecasting; Image segmentation; Signal encoding; Support vector machines; Behavioral cues; Generic approach; Image segmentation model; Image segments; Model training; Natural interactions; Sequence encoding; Visual stimulus; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85086181657
"Castner N., Küebler T.C., Scheiter K., Richter J., Eder T., Hüettig F., Keutel C., Kasneci E.","57193611337;55701951700;6507519353;57008653700;57202894457;57195118616;54393473100;56059892600;","Deep semantic gaze embedding and scanpath comparison for expertise classification during OPT viewing",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,6,"10.1145/3379155.3391320","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086181141&doi=10.1145%2f3379155.3391320&partnerID=40&md5=fbc09bcb40e162c481e41276af366f7f","Perception Engineering, University of Tübingen, Tübingen, Germany; Leibniz-Institut für Wissensmedien, Tübingen, Germany; University Hospital Tübingen, Tübingen, Germany; Department of Prosthodontics; Department of Radiology, Center of Dentistry, Oral Medicine and Maxillofacial Surgery","Castner, N., Perception Engineering, University of Tübingen, Tübingen, Germany; Küebler, T.C., Perception Engineering, University of Tübingen, Tübingen, Germany; Scheiter, K., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Richter, J., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Eder, T., Leibniz-Institut für Wissensmedien, Tübingen, Germany; Hüettig, F., University Hospital Tübingen, Tübingen, Germany, Department of Prosthodontics; Keutel, C., University Hospital Tübingen, Tübingen, Germany, Department of Radiology, Center of Dentistry, Oral Medicine and Maxillofacial Surgery; Kasneci, E., Perception Engineering, University of Tübingen, Tübingen, Germany","Modeling eye movement indicative of expertise behavior is decisive in user evaluation. However, it is indisputable that task semantics affect gaze behavior. We present a novel approach to gaze scanpath comparison that incorporates convolutional neural networks (CNN) to process scene information at the fixation level. Image patches linked to respective fixations are used as input for a CNN and the resulting feature vectors provide the temporal and spatial gaze information necessary for scanpath similarity comparison. We evaluated our proposed approach on gaze data from expert and novice dentists interpreting dental radiographs using a local alignment similarity score. Our approach was capable of distinguishing experts from novices with 93% accuracy while incorporating the image semantics. Moreover, our scanpath comparison using image patch features has the potential to incorporate task semantics from a variety of tasks. © 2020 ACM.","Deep Learning; Eye Tracking; Learning; Medical image interpretation; Scanpath analysis","Convolutional neural networks; Eye movements; Semantics; Dental radiographs; Expert and novices; Feature vectors; Local alignment; Scanpath comparisons; Similarity scores; Temporal and spatial; User evaluations; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086181141
"Venuprasad P., Xu L., Huang E., Gilman A., Leanne Chukoskie L., Cosman P.","57210106962;57217096675;57217096729;16318668400;6507740817;7003359562;","Analyzing gaze behavior using object detection and unsupervised clustering",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,2,"10.1145/3379155.3391316","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086180732&doi=10.1145%2f3379155.3391316&partnerID=40&md5=bd435c74df2eaaff3bdb37059656f50b","University of California San Diego, San Diego, CA, United States; Canyon Crest Academy, San Diego, CA, United States; Massey University, Auckland, New Zealand","Venuprasad, P., University of California San Diego, San Diego, CA, United States; Xu, L., University of California San Diego, San Diego, CA, United States; Huang, E., Canyon Crest Academy, San Diego, CA, United States; Gilman, A., Massey University, Auckland, New Zealand; Leanne Chukoskie, L., University of California San Diego, San Diego, CA, United States; Cosman, P., University of California San Diego, San Diego, CA, United States","Gaze behavior is important in early development, and atypical gaze behavior is among the first symptoms of autism. Here we describe a system that quantitatively assesses gaze behavior using eye-tracking glasses. Objects in the subject's field of view are detected using a deep learning model on the video captured by the glasses' world-view camera, and a stationary frame of reference is estimated using the positions of the detected objects. The gaze positions relative to the new frame of reference are subjected to unsupervised clustering to obtain the time sequence of looks. The clustering method increases the accuracy of look detection on test videos compared against a previous algorithm, and is considerably more robust on videos with poor calibration. © 2020 ACM.","computer vision; eye-tracking; gaze behavior; Kalman filtering; object detection; unsupervised clustering","Cameras; Deep learning; Glass; Object detection; Clustering methods; Field of views; Frame of reference; Gaze behavior; Learning models; Stationary frame; Time sequences; Unsupervised clustering; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086180732
"Bâce M., Becker V., Wang C., Bulling A.","44461063200;57195528513;57217097026;6505807414;","Combining gaze estimation and optical flow for pursuits interaction",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",,1,"10.1145/3379155.3391315","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086180684&doi=10.1145%2f3379155.3391315&partnerID=40&md5=0fd144aa5451fb21505e263324c66602","Department of Computer Science, ETH Zürich, Switzerland; Department of Information Technology and Electrical Engineering, ETH Zürich, Switzerland; Institute for Visualisation and Interactive Systems, University of Stuttgart, Germany","Bâce, M., Department of Computer Science, ETH Zürich, Switzerland; Becker, V., Department of Computer Science, ETH Zürich, Switzerland; Wang, C., Department of Information Technology and Electrical Engineering, ETH Zürich, Switzerland; Bulling, A., Institute for Visualisation and Interactive Systems, University of Stuttgart, Germany","Pursuit eye movements have become widely popular because they enable spontaneous eye-based interaction. However, existing methods to detect smooth pursuits require special-purpose eye trackers. We propose the first method to detect pursuits using a single off-the-shelf RGB camera in unconstrained remote settings. The key novelty of our method is that it combines appearance-based gaze estimation with optical flow in the eye region to jointly analyse eye movement dynamics in a single pipeline. We evaluate the performance and robustness of our method for different numbers of targets and trajectories in a 13-participant user study. We show that our method not only outperforms the current state of the art but also achieves competitive performance to a consumer eye tracker for a small number of targets. As such, our work points towards a new family of methods for pursuit interaction directly applicable to an ever-increasing number of devices readily equipped with cameras. © 2020 ACM.","Gaze Estimation; Optical Flow; Pursuit Interaction; Smooth Pursuit","Cameras; Eye movements; Flow interactions; Optical flows; Appearance based; Competitive performance; Eye trackers; Gaze estimation; Remote settings; RGB cameras; Smooth pursuit; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85086180684
[无可用作者姓名],[无可用的作者 ID],"Proceedings ETRA 2020 Short Papers - ACM Symposium on Eye Tracking Research and Applications, ETRA 2020",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",313,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085757452&partnerID=40&md5=5f271f163e2189bd865fa525a559b284",,"","The proceedings contain 63 papers. The topics discussed include: polarized near-infrared light emission for eye gaze estimation; estimating point-of-gaze using smooth pursuit eye movements without implicit and explicit user-calibration; neural networks for optical vector and eye ball parameter estimation; tiny convolution, decision tree, and binary neuronal networks for robust and real time pupil outline estimation; protecting from lunchtime attack using an uncalibrated eye tracker signal; detection of saccades and quick-phases in eye movement recordings with Nystagmus.; challenges in interpretability of neural networks for eye movement data; predicting image influence on visual saliency distribution: the focal and ambient dichotomy; and positional head-eye tracking outside the lab: an open-source solution.",,,Conference Review,"Final","",Scopus,2-s2.0-85085757452
"Barbara N., Camilleri T.A., Camilleri K.P.","57193677149;56041755400;8301303700;","EOG-Based ocular and gaze angle estimation using an extended kalman filter",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"23","","",,2,"10.1145/3379156.3391357","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085735282&doi=10.1145%2f3379156.3391357&partnerID=40&md5=92cba791e7d71339115d29a6e5dbd5e9","Centre for Biomedical Cybernetics, Department of Systems and Control Engineering, University of Malta, Msida, Malta","Barbara, N., Centre for Biomedical Cybernetics, Department of Systems and Control Engineering, University of Malta, Msida, Malta; Camilleri, T.A., Centre for Biomedical Cybernetics, Department of Systems and Control Engineering, University of Malta, Msida, Malta; Camilleri, K.P., Centre for Biomedical Cybernetics, Department of Systems and Control Engineering, University of Malta, Msida, Malta","In this work, a novel method to estimate the ocular pose from electrooculography (EOG) signals is proposed. This method is based on an electrical battery model of the eye which relates the EOG potential to the distances between an electrode and the left/right cornea and retina centre points. In this work, this model is used to estimate the ocular angles (OAs), that is the orientation of the two ocular globes separately. Using this approach, an average cross-validated horizontal and vertical OA estimation error of 2.91 ± 0.86° and 2.42 ± 0.58° respectively was obtained. Furthermore, we show how these OA estimates may be used to estimate the gaze angles (GAs) without requiring the distance between the subject's face-plane and the target-plane, as in previous work. Using the proposed method, a cross-validated horizontal and vertical GA estimation error of 2.13 ± 0.73° and 2.42 ± 0.58° respectively was obtained, which compares well with the previous distance-based GA estimation technique. © 2020 ACM.","Electrooculography; Gaze estimation; Ocular angle estimation","Battery management systems; Extended Kalman filters; Horizontal wells; Software packages; Distance-based; Electrical battery models; Estimation errors; Estimation techniques; Gaze angles; Ocular globe; Target plane; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085735282
"Garde G., Larumbe-Bergera A., Bossavit B., Cabeza R., Porta S., Villanueva A.","57215963483;57210106737;36730794700;36763933900;7005292345;7101612861;","Gaze estimation problem tackled through synthetic images",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"34","","",,1,"10.1145/3379156.3391368","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734782&doi=10.1145%2f3379156.3391368&partnerID=40&md5=a0cdb6210711bebdc5ac60358ca9b701","Public University of Navarre, Pamplona, Spain; Trinity College Dublin, Dublin, Ireland","Garde, G., Public University of Navarre, Pamplona, Spain; Larumbe-Bergera, A., Public University of Navarre, Pamplona, Spain; Bossavit, B., Trinity College Dublin, Dublin, Ireland; Cabeza, R., Public University of Navarre, Pamplona, Spain; Porta, S., Public University of Navarre, Pamplona, Spain; Villanueva, A., Public University of Navarre, Pamplona, Spain","In this paper, we evaluate a synthetic framework to be used in the field of gaze estimation employing deep learning techniques. The lack of sufficient annotated data could be overcome by the utilization of a synthetic evaluation framework as far as it resembles the behavior of a real scenario. In this work, we use U2Eyes synthetic environment employing I2Head datataset as real benchmark for comparison based on alternative training and testing strategies. The results obtained show comparable average behavior between both frameworks although significantly more robust and stable performance is retrieved by the synthetic images. Additionally, the potential of synthetically pretrained models in order to be applied in user's specific calibration strategies is shown with outstanding performances. © 2020 ACM.","Datasets gaze estimation; Neural networks","Deep learning; Average behavior; Gaze estimation; Learning techniques; Stable performance; Synthetic environments; Synthetic evaluation; Synthetic images; Training and testing; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085734782
"Tavakoli H.R., Borji A., Kannala J., Rahtu E.","57201603431;23395793600;8928446400;6505786260;","Deep audio-visual saliency: Baseline model and data",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"3","","",,2,"10.1145/3379156.3391337","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734752&doi=10.1145%2f3379156.3391337&partnerID=40&md5=c9d2323927a2f1a904c4593bfa2dcc92","Nokia Technologies, Finland; Aalto University, Finland; Tampere University, Finland","Tavakoli, H.R., Nokia Technologies, Finland; Borji, A.; Kannala, J., Aalto University, Finland; Rahtu, E., Tampere University, Finland","This paper introduces a conceptually simple and effective Deep Audio-Visual Embedding for dynamic saliency prediction dubbed ""DAVE"" in conjunction with our efforts towards building an Audio-Visual Eye-tracking corpus named ""AVE"". Despite existing a strong relation between auditory and visual cues for guiding gaze during perception, video saliency models only consider visual cues and neglect the auditory information that is ubiquitous in dynamic scenes. Here, we propose a baseline deep audio-visual saliency model for multi-modal saliency prediction in the wild. Thus the proposed model is intentionally designed to be simple. A video baseline model is also developed on the same architecture to assess effectiveness of the audio-visual models on a fair basis. We demonstrate that audio-visual saliency model outperforms the video saliency models. The data and code are available at https://hrtavakoli.github.io/AVE/and https://github.com/hrtavakoli/DAVE. © 2020 ACM.","Audio-Visual Saliency; Deep Learning; Dynamic Visual Attention","HTTP; Object recognition; Visualization; Baseline models; Dynamic scenes; Multi-modal; Video saliencies; Visual cues; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085734752
"Palmero Cantarino C., Komogortsev O.V., Talathi S.S.","57188829002;6506328653;57224997923;","Benefits of temporal information for appearance-based gaze estimation",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"42","","",,3,"10.1145/3379156.3391376","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085734633&doi=10.1145%2f3379156.3391376&partnerID=40&md5=b0cfeaf0e12cbfba8fb2f1a968f6bbc0","Universitat de Barcelona, Barcelona, Spain; Computer Vision Center, Barcelona, Spain; Texas State University, Texas, United States; Facebook Reality Labs, Redmond, United States","Palmero Cantarino, C., Universitat de Barcelona, Barcelona, Spain, Computer Vision Center, Barcelona, Spain; Komogortsev, O.V., Texas State University, Texas, United States, Facebook Reality Labs, Redmond, United States; Talathi, S.S., Facebook Reality Labs, Redmond, United States","State-of-the-art appearance-based gaze estimation methods, usually based on deep learning techniques, mainly rely on static features. However, temporal trace of eye gaze contains useful information for estimating a given gaze point. For example, approaches leveraging sequential eye gaze information when applied to remote or low-resolution image scenarios with off-the-shelf cameras are showing promising results. The magnitude of contribution from temporal gaze trace is yet unclear for higher resolution/frame rate imaging systems, in which more detailed information about an eye is captured. In this paper, we investigate whether temporal sequences of eye images, captured using a high-resolution, high-frame rate head-mounted virtual reality system, can be leveraged to enhance the accuracy of an end-to-end appearance-based deep-learning model for gaze estimation. Performance is compared against a static-only version of the model. Results demonstrate statistically-significant benefits of temporal information, particularly for the vertical component of gaze. © 2020 ACM.","Convolutional neural networks; Eye movements; Fixation; Gaze dynamics; Gaze estimation; Recurrent neural networks; Saccade","Deep learning; Image enhancement; Image resolution; Learning systems; Virtual reality; Appearance based; Head mounted virtual reality; Higher resolution; Learning techniques; Low resolution images; Temporal information; Temporal sequences; Vertical component; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085734633
"Siegfried R., Aminian B., Odobez J.-M.","57195685304;57217013696;57203103085;","ManiGaze: A dataset for evaluating remote gaze estimator in object manipulation situations",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"35","","",,,"10.1145/3379156.3391369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085733447&doi=10.1145%2f3379156.3391369&partnerID=40&md5=2dfdcf53acfbce914e088651a1b91dfa","Idiap Reseach Institute, Martigny, Switzerland; EPFL, Lausanne, Switzerland","Siegfried, R., Idiap Reseach Institute, Martigny, Switzerland, EPFL, Lausanne, Switzerland; Aminian, B., Idiap Reseach Institute, Martigny, Switzerland, EPFL, Lausanne, Switzerland; Odobez, J.-M., Idiap Reseach Institute, Martigny, Switzerland, EPFL, Lausanne, Switzerland","Gaze estimation allows robots to better understand users and thus to more precisely meet their needs. In this paper, we are interested in gaze sensing for analyzing collaborative tasks and manipulation behaviors in human-robot interactions (HRI), which differs from screen gazing and other communicative HRI settings. Our goal is to study the accuracy that remote vision gaze estimators can provide, as they are a promising alternative to current accurate but intrusive wearable sensors. In this view, our contributions are: 1) we collected and make public a labeled dataset involving manipulation tasks and gazing behaviors in an HRI context; 2) we evaluate the performance of a state-of-the-art gaze estimation system on this dataset. Our results show a low default accuracy, which is improved by calibration, but that more research is needed if one wishes to distinguish gazing at one object amongst a dozen on a table. © 2020 ACM.","Dataset; Gaze estimation; Human-robot interaction; Remote recording","Behavioral research; Human robot interaction; Collaborative tasks; Gaze estimation; Human robot Interaction (HRI); Labeled dataset; Manipulation task; Object manipulation; Remote vision; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085733447
"Mokatren M., Kuflik T., Shimshoni I.","57190072235;6602259371;7003816361;","EyeLinks: Methods to compute reliable stereo mappings used for eye gaze tracking",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"20","","",,,"10.1145/3379156.3391354","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085731939&doi=10.1145%2f3379156.3391354&partnerID=40&md5=d5142544329c1ee9c190a95d4024e581","University of Haifa, Mount Carmel, Haifa, 31905, Israel","Mokatren, M., University of Haifa, Mount Carmel, Haifa, 31905, Israel; Kuflik, T., University of Haifa, Mount Carmel, Haifa, 31905, Israel; Shimshoni, I., University of Haifa, Mount Carmel, Haifa, 31905, Israel","We present methods for extracting corneal images and estimating pupil centers continuously and reliably using head worn glasses that consists of two eye cameras. An existing CNN was modified for detecting pupils in IR and RGB images, and stereo vision together with 2D and 3D models are used. We confirm the feasibility of the proposed methods through user study results, which show that the methods can be used in future real gaze estimation systems. © 2020 ACM.","Corneal Imaging; Gaze Estimation; Mapping Transformation","Stereo image processing; Stereo vision; Corneal images; Eye camera; Eye gaze tracking; Gaze estimation; Pupil centers; RGB images; Stereo mapping; User study; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085731939
"Koshikawa K., Sasaki M., Utsu T., Takemura K.","57217013667;57205675368;57210122289;8575290600;","Polarized near-infrared light emission for eye gaze estimation",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"8","","",,,"10.1145/3379156.3391342","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085731068&doi=10.1145%2f3379156.3391342&partnerID=40&md5=fbe1e634278a920235d37d22978b8617","Tokai University, Hiratsuka, Kanagawa, Japan","Koshikawa, K., Tokai University, Hiratsuka, Kanagawa, Japan; Sasaki, M., Tokai University, Hiratsuka, Kanagawa, Japan; Utsu, T., Tokai University, Hiratsuka, Kanagawa, Japan; Takemura, K., Tokai University, Hiratsuka, Kanagawa, Japan","The number of near-infrared light-emitting diodes (LEDs) is increasing to improve the accuracy and robustness of eye-tracking methods, and it is necessary to determining the identifiers (IDs) of the LEDs when applying multiple light sources. Therefore, we propose polarized near-infrared light emissions for an eye gaze estimation. We succeeded in determining the IDs of LEDs using polarization information. In addition, we remove glints from the cornea for correctly detecting the pupil center. We confirmed the effectiveness of using polarized near-infrared light emissions through evaluation experiments. © 2020 ACM.","Eye-tracking; Glint; Polarization","Infrared devices; Light emission; Light emitting diodes; Evaluation experiments; Eye tracking methods; Eye-gaze; Multiple light source; Near infrared light; Pupil centers; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085731068
"Hausamann P., Sinnott C., MacNeilage P.R.","57210106409;57212193834;10240387800;","Positional head-eye tracking outside the lab: An open-source solution",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"31","","",,6,"10.1145/3379156.3391365","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085728310&doi=10.1145%2f3379156.3391365&partnerID=40&md5=065cb127b5c7eaa8b042ba61659786cd","Technical University of Munich, Munich, Germany; University of Nevada-Reno, Reno, Nevada, United States","Hausamann, P., Technical University of Munich, Munich, Germany; Sinnott, C., University of Nevada-Reno, Reno, Nevada, United States; MacNeilage, P.R., University of Nevada-Reno, Reno, Nevada, United States","Simultaneous head and eye tracking has traditionally been confined to a laboratory setting and real-world motion tracking limited to measuring linear acceleration and angular velocity. Recently available mobile devices such as the Pupil Core eye tracker and the Intel RealSense T265 motion tracker promise to deliver accurate measurements outside the lab. Here, the researchers propose a hard-and software framework that combines both devices into a robust, usable, low-cost head and eye tracking system. The developed software is open source and the required hardware modifications can be 3D printed. The researchers demonstrate the system's ability to measure head and eye movements in two tasks: an eyes-fixed head rotation task eliciting the vestibulo-ocular reflex inside the laboratory, and a natural locomotion task where a subject walks around a building outside of the laboratory. The resultant head and eye movements are discussed, as well as future implementations of this system. © 2020 ACM.","Eye tracking; Gaze estimation; Head tracking; Locomotion; Mobile; Open source; Simultaneous localization and mapping","3D printers; Computer programming; Eye movements; Laboratories; Motion tracking; Open source software; Open systems; Accurate measurement; Eye tracking systems; Hardware modifications; Linear accelerations; Natural locomotions; Open-source solutions; Software frameworks; Vestibulo-ocular reflex; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085728310
"Bozkir E., Ünal A.B., Akgün M., Kasneci E., Pfeifer N.","57210111357;57212002288;26021030000;56059892600;23670837500;","Privacy preserving gaze estimation using synthetic images via a randomized encoding based framework",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"30","","",,3,"10.1145/3379156.3391364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085727385&doi=10.1145%2f3379156.3391364&partnerID=40&md5=a7d766e9c0c8050dffac4c811e69ef98","Human-Computer Interaction, University of Tübingen, Germany; Methods in Medical Informatics, Translational Bioinformatics, University of Tübingen, Germany; Statistical Learning in Computational Biology, Max Planck Institute for Informatics, Saarbrücken, Germany","Bozkir, E., Human-Computer Interaction, University of Tübingen, Germany; Ünal, A.B., Methods in Medical Informatics, Translational Bioinformatics, University of Tübingen, Germany; Akgün, M., Methods in Medical Informatics, Translational Bioinformatics, University of Tübingen, Germany; Kasneci, E., Human-Computer Interaction, University of Tübingen, Germany; Pfeifer, N., Methods in Medical Informatics, Translational Bioinformatics, University of Tübingen, Germany, Statistical Learning in Computational Biology, Max Planck Institute for Informatics, Saarbrücken, Germany","Eye tracking is handled as one of the key technologies for applications that assess and evaluate human attention, behavior, and biometrics, especially using gaze, pupillary, and blink behaviors. One of the challenges with regard to the social acceptance of eye tracking technology is however the preserving of sensitive and personal information. To tackle this challenge, we employ a privacy-preserving framework based on randomized encoding to train a Support Vector Regression model using synthetic eye images privately to estimate the human gaze. During the computation, none of the parties learn about the data or the result that any other party has. Furthermore, the party that trains the model cannot reconstruct pupil, blinks or visual scanpath. The experimental results show that our privacy-preserving framework is capable of working in real-time, with the same accuracy as compared to non-private version and could be extended to other eye tracking related problems. © 2020 ACM.","Eye tracking; Gaze estimation; Human computer interaction; Privacy preserving machine learning; Randomized encoding","Behavioral research; Encoding (symbols); Image coding; Signal encoding; Support vector regression; Eye tracking technologies; Gaze estimation; Key technologies; Personal information; Privacy preserving; Social acceptance; Support vector regression models; Synthetic images; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085727385
"Tamura Y., Takemura K.","57211683000;8575290600;","Estimating point-of-gaze using smooth pursuit eye movements without implicit and explicit user-calibration",2020,"Eye Tracking Research and Applications Symposium (ETRA)",,,"9","","",,,"10.1145/3379156.3391343","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085726287&doi=10.1145%2f3379156.3391343&partnerID=40&md5=914d5c3678263281c3132d36a5bcf649","Tokai University, Hiratsuka, Kanagawa, Japan","Tamura, Y., Tokai University, Hiratsuka, Kanagawa, Japan; Takemura, K., Tokai University, Hiratsuka, Kanagawa, Japan","Detecting the point-of-gaze in the real world is a challenging problem in eye-tracking applications. The point-of-gaze is estimated using geometry constraints, and user-calibration is required. In addition, the distances of the focused targets are variable and large in the real world. Therefore, a calibration-free approach without geometry constraints is needed to estimate the point-of-gaze. Recent studies have investigated smooth pursuit eye movements (smooth pursuits) for human-computer interaction applications, and we consider that these smooth pursuits can also be employed in eye tracking. Therefore, we developed a method for estimating the point-of-gaze using smooth pursuits without any requirement for implicit and explicit user-calibration. In this method, interest points are extracted from the scene image, and the point-of-gaze is detected using these points, which are strongly correlated with eye movements. We performed a comparative experiment in a real environment and demonstrated the feasibility of the proposed method. © 2020 ACM.","Eye tracking; Gaze estimation; Smooth pursuit eye movement","Calibration; Eye movements; Human computer interaction; Calibration free; Comparative experiments; Geometry constraints; Point of gaze; Real environments; Smooth pursuit; Smooth pursuit eye movement; User calibration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85085726287
"Yoo M.W., Han D.S.","57216618843;7403219442;","Optimization Algorithm for Driver Monitoring System using Deep Learning Approach",2020,"2020 International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2020",,,"9065222","043","046",,,"10.1109/ICAIIC48513.2020.9065222","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084058202&doi=10.1109%2fICAIIC48513.2020.9065222&partnerID=40&md5=9c835399b2dabc04da72310c9bab46b9","Kyungpook National University, Department of Future Automotive and IT Convergence, Daegu, South Korea; School of Electronics Engineering, Kyungpook National University, Daegu, South Korea","Yoo, M.W., Kyungpook National University, Department of Future Automotive and IT Convergence, Daegu, South Korea; Han, D.S., School of Electronics Engineering, Kyungpook National University, Daegu, South Korea","The driver monitoring system (DMS), also known as driver attention monitor, plays an important role for vehicle safety systems. In DMS, the system detects the driver's activities such as the state of being a driver sleepy or failure to give sufficient attention to avoid accidents. To reduce the driver mistakes while driving, the system warns the driver with an alarm sound or vibrations. For the efficient implementation of DMS, the system should work in real time without any delay. However, the current DMS has many challenges implementation to the complexity of network implementation. To reduce the DMS complexity for real time implementation, we propose an optimization algorithm, which uses a camera images for monitoring the driver activities. From the input camera images, we extract the driver's state information from the region of interest (ROI). In addition, the proposed system also extracts the driver's head pose and gaze information and monitors the driver states during driving. The experiment results from the proposed methods show accurate driver's state information and warns the driver immediately when any mistake occur from driver side. © 2020 IEEE.","3D Transformation; camera calibration; Deep neural network (DNN); Key-point detection","Cameras; Complex networks; Deep learning; Image segmentation; Monitoring; Optimization; Real time control; Safety engineering; Driver monitoring system; Efficient implementation; Learning approach; Optimization algorithms; Real-time implementations; State information; The region of interest (ROI); Vehicle safety systems; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85084058202
[无可用作者姓名],[无可用的作者 ID],"Stereoscopic Displays and Applications XXXI",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","2",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095603494&partnerID=40&md5=842475438846f0420480ae676bee3db6",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85095603494
[无可用作者姓名],[无可用的作者 ID],"Material Appearance 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","5",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095595345&partnerID=40&md5=92e5670ad2845b9ee692d0c5fbabfaa1",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85095595345
[无可用作者姓名],[无可用的作者 ID],"3D Measurement and Data Processing 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","17",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095588706&partnerID=40&md5=531ef20f156fc561873271aed850b040",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85095588706
[无可用作者姓名],[无可用的作者 ID],"Food and Agricultural Imaging Systems 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","12",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095574521&partnerID=40&md5=b83dac738d2d3a8fef7f932f304ea704",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85095574521
[无可用作者姓名],[无可用的作者 ID],"Visualization and Data Analysis 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","1",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095565231&partnerID=40&md5=160c7691e0baa6e0162326af5045805a",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85095565231
[无可用作者姓名],[无可用的作者 ID],"Image Quality and System Performance XVII",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","9",,"","",2224,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095454654&partnerID=40&md5=f7b23765cc404956f9c7ed62b7890cf4",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85095454654
[无可用作者姓名],[无可用的作者 ID],"Imaging Sensors and Systems 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","7",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095420902&partnerID=40&md5=7fccc0926e8bfd399a5f8464164e09ac",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85095420902
[无可用作者姓名],[无可用的作者 ID],"Imaging and Multimedia Analytics in a Web and Mobile World 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","8",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095115365&partnerID=40&md5=9719764f43d64eb9e5c98625689c63da",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85095115365
"Ewaisha M., El Shawarby M., Abbas H., Sobh I.","57219710517;57219717150;7102564939;57209617737;","End-to-end multitask learning for driver gaze and head pose estimation",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","16","110","","",,,"10.2352/ISSN.2470-1173.2020.16.AVM-110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094927422&doi=10.2352%2fISSN.2470-1173.2020.16.AVM-110&partnerID=40&md5=fc4633ae0dae18af8faad4d2e1891e2e","Valeo Group, Cairo, Egypt; Ain Shams University, Cairo, Egypt","Ewaisha, M., Valeo Group, Cairo, Egypt; El Shawarby, M.; Abbas, H., Ain Shams University, Cairo, Egypt; Sobh, I., Valeo Group, Cairo, Egypt","Modern automobiles accidents occur mostly due to inattentive behavior of drivers, which is why driver's gaze estimation is becoming a critical component in automotive industry. Gaze estimation has introduced many challenges due to the nature of the surrounding environment like changes in illumination, or driver's head motion, partial face occlusion, or wearing eye decorations. Previous work conducted in this field includes explicit extraction of hand-crafted features such as eye corners and pupil center to be used to estimate gaze, or appearance-based methods like Convolutional Neural Networks which implicitly extracts features from an image and directly map it to the corresponding gaze angle. In this work, a multitask Convolutional Neural Network architecture is proposed to predict subject's gaze yaw and pitch angles, along with the head pose as an auxiliary task, making the model robust to head pose variations, without needing any complex preprocessing or hand-crafted feature extraction.Then the network's output is clustered into nine gaze classes relevant in the driving scenario. The model achieves 95.8% accuracy on the test set and 78.2% accuracy in cross-subject testing, proving the model's generalization capability and robustness to head pose variation. © 2020, Society for Imaging Science and Technology.",,"Accidents; Convolution; Multi-task learning; Network architecture; Appearance-based methods; Critical component; Gaze estimation; Generalization capability; Head Pose Estimation; Partial faces; Pupil centers; Surrounding environment; Convolutional neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85094927422
[无可用作者姓名],[无可用的作者 ID],"Intelligent Robotics and Industrial Applications using Computer Vision 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","6",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094922404&partnerID=40&md5=27c3cfff7fc843e9f69d717521eef01d",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85094922404
[无可用作者姓名],[无可用的作者 ID],"The Engineering Reality of Virtual Reality 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","13",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094913958&partnerID=40&md5=5548cb931d640127668f820d61644a57",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85094913958
[无可用作者姓名],[无可用的作者 ID],"Human Vision and Electronic Imaging 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","11",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094904012&partnerID=40&md5=ea1160f94a8c57b8112f94bb6cc0f90a",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85094904012
[无可用作者姓名],[无可用的作者 ID],"Computational Imaging XVIII",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","14",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094899234&partnerID=40&md5=28da886280aa9867d35766505ce33cbe",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85094899234
[无可用作者姓名],[无可用的作者 ID],"Image Processing: Algorithms and Systems XVIII",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","10",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094898775&partnerID=40&md5=0b135bb3ed94a33ce57653a98a8381ce",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85094898775
[无可用作者姓名],[无可用的作者 ID],"Autonomous Vehicles and Machines 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","16",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094886601&partnerID=40&md5=ac1d02ed9bc3be2c40c35120ff4270ef",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85094886601
[无可用作者姓名],[无可用的作者 ID],"Media Watermarking, Security, and Forensics 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","4",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094882520&partnerID=40&md5=983eee634f31771dff98bbdb53205fbb",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85094882520
[无可用作者姓名],[无可用的作者 ID],"Mobile Devices and Multimedia: Technologies, Algorithms and Applications 2020",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","3",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094877225&partnerID=40&md5=142d029f79dfc7193787fb786e4f5d7d",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85094877225
[无可用作者姓名],[无可用的作者 ID],"Color Imaging XXV: Displaying, Processing, Hardcopy, and Applications",2020,"IS and T International Symposium on Electronic Imaging Science and Technology","2020","15",,"","",2244,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094876434&partnerID=40&md5=b24590cef27fb1f1573ae36876973ebf",,"","The proceedings contain 283 papers. The topics discussed include: a new training model for object detection in aerial images; managing crops across spatial and temporal scales - the roles of UAS and satellite remote sensing; a gaze-contingent system for foveated multiresolution visualization of vector and volumetric data; capturing and 3D rendering of optical behavior: the physical approach to realism; tereoscopic 3D optic flow distortions caused by mismatches between image acquisition and display parameters (JIST-first); active shooter response training environment for a building evacuation in a collaborative virtual environment; learning a CNN on multiple sclerosis lesion segmentation with self-supervision; reducing invertible embedding distortion using graph matching model; creating high-resolution 360-degree single-line 25k video content for modern conference rooms using film compositing techniques; spectral reproduction: drivers, use cases and workflow; SPECTRANET: a deep model for skin oxygenation measurement from multi-spectral data; automotive image quality concepts for the next SAE levels: color separation and contrast detection probability; a 4-tap global shutter pixel with enhanced IR sensitivity for VGA time-of-flight CMOS image sensors; small object bird detection in infrared drone videos using mask R-CNN deep learning; real-world fence removal from a single-image via deep neural network; and perceptual quality assessment of enhanced images using a crowd-sourcing framework.",,,Conference Review,"Final","",Scopus,2-s2.0-85094876434
"Han S.Y., Cho N.I.","57193417343;7201718669;","User-independent gaze estimation by extracting pupil parameter and its mapping to the gaze angle",2020,"Proceedings - International Conference on Pattern Recognition",,,"9412709","1993","2000",,,"10.1109/ICPR48806.2021.9412709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110536822&doi=10.1109%2fICPR48806.2021.9412709&partnerID=40&md5=04756865f7bd34c715e647e738b697de","Dep. of ECE, INMC Seoul National Univ, South Korea","Han, S.Y., Dep. of ECE, INMC Seoul National Univ, South Korea; Cho, N.I., Dep. of ECE, INMC Seoul National Univ, South Korea","Since gaze estimation plays a crucial role in recognizing human intentions, it has been researched for a long time, and its accuracy is ever increasing. However, due to the wide variation in eye shapes and focusing abilities between the individuals, accuracies of most algorithms vary depending on each person in the test group, especially when the initial calibration is not well performed. To alleviate the user-dependency, we attempt to derive features that are general for most people and use them as the input to a deep network instead of using the images as the input. Specifically, we use the pupil shape as the core feature because it is directly related to the 3D eyeball rotation, and thus the gaze direction. While existing deep learning methods learn the gaze point by extracting various features from the image, we focus on the mapping function from the eyeball rotation to the gaze point by using the pupil shape as the input. It is shown that the accuracy of gaze point estimation also becomes robust for the uncalibrated points by following the characteristics of the mapping function. Also, our gaze network learns the gaze difference to facilitate the re-calibration process to fix the calibration-drift problem that typically occurs with glass-type or head-mount devices. © 2020 IEEE",,"Calibration; Deep learning; Learning systems; Mapping; Calibration drift; Gaze direction; Gaze estimation; Gaze point estimations; Human intentions; Learning methods; Mapping functions; User independents; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85110536822
"Chugh S., Brousseau B., Rose J., Eizenman M.","57226118880;55601672200;35586991000;6701402159;","Detection and correspondence matching of corneal reflections for eye tracking using deep learning",2020,"Proceedings - International Conference on Pattern Recognition",,,"9412066","2210","2217",,,"10.1109/ICPR48806.2021.9412066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110506988&doi=10.1109%2fICPR48806.2021.9412066&partnerID=40&md5=857c88950038a4f82d4b84ec4c891c63","Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Department of Electrical and Computer Engineering, Ophthalmology and Vision Sciences, University of Toronto, Toronto, Canada","Chugh, S., Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Brousseau, B., Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Rose, J., Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Eizenman, M., Department of Electrical and Computer Engineering, Ophthalmology and Vision Sciences, University of Toronto, Toronto, Canada","Eye tracking systems that estimate the point-of-gaze are essential in extended reality (XR) systems as they enable new interaction paradigms and technological improvements. It is important for these systems to maintain accuracy when the headset moves relative to the head (known as device slippage) due to head movements or user adjustment. One of the most accurate eye tracking techniques, which is also insensitive to shifts of the system relative to the head, uses two or more infrared (IR) light emitting diodes to illuminate the eye and an IR camera to capture images of the eye. An essential step in estimating the point-of-gaze in these systems is the precise determination of the location of two or more corneal reflections (virtual images of the IR-LEDs that illuminate the eye) in images of the eye. Eye trackers tend to have multiple light sources to ensure at least one pair of reflections for each gaze position. The use of multiple light sources introduces a difficult problem: the need to match the corneal reflections with the corresponding light source over the range of expected eye movements. Corneal reflection detection and matching often fail in XR systems due to the proximity of camera and steep illumination angles of light sources with respect to the eye. The failures are caused by corneal reflections having varying shape and intensity levels or disappearance due to rotation of the eye, or the presence of spurious reflections. We have developed a fully convolutional neural network, based on the UNET architecture, that solves the detection and matching problem in the presence of spurious and missing reflections. Eye images of 25 people were collected in a virtual reality headset using a binocular eye tracking module consisting of five infrared light sources per eye. A set of 4,000 eye images were manually labelled for each of the corneal reflections, and data augmentation was used to generate a dataset of 40,000 images. The network is able to correctly identify and match 91% of corneal reflections present in the test set. This is comparable to a state-of-the-art deep learning system, but our approach requires 33 times less memory and executes 10 times faster. The proposed algorithm, when used in an eye tracker in a VR system, achieved an average mean absolute gaze error of 1°. This is a significant improvement over the state-of-the-art learning-based XR eye tracking systems that have reported gaze errors of 2-3°. © 2020 IEEE","Corneal reflections; Eye tracking; Pattern recognition; Semantic segmentation; Virtual reality","Cameras; Convolutional neural networks; Deep learning; Eye movements; Learning systems; Light sources; Pattern recognition; Virtual reality; Infrared light emitting diodes; Infrared light sources; Interaction paradigm; Multiple light source; Precise determinations; Spurious reflections; Technological improvements; Virtual-reality headsets; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85110506988
[无可用作者姓名],[无可用的作者 ID],"16th International Forum on Digital Media Communication, IFTC 2019",2020,"Communications in Computer and Information Science","1181",,,"","",428,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108577100&partnerID=40&md5=cc5ede6b2cb610465c94ec1b9ffe78fb",,"","The proceedings contain 34 papers. The special focus in this conference is on Digital Media Communication. The topics include: Interactive Face Liveness Detection Based on OpenVINO and Near Infrared Camera; multi-scale Generative Adversarial Learning for Facial Attribute Transfer; convolutional-Block-Attention Dual Path Networks for Slide Transition Detection in Lecture Videos; preface; fast Traffic Sign Detection Using Color-Specific Quaternion Gabor Filters; attention-Based Top-Down Single-Task Action Recognition in Still Images; adaptive Person-Specific Appearance-Based Gaze Estimation; preliminary Study on Visual Attention Maps of Experts and Nonexperts When Examining Pathological Microscopic Images; few-Shot Learning for Crossing-Sentence Relation Classification; image Classification of Submarine Volcanic Smog Map Based on Convolution Neural Network; multi-Scale Depthwise Separable Convolutional Neural Network for Hyperspectral Image Classification; Joint SPSL and CCWR for Chinese Short Text Entity Recognition and Linking; a Reading Assistant System for Blind People Based on Hand Gesture Recognition; intrusion Detection Based on Fusing Deep Neural Networks and Transfer Learning; the Competition of Garbage Classification Visual Recognition; smoke Detection Based on Image Analysis Technology; an Academic Achievement Prediction Model Enhanced by Stacking Network; blind Panoramic Image Quality Assessment Based on Project-Weighted Local Binary Pattern; blind 3D Image Quality Assessment Based on Multi-scale Feature Learning; research on Influence of Content Diversity on Full-Reference Image Quality Assessment; screen Content Picture Quality Evaluation by Colorful Sparse Reference Information; PMIQD 2019: A Pathological Microscopic Image Quality Database with Nonexpert and Expert Scores; a Generalized Cellular Automata Approach to Modelling Contagion and Monitoring for Emergent Events in Sensor Networks.",,,Conference Review,"Final","",Scopus,2-s2.0-85108577100
"Zheng C., Zhou J., Sun J., Zhao L.","57218370643;56203158200;57223816983;57224881018;","Adaptive Person-Specific Appearance-Based Gaze Estimation",2020,"Communications in Computer and Information Science","1181",,,"126","139",,,"10.1007/978-981-15-3341-9_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108459958&doi=10.1007%2f978-981-15-3341-9_11&partnerID=40&md5=896c2b4530b746eaddf16b9e2d255728","Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China; Shanghai Key Lab of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, 200240, China; Children’s Hospital of Shanghai, Shanghai, 200062, China","Zheng, C., Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China, Shanghai Key Lab of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, 200240, China; Zhou, J., Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China, Shanghai Key Lab of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, 200240, China; Sun, J., Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China, Shanghai Key Lab of Digital Media Processing and Transmissions, Shanghai Jiao Tong University, Shanghai, 200240, China; Zhao, L., Children’s Hospital of Shanghai, Shanghai, 200062, China","Non-invasive gaze estimation from only eye images captured by camera is a challenging problem due to various eye shapes, eye structures and image qualities. Recently, CNN network has been applied to directly regress eye image to gaze direction and obtains good performance. However, generic approaches are susceptible to bias and variance highly relating to different individuals. In this paper, we study the person-specific bias when applying generic methods on new person. And we introduce a novel appearance-based deep neural network integrating meta-learning to reduce the person-specific bias. Given only a few person-specific calibration images collected in normal calibration process, our model adapts quickly to test person and predicts more accurate gaze directions. Experiments on public MPIIGaze dataset and Eyediap dataset show our approach has achieved competitive accuracy to current state-of-the-art methods and are able to alleviate person-specific bias problem. © 2020, Springer Nature Singapore Pte Ltd.","Deep learning; Gaze estimation; Meta-learning; Person-specific bias","Calibration; Digital television; Multimedia systems; Appearance based; Bias and variance; Calibration process; Gaze direction; Gaze estimation; Generic approach; Generic method; State-of-the-art methods; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85108459958
"Zheng Y., Park S., Zhang X., de Mello S., Hilliges O.","57221387496;57195422868;57142162900;57201314496;14041644100;","Self-learning transformations for improving gaze and head redirection",2020,"Advances in Neural Information Processing Systems","2020-December",,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108406905&partnerID=40&md5=0f3148520704d8957043182e9933be0f","Department of Computer Science, ETH Zurich, Switzerland; NVIDIA","Zheng, Y., Department of Computer Science, ETH Zurich, Switzerland; Park, S., Department of Computer Science, ETH Zurich, Switzerland; Zhang, X., Department of Computer Science, ETH Zurich, Switzerland; de Mello, S., NVIDIA; Hilliges, O., Department of Computer Science, ETH Zurich, Switzerland","Many computer vision tasks rely on labeled data. Rapid progress in generative modeling has led to the ability to synthesize photorealistic images. However, controlling specific aspects of the generation process such that the data can be used for supervision of downstream tasks remains challenging. In this paper we propose a novel generative model for images of faces, that is capable of producing high-quality images under fine-grained control over eye gaze and head orientation angles. This requires the disentangling of many appearance related factors including gaze and head orientation but also lighting, hue etc. We propose a novel architecture which learns to discover, disentangle and encode these extraneous variations in a self-learned manner. We further show that explicitly disentangling task-irrelevant factors results in more accurate modelling of gaze and head orientation. A novel evaluation scheme shows that our method improves upon the state-of-the-art in redirection accuracy and disentanglement between gaze direction and head orientation changes. Furthermore, we show that in the presence of limited amounts of real-world training data, our method allows for improvements in the downstream task of semi-supervised cross-dataset gaze estimation. Please check our project page at: https://ait.ethz.ch/projects/2020/STED-gaze/ © 2020 Neural information processing systems foundation. All rights reserved.",,"Evaluation scheme; Fine-grained control; Generation process; High quality images; Novel architecture; Orientation angles; Orientation changes; Photorealistic images; Quality control",Conference Paper,"Final","",Scopus,2-s2.0-85108406905
"Bao Y., Cheng Y., Liu Y., Lu F.","57222730666;57220572010;57215289395;54956194300;","Adaptive feature fusion network for gaze tracking in mobile tablets",2020,"Proceedings - International Conference on Pattern Recognition",,,"9412205","9936","9943",,1,"10.1109/ICPR48806.2021.9412205","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103871941&doi=10.1109%2fICPR48806.2021.9412205&partnerID=40&md5=8a669c1894913fde791624b07644fe97","State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University, Beijing, China; Peng Cheng Laboratory, Shenzhen, China","Bao, Y., State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University, Beijing, China; Cheng, Y., State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University, Beijing, China; Liu, Y., State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University, Beijing, China; Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University, Beijing, China, Peng Cheng Laboratory, Shenzhen, China","Recently, many multi-stream gaze estimation methods have been proposed. They estimate gaze from eye and face appearances and achieve reasonable accuracy. However, most of the methods simply concatenate the features extracted from eye and face appearance. The feature fusion process has been ignored. In this paper, we propose a novel Adaptive Feature Fusion Network (AFF-Net), which performs gaze tracking task in mobile tablets. We stack two-eye feature maps and utilize Squeeze-and-Excitation layers to adaptively fuse two-eye features according to their similarity on appearance. Meanwhile, we also propose Adaptive Group Normalization to recalibrate eye features with the guidance of facial feature. Extensive experiments on both GazeCapture and MPIIFaceGaze datasets demonstrate consistently superior performance of the proposed method. © 2020 IEEE",,"Pattern recognition; Adaptive features; Facial feature; Feature fusion; Feature map; Gaze estimation; Gaze tracking; Multi-stream; Reasonable accuracy; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85103871941
"Wan Z., Xiong C., Li Q., Chen W., Wong K.K.L., Wu S.","57195490237;57211738191;57204585324;55715974300;26434942800;55913991500;","Accurate regression-based 3d gaze estimation using multiple mapping surfaces",2020,"IEEE Access","8",,,"166460","166471",,,"10.1109/ACCESS.2020.3023448","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102869728&doi=10.1109%2fACCESS.2020.3023448&partnerID=40&md5=f5d9066962878a17b78a41a3828ebe0d","State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, 430074, China; School of Information Science and Engineering, Institute of Robotics and Intelligent Systems, Wuhan University of Science and Technology, Wuhan, 430081, China","Wan, Z., State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, 430074, China; Xiong, C., State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, 430074, China; Li, Q., State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, 430074, China; Chen, W., State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, 430074, China; Wong, K.K.L., School of Information Science and Engineering, Institute of Robotics and Intelligent Systems, Wuhan University of Science and Technology, Wuhan, 430081, China; Wu, S., School of Information Science and Engineering, Institute of Robotics and Intelligent Systems, Wuhan University of Science and Technology, Wuhan, 430081, China","Accurate 3D gaze estimation using a simple setup remains a challenging issue for head-mounted eye tracking. Current regression-based gaze direction estimation methods implicitly assume that all gaze directions intersect at one point called the eyeball pseudo-center. The effect of this implicit assumption on gaze estimation is unknown. In this paper, we find that this assumption is approximate based on a simulation of all intersections of gaze directions, and it is conditional based on a sensitivity analysis of the assumption in gaze estimation. Hence, we propose a gaze direction estimation method with one mapping surface that satisfies conditions of the assumption by configuring one mapping surface and achieving a high-quality calibration of the eyeball pseudo-center. This method only adds two additional calibration points outside the mapping surface. Furthermore, replacing the eyeball pseudo-center with an additional calibrated surface, we propose a gaze direction estimation method with two mapping surfaces that further improves the accuracy of gaze estimation. This method improves accuracy on the state-of-the-art method by 20 percent (from a mean error of 1.84 degrees to 1.48 degrees) on a public dataset with a usage range of 1 meter and by 17 percent (from a mean error of 2.22 degrees to 1.85 degrees) on a public dataset with a usage range of 2 meters. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","3D gaze estimation; Eyeball center; Gaze direction estimation; Head-mounted eye tracking; Mapping surface","Calibration; Mapping; Sensitivity analysis; Calibration points; Gaze direction; Gaze estimation; Head-mounted eye tracking; High quality; Pseudo centers; Public dataset; State-of-the-art methods; Eye tracking",Article,"Final","",Scopus,2-s2.0-85102869728
"Valenzuela A., Arellano C., Tapia J.E.","57216819899;57207768339;7005419930;","Towards an efficient segmentation algorithm for near-infrared eyes images",2020,"IEEE Access","8",,,"171598","171607",,2,"10.1109/ACCESS.2020.3025195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102826657&doi=10.1109%2fACCESS.2020.3025195&partnerID=40&md5=b1c8a7c18899d23d9e50b6587f933b78","R+D CORFO Center SR-226, TOC Biometric, Santiago, 7520000, Chile; R+D Visiometrica, Santiago, 7500000, Chile; Escuela de Negocios, Universidad Adolfo Ibañez, Santiago, 7911547, Chile; Departamento de Ingenieria Informatica, Universidad de Santiago, Santiago, 9170197, Chile","Valenzuela, A., R+D CORFO Center SR-226, TOC Biometric, Santiago, 7520000, Chile; Arellano, C., R+D Visiometrica, Santiago, 7500000, Chile, Escuela de Negocios, Universidad Adolfo Ibañez, Santiago, 7911547, Chile; Tapia, J.E., R+D CORFO Center SR-226, TOC Biometric, Santiago, 7520000, Chile, Departamento de Ingenieria Informatica, Universidad de Santiago, Santiago, 9170197, Chile","Semantic segmentation has been widely used for several applications, including the detection of eye structures. This is used in tasks such as eye-tracking and gaze estimation, which are useful techniques for human-computer interfaces, salience detection, and Virtual reality (VR), amongst others. Most of the state of the art techniques achieve high accuracy but with a considerable number of parameters. This article explores alternatives to improve the efficiency of the state of the art method, namely DenseNet Tiramisu, when applied to NIR image segmentation. This task is not trivial; the reduction of block and layers also affects the number of feature maps. The growth rate (k) of the feature maps regulates how much new information each layer contributes to the global state, therefore the trade-off amongst grown rate (k), IOU, and the number of layers needs to be carefully studied. The main goal is to achieve a light-weight and efficient network with fewer parameters than traditional architectures in order to be used for mobile device applications. As a result, a DenseNet with only three blocks and ten layers is proposed (DenseNet10). Experiments show that this network achieved higher IOU rates when comparing with Encoder-Decoder, DensetNet56-67-103, MaskRCNN, and DeeplabV3+ models in the Facebook database. Furthermore, this method reached 8th place in The Facebook semantic segmentation challenge with 0.94293 mean IOU and 202.084 parameters with a final score of 0.97147. This score is only 0,001 lower than the first place in the competition. The sclera was identified as the more challenging structure to be segmented. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Biometrics; Deep learning; Semantic segmentation","Economic and social effects; Eye tracking; Growth rate; Image enhancement; Infrared devices; Semantics; Social networking (online); Human computer interfaces; Mobile device applications; Number of layers; Segmentation algorithms; Semantic segmentation; State-of-the-art methods; State-of-the-art techniques; Traditional architecture; Image segmentation",Article,"Final","",Scopus,2-s2.0-85102826657
"Chi J., Wang D., Lu N., Wang Z.","8702376200;57222478849;57217305267;55880036500;","Cornea radius calibration for remote 3D gaze tracking systems",2020,"IEEE Access","8",,,"187634","187647",,,"10.1109/ACCESS.2020.3029300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102809780&doi=10.1109%2fACCESS.2020.3029300&partnerID=40&md5=df0ec9c4a14ac64e7d4a6324da4254c0","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, 100083, China","Chi, J., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Wang, D., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Lu, N., School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China; Wang, Z., School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, 100083, China","Cornea radius estimation is a key technique for 3D gaze estimation in the single-camera 3D gaze tracking system. Traditional methods with one-camera-one-light-source systems or one-camera-two- light-source systems cannot achieve 3D gaze estimation. The 3D line-of-sight can be estimated only when the cornea radius is pre-calibrated by the user. A cornea radius calibration method based on the iris radius is proposed in this paper for 3D gaze estimation in remote one-camera-two-light-source systems. We first calibrate the iris radius based on the binocular strategy, estimate the spatial iris center using the calibrated iris radius, and then calibrate the cornea radius by a set of non-linear equations under the constraint of equivalent distances from the cornea center to the iris edge points. The calibrated cornea radius is verified by binocular optimization constraints. Simulations and physical experiments validate the effectiveness of the proposed method. The iris-based cornea radius calibration approach is novel; it can be used to obtain the cornea radius and 3D gaze using remote one-camera-one-light-source or one-camera-multi-light-source systems. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","3D line-of-sight estimation; Cornea radius; Corneal refiections; Gaze tracking; Optimization constraints; User calibration","Binoculars; Calibration; Cameras; Light sources; Calibration method; Gaze estimation; Gaze tracking system; Light-source systems; Multi-light sources; Physical experiments; Radius estimation; Single cameras; Eye tracking",Article,"Final","",Scopus,2-s2.0-85102809780
"He C., Zhou X., Wang C.","57222196402;55793091700;55890923700;","MPB: Multi-Peak Binarization for Pupil Detection",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12447 LNAI",,,"283","298",,,"10.1007/978-3-030-65390-3_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101879362&doi=10.1007%2f978-3-030-65390-3_22&partnerID=40&md5=dc87a6c784d3db34fa17beaa98cca586","School of Science, RMIT University, Melbourne, Australia; CSIRO Data61, Canberra, Australia","He, C., School of Science, RMIT University, Melbourne, Australia; Zhou, X., School of Science, RMIT University, Melbourne, Australia; Wang, C., CSIRO Data61, Canberra, Australia","Automatic pupil detection is a fundamental part of eye-related tasks like eye tracking, gaze estimation and eye movement identification. Especially, in ophthalmology, to provide assistance and fulfil the demand of diagnosis and treatment, an accurate and real-time algorithm is required. In this paper, we propose a fast and robust Multi-Peak Binarization (MPB) based method for pupil detection in ophthalmology scenarios. A novel strategy for region of interest and candidate connected area detection is presented. Constraints for pruning the irregular shapes and accelerating the MPB algorithm are defined. The proposed method is evaluated on an open-dataset and the experimental results demonstrate the high performance of our approach. © 2020, Springer Nature Switzerland AG.","High speed; Pupil center; Pupil detection","Eye movements; Eye tracking; Image segmentation; Ophthalmology; Area detection; Binarizations; Gaze estimation; Irregular shape; Novel strategies; Pupil detection; Real time algorithms; Region of interest; Data mining",Conference Paper,"Final","",Scopus,2-s2.0-85101879362
"Ortega J.D., Kose N., Cañas P., Chao M.-A., Unnervik A., Nieto M., Otaegui O., Salgado L.","55973067300;57217464990;57221350810;57221341228;57210321230;13106199700;8638431600;35230760000;","DMD: A Large-Scale Multi-modal Driver Monitoring Dataset for Attention and Alertness Analysis",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12538 LNCS",,,"387","405",,2,"10.1007/978-3-030-66823-5_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101757261&doi=10.1007%2f978-3-030-66823-5_23&partnerID=40&md5=04d8e6cf6884b866f6fea211608864c0","Vicomtech Foundation, Basque Research and Technology Alliance (BRTA), Mendaro, Spain; Dependability Research Lab, Intel Labs Europe, Intel Deutschland GmbH, Neubiberg, Germany; ETS Ingenieros de Telecomunicación, Universidad Politécnica de Madrid, Madrid, Spain","Ortega, J.D., Vicomtech Foundation, Basque Research and Technology Alliance (BRTA), Mendaro, Spain, ETS Ingenieros de Telecomunicación, Universidad Politécnica de Madrid, Madrid, Spain; Kose, N., Dependability Research Lab, Intel Labs Europe, Intel Deutschland GmbH, Neubiberg, Germany; Cañas, P., Vicomtech Foundation, Basque Research and Technology Alliance (BRTA), Mendaro, Spain; Chao, M.-A., Dependability Research Lab, Intel Labs Europe, Intel Deutschland GmbH, Neubiberg, Germany; Unnervik, A., Dependability Research Lab, Intel Labs Europe, Intel Deutschland GmbH, Neubiberg, Germany; Nieto, M., Vicomtech Foundation, Basque Research and Technology Alliance (BRTA), Mendaro, Spain; Otaegui, O., Vicomtech Foundation, Basque Research and Technology Alliance (BRTA), Mendaro, Spain; Salgado, L., ETS Ingenieros de Telecomunicación, Universidad Politécnica de Madrid, Madrid, Spain","Vision is the richest and most cost-effective technology for Driver Monitoring Systems (DMS), especially after the recent success of Deep Learning (DL) methods. The lack of sufficiently large and comprehensive datasets is currently a bottleneck for the progress of DMS development, crucial for the transition of automated driving from SAE Level-2 to SAE Level-3. In this paper, we introduce the Driver Monitoring Dataset (DMD), an extensive dataset which includes real and simulated driving scenarios: distraction, gaze allocation, drowsiness, hands-wheel interaction and context data, in 41 h of RGB, depth and IR videos from 3 cameras capturing face, body and hands of 37 drivers. A comparison with existing similar datasets is included, which shows the DMD is more extensive, diverse, and multi-purpose. The usage of the DMD is illustrated by extracting a subset of it, the dBehaviourMD dataset, containing 13 distraction activities, prepared to be used in DL training processes. Furthermore, we propose a robust and real-time driver behaviour recognition system targeting a real-world application that can run on cost-efficient CPU-only platforms, based on the dBehaviourMD. Its performance is evaluated with different types of fusion strategies, which all reach enhanced accuracy still providing real-time response. © 2020, Springer Nature Switzerland AG.","Driver actions; Driver behaviour recognition; Driver Monitoring Dataset; Driver state analysis; Multi-modal fusion","Computer vision; Cost effectiveness; Deep learning; Large dataset; Automated driving; Cost-effective technology; Driver monitoring; Driver monitoring system; Fusion strategies; Real time response; Simulated driving; Training process; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85101757261
"Lorenzo J., Parra I., Wirth F., Stiller C., Llorca D.F., Sotelo M.A.","57202483480;6701313464;57208073979;7102694878;16426189200;7004442920;","RNN-based Pedestrian Crossing Prediction using Activity and Pose-related Features",2020,"IEEE Intelligent Vehicles Symposium, Proceedings",,,"9304652","1801","1806",,3,"10.1109/IV47402.2020.9304652","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099884038&doi=10.1109%2fIV47402.2020.9304652&partnerID=40&md5=199ed166a4c3a6071c637420bddcea58","Universidad de Alcalá, Department of Computer Engineering, Madrid, Spain; Institut für Mess- und Regelungstechnik, Karlsruher Institut für Technologie, Karlsruhe, Germany","Lorenzo, J., Universidad de Alcalá, Department of Computer Engineering, Madrid, Spain; Parra, I., Universidad de Alcalá, Department of Computer Engineering, Madrid, Spain; Wirth, F., Institut für Mess- und Regelungstechnik, Karlsruher Institut für Technologie, Karlsruhe, Germany; Stiller, C., Institut für Mess- und Regelungstechnik, Karlsruher Institut für Technologie, Karlsruhe, Germany; Llorca, D.F., Universidad de Alcalá, Department of Computer Engineering, Madrid, Spain; Sotelo, M.A., Universidad de Alcalá, Department of Computer Engineering, Madrid, Spain","Pedestrian crossing prediction is a crucial task for autonomous driving. Numerous studies show that an early estimation of the pedestrian's intention can decrease or even avoid a high percentage of accidents. In this paper, different variations of a deep learning system are proposed to attempt to solve this problem. The proposed models are composed of two parts: a CNN-based feature extractor and an RNN module. All the models were trained and tested on the JAAD dataset. The results obtained indicate that the choice of the features extraction method, the inclusion of additional variables such as pedestrian gaze direction and discrete orientation, and the chosen RNN type have a significant impact on the final performance. © 2020 IEEE.",,"Crosswalks; Deep learning; Footbridges; Autonomous driving; Feature extractor; Features extraction; Gaze direction; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85099884038
"Rangesh A., Zhang B., Trivedi M.M.","57148239200;57221706345;7103153314;","Driver Gaze Estimation in the Real World: Overcoming the Eyeglass Challenge",2020,"IEEE Intelligent Vehicles Symposium, Proceedings",,,"9304573","1054","1059",,,"10.1109/IV47402.2020.9304573","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099878696&doi=10.1109%2fIV47402.2020.9304573&partnerID=40&md5=1949b29c97de2f728106211cd7d5c346","Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, CA  92092, United States","Rangesh, A., Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, CA  92092, United States; Zhang, B., Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, CA  92092, United States; Trivedi, M.M., Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, CA  92092, United States","A driver's gaze is critical for determining the driver's attention level, state, situational awareness, and readiness to take over control from partially and fully automated vehicles. Tracking both the head and eyes (pupils) can provide a reliable estimation of a driver's gaze using face images under ideal conditions. However, the vehicular environment introduces a variety of challenges that are usually unaccounted for - harsh illumination, nighttime conditions, and reflective/dark eyeglasses. Unfortunately, relying on head pose alone under such conditions can prove to be unreliable owing to significant eye movements. In this study, we offer solutions to address these problems encountered in the real world. To solve issues with lighting, we demonstrate that using an infrared camera with suitable equalization and normalization usually suffices. To handle eyeglasses and their corresponding artifacts, we adopt the idea of image-to-image translation using generative adversarial networks (GANs) to pre-process images prior to gaze estimation. To this end, we propose the Gaze Preserving CycleGAN (GPCycleGAN). This network preserves the driver's gaze while removing potential eyeglasses from infrared face images. Our approach exhibits improved performance and robustness on challenging real-world data spanning 13 subjects and a variety of driving conditions. © 2020 IEEE.",,"Eye movements; Adversarial networks; Attention level; Driving conditions; Image translation; Infra-red cameras; Infrared face images; Situational awareness; Vehicular environments; Eyeglasses",Conference Paper,"Final","",Scopus,2-s2.0-85099878696
"Tureckova A., Holik T., Oplatkova Z.K.","57205127502;57221118119;15043128400;","Dog face detection using yolo network",2020,"Mendel","26","2",,"17","22",,,"10.13164/mendel.2020.2.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098254118&doi=10.13164%2fmendel.2020.2.017&partnerID=40&md5=f07925328609266fe8335f3ec45c231b","Tomas Bata University in Zlin, Faculty of Applied Informatics, Czech Republic","Tureckova, A., Tomas Bata University in Zlin, Faculty of Applied Informatics, Czech Republic; Holik, T., Tomas Bata University in Zlin, Faculty of Applied Informatics, Czech Republic; Oplatkova, Z.K., Tomas Bata University in Zlin, Faculty of Applied Informatics, Czech Republic","This work presents the real-world application of the object detection which belongs to one of the current research lines in computer vision. Researchers are commonly focused on human face detection. Compared to that, the current paper presents a challenging task of detecting a dog face instead that is an object with extensive variability in appearance. The system utilises YOLO network, a deep convolution neural network, to predict bounding boxes and class confidences simultaneously. This paper documents the extensive dataset of dog faces gathered from two different sources and the training procedure of the detector. The proposed system was designed for realization on mobile hardware. This Doggie Smile application helps to snapshot dogs at the moment when they face the camera. The proposed mobile application can simultaneously evaluate the gaze directions of three dogs in scene more than 13 times per second, measured on iPhone XR. The average precision of the dogface detection system is 0.92. © 2020, Brno University of Technology. All rights reserved.","Deep Convolution Networks; Deep Learning; Dog Face Detection; IOS Mobile Application; Object Detection; YOLO",,Article,"Final","",Scopus,2-s2.0-85098254118
"Zhang X., Park S., Beeler T., Bradley D., Tang S., Hilliges O.","57142162900;57195422868;36496499200;35955665700;55914554200;14041644100;","ETH-XGaze: A Large Scale Dataset for Gaze Estimation Under Extreme Head Pose and Gaze Variation",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12350 LNCS",,,"365","381",,3,"10.1007/978-3-030-58558-7_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097383797&doi=10.1007%2f978-3-030-58558-7_22&partnerID=40&md5=68527fcf1f7daa7aef0b5d1957745f0c","Department of Computer Science, ETH Zurich, Zürich, Switzerland; Google Inc., Zürich, Switzerland; Zürich, Switzerland","Zhang, X., Department of Computer Science, ETH Zurich, Zürich, Switzerland; Park, S., Department of Computer Science, ETH Zurich, Zürich, Switzerland; Beeler, T., Google Inc., Zürich, Switzerland; Bradley, D., Zürich, Switzerland; Tang, S., Department of Computer Science, ETH Zurich, Zürich, Switzerland; Hilliges, O., Department of Computer Science, ETH Zurich, Zürich, Switzerland","Gaze estimation is a fundamental task in many applications of computer vision, human computer interaction and robotics. Many state-of-the-art methods are trained and tested on custom datasets, making comparison across methods challenging. Furthermore, existing gaze estimation datasets have limited head pose and gaze variations, and the evaluations are conducted using different protocols and metrics. In this paper, we propose a new gaze estimation dataset called ETH-XGaze, consisting of over one million high-resolution images of varying gaze under extreme head poses. We collect this dataset from 110 participants with a custom hardware setup including 18 digital SLR cameras and adjustable illumination conditions, and a calibrated system to record ground truth gaze targets. We show that our dataset can significantly improve the robustness of gaze estimation methods across different head poses and gaze angles. Additionally, we define a standardized experimental protocol and evaluation metric on ETH-XGaze, to better unify gaze estimation research going forward. The dataset and benchmark website are available at https://ait.ethz.ch/projects/2020/ETH-XGaze. © 2020, Springer Nature Switzerland AG.",,"Computer vision; Human computer interaction; Human robot interaction; Custom hardwares; Different protocols; Evaluation metrics; Experimental protocols; High resolution image; Illumination conditions; Large-scale dataset; State-of-the-art methods; Large dataset",Conference Paper,"Final","",Scopus,2-s2.0-85097383797
"Lv K., Sheng H., Xiong Z., Li W., Zheng L.","57192543016;7201933025;7202955989;57248763800;55437573700;","Improving Driver Gaze Prediction with Reinforced Attention",2020,"IEEE Transactions on Multimedia",,,,"","",,,"10.1109/TMM.2020.3038311","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097174963&doi=10.1109%2fTMM.2020.3038311&partnerID=40&md5=d229a8de0e0c841176784c4253f0b112","school of computer science and engineering, Beihang University, 12633 Beijing, China, 100083 (e-mail: lvkai@buaa.edu.cn); School of Computer Science and Engineering, Beihang University, 12633 Beijing, China, 100083 (e-mail: shenghao@buaa.edu.cn); School of Computer Science and Engineering, Beihang University, 12633 Beijing, China, (e-mail: xiongz@buaa.edu.cn); School of Computer Science and Engineering, Beihang University, 12633 Beijing, China, (e-mail: liwei@nlsde.buaa.edu.cn); Research School of Computer Science, Australian National University, Canberra, Australia, (e-mail: liang.zheng@anu.edu.au)","Lv, K., school of computer science and engineering, Beihang University, 12633 Beijing, China, 100083 (e-mail: lvkai@buaa.edu.cn); Sheng, H., School of Computer Science and Engineering, Beihang University, 12633 Beijing, China, 100083 (e-mail: shenghao@buaa.edu.cn); Xiong, Z., School of Computer Science and Engineering, Beihang University, 12633 Beijing, China, (e-mail: xiongz@buaa.edu.cn); Li, W., School of Computer Science and Engineering, Beihang University, 12633 Beijing, China, (e-mail: liwei@nlsde.buaa.edu.cn); Zheng, L., Research School of Computer Science, Australian National University, Canberra, Australia, (e-mail: liang.zheng@anu.edu.au)","We consider the task of driver gaze prediction: estimating where the location of the focus of a driver should be, based on a raw video of the outside environment. In practice, we output a probability map that gives the normalized probability of each point in a given scene being the object of the driver attention. Most existing methods (i.e., Coarse-to-Fine and Multi-branch) take an image or a video as input and directly output the fixation map. While successful, these methods can often produce highly scattered predictions, rendering them unreliable for real-world usage. Motivated by this observation, we propose the reinforced attention (RA) model as a regulatory mechanism to increase prediction density. Our method is built directly on top of existing methods, making it complementary to current approaches. Specifically, we first use Multi-branch to obtain an initial fixation map. Then, RA is trained using deep reinforcement learning to learn a location prediction policy, producing a reinforced attention. Finally, in order to obtain the final gaze prediction result, we combine the fixation map and the reinforced attention by a mask-guided multiplication. Experimental results show that our framework improves the accuracy of gaze prediction, and provides state-of-the-art performance on the DR(eye)VE dataset. CCBY","Computational modeling; Computer architecture; deep learning; driver attention; gaze prediction; Hidden Markov models; Predictive models; reinforcement learning; Semantics; Task analysis; Vehicles; video processing","Deep learning; Reinforcement learning; Coarse to fine; Driver attention; Fixation map; Location prediction; Probability maps; Real-world; Regulatory mechanism; State-of-the-art performance; Forecasting",Article,"Article in Press","",Scopus,2-s2.0-85097174963
"Nguyen D.-L., Putro M.D., Jo K.-H.","57214528479;57189328179;56978116100;","Human Eye Detector with Light-Weight and Efficient Convolutional Neural Network",2020,"Communications in Computer and Information Science","1287",,,"186","196",,,"10.1007/978-3-030-63119-2_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097080431&doi=10.1007%2f978-3-030-63119-2_16&partnerID=40&md5=b4dd31ad63511bd75a5246c6816ef165","School of Electrical Engineering, University of Ulsan, Ulsan, South Korea","Nguyen, D.-L., School of Electrical Engineering, University of Ulsan, Ulsan, South Korea; Putro, M.D., School of Electrical Engineering, University of Ulsan, Ulsan, South Korea; Jo, K.-H., School of Electrical Engineering, University of Ulsan, Ulsan, South Korea","The human eye detection plays an important role in computer vision. Along with face detection, it is widely applied in practical security, surveillance, and warning systems such as eye tracking system, eye disease detection, gaze detection, eye blink, and drowsiness detection system. There have been many studies to detect eyes from applying traditional methods to using modern methods based on machine learning and deep learning. This network is deployed with two main blocks, namely the feature extraction block and the detection block. The feature extraction block starts with the use of the convolution layers, C.ReLU (Concatenated Rectified Linear Unit) module, and max pooling layers alternately, followed by the last six inception modules and four convolution layers. The detection block is constructed by two sibling convolution layers using for classification and regression. The experiment was trained and tested on CEW (Closed Eyes In The Wild), BioID Face and GI4E (Gaze Interaction for Everybody) dataset with the results achieved 96.48%, 99.58%, and 75.52% of AP (Average Precision), respectively. The speed was tested in real-time by 37.65 fps (frames per second) on Intel Core I7-4770 CPU @ 3.40 GHz. © 2020, Springer Nature Switzerland AG.","Convolutional Neural Network (CNN); Deep learning; Drowsiness detection system; Eye detection","Convolution; Convolutional neural networks; Deep learning; Extraction; Eye protection; Eye tracking; Face recognition; Detection blocks; Drowsiness detection; Eye tracking systems; Frames per seconds; Gaze detection; Gaze interaction; Intel core i7; Linear units; Feature extraction",Conference Paper,"Final","",Scopus,2-s2.0-85097080431
"Zhang L., Zhang J., Lin Z., Měch R., Lu H., He Y.","37007172100;55444770600;7404230086;24169244200;8218163400;57212448603;","Unsupervised Video Object Segmentation with Joint Hotspot Tracking",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12359 LNCS",,,"490","506",,,"10.1007/978-3-030-58568-6_29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097061173&doi=10.1007%2f978-3-030-58568-6_29&partnerID=40&md5=b8f17a016d5628618c51d70ffe558808","Dalian University of Technology, Dalian, China; Adobe Research, Beijing, China; Naval Aviation University, Yantai, China","Zhang, L., Dalian University of Technology, Dalian, China; Zhang, J., Adobe Research, Beijing, China; Lin, Z., Adobe Research, Beijing, China; Měch, R., Adobe Research, Beijing, China; Lu, H., Dalian University of Technology, Dalian, China; He, Y., Naval Aviation University, Yantai, China","Object tracking is a well-studied problem in computer vision while identifying salient spots of objects in a video is a less explored direction in the literature. Video eye gaze estimation methods aim to tackle a related task but salient spots in those methods are not bounded by objects and tend to produce very scattered, unstable predictions due to the noisy ground truth data. We reformulate the problem of detecting and tracking of salient object spots as a new task called object hotspot tracking. In this paper, we propose to tackle this task jointly with unsupervised video object segmentation, in real-time, with a unified framework to exploit the synergy between the two. Specifically, we propose a Weighted Correlation Siamese Network (WCS-Net) which employs a Weighted Correlation Block (WCB) for encoding the pixel-wise correspondence between a template frame and the search frame. In addition, WCB takes the initial mask/hotspot as guidance to enhance the influence of salient regions for robust tracking. Our system can operate online during inference and jointly produce the object mask and hotspot track-lets at 33 FPS. Experimental results validate the effectiveness of our network design, and show the benefits of jointly solving the hotspot tracking and object segmentation problems. In particular, our method performs favorably against state-of-the-art video eye gaze models in object hotspot tracking, and outperforms existing methods on three benchmark datasets for unsupervised video object segmentation. © 2020, Springer Nature Switzerland AG.","Hotspot tracking; Unsupervised video object segmentation; Weighted correlation siamese network","Computer vision; Image segmentation; Motion compensation; Benchmark datasets; Ground truth data; Object segmentation; Salient objects; State of the art; Unified framework; Video-object segmentation; Weighted correlation; Object tracking",Conference Paper,"Final","",Scopus,2-s2.0-85097061173
[无可用作者姓名],[无可用的作者 ID],"15th International Conference on Hybrid Artificial Intelligent Systems, HAIS 2020",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12344 LNAI",,,"","",786,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097053965&partnerID=40&md5=69181a30cb4100bd6ab821053c2d6318",,"","The proceedings contain 65 papers. The special focus in this conference is on Hybrid Artificial Intelligent Systems. The topics include: Employing Decision Templates to Imbalanced Data Classification; comparison of Labeling Methods for Behavioral Activity Classification Based on Gaze Ethograms; PBIL for Optimizing Hyperparameters of Convolutional Neural Networks and STL Decomposition; an Evolutionary Approach to Automatic Keyword Selection for Twitter Data Analysis; PreCLAS: An Evolutionary Tool for Unsupervised Feature Selection; RADSSo: An Automated Tool for the multi-CASH Machine Learning Problem; a Metaheuristic Algorithm to Face the Graph Coloring Problem; tardiness Minimisation for Job Shop Scheduling with Interval Uncertainty; modified Grid Searches for Hyper-Parameter Optimization; fake News Detection by Means of Uncertainty Weighted Causal Graphs; supervised Hyperparameter Estimation for Anomaly Detection; Using the Variational-Quantum-Eigensolver (VQE) to Create an Intelligent Social Workers Schedule Problem Solver; fully Fuzzy Multi-objective Berth Allocation Problem; analysis of the Genetic Algorithm Operators for the Node Location Problem in Local Positioning Systems; Optimization of Learning Strategies for ARTM-Based Topic Models; a Cluster-Based Under-Sampling Algorithm for Class-Imbalanced Data; comparing Knowledge-Based Reinforcement Learning to Neural Networks in a Strategy Game; clustering Techniques Performance Analysis for a Solar Thermal Collector Hybrid Model Implementation; a Hybrid One-Class Topology for Non-convex Sets; a Machine Consciousness Architecture Based on Deep Learning and Gaussian Processes; An Hybrid Registration Method for SLAM with the M8 Quanergy LiDAR; some Experiments on the Influence of Problem Hardness in Morphological Development Based Learning of Neural Controllers; importance Weighted Adversarial Variational Bayes; averaging-Based Ensemble Methods for the Partial Label Ranking Problem.",,,Conference Review,"Final","",Scopus,2-s2.0-85097053965
"Cheng Y., Huang S., Wang F., Qian C., Lu F.","57220572010;57204286590;57197712252;57204290076;54956194300;","A Coarse-to-fine adaptive network for appearance-based gaze estimation",2020,"AAAI 2020 - 34th AAAI Conference on Artificial Intelligence",,,,"10623","10630",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095506412&partnerID=40&md5=8e7d800d7c8df133c5b8cc3c1822232b","State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University, China; SenseTime Co., Ltd.; Peng Cheng Laboratory, Shenzhen, China; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, China","Cheng, Y., State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University, China; Huang, S., SenseTime Co., Ltd.; Wang, F., SenseTime Co., Ltd.; Qian, C., SenseTime Co., Ltd.; Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University, China, Peng Cheng Laboratory, Shenzhen, China, Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, China","Human gaze is essential for various appealing applications. Aiming at more accurate gaze estimation, a series of recent works propose to utilize face and eye images simultaneously. Nevertheless, face and eye images only serve as independent or parallel feature sources in those works, the intrinsic correlation between their features is overlooked. In this paper we make the following contributions: 1) We propose a coarseto- fine strategy which estimates a basic gaze direction from face image and refines it with corresponding residual predicted from eye images. 2) Guided by the proposed strategy, we design a framework which introduces a bi-gram model to bridge gaze residual and basic gaze direction, and an attention component to adaptively acquire suitable fine-grained feature. 3) Integrating the above innovations, we construct a coarse-to-fine adaptive network named CA-Net and achieve state-of-the-art performances on MPIIGaze and EyeDiap. © AAAI 2020 - 34th AAAI Conference on Artificial Intelligence. All Rights Reserved.",,"Adaptive networks; Appearance based; Coarse to fine; Coarse-to-fine strategy; Fine grained; Gaze direction; Gaze estimation; State-of-the-art performance; Artificial intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85095506412
"Rakhmatulin I., Duchowski A.T.","57214824551;6701824388;","Deep neural networks for low-cost eye tracking",2020,"Procedia Computer Science","176",,,"685","694",,1,"10.1016/j.procs.2020.09.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093361943&doi=10.1016%2fj.procs.2020.09.041&partnerID=40&md5=8e47baac9dc6d55b60addbcc9e082c65","South Ural State University, Department of Power Plants Networks and Systems, Chelyabinsk, 454080, Russian Federation; Clemson University, 100 McAdams Hall, Clemson, SC  29634, United States","Rakhmatulin, I., South Ural State University, Department of Power Plants Networks and Systems, Chelyabinsk, 454080, Russian Federation; Duchowski, A.T., Clemson University, 100 McAdams Hall, Clemson, SC  29634, United States","The paper presents a detailed analysis of modern techniques that can be used to track gaze with a webcam. We present a practical implementation of the most popular methods for tracking gaze. Various models of deep neural networks that can be involved in the process of online gaze monitoring are reviewed. We introduce a new eye-tracking approach where the effectiveness of using a deep learning method is significantly increased. Implementation is in Python where its application is demonstrated by controlling interaction with the computer. Specifically, a dual coordinate system is given for controlling the computer with the help of a gaze. The first set of coordinates-the position of the face relative to the computer, is implemented by detecting color from the infrared LED via the OpenCV library. The second set of coordinates-giving gaze position-is obtained via the YOLO (v3) package. A method of labeling the eyes is given, in which 3 objects are used to track gaze (to the left, to the right, and in the center). © 2020 The Authors. Published by Elsevier B.V.","Deep learning in eye tracking; Deep learning in gaze tracking; Eye tracking; Yolov3 in eye tracking","Costs; Deep learning; Deep neural networks; Knowledge based systems; Learning systems; Neural networks; Co-ordinate system; Infrared leds; ITS applications; Learning methods; Low cost eye tracking; Modern techniques; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85093361943
"Park S., Aksan E., Zhang X., Hilliges O.","57195422868;57073715000;57142162900;14041644100;","Towards End-to-End Video-Based Eye-Tracking",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12357 LNCS",,,"747","763",,3,"10.1007/978-3-030-58610-2_44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093118867&doi=10.1007%2f978-3-030-58610-2_44&partnerID=40&md5=f16ab6bab356ba6b8a6cb197236b0ed9","Department of Computer Science, ETH Zurich, Zürich, Switzerland","Park, S., Department of Computer Science, ETH Zurich, Zürich, Switzerland; Aksan, E., Department of Computer Science, ETH Zurich, Zürich, Switzerland; Zhang, X., Department of Computer Science, ETH Zurich, Zürich, Switzerland; Hilliges, O., Department of Computer Science, ETH Zurich, Zürich, Switzerland","Estimating eye-gaze from images alone is a challenging task, in large parts due to un-observable person-specific factors. Achieving high accuracy typically requires labeled data from test users which may not be attainable in real applications. We observe that there exists a strong relationship between what users are looking at and the appearance of the user’s eyes. In response to this understanding, we propose a novel dataset and accompanying method which aims to explicitly learn these semantic and temporal relationships. Our video dataset consists of time-synchronized screen recordings, user-facing camera views, and eye gaze data, which allows for new benchmarks in temporal gaze tracking as well as label-free refinement of gaze. Importantly, we demonstrate that the fusion of information from visual stimuli as well as eye images can lead towards achieving performance similar to literature-reported figures acquired through supervised personalization. Our final method yields significant performance improvements on our proposed EVE dataset, with up to 28 % improvement in Point-of-Gaze estimates (resulting in 2. 49∘ in angular error), paving the path towards high-accuracy screen-based eye tracking purely from webcam sensors. The dataset and reference source code are available at https://ait.ethz.ch/projects/2020/EVE. © 2020, Springer Nature Switzerland AG.","Computer vision dataset; Eye tracking; Gaze estimation","Computer vision; Semantics; Angular errors; Personalizations; Real applications; Reference source; Temporal relationships; Video dataset; Video-based eye-tracking; Visual stimulus; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85093118867
"Yu Y., Odobez J.-M.","57188644020;57203103085;","Unsupervised representation learning for gaze estimation",2020,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",,,"9157235","7312","7322",,9,"10.1109/CVPR42600.2020.00734","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093107084&doi=10.1109%2fCVPR42600.2020.00734&partnerID=40&md5=76efd0300079e57e6b1e24c46b839875","Idiap Research Institute, Martigny, CH-1920, Switzerland; EPFL, Lausanne, CH-1015, Switzerland","Yu, Y., Idiap Research Institute, Martigny, CH-1920, Switzerland, EPFL, Lausanne, CH-1015, Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, CH-1920, Switzerland, EPFL, Lausanne, CH-1015, Switzerland","Although automatic gaze estimation is very important to a large variety of application areas, it is difficult to train accurate and robust gaze models, in great part due to the difficulty in collecting large and diverse data (annotating 3D gaze is expensive and existing datasets use different setups). To address this issue, our main contribution in this paper is to propose an effective approach to learn a low dimensional gaze representation without gaze annotations, which to the best of our best knowledge, is the first work to do so. The main idea is to rely on a gaze redirection network and use the gaze representation difference of the input and target images (of the redirection network) as the redirection variable. A redirection loss in image domain allows the joint training of both the redirection network and the gaze representation network. In addition, we propose a warping field regularization which not only provides an explicit physical meaning to the gaze representations but also avoids redirection distortions. Promising results on few-shot gaze estimation (competitive results can be achieved with as few as ≤ 100 calibration samples), cross-dataset gaze estimation, gaze network pretraining, and another task (head pose estimation) demonstrate the validity of our framework. © 2020 IEEE",,"Pattern recognition; Three dimensional computer graphics; Application area; Calibration samples; Effective approaches; Gaze estimation; Gaze representation; Head Pose Estimation; Low dimensional; Physical meanings; Large dataset",Conference Paper,"Final","",Scopus,2-s2.0-85093107084
"Jiao J., Cai Y., Alsharid M., Drukker L., Papageorghiou A.T., Noble J.A.","56414059500;57202376461;57189700785;36241434600;57194082999;56185660000;","Self-Supervised Contrastive Video-Speech Representation Learning for Ultrasound",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12263 LNCS",,,"534","543",,,"10.1007/978-3-030-59716-0_51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092701936&doi=10.1007%2f978-3-030-59716-0_51&partnerID=40&md5=15e2c88453c0f29c0b1f6480ba0a610b","Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Nuffield Department of Women’s & Reproductive Health, University of Oxford, Oxford, United Kingdom","Jiao, J., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Cai, Y., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Alsharid, M., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Drukker, L., Nuffield Department of Women’s & Reproductive Health, University of Oxford, Oxford, United Kingdom; Papageorghiou, A.T., Nuffield Department of Women’s & Reproductive Health, University of Oxford, Oxford, United Kingdom; Noble, J.A., Department of Engineering Science, University of Oxford, Oxford, United Kingdom","In medical imaging, manual annotations can be expensive to acquire and sometimes infeasible to access, making conventional deep learning-based models difficult to scale. As a result, it would be beneficial if useful representations could be derived from raw data without the need for manual annotations. In this paper, we propose to address the problem of self-supervised representation learning with multi-modal ultrasound video-speech raw data. For this case, we assume that there is a high correlation between the ultrasound video and the corresponding narrative speech audio of the sonographer. In order to learn meaningful representations, the model needs to identify such correlation and at the same time understand the underlying anatomical features. We designed a framework to model the correspondence between video and audio without any kind of human annotations. Within this framework, we introduce cross-modal contrastive learning and an affinity-aware self-paced learning scheme to enhance correlation modelling. Experimental evaluations on multi-modal fetal ultrasound video and audio show that the proposed approach is able to learn strong representations and transfers well to downstream tasks of standard plane detection and eye-gaze prediction. © 2020, Springer Nature Switzerland AG.","Representation learning; Self-supervised; Video-audio","Deep learning; Medical computing; Medical imaging; Ultrasonic applications; Anatomical features; Correlation modelling; Experimental evaluation; Eye-gaze predictions; Learning Based Models; Manual annotation; Self-paced learning; Ultrasound videos; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85092701936
"Kanda D., Kawai S., Nobuhara H.","57219345132;54393290500;7004339328;","Visualization method corresponding to regression problems and its application to deep learning-based gaze estimation model",2020,"Journal of Advanced Computational Intelligence and Intelligent Informatics","24","5",,"676","684",,1,"10.20965/JACIII.2020.P0676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092266729&doi=10.20965%2fJACIII.2020.P0676&partnerID=40&md5=932b018997ef71b36c984251ed376b36","Department of Intelligent Interaction Technologies, Graduate School of Systems and Information Engineering, University of Tsukuba, 1-1-1 Tennoudai, Tsukuba, Ibaraki, 305-8573, Japan","Kanda, D., Department of Intelligent Interaction Technologies, Graduate School of Systems and Information Engineering, University of Tsukuba, 1-1-1 Tennoudai, Tsukuba, Ibaraki, 305-8573, Japan; Kawai, S., Department of Intelligent Interaction Technologies, Graduate School of Systems and Information Engineering, University of Tsukuba, 1-1-1 Tennoudai, Tsukuba, Ibaraki, 305-8573, Japan; Nobuhara, H., Department of Intelligent Interaction Technologies, Graduate School of Systems and Information Engineering, University of Tsukuba, 1-1-1 Tennoudai, Tsukuba, Ibaraki, 305-8573, Japan","The human gaze contains substantial personal information and can be extensively employed in several applications if its relevant factors can be accurately measured. Further, several fields could be substantially innovated if the gaze could be analyzed using popular and familiar smart devices. Deep learning-based methods are robust, making them crucial for gaze estimation on smart devices. However, because internal functions in deep learning are black boxes, deep learning systems often make estimations for unclear reasons. In this paper, we propose a visualization method corresponding to a regression problem to solve the black box problem of the deep learning-based gaze estimation model. The proposed visualization method can clarify which region of an image contributes to deep learning-based gaze estimation. We visualized the gaze estimation model proposed by a research group at the Massachusetts Institute of Technology. The accuracy of the estimation was low, even when the facial features important for gaze estimation were recognized correctly. The effectiveness of the proposed method was further determined through quantitative evaluation using the area over the MoRF perturbation curve (AOPC). © 2020 Fuji Technology Press. All rights reserved.","CNN; Eye tracking; Grad-CAM; Regression problem","Learning systems; Visualization; Internal function; ITS applications; Learning-based methods; Massachusetts Institute of Technology; Personal information; Quantitative evaluation; Regression problem; Visualization method; Deep learning",Article,"Final","",Scopus,2-s2.0-85092266729
"Gonzalez-Sosa E., Perez P., Tolosana R., Kachach R., Villegas A.","56462764900;55914074500;55605251600;57190031363;55116907700;","Enhanced Self-Perception in Mixed Reality: Egocentric Arm Segmentation and Database with Automatic Labeling",2020,"IEEE Access","8",,"9152989","146887","146900",,,"10.1109/ACCESS.2020.3013016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090288040&doi=10.1109%2fACCESS.2020.3013016&partnerID=40&md5=0f8833470fd2ef21c5b1c35dc07c32c2","Nokia Bell Labs, Madrid, Spain; Departamento de Tecnologia Electronica y de Las Comunicaciones, Universidad Autonoma de Madrid, Madrid, Spain","Gonzalez-Sosa, E., Nokia Bell Labs, Madrid, Spain; Perez, P., Nokia Bell Labs, Madrid, Spain; Tolosana, R., Departamento de Tecnologia Electronica y de Las Comunicaciones, Universidad Autonoma de Madrid, Madrid, Spain; Kachach, R., Nokia Bell Labs, Madrid, Spain; Villegas, A., Nokia Bell Labs, Madrid, Spain","In this study, we focus on the egocentric segmentation of arms to improve self-perception in Augmented Virtuality (AV). The main contributions of this work are: $i$ ) a comprehensive survey of segmentation algorithms for AV; $ii$ ) an Egocentric Arm Segmentation Dataset (EgoArm), composed of more than 10, 000 images, demographically inclusive (variations of skin color, and gender), and open for research purposes. We also provide all details required for the automated generation of groundtruth and semi-synthetic images; $iii$ ) the proposal of a deep learning network to segment arms in AV; $iv$ ) a detailed quantitative and qualitative evaluation to showcase the usefulness of the deep network and EgoArm dataset, reporting results on different real egocentric hand datasets, including GTEA Gaze+, EDSH, EgoHands, Ego Youtube Hands, THU-Read, TEgO, FPAB, and Ego Gesture, which allow for direct comparisons with existing approaches using color or depth. Results confirm the suitability of the EgoArm dataset for this task, achieving improvements up to 40% with respect to the baseline network, depending on the particular dataset. Results also suggest that, while approaches based on color or depth can work under controlled conditions (lack of occlusion, uniform lighting, only objects of interest in the near range, controlled background, etc.), deep learning is more robust in real AV applications. © 2013 IEEE.","arm segmentation; augmented virtuality; automatic labeling; demographically inclusive; EgoArm dataset; Egocentric arm segmentation; mixed reality; self-perception","Color; Deep learning; Image segmentation; Mixed reality; Augmented virtualities; Automated generation; Automatic labeling; Baseline network; Controlled conditions; Qualitative evaluations; Research purpose; Segmentation algorithms; Color image processing",Article,"Final","",Scopus,2-s2.0-85090288040
"Akinyelu A.A., Blignaut P.","56132371000;6602384906;","Convolutional Neural Network-Based Methods for Eye Gaze Estimation: A Survey",2020,"IEEE Access","8",,"9153754","142581","142605",,4,"10.1109/ACCESS.2020.3013540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089957381&doi=10.1109%2fACCESS.2020.3013540&partnerID=40&md5=b6f5c2236a501b8613fa247523d30e8f","Department of Computer Science and Informatics, University of the Free State, Bloemfontein, 9301, South Africa","Akinyelu, A.A., Department of Computer Science and Informatics, University of the Free State, Bloemfontein, 9301, South Africa; Blignaut, P., Department of Computer Science and Informatics, University of the Free State, Bloemfontein, 9301, South Africa","Eye tracking is becoming a very important tool across many domains, including human-computer-interaction, psychology, computer vision, and medical diagnosis. Different methods have been used to tackle eye tracking, however, some of them are inaccurate under real-world conditions, while some require explicit user calibration which can be burdensome. Some of these methods suffer from poor image quality and variable light conditions. The recent success and prevalence of deep learning have greatly improved the performance of eye-tracking. The availability of large-scale datasets has further improved the performance of deep learning-based methods. This article presents a survey of the current state-of-the-art on deep learning-based gaze estimation techniques, with a focus on Convolutional Neural Networks (CNN). This article also provides a survey on other machine learning-based gaze estimation techniques. This study aims to empower the research community with valuable and useful insights that can enhance the design and development of improved and efficient deep learning-based eye-tracking models. This study also provides information on various pre-trained models, network architectures, and open-source datasets that are useful for training deep learning models. © 2013 IEEE.","computer vision; Convolutional neural network; deep learning; eye movements; eye tracking; gaze estimation; region of interest","Convolution; Convolutional neural networks; Deep learning; Diagnosis; Human computer interaction; Large dataset; Learning systems; Network architecture; Surveys; Design and Development; Large-scale datasets; Learning models; Learning-based methods; Light conditions; Research communities; State of the art; User calibration; Eye tracking",Article,"Final","",Scopus,2-s2.0-85089957381
"Yoon H.S., Park K.R.","57191035819;8983316300;","CycleGAN-Based Deblurring for Gaze Tracking in Vehicle Environments",2020,"IEEE Access","8",,"9149867","137418","137437",,1,"10.1109/ACCESS.2020.3012191","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089570104&doi=10.1109%2fACCESS.2020.3012191&partnerID=40&md5=9d3cf1ff3a440e7b6c2a5f5745697e06","Division of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea","Yoon, H.S., Division of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea; Park, K.R., Division of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea","Recently, the need for research on tracking driver gazes is increasing owing to the development of driver convenience systems, such as autonomous driving or intelligent driver monitoring system, to address traffic accidents caused by negligence. A camera is installed in a vehicle to track the driver's gaze in vehicle environment. The accuracy of estimating driver gazes in vehicle environments reduces if a motion blur of the driver occurs, owing to vehicular vibrations during driving. Most past studies on gaze-tracking of a driver in a vehicle did not consider the motion blurs in their experiments. To address this concern, we propose a method for improving the accuracy of gaze estimation by deblurring the blurred images of a driver from the vehicle. This study is the first attempt to calculate a driver's gaze by deblurring a motion blurred image with CycleGAN, whereas simultaneously using the image information from the two cameras in the vehicle. In previous studies, multiple deep CNNs were used for obtaining the images of a driver's eyes and face. In this study, information obtained from the two cameras in the vehicle are integrated into an image with three channels and thereafter deblurred, consequently reducing the time required for training. Whereas in previous studies the gaze position was not calculated for severe blurs by measuring the level of blur from the input image, the gaze position was calculated for all the input images in this study. From the database (Dongguk blurred gaze database (DBGD)) from 26 drivers in actual vehicles and the Columbia gaze dataset (CAVE-DB) that is an open database, the proposed method exhibited greater accuracy than the existing methods. © 2013 IEEE.","CycleGAN; Deblurring; driver's gaze detection; vehicle environment","Cameras; Database systems; Image enhancement; Motion tracking; Vehicles; Vibrations (mechanical); Autonomous driving; Blurred image; Gaze estimation; Gaze tracking; Image information; Intelligent drivers; Motion blurred image; Three channel; Eye tracking",Article,"Final","",Scopus,2-s2.0-85089570104
"Hasegawa S., Hirako A., Zheng X., Karimah S.N., Ota K., Unoki T.","7401474763;57218363667;57218365161;57190859275;14021969000;57203460019;","Learner’s mental state estimation with pc built-in camera",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12206 LNCS",,,"165","175",,1,"10.1007/978-3-030-50506-6_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088916166&doi=10.1007%2f978-3-030-50506-6_12&partnerID=40&md5=8e23b164582ce69b5e34a0e4704d7032","Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; IMAGICA GROUP/Photron, 1-105 Jinboucho, Kanda, Chiyoda, Tokyo, 101-0051, Japan","Hasegawa, S., Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; Hirako, A., Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; Zheng, X., Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; Karimah, S.N., Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; Ota, K., Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Ishikawa, Nomi, 923-1292, Japan; Unoki, T., IMAGICA GROUP/Photron, 1-105 Jinboucho, Kanda, Chiyoda, Tokyo, 101-0051, Japan","The purpose of this research is to estimate learners’ mental states such as difficulty, interest, fatigue, and concentration that change with the time series between learners and their learning tasks. Nowadays, we have many opportunities to learn specific topics in the individual learning process, such as active learning and self-directed learning. In such situations, it is challenging to grasp learners’ progress and engagement in their learning process. Several studies have estimated learners’ engagement from facial images/videos in the learning process. However, there is no extensive benchmark dataset except for the video watching process. Therefore, we gathered learners’ videos with facial expression and retrospective self-report from 19 participants through the CAB test process using a PC built-in camera. In this research, we applied an existing face image recognition library Face++ to extract the data such as estimated emotion, eye gaze, face orientation, face position (percentage on the screen) by each frame of the videos. Then, we built a couple of machine learning models, including deep learning methods, to estimate their mental states from the facial expressions and compared them with the average accuracy of prediction. The results demonstrated the potential of the proposed method to the estimation and provided the improvement plan from the accuracy point of view. © Springer Nature Switzerland AG 2020.","Machine learning; Mental state estimation; PC built-in camera","Built-in self test; Cameras; Deep learning; Face recognition; Human computer interaction; Image recognition; Benchmark datasets; Face image recognition; Facial Expressions; Improvement plans; Individual learning process; Learning methods; Machine learning models; Self-directed learning; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85088916166
[无可用作者姓名],[无可用的作者 ID],"22nd International Conference on Human-Computer Interaction, HCII 2020",2020,"Communications in Computer and Information Science","1224 CCIS",,,"","",1449,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088789243&partnerID=40&md5=3fd9f95c0d8f01143986b7c3ab1f72f5",,"","The proceedings contain 185 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: An AI-Based Approach to Automatic Waste Sorting; software Log Anomaly Detection Through One Class Clustering of Transformer Encoder Representation; automatic Spoken Language Identification Using Emotional Speech; research on Aesthetic Perception of Artificial Intelligence Style Transfer; the Development Dilemma and Countermeasures of Strong Artificial Intelligence in Meeting Human Emotional Needs; preliminary Investigation of Women Car Sharing Perceptions Through a Machine Learning Approach; WINS: Web Interface for Network Science via Natural Language Distributed Representations; conceptual Structure of the Virtual Environment as a Factor of Human-Computer Interaction; Classification and Recognition of Space Debris and Its Pose Estimation Based on Deep Learning of CNNs; simulation of Pseudo Inner Reading Voices and Evaluation of Effect on Human Processing; effect of Dialogs’ Arrangement on Accuracy and Workload for Confirming Input Data; BCI-Controlled Motor Imagery Training Can Improve Performance in e-Sports; Processing of Sensory Information is Affected by BCI Feedback Being Perceived; research on Visual Search Performance of Security Inspection Operations Based on Eye Movement Data; Evaluation of Incongruent Feeling During Mouse Operation Using Eye Gaze and EEG; EEG-Based Methods to Characterize Memorised Visual Space; the Impact of Viewing and Listening to Fantastic Events on Children’s Inhibitory Control; defect Annotation on Objects Using a Laser Remote Conrol; a Design Kit for Mobile Device-Based Interaction Techniques; comparison of the Remembering Ability by the Difference Between Handwriting and Typeface; an Interactive Game for Changing Youth Behavior Regarding E-cigarettes.",,,Conference Review,"Final","",Scopus,2-s2.0-85088789243
"Greinacher R., Voigt-Antons J.-N.","25628059700;57194940462;","Accuracy Assessment of ARKit 2 Based Gaze Estimation",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12181 LNCS",,,"439","449",,,"10.1007/978-3-030-49059-1_32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088750050&doi=10.1007%2f978-3-030-49059-1_32&partnerID=40&md5=3c56fd782a15f7db5275d621250c98be","Quality and Usability Lab, Technische Universität Berlin, Berlin, Germany; German Research Center for Artificial Intelligence (DFKI), Berlin, Germany","Greinacher, R., Quality and Usability Lab, Technische Universität Berlin, Berlin, Germany; Voigt-Antons, J.-N., Quality and Usability Lab, Technische Universität Berlin, Berlin, Germany, German Research Center for Artificial Intelligence (DFKI), Berlin, Germany","With the growing amount of mobile application usage, assuring a high quality of experience became more and more important. Besides traditional subjective methods to test and prototype new developments, eye tracking is a prominent tool to assess quality and UX of a software product. Although portable eye trackers exist, the technology is still mostly associated with expensive laboratory equipment. To change that and to run quick and cheap eye-tracking studies in the field, attempts have been made to turn everyday hardware like smartphone cameras and webcams into eye trackers. This study explores the possibility of using a standard library of iOS to tackle the vast technical complexity usually coming with such approaches. The accuracy of an eye-tracking system purely based on the ARKit APIs of iOS is evaluated in two user studies (N = 9 & N = 8). The results indicate that an ARKit based gaze tracker provides comparable performance in terms of accuracy (1.44 cm on screen), while at the same time, it uses far fewer hardware resources and provides a higher sample-rate than any other smartphone eye tracker. Especially the easy to use API is the main advantage over the technical complex systems which rely on their own image analysis for gaze estimation. Privacy implications are discussed. © 2020, Springer Nature Switzerland AG.","Accuracy; ARKit; Mobile eye tracking","Computer hardware; Human computer interaction; iOS (operating system); Laboratories; Quality of service; Smartphones; Software prototyping; Software testing; User experience; Accuracy assessment; Eye tracking systems; Eye-tracking studies; Laboratory equipments; Mobile applications; Smart-phone cameras; Standard libraries; Technical complexity; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85088750050
[无可用作者姓名],[无可用的作者 ID],"17th International Conference on Image Analysis and Recognition, ICIAR 2020",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12131 LNCS",,,"","",858,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087488656&partnerID=40&md5=4d087e234b7bf24aa3833d4432d4332b",,"","The proceedings contain 73 papers. The special focus in this conference is on Image Analysis and Recognition. The topics include: Slicing and dicing soccer: Automatic detection of complex events from spatio-temporal data; RN-vid: A feature fusion architecture for video object detection; color inference from semantic labeling for person search in videos; 2D bidirectional gated recurrent unit convolutional neural networks for end-to-end violence detection in videos; video based live tracking of fishes in tanks; using external knowledge to improve zero-shot action recognition in egocentric videos; a semantics-guided warping for semi-supervised video object instance segmentation; two-stream framework for activity recognition with 2d human pose estimation; video object segmentation using convex optimization of foreground and background distributions; flowChroma - A deep recurrent neural network for video colorization; deep learning for partial fingerprint inpainting and recognition; a visual perception framework to analyse neonatal pain in face images; combining asynchronous events and traditional frames for steering angle prediction; survey of preprocessing techniques and classification approaches in online signature verification; SSIM based signature of facial micro-expressions; learning to search for objects in images from human gaze sequences; detecting defects in materials using deep convolutional neural networks; visual perception ranking of chess players; video tampering detection for decentralized video transcoding networks; generalized subspace learning by roweis discriminant analysis; benchmark for generic product detection: A low data baseline for dense object detection; understanding public speakers’ performance: First contributions to support a computational approach; open source multipurpose multimedia annotation tool; SLAM-based multistate tracking system for mobile human-robot interaction.",,,Conference Review,"Final","",Scopus,2-s2.0-85087488656
[无可用作者姓名],[无可用的作者 ID],"Joint Conference ISASE-MAICS 2018 - 4th International Symposium on Affective Science and Engineering 2018, and the 29th Modern Artificial Intelligence and Cognitive Science Conference",2020,"Joint Conference ISASE-MAICS 2018 - 4th International Symposium on Affective Science and Engineering 2018, and the 29th Modern Artificial Intelligence and Cognitive Science Conference",,,,"","",206,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087475109&partnerID=40&md5=ec0bcfc17fca09efa7782d561ded8d5f",,"","The proceedings contain 39 papers. The topics discussed include: a convergence of decision-making criterions in a human-robot group; clinical desires to catch signals of human expression; holistic framework for accelerated learning by adapting & personalizing lesson plan for children based on emotions; proposal of a recommendation method by direct setting of preference patterns based on interrelationship mining; performance evaluation of interactive evolutionary computation applying gaze information; real-time grayscale dehazing scheme for car vision; motor imagery multi-task classification method .; expectation about contribution on comfortable nursing care from affective science and engineering; fatigue detection using facial landmarks; and deep learning of 2-D images representing N-D data in general line coordinates.",,,Conference Review,"Final","",Scopus,2-s2.0-85087475109
"Liu M., Li Y., Liu H.","57217199438;8589964900;56450301400;","3D Gaze Estimation for Head-Mounted Eye Tracking System with Auto-Calibration Method",2020,"IEEE Access","8",,"9107144","104207","104215",,7,"10.1109/ACCESS.2020.2999633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086730972&doi=10.1109%2fACCESS.2020.2999633&partnerID=40&md5=19d72f09a87bc0df58f3bf1758bbd30c","Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, 430079, China","Liu, M., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Li, Y., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Liu, H., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, 430079, China","The general challenges of 3D gaze estimation for head-mounted eye tracking systems are inflexible marker-based calibration procedure and significant errors of depth estimation. In this paper, we propose a 3D gaze estimation with an auto-calibration method. To acquire the accurate 3D structure of the environment, an RGBD camera is applied as the scene camera of our system. By adopting the saliency detection method, saliency maps can be acquired through scene images, and 3D salient pixels in the scene are considered potential 3D calibration targets. The 3D eye model is built on the basis of eye images to determine gaze vectors. By combining 3D salient pixels and gaze vectors, the auto-calibration can be achieved with our calibration method. Finally, the 3D gaze point is obtained through the calibrated gaze vectors, and the point cloud is generated from the RGBD camera. The experimental result shows that the proposed system can achieve an average accuracy of 3.7° in the range of 1 m to 4 m indoors and 4.0° outdoors. The proposed system also presents a great improvement in depth measurement, which is sufficient for tracking users' visual attention in real scenes. © 2013 IEEE.","3D gaze estimation; Auto-calibration; Head-mounted gaze tracking system; Saliency maps","3D modeling; Behavioral research; Calibration; Cameras; Pixels; Auto calibration; Auto-calibration method; Calibration method; Calibration procedure; Depth Estimation; Head-mounted eye tracking; Saliency detection; Visual Attention; Eye tracking",Article,"Final","",Scopus,2-s2.0-85086730972
"Hashimoto K., Haruta K., Tabiraki K., Nakamura K.W., Sakurai R., Nakata T.","57217175398;57217176276;57208134624;55926512500;30567697600;35305504000;","Head orientation detection with small camera in outdoor education using background images",2020,"Proceedings of SPIE - The International Society for Optical Engineering","11515",,"115151L","","",,,"10.1117/12.2566274","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086638670&doi=10.1117%2f12.2566274&partnerID=40&md5=78a76dbdc796a45d4d235fedb524234d","Toyama Prefectural University, Information Systems Engineering, Toyama, Japan; Matsumoto University, Department of Tourism and Hospitality, Nagano, Japan; University of Tokyo, Graduate School of Agricultural and Life Science, Tokyo, Japan; Ritsumeikan University, College of Policy Science, Osaka, Japan","Hashimoto, K., Toyama Prefectural University, Information Systems Engineering, Toyama, Japan; Haruta, K., Toyama Prefectural University, Information Systems Engineering, Toyama, Japan; Tabiraki, K., Matsumoto University, Department of Tourism and Hospitality, Nagano, Japan; Nakamura, K.W., University of Tokyo, Graduate School of Agricultural and Life Science, Tokyo, Japan; Sakurai, R., Ritsumeikan University, College of Policy Science, Osaka, Japan; Nakata, T., Toyama Prefectural University, Information Systems Engineering, Toyama, Japan","Evaluation of an outdoor education program is generally based on questionnaires or evaluator observations, which require considerable manpower and time. Therefore, assuming that the gaze information of learners such as an object of attention and fixation time can be used for educational evaluation, we propose the method that detects the learners' head orientation during a learning activity to use the parameter which input gaze estimation system using detected head orientation. This can be achieved by sequentially calculating the position and posture of a small camera mounted on each learner's head using a Structure from Motion (SfM) system. Besides, to match the features of key points extracted from stationary objects, this method also requires background images. The experimental results are obtained from a video recording of the actual field, a part of which is used to extract 16 input images. Furthermore, 781 input background images of the stationary objects in the educational field were captured after the learning activity was completed. We observed that among the 16 head-mounted camera images extracted every second, nine showed errors in a range of less than 7.13 degrees. © 2020 SPIE CCC.","Camera posture; Educational evaluation; Head orientation; Structure from Motion","Cameras; Surveys; Video recording; Background image; Educational evaluation; Head mounted Camera; Learning Activity; Orientation detections; Outdoor education; Stationary objects; Structure from motion; Learning systems",Conference Paper,"Final","",Scopus,2-s2.0-85086638670
"Zhou X., Jiang J., Liu Q., Fang J., Chen S., Cai H.","55743240400;57211817476;57202720113;37050903500;24491760700;56763253600;","Learning a 3D Gaze Estimator with Adaptive Weighted Strategy",2020,"IEEE Access","8",,"9079544","82142","82152",,,"10.1109/ACCESS.2020.2990685","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084929459&doi=10.1109%2fACCESS.2020.2990685&partnerID=40&md5=2bb466304ae455f7393019a01b630a61","College of Electrical and Information Engineering, Quzhou University, Quzhou, 324000, China; College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, 310023, China; School of Computer Communication and Engineering, Tianjin University of Technology, Tianjin, 300384, China; Department of Computer Science, Loughborough University, Loughborough, LE11 3TU, United Kingdom","Zhou, X., College of Electrical and Information Engineering, Quzhou University, Quzhou, 324000, China, College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, 310023, China; Jiang, J., College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, 310023, China; Liu, Q., College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, 310023, China; Fang, J., College of Electrical and Information Engineering, Quzhou University, Quzhou, 324000, China; Chen, S., College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, 310023, China, School of Computer Communication and Engineering, Tianjin University of Technology, Tianjin, 300384, China; Cai, H., Department of Computer Science, Loughborough University, Loughborough, LE11 3TU, United Kingdom","As a method of predicting the target's attention distribution, gaze estimation plays an important role in human-computer interaction. In this paper, we learn a 3D gaze estimator with adaptive weighted strategy to get the mapping from the complete images to the gaze vector. We select the both eyes, the complete face and their fusion features as the input of the regression model of gaze estimator. Considering that the different areas of the face have different contributions on the results of gaze estimation under free head movement, we design a new learning strategy for the regression net. To improve the efficiency of the regression model to a great extent, we propose a weighted network that can adjust the learning strategy of the regression net adaptively. Experimental results conducted on the MPIIGaze and EyeDiap datasets demonstrate that our method can achieve superior performance compared with other state-of-the-art 3D gaze estimation methods. © 2013 IEEE.","adaptive strategy; Gaze estimation; regression model; weighted network","Human computer interaction; Regression analysis; Free-head; Fusion features; Gaze estimation; Learning strategy; Regression model; State of the art; Weighted networks; Learning systems",Article,"Final","",Scopus,2-s2.0-85084929459
"Abdelwahab A., Landwehr N.","57205686686;10244371900;","Quantile Layers: Statistical Aggregation in Deep Neural Networks for Eye Movement Biometrics",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11907 LNAI",,,"332","348",,,"10.1007/978-3-030-46147-8_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084793074&doi=10.1007%2f978-3-030-46147-8_20&partnerID=40&md5=691d60ff05070a630c87e589a415816a","Leibniz Institute of Agricultural Engineering and Bioeconomy e.V. (ATB), Potsdam, Germany","Abdelwahab, A., Leibniz Institute of Agricultural Engineering and Bioeconomy e.V. (ATB), Potsdam, Germany; Landwehr, N., Leibniz Institute of Agricultural Engineering and Bioeconomy e.V. (ATB), Potsdam, Germany","Human eye gaze patterns are highly individually characteristic. Gaze patterns observed during the routine access of a user to a device or document can therefore be used to identify subjects unobtrusively, that is, without the need to perform an explicit verification such as entering a password. Existing approaches to biometric identification from gaze patterns segment raw gaze data into short, local patterns called saccades and fixations. Subjects are then identified by characterizing the distribution of these patterns or deriving hand-crafted features for them. In this paper, we follow a different approach by training deep neural networks directly on the raw gaze data. As the distribution of short, local patterns has been shown to be particularly informative for distinguishing subjects, we introduce a parameterized and end-to-end learnable statistical aggregation layer called the quantile layer that enables the network to explicitly fit the distribution of filter activations in preceding layers. We empirically show that deep neural networks with quantile layers outperform existing probabilistic and feature-based methods for identifying subjects based on eye movements by a large margin. © Springer Nature Switzerland AG 2020.","Biometry; Deep learning; Eye movements","Biometrics; Deep learning; Eye movements; Learning systems; Multilayer neural networks; Network layers; Biometric identifications; End to end; Feature-based method; Human eye; Large margins; Local patterns; Parameterized; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85084793074
"Saurav S., Mathur S., Sang I., Prasad S.S., Singh S.","56340567500;57216592395;57216591377;57216582958;55740404900;","Yawn Detection for Driver’s Drowsiness Prediction Using Bi-Directional LSTM with CNN Features",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11886 LNCS",,,"189","200",,2,"10.1007/978-3-030-44689-5_17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083998751&doi=10.1007%2f978-3-030-44689-5_17&partnerID=40&md5=0f05e4a04c493128e89bd5c76a949bf3","Academy of Scientific and Innovative Research (AcSIR), Ghaziabad, India; Birla-Institute of Technology and Science (BITS, Pilani), Goa Campus, Goa, India; CSIR - Central Electronics Engineering Research Institute (CSIR-CEERI), Pilani, India","Saurav, S., Academy of Scientific and Innovative Research (AcSIR), Ghaziabad, India, CSIR - Central Electronics Engineering Research Institute (CSIR-CEERI), Pilani, India; Mathur, S., Birla-Institute of Technology and Science (BITS, Pilani), Goa Campus, Goa, India; Sang, I., Birla-Institute of Technology and Science (BITS, Pilani), Goa Campus, Goa, India; Prasad, S.S., Academy of Scientific and Innovative Research (AcSIR), Ghaziabad, India, CSIR - Central Electronics Engineering Research Institute (CSIR-CEERI), Pilani, India; Singh, S., CSIR - Central Electronics Engineering Research Institute (CSIR-CEERI), Pilani, India","Drowsiness of drivers is a critical problem and has recently attracted a lot of attention from both academia and industry. A real-time driver’s drowsiness detection system is often considered as a crucial component of an Advanced Driver Assistance System (ADAS). Although, there are a number of physical parameters associated with drowsiness like blink frequency, eye closure duration, pose, gaze, etc., yawing can also be used as an indicator of drowsiness. This work presents a novel deep learning-based framework for driver’s drowsiness prediction based on yawn detection in a video stream. The proposed approach uses a combination of a convolutional neural network (CNN), 1D-CNN, and bi-directional LSTM (Bi-LSTM). In the first step, the pipeline extracts the mouth region from each frame of the video using a combination of face and landmark detector. In the subsequent step, spatial information from the mouth region is extracted using a pre-trained deep convolutional neural network (DCNN). Finally, temporal information which models the evaluation of yawn using the extracted mouth feature is learned using a blend of 1D-CNN and bi-directional LSTM (Bi-LSTM). Experiments were performed on manually extracted and annotated video clips obtained from two publically available drowsiness detection dataset namely YawDD and NTHU-DDD. Experimental results show the effectiveness of the proposed approach both in terms of recognition accuracy and computational efficiency. Thus, the proposed pipeline is a good candidate for real-time implementation of yawn detection system for driver’s drowsiness prediction on an embedded device. © 2020, Springer Nature Switzerland AG.","Bi-directional LSTM (Bi-LSTM); Convolutional neural networks; Drowsiness detection; Long short-term memory (LSTM)","Automobile drivers; Bismuth compounds; Computational efficiency; Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Forecasting; Human computer interaction; Long short-term memory; Pipelines; Real time control; Blink frequencies; Critical problems; Drowsiness detection; Physical parameters; Real-time implementations; Recognition accuracy; Spatial informations; Temporal information; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85083998751
"Ali A., Kim Y.-G.","57216564371;24081003200;","Deep Fusion for 3D Gaze Estimation from Natural Face Images Using Multi-Stream CNNs",2020,"IEEE Access","8",,"9062592","69212","69221",,2,"10.1109/ACCESS.2020.2986815","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083892421&doi=10.1109%2fACCESS.2020.2986815&partnerID=40&md5=9af37973f948c91545a4a08a372135b8","Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea","Ali, A., Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea; Kim, Y.-G., Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea","Over the last few decades, eye gaze estimation techniques have been thoroughly investigated by many researchers. However, predicting a 3D gaze from a 2D natural image remains challenging because it has to deal with several issues such as diverse head positions, face shape transformation, illumination variations, and subject individuality. Many previous studies employ convolutional neural networks (CNNs) for this task, and yet the accuracy needs improvement for its practical use. In this paper, we propose a 3D gaze estimation framework based on the data science perspective: First, a novel neural network architecture is designed to exploit every possible visual attribute such as the states of both eyes and the head position, including several augmentations; secondly, the data fusion method is utilized by incorporating multiple gaze datasets. Extensive experiments were carried out using two standard eye gaze datasets, including comparative analysis. The experimental results suggest that our method outperforms state-of-the-art with 2.8 degrees for MPIIGaze and 3.05 degrees for EYEDIAP dataset, respectively, indicating that it has a potential for real applications. © 2013 IEEE.","convolutional neural networks; data fusion; EYEDIAP; Gaze estimation; MPIIGaze","Data fusion; Network architecture; Comparative analysis; Data fusion methods; Gaze estimation; Illumination variation; Novel neural network; Real applications; State of the art; Visual attributes; Convolutional neural networks",Article,"Final","",Scopus,2-s2.0-85083892421
"Han S.Y., Kwon H.J., Kim Y., Cho N.I.","57193417343;57202583011;57194873756;7201718669;","Noise-Robust Pupil Center Detection through CNN-Based Segmentation with Shape-Prior Loss",2020,"IEEE Access","8",,"9055424","64739","64749",,8,"10.1109/ACCESS.2020.2985095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083699962&doi=10.1109%2fACCESS.2020.2985095&partnerID=40&md5=6a3166d863e409e4a0b9a04c99484c56","Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, 08826, South Korea","Han, S.Y., Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, 08826, South Korea; Kwon, H.J., Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, 08826, South Korea; Kim, Y., Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, 08826, South Korea; Cho, N.I., Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, 08826, South Korea","Detecting the pupil center plays a key role in human-computer interaction, especially for gaze tracking. The conventional deep learning-based method for this problem is to train a convolutional neural network (CNN), which takes the eye image as the input and gives the pupil center as a regression result. In this paper, we propose an indirect use of the CNN for the task, which first segments the pupil region by a CNN as a classification problem, and then finds the center of the segmented region. This is based on the observation that CNN works more robustly for the pupil segmentation than for the pupil center-point regression when the inputs are noisy IR images. Specifically, we use the UNet model for the segmentation of pupil regions in IR images and then find the pupil center as the center of mass of the segment. In designing the loss function for the segmentation, we propose a new loss term that encodes the convex shape-prior for enhancing the robustness to noise. Precisely, we penalize not only the deviation of each predicted pixel from the ground truth label but also the non-convex shape of pupils caused by the noise and reflection. For the training, we make a new dataset of 111,581 images with hand-labeled pupil regions from 29 IR eye video sequences. We also label commonly used datasets (ExCuSe and ElSe dataset) that are considered real-world noisy ones to validate our method. Experiments show that the proposed method performs better than the conventional methods that directly find the pupil center as a regression result. © 2013 IEEE.","Convex shape prior; deep learning; pupil segmentation; U-Net","Convolutional neural networks; Deep learning; Eye tracking; Human computer interaction; Infrared imaging; Regression analysis; Center of mass; Conventional methods; Learning-based methods; Non-convex shapes; Pupil segmentation; Robustness to noise; Segmented regions; Video sequences; Image segmentation",Article,"Final","",Scopus,2-s2.0-85083699962
"Selim M., Firintepe A., Pagani A., Stricker D.","56785358000;57216462432;23398181100;6701489212;","Autopose: Large-scale automotive driver head pose and gaze dataset with deep head orientation baseline",2020,"VISIGRAPP 2020 - Proceedings of the 15th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","4",,,"599","606",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083558622&partnerID=40&md5=781b51a8be76b3b98a8ac3c4592e3066","German Research Center for Artificial Intelligence (DFKI), Trippstadter Str. 122, Kaiserslautern, Germany; BMW Group, Munich, Germany","Selim, M., German Research Center for Artificial Intelligence (DFKI), Trippstadter Str. 122, Kaiserslautern, Germany; Firintepe, A., BMW Group, Munich, Germany; Pagani, A., German Research Center for Artificial Intelligence (DFKI), Trippstadter Str. 122, Kaiserslautern, Germany; Stricker, D., German Research Center for Artificial Intelligence (DFKI), Trippstadter Str. 122, Kaiserslautern, Germany","In computer vision research, public datasets are crucial to objectively assess new algorithms. By the wide use of deep learning methods to solve computer vision problems, large-scale datasets are indispensable for proper network training. Various driver-centered analysis depend on accurate head pose and gaze estimation. In this paper, we present a new large-scale dataset, AutoPOSE. The dataset provides ∼ 1.1M IR images taken from the dashboard view, and ∼ 315K from Kinect v2 (RGB, IR, Depth) taken from center mirror view. AutoPOSE’s ground truth -head orientation and position- was acquired with a sub-millimeter accurate motion capturing system. Moreover, we present a head orientation estimation baseline with a state-of-the-art method on our AutoPOSE dataset. We provide the dataset as a downloadable package from a public website. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Deep Learning; Driving; Eye Gaze; Head Pose Estimation; Infrared Camera; Kinect V2","Computer graphics; Computer vision; Deep learning; Infrared imaging; Learning systems; Accurate motion; Computer vision problems; Head orientation estimations; Large-scale dataset; Large-scale datasets; Learning methods; Network training; State-of-the-art methods; Large dataset",Conference Paper,"Final","",Scopus,2-s2.0-85083558622
"Cheng Y., Zhang X., Lu F., Sato Y.","57220572010;57142162900;54956194300;35230954300;","Gaze Estimation by Exploring Two-Eye Asymmetry",2020,"IEEE Transactions on Image Processing","29",,"9050633","5259","5272",,8,"10.1109/TIP.2020.2982828","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082921437&doi=10.1109%2fTIP.2020.2982828&partnerID=40&md5=96fcc7405e8bbd76cd9c02f775a9590c","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science, ETH Zurich, Zurich, Switzerland; Institute of Industrial Science, University of Tokyo, Tokyo, Japan","Cheng, Y., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Zhang, X., Department of Computer Science, ETH Zurich, Zurich, Switzerland; Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Sato, Y., Institute of Industrial Science, University of Tokyo, Tokyo, Japan","Eye gaze estimation is increasingly demanded by recent intelligent systems to facilitate a range of interactive applications. Unfortunately, learning the highly complicated regression from a single eye image to the gaze direction is not trivial. Thus, the problem is yet to be solved efficiently. Inspired by the two-eye asymmetry as two eyes of the same person may appear uneven, we propose the face-based asymmetric regression-evaluation network (FARE-Net) to optimize the gaze estimation results by considering the difference between left and right eyes. The proposed method includes one face-based asymmetric regression network (FAR-Net) and one evaluation network (E-Net). The FAR-Net predicts 3D gaze directions for both eyes and is trained with the asymmetric mechanism, which asymmetrically weights and sums the loss generated by two-eye gaze directions. With the asymmetric mechanism, the FAR-Net utilizes the eyes that can achieve high performance to optimize network. The E-Net learns the reliabilities of two eyes to balance the learning of the asymmetric mechanism and symmetric mechanism. Our FARE-Net achieves leading performances on MPIIGaze, EyeDiap and RT-Gene datasets. Additionally, we investigate the effectiveness of FARE-Net by analyzing the distribution of errors and ablation study. © 1992-2012 IEEE.","asymmetric regression; evaluation network; eye appearance; Gaze estimation","Intelligent systems; Asymmetric mechanisms; asymmetric regression; eye appearance; Eye images; Eye-gaze; Gaze direction; Gaze estimation; Interactive applications; Regression analysis",Article,"Final","",Scopus,2-s2.0-85082921437
"Panetta K., Wan Q., Rajeev S., Kaszowska A., Gardony A.L., Naranjo K., Taylor H.A., Agaian S.","6507727793;57188747389;57191541248;57203481497;36570266900;57216039069;7403057334;35321805400;","ISeeColor: Method for Advanced Visual Analytics of Eye Tracking Data",2020,"IEEE Access","8",,"9036879","52278","52287",,2,"10.1109/ACCESS.2020.2980901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082536811&doi=10.1109%2fACCESS.2020.2980901&partnerID=40&md5=b7ac1a242f50f86a339f324bc14e8e16","Department of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Department of Psychology, Tufts University, Medford, MA  02155, United States; U.S. Army Combat Capabilities Development Command Soldier Center, Natick, MA  01760, United States; Department of Computer Science, City University of New York, New York, NY  10019, United States","Panetta, K., Department of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Wan, Q., Department of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Rajeev, S., Department of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Kaszowska, A., Department of Psychology, Tufts University, Medford, MA  02155, United States; Gardony, A.L., U.S. Army Combat Capabilities Development Command Soldier Center, Natick, MA  01760, United States; Naranjo, K., Department of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Taylor, H.A., Department of Psychology, Tufts University, Medford, MA  02155, United States; Agaian, S., Department of Computer Science, City University of New York, New York, NY  10019, United States","Recent advances in head-mounted eye-tracking technology have allowed researchers to monitor eye movements during locomotion in real-world environments, increasing the ecological validity of research on human gaze behavior. While collecting eye-tracking data is becoming more accessible, visual analytics of eye-tracking data remains difficult and time-consuming. As such, there is a significant need for developing efficient visualization and analysis tools for large-scale eye-tracking data. This work develops a first-of-its-kind eye-tracking data visualization and analysis system that allows for automatic recognition of independent objects within field-of-vision, using deep-learning-based semantic segmentation. This system recolors the fixated objects-of-interest by integrating gaze fixation information with semantic maps. The system effectively allows researchers to automatically infer what objects users view and for how long in dynamic contexts. The contributions are 1) a data visualization and analysis system that uses deep-learning technology along with eye-tracking data to automatically recognize objects-of-interest from head-mounted eye-tracking video recordings, and 2) a graphical user interface that presents objects-of-interest annotation along with eye-tracking data information. The architecture is tested with an outdoor case study of users walking around the Tufts University campus as part of a navigation study, which was administered by a team of research psychologists. © 2013 IEEE.","cognitive science; data analysis; data visualization; deep-learning; Eye-trackers","Advanced Analytics; Behavioral research; Data reduction; Data visualization; Deep learning; Eye movements; Graphical user interfaces; Learning systems; Object tracking; Semantics; Video recording; Visualization; Automatic recognition; Cognitive science; Eye trackers; Head-mounted eye tracking; Independent objects; Real world environments; Semantic segmentation; Visualization and analysis; Eye tracking",Article,"Final","",Scopus,2-s2.0-85082536811
"Emoto J., Hirata Y.","57215926029;35797191900;","Lightweight convolutional neural network for image processing method for gaze estimation and eye movement event detection",2020,"IPSJ Transactions on Bioinformatics","13",,,"7","15",,2,"10.2197/ipsjtbio.13.7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082392820&doi=10.2197%2fipsjtbio.13.7&partnerID=40&md5=467f5e6f5adbb86e0870d1ef995f7570","Chubu University, Kasugai, Aichi, 487-8501, Japan","Emoto, J., Chubu University, Kasugai, Aichi, 487-8501, Japan; Hirata, Y., Chubu University, Kasugai, Aichi, 487-8501, Japan","Advancements in technology have recently made it possible to obtain various types of biometric information from humans, enabling studies on estimation of human conditions in medicine, automobile safety, marketing, and other areas. These studies have particularly pointed to eye movement as an effective indicator of human conditions, and research on its applications is actively being pursued. The devices now widely used for measuring eye movements are based on the video-oculography (VOG) method, wherein the direction of gaze is estimated by processing eye images obtained through a camera. Applying convolutional neural networks (ConvNet) to the processing of eye images has been shown to enable accurate and robust gaze estimation. Conventional image processing, however, is premised on execution using a personal computer, making it difficult to carry out real-time gaze estimation using ConvNet, which involves the use of a large number of parameters, in a small arithmetic unit. Also, detecting eye movement events, such as blinking and saccadic movements, from the inferred gaze direction sequence for particular purposes requires the use of a separate algorithm. We therefore propose a new eye image processing method that batch-processes gaze estimation and event detection from end to end using an independently designed lightweight ConvNet. This paper discusses the structure of the proposed lightweight ConvNet, the methods for learning and evaluation used, and the proposed method's ability to simultaneously detect gaze direction and event occurrence using a smaller memory and at lower computational complexity than conventional methods. © 2020 Information Processing Society of Japan. All rights reserved.","Blink; Deep learning; Multi-task; Pupil; Saccade","Batch data processing; Convolution; Convolutional neural networks; Deep learning; Image processing; Multi-task learning; Personal computers; Processing; Biometric informations; Blink; Conventional methods; Human conditions; Image processing - methods; Pupil; Separate algorithm; Video oculography; Eye movements",Article,"Final","",Scopus,2-s2.0-85082392820
"Wang W., Chen X., Zheng S., Li H.","57215831361;55739042600;57215828936;57215834831;","Fast Head Pose Estimation via Rotation-Adaptive Facial Landmark Detection for Video Edge Computation",2020,"IEEE Access","8",,"9020163","45023","45032",,3,"10.1109/ACCESS.2020.2977729","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082003182&doi=10.1109%2fACCESS.2020.2977729&partnerID=40&md5=81cf30ed53752f8c1011b7d5a2b9ea89","College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, 300222, China; IriStar Technology Ltd., Beijing, 100191, China","Wang, W., College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, 300222, China; Chen, X., College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, 300222, China; Zheng, S., College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, 300222, China; Li, H., IriStar Technology Ltd., Beijing, 100191, China","The human head pose estimation is an important and challenging problem, which provides the estimation of the head posture in 3D space from 2D image. It is a crucial technique for face recognition, gaze estimation, facial attribute recognition, etc. However, fast head pose estimation executing on the terminal for video edge computation has many challenges due to the computational complexity of the existing algorithms. In this paper, we propose a fast head pose estimation method based on a novel Rotation-Adaptive facial landmark detection powered by Local Binary Feature (RALBF). The landmark detection method is structured through fusing the prior of the rotation information provided by the Progressive Calibration Networks (PCN) face detector to a Local Binary Feature (LBF) based landmark detection method, which improves the robustness against head pose variations and simultaneously keep the computing efficiency. RALBF is trained and tested on 300W dataset and AFLW2000 dataset, it is verified by the accuracy evaluation that RALBF performs better than LBF. To improve the speed of head pose estimation, the 68, 51 and 10 landmarks distribution schemes are explored and compared on speed and accuracy. In the 10 landmarks scheme, the head pose estimation running once only takes 8.3ms on Intel i7-6700HQ CPU and takes 21.8ms on HiSilicon SoC Hi3519AV100, and the average error of Euler angle is 5.9973° when the face yaw angle is between ±35° on AFLW2000 3D dataset. Experiments demonstrate our approach performing well on real scenes. © 2013 IEEE.","facial landmark detection; Head pose estimation; local binary features; PnP problem","Edge computing; Feature extraction; System-on-chip; Attribute recognition; Binary features; Calibration network; Computing efficiency; Distribution scheme; Facial landmark detection; Head Pose Estimation; PnP problems; Face recognition",Article,"Final","",Scopus,2-s2.0-85082003182
"Murakami J., Mitsugami I.","57205643723;13106192400;","Gaze from Head: Gaze Estimation Without Observing Eye",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12046 LNCS",,,"254","267",,,"10.1007/978-3-030-41404-7_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081601763&doi=10.1007%2f978-3-030-41404-7_18&partnerID=40&md5=af52208fd2db482ea84f014e4473fbf8","Hiroshima City University, Hiroshima, Japan","Murakami, J., Hiroshima City University, Hiroshima, Japan; Mitsugami, I., Hiroshima City University, Hiroshima, Japan","We propose a gaze estimation method not from eye observation but from head motion. This proposed method is based on physiological studies about the eye-head coordination, and the gaze direction is estimated from observation of head motion by using the eye-head coordination model trained by preliminarily collected data of gaze direction and head pose sequence. We collected gaze-head datasets of from people who walked around under real and VR environments, constructed the eye-head coordination models from those datasets, and evaluated them quantitatively. In addition, we confirmed that there was no significant difference between the models from the real and VR datasets in their estimation accuracy. © Springer Nature Switzerland AG 2020.","Eye-head coordination; Gaze estimation; Virtual realty","Physiological models; Eye-head coordination; Gaze direction; Gaze estimation; Head motion; Head pose; Virtual realty; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85081601763
[无可用作者姓名],[无可用的作者 ID],"MMISR 2019 - Proceedings of Models and Methods of Information Systems Research Workshop in the frame of the Betancourt International Engineering Forum 2019",2020,"CEUR Workshop Proceedings","2556",,,"","",111,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081594570&partnerID=40&md5=bf9cb48133a5d87ff014e73e22e5d2b6",,"","The proceedings contain 19 papers. The topics discussed include: the socio-technical impact of the current disruptive technologies on the smart city concept realization; recognition of manuscript tables in computer processing of technical transport documentation; verification of the results of psychosemantic survey by eyes-gaze-tracking; the use of self-learning systems to solve the problems of finding failures on the railway; stochastic model of a high-loaded monitoring system of data transmission network; digital services for open e-learning quality assurance; about complex objects defining via integration of data from various sources; implementation of deep learning methods in the tasks of ensuring information and psychological safety for operators of automated railway traffic control systems; and problems of building the intelligent consistent control logic for complex technical systems in transport industry.",,,Conference Review,"Final","",Scopus,2-s2.0-85081594570
[无可用作者姓名],[无可用的作者 ID],"5th Asian Conference on Pattern Recognition, ACPR 2019",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12046 LNCS",,,"","",1688,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081562242&partnerID=40&md5=c0b8663cbdaf15c3b03c76da21daa430",,"","The proceedings contain 123 papers. The special focus in this conference is on Pattern Recognition. The topics include: A New Forged Handwriting Detection Method Based on Fourier Spectral Density and Variation; robust Pedestrian Detection: Faster Deployments with Fusion of Models; perceptual Image Anomaly Detection; deep Similarity Fusion Networks for One-Shot Semantic Segmentation; seeing Things in Random-Dot Videos; boundary Extraction of Planar Segments from Clouds of Unorganised Points; real-Time Multi-class Instance Segmentation with One-Time Deep Embedding Clustering; multi-person Pose Estimation with Mid-Points for Human Detection under Real-World Surveillance; gaze from Head: Gaze Estimation Without Observing Eye; interaction Recognition Through Body Parts Relation Reasoning; label-Smooth Learning for Fine-Grained Visual Categorization; DeepHuMS: Deep Human Motion Signature for 3D Skeletal Sequences; towards a Universal Appearance for Domain Generalization via Adversarial Learning; pre-trained and Shared Encoder in Cycle-Consistent Adversarial Networks to Improve Image Quality; MobileGAN: Compact Network Architecture for Generative Adversarial Network; denoising and Inpainting of Sea Surface Temperature Image with Adversarial Physical Model Loss; confidence Map Based 3D Cost Aggregation with Multiple Minimum Spanning Trees for Stereo Matching; optical Flow Assisted Monocular Visual Odometry; coarse-to-Fine Deep Orientation Estimator for Local Image Matching; geometric Total Variation for Image Vectorization, Zooming and Pixel Art Depixelizing; information Theory-Based Curriculum Learning Factory to Optimize Training; forestNet – Automatic Design of Sparse Multilayer Perceptron Network Architectures Using Ensembles of Randomized Trees; A Factorization Strategy for Tensor Robust PCA; speeding up of the Nelder-Mead Method by Data-Driven Speculative Execution; efficient Bayesian Optimization Based on Parallel Sequential Random Embeddings.",,,Conference Review,"Final","",Scopus,2-s2.0-85081562242
"Ohshima Y., Nakazawa A.","57215657216;35807510800;","Eye Contact Detection from Third Person Video",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12047 LNCS",,,"667","677",,,"10.1007/978-3-030-41299-9_52","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081550936&doi=10.1007%2f978-3-030-41299-9_52&partnerID=40&md5=2ff0a4b86b0d9bc37177fbff8ce69d69","Department of Informatics, Kyoto University, Kyoto, Japan","Ohshima, Y., Department of Informatics, Kyoto University, Kyoto, Japan; Nakazawa, A., Department of Informatics, Kyoto University, Kyoto, Japan","Eye contact is fundamental for human communication and social interactions; therefore much effort has been made to develop automated eye-contact detection using image recognition techniques. However, existing methods use first-person-videos (FPV) that need participants to equip wearable cameras. In this work, we develop an novel eye contact detection algorithm taken from normal viewpoint (third person video) assuming the scenes of conversations or social interactions. Our system have high affordability since it does not require special hardware or recording setups, moreover, can use pre-recorded videos such as Youtube and home videos. In designing algorithm, we first develop DNN-based one-sided gaze estimation algorithms which output the states whether the one subject looks at another. Afterwards, eye contact is found at the frame when the pair of one-sided gaze happens. To verify the proposed algorithm, we generate third-person eye contact video dataset using publicly available videos from Youtube. As the result, proposed algorithms performed 0.775 in precision and 0.671 in recall, while the existing method performed 0.484 in precision and 0.061 in recall, respectively. © 2020, Springer Nature Switzerland AG.","Deep neural nets; Eye contact; Human communication; Image recognition; Third person video","Image recognition; Deep neural nets; Eye contact; Gaze estimation; Human communications; Social interactions; Special hardware; Third person video; Wearable cameras; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85081550936
"Guan J., Yin L., Sun J., Qi S., Wang X., Liao Q.","57202816841;57215423484;56683220200;54397704900;57192625391;55903680000;","Enhanced Gaze Following via Object Detection and Human Pose Estimation",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11962 LNCS",,,"502","513",,,"10.1007/978-3-030-37734-2_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080950894&doi=10.1007%2f978-3-030-37734-2_41&partnerID=40&md5=26a2524c83093faa3ed3ee806c7d1080","College of Computer Science and Technology, Harbin Engineering University, Harbin, 150001, China; Department of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China; Peng Cheng Laboratory, Shenzhen, 518000, China","Guan, J., College of Computer Science and Technology, Harbin Engineering University, Harbin, 150001, China; Yin, L., Department of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China; Sun, J., College of Computer Science and Technology, Harbin Engineering University, Harbin, 150001, China; Qi, S., Department of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China; Wang, X., Department of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China; Liao, Q., Department of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China, Peng Cheng Laboratory, Shenzhen, 518000, China","The aim of gaze following is to estimate the gaze direction, which is useful for the understanding of human behaviour in various applications. However, it is still an open problem that has not been fully studied. In this paper, we present a novel framework for gaze following problem, where both the front/side face case and the back face case are taken into account. For the front/side face case, head pose estimation is applied to estimate the gaze, and then object detection is used to further refine the gaze direction by selecting the object that intersects with the gaze in a certain range. For the back face case, a deep neural network with the human pose information is proposed for gaze estimation. Experiments are carried out to demonstrate the superiority of the proposed method, as compared with the state-of-the-art method. © 2020, Springer Nature Switzerland AG.","Deep neural network; Gaze following; Human pose estimation; Objection detection","Behavioral research; Deep neural networks; Object recognition; Following problem; Gaze direction; Gaze estimation; Gaze following; Head Pose Estimation; Human behaviours; Human pose estimations; State-of-the-art methods; Object detection",Conference Paper,"Final","",Scopus,2-s2.0-85080950894
"Yan X., Wang Z., Sun M.","57204951521;24175350400;8927434200;","Eye fixation assisted detection of video salient objects",2020,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11691 LNAI",,,"211","223",,,"10.1007/978-3-030-39431-8_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080892715&doi=10.1007%2f978-3-030-39431-8_20&partnerID=40&md5=e3aeb0736ae2464ce63b2dd486be2402","College of Intelligence and Computing, Tianjin University, Tianjin, China","Yan, X., College of Intelligence and Computing, Tianjin University, Tianjin, China; Wang, Z., College of Intelligence and Computing, Tianjin University, Tianjin, China; Sun, M., College of Intelligence and Computing, Tianjin University, Tianjin, China","With the increasing maturity of image saliency detection, more and more people are focusing their research on video saliency detection. Currently, video saliency detection can be divided into two forms, eye fixation detection and salient objects detection. In this article, we focus on exploring the relationship between them. Firstly, we propose a network called fixation assisted video salient object detection network (FAVSODNet), which uses the eye gaze information in videos to assist in detecting video salient objects. A fixation assisted module (FAM) is designed to connect FP task and SOD task deeply. Under the guidance of the eye fixation information, multiple salient objects in complex scene can be detected more correctly. Moreover, when the scene suddenly changes or a new person appears, it can better to detect the correct salient objects with the aid of fixation maps. In addition, we adopt an extended multi-scale feature extraction module (EMFEM) to extract rich object features. Thus, the neural network can aware the objects with variable scales in videos more comprehensively. Finally, the experimental results show that our method advances the state-of-art in video salient object detection. © Springer Nature Switzerland AG 2020.","Deep learning; Eye fixation prediction; Video salient object detection","Brain; Cognitive systems; Deep learning; Object recognition; Complex scenes; Eye fixations; Fixation map; Image saliencies; Multi-scale features; Salient object detection; Salient objects; Video saliencies; Object detection",Conference Paper,"Final","",Scopus,2-s2.0-85080892715
"Zemmari A., Benois-Pineau J.","6507308959;6701750610;","Case study for digital cultural content mining",2020,"SpringerBriefs in Computer Science",,,,"71","85",,,"10.1007/978-3-030-34376-7_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078856360&doi=10.1007%2f978-3-030-34376-7_8&partnerID=40&md5=791e04c49c477f75fcc1e68584464166","Laboratoire Bordelais de Recherche en Informatique (LaBRI), University of Bordeaux, Talence Cedex, France","Zemmari, A., Laboratoire Bordelais de Recherche en Informatique (LaBRI), University of Bordeaux, Talence Cedex, France; Benois-Pineau, J., Laboratoire Bordelais de Recherche en Informatique (LaBRI), University of Bordeaux, Talence Cedex, France","In this chapter we consider an application case of Deep Learning in the task of architectural recognition. The main objective is to identify both: architectural styles and specific architectural structures. We are interested in attention mechanisms in Deep CNNs and explain how real visual attention maps built upon human gaze fixations can help in the training of deep neural networks. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2020.",,"Architecture; Behavioral research; Data mining; Deep neural networks; Architectural structure; Architectural style; Attention mechanisms; Cultural content; Visual Attention; Deep learning",Book Chapter,"Final","",Scopus,2-s2.0-85078856360
"Vo M.T., Nguyen T., Le T.","57203062383;57194430442;55513981400;","Robust Head Pose Estimation Using Extreme Gradient Boosting Machine on Stacked Autoencoders Neural Network",2020,"IEEE Access","8",,"8945218","3687","3694",,6,"10.1109/ACCESS.2019.2962974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078479614&doi=10.1109%2fACCESS.2019.2962974&partnerID=40&md5=c76146b0261f98792c31515c2b2ebd1f","Institute of Research and Development, Duy Tan University, Da Nang, 550000, Viet Nam; Faculty of Information Technology, Ho Chi Minh City Open University, Ho Chi Minh City, 700000, Viet Nam; Faculty of Information Technology, Ho Chi Minh City University of Technology (HUTECH), Ho Chi Minh, 700000, Viet Nam","Vo, M.T., Institute of Research and Development, Duy Tan University, Da Nang, 550000, Viet Nam; Nguyen, T., Faculty of Information Technology, Ho Chi Minh City Open University, Ho Chi Minh City, 700000, Viet Nam; Le, T., Faculty of Information Technology, Ho Chi Minh City University of Technology (HUTECH), Ho Chi Minh, 700000, Viet Nam","Head pose estimation is an important sign in helping robots and other intelligence machines understand human. It plays a vital role in designing human computer interaction systems because many applications rely on precise results of head pose angles such as human behavior analysis, gaze estimation, 3D head reconstruction etc. This study presents a robust approach for estimating the head pose angles in a single image. More specifically, the proposed system first encodes the global features extracted from Histogram of Oriented Gradients in a multi stacked autoencoders neural network. Based on the hidden nodes in deep layers, Autoencoder has been proposed for feature reduction while maintaining the key information of data. A scalable gradient boosting machine is then employed to train and classify the embedded features. Experiences have evaluated on the Pointing 04 dataset and show that the proposed approach outperforms the state-of-the-art methods with the low head pose angle errors in pitch and yaw as 6.16° and 7.17°, respectively. © 2013 IEEE.","autoencoder; feature reduction; global features; gradient boosting; Head pose estimation","Behavioral research; Intelligent robots; Learning systems; Auto encoders; Feature reduction; Global feature; Gradient boosting; Head Pose Estimation; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85078479614
"Martinikorena I., Larumbe-Bergera A., Ariz M., Porta S., Cabeza R., Villanueva A.","56655118600;57210106737;57204069029;7005292345;36763933900;7101612861;","Low Cost Gaze Estimation: Knowledge-Based Solutions",2020,"IEEE Transactions on Image Processing","29",,"8876860","2328","2343",,2,"10.1109/TIP.2019.2946452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078305098&doi=10.1109%2fTIP.2019.2946452&partnerID=40&md5=d7acc940f9a89d2c8b7d7e15537b1f2a","Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, 31006, Spain; IDISNA, Imaging Platform, Center for Applied Medical Research, University of Navarra, Pamplona, 31009, Spain","Martinikorena, I., Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, 31006, Spain; Larumbe-Bergera, A., Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, 31006, Spain; Ariz, M., Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, 31006, Spain, IDISNA, Imaging Platform, Center for Applied Medical Research, University of Navarra, Pamplona, 31009, Spain; Porta, S., Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, 31006, Spain; Cabeza, R., Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, 31006, Spain; Villanueva, A., Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, 31006, Spain","Eye tracking technology in low resolution scenarios is not a completely solved issue to date. The possibility of using eye tracking in a mobile gadget is a challenging objective that would permit to spread this technology to non-explored fields. In this paper, a knowledge based approach is presented to solve gaze estimation in low resolution settings. The understanding of the high resolution paradigm permits to propose alternative models to solve gaze estimation. In this manner, three models are presented: a geometrical model, an interpolation model and a compound model, as solutions for gaze estimation for remote low resolution systems. Since this work considers head position essential to improve gaze accuracy, a method for head pose estimation is also proposed. The methods are validated in an optimal framework, I2Head database, which combines head and gaze data. The experimental validation of the models demonstrates their sensitivity to image processing inaccuracies, critical in the case of the geometrical model. Static and extreme movement scenarios are analyzed showing the higher robustness of compound and geometrical models in the presence of user's displacement. Accuracy values of about 3° have been obtained, increasing to values close to 5° in extreme displacement settings, results fully comparable with the state-of-the-art. © 1992-2012 IEEE.","eye tracking; Gaze estimation methods; low resolution","Costs; Geometry; Image processing; Knowledge based systems; Experimental validations; Eye tracking technologies; Gaze estimation; Geometrical modeling; Geometrical models; Head Pose Estimation; Knowledge-based approach; Low resolution; Eye tracking",Article,"Final","",Scopus,2-s2.0-85078305098
"Su D., Li Y.-F., Chen H.","57193014913;8589964900;57191584510;","Cross-validated locally polynomial modeling for 2-D/3-D gaze tracking with head-worn devices",2020,"IEEE Transactions on Industrial Informatics","16","1","8789483","510","521",,6,"10.1109/TII.2019.2933481","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078230221&doi=10.1109%2fTII.2019.2933481&partnerID=40&md5=1a4d116f0e72b36bd8e0c329c9a3ef51","Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, Hong Kong","Su, D., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, Hong Kong; Li, Y.-F., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, Hong Kong; Chen, H., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, Hong Kong","In the context of wearable gaze tracking techniques, the problems of two-dimensional (2-D) and three-dimensional (3-D) gaze estimation can be viewed as inferring 2-D epipolar lines and 3-D visual axes from eye monitoring cameras. To this end, in this article, a simple local polynomial model is proposed to back-project a pupil center onto its corresponding visual axis. Based on this approximation, a homographylike relation is derived in a local manner, and via the Leave-One-Out cross-validation criterion, training gaze samples at one certain depth is leveraged to partition entire input space into multiple overlapping subregions. Then, the gaze data at another depth are utilized to recover the epipolar point, i.e., the image eyeball center. Thus, given a pupil image, the corresponding epipolar line can be determined by the resolved homographylike mapping and the epipolar point. By using the same partition structure, 3-D gaze prediction model can be inferred by solving a nonlinear optimization problem, which aims to minimize the angular disparities between training visual directions and prediction ones. Meanwhile, it is necessary to form a good starting point and suitable constraints for the optimization problem. Otherwise, it may end up with trivial solutions, i.e., faraway eye positions. To facilitate the practical implementation of our proposed method, we also analyze how the spatial distribution of calibration points impacts the model learning accuracy. The experiment results justify the effectiveness of our proposed gaze estimation method for both the normal vision and eyewear users. © 2005-2012 IEEE.","human-machine interaction; locally polynomial model; mobile gaze tracking; parallax; Two-dimensional/three-dimensional (2-D/3-D) gaze point","Geometrical optics; Nonlinear programming; Polynomials; Statistical methods; Gaze point; Gaze tracking; Human machine interaction; parallax; Polynomial modeling; Eye tracking",Article,"Final","",Scopus,2-s2.0-85078230221
"Rot P., Vitek M., Grm K., Emeršič Ž., Peer P., Štruc V.","57201851285;57211963704;57189093547;56097253100;7003277146;17347474600;","Deep Sclera Segmentation and Recognition",2020,"Advances in Computer Vision and Pattern Recognition",,,,"395","432",,9,"10.1007/978-3-030-27731-4_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075529211&doi=10.1007%2f978-3-030-27731-4_13&partnerID=40&md5=5e80859e233059234516cb2b520c097e","Faculty of Computer and Information Science, University of Ljubljana, Večna pot 113, Ljubljana, 1000, Slovenia; Faculty of Electrical Engineering, University of Ljubljana, Tržaška cesta 25, Ljubljana, 1000, Slovenia","Rot, P., Faculty of Computer and Information Science, University of Ljubljana, Večna pot 113, Ljubljana, 1000, Slovenia; Vitek, M., Faculty of Computer and Information Science, University of Ljubljana, Večna pot 113, Ljubljana, 1000, Slovenia; Grm, K., Faculty of Electrical Engineering, University of Ljubljana, Tržaška cesta 25, Ljubljana, 1000, Slovenia; Emeršič, Ž., Faculty of Computer and Information Science, University of Ljubljana, Večna pot 113, Ljubljana, 1000, Slovenia; Peer, P., Faculty of Computer and Information Science, University of Ljubljana, Večna pot 113, Ljubljana, 1000, Slovenia; Štruc, V., Faculty of Electrical Engineering, University of Ljubljana, Tržaška cesta 25, Ljubljana, 1000, Slovenia","In this chapter, we address the problem of biometric identity recognition from the vasculature of the human sclera. Specifically, we focus on the challenging task of multi-view sclera recognition, where the visible part of the sclera vasculature changes from image to image due to varying gaze (or view) directions. We propose a complete solution for this task built around Convolutional Neural Networks (CNNs) and make several contributions that result in state-of-the-art recognition performance, i.e.: (i) we develop a cascaded CNN assembly that is able to robustly segment the sclera vasculature from the input images regardless of gaze direction, and (ii) we present ScleraNET, a CNN model trained in a multi-task manner (combining losses pertaining to identity and view-direction recognition) that allows for the extraction of discriminative vasculature descriptors that can be used for identity inference. To evaluate the proposed contributions, we also introduce a new dataset of ocular images, called the Sclera Blood Vessels, Periocular and Iris (SBVPI) dataset, which represents one of the few publicly available datasets suitable for research in multi-view sclera segmentation and recognition. The datasets come with a rich set of annotations, such as a per-pixel markup of various eye parts (including the sclera vasculature), identity, gaze-direction and gender labels. We conduct rigorous experiments on SBVPI with competing techniques from the literature and show that the combination of the proposed segmentation and descriptor-computation models results in highly competitive recognition performance. © 2020, The Author(s).","Dataset; Deep learning; Eye recognition; Ocular biometrics; Sclera recognition; Sclera segmentation; Vascular biometrics",,Book Chapter,"Final","",Scopus,2-s2.0-85075529211
"Xia Y., Lou J., Dong J., Qi L., Li G., Yu H.","57192671097;57192670261;22634069200;36606745500;13408307700;56115992300;","Hybrid regression and isophote curvature for accurate eye center localization",2020,"Multimedia Tools and Applications","79","1-2",,"805","824",,4,"10.1007/s11042-019-08160-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073823365&doi=10.1007%2fs11042-019-08160-5&partnerID=40&md5=5d73a8024dbcdf7a94e93763eb4922fc","School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Key Laboratory of Metallurgical Equipment and Control Technology, Wuhan University of Science and Technology, Wuhan, China","Xia, Y., School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Lou, J., School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Dong, J., Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Qi, L., Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Li, G., Key Laboratory of Metallurgical Equipment and Control Technology, Wuhan University of Science and Technology, Wuhan, China; Yu, H., School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom","The eye center localization is a crucial requirement for various human-computer interaction applications such as eye gaze estimation and eye tracking. However, although significant progress has been made in the field of eye center localization in recent years, it is still very challenging for tasks under the significant variability situations caused by different illumination, shape, color and viewing angles. In this paper, we propose a hybrid regression and isophote curvature for accurate eye center localization under low resolution. The proposed method first applies the regression method, which is called Supervised Descent Method (SDM), to obtain the rough location of eye region and eye centers. SDM is robust against the appearance variations in the eye region. To make the center points more accurate, isophote curvature method is employed on the obtained eye region to obtain several candidate points of eye center. Finally, the proposed method selects several estimated eye center locations from the isophote curvature method and SDM as our candidates and a SDM-based means of gradient method further refine the candidate points. Therefore, we combine regression and isophote curvature method to achieve robustness and accuracy. In the experiment, we have extensively evaluated the proposed method on the two public databases which are very challenging and realistic for eye center localization and compared our method with existing state-of-the-art methods. The results of the experiment confirm that the proposed method outperforms the state-of-the-art methods with a significant improvement in accuracy and robustness and has less computational complexity. © 2019, The Author(s).","Eye center localization; Eye gaze estimation; Eye tracking; Human-computer interaction","Gradient methods; Human computer interaction; Regression analysis; Curvature method; Experiment confirm; Eye center localization; Eye center locations; Eye-gaze; Public database; Regression method; State-of-the-art methods; Eye tracking",Article,"Final","",Scopus,2-s2.0-85073823365
"Shen J., Li X., Zeng H., Song A.","57209180656;57215436831;7401472202;57205479536;","Single-trial Classification of Fixation-related Potentials in Guided Visual Search Tasks using A Riemannian Network",2019,"2019 IEEE Symposium Series on Computational Intelligence, SSCI 2019",,,"9002946","375","379",,1,"10.1109/SSCI44817.2019.9002946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080874580&doi=10.1109%2fSSCI44817.2019.9002946&partnerID=40&md5=e6bf1fcd72f0212bfcce88a0dc22d555","Southeast University, Jiangsu Key Lab of Remote Sense and Control, Nanjing, China","Shen, J., Southeast University, Jiangsu Key Lab of Remote Sense and Control, Nanjing, China; Li, X., Southeast University, Jiangsu Key Lab of Remote Sense and Control, Nanjing, China; Zeng, H., Southeast University, Jiangsu Key Lab of Remote Sense and Control, Nanjing, China; Song, A., Southeast University, Jiangsu Key Lab of Remote Sense and Control, Nanjing, China","Brain responses to visual stimulus can provide information about cognitive process or intentions. Several studies show that it is feasible to use stimulus-dependent modulation of the evoked brain responses after gaze movements (i.e., Fixation Related Potential, FRP) to predict the interested object of human. However, the performance of the state-of the-art shallow models for FRP classification is still far from satisfactory. Recent years, Riemannian geometry based on deep learning has gained its popularity in many image and video processing tasks, thanks to their ability to learn appropriate statistical representations while respecting Riemannian geometry of the data in such fields. In this paper, we have investigated a Riemannian network for classifying FRP in guided visual search task. Experiment results showed that the Riemannian network improved classification performance significantly in comparison to the shallow methods. © 2019 IEEE.","deep learning; fixation related potential; guided visual search; Riemannian geometry","Deep learning; Video signal processing; Classification performance; Cognitive process; Image and video processing; Riemannian geometry; Single-trial classifications; State of the art; Statistical representations; Visual search; Geometry",Conference Paper,"Final","",Scopus,2-s2.0-85080874580
"Rowntree T., Pontecorvo C., Reid I.","57207874416;56021773000;55643335100;","Real-Time Human Gaze Estimation",2019,"2019 Digital Image Computing: Techniques and Applications, DICTA 2019",,,"8945919","","",,,"10.1109/DICTA47822.2019.8945919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078707945&doi=10.1109%2fDICTA47822.2019.8945919&partnerID=40&md5=f5d28f8c0bbdad4b2cb667adaf486133","University of Adelaide, Australia; Defence Science and Technology, Australia","Rowntree, T., University of Adelaide, Australia; Pontecorvo, C., Defence Science and Technology, Australia; Reid, I., University of Adelaide, Australia","This paper describes a system for estimating the course gaze or 1D head pose of multiple people in a video stream from a moving camera in an indoor scene. The system runs at 30 Hz and can detect human heads with a F-Score of 87.2% and predict their gaze with an average error 20.9° including when they are facing directly away from the camera. The system uses two Convolutional Neural Networks (CNNs) for head detection and gaze estimation respectively and uses common tracking and filtering techniques for smoothing predictions over time. This paper is application-focused and so describes the individual components of the system as well as the techniques used for collecting data and training the CNNs. © 2019 IEEE.",,"Neural networks; Average errors; Convolutional neural network; Filtering technique; Gaze estimation; Head detection; Individual components; Moving cameras; Multiple people; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-85078707945
"Al-Naser M., Siddiqui S.A., Ohashi H., Ahmed S., Katsuyki N., Takuto S., Dengel A.","53979297700;57200439899;56004686500;57217052666;16200436800;57215305075;6603764314;","OGaze: Gaze Prediction in Egocentric Videos for Attentional Object Selection",2019,"2019 Digital Image Computing: Techniques and Applications, DICTA 2019",,,"8945893","","",,3,"10.1109/DICTA47822.2019.8945893","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078703689&doi=10.1109%2fDICTA47822.2019.8945893&partnerID=40&md5=e71552b549da28588e2f033d49c06105","German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Hitachi Ltd., Tokyo, Japan","Al-Naser, M., German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Siddiqui, S.A., German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Ohashi, H., Hitachi Ltd., Tokyo, Japan; Ahmed, S., German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Katsuyki, N., Hitachi Ltd., Tokyo, Japan; Takuto, S., Hitachi Ltd., Tokyo, Japan; Dengel, A., German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany","This paper proposes a novel gaze-estimation model for attentional object selection tasks. The key features of our model are two-fold: (i) usage of the deformable convolutional layers to better incorporate spatial dependencies of different shapes of objects and background, (ii) formulation of the gaze-estimation problem in two different ways, i.e. as a classification as well as a regression problem. We combine the two different formulations using a joint loss that incorporates both the cross-entropy as well as the mean-squared error in order to train our model. The experimental results on two publicly available datasets indicates that our model not only achieved real-time performance (13-18 FPS), but also outperformed the state-of-the-art models on the OSdataset along with comparable performance on GTEA-plus dataset. © 2019 IEEE.",,"Computer programming; Computer science; Different shapes; Gaze estimation; Mean squared error; Object selection; Real time performance; Regression problem; Spatial dependencies; State of the art; Mean square error",Conference Paper,"Final","",Scopus,2-s2.0-85078703689
"Ullah A., Wang J., Shahid Anwar M., Ahmad U., Saeed U., Fei Z.","57204010903;55904263200;57208044272;57204023672;57206662682;7005339161;","Facial expression recognition of nonlinear facial variations using deep locality de-expression residue learning in the wild",2019,"Electronics (Switzerland)","8","12","1487","","",,3,"10.3390/electronics8121487","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076624713&doi=10.3390%2felectronics8121487&partnerID=40&md5=255a1ab53f795553921684db6b31fe7d","School of Information and Electronics, Beijing Institute of Technology, Haidian District, Beijing, 100081, China; Riphah International University, Faisalabad Campus, Faisalabad, 38000, Pakistan; School of Computer Science and Technology, Beijing Institute of Technology, Haidian District, Beijing, 100081, China","Ullah, A., School of Information and Electronics, Beijing Institute of Technology, Haidian District, Beijing, 100081, China, Riphah International University, Faisalabad Campus, Faisalabad, 38000, Pakistan; Wang, J., School of Information and Electronics, Beijing Institute of Technology, Haidian District, Beijing, 100081, China; Shahid Anwar, M., School of Information and Electronics, Beijing Institute of Technology, Haidian District, Beijing, 100081, China; Ahmad, U., School of Computer Science and Technology, Beijing Institute of Technology, Haidian District, Beijing, 100081, China; Saeed, U., School of Computer Science and Technology, Beijing Institute of Technology, Haidian District, Beijing, 100081, China; Fei, Z., School of Information and Electronics, Beijing Institute of Technology, Haidian District, Beijing, 100081, China","Automatic facial expression recognition is an emerging field. Moreover, the interest has been increased with the transition from laboratory-controlled conditions to in the wild scenarios. Most of the research has been done over nonoccluded faces under the constrained environment, while automatic facial expression is less understood/implemented for partial occlusion in the real world conditions. Apart from that, our research aims to tackle the issues of overfitting (caused by the shortage of adequate training data) and to alleviate the expression-unrelated/intraclass/nonlinear facial variations, such as head pose estimation, eye gaze estimation, intensity and microexpressions. In our research, we control the magnitude of each Action Unit (AU) and combine several of the Action Unit combinations to leverage learning from the generative and discriminative representations for automatic FER. We have also addressed the problem of diversification of expressions from lab controlled to real-world scenarios from our cross-database study and proposed a model for enhancement of the discriminative power of deep features while increasing the interclass scatters, by preserving the locality closeness. Furthermore, facial expression consists of an expressive component as well as neutral component, so we proposed a generative model which is capable of generating neutral expression from an input image using cGAN. The expressive component is filtered and passed to the intermediate layers and the process is called De-expression Residue Learning. The residue in the intermediate/middle layers is very important for learning through expressive components. Finally, we validate the effectiveness of our method (DLP-DeRL) through qualitative and quantitative experimental results using four databases. Our method is more accurate and robust, and outperforms all the existing methods (hand crafted features and deep learning) while dealing the images in the wild. © 2019, MDPI AG. All rights reserved.","Conditional graphical adversarial network; De-expression residue; Deep locality preserving; Facial variations",,Article,"Final","",Scopus,2-s2.0-85076624713
"Ma Y., Wu J., Long C.","57212409081;56428178500;8718008500;","GazeFCW: Filter collisionwarning triggers by detecting driver's gaze area",2019,"SenSys-ML 2019 - Proceedings of the 1st Workshop on Machine Learning on Edge in Sensor Systems, Part of SenSys 2019",,,,"13","18",,,"10.1145/3362743.3362962","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076616469&doi=10.1145%2f3362743.3362962&partnerID=40&md5=10ff9f223a0add628b30b55596a1f1a6","Department of Automation, School of Electronic Information and Electrical Engineering, AI Institute Shanghai Jiao Tong University, Shanghai, China","Ma, Y., Department of Automation, School of Electronic Information and Electrical Engineering, AI Institute Shanghai Jiao Tong University, Shanghai, China; Wu, J., Department of Automation, School of Electronic Information and Electrical Engineering, AI Institute Shanghai Jiao Tong University, Shanghai, China; Long, C., Department of Automation, School of Electronic Information and Electrical Engineering, AI Institute Shanghai Jiao Tong University, Shanghai, China","Collision warning is essential in Advanced Driver Assistance Systems. However, all the studies focus on accurate assessment of risky situations without considering the driver's attention mechanism, which causes the proportion of valid warning triggers to be extremely low. In this paper, we present GazeFCW - a novel system that uses the driver's gaze direction to filter out unnecessary warning triggers. We verified the proposed system against several roads of different conditions. Our evaluation, across different crowded roads, shows a significant enhancement in warning trigger efficiency- compared to the standard system-reflected by an increase in the valid trigger proportion by 42%, a decrease in the invalid trigger proportion from 74.7% down to 32.7%, while adding only 50 ms run time execution overhead and causing negligible missing triggers. © 2019 Association for Computing Machinery.","Collision Warning System; Deep Learning; Gaze Area","Automobile drivers; Deep learning; Embedded systems; Machine learning; Attention mechanisms; Collision warning; Collision warning system; Gaze areas; Gaze direction; Run-time execution; Standard system; Trigger efficiencies; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85076616469
"Valenzuela A., Arellano C., Tapia J.","57216819899;57207768339;7005419930;","An efficient dense network for semantic segmentation of eyes images captured with virtual reality lens",2019,"Proceedings - 15th International Conference on Signal Image Technology and Internet Based Systems, SISITS 2019",,,"9067945","28","34",,1,"10.1109/SITIS.2019.00017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084850623&doi=10.1109%2fSITIS.2019.00017&partnerID=40&md5=1015867287d9c9869cfa456c4142196b","Universidad Andres Bello, Department of Engineering Sciences, Chile; Universidad Tecnologica de Chile, INACAP, Santiago, Chile","Valenzuela, A., Universidad Andres Bello, Department of Engineering Sciences, Chile; Arellano, C.; Tapia, J., Universidad Andres Bello, Department of Engineering Sciences, Chile, Universidad Tecnologica de Chile, INACAP, Santiago, Chile","Eye-tracking and Gaze estimation are difficult tasks that may be used for several applications including human-computer interfaces, salience detection and Virtual reality amongst others. This paper presents a segmentation algorithm based on deep learning that efficiently discriminates pupils, iris, and sclera from the background in images captured using a Virtual Reality lens. A light network called DensetNet10 trained from scratch is proposed. It contains fewer parameters than traditional architectures based on DenseNet which allows it to be used in mobile device applications. Experiments show that this network achieved higher IOU rates when comparing with DensetNet56-67-103 and DeeplabV3+ models in the Facebook database. Furthermore, this method reached 8th place in The Facebook semantic segmentation challenge with 0.94293 mean IOU and 202.084 parameters with a final score of 0.97147. © 2019 IEEE.","Biometrics; Facebook Challengue; Semantic Segmentation","Deep learning; Eye tracking; Semantic Web; Semantics; Social networking (online); Virtual reality; Dense network; Facebook; Gaze estimation; Human computer interfaces; Mobile device applications; Segmentation algorithms; Semantic segmentation; Traditional architecture; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85084850623
"Safarzadeh M., Ghasemi M., Khoramdel J., Ardekany A.N.","57216710341;57216706124;57216709284;57206192838;","A Secure Face Anti-spoofing Approach Using Deep Learning",2019,"ICRoM 2019 - 7th International Conference on Robotics and Mechatronics",,,"9071842","322","327",,,"10.1109/ICRoM48714.2019.9071842","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084356902&doi=10.1109%2fICRoM48714.2019.9071842&partnerID=40&md5=f9b16a0c7b3e983f81e1284e69c618d3","Mechatronics Lab., K. N. Toosi University of Technology, Tehran, Iran","Safarzadeh, M., Mechatronics Lab., K. N. Toosi University of Technology, Tehran, Iran; Ghasemi, M., Mechatronics Lab., K. N. Toosi University of Technology, Tehran, Iran; Khoramdel, J., Mechatronics Lab., K. N. Toosi University of Technology, Tehran, Iran; Ardekany, A.N., Mechatronics Lab., K. N. Toosi University of Technology, Tehran, Iran","Face recognition is an attractive field for researchers since the past years and it is going to be the most practical method in identity recognition in the future. But even modern face detection systems have the problem of spoofing and it is safe to say it is the most serious obstacle in the way of becoming practical in more secure fields of identity recognition. In this paper, firstly, we discuss object detection systems in particular face detection and their problems such as using the person's picture in mobile phones or any other screen-based devices instead of a real person's face and also using masks to make faces like someone's face or other spoofing goals. Then, we introduce our method to prevent these attacks to have a high-secure and reliable face recognition system. Finally, some illustrations will be provided to demonstrate the practical and economic benefits of the presented approach. © 2019 IEEE.","Anti-spoofing; Face classification; Face detection; gaze; Gaze response; Thermal; Thermal photography","Agricultural robots; Deep learning; Object detection; Robotics; Anti-spoofing; Economic benefits; Face detection system; Face recognition systems; Identity recognition; Object detection systems; Practical method; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85084356902
"Feng J., Li S., Sui Y., Meng L., Zhu C.","57204664038;57201588067;57201984250;57204672723;7403439404;","Integrating action-aware features for saliency prediction via weakly supervised learning",2019,"2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2019",,,"9023127","974","979",,,"10.1109/APSIPAASC47483.2019.9023127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082383788&doi=10.1109%2fAPSIPAASC47483.2019.9023127&partnerID=40&md5=24c05acbb4a568b1f58c9379d6e55d10","University of Electronic Science and Technology of China, Chengdu, China; Research Center of Second Research Insititute of Caac, Chengdu, China","Feng, J., University of Electronic Science and Technology of China, Chengdu, China, Research Center of Second Research Insititute of Caac, Chengdu, China; Li, S., University of Electronic Science and Technology of China, Chengdu, China; Sui, Y., Research Center of Second Research Insititute of Caac, Chengdu, China; Meng, L., University of Electronic Science and Technology of China, Chengdu, China; Zhu, C., University of Electronic Science and Technology of China, Chengdu, China","Deep learning has been widely studied for saliency prediction. Despite the great performance improvement introduced by deep saliency models, some high-level concepts that contribute to the saliency prediction, such as text, objects of gaze and action, locations of motion, and expected locations of people, have not been explicitly considered. This paper investigates the objects of action and motion, and proposes to use action-aware features to compensate deep saliency models. The action-aware features are generated via weakly supervised learning using an extra action classification network trained with existing image based action datasets. Then a feature fusion module is developed to integrate the action-aware features for saliency prediction. Experiments show that the proposed saliency model with the action-aware features achieves better performance on three public benchmark datasets. More experiments are further conducted to analyze the effectiveness of the action-aware features in saliency prediction. To the best of our knowledge, this study is the first attempt on explicitly integrating objects of action and motion concept into deep saliency models. © 2019 IEEE.",,"Benchmarking; Classification (of information); Forecasting; Supervised learning; Action classifications; Benchmark datasets; Feature fusion; Image-based; Saliency modeling; Weakly supervised learning; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85082383788
"Su D., Li Y.F., Chen H.","57193014913;8589964900;57191584510;","Region-wise Polynomial Regression for 3D Mobile Gaze Estimation",2019,"IEEE International Conference on Intelligent Robots and Systems",,,"8968536","907","913",,,"10.1109/IROS40897.2019.8968536","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081168064&doi=10.1109%2fIROS40897.2019.8968536&partnerID=40&md5=93eb16055cb1084fddd1515e4a43e745","City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong","Su, D., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Li, Y.F., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong; Chen, H., City University of Hong Kong, Department of Mechanical Engineering, Kowloon, Hong Kong","In the context of mobile gaze tracking techniques, a 3D gaze point can be calculated as the middle point between two 3D visual axes. To infer gaze directions and eyeball positions, a nonlinear optimization problem is typically formulated to minimize the angular disparities between the training gaze directions and prediction ones. Nonetheless, the experimental results reported by some previous works show that this kind of approaches are very likely to yield large prediction errors hence considered less useful for human-machine interactions. In this study, we aim to address this widespread issue in three aspects. At first, instead of using a global regression model, a simple local polynomial model is proposed to back-project a pupil center onto its corresponding visual axis. Based on the Leave-One-Out cross-validation criterion, the partition structure is automatically learned in the process of resolving a homography-like relationship. Secondly, a good starting point for nonlinear-optimization is obtained by the image eyeball center, which can be estimated by systematic parallax errors. Meanwhile, it is necessary to add the suitable constraints for 3D eye positions. Otherwise, the optimization may end up with trivial solutions, i.e., faraway eye positions. Thirdly, we explore a strategy for designing the spatial distribution of calibration points in a principled manner. The experiment results demonstrate that an encouraging gaze estimation accuracy can be achieved by our proposed framework for both the normal vision and eyewear users. © 2019 IEEE.",,"Geometrical optics; Intelligent robots; Nonlinear programming; Polynomial regression; Systematic errors; Calibration points; Human machine interaction; Leave-one-out cross validations; Local polynomials; Non-linear optimization; Non-linear optimization problems; Partition structures; Trivial solutions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85081168064
"Chen Y., Liu C., Tai L., Liu M., Shi B.E.","57192879444;57194613111;57192430548;55512872300;7402547071;","Gaze Training by Modulated Dropout Improves Imitation Learning",2019,"IEEE International Conference on Intelligent Robots and Systems",,,"8967843","7756","7761",,3,"10.1109/IROS40897.2019.8967843","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080891370&doi=10.1109%2fIROS40897.2019.8967843&partnerID=40&md5=f3ed942036657eb0fb713d298c962f3a","University of Science and Technology, Hong Kong","Chen, Y., University of Science and Technology, Hong Kong; Liu, C., University of Science and Technology, Hong Kong; Tai, L., University of Science and Technology, Hong Kong; Liu, M., University of Science and Technology, Hong Kong; Shi, B.E., University of Science and Technology, Hong Kong","Imitation learning by behavioral cloning is a prevalent method that has achieved some success in vision-based autonomous driving. The basic idea behind behavioral cloning is to have the neural network learn from observing a human expert's behavior. Typically, a convolutional neural network learns to predict the steering commands from raw driver-view images by mimicking the behaviors of human drivers. However, there are other cues, such as gaze behavior, available from human drivers that have yet to be exploited. Previous researches have shown that novice human learners can benefit from observing experts' gaze patterns. We present here that deep neural networks can also profit from this. We propose a method, gaze-modulated dropout, for integrating this gaze information into a deep driving network implicitly rather than as an additional input. Our experimental results demonstrate that gaze-modulated dropout enhances the generalization capability of the network to unseen scenes. Prediction error in steering commands is reduced by 23.5% compared to uniform dropout. Running closed loop in the simulator, the gaze-modulated dropout net increased the average distance travelled between infractions by 58.5%. Consistent with these results, the gazemodulated dropout net shows lower model uncertainty. © 2019 IEEE.",,"Automobile steering equipment; Clone cells; Cloning; Convolutional neural networks; Deep learning; Deep neural networks; Intelligent robots; Uncertainty analysis; Autonomous driving; Average Distance; Behavioral cloning; Driving network; Generalization capability; Imitation learning; Model uncertainties; Prediction errors; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85080891370
"Kaplanyan A.S., Sochenov A., Leimkühler T., Okunev M., Goodall T., Rufo G.","35867824000;57210730049;57079088200;57210733387;55513291000;57210727764;","DeepFovea: Neural reconstruction for foveated rendering and video compression using learned statistics of natural videos",2019,"ACM Transactions on Graphics","38","6","3356557","","",,22,"10.1145/3355089.3356557","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078906560&doi=10.1145%2f3355089.3356557&partnerID=40&md5=a43d8605b12b9afb8940d18d0dd02edb","Facebook Reality Labs","Kaplanyan, A.S., Facebook Reality Labs; Sochenov, A., Facebook Reality Labs; Leimkühler, T., Facebook Reality Labs; Okunev, M., Facebook Reality Labs; Goodall, T., Facebook Reality Labs; Rufo, G., Facebook Reality Labs","In order to provide an immersive visual experience, modern displays require head mounting, high image resolution, low latency, as well as high refresh rate. This poses a challenging computational problem. On the other hand, the human visual system can consume only a tiny fraction of this video stream due to the drastic acuity loss in the peripheral vision. Foveated rendering and compression can save computations by reducing the image quality in the peripheral vision. However, this can cause noticeable artifacts in the periphery, or, if done conservatively, would provide only modest savings. In this work, we explore a novel foveated reconstruction method that employs the recent advances in generative adversarial neural networks. We reconstruct a plausible peripheral video from a small fraction of pixels provided every frame. The reconstruction is done by finding the closest matching video to this sparse input stream of pixels on the learned manifold of natural videos. Our method is more efficient than the state-of-the-art foveated rendering, while providing the visual experience with no noticeable quality degradation. We conducted a user study to validate our reconstruction method and compare it against existing foveated rendering and video compression techniques. Our method is fast enough to drive gaze-contingent head-mounted displays in real time on modern hardware. We plan to publish the trained network to establish a new quality bar for foveated rendering and compression as well as encourage follow-up research. © 2019 Copyright held by the owner/author(s).","Deep learning; Foveated rendering; Gaze-contingent rendering; Generative networks; Perceptual rendering; Video compression; Video generation; Virtual reality","Deep learning; Helmet mounted displays; Image compression; Image reconstruction; Image resolution; Pixels; Virtual reality; Vision; Compression techniques; Computational problem; Foveated rendering; Gaze-contingent; Head mounted displays; Perceptual rendering; Reconstruction method; Video generation; Rendering (computer graphics)",Article,"Final","",Scopus,2-s2.0-85078906560
"Ishibashi K.","36147517800;","Application of deep learning to pre-processing of cousumer's eye tracking data in supermarket",2019,"IEEE International Conference on Data Mining Workshops, ICDMW","2019-November",,"8955495","341","348",,,"10.1109/ICDMW.2019.00057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078795961&doi=10.1109%2fICDMW.2019.00057&partnerID=40&md5=a0c0e0771f5b0143fd56e26459366b82","Research Institute for Socionetwork Strategies, Kansai University, Osaka, Japan","Ishibashi, K., Research Institute for Socionetwork Strategies, Kansai University, Osaka, Japan","The purpose of this study is to automate pre-processing of eye tracking data. In an investigation with eye tracking in a field such as supermarket, pre-processing of data has enormous cost. This is because it is very difficult to map consumer's gaze points to certain snapshot due to her/his movement. This study uses deep learning for automating pre-processing of eye tracking data. General object recognition using deep learning can classify an image into various classes. The proposed method attempts to classify fixated object of consumer into three classes, product, promotion and etc., based on the result of general object recognition. This paper discusses the applicability of proposed method through cross validation using eye tracking data preprocessed manually. © 2019 IEEE.","Consumer behavior; Depp learning; Eye tracking; General object recognition; Pre-processing","Consumer behavior; Data handling; Data mining; Deep learning; Object recognition; Retail stores; Cross validation; Depp learning; Gaze point; Pre-processing; Pre-processing of data; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85078795961
"Anh B.N., Son N.T., Lam P.T., Chi L.P., Tuan N.H., Dat N.C., Trung N.H., Aftab M.U., Dinh T.V.","57200646112;57200657978;57208879110;57208883857;57211885622;57211888115;57209461633;56526625000;57208878567;","A Computer-vision based application for student behavior monitoring in classroom",2019,"Applied Sciences (Switzerland)","9","22","4729","","",,9,"10.3390/app9224729","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075236069&doi=10.3390%2fapp9224729&partnerID=40&md5=2889c8508bb3467cecd38572adf84e35","ICT Department, FPT University, Hanoi, 10000, Viet Nam; School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, 610054, China; Department of Computer Science, University of Freiburg, Freiburg, 79098, Germany","Anh, B.N., ICT Department, FPT University, Hanoi, 10000, Viet Nam; Son, N.T., ICT Department, FPT University, Hanoi, 10000, Viet Nam; Lam, P.T., ICT Department, FPT University, Hanoi, 10000, Viet Nam; Chi, L.P., ICT Department, FPT University, Hanoi, 10000, Viet Nam; Tuan, N.H., ICT Department, FPT University, Hanoi, 10000, Viet Nam; Dat, N.C., ICT Department, FPT University, Hanoi, 10000, Viet Nam; Trung, N.H., ICT Department, FPT University, Hanoi, 10000, Viet Nam; Aftab, M.U., School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, 610054, China; Dinh, T.V., Department of Computer Science, University of Freiburg, Freiburg, 79098, Germany","Automated learning analytics is becoming an essential topic in the educational area, which needs effective systems to monitor the learning process and provides feedback to the teacher. Recent advances in visual sensors and computer vision methods enable automated monitoring of behavior and affective states of learners at different levels, from university to pre-school. The objective of this research was to build an automatic system that allowed the faculties to capture and make a summary of student behaviors in the classroom as a part of data acquisition for the decision making process. The system records the entire session and identifies when the students pay attention in the classroom, and then reports to the facilities. Our design and experiments show that our system is more flexible and more accurate than previously published work. © 2019 by the authors.","Classification; Face detection; Facial recognition; Gaze estimation; Student's behavior; Visual attention",,Article,"Final","",Scopus,2-s2.0-85075236069
"Huang T., Fu Y., Wang Y., Wang Z.","57214224982;57214232484;57214236550;57209833257;","Binocular gaze estimation with single camera and single light source",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"10","14",,,"10.1145/3369318.3369326","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078400979&doi=10.1145%2f3369318.3369326&partnerID=40&md5=e56f3cea96d40b38de186494ea373d2e","Invensun Technology Co.Ltd., Beijing, China; TnB Galaxy, Beijing, China; University of Science and Technology Beijing, Beijing, China","Huang, T., Invensun Technology Co.Ltd., Beijing, China; Fu, Y., TnB Galaxy, Beijing, China; Wang, Y., University of Science and Technology Beijing, Beijing, China; Wang, Z., Invensun Technology Co.Ltd., Beijing, China","According to commonly consented theories, the minimum hardware requirement for gaze tracker is one camera and two light sources to realize gaze estimation with free head movements [1]. However, in some scenarios such as eye tracking on mobile devices, it is preferable to use less components, especially light sources. We propose a gaze estimation method with one camera and one light source. A “virtual light source” is introduced, which is geometrically placed symmetrically to the real light source with respect to the camera, and generates a “virtual glint” in the acquired image. We estimate the “virtual glint” by exploiting the relationship between the distance between two pupils and two glints in the captured image, and estimate the gaze with polynomial regression assuming two light sources are available. A new normalization factor for regression method is verified, which turns out to be practical for one-glint system. The performance is proved to be acceptable, while degradation is noticed compared to system with two actual light sources. © 2019 Association for Computing Machinery.","Binocular; Gaze estimation; Polynomial regression; Video oculography","Binoculars; Cameras; Eye tracking; Image processing; Light sources; Video signal processing; Binocular gazes; Gaze estimation; Gaze tracker; Normalization factors; One camera; Regression method; Single cameras; Video oculography; Polynomial regression",Conference Paper,"Final","",Scopus,2-s2.0-85078400979
"Chen Y., Tao G., Xie Q., Song M.","35408499700;57194090952;57215300148;57216495309;","Video attention prediction using gaze saliency",2019,"Multimedia Tools and Applications","78","19",,"26867","26884",,3,"10.1007/s11042-016-4294-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008158163&doi=10.1007%2fs11042-016-4294-1&partnerID=40&md5=5bd8770a872c6cd84767c7df583a4af3","School of Computer and Information, Hefei University of Technology, Hefei, 230009, China; Anhui Keli Information Industry Co. Ltd., Hefei, 230088, China","Chen, Y., School of Computer and Information, Hefei University of Technology, Hefei, 230009, China; Tao, G., Anhui Keli Information Industry Co. Ltd., Hefei, 230088, China; Xie, Q., School of Computer and Information, Hefei University of Technology, Hefei, 230009, China; Song, M., School of Computer and Information, Hefei University of Technology, Hefei, 230009, China","In recent years, the significant progress has been achieved in the field of visual saliency modeling. Our research key is in video saliency, which differs substantially from image saliency and could be better detected by adding the gaze information from the movement of eyes while people are looking at the video. In this paper we purposed a novel gaze saliency method to predict video attention, which is inspired by the widespread usage of mobile smart devices with camera. It is a non-contacted method to predict visual attention, and it does not bring the burden on the hardware. Our method first extracts the bottom-up saliency maps from the video frames, and then constructs the mapping from eye images obtained by the camera in synchronization with the video frames to the screen region. Finally the combination between top-down gaze information and bottom-up saliency maps is conducted by point-wise multiplication to predict the video attention. Furthermore, the proposed approach is validated on the two datasets: one is the public dataset MIT, the other is the dataset we collected, versus other four usual methods, and the experiment results show that our method achieves the state-of-the-art. © 2016, Springer Science+Business Media New York.","Facial landmark detection; Gaze estimation; Gaze saliency","Behavioral research; Cameras; Eye movements; Object recognition; Bottom-up saliencies; Facial landmark detection; Gaze estimation; Gaze saliency; Image saliencies; Mobile smart devices; Video saliencies; Visual saliency model; Forecasting",Article,"Final","",Scopus,2-s2.0-85008158163
"Sasaki M., Nagamatsu T., Takemura K.","57205675368;23398000100;8575290600;","Cross-ratio based gaze estimation for multiple displays using a polarization camera",2019,"UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology",,,,"1","3",,,"10.1145/3332167.3357095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074833818&doi=10.1145%2f3332167.3357095&partnerID=40&md5=f1a115f9ce75e6179e62d0c00b9b3b68","Tokai University, Hiratsuka, Kanagawa, Japan; Kobe University, Kobe, Hyogo, Japan","Sasaki, M., Tokai University, Hiratsuka, Kanagawa, Japan; Nagamatsu, T., Kobe University, Kobe, Hyogo, Japan; Takemura, K., Tokai University, Hiratsuka, Kanagawa, Japan","While eye tracking has been typically used for achieving intuitive user interfaces, it is not sufficient when it comes to dealing with multiple-display environments. In such environments, which have become popular recently, the point-of-gaze should be estimated on multiple screens. Therefore, we propose a cross-ratio based gaze estimation using a polarization camera for multiple screens. The point-of-gaze can be estimated on each monitor by identifying the screen reflected on the corneal surface at a polarization angle. Near-infrared light emitting diodes (NIR-LEDs) attached to the display are not required. This means that standard displays can be used with high general versatility as an advantage. © 2019 Copyright is held by the owner/author(s).","Gaze estimation; Multiple displays; Polarization","Cameras; Eye tracking; Infrared devices; Polarization; Cross-ratios; Gaze estimation; Intuitive user interface; Multiple displays; Near infrared light; Point of gaze; Polarization angle; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85074833818
"Liu Z., Chen Z., Bai J., Li S., Lian S.","57211257370;57215289589;55889538000;57215969499;7005702391;","Facial pose estimation by deep learning from label distributions",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022536","1232","1240",,7,"10.1109/ICCVW.2019.00156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082506421&doi=10.1109%2fICCVW.2019.00156&partnerID=40&md5=87f2ffe6013cc1f661fa5d2d9f104583","Cloudminds, China; Beihang University, China","Liu, Z., Cloudminds, China; Chen, Z., Cloudminds, China; Bai, J., Beihang University, China; Li, S., Cloudminds, China; Lian, S., Cloudminds, China","Facial pose estimation has gained a lot of attentions in many practical applications, such as human-robot interaction, gaze estimation and driver monitoring. Meanwhile, end-to-end deep learning-based facial pose estimation is becoming more and more popular. However, facial pose estimation suffers from a key challenge: the lack of sufficient training data for many poses, especially for large poses. Inspired by the observation that the faces under close poses look similar, we reformulate the facial pose estimation as a label distribution learning problem, considering each face image as an example associated with a Gaussian label distribution rather than a single label, and construct a convolutional neural network which is trained with a multi-loss function on AFLW dataset and 300W-LP dataset to predict the facial poses directly from color image. Extensive experiments are conducted on several popular benchmarks, including AFLW2000, BIWI, AFLW and AFW, where our approach shows a significant advantage over other state-of-the-art methods. © 2019 IEEE.","Deep learning; Facial pose estimation; Label distribution","Computer vision; Convolutional neural networks; Human robot interaction; Driver monitoring; Facial pose estimation; Gaze estimation; Label distribution; Learning problem; Loss functions; State-of-the-art methods; Training data; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85082506421
"He J., Pham K., Valliappan N., Xu P., Roberts C., Lagun D., Navalpakkam V.","55714925900;57208582090;35333214700;55267609700;57215969114;36727735000;6507746235;","On-device few-shot personalization for real-time gaze estimation",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9021975","1149","1158",,11,"10.1109/ICCVW.2019.00146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082505641&doi=10.1109%2fICCVW.2019.00146&partnerID=40&md5=7ac0f3f79edebf52e461962f1d0b4efb","Google Inc., United States; University of Maryland, College Park, United States","He, J., Google Inc., United States; Pham, K., University of Maryland, College Park, United States; Valliappan, N., Google Inc., United States; Xu, P., Google Inc., United States; Roberts, C., Google Inc., United States; Lagun, D., Google Inc., United States; Navalpakkam, V., Google Inc., United States","Building fast and accurate gaze estimation models without additional specialized hardware is a hard problem. In this paper, we present on-device few-shot personalization methods for 2D gaze estimation. The proposed supervised method achieves better accuracy using as few as 2-5 calibration points per user compared to prior methods that require more than 13 calibration points. In addition, we propose an unsupervised personalization method which uses only unlabeled facial images to improve gaze estimation accuracy. Our best personalized model achieves 24-26% better accuracy (measured by mean error) on phones compared to the state-of-the-art using <=5 calibration points per user. It is also computationally efficient, requiring 20x fewer FLOPS when compared to prior methods. This unlocks a variety of important real world applications such as using gaze for accessibility, gaming and human-computer interaction while running entirely on-device in real-time. © 2019 IEEE.","Few shot learning; Gaze; Haze tracking; On device learning; Personalization","Calibration; Computer vision; Image enhancement; Computationally efficient; Few shot learning; Gaze; On device learning; Personalizations; Personalized model; Specialized hardware; Supervised methods; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85082505641
"Kapidis G., Poppe R., Van Dam E., Noldus L., Veltkamp R.","57204631005;19934245500;55809553500;6603151066;7003421646;","Multitask learning to improve egocentric action recognition",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022063","4396","4405",,7,"10.1109/ICCVW.2019.00540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082503469&doi=10.1109%2fICCVW.2019.00540&partnerID=40&md5=41d1f71aa206da388bb59d77f66e6530","Noldus Information Technology, Utrecht University, Netherlands; Utrecht University, Netherlands","Kapidis, G., Noldus Information Technology, Utrecht University, Netherlands, Utrecht University, Netherlands; Poppe, R., Utrecht University, Netherlands; Van Dam, E., Noldus Information Technology, Utrecht University, Netherlands; Noldus, L., Noldus Information Technology, Utrecht University, Netherlands; Veltkamp, R., Utrecht University, Netherlands","In this work we employ multitask learning to capitalize on the structure that exists in related supervised tasks to train complex neural networks. It allows training a network for multiple objectives in parallel, in order to improve performance on at least one of them by capitalizing on a shared representation that is developed to accommodate more information than it otherwise would for a single task. We employ this idea to tackle action recognition in egocentric videos by introducing additional supervised tasks. We consider learning the verbs and nouns from which action labels consist of and predict coordinates that capture the hand locations and the gaze-based visual saliency for all the frames of the input video segments. This forces the network to explicitly focus on cues from secondary tasks that it might otherwise have missed resulting in improved inference. Our experiments on EPIC-Kitchens and EGTEA Gaze+ show consistent improvements when training with multiple tasks over the single-task baseline. Furthermore, in EGTEA Gaze+ we outperform the state-of-the-art in action recognition by 3.84%. Apart from actions, our method produces accurate hand and gaze estimations as side tasks, without requiring any additional input at test time other than the RGB video clips. © 2019 IEEE.","Action recognition; Egocentric vision; Hand detection; Haze estimation; Multitask learning","Learning systems; Multi-task learning; Neural networks; Action recognition; Complex neural networks; Hand detection; Improve performance; Multiple-objectives; Shared representations; State of the art; Visual saliency; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85082503469
"Cortacero K., Fischer T., Demiris Y.","57215970582;57190126084;6506125343;","RT-BENE: A dataset and baselines for real-time blink estimation in natural environments",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022030","1159","1168",,3,"10.1109/ICCVW.2019.00147","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082494646&doi=10.1109%2fICCVW.2019.00147&partnerID=40&md5=03983e4d1a87d85393a51146d3d3dc02","Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom","Cortacero, K., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom; Fischer, T., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom; Demiris, Y., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, United Kingdom","In recent years gaze estimation methods have made substantial progress, driven by the numerous application areas including human-robot interaction, visual attention estimation and foveated rendering for virtual reality headsets. However, many gaze estimation methods typically assume that the subject's eyes are open; for closed eyes, these methods provide irregular gaze estimates. Here, we address this assumption by first introducing a new open-sourced dataset with annotations of the eye-openness of more than 200,000 eye images, including more than 10,000 images where the eyes are closed. We further present baseline methods that allow for blink detection using convolutional neural networks. In extensive experiments, we show that the proposed baselines perform favourably in terms of precision and recall. We further incorporate our proposed RT-BENE baselines in the recently presented RT-GENE gaze estimation framework where it provides a real-time inference of the openness of the eyes. We argue that our work will benefit both gaze estimation and blink estimation methods, and we take steps towards unifying these methods. © 2019 IEEE.","Blink detection; Convolutional neural network; Deep learning; Haze estimation","Behavioral research; Computer vision; Convolution; Deep learning; Deep neural networks; Human robot interaction; Virtual reality; Application area; Blink detections; Estimation methods; Natural environments; Precision and recall; Real-time inference; Virtual-reality headsets; Visual Attention Estimation; Convolutional neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85082494646
"Porta S., Bossavit B., Cabeza R., Larumbe-Bergera A., Garde G., Villanueva A.","7005292345;36730794700;36763933900;57210106737;57215963483;7101612861;","U2Eyes: A binocular dataset for eye tracking and gaze estimation",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022577","3660","3664",,8,"10.1109/ICCVW.2019.00451","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082489044&doi=10.1109%2fICCVW.2019.00451&partnerID=40&md5=89905765d2816f5d7a324e6b4d06579f","Public University of Navarre, Pamplona, Spain; Trinity College Dublin, Dublin, Ireland","Porta, S., Public University of Navarre, Pamplona, Spain; Bossavit, B., Trinity College Dublin, Dublin, Ireland; Cabeza, R., Public University of Navarre, Pamplona, Spain; Larumbe-Bergera, A., Public University of Navarre, Pamplona, Spain; Garde, G., Public University of Navarre, Pamplona, Spain; Villanueva, A., Public University of Navarre, Pamplona, Spain","Theory shows that huge amount of labelled data are needed in order to achieve reliable classification/regression methods when using deep/machine learning techniques. However, in the eye tracking field, manual annotation is not a feasible option due to the wide variability to be covered. Hence, techniques devoted to synthesizing images show up as an opportunity to provide vast amounts of annotated data. Considering that the well-known UnityEyes tool provides a framework to generate single eye images and taking into account that both eyes information can contribute to improve gaze estimation accuracy we present U2Eyes dataset, that is publicly available. It comprehends about 6 million of synthetic images containing binocular data. Furthermore, the physiology of the eye model employed is improved, simplified dynamics of binocular vision are incorporated and more detailed 2D and 3D labelled data are provided. Additionally, an example of application of the dataset is shown as work in progress. Employing U2Eyes as training framework Supervised Descent Method (SDM) is used for eyelids segmentation. The model obtained as result of the training process is then applied on real images from GI4E dataset showing promising results. © 2019 IEEE.","Annotation; Binocular dataset; Eye tracking; Low resolution; Unity","Binocular vision; Binoculars; Computer vision; Deep learning; Image enhancement; Learning systems; Stereo image processing; Annotation; Learning techniques; Low resolution; Manual annotation; Synthetic images; Training framework; Unity; Work in progress; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082489044
"Wu Z., Rajendran S., Van As T., Badrinarayanan V., Rabinovich A.","57141032200;57215971788;57215965908;24821754400;8764903400;","EyeNet: A multi-task deep network for off-axis eye gaze estimation",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022317","3683","3687",,2,"10.1109/ICCVW.2019.00455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082483189&doi=10.1109%2fICCVW.2019.00455&partnerID=40&md5=3f1c5dce3b5417a88d0ac97d0ecb83c2","Magic Leap, Inc., United States","Wu, Z., Magic Leap, Inc., United States; Rajendran, S., Magic Leap, Inc., United States; Van As, T., Magic Leap, Inc., United States; Badrinarayanan, V., Magic Leap, Inc., United States; Rabinovich, A., Magic Leap, Inc., United States","Eye gaze estimation is a crucial component in Virtual and Mixed Reality. In head-mounted VR/MR devices the eyes are imaged off-axis to avoid blocking the user's gaze, this view-point makes drawing eye related inferences very challenging. In this work, we present EyeNet, the first single deep neural network which solves multiple heterogeneous tasks related to eye gaze estimation for an off-axis camera setting. The tasks include eye segmentation, IR LED glints detection, pupil and cornea center estimation. We benchmark all tasks on MagicEyes, a large and new dataset of 587 subjects with varying morphology, gender, skin-color, make-up and imaging conditions. © 2019 IEEE.","Computer vision; Deep learning; Eye tracking; Gaze estimation; Mixed reality; Multi task learning","Computer vision; Deep learning; Deep neural networks; Large dataset; Mixed reality; Multi-task learning; Center estimations; Eye-gaze; Gaze estimation; Imaging conditions; Off-axis; Off-axis cameras; Skin color; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082483189
"Linden E., Sjostrand J., Proutiere A.","57215962232;57217753974;6603905943;","Learning to personalize in appearance-based gaze tracking",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022231","1140","1148",,7,"10.1109/ICCVW.2019.00145","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082478391&doi=10.1109%2fICCVW.2019.00145&partnerID=40&md5=6b80ab60ad317303056e3080520e4107","Tobii, Sweden; KTH Royal Insitute of Technology, Sweden","Linden, E., Tobii, Sweden; Sjostrand, J., Tobii, Sweden; Proutiere, A., KTH Royal Insitute of Technology, Sweden","Personal variations severely limit the performance of appearance-based gaze tracking. Adapting to these variations using standard neural network model-adaption methods is difficult. The problems range from overfitting, due to small amounts of training data, to underfitting, due to restrictive model architectures. We tackle these problems by introducing SPatial Adaptive GaZe Estimator (SPAZE ). By modeling personal variations as a low-dimensional latent parameter space, SPAZE provides just enough adaptability to capture the range of personal variations without being prone to overfitting. Calibrating SPAZE for a new person reduces to solving a small and simple optimization problem. SPAZE achieves an error of 2.70 degrees on the MPIIGaze dataset, improving on the state-of-the-art by 14 %. We contribute to gaze tracking research by empirically showing that personal variations are well-modeled as a 3-dimensional latent parameter space for each eye. We show that this low-dimensionality is expected by examining model-based approaches to gaze tracking. © 2019 IEEE.","Appearance based gaze estimation; Convolutional neural network; Deep learning; Haze tracking","Computer vision; Convolutional neural networks; Deep learning; Deep neural networks; Gaze estimation; Low dimensionality; Model architecture; Model based approach; Optimization problems; Parameter spaces; Standard neural network models; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082478391
"Chang Z., DI Martino J.M., Qiu Q., Espinosa S., Sapiro G.","57190190445;36664485600;54956074400;14017589600;7005450011;","Salgaze: Personalizing gaze estimation using visual saliency",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022412","1169","1178",,2,"10.1109/ICCVW.2019.00148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082468271&doi=10.1109%2fICCVW.2019.00148&partnerID=40&md5=5c447c56aabf57874f7c502c1863e8f5","Duke University, Durham, NC  27708, United States","Chang, Z., Duke University, Durham, NC  27708, United States; DI Martino, J.M., Duke University, Durham, NC  27708, United States; Qiu, Q., Duke University, Durham, NC  27708, United States; Espinosa, S., Duke University, Durham, NC  27708, United States; Sapiro, G., Duke University, Durham, NC  27708, United States","Traditional gaze estimation methods typically require explicit user calibration to achieve high accuracy. This process is cumbersome and recalibration is often required when there are changes in factors such as illumination and pose. To address this challenge, we introduce SalGaze, a framework that utilizes saliency information in the visual content to transparently adapt the gaze estimation algorithm to the user without explicit user calibration. We design an algorithm to transform a saliency map into a differentiable loss map that can be used for the optimization of CNN-based models. SalGaze is also able to greatly augment standard point calibration data with implicit video saliency calibration data using a unified framework. We show accuracy improvements over 24% using our technique on existing methods. © 2019 IEEE.","Calibration; Convolutional neural network; Deep learning; Gaze estimation; Saliency","Computer vision; Convolutional neural networks; Deep learning; Deep neural networks; Object recognition; Accuracy Improvement; Calibration data; Gaze estimation; Saliency; Unified framework; User calibration; Video saliencies; Visual saliency; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-85082468271
"Guo T., Liu Y., Zhang H., Liu X., Kwak Y., Yoo B.I., Han J.-J., Choi C.","55624159300;57215967428;57216240708;57215965302;57201425904;36096230000;55646340200;7402961607;","A generalized and robust method towards practical gaze estimation on smart phone",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022325","1131","1139",,7,"10.1109/ICCVW.2019.00144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082466578&doi=10.1109%2fICCVW.2019.00144&partnerID=40&md5=8c95e9c953c23d0b895bbc0752bcc035","Samsung Research China - Beijing, China; Samsung Advanced Institute of Technology, China","Guo, T., Samsung Research China - Beijing, China; Liu, Y., Samsung Research China - Beijing, China; Zhang, H., Samsung Research China - Beijing, China; Liu, X., Samsung Research China - Beijing, China; Kwak, Y., Samsung Research China - Beijing, China; Yoo, B.I., Samsung Advanced Institute of Technology, China; Han, J.-J., Samsung Advanced Institute of Technology, China; Choi, C., Samsung Research China - Beijing, China","Gaze estimation for ordinary smart phone, e.g. estimating where the user is looking at on the phone screen, can be applied in various applications. However, the widely used appearance-based CNN methods still have two issues for practical adoption. First, due to the limited dataset, gaze estimation is very likely to suffer from over-fitting, leading to poor accuracy at run time. Second, the current methods are usually not robust, i.e. their prediction results having notable jitters even when the user is performing gaze fixation, which degrades user experience greatly. For the first issue, we propose a new tolerant and talented (TAT) training scheme, which is an iterative random knowledge distillation framework enhanced with cosine similarity pruning and aligned orthogonal initialization. The knowledge distillation is a tolerant teaching process providing diverse and informative supervision. The enhanced pruning and initialization is a talented learning process prompting the network to escape from the local minima and re-born from a better start. For the second issue, we define a new metric to measure the robustness of gaze estimator, and propose an adversarial training based Disturbance with Ordinal loss (DwO) method to improve it. The experimental results show that our TAT method achieves state-of-the-art performance on GazeCapture dataset, and that our DwO method improves the robustness while keeping comparable accuracy. © 2019 IEEE.","Gaze estimation; Over fitting; Robustness; Smart phone","Computer vision; Distillation; Iterative methods; Robustness (control systems); User experience; Appearance based; Cosine similarity; Gaze estimation; Learning process; Overfitting; State-of-the-art performance; Teaching process; Training schemes; Smartphones",Conference Paper,"Final","",Scopus,2-s2.0-85082466578
"Chaudhary A.K., Kothari R., Acharya M., Dangi S., Nair N., Bailey R., Kanan C., DIaz G., Pelz J.B.","57210103548;56533410500;57215962240;56895915700;57215969974;16641965200;35185157400;55436582600;7007018556;","RITnet: Real-time semantic segmentation of the eye for gaze tracking",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022181","3698","3702",,15,"10.1109/ICCVW.2019.00568","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082462711&doi=10.1109%2fICCVW.2019.00568&partnerID=40&md5=8009313825f186d6ea556bfe1b69787f","Rochester Institute of Technology, United States","Chaudhary, A.K., Rochester Institute of Technology, United States; Kothari, R., Rochester Institute of Technology, United States; Acharya, M., Rochester Institute of Technology, United States; Dangi, S., Rochester Institute of Technology, United States; Nair, N., Rochester Institute of Technology, United States; Bailey, R., Rochester Institute of Technology, United States; Kanan, C., Rochester Institute of Technology, United States; DIaz, G., Rochester Institute of Technology, United States; Pelz, J.B., Rochester Institute of Technology, United States","Accurate eye segmentation can improve eye-gaze estimation and support interactive computing based on visual attention; however, existing eye segmentation methods suffer from issues such as person-dependent accuracy, lack of robustness, and an inability to be run in real-time. Here, we present the RITnet model, which is a deep neural network that combines U-Net and DenseNet. RITnet is under 1 MB and achieves 95.3% accuracy on the 2019 OpenEDS Semantic Segmentation challenge. Using a GeForce GTX 1080 Ti, RITnet tracks at > 300Hz, enabling real-time gaze tracking applications. Pre-trained models and source code are available. © 2019 IEEE.",,"Behavioral research; Computer vision; Deep neural networks; Semantics; Gaze tracking; Interactive computing; Person-dependent; Real-time semantics; Segmentation methods; Semantic segmentation; Source codes; Visual Attention; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85082462711
"Damer N., Boutros F., Kirchbuchner F., Kuijper A.","50861109400;57205379838;57031859600;56131137100;","D-ID-net: Two-stage domain and identity learning for identity-preserving image generation from semantic segmentation",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9021978","3677","3682",,4,"10.1109/ICCVW.2019.00454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082461905&doi=10.1109%2fICCVW.2019.00454&partnerID=40&md5=37b4282b9d78977a24e6ad32b906525d","Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany; Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany","Damer, N., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Boutros, F., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Kirchbuchner, F., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany; Kuijper, A., Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany, Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany","Training functionality-demanding AR/VR systems require accurate and robust gaze estimation and tracking solutions. Achieving such a performance requires the availability of diverse eye image data that might only be acquired by the means of image generation. Works addressing the generation of such images did not target realistic and identity-specific images, nor did they address the practical-relevant case of generation from semantic labels. Therefore, this work proposes a solution to generate realistic and identity-specific images that correspond to semantic labels, given samples of a specific identity. Our proposed solution consists of two stages. In the first stage, a network is trained to transform the semantic label into a corresponding eye image of a generic identity. The second stage is an identity-specific network that induces identity details on the generic eye image. The results of our D-ID-Net solutions shows a high degree of identity-preservation and similarity to the ground-truth images, with an RMSE of 7.235. © 2019 IEEE.","Biometric image generation; Eye biometrics; Identity preservation","Biometrics; Computer vision; Semantics; Biometric image; Eye images; Gaze estimation; Ground truth; Image generations; Semantic labels; Semantic segmentation; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-85082461905
"Chen Z., Deng D., Pi J., Shi B.E.","56808413900;57215966283;57195220788;7402547071;","Unsupervised outlier detection in appearance-based gaze estimation",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022626","1088","1097",,1,"10.1109/ICCVW.2019.00139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082444548&doi=10.1109%2fICCVW.2019.00139&partnerID=40&md5=9b946867748c37909753d83a7b4fec26","Hong Kong University of Science and Technology, Hong Kong","Chen, Z., Hong Kong University of Science and Technology, Hong Kong; Deng, D., Hong Kong University of Science and Technology, Hong Kong; Pi, J., Hong Kong University of Science and Technology, Hong Kong; Shi, B.E., Hong Kong University of Science and Technology, Hong Kong","Appearance-based gaze estimation maps RGB images to estimates of gaze directions. One problem in gaze estimation is that there always exist low-quality samples (outliers) in which the eyes are barely visible. These low-quality samples are mainly caused by blinks, occlusions (e.g. by eye glasses), blur (e.g. due to motion) and failures of the eye landmark detection. Training on these outliers degrades the performance of gaze estimators, since they have no or limited information about gaze directions. It is also risky to give estimates based on these images in real-world applications, as these estimates may be unreliable. To solve this problem, we propose an algorithm that detects outliers without supervision. Based on the input images with only gaze labels, the proposed algorithm learns to predict a gaze estimates and an additional confidence score, which alleviates the impact of outliers during learning. We evaluated this algorithm on the MPIIGaze dataset and on an internal dataset. In cross-subject evaluation, our experimental results show that the proposed algorithm results in a better gaze estimator (8% improvement). The proposed algorithm is also able to reliably detect outliers during testing, with a precision of 0.71 when the recall is 0.63. © 2019 IEEE.","Appearance based gaze estimation; Outlier detection","Anomaly detection; Computer vision; Data handling; Appearance based; Confidence score; Gaze direction; Gaze estimation; Input image; Landmark detection; Limited information; Low qualities; Statistics",Conference Paper,"Final","",Scopus,2-s2.0-85082444548
"Lu M., Liao D., Li Z.-N.","55968784700;56042275100;55885610400;","Learning spatiotemporal attention for egocentric action recognition",2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,"9022139","4425","4434",,4,"10.1109/ICCVW.2019.00543","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082439068&doi=10.1109%2fICCVW.2019.00543&partnerID=40&md5=6263c7846819cb82511192fa1cf076c0","School of Computing Science, Simon Fraser University, Canada; Huawei Technologies, Canada; College of Computer Science and Technology, Zhejiang University, China","Lu, M., School of Computing Science, Simon Fraser University, Canada, Huawei Technologies, Canada; Liao, D., College of Computer Science and Technology, Zhejiang University, China; Li, Z.-N., School of Computing Science, Simon Fraser University, Canada","Recognizing camera wearers' actions from videos captured by the head-mounted camera is a challenging task. Previous methods often utilize attention models to characterize the relevant spatial regions to facilitate egocentric action recognition. Inspired by the recent advances of spatiotemporal feature learning using 3D convolutions, we propose a simple yet efficient module for learning spatiotemporal attention in egocentric videos with human gaze as supervision. Our model employs a two-stream architecture which consists of an appearance-based stream and motion-based stream. Each stream has the spatiotemporal attention module (STAM) to produce an attention map, which helps our model to focus on the relevant spatiotemporal regions of the video for action recognition. The experimental results demonstrate that our model is able to outperform the state-of-the-art methods by a large margin on the standard EGTEA Gaze+ dataset and produce attention maps that are consistent with human gaze. © 2019 IEEE.","Egocentric action recognition; Gaze; Spatiotemporal attention; STAM","Cameras; Large dataset; Action recognition; Gaze; Head mounted Camera; Spatio temporal features; Spatiotemporal attention; Spatiotemporal regions; STAM; State-of-the-art methods; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85082439068
"Kellnhofer P., Recasens A., Stent S., Matusik W., Torralba A.","55250016000;57189096003;56229805900;56230515000;57216041654;","Gaze360: Physically unconstrained gaze estimation in the wild",2019,"Proceedings of the IEEE International Conference on Computer Vision","2019-October",,"9010825","6911","6920",,31,"10.1109/ICCV.2019.00701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081928506&doi=10.1109%2fICCV.2019.00701&partnerID=40&md5=b2c73c34b069d1f3d2ccfabe75cf183b","Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Toyota Research Institute, Cambridge, MA  02139, United States","Kellnhofer, P., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Recasens, A., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Stent, S., Toyota Research Institute, Cambridge, MA  02139, United States; Matusik, W., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Torralba, A., Massachusetts Institute of Technology, Cambridge, MA  02139, United States","Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale remote gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models will be made available at http://gaze360.csail.mit.edu. © 2019 IEEE.",,"3D modeling; Benchmarking; Computer vision; Large dataset; Uncertainty analysis; Benchmark datasets; Collection methods; Cross-dataset evaluation; Domain adaptation; Gaze estimation; Generalization performance; Outdoor environment; Temporal information; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85081928506
"Park S., Mello S.D., Molchanov P., Iqbal U., Hilliges O., Kautz J.","57195422868;57201314496;57191194479;57191075029;14041644100;7006458237;","Few-shot adaptive gaze estimation",2019,"Proceedings of the IEEE International Conference on Computer Vision","2019-October",,"9008783","9367","9376",,22,"10.1109/ICCV.2019.00946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081911053&doi=10.1109%2fICCV.2019.00946&partnerID=40&md5=84463c72b6c4518a837b78d05dc9c9c9","NVIDIA; ETH Zurich, Switzerland","Park, S., NVIDIA, ETH Zurich, Switzerland; Mello, S.D., NVIDIA; Molchanov, P., NVIDIA; Iqbal, U., NVIDIA; Hilliges, O., ETH Zurich, Switzerland; Kautz, J., NVIDIA","Inter-personal anatomical differences limit the accuracy of person-independent gaze estimation networks. Yet there is a need to lower gaze errors further to enable applications requiring higher quality. Further gains can be achieved by personalizing gaze networks, ideally with few calibration samples. However, over-parameterized neural networks are not amenable to learning from few examples as they can quickly over-fit. We embrace these challenges and propose a novel framework for Few-shot Adaptive GaZE Estimation (Faze) for learning person-specific gaze networks with very few (≤ 9) calibration samples. Faze learns a rotation-aware latent representation of gaze via a disentangling encoder-decoder architecture along with a highly adaptable gaze estimator trained using meta-learning. It is capable of adapting to any new person to yield significant performance gains with as few as 3 samples, yielding state-of-the-art performance of 3.18-deg on GazeCapture, a 19% improvement over prior art. We open-source our code at https://github.com/NVlabs/few-shot-gaze. © 2019 IEEE.",,"Calibration; Open systems; Calibration samples; Encoder-decoder architecture; Gaze estimation; Metalearning; Parameterized neural network; Performance Gain; Person-independent; State-of-the-art performance; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85081911053
"He Z., Spurr A., Zhang X., Hilliges O.","57215780380;57200213697;57142162900;14041644100;","Photo-realistic monocular gaze redirection using generative adversarial networks",2019,"Proceedings of the IEEE International Conference on Computer Vision","2019-October",,"9008804","6931","6940",,10,"10.1109/ICCV.2019.00703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081906519&doi=10.1109%2fICCV.2019.00703&partnerID=40&md5=3f70e9fd8179b8a914c62833e8d94d9d","AIT Lab, ETH Zürich, Switzerland; Institute of Neuroinformatics, ETH Zürich, University of Zürich, Switzerland","He, Z., AIT Lab, ETH Zürich, Switzerland, Institute of Neuroinformatics, ETH Zürich, University of Zürich, Switzerland; Spurr, A., AIT Lab, ETH Zürich, Switzerland; Zhang, X., AIT Lab, ETH Zürich, Switzerland; Hilliges, O., AIT Lab, ETH Zürich, Switzerland","Gaze redirection is the task of changing the gaze to a desired direction for a given monocular eye patch image. Many applications such as videoconferencing, films, games, and generation of training data for gaze estimation require redirecting the gaze, without distorting the appearance of the area surrounding the eye and while producing photo-realistic images. Existing methods lack the ability to generate perceptually plausible images. In this work, we present a novel method to alleviate this problem by leveraging generative adversarial training to synthesize an eye image conditioned on a target gaze direction. Our method ensures perceptual similarity and consistency of synthesized images to the real images. Furthermore, a gaze estimation loss is used to control the gaze direction accurately. To attain high-quality images, we incorporate perceptual and cycle consistency losses into our architecture. In extensive evaluations we show that the proposed method outperforms state-of-the-art approaches in terms of both image quality and redirection precision. Finally, we show that generated images can bring significant improvement for the gaze estimation task if used to augment real training data. © 2019 IEEE.",,"Computer vision; Quality control; Video conferencing; Adversarial networks; Gaze estimation; High quality images; Perceptual similarity; Photo-realistic; Photorealistic images; State-of-the-art approach; Synthesized images; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85081906519
"Dimas G., Iakovidis D., Koulaouzidis A.","57195485922;6603967427;14627591700;","MedGaze: Gaze Estimation on WCE Images Based on a CNN Autoencoder",2019,"Proceedings - 2019 IEEE 19th International Conference on Bioinformatics and Bioengineering, BIBE 2019",,,"8941676","363","367",,1,"10.1109/BIBE.2019.00071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078573803&doi=10.1109%2fBIBE.2019.00071&partnerID=40&md5=dc6225afbc5ec3e651c891aeb88da295","Dept. of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Endoscopy Unit, Royal Infirmary of Edinburgh, Edinburgh, United Kingdom","Dimas, G., Dept. of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Iakovidis, D., Dept. of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Koulaouzidis, A., Endoscopy Unit, Royal Infirmary of Edinburgh, Edinburgh, United Kingdom","The interpretation of medical images depends on physicians' experience. Over time, physicians develop their ability to examine the images, and this is usually reflected on gaze patterns they follow to observe visual cues, which lead them to diagnostic decisions. In the context of gaze prediction, graph and machine learning methods have been proposed for the visual saliency estimation on generic images. In this work we preset a novel and robust gaze estimation methodology based on physicians' eye fixations, using convolutional neural networks combined with regularization methods, on medical images taken during Wireless Capsule Endoscopy (WCE). Furthermore, we present a novel dataset of physicians' eye fixation patterns which was used for the training of the neural network model. The model was able to achieve 68.5% Judd's Area Under the receiver operating Characteristic (AUC-J). © 2019 IEEE.","Convolutional neural networks; Eye-tracking; Gaze estimation; Machine learning; Saliency","Bioinformatics; Convolution; Endoscopy; Gallium compounds; Learning algorithms; Learning systems; Machine learning; Medical imaging; Neural networks; Convolutional neural network; Gaze estimation; Machine learning methods; Neural network model; Receiver operating characteristics; Regularization methods; Saliency; Wireless capsule endoscopy; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85078573803
"Klein Salvalaio B., De Oliveira Ramos G.","57215354145;55617224100;","Self-adaptive appearance-based eye-tracking with online transfer learning",2019,"Proceedings - 2019 Brazilian Conference on Intelligent Systems, BRACIS 2019",,,"8924035","383","388",,,"10.1109/BRACIS.2019.00074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077026019&doi=10.1109%2fBRACIS.2019.00074&partnerID=40&md5=7b0988d351a586db34bfaea222724813","SAP Labs Latin America, São Leopoldo, RS, Brazil; Universidade Do Vale Do Rio Dos Sinos, São Leopoldo, RS, Brazil","Klein Salvalaio, B., SAP Labs Latin America, São Leopoldo, RS, Brazil; De Oliveira Ramos, G., Universidade Do Vale Do Rio Dos Sinos, São Leopoldo, RS, Brazil","Eye-tracking plays a role in human-computer interactions and has proven useful in a wide variety of domains. We consider appearance-based eye-tracking, where one tracks eye movements based solely on conventional images (rather than on sophisticated additional hardware). Recent advances made in Deep Learning and, in particular, convolutional neural networks have allowed appearance-based eye-tracking to achieve better results than ever. However, current literature still lacks methods that generalize to different combinations of user, environment and device. In this work, we introduce Online Deep Appearance-Based Eye-Tracking (ODABE), which overcomes such a limitation by considering online transfer learning, thus enabling eye-tracking models to self-adapt to different context very rapidly. Our results show that ODABE improves upon previous research when context changes, decreasing the prediction error by 50.95% on average, on tested cases. © 2019 IEEE.","Deep Learning; Eye-Tracking; Gaze-Tracking; Transfer Learning","Deep learning; E-learning; Eye movements; Human computer interaction; Intelligent systems; Neural networks; Appearance based; Convolutional neural network; Gaze tracking; Prediction errors; Self-adapt; Transfer learning; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85077026019
"Lian D., Hu L., Luo W., Xu Y., Duan L., Yu J., Gao S.","57203743979;57203744059;57206747739;57192081433;56647447800;8569656400;35224747100;","Multiview multitask gaze estimation with deep convolutional neural networks",2019,"IEEE Transactions on Neural Networks and Learning Systems","30","10","8454246","3010","3023",,13,"10.1109/TNNLS.2018.2865525","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052799761&doi=10.1109%2fTNNLS.2018.2865525&partnerID=40&md5=e3de36913221eb1ba50d92281ff5be02","School of Information Science and Technology, ShanghaiTech University, Shanghai, China; University of Chinese Academy of Sciences, Beijing, 100049, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China","Lian, D., School of Information Science and Technology, ShanghaiTech University, Shanghai, China, University of Chinese Academy of Sciences, Beijing, 100049, China; Hu, L., School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Luo, W., School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Xu, Y., School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Duan, L., School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Yu, J., School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Gao, S., School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China","Gaze estimation, which aims to predict gaze points with given eye images, is an important task in computer vision because of its applications in human visual attention understanding. Many existing methods are based on a single camera, and most of them only focus on either the gaze point estimation or gaze direction estimation. In this paper, we propose a novel multitask method for the gaze point estimation using multiview cameras. Specifically, we analyze the close relationship between the gaze point estimation and gaze direction estimation, and we use a partially shared convolutional neural networks architecture to simultaneously estimate the gaze direction and gaze point. Furthermore, we also introduce a new multiview gaze tracking data set that consists of multiview eye images of different subjects. As far as we know, it is the largest multiview gaze tracking data set. Comprehensive experiments on our multiview gaze tracking data set and existing data sets demonstrate that our multiview multitask gaze point estimation solution consistently outperforms existing methods. © 2012 IEEE.","Convolutional neural networks (CNNs); gaze tracking; multitask learning (MTL); multiview learning","Behavioral research; Cameras; Convolution; Deep neural networks; Estimation; Feature extraction; Job analysis; Neural networks; Robustness (control systems); Convolutional neural network; Gaze tracking; Head; Multi-view learning; Multitask learning; Task analysis; Eye tracking; attention; behavior; eye fixation; human; photostimulation; physiology; procedures; Attention; Fixation, Ocular; Humans; Multitasking Behavior; Neural Networks, Computer; Photic Stimulation",Article,"Final","",Scopus,2-s2.0-85052799761
[无可用作者姓名],[无可用的作者 ID],"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems, BTAS 2019",2019,"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems, BTAS 2019",,,,"","",359,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092305998&partnerID=40&md5=a58bed479048b2ee389696e968f08306",,"","The proceedings contain 43 papers. The topics discussed include: gaze-angle impact on iris segmentation using CNNs; effects of postmortem decomposition on face recognition; face phylogeny tree: deducing relationships between near-duplicate face images using legendre polynomials and radial basis functions; make the bag disappear: carrying status-invariant gait-based human age estimation using parallel generative adversarial networks; multi-task learning for detecting and segmenting manipulated facial images and videos; reliable age and gender estimation from face images: stating the confidence of model predictions; unconstrained thermal hand segmentation; hierarchical bloom filter framework for security, space-efficiency, and rapid query handling in biometric systems; and deep learning-based feature extraction in iris recognition: use existing models, fine-tune or train from scratch?.",,,Conference Review,"Final","",Scopus,2-s2.0-85092305998
"Jalilian E., Uhl A., Karakaya M.","57195436690;7005841206;35223530800;","Gaze-angle Impact on Iris Segmentation using CNNs",2019,"2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems, BTAS 2019",,,"9185970","","",,1,"10.1109/BTAS46853.2019.9185970","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092266161&doi=10.1109%2fBTAS46853.2019.9185970&partnerID=40&md5=a0bf55c5539d468015d143569081bac3","University of Salzburg, Salzburg, Austria; University of Central Arkansas, Conway, AK, United States","Jalilian, E., University of Salzburg, Salzburg, Austria; Uhl, A., University of Salzburg, Salzburg, Austria; Karakaya, M., University of Central Arkansas, Conway, AK, United States","Emerging standoff iris recognition systems operate under unconstrained conditions and the iris images captured by these systems are more subject to off-angle acquisition distortions. While deep learning techniques (e.g. convolutional neural networks (CNNs)) are increasingly becoming a tool of choice for iris segmentation tasks, yet there is a significant lack of information about how these distortions affect the performance of such networks. In this work, we thoroughly discuss the general effect of different gaze-angles on ocular biometrics and relate the findings to off-angle iris segmentation using CNNs. In particular, we conduct systematical analysis on the impact of different gaze-angles on segmentation performance of two CNNs with different architectures. The networks' performance turns out to have a direct relation to the closeness of gaze-angles in the training and testing images, and it declines as the gaze-angles diverge. We further investigate the effect of (i) increasing the quantity of iris training data in case of gaze-angles in training and test data match, and (ii) considering iris training data consisting of several distinct gaze-angles (we obtain promising results using the second configuration). Finally, we compare our results to those of some classical iris segmentation algorithms, where the CNNs are found to outperform the classical algorithms. © 2019 IEEE.",,"Biometrics; Deep learning; Image segmentation; Iris recognition systems; Iris segmentation; Learning techniques; Ocular biometrics; Segmentation performance; Systematical analysis; Training and testing; Training data; Convolutional neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85092266161
"Starostenko A., Kozin F., Gorbachev R.","57193341177;57215664544;56769537700;","Real-Time Algorithms for Head Mounted Gaze Tracker",2019,"Proceedings - 2019 International Conference on Artificial Intelligence: Applications and Innovations, IC-AIAI 2019",,,"9007317","86","89",,,"10.1109/IC-AIAI48757.2019.00025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081626788&doi=10.1109%2fIC-AIAI48757.2019.00025&partnerID=40&md5=2fc273be145472e8fcd4c91a5f24c805","Moscow Institute of Physics and Technology, Dolgoprudny, Moscow Region, Russian Federation","Starostenko, A., Moscow Institute of Physics and Technology, Dolgoprudny, Moscow Region, Russian Federation; Kozin, F., Moscow Institute of Physics and Technology, Dolgoprudny, Moscow Region, Russian Federation; Gorbachev, R., Moscow Institute of Physics and Technology, Dolgoprudny, Moscow Region, Russian Federation","We introduce a set of real-time algorithms for head mounted gaze tracker consisting of three cameras: two cameras for the eyes and one camera for the scene. The direction of the optical axis of the eye in three-dimensional space is calculated using the reflection of IR LEDs from the cornea. Individual features of the user are taken into account using the short-term calibration procedure. The described algorithms combine high accuracy in determining the point of gaze with high speed. The procedure for determining the point of gaze consists of the following algorithms: estimation of the position of the pupils on the eye cameras frames using of the threshold processing taking into account the histogram of the frame and further approximation of the pupil by an ellipse; estimation of the IR LEDs glare position on the frames of the eye cameras using threshold processing; filtration of the glares by brightness, size, circularity, and of the glares beyond the iris, the size of the iris is estimated by the distance from eye camera to pupil position calculated on the previous frame; indexation of the glares with the template matching; estimation of the optical axis angles of the eye using a spherical model of the cornea with the nonlinear optimization methods; estimation of the point of gaze on the scene camera frame using individual user features found during the calibration process. During calibration, the movement of the ArUco calibration mark and its selection on the scene camera frame are used. To calculate the gaze position on the scene camera, a regression algorithm is used, which implicitly takes into account the individual characteristics of the user. © 2019 IEEE.","gaze estimation; head mount devices; human-computer interaction; object detection and tracking","Approximation algorithms; Artificial intelligence; Calibration; Cameras; Glare; Human computer interaction; Light emitting diodes; Nonlinear programming; Object tracking; Real time systems; Template matching; Calibration procedure; Gaze estimation; head mount devices; Individual characteristics; Nonlinear optimization methods; Object detection and tracking; Regression algorithms; Three dimensional space; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85081626788
"Dipaola S., Yalcin O.N.","14035416700;57203034300;","A multi-layer artificial intelligence and sensing based affective conversational embodied agent",2019,"2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos, ACIIW 2019",,,"8925291","91","92",,1,"10.1109/ACIIW.2019.8925291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077818348&doi=10.1109%2fACIIW.2019.8925291&partnerID=40&md5=49ce11dd00a59cacaee1067695a622c4","School of Interactive Art and Tech, Simon Fraser University, Vancouver, Canada","Dipaola, S., School of Interactive Art and Tech, Simon Fraser University, Vancouver, Canada; Yalcin, O.N., School of Interactive Art and Tech, Simon Fraser University, Vancouver, Canada","Building natural and conversational virtual humans is a task of formidable complexity. We believe that, especially when building agents that affectively interact with biological humans in real-time, a cognitive science-based, multilayered sensing and artificial intelligence (AI) systems approach is needed. For this demo, we show a working version (through human interaction with it) our modular system of natural, conversation 3D virtual human using AI or sensing layers. These including sensing the human user via facial emotion recognition, voice stress, semantic meaning of the words, eye gaze, heart rate, and galvanic skin response. These inputs are combined with AI sensing and recognition of the environment using deep learning natural language captioning or dense captioning. These are all processed by our AI avatar system allowing for an affective and empathetic conversation using an NLP topic-based dialogue capable of using facial expressions, gestures, breath, eye gaze and voice language-based two-way back and forth conversations with a sensed human. Our lab has been building these systems in stages over the years. © 2019 IEEE.","affective computing; artificial intelligence; biosensing; conversational agent; deep learning; embodied agent; embodied character agents; sensing systems","Artificial intelligence; Cognitive systems; Deep learning; Electrophysiology; FORTH (programming language); Intelligent computing; Real time systems; Semantics; Speech recognition; Affective Computing; Biosensing; Conversational agents; Embodied agent; Embodied characters; Sensing systems; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-85077818348
"Bruckert A., Lam Y.H., Christie M., Meur O.L.","57212481599;57219525292;57219918829;8611330900;","Deep Learning for Inter-Observer Congruency Prediction",2019,"Proceedings - International Conference on Image Processing, ICIP","2019-September",,"8803596","3766","3770",,2,"10.1109/ICIP.2019.8803596","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076816671&doi=10.1109%2fICIP.2019.8803596&partnerID=40&md5=4d21d96e714006947294d8ef0d28a828","Univ Rennes, IRISA, CNRS, France; Nokia Technologies, Tampere, Finland","Bruckert, A., Univ Rennes, IRISA, CNRS, France; Lam, Y.H., Nokia Technologies, Tampere, Finland; Christie, M., Univ Rennes, IRISA, CNRS, France; Meur, O.L., Univ Rennes, IRISA, CNRS, France","According to the literature regarding visual saliency, observers may exhibit considerable variations in their gaze behaviors. These variations are influenced by aspects such as cultural background, age or prior experiences, but also by features in the observed images. The dispersion between the gaze of different observers looking at the same image is commonly referred as inter-observer congruency (IOC). Predicting this congruence can be of great interest when it comes to study the visual perception of an image. In this paper, we introduce a new method based on deep learning techniques to predict the IOC of an image. This is achieved by first extracting features from an image through a deep convolutional network. We then show that using such features to train a model with a shallow network regression technique significantly improves the precision of the prediction over existing approaches. © 2019 IEEE.","deep features; gaze patterns; prediction; visual dispersion",,Conference Paper,"Final","",Scopus,2-s2.0-85076816671
"Stavridis K., Psaltis A., Dimou A., Papadopoulos G.Th., Daras P.","57205366411;57190073285;56402999100;57196706610;6602644373;","Deep spatio-temporal modeling for object-level gaze-based relevance assessment",2019,"European Signal Processing Conference","2019-September",,,"","",,1,"10.23919/EUSIPCO.2019.8902990","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075622335&doi=10.23919%2fEUSIPCO.2019.8902990&partnerID=40&md5=02ad16c6e4f83310027da396d6934e81","Centre for Research and Technology, Hellas, Greece","Stavridis, K., Centre for Research and Technology, Hellas, Greece; Psaltis, A., Centre for Research and Technology, Hellas, Greece; Dimou, A., Centre for Research and Technology, Hellas, Greece; Papadopoulos, G.Th., Centre for Research and Technology, Hellas, Greece; Daras, P., Centre for Research and Technology, Hellas, Greece","The current work investigates the problem of object-level relevance assessment prediction, taking into account the user's captured gaze signal (behaviour) and following the Deep Learning (DL) paradigm. Human gaze, as a sub-conscious response, is influenced from several factors related to the human mental activity. Several studies have so far proposed methodologies based on the use of gaze statistical modeling and naive classifiers for assessing images or image patches as relevant or not to the user's interests. Nevertheless, the outstanding majority of literature approaches only relied so far on the use of handcrafted features and relative simple classification schemes. On the contrary, the current work focuses on the use of DL schemes that will enable the modeling of complex patterns in the captured gaze signal and the subsequent derivation of corresponding discriminant features. Novel contributions of this study include: a) the introduction of a large-scale annotated gaze dataset, suitable for training DL models, b) a novel method for gaze modeling, capable of handling gaze sensor errors, and c) a DL based method, able to capture gaze patterns for assessing image objects as relevant or non-relevant, with respect to the user's preferences. Extensive experiments demonstrate the efficiency of the proposed method, taking also into consideration key factors related to the human gaze behaviour. © 2019 IEEE","DL; Gaze modeling; Relevance assessment","C (programming language); Deep learning; Large dataset; Signal processing; Classification scheme; Complex pattern; Mental activity; Naive classifiers; Relevance assessments; Spatio-temporal models; Statistical modeling; User's preferences; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85075622335
"Cristina S., Camilleri K.P.","49963155000;8301303700;","Gaze tracking by joint head and eye pose estimation under free head movement",2019,"European Signal Processing Conference","2019-September",,,"","",,,"10.23919/EUSIPCO.2019.8902786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075621911&doi=10.23919%2fEUSIPCO.2019.8902786&partnerID=40&md5=ba8888192d6c60ba861c0efe632f1595","Department of Systems and Control Engineering, University of Malta, Msida, Malta","Cristina, S., Department of Systems and Control Engineering, University of Malta, Msida, Malta; Camilleri, K.P., Department of Systems and Control Engineering, University of Malta, Msida, Malta","Recent trends in the field of eye-gaze tracking have been shifting towards the estimation of gaze direction in everyday life settings, hence calling for methods that alleviate the constraints typically associated with existing methods, which limit their applicability in less controlled conditions. In this paper, we propose a method for eye-gaze estimation as a function of both eye and head pose components, without requiring prolonged user-cooperation prior to gaze estimation. Our method exploits the trajectories of salient feature trackers spread randomly over the face region for the estimation of the head rotation angles, which are subsequently used to drive a spherical eye-in-head rotation model that compensates for the changes in eye region appearance under head rotation. We investigate the validity of the proposed method on a publicly available data set. © 2019 IEEE","Eye-gaze tracking; Passive; Pervasive","Digital storage; Eye movements; Image segmentation; Controlled conditions; Eye gaze tracking; Gaze estimation; Passive; Pervasive; Pose estimation; Salient features; User cooperation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85075621911
"Xia Y., Yu H., Wang F.-Y.","57192671097;56115992300;57211758869;","Accurate and robust eye center localization via fully convolutional networks",2019,"IEEE/CAA Journal of Automatica Sinica","6","5","8823575","1127","1138",,24,"10.1109/JAS.2019.1911684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072205184&doi=10.1109%2fJAS.2019.1911684&partnerID=40&md5=784d75d4187458d061ca3bd484cda82d","School of Creative Technologies, University of Portsmouth, Portsmouth, PO1 2DJ, United Kingdom; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China","Xia, Y., School of Creative Technologies, University of Portsmouth, Portsmouth, PO1 2DJ, United Kingdom; Yu, H., School of Creative Technologies, University of Portsmouth, Portsmouth, PO1 2DJ, United Kingdom; Wang, F.-Y., State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China","Eye center localization is one of the most crucial and basic requirements for some human-computer interaction applications such as eye gaze estimation and eye tracking. There is a large body of works on this topic in recent years, but the accuracy still needs to be improved due to challenges in appearance such as the high variability of shapes, lighting conditions, viewing angles and possible occlusions. To address these problems and limitations, we propose a novel approach in this paper for the eye center localization with a fully convolutional network (FCN), which is an end-to-end and pixels-to-pixels network and can locate the eye center accurately. The key idea is to apply the FCN from the object semantic segmentation task to the eye center localization task since the problem of eye center localization can be regarded as a special semantic segmentation problem. We adapt contemporary FCN into a shallow structure with a large kernel convolutional block and transfer their performance from semantic segmentation to the eye center localization task by fine-tuning. Extensive experiments show that the proposed method outperforms the state-of-the-art methods in both accuracy and reliability of eye center localization. The proposed method has achieved a large performance improvement on the most challenging database and it thus provides a promising solution to some challenging applications. © 2014 Chinese Association of Automation.","Deep learning; eye center localization; eye gaze estimation; eye tracking; fully convolutional network (FCN); humancomputer interaction","Convolution; Convolutional neural networks; Deep learning; Human computer interaction; Pixels; Semantics; Convolutional networks; eye center localization; Eye-gaze; Lighting conditions; Object semantic; Semantic segmentation; Shallow structure; State-of-the-art methods; Eye tracking",Article,"Final","",Scopus,2-s2.0-85072205184
"Corcoran P., Lemley J., Costache C., Varkarakis V.","57190839462;23005117400;56956976400;57204764493;","Deep Learning for Consumer Devices and Services 2-AI Gets Embedded at the Edge",2019,"IEEE Consumer Electronics Magazine","8","5","8822518","10","19",,2,"10.1109/MCE.2019.2923042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072088033&doi=10.1109%2fMCE.2019.2923042&partnerID=40&md5=668c048809a00ac216491b89ce10bab0","National University of Ireland Galway (NUIG), Galway, Ireland","Corcoran, P., National University of Ireland Galway (NUIG), Galway, Ireland; Lemley, J., National University of Ireland Galway (NUIG), Galway, Ireland; Costache, C., National University of Ireland Galway (NUIG), Galway, Ireland; Varkarakis, V., National University of Ireland Galway (NUIG), Galway, Ireland","The recent explosive growth of deep learning is enabling a new generation of intelligent consumer devices. Specialized deep learning inference now provides data analysis capabilities that once required an active cloud connection, while reducing latency and enhancing data privacy. This paper addresses current progress in Edge artificial intelligence (AI) technology in several consumer contexts including privacy, biometrics, eye gaze, driver monitoring systems, and more. New developments and challenges in edge hardware and emerging opportunities are identified. Our previous article, Deep learning for consumer devices and services, introduced many of the basics of deep learning and AI. In this paper, we explore the current paradigm shift of AI from the data center into CE devices-Edge-AI. © 2012 IEEE.",,"Data privacy; Analysis capabilities; Artificial intelligence technologies; Cloud connections; Consumer devices; Data centers; Driver monitoring system; Explosive growth; Paradigm shifts; Deep learning",Article,"Final","",Scopus,2-s2.0-85072088033
"Gite S., Agrawal H., Kotecha K.","56656365900;56537986300;6506676097;","Early anticipation of driver’s maneuver in semiautonomous vehicles using deep learning",2019,"Progress in Artificial Intelligence","8","3",,"293","305",,4,"10.1007/s13748-019-00177-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070707497&doi=10.1007%2fs13748-019-00177-z&partnerID=40&md5=21d5bd018e84abededa05dec3d5bc05d","Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, India","Gite, S., Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, India; Agrawal, H., Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, India; Kotecha, K., Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, India","Making machines to anticipate human action is a complex research problem. Some of the recent research studies on computer vision and assistive driving have reported that the anticipation of driver’s action few seconds in advance is a challenging problem. These studies are based on the driver’s head movement tracking, eye gaze tracking, and spatiotemporal interest points. The study is aimed to address an important question of how to anticipate a driver’s action while driving and improve the anticipation time. The goal of this study is to review the existing deep learning framework for assistive driving. This paper differs from the existing solutions in two ways. First, it proposes a simplified framework using the driver’s inside video data and develops a driver’s movement tracking (DMT) algorithm. Majority of the existing state of the art is based on inside and outside features of the vehicles. Second, the proposed work tends to improve the image pattern recognition by introducing a fusion of spatiotemporal data points (STIPs) for movement tracking along with eye cuboids and then action anticipation by using deep learning. The proposed DMT algorithm tracks the driver’s movement using STIPs from the input video. Also, a fast eye gaze algorithm tracks eye movements. The features extracted from STIP and eye gaze are fused and analyzed by a deep recurrent neural network to improve the prediction time, thereby giving a few extra seconds to anticipate the driver’s correct action. The performance of the DMT algorithm is compared with the previous algorithms and found that DMT offers 30% improvement with regards to anticipating driver’s action over two recently proposed deep learning algorithms. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Cuboids; Driver safety; Driver’s action tracking; Eye gaze features; Recurrent neural network; Spatiotemporal interest points","Eye movements; Eye tracking; Image enhancement; Learning algorithms; Motion analysis; Pattern recognition; Recurrent neural networks; Action tracking; Cuboids; Driver safety; Eye-gaze; Spatio-temporal interest points; Deep learning",Article,"Final","",Scopus,2-s2.0-85070707497
"He X., Peng Y., Zhao J.","57195953640;7403419173;57197831667;","Which and How Many Regions to Gaze: Focus Discriminative Regions for Fine-Grained Visual Categorization",2019,"International Journal of Computer Vision","127","9",,"1235","1255",,22,"10.1007/s11263-019-01176-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069847800&doi=10.1007%2fs11263-019-01176-2&partnerID=40&md5=a99023e44b21ac501ded196a52eb5f1f","Institute of Computer Science and Technology, Peking University, Beijing, 100871, China","He, X., Institute of Computer Science and Technology, Peking University, Beijing, 100871, China; Peng, Y., Institute of Computer Science and Technology, Peking University, Beijing, 100871, China; Zhao, J., Institute of Computer Science and Technology, Peking University, Beijing, 100871, China","Fine-grained visual categorization (FGVC) aims to discriminate similar subcategories that belong to the same superclass. Since the distinctions among similar subcategories are quite subtle and local, it is highly challenging to distinguish them from each other even for humans. So the localization of distinctions is essential for fine-grained visual categorization, and there are two pivotal problems: (1) Which regions are discriminative and representative to distinguish from other subcategories? (2) How many discriminative regions are necessary to achieve the best categorization performance? It is still difficult to address these two problems adaptively and intelligently. Artificial prior and experimental validation are widely used in existing mainstream methods to discover which and how many regions to gaze. However, their applications extremely restrict the usability and scalability of the methods. To address the above two problems, this paper proposes a multi-scale and multi-granularity deep reinforcement learning approach (M2DRL), which learns multi-granularity discriminative region attention and multi-scale region-based feature representation. Its main contributions are as follows: (1) Multi-granularity discriminative localization is proposed to localize the distinctions via a two-stage deep reinforcement learning approach, which discovers the discriminative regions with multiple granularities in a hierarchical manner (“which problem”), and determines the number of discriminative regions in an automatic and adaptive manner (“how many problem”). (2) Multi-scale representation learning helps to localize regions in different scales as well as encode images in different scales, boosting the fine-grained visual categorization performance. (3) Semantic reward function is proposed to drive M2DRL to fully capture the salient and conceptual visual information, via jointly considering attention and category information in the reward function. It allows the deep reinforcement learning to localize the distinctions in a weakly supervised manner or even an unsupervised manner. (4) Unsupervised discriminative localization is further explored to avoid the heavy labor consumption of annotating, and extremely strengthen the usability and scalability of our M2DRL approach. Compared with state-of-the-art methods on two widely-used fine-grained visual categorization datasets, our M2DRL approach achieves the best categorization accuracy. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Deep reinforcement learning; Fine-grained visual categorization; Multi-granularity discriminative localization; Multi-scale representation learning; Semantic reward; Unsupervised discriminative localization","Deep learning; Machine learning; Scalability; Semantics; Experimental validations; Feature representation; Multi-granularity; Multiscale representations; Reinforcement learning approach; State-of-the-art methods; Unsupervised discriminative localization; Visual categorization; Reinforcement learning",Article,"Final","",Scopus,2-s2.0-85069847800
"Fasanmade A., Aliyu S., He Y., Al-Bayatti A.H., Sharif M.S., Alfakeeh A.S.","57215319726;56743380400;57197817050;27867501600;26428534500;57209777937;","Context-aware driver distraction severity classification using LSTM network",2019,"Proceedings - 2019 International Conference on Computing, Electronics and Communications Engineering, iCCECE 2019",,,"8941966","147","152",,1,"10.1109/iCCECE46942.2019.8941966","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078319267&doi=10.1109%2fiCCECE46942.2019.8941966&partnerID=40&md5=3458e6db0d3751e7a3faf9f0ce0c042a","Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom; School of Architecture Computing and Engineering, College of Arts, Technology and Innovation, UEL University Way, Dockland Campus, London, E16 2RD, United Kingdom","Fasanmade, A., Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom; Aliyu, S., Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom; He, Y., Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom; Al-Bayatti, A.H., Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom; Sharif, M.S., School of Architecture Computing and Engineering, College of Arts, Technology and Innovation, UEL University Way, Dockland Campus, London, E16 2RD, United Kingdom; Alfakeeh, A.S., Faculty of Computing, Engineering and Media, De Montfort University, Leicester, United Kingdom","Advanced Driving Assistance Systems (ADAS) has been a critical component in vehicles and vital to the safety of vehicle drivers and public road transportation systems. In this paper, we present a deep learning technique that classifies drivers' distraction behaviour using three contextual awareness parameters: speed, manoeuver and event type. Using a video coding taxonomy, we study drivers' distractions based on events information from Regions of Interest (RoI) such as hand gestures, facial orientation and eye gaze estimation. Furthermore, a novel probabilistic (Bayesian) model based on the Long short-term memory (LSTM) network is developed for classifying driver's distraction severity. This paper also proposes the use of frame-based contextual data from the multi-view TeleFOT naturalistic driving study (NDS) data monitoring to classify the severity of driver distractions. Our proposed methodology entails recurrent deep neural network layers trained to predict driver distraction severity from time series data. © 2019 IEEE.","Context awareness; Driver Distraction; Dynamic Bayesian networks (DBN); LSTM networks; Severity prediction; Time series","Advanced public transportation systems; Bayesian networks; Deep neural networks; Long short-term memory; Multilayer neural networks; Network layers; Time series; Video signal processing; Context- awareness; Driver distractions; Driving assistance systems; Dynamic Bayesian networks; Facial orientations; Learning techniques; Naturalistic driving studies; Regions of interest; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85078319267
"MacHado E., Carrillo I., Collado M., Chen L.","57204632417;57191370301;57209098497;35519432500;","Visual attention-based object detection in cluttered environments",2019,"Proceedings - 2019 IEEE SmartWorld, Ubiquitous Intelligence and Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Internet of People and Smart City Innovation, SmartWorld/UIC/ATC/SCALCOM/IOP/SCI 2019",,,"9060082","133","139",,2,"10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075151756&doi=10.1109%2fSmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00064&partnerID=40&md5=17d2071fd02fe6a1be3a77bfeeeda816","Department of Innovation and Technology, Ingenieria y Soluciones Informatica, Sevilla, Spain; School of Computer Science and Informatics, De Montfort University, Leicester, United Kingdom","MacHado, E., Department of Innovation and Technology, Ingenieria y Soluciones Informatica, Sevilla, Spain; Carrillo, I., Department of Innovation and Technology, Ingenieria y Soluciones Informatica, Sevilla, Spain, School of Computer Science and Informatics, De Montfort University, Leicester, United Kingdom; Collado, M., Department of Innovation and Technology, Ingenieria y Soluciones Informatica, Sevilla, Spain; Chen, L., School of Computer Science and Informatics, De Montfort University, Leicester, United Kingdom","The study of human visual attention is considered a hot topic in the field of activity recognition, experimental psychology research and human computer interaction. The importance of detecting user objects of interest in real time is critical to provide accurate cues about the user intentions.However, current methods for visual attention extraction and object detection suffer from low performance when moving to ongoing condition. Inherent complexity of cluttered environmentsis considered the major barrier to achieve good performances. To address this challenge, we present a novel method that includes head-worn eye tracker and egocentric video. Our method exploits sliding window-based time series approach in conjunction with aHeuristic probabilistic function to analyse user fixations around potential object of interest in an egocentric video. We evaluate the proposed method using a new dataset annotated with user gaze data and object within a frame image. Our experimental results show that our approach can outperforms several state-of-the-art commonality visual attention-based object detection methods. © 2019 IEEE.","Cluttered Environments; Deep Learning; Fixations; Object Detection; Visual Attention","Behavioral research; Eye tracking; Human computer interaction; Object recognition; Smart city; Trusted computing; Ubiquitous computing; Activity recognition; Cluttered environments; Human visual attention; Inherent complexity; Object detection method; Probabilistic functions; Sliding window-based; State of the art; Object detection",Conference Paper,"Final","",Scopus,2-s2.0-85075151756
"Alarifi J., Fry J., Dancey D., Yap M.H.","57194786999;36179593500;17343675200;15129176400;","Understanding face age estimation: Humans and machine",2019,"CITS 2019 - Proceeding of the 2019 International Conference on Computer, Information and Telecommunication Systems",,,"8862107","","",,3,"10.1109/CITS.2019.8862107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074167227&doi=10.1109%2fCITS.2019.8862107&partnerID=40&md5=c389c5d24d00624cf41006ebb6b56019","Faculty of Science and Engineering, Manchester Metropolitan University, Chester Street, M1 5GD, United Kingdom","Alarifi, J., Faculty of Science and Engineering, Manchester Metropolitan University, Chester Street, M1 5GD, United Kingdom; Fry, J., Faculty of Science and Engineering, Manchester Metropolitan University, Chester Street, M1 5GD, United Kingdom; Dancey, D., Faculty of Science and Engineering, Manchester Metropolitan University, Chester Street, M1 5GD, United Kingdom; Yap, M.H., Faculty of Science and Engineering, Manchester Metropolitan University, Chester Street, M1 5GD, United Kingdom","Face age estimation is an important part of many disciplines, including dermatology, cosmetology, and computer vision. Traditional age estimation studies focus on certain parts of the face to analyse its surface topology. With the advances of deep learning, many Convolutional Neural Networks (CNNs) now outperform traditional methods in the age estimation task. However, it is still not clear what type of features these networks learn when estimating age. This study aims to investigate which facial features are important, for humans and for CNNs, on the age estimation of women in five age groups. We then compare the heat-maps from the human eye gaze with those of the CNN. We consider two main research questions: (1) Which facial regions do humans look at when performing age estimation? (2) Do humans and machine focus on the same facial regions when estimating the age? We answered these questions by conducting two experiments. In the first experiment, we used an eye-tracking software to detect where on the face the gaze of human participants focused the most when they were asked to assign the person in the image to an age class. In the second experiment, we used transfer learning on the network pre-trained on ImageNet, then fine-tuned the network on a benchmark face age dataset to classify the same images shown to the participants. The heat-maps of the VGG16 network were then visualised using Gradient-based Class Activation Map (Grad-CAM). The results showed how our model was almost as accurate as humans in predicting the age of a person from a single image of their face (CNN: 60%; humans: 61%). The results also showed that people mainly look at the eyes and nose when predicting a person's age, while the features learned by the CNN included the eyes, the mouth, and the skin surface. © 2019 IEEE.",,"Classification (of information); Deep learning; Neural networks; Age estimation; Convolutional neural network; Facial feature; Facial regions; Gradient based; Research questions; Surface topology; Transfer learning; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85074167227
"Nguyen P.D.H., Hoffmann M., Pattacini U., Metta G.","57192431670;34971244000;23493590300;6602884280;","Reaching development through visuo-proprioceptive-tactile integration on a humanoid robot - A deep learning approach",2019,"2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics, ICDL-EpiRob 2019",,,"8850681","163","170",,1,"10.1109/DEVLRN.2019.8850681","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073680547&doi=10.1109%2fDEVLRN.2019.8850681&partnerID=40&md5=70641cfc9203f7170504e74444b27637","ICub Facility, Istituto Italiano di Tecnologia, Genova, Italy; Knowledge Technology Institute, Department of Informatics, Universit at Hamburg, Germany; Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic","Nguyen, P.D.H., ICub Facility, Istituto Italiano di Tecnologia, Genova, Italy, Knowledge Technology Institute, Department of Informatics, Universit at Hamburg, Germany; Hoffmann, M., Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, Czech Republic; Pattacini, U., ICub Facility, Istituto Italiano di Tecnologia, Genova, Italy; Metta, G., ICub Facility, Istituto Italiano di Tecnologia, Genova, Italy","The development of reaching in infants has been studied for nearly nine decades. Originally, it was thought that early reaching is visually guided, but more recent evidence is suggestive of 'visually elicited' reaching, i.e. infant is gazing at the object rather than its hand during the reaching movement. The importance of haptic feedback has also been emphasized. Inspired by these findings, in this work we use the simulated iCub humanoid robot to construct a model of reaching development. The robot is presented with different objects, gazes at them, and performs motor babbling with one of its arms. Successful contacts with the object are detected through tactile sensors on hand and forearm. Such events serve as the training set, constituted by images from the robot's two eyes, head joints, tactile activation, and arm joints. A deep neural network is trained with images and head joints as inputs and arm configuration and touch as output. After learning, the network can successfully infer arm configurations that would result in a successful reach, together with prediction of tactile activation (i.e. which body part would make contact). Our main contribution is twofold: (i) our pipeline is end-to-end from stereo images and head joints (6 DoF) to armtorso configurations (10 DoF) and tactile activations, without any preprocessing, explicit coordinate transformations etc.; (ii) unique to this approach, reaches with multiple effectors corresponding to different regions of the sensitive skin are possible. © 2019 IEEE.",,"Anthropomorphic robots; Chemical activation; Object detection; Robotics; Stereo image processing; Co-ordinate transformation; Haptic feedbacks; Humanoid robot; Learning approach; Reaching movements; Sensitive skin; Tactile sensors; Training sets; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85073680547
"Lu M., Li Z.-N., Wang Y., Pan G.","55968784700;55885610400;35231746900;35254246400;","Deep Attention Network for Egocentric Action Recognition",2019,"IEEE Transactions on Image Processing","28","8","8653357","3703","3713",,17,"10.1109/TIP.2019.2901707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067381494&doi=10.1109%2fTIP.2019.2901707&partnerID=40&md5=171cbb3364eac645ecee8888cab3deff","College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China; School of Computing Science, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada; Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou, 310027, China","Lu, M., College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China, School of Computing Science, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada; Li, Z.-N., School of Computing Science, Simon Fraser University, Burnaby, BC  V5A 1S6, Canada; Wang, Y., Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou, 310027, China; Pan, G., College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China","Recognizing a camera wearer's actions from videos captured by an egocentric camera is a challenging task. In this paper, we employ a two-stream deep neural network composed of an appearance-based stream and a motion-based stream to recognize egocentric actions. Based on the insight that human action and gaze behavior are highly coordinated in object manipulation tasks, we propose a spatial attention network to predict human gaze in the form of attention map. The attention map helps each of the two streams to focus on the most relevant spatial region of the video frames to predict actions. To better model the temporal structure of the videos, a temporal network is proposed. The temporal network incorporates bi-directional long short-term memory to model the long-range dependencies to recognize egocentric actions. The experimental results demonstrate that our method is able to predict attention maps that are consistent with human attention and achieve competitive action recognition performance with the state-of-the-art methods on the GTEA Gaze and GTEA Gaze+ datasets. © 1992-2012 IEEE.","action recognition; Attention network; egocentric vision; LSTM; top-down attention","Cameras; Deep neural networks; Forecasting; Action recognition; Long-range dependencies; LSTM; Object manipulation; State-of-the-art methods; Temporal networks; Temporal structures; Topdown; Long short-term memory; attention; automated pattern recognition; classification; factual database; human; human activities; image processing; physiology; procedures; videorecording; Attention; Databases, Factual; Human Activities; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Pattern Recognition, Automated; Video Recording",Article,"Final","",Scopus,2-s2.0-85067381494
"Susan S., Agarwal A., Gulati C., Singh S., Chauhan V.","26423246100;57213411800;57212409209;57212408746;57212409648;","Human attention span modeling using 2D visualization plots for gaze progression and gaze sustenance",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"42","46",,1,"10.1145/3348488.3348494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076592210&doi=10.1145%2f3348488.3348494&partnerID=40&md5=b8117ea64a8c5ce7c81dee9067f6dfb8","Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India","Susan, S., Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India; Agarwal, A., Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India; Gulati, C., Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India; Singh, S., Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India; Chauhan, V., Department of Information Technology, Delhi Technological University, Bawana Road, Delhi, 110042, India","This paper presents a novel perspective on human attention span modeling based on gaze estimation from head pose data extracted from videos. This is achieved by devising specialized 2D visualization plots that capture gaze progression and gaze sustenance over time. In doing so, a low-resolution analysis is assumed, as is the case with most crowd surveillance videos wherein the retinal analysis and iris pattern extraction of individual subjects is made impossible. The information is useful for studies involving the random gaze behavior pattern of humans in a crowded place, or in a controlled environment in seminars or office meetings. The extraction of useful information regarding the attention span of the individual from the spatial and temporal analysis of gaze points is the subject of study in this paper. Different solutions ranging from plotting temporal gaze plots to sustained attention span graphs are investigated, and the results are compared with the existing techniques of attention span modeling and visualization. © 2019 Association for Computing Machinery.","Attention span modeling; Gaze analysis; Head pose estimation","Extraction; Security systems; Virtual reality; Visualization; 2-D visualizations; Controlled environment; Crowd surveillance; Gaze analysis; Gaze estimation; Head Pose Estimation; Spatial and temporal analysis; Sustained attention; Artificial intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85076592210
"Arita R., Suzuki S.","57214758354;7405354773;","Maneuvering assistance of teleoperation robot based on identification of gaze movement",2019,"IEEE International Conference on Industrial Informatics (INDIN)","2019-July",,"8972290","565","570",,2,"10.1109/INDIN41052.2019.8972290","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079067801&doi=10.1109%2fINDIN41052.2019.8972290&partnerID=40&md5=55aba8ed22d0bec8da027f07e3aec97c","Tokyo Denki University, Department of Robotics and Mechatronics, Tokyo, 120-8551, Japan","Arita, R., Tokyo Denki University, Department of Robotics and Mechatronics, Tokyo, 120-8551, Japan; Suzuki, S., Tokyo Denki University, Department of Robotics and Mechatronics, Tokyo, 120-8551, Japan","Narrow angle of view and the time delay of command make operation of a teleoperation robots difficult. In order to solve this problem, a teleoperation system utilizing the followings is presented in this paper: measurement of operator's gaze movement, estimation of gaze point, and the saliency of monitor image proposes. In this system, operator's future gaze target beyond several seconds is estimated using the gaze motion model and an estimated gaze region from probability of saliency map. This paper introduces the configuration of the support system, modeling of gaze movement, generation of the probability of saliency map, and measurement / analysis in robot maneuver preliminary experiment. © 2019 IEEE.","Gaze estimation; Saliency map; Teleoperation robot","Remote control; Robots; Gaze estimation; Gaze movements; Gaze point; Motion modeling; Saliency map; Support systems; Teleoperation robot; Teleoperation systems; Industrial informatics",Conference Paper,"Final","",Scopus,2-s2.0-85079067801
"Pichitwong W., Chamnongthai K.","57191333091;57202765861;","Obscured 3D point-of-gaze estimation by multipoint cloud data",2019,"Proceedings of the 16th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2019",,,"8955244","947","950",,,"10.1109/ECTI-CON47248.2019.8955244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078850350&doi=10.1109%2fECTI-CON47248.2019.8955244&partnerID=40&md5=c968b90b3d53e2893877c20a16c4bf82","King Mongkut's University of Technology Thonburi, Dept. Electronic and Telecommunication Engineering, Bangkok, Thailand","Pichitwong, W., King Mongkut's University of Technology Thonburi, Dept. Electronic and Telecommunication Engineering, Bangkok, Thailand; Chamnongthai, K., King Mongkut's University of Technology Thonburi, Dept. Electronic and Telecommunication Engineering, Bangkok, Thailand","Point cloud sensor is currently used to sense point cloud data information which is 3D position on the surface of target object. Point cloud data information can be input to improve for 3D point of gaze estimation (3D POG). Presently, there are limitation on creating point cloud data information on target object since point cloud data cannot be found if any obstacle in front of the sensor, there are shadow projection. This paper proposes method of multipoint cloud data to create point cloud data on the surface of target object and obscured point cloud data in the shadow projection. Eye tracker sensor provides 3D eyes position data and 2D POG on screen data which each origin represents center of eye tracker and center of screen respectively. These mentioned data are integrated by model fitting to draw a straight line, originating from the center point between left pupil and right pupil, which passes through the2D POG on virtual screen and ends when the line meets the closest point on the target object. In performance evaluation of proposed method, firstly the obscured point cloud data are successfully defined. Secondly, experiment by 4 participants by watching 9 units of testing objects at 2 seconds in free move provide the result of 3D POG estimation at average distance errors by 1.09 cm. © 2019 IEEE.","3-D gaze estimation; Multipoint cloud data","Computer science; Computers; Electrical engineering; Mathematical techniques; Average Distance; Center points; Cloud data; Gaze estimation; Model fitting; Point cloud data; Point of gaze; Target object; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85078850350
"Elbattah M., Carette R., Dequen G., Guerin J.-L., Cilia F.","57163770900;57200860085;23396657900;57200857001;57200855464;","Learning Clusters in Autism Spectrum Disorder: Image-Based Clustering of Eye-Tracking Scanpaths with Deep Autoencoder",2019,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,,"8856904","1417","1420",,5,"10.1109/EMBC.2019.8856904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077873529&doi=10.1109%2fEMBC.2019.8856904&partnerID=40&md5=019b4e524347886403186ca0b1956b87","Laboratoire MIS, Université de Picardie Jules Verne, France","Elbattah, M., Laboratoire MIS, Université de Picardie Jules Verne, France; Carette, R., Laboratoire MIS, Université de Picardie Jules Verne, France; Dequen, G., Laboratoire MIS, Université de Picardie Jules Verne, France; Guerin, J.-L., Laboratoire MIS, Université de Picardie Jules Verne, France; Cilia, F., Laboratoire MIS, Université de Picardie Jules Verne, France","Autism spectrum disorder (ASD) is a lifelong condition characterized by social and communication impairments. This study attempts to apply unsupervised Machine Learning to discover clusters in ASD. The key idea is to learn clusters based on the visual representation of eye-tracking scanpaths. The clustering model was trained using compressed representations learned by a deep autoencoder. Our experimental results demonstrate a promising tendency of clustering structure. Further, the clusters are explored to provide interesting insights into the characteristics of the gaze behavior involved in autism. © 2019 IEEE.","Autism Spectrum Disorder; autoencoder; Clustering; Eye-Tracking; Machine Learning; Scanpath","Deep learning; Diseases; Learning systems; Autism spectrum disorders; Auto encoders; Clustering; Clustering model; Gaze behavior; Scan path; Unsupervised machine learning; Visual representations; Eye tracking; autism; cluster analysis; human; unsupervised machine learning; Autism Spectrum Disorder; Cluster Analysis; Humans; Unsupervised Machine Learning",Conference Paper,"Final","",Scopus,2-s2.0-85077873529
"Atapattu C., Rekabdar B.","57204964581;55217603500;","Improving the realism of synthetic images through a combination of adversarial and perceptual losses",2019,"Proceedings of the International Joint Conference on Neural Networks","2019-July",,"8852449","","",,3,"10.1109/IJCNN.2019.8852449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073251632&doi=10.1109%2fIJCNN.2019.8852449&partnerID=40&md5=25e919e660b95e3897331c2a98a9fa9e","Department of Computer Science, Southern Illinois University, Carbondale, United States","Atapattu, C., Department of Computer Science, Southern Illinois University, Carbondale, United States; Rekabdar, B., Department of Computer Science, Southern Illinois University, Carbondale, United States","In recent years, deep learning methods are becoming more widely used; however, large quantities of labeled training data are required for most models. Labeling large datasets is tedious, expensive, and time consuming. Generating large labeled synthetic datasets, on the other hand, is easier and less expensive since annotations are available. But there is usually a large gap between the distribution of the synthetic and real data. In this paper, we propose a novel method based on Generative Adversarial Networks (GANs) to improve the realism of the synthetic images while preserving the annotation information. In our work the inputs of the GANs are synthetic images instead of random vectors. Furthermore, we describe how a perceptual loss can be utilized while introducing the basic features and techniques from adversarial networks for obtaining better results. We evaluate our approach for appearance-based gaze direction classification on the MPIIGaze dataset. The results show that our generated refined images are more realistic and better preserve the annotation information than the refined images generated by the state-of-the-art methods. © 2019 IEEE.","adversarial network; deep learning; generative adversarial network; synthetic images; unsupervised learning","Classification (of information); Deep learning; Large dataset; Unsupervised learning; Adversarial networks; Appearance based; Labeled training data; Learning methods; State-of-the-art methods; Synthetic and real data; Synthetic datasets; Synthetic images; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85073251632
"Dubey N., Ghosh S., Dhall A.","57211276941;57202710986;35229206900;","Unsupervised Learning of Eye Gaze Representation from the Web",2019,"Proceedings of the International Joint Conference on Neural Networks","2019-July",,"8851961","","",,1,"10.1109/IJCNN.2019.8851961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073227265&doi=10.1109%2fIJCNN.2019.8851961&partnerID=40&md5=ac0f8409519137c45cfc460373049467","Learning Affect and Semantic Image AnalysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India","Dubey, N., Learning Affect and Semantic Image AnalysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India; Ghosh, S., Learning Affect and Semantic Image AnalysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India; Dhall, A., Learning Affect and Semantic Image AnalysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India","Automatic eye gaze estimation has interested researchers for a while now. In this paper, we propose an unsupervised learning based method for estimating the eye gaze region. To train the proposed network ""Ize-Net"" in self-supervised manner, we collect a large 'in the wild' dataset containing 1,54,251 images from the web. For the images in the database, we divide the gaze into three regions based on an automatic technique based on pupil-centers localization and then use a feature-based technique to determine the gaze region. The performance is evaluated on the Tablet Gaze and CAVE datasets by fine-tuning results of Ize-Net for the task of eye gaze estimation. The feature representation learned is also used to train traditional machine learning algorithms for eye gaze estimation. The results demonstrate that the proposed method learns a rich data representation, which can be efficiently finetuned for any eye gaze estimation dataset. © 2019 IEEE.",,"Large dataset; Learning algorithms; Unsupervised learning; Automatic technique; Data representations; Eye-gaze; Feature representation; Feature-based techniques; Fine tuning; Pupil centers; Machine learning",Conference Paper,"Final","",Scopus,2-s2.0-85073227265
"Zhao T., Yan Y., Peng J., Wang H., Fu X.","57192707963;57204644440;57192708626;55986542800;7402204912;","Mask-guided style transfer network for purifying real images",2019,"Proceedings - 2019 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2019",,,"8795010","429","434",,,"10.1109/ICMEW.2019.00080","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071426708&doi=10.1109%2fICMEW.2019.00080&partnerID=40&md5=c7b3a63c0daf9e1b369062b5ca2dedbd","Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Yan, Y., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Peng, J., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Wang, H., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. However, due to the different distribution of synthetic images compared with real images, the desired performance cannot be achieved. To solve this problem, the previous method learned a model to improve the realism of the synthetic images. Different from the previous methods, this paper try to purify real image by extracting discriminative and robust features to convert outdoor real images to indoor synthetic images. In this paper, we first introduce the segmentation masks to construct RGB-mask pairs as inputs, then we design a mask-guided style transfer network to learn style features separately from the attention and bkgd(background) regions and learn content features from full and attention region. Moreover, we propose a novel region-level task-guided loss to restrain the features learnt from style and content. Experiments were performed using mixed studies (qualitative and quantitative) methods to demonstrate the possibility of purifying real images in complex directions. We evaluate the proposed method on various public datasets, including LPW, COCO and MPIIGaze. Experimental results show that the proposed method is effective and achieves the state-of-the-art results. © 2019 IEEE.","Gaze estimation; Learning-by-synthesis; Mask guided Style Transfer Network; Style Transfer","Gallium compounds; Complex directions; Different distributions; Gaze estimation; Material resources; Segmentation masks; Style and contents; Style Transfer; Transfer network; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85071426708
"Zhou X., Lin J., Jiang J., Chen S.","55743240400;57210574380;57211817476;24491760700;","Learning a 3D gaze estimator with improved itracker combined with bidirectional LSTM",2019,"Proceedings - IEEE International Conference on Multimedia and Expo","2019-July",,"8784770","850","855",,5,"10.1109/ICME.2019.00151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071031540&doi=10.1109%2fICME.2019.00151&partnerID=40&md5=c89a7ca6c2c8a609f81d446acad41cf7","College of Computer Science and Technology, Zhejiang University of Technology, China","Zhou, X., College of Computer Science and Technology, Zhejiang University of Technology, China; Lin, J., College of Computer Science and Technology, Zhejiang University of Technology, China; Jiang, J., College of Computer Science and Technology, Zhejiang University of Technology, China; Chen, S., College of Computer Science and Technology, Zhejiang University of Technology, China","Free-head 3D gaze estimation which outputs gaze vector in 3D space has wide application in human-computer interaction. In this paper, we propose a novel 3D gaze estimator by improving the Itracker and employing a many-to-one bidirectional LSTM (bi-LSTM). First, we improve the conventional Itracker by removing the face-grid and reducing one network branch via concatenating the two-eye region images to predict the subject's gaze of a single frame. Then, we employ the bi-LSTM to fit the temporal information between frames to estimate gaze vector for video sequence. Experimental results show that our improved Itracker obtains 11.6% significant improvement over the state-of-the-art methods on MPIIGaze dataset (single image frame) and has robust estimation accuracy for different image resolutions. Moreover, experimental results on EyeDiap dataset (video sequence) further bring 3% accuracy improvement by employing the bi-LSTM. © 2019 IEEE.","Gaze estimation; Itracker; LSTM; RNN","Human computer interaction; Image enhancement; Image resolution; Vector spaces; Video recording; Accuracy Improvement; Gaze estimation; Itracker; LSTM; Robust estimation; State-of-the-art methods; Temporal information; Video sequences; Long short-term memory",Conference Paper,"Final","",Scopus,2-s2.0-85071031540
"Zhao T., Yan Y., Peng J., Mi Z., Fu X.","57192707963;57204644440;57192708626;57205505628;7402204912;","Guiding intelligent surveillance system by learning-by-synthesis gaze estimation",2019,"Pattern Recognition Letters","125",,,"556","562",,4,"10.1016/j.patrec.2019.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068013180&doi=10.1016%2fj.patrec.2019.02.008&partnerID=40&md5=ac821aad17798926b88e9c1b1d6192a2","Information Science and Technology College, Dalian Maritime University, Dalian, China","Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian, China; Yan, Y., Information Science and Technology College, Dalian Maritime University, Dalian, China; Peng, J., Information Science and Technology College, Dalian Maritime University, Dalian, China; Mi, Z., Information Science and Technology College, Dalian Maritime University, Dalian, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, China","We describe a novel learning-by-synthesis method for estimating the gaze direction of an automated intelligent surveillance system. Recently, the progress of integrated learning has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. However, due to the different distributions between the real and synthetic images, the desired performance cannot be achieved from the synthetic image learning as compared to the real images. In order to solve this problem, the previous method was to improve the authenticity of the composite image by learning the model. However, this kind of method had the disadvantage that the distortion was not improved and the level of authenticity was unstable. In order to solve this problem, we propose a new structure to improve the composite image. By referring to the idea of style transformation, we can effectively reduce the distortion of the image and minimize the need for actual data annotation. We estimate that this can produce highly realistic images, which we have demonstrated through qualitative and user research. We quantitatively evaluate the generated images by training the gaze estimation model. We use the refined synthetic dataset to show significant improvements compared with using the raw synthetic dataset. © 2019 Elsevier B.V.",,"Authentication; Metadata; Security systems; Composite images; Different distributions; Integrated learning; Intelligent surveillance systems; Material resources; Realistic images; Synthesis method; Synthetic images; Image enhancement",Article,"Final","",Scopus,2-s2.0-85068013180
"Xia C., Han J., Qi F., Shi G.","56102195500;24450644400;48161721000;55536676300;","Predicting Human Saccadic Scanpaths Based on Iterative Representation Learning",2019,"IEEE Transactions on Image Processing","28","7","8637020","3502","3515",,10,"10.1109/TIP.2019.2897966","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061549642&doi=10.1109%2fTIP.2019.2897966&partnerID=40&md5=461b6fa20948ade9cc74a5c437b843f1","School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; School of Artificial Intelligence, Xidian University, Xi'an, 710071, China","Xia, C., School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Han, J., School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Qi, F., School of Artificial Intelligence, Xidian University, Xi'an, 710071, China; Shi, G., School of Artificial Intelligence, Xidian University, Xi'an, 710071, China","Visual attention is a dynamic process of scene exploration and information acquisition. However, existing research on attention modeling has concentrated on estimating static salient locations. In contrast, dynamic attributes presented by saccade have not been well explored in previous attention models. In this paper, we address the problem of saccadic scanpath prediction by introducing an iterative representation learning framework. Within the framework, saccade can be interpreted as an iterative process of predicting one fixation according to the current representation and updating the representation based on the gaze shift. In the predicting phase, we propose a Bayesian definition of saccade to combine the influence of perceptual residual and spatial location on the selection of fixations. In implementation, we compute the representation error of an autoencoder-based network to measure perceptual residuals of each area. Simultaneously, we integrate saccade amplitude and center-weighted mechanism to model the influence of spatial location. Based on estimating the influence of two parts, the final fixation is defined as the point with the largest posterior probability of gaze shift. In the updating phase, we update the representation pattern for the subsequent calculation by retraining the network with samples extracted around the current fixation. In the experiments, the proposed model can replicate the fundamental properties of psychophysics in visual search. In addition, it can achieve superior performance on several benchmark eye-tracking data sets. © 1992-2012 IEEE.","deep learning; representation learning; saccade; scanpath; Visual attention","Behavioral research; Benchmarking; Brain models; Data structures; Data visualization; Estimation; Eye movements; Eye tracking; Feature extraction; Flow visualization; Forecasting; Iterative methods; Location; Computational model; Predictive models; representation learning; Scan path; Visual Attention; Deep learning; adult; algorithm; attention; biological model; factual database; female; human; image processing; male; physiology; procedures; saccadic eye movement; young adult; Adult; Algorithms; Attention; Databases, Factual; Deep Learning; Female; Humans; Image Processing, Computer-Assisted; Male; Models, Neurological; Saccades; Young Adult",Article,"Final","",Scopus,2-s2.0-85061549642
"Goltz J., Grossberg M., Etemadpour R.","57210122607;57190595232;36165000200;","Exploring simple neural network architectures for eye movement classification",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a4","","",,,"10.1145/3314111.3319813","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069533165&doi=10.1145%2f3314111.3319813&partnerID=40&md5=932138a42630f9ff1181f7bb3bb53181","University of Applied Science Munich, Munich, Germany; City University of New York, New York, NY, United States","Goltz, J., University of Applied Science Munich, Munich, Germany; Grossberg, M., City University of New York, New York, NY, United States; Etemadpour, R., City University of New York, New York, NY, United States","Analysis of eye-gaze is a critical tool for studying human-computer interaction and visualization. Yet eye tracking systems only report eye-gaze on the scene by producing large volumes of coordinate time series data. To be able to use this data, we must first extract salient events such as eye fixations, saccades, and post-saccadic oscillations (PSO). Manually extracting these events is time-consuming, labor-intensive and subject to variability. In this paper, we present and evaluate simple and fast automatic solutions for eye-gaze analysis based on supervised learning. Similar to some recent studies, we developed different simple neural networks demonstrating that feature learning produces superior results in identifying events from sequences of gaze coordinates. We do not apply any ad-hoc post-processing, thus creating a fully automated end-to-end algorithms that perform as good as current state-of-the-art architectures. Once trained they are fast enough to be run in a near real time setting. © 2019 Association for Computing Machinery.","Deep learning; Event detection; Eye movement; Machine learning","Deep learning; Eye movements; Human computer interaction; Learning systems; Machine learning; Network architecture; Neural networks; Coordinate time series; Event detection; Eye movement classifications; Eye tracking systems; Feature learning; Fully automated; Post processing; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069533165
"Venuprasad P., Dobhal T., Paul A., Nguyen T.N.M., Gilman A., Cosman P., Chukoskie L.","57210106962;57210105601;57210123331;57210107998;16318668400;7003359562;6507740817;","Characterizing joint attention behavior during real world interactions using automated object and gaze detection",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a21","","",,4,"10.1145/3314111.3319843","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069528016&doi=10.1145%2f3314111.3319843&partnerID=40&md5=b774889d47583f5b94177941b50cbec1","University of California San Diego, San Diego, CA, United States; Minerva Schools at the Keck Graduate Institute, United States; Massey University, Auckland, New Zealand","Venuprasad, P., University of California San Diego, San Diego, CA, United States; Dobhal, T., University of California San Diego, San Diego, CA, United States; Paul, A., University of California San Diego, San Diego, CA, United States; Nguyen, T.N.M., Minerva Schools at the Keck Graduate Institute, United States; Gilman, A., Massey University, Auckland, New Zealand; Cosman, P., University of California San Diego, San Diego, CA, United States; Chukoskie, L., University of California San Diego, San Diego, CA, United States","Joint attention is an essential part of the development process of children, and impairments in joint attention are considered as one of the first symptoms of autism. In this paper, we develop a novel technique to characterize joint attention in real time, by studying the interaction of two human subjects with each other and with multiple objects present in the room. This is done by capturing the subjects’ gaze through eye-tracking glasses and detecting their looks on predefined indicator objects. A deep learning network is trained and deployed to detect the objects in the field of vision of the subject by processing the video feed of the world view camera mounted on the eye-tracking glasses. The looking patterns of the subjects are determined and a real-time audio response is provided when a joint attention is detected, i.e., when their looks coincide. Our findings suggest a trade-off between the accuracy measure (Look Positive Predictive Value) and the latency of joint look detection for various system parameters. For more accurate joint look detection, the system has higher latency, and for faster detection, the detection accuracy goes down. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Autism; Computer Vision; Deep Learning; Eye-Tracking; Gaze Behavior; Joint Attention; Object Detection","Cameras; Computer vision; Deep learning; Diseases; Economic and social effects; Glass; Object detection; Accuracy measures; Autism; Detection accuracy; Development process; Gaze behavior; Joint attention; Novel techniques; Positive predictive values; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069528016
"Dierkes K., Kassner M., Bulling A.","57202889988;56406193700;6505807414;","A fast approach to refraction-aware eye-model fitting and gaze prediction",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a23","","",,4,"10.1145/3314111.3319819","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069521443&doi=10.1145%2f3314111.3319819&partnerID=40&md5=2463fbba3947b6cf5acba4c35be2b371","Pupil Labs Research, Berlin, Germany","Dierkes, K., Pupil Labs Research, Berlin, Germany; Kassner, M., Pupil Labs Research, Berlin, Germany; Bulling, A., Pupil Labs Research, Berlin, Germany","By temporally integrating information about pupil contours extracted from eye images, model-based methods for glint-free gaze estimation can mitigate pupil detection noise. However, current approaches require time-consuming iterative solving of a nonlinear minimization problem to estimate key parameters, such as eyeball position. Based on the method presented by [Swirski and Dodgson 2013], we propose a novel approach to glint-free 3D eye-model fitting and gaze prediction using a single near-eye camera. By recasting model optimization as a least-squares intersection of lines, we make it amenable to a fast non-iterative solution. We further present a method for estimating deterministic refraction-correction functions from synthetic eye images and validate them on both synthetic and real eye images. We demonstrate the robustness of our method in the presence of pupil detection noise and show the benefit of temporal integration of pupil contour information on eyeball position and gaze estimation accuracy. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D eye model; Contour-based; Eye tracking; Glint-free; Pupil detection; Refraction","3D modeling; Iterative methods; Refraction; 3D eye models; Contour-based; Glint-free; Integrating information; Non-iterative solutions; Nonlinear minimization; Pupil detection; Temporal integration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069521443
"Sasaki M., Nagamatsu T., Takemura K.","57205675368;23398000100;8575290600;","Screen corner detection using polarization camera for cross-ratio based gaze estimation",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a24","","",,3,"10.1145/3314111.3319814","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069519975&doi=10.1145%2f3314111.3319814&partnerID=40&md5=7e39f3ba9e8b6dbabe6118605249d114","Tokai University, Hiratsuka, Kanagawa, Japan; Kobe University, Kobe, Hyogo, Japan","Sasaki, M., Tokai University, Hiratsuka, Kanagawa, Japan; Nagamatsu, T., Kobe University, Kobe, Hyogo, Japan; Takemura, K., Kobe University, Kobe, Hyogo, Japan","Eye tracking, which measures line of sight, is expected to advance as an intuitive and rapid input method for user interfaces, and a cross-ratio based method that calculates the point-of-gaze using homography matrices has attracted attention because it does not require hardware calibration to determine the geometric relationship between an eye camera and a screen. However, this method requires near-infrared (NIR) light-emitting diodes (LEDs) attached to the display in order to detect screen corners. Consequently, LEDs must be installed around the display to estimate the point-of-gaze. Without these requirements, cross-ratio based gaze estimation can be distributed smoothly. Therefore, we propose the use of a polarization camera for detecting the screen area reflected on a corneal surface. The reflection area of display light is easily detected by the polarized image because the light radiated from the display is polarized linearly by the internal polarization filter. With the proposed method, the screen corners can be determined without using NIR LEDs, and the point-of-gaze can be estimated using the detected corners on the corneal surface. We investigated the accuracy of the estimated point-of-gaze based on a cross-ratio method under various illumination and display conditions. Cross-ratio based gaze estimation is expected to be utilized widely in commercial products because the proposed method does not require infrared light sources at display corners. © 2019 Association for Computing Machinery.","Cross-ratio method; Eye gaze estimation; Polarized image","Cameras; Edge detection; Infrared devices; Light emitting diodes; Polarization; User interfaces; Commercial products; Cross-ratios; Eye-gaze; Geometric relationships; Homography matrices; Infrared light sources; Polarization filters; Polarized image; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069519975
"Wang H., Shi B.E.","56809110800;7402547071;","Gaze awareness improves collaboration efficiency in a collaborative assembly task",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a88","","",,1,"10.1145/3317959.3321492","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069518064&doi=10.1145%2f3317959.3321492&partnerID=40&md5=612ee57489e7bff637e1f61a9753fe64","Hong Kong University of Science and Technology, Hong Kong, Hong Kong","Wang, H., Hong Kong University of Science and Technology, Hong Kong, Hong Kong; Shi, B.E., Hong Kong University of Science and Technology, Hong Kong, Hong Kong","In building human robot interaction systems, it would be helpful to understand how humans collaborate, and in particular, how humans use others’ gaze behavior to estimate their intent. Here we studied the use of gaze in a collaborative assembly task, where a human user assembled an object with the assistance of a human helper. We found that the being aware of the partner’s gaze significantly improved collaboration efficiency. Task completion times were much shorter when gaze communication was available, than when it was blocked. In addition, we found that the user’s gaze was more likely to lie on the object of interest in the gaze-aware case than the gaze-blocked case. In the context of human-robot collaboration systems, our results suggest that gaze data in the period surrounding verbal requests will be more informative and can be used to predict the target object. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D gaze estimation; Gaze awareness; Gaze tracking; Human robot collaboration","Distributed computer systems; Efficiency; Human robot interaction; Collaborative assembly; Completion time; Gaze awareness; Gaze behavior; Gaze communications; Gaze estimation; Gaze tracking; Human-robot collaboration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069518064
"Mardanbegi D., Clarke C., Gellersen H.","42761947400;56559308000;6701531333;","Monocular gaze depth estimation using the vestibulo-ocular reflex",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a20","","",,3,"10.1145/3314111.3319822","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069500554&doi=10.1145%2f3314111.3319822&partnerID=40&md5=6805937d9fb8e3555f59b748b254276e","Lancaster University, United Kingdom","Mardanbegi, D., Lancaster University, United Kingdom; Clarke, C., Lancaster University, United Kingdom; Gellersen, H., Lancaster University, United Kingdom","Gaze depth estimation presents a challenge for eye tracking in 3D. This work investigates a novel approach to the problem based on eye movement mediated by the vestibulo-ocular reflex (VOR). VOR stabilises gaze on a target during head movement, with eye movement in the opposite direction, and the VOR gain increases the closer the fixated target is to the viewer. We present a theoretical analysis of the relationship between VOR gain and depth which we investigate with empirical data collected in a user study (N=10). We show that VOR gain can be captured using pupil centres, and propose and evaluate a practical method for gaze depth estimation based on a generic function of VOR gain and two-point depth calibration. The results show that VOR gain is comparable with vergence in capturing depth while only requiring one eye, and provide insight into open challenges in harnessing VOR gain as a robust measure. © 2019 Association for Computing Machinery.","3D gaze estimation; Eye movement; Eye tracking; Fixation depth; Gaze depth estimation; VOR","Eye movements; Depth calibration; Depth Estimation; Gaze estimation; Generic functions; Head movements; Practical method; Robust measures; Vestibulo-ocular reflex; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069500554
[无可用作者姓名],[无可用的作者 ID],"Proceedings - ETRA 2019: 2019 ACM Symposium on Eye Tracking Research and Applications",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",607,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069485780&partnerID=40&md5=06f5bf72746898d549351fa4da8314d9",,"","The proceedings contain 93 papers. The topics discussed include: deep learning investigation for chess player attention prediction using eye-tracking and game data; semantic gaze labeling for human-robot shared manipulation; EyeFlow: pursuit interactions using an unmodified camera; exploring simple neural network architectures for eye movement classification; analyzing gaze transition behavior using Bayesian mixed effects Markov models; gaze behavior on interacted objects during hand interaction in virtual reality for eye tracking calibration; time- and space-efficient eye tracker calibration; task-embedded online eye-tracker calibration for improving robustness to head motion; and reducing calibration drift in mobile eye trackers by exploiting mobile phone usage.",,,Conference Review,"Final","",Scopus,2-s2.0-85069485780
"Katrychuk D., Griffith H.K., Komogortsev O.V.","57210105027;57189386560;6506328653;","Power-efficient and shift-robust eye-tracking sensor for portable VR headsets",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a19","","",,8,"10.1145/3314111.3319821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069483095&doi=10.1145%2f3314111.3319821&partnerID=40&md5=457b404170995b342473d55c75de6d33","Department of Computer Science, Texas State University, San Marcos, TX, United States","Katrychuk, D., Department of Computer Science, Texas State University, San Marcos, TX, United States; Griffith, H.K., Department of Computer Science, Texas State University, San Marcos, TX, United States; Komogortsev, O.V., Department of Computer Science, Texas State University, San Marcos, TX, United States","Photosensor oculography (PSOG) is a promising solution for reducing the computational requirements of eye tracking sensors in wireless virtual and augmented reality platforms. This paper proposes a novel machine learning-based solution for addressing the known performance degradation of PSOG devices in the presence of sensor shifts. Namely, we introduce a convolutional neural network model capable of providing shift-robust end-to-end gaze estimates from the PSOG array output. Moreover, we propose a transfer-learning strategy for reducing model training time. Using a simulated workflow with improved realism, we show that the proposed convolutional model offers improved accuracy over a previously considered multilayer perceptron approach. In addition, we demonstrate that the transfer of initialization weights from pre-trained models can substantially reduce training time for new users. In the end, we provide the discussion regarding the design trade-offs between accuracy, training time, and power consumption among the considered models. © 2019 Association for Computing Machinery.","Eye-tracking; Machine learning; ML; Photo-sensor oculography; PSOG; Virtual reality; VR","Augmented reality; Convolution; Economic and social effects; Learning systems; Machine learning; Neural networks; Optical sensors; Virtual reality; Computational requirements; Convolutional model; Convolutional neural network; Eye-tracking sensors; Performance degradation; Photo-sensors; PSOG; Virtual and augmented reality; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069483095
"Siegfried R., Yu Y., Odobez J.-M.","57195685304;57188644020;57203103085;","A deep learning approach for robust head pose independent eye movements recognition from videos",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a31","","",,1,"10.1145/3314111.3319844","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069475315&doi=10.1145%2f3314111.3319844&partnerID=40&md5=9518db12f9618cf737194d30b3d78a0b","Idiap Research Institute Martigny, Switzerland EPFL, Lausanne, Switzerland","Siegfried, R., Idiap Research Institute Martigny, Switzerland EPFL, Lausanne, Switzerland; Yu, Y., Idiap Research Institute Martigny, Switzerland EPFL, Lausanne, Switzerland; Odobez, J.-M., Idiap Research Institute Martigny, Switzerland EPFL, Lausanne, Switzerland","Recognizing eye movements is important for gaze behavior understanding like in human communication analysis (human-human or robot interactions) or for diagnosis (medical, reading impairments). In this paper, we address this task using remote RGB-D sensors to analyze people behaving in natural conditions. This is very challenging given that such sensors have a normal sampling rate of 30 Hz and provide low-resolution eye images (typically 36x60 pixels), and natural scenarios introduce many variabilities in illumination, shadows, head pose, and dynamics. Hence gaze signals one can extract in these conditions have lower precision compared to dedicated IR eye trackers, rendering previous methods less appropriate for the task. To tackle these challenges, we propose a deep learning method that directly processes the eye image video streams to classify them into fixation, saccade, and blink classes, and allows to distinguish irrelevant noise (illumination, low-resolution artifact, inaccurate eye alignment, difficult eye shapes) from true eye motion signals. Experiments on natural 4-party interactions demonstrate the benefit of our approach compared to previous methods, including deep learning models applied to gaze outputs. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Blink; Convolutional neural network; Eye movements; Remote sensors; Saccade; Video processing","Deep learning; Diagnosis; Eye tracking; Human robot interaction; Motion estimation; Neural networks; Remote sensing; Video signal processing; Blink; Convolutional neural network; Human communications; Learning approach; Natural conditions; Remote sensors; Robot interactions; Video processing; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85069475315
"Santini T., Niehorster D.C., Kasneci E.","54881866000;36466775900;56059892600;","Get a grip: Slippage-robust and glint-free gaze estimation for real-time pervasive head-mounted eye tracking",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a17","","",,13,"10.1145/3314111.3319835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069454357&doi=10.1145%2f3314111.3319835&partnerID=40&md5=d78d87bb5de84d81ed750f9aa23aafbf","University of Tübingen, Perception Engineering, Tübingen, Germany; Lund University Humanities Lab, Department of Psychology, Lund, Sweden","Santini, T., University of Tübingen, Perception Engineering, Tübingen, Germany; Niehorster, D.C., Lund University Humanities Lab, Department of Psychology, Lund, Sweden; Kasneci, E., University of Tübingen, Perception Engineering, Tübingen, Germany","A key assumption conventionally made by flexible head-mounted eye-tracking systems is often invalid: The eye center does not remain stationary w.r.t. the eye camera due to slippage. For instance, eye-tracker slippage might happen due to head acceleration or explicit adjustments by the user. As a result, gaze estimation accuracy can be significantly reduced. In this work, we propose Grip, a novel gaze estimation method capable of instantaneously compensating for eye-tracker slippage without additional hardware requirements such as glints or stereo eye camera setups. Grip was evaluated using previously collected data from a large scale unconstrained pervasive eye-tracking study. Our results indicate significant slippage compensation potential, decreasing average participant median angular offset by more than 43% w.r.t. a non-slippage-robust gaze estimation method. A reference implementation of Grip was integrated into EyeRecToo, an open-source hardware-agnostic eye-tracking software, thus making it readily accessible for multiple eye trackers (Available at: www.ti.uni-tuebingen.de/perception). © 2019 Association for Computing Machinery.","Calibration; Drift; Embedded; Eye tracking; Gaze estimation; Open source; Pervasive; Pupil tracking; Real-time; Slippage","Calibration; Cameras; Open source software; Open systems; Drift; Embedded; Gaze estimation; Open sources; Pervasive; Pupil tracking; Real time; Slippage; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069454357
"Borsato F.H., Morimoto C.H.","7801411327;7102275798;","Towards a low cost and high speed mobile eye tracker",2019,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a16","","",,1,"10.1145/3314111.3319841","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069432666&doi=10.1145%2f3314111.3319841&partnerID=40&md5=455b24cfaf39cc4e5033eae4a45939b5","Federal University of Technology, Paraná, Brazil; University of São Paulo, São Paulo, Brazil","Borsato, F.H., Federal University of Technology, Paraná, Brazil; Morimoto, C.H., University of São Paulo, São Paulo, Brazil","Despite recent developments in eye tracking technology, mobile eye trackers (ET) are still expensive devices limited to a few hundred samples per second. High speed ETs (closer to 1 KHz) can provide improved flexibility for data filtering and more reliable event detection. To address these challenges, we present the Stroboscopic Catadioptric Eye Tracking (SCET) system, a novel approach for mobile ET based on rolling shutter cameras and stroboscopic structured infrared lighting. SCET proposes a geometric model where the cornea acts as a spherical mirror in a catadioptric system, changing the projection as it moves. Calibration methods for the geometry of the system and for the gaze estimation are presented. Instead of tracking common eye features, such as the pupil center, we track multiple glints on the cornea. By carefully adjusting the camera exposure and the lighting period, we show how one image frame can be divided into several bands to increase the temporal resolution of the gaze estimates. We assess the model in a simulated environment and also describe a prototype implementation that demonstrates the feasibility of SCET, which we envision as a step further in the direction of a mobile, robust, affordable, and high-speed eye tracker. © 2019 Association for Computing Machinery.","Catadioptric system; Mobile eye-tracking; Rolling shutter; Stroboscopic lighting","Cameras; Costs; Imaging systems; Lighting; Telescopes; Catadioptric system; Eye tracking technologies; Mobile eye-tracking; Prototype implementations; Rolling shutter cameras; Rolling shutters; Simulated environment; Temporal resolution; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85069432666
"Bao H., Fang W., Guo B., Wang J.","57194698108;7202236854;7403276500;57204683782;","Real-time wide-view eye tracking based on resolving the spatial depth",2019,"Multimedia Tools and Applications","78","11",,"14633","14655",,2,"10.1007/s11042-018-6754-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056612415&doi=10.1007%2fs11042-018-6754-2&partnerID=40&md5=9bb15f1a10da5a12e3df0b17437a4820","State Key Lab Rail Traff Control & Safety, School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, No. 3 Shangyuancun Haidian District, Beijing, 100044, China","Bao, H., State Key Lab Rail Traff Control & Safety, School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, No. 3 Shangyuancun Haidian District, Beijing, 100044, China; Fang, W., State Key Lab Rail Traff Control & Safety, School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, No. 3 Shangyuancun Haidian District, Beijing, 100044, China; Guo, B., State Key Lab Rail Traff Control & Safety, School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, No. 3 Shangyuancun Haidian District, Beijing, 100044, China; Wang, J., State Key Lab Rail Traff Control & Safety, School of Mechanical, Electronic and Control Engineering, Beijing Jiaotong University, No. 3 Shangyuancun Haidian District, Beijing, 100044, China","Current eye-tracking systems include many types that have been applied in numerous applications; however, they are unsuitable for industrial environments such as aircraft cockpits and train cabs. While wearable eye-tracking glasses have a wide field of view, they provide only a relative point of view in a captured video image; thus, they cannot function as a real-time interface to digital monitors. Desktop eye-tracking applications can interact with a single screen in real-time using pre-attached fixed cameras or infrared sensors; however, these applications allow only a narrow field of view. In this study, we developed a novel eye-tracking solution that integrates both the requirement for interaction and a wide field of view. The eye-tracking glasses gather eye movement data while a motion capture device provides data concerning the position and orientation of the head. A spatial depth-resolving algorithm is proposed to estimate the distance from the eyes to the digital screen, making it possible to locate the screens. Our proposed method is a generalized solution for estimating gaze points on wide screens; it is not dependent on specific devices. We tested the method in a virtual environment using unity3d and reached three conclusions: the algorithm theoretically has good accuracy and stability; it cannot be simplified; and when applied in a real environment it should have a satisfying and acceptable usability. Subsequently, we performed experiments in a real environment that validated the theory. Further applications: This type of unique wearable equipment can function as a real-time machine input interface to enhance the machine’s perception of the user’s situational awareness and improve the machine’s dynamic service capabilities. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Eye tracking; Gaze estimation; Resolving spatial depth; Wearable Computering","Aircraft detection; Cockpits (aircraft); Eye movements; Glass; Infrared detectors; Virtual reality; Wearable technology; Gaze estimation; Industrial environments; Motion capture devices; Position and orientations; Real time interfaces; Resolving spatial depth; Situational awareness; Wearable Computering; Eye tracking",Article,"Final","",Scopus,2-s2.0-85056612415
"Xu J., Park S.H., Zhang X.","55801927200;7501838057;35232030000;","A bio-inspired motion sensitive model and its application to estimating human gaze positions under classified driving conditions",2019,"Neurocomputing","345",,,"23","35",,7,"10.1016/j.neucom.2018.09.093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061262456&doi=10.1016%2fj.neucom.2018.09.093&partnerID=40&md5=3eb757fcbfd8703627c394f1949f4cb7","School of Computing, Newcastle University, Newcastle-upon-Tyne, NE4 5TG, United Kingdom; Department of Electronic Engineering, Hallym University200-702, South Korea; Department of Computer Science, Wenzhou University, Wenzhou, 325035, China","Xu, J., School of Computing, Newcastle University, Newcastle-upon-Tyne, NE4 5TG, United Kingdom; Park, S.H., Department of Electronic Engineering, Hallym University200-702, South Korea; Zhang, X., Department of Computer Science, Wenzhou University, Wenzhou, 325035, China","Human visual attention performs the spatio-temporal regularity, an inherent regularity in dynamic vision, for the natural visual tasks. However, recent computational visual attention models still have deficiency to reflect the spatio-temporal regularity thoroughly. Motivated by this, we propose a bio-inspired motion sensitive model to estimate human gaze positions in driving. The proposed model has four key advantages. First, inspired by two types of motion sensitive neurons, this model can perceive the motion cues from different directions spatially and temporally. Second, compared to conventional deep learning based models, this model does not rely on expensive training samples with gaze annotations. Third, the proposed model is based upon the visual signal processing without constructing a complex deep neural network architecture, which enables this model to be implemented with low-cost hardware. Fourth, inspired by the visual pathway in drosophila motion vision, the visual signal processing on directional and depth motion sensitive map largely enhances this model's competence in a similar way of human gaze positions in driving. To test this proposed model, we collect a driving scene dataset from the perspective of egocentric vision that aims to systematically evaluate the performance of spatio-temporal visual attentional models. The video clips in the dataset are categorized into ten popular driving conditions. The proposed model is evaluated by comparing with the human baseline of the gaze positions. Experimental results demonstrate that the proposed model can effectively estimate the human gaze positions in driving and consistently outperforms traditional visual attention models as well as the deep learning based model. © 2019","Biological motion vision; Human gaze positions in driving; Sensitivity on motion cues; Spatio-temporal regularity","Behavioral research; Biomimetics; Deep learning; Deep neural networks; Network architecture; Signal processing; Statistical tests; Vision; Biological motion; Human gaze positions in driving; Human visual attention; Learning Based Models; Motion cues; Spatio temporal; Visual attention model; Visual signal processing; Learning systems; animal cell; Article; association; bioinspired motion sensitive model; biological model; car driving; deep learning; Drosophila; eye fixation; frequency discrimination; gaze; human; human cell; interneuron; mathematical analysis; mathematical model; motoneuron; nerve cell network; nonhuman; photoreceptor cell; priority journal; retina fovea; saccadic eye movement; signal processing; task performance; visual attention; visual discrimination; visual information; visual system",Article,"Final","",Scopus,2-s2.0-85061262456
"Zhang W., Yan J., Liu Z.-Y., Zeng Z.","56119460300;36026971200;57193088948;35316687300;","Special issue on deep learning for intelligent sensing, decision-making and control",2019,"Neurocomputing","345",,,"1","2",,,"10.1016/j.neucom.2019.01.082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061141135&doi=10.1016%2fj.neucom.2019.01.082&partnerID=40&md5=e57ef255743a94bc31ec0b5fdb37e599","Shandong University, China; Shanghai Jiao Tong University, China; Chinese Academy of Sciences, China; Huazhong University of Science and Technology, China","Zhang, W., Shandong University, China; Yan, J., Shanghai Jiao Tong University, China; Liu, Z.-Y., Chinese Academy of Sciences, China; Zeng, Z., Huazhong University of Science and Technology, China",[无可用摘要],,"avoidance behavior; computer model; decision making; deep learning; Editorial; gaze; human; image processing; intelligence; intelligent sensing; priority journal; recognition; reinforcement; robotics; task performance; visual attention",Editorial,"Final","",Scopus,2-s2.0-85061141135
"Yu Y., Liu G., Odobez J.-M.","57188644020;56420692700;57203103085;","Improving few-shot user-specific gaze adaptation via gaze redirection synthesis",2019,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June",,"8953732","11929","11938",,26,"10.1109/CVPR.2019.01221","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078808349&doi=10.1109%2fCVPR.2019.01221&partnerID=40&md5=c63dd3b3386115bad7b4a933e880309a","Idiap Research Institute, Martigny, CH-1920, Switzerland; EPFL, Lausanne, CH-1015, Switzerland","Yu, Y., Idiap Research Institute, Martigny, CH-1920, Switzerland, EPFL, Lausanne, CH-1015, Switzerland; Liu, G., Idiap Research Institute, Martigny, CH-1920, Switzerland, EPFL, Lausanne, CH-1015, Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, CH-1920, Switzerland, EPFL, Lausanne, CH-1015, Switzerland","As an indicator of human attention gaze is a subtle behavioral cue which can be exploited in many applications. However, inferring 3D gaze direction is challenging even for deep neural networks given the lack of large amount of data (groundtruthing gaze is expensive and existing datasets use different setups) and the inherent presence of gaze biases due to person-specific difference. In this work, we address the problem of person-specific gaze model adaptation from only a few reference training samples. The main and novel idea is to improve gaze adaptation by generating additional training samples through the synthesis of gaze-redirected eye images from existing reference samples. In doing so, our contributions are threefold:(i) we design our gaze redirection framework from synthetic data, allowing us to benefit from aligned training sample pairs to predict accurate inverse mapping fields; (ii) we proposed a self-supervised approach for domain adaptation; (iii) we exploit the gaze redirection to improve the performance of person-specific gaze estimation. Extensive experiments on two public datasets demonstrate the validity of our gaze retargeting and gaze estimation framework. © 2019 IEEE.","And Body Pose; Face; Gesture; Image and Video Synthesis; Vision Applications and Systems","Computer vision; Deep neural networks; Large dataset; Sampling; Body pose; Face; Gesture; Video synthesis; Vision applications; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85078808349
"Cordel M.O., Fan S., Shen Z., Kankanhalli M.S.","54981663000;24512310200;57195548848;7003629165;","Emotion-aware human attention prediction",2019,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June",,"8953511","4021","4030",,11,"10.1109/CVPR.2019.00415","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078799752&doi=10.1109%2fCVPR.2019.00415&partnerID=40&md5=ebb4836078aa880325cfde4daf363400","De la Salle University, Philippines; National University of Singapore, Singapore","Cordel, M.O., De la Salle University, Philippines; Fan, S., National University of Singapore, Singapore; Shen, Z., National University of Singapore, Singapore; Kankanhalli, M.S., National University of Singapore, Singapore","Despite the recent success in face recognition and object classification, in the field of human gaze prediction, computer models are still struggling to accurately mimic human attention. One main reason is that visual attention is a complex human behavior influenced by multiple factors, ranging from low-level features (e.g., color, contrast) to high-level human perception (e.g., objects interactions, object sentiment), making it difficult to model computationally. In this work, we investigate the relation between object sentiment and human attention. We first introduce a new evaluation metric (AttI) for measuring human attention that focuses on human fixation consensus. A series of empirical data analyses with AttI indicate that emotion-evoking objects receive attention favor, especially when they co-occur with emotionally-neutral objects, and this favor varies with different image complexity. Based on the empirical analyses, we design a deep neural network for human attention prediction which allows the attention bias on emotion-evoking objects to be encoded in its feature space. Experiments on two benchmark datasets demonstrate its superior performance, especially on metrics that evaluate relative importance of salient regions. This research provides the clearest picture to date on how object sentiments influence human attention, and it makes one of the first attempts to model this phenomenon computationally. © 2019 IEEE.","Deep Learning; Others; Representation Learning; Scene Analysis and Understanding; Vision Applications and Systems","Benchmarking; Complex networks; Computer vision; Deep learning; Deep neural networks; Face recognition; Forecasting; Predictive analytics; Benchmark datasets; Empirical analysis; Evaluation metrics; Object classification; Others; Representation Learning; Scene analysis; Vision applications; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85078799752
"Marin-Jimenez M.J., Kalogeiton V., Medina-Suarez P., Zisserman A.","14054463100;55583071400;57214469324;7006619672;","Laeo-net: Revisiting people looking at each other in videos",2019,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June",,"8954303","3472","3480",,7,"10.1109/CVPR.2019.00359","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078784810&doi=10.1109%2fCVPR.2019.00359&partnerID=40&md5=de9bc064793a80f0e93d265e1b294b41","University of Córdoba, Spain; University of Oxford, United Kingdom","Marin-Jimenez, M.J., University of Córdoba, Spain; Kalogeiton, V., University of Oxford, United Kingdom; Medina-Suarez, P., University of Córdoba, Spain; Zisserman, A., University of Oxford, United Kingdom","Capturing the 'mutual gaze' of people is essential for understanding and interpreting the social interactions between them. To this end, this paper addresses the problem of detecting people Looking At Each Other (LAEO) in video sequences. For this purpose, we propose LAEO-Net, a new deep CNN for determining LAEO in videos. In contrast to previous works, LAEO-Net takes spatio-temporal tracks as input and reasons about the whole track. It consists of three branches, one for each character's tracked head and one for their relative position. Moreover, we introduce two new LAEO datasets: UCO-LAEO and AVA-LAEO. A thorough experimental evaluation demonstrates the ability of LAEO-Net to successfully determine if two people are LAEO and the temporal window where it happens. Our model achieves state-of-the-art results on the existing TVHID-LAEO video dataset, significantly outperforming previous approaches. © 2019 IEEE.","Action Recognition; And Body Pose; Deep Learning; Face; Gesture; Video Analytics; Vision Applications and Systems","Deep learning; Action recognition; Body pose; Face; Gesture; Video analytics; Vision applications; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85078784810
"Wang K., Su H., Ji Q.","56637259500;57202802558;18935108400;","Neuro-inspired eye tracking with eye movement dynamics",2019,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June",,"8953735","9823","9832",,9,"10.1109/CVPR.2019.01006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078783804&doi=10.1109%2fCVPR.2019.01006&partnerID=40&md5=47c421f9813c76ad2fddae6a4d6bfbaa","RPI, United Kingdom; IBM, United States","Wang, K., RPI, United Kingdom; Su, H., RPI, United Kingdom, IBM, United States; Ji, Q., RPI, United Kingdom","Generalizing eye tracking to new subjects/environments remains challenging for existing appearance-based methods. To address this issue, we propose to leverage on eye movement dynamics inspired by neurological studies. Studies show that there exist several common eye movement types, independent of viewing contents and subjects, such as fixation, saccade, and smooth pursuits. Incorporating generic eye movement dynamics can therefore improve the generalization capabilities. In particular, we propose a novel Dynamic Gaze Transition Network (DGTN) to capture the underlying eye movement dynamics and serve as the topdown gaze prior. Combined with the bottom-up gaze measurements from the deep convolutional neural network, our method achieves better performance for both within-dataset and cross-dataset evaluations compared to state-of-the-art. In addition, a new DynamicGaze dataset is also constructed to study eye movement dynamics and eye gaze estimation. © 2019 IEEE.","And Body Pose; Face; Gesture","Computer vision; Convolutional neural networks; Deep neural networks; Dynamics; Eye tracking; Appearance-based methods; Body pose; Cross-dataset evaluation; Face; Generalization capability; Gesture; Smooth pursuit; State of the art; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85078783804
"Wang K., Zhao R., Su H., Ji Q.","56637259500;56461916600;57202802558;18935108400;","Generalizing eye tracking with bayesian adversarial learning",2019,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June",,"8953976","11899","11908",,12,"10.1109/CVPR.2019.01218","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078714537&doi=10.1109%2fCVPR.2019.01218&partnerID=40&md5=16970fbcb634c6dc04b201329b0aa55d","RPI; IBM","Wang, K., RPI; Zhao, R., RPI; Su, H., RPI, IBM; Ji, Q., RPI","Existing appearance-based gaze estimation approaches with CNN have poor generalization performance. By systematically studying this issue, we identify three major factors: 1) appearance variations; 2) head pose variations and 3) over-fitting issue with point estimation. To improve the generalization performance, we propose to incorporate adversarial learning and Bayesian inference into a unified framework. In particular, we first add an adversarial component into traditional CNN-based gaze estimator so that we can learn features that are gaze-responsive but can generalize to appearance and pose variations. Next, we extend the point-estimation based deterministic model to a Bayesian framework so that gaze estimation can be performed using all parameters instead of only one set of parameters. Besides improved performance on several benchmark datasets, the proposed method also enables online adaptation of the model to new subjects/environments, demonstrating the potential usage for practical real-time eye tracking applications. © 2019 IEEE.","And Body Pose; Face; Gesture","Bayesian networks; Benchmarking; Computer vision; Inference engines; Adversarial learning; Bayesian frameworks; Body pose; Deterministic modeling; Face; Generalization performance; Gesture; Real-time eye tracking; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85078714537
"Xiong Y., Kim H.J., Singh V.","57201315887;56336378000;57207179029;","Mixed effects neural networks (menets) with applications to gaze estimation",2019,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June",,"8954429","7735","7744",,15,"10.1109/CVPR.2019.00793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077366629&doi=10.1109%2fCVPR.2019.00793&partnerID=40&md5=cb6e16d1357d6e8f6105fc8cef0431f1","University of Wisconsin-Madison, United States; Korea University, South Korea","Xiong, Y., University of Wisconsin-Madison, United States; Kim, H.J., Korea University, South Korea; Singh, V., University of Wisconsin-Madison, United States","There is much interest in computer vision to utilize commodity hardware for gaze estimation. A number of papers have shown that algorithms based on deep convolutional architectures are approaching accuracies where streaming data from mass-market devices can offer good gaze tracking performance, although a gap still remains between what is possible and the performance users will expect in real deployments. We observe that one obvious avenue for improvement relates to a gap between some basic technical assumptions behind most existing approaches and the statistical properties of the data used for training. Specifically, most training datasets involve tens of users with a few hundreds (or more) repeated acquisitions per user. The non i.i.d. nature of this data suggests better estimation may be possible if the model explicitly made use of such 'repeated measurements' from each user as is commonly done in classical statistical analysis using so-called mixed effects models. The goal of this paper is to adapt these 'mixed effects' ideas from statistics within a deep neural network architecture for gaze estimation, based on eye images. Such a formulation seeks to specifically utilize information regarding the hierarchical structure of the training data-each node in the hierarchy is a user who provides tens or hundreds of repeated samples. This modification yields an architecture that offers state of the art performance on various publicly available datasets improving results by 10-20%. © 2019 IEEE.","And Body Pose; Face; Gesture; Motion and Tracking","Computer hardware; Computer vision; Deep neural networks; Eye tracking; Motion tracking; Network architecture; Body pose; Face; Gesture; Hierarchical structures; Motion and tracking; Repeated measurements; State-of-the-art performance; Statistical properties; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85077366629
"Siddharth, Trivedi M.M.","55234213700;7103153314;","Attention Monitoring and Hazard Assessment with Bio-Sensing and Vision: Empirical Analysis Utilizing CNNs on the KITTI Dataset",2019,"IEEE Intelligent Vehicles Symposium, Proceedings","2019-June",,"8813874","1673","1678",,2,"10.1109/IVS.2019.8813874","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072286643&doi=10.1109%2fIVS.2019.8813874&partnerID=40&md5=7caed011cd016e93584426bc74d66dbb","Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, San Diego, CA  92093, United States","Siddharth, Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, San Diego, CA  92093, United States; Trivedi, M.M., Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, San Diego, CA  92093, United States","Assessing the driver's attention and detecting various hazardous and non-hazardous events during a drive are critical for driver's safety. Attention monitoring in driving scenarios has mostly been carried out using vision (camera-based) modality by tracking the driver's gaze and facial expressions. It is only recently that bio-sensing modalities such as Electroencephalogram (EEG) are being explored. But, there is another open problem which has not been explored sufficiently yet in this paradigm. This is the detection of specific events, hazardous and non-hazardous, during driving that affects the driver's mental and physiological states. The other challenge in evaluating multi-modal sensory applications is the absence of very large scale EEG data because of the various limitations of using EEG in the real world. In this paper, we use both of the above sensor modalities and compare them against the two tasks of assessing the driver's attention and detecting hazardous vs. non-hazardous driving events. We collect user data on twelve subjects and show how in the absence of very large-scale datasets, we can still use pre-trained deep learning convolution networks to extract meaningful features from both of the above modalities. We used the publicly available KITTI dataset for evaluating our platform and to compare it with previous studies. Finally, we show that the results presented in this paper surpass the previous benchmark set up in the above driver awareness-related applications. © 2019 IEEE.",,"Benchmarking; Biohazards; Deep learning; Digital storage; Electroencephalography; Intelligent vehicle highway systems; Large dataset; Driver's safety; Electro-encephalogram (EEG); Empirical analysis; Facial Expressions; Hazard Assessment; Hazardous events; Large-scale datasets; Physiological state; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85072286643
"Borsato F.H., Diaz-Tula A., Morimoto C.H.","7801411327;55848005200;7102275798;","xSDL: stroboscopic differential lighting eye tracker with extended temporal support",2019,"Machine Vision and Applications","30","4",,"689","703",,2,"10.1007/s00138-019-01022-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064553092&doi=10.1007%2fs00138-019-01022-y&partnerID=40&md5=c1263cfd392a3667851014fa6d5f3c8e","Universidade Tecnológica Federal do Paraná, Via Rosalina Maria dos Santos 1233, Campo Mourão, 87301-899, Brazil; Universidade de São Paulo, Rua do Matão 1010, São Paulo, 05508-090, Brazil","Borsato, F.H., Universidade Tecnológica Federal do Paraná, Via Rosalina Maria dos Santos 1233, Campo Mourão, 87301-899, Brazil; Diaz-Tula, A., Universidade de São Paulo, Rua do Matão 1010, São Paulo, 05508-090, Brazil; Morimoto, C.H., Universidade de São Paulo, Rua do Matão 1010, São Paulo, 05508-090, Brazil","Eye tracking (ET) for gaze interaction in wearable computing imposes harder constraints on computational efficiency and illumination conditions than remote ET. In this paper we present xSDL, an extended temporal support computer vision algorithm for accurate, robust, and efficient pupil detection and gaze estimation. The robustness and efficiency of xSDL partly come from the use of stroboscopic differential lighting (SDL), an extension of the differential lighting pupil detection technique developed in the 90’s. Due to the erratic behavior of eye movements, traditional computer vision tracking techniques (such as Kalman filters) do not perform well, so most ET techniques simply detect some eye feature (such as the pupil center) at every frame. Extended temporal support uses keyframes selected during eye fixations and a simple translation model of the pupil to further improve the computational performance of SDL. A prototype composed of two independent acquisition systems was developed to evaluate the performance of xSDL and other four state-of-the-art ET techniques under similar conditions. Our results show that xSDL outperforms those four algorithms, both in speed (close to 2000 Hz using 240 line frames) and accuracy. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Extended temporal support; Eye tracking; Stroboscopic differential lighting; xSDL","Computational efficiency; Computer vision; Efficiency; Eye movements; Lighting; Acquisition systems; Computational performance; Computer vision algorithms; Illumination conditions; Temporal support; Traditional computers; Wearable computing; xSDL; Eye tracking",Article,"Final","",Scopus,2-s2.0-85064553092
"Kato T., Jo K., Shibasato K., Hakata T.","55483065800;57211944106;8295048700;57211948380;","Gaze region estimation algorithm without calibration using convolutional neural network",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"3325367","","",,1,"10.1145/3325291.3325367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075474779&doi=10.1145%2f3325291.3325367&partnerID=40&md5=3401918fa8065802c419151b066165b7","National Institute of Technology, Kumamoto College, 2659-2 Suya, Koshi, Kumamoto, 861-1102, Japan","Kato, T., National Institute of Technology, Kumamoto College, 2659-2 Suya, Koshi, Kumamoto, 861-1102, Japan; Jo, K., National Institute of Technology, Kumamoto College, 2659-2 Suya, Koshi, Kumamoto, 861-1102, Japan; Shibasato, K., National Institute of Technology, Kumamoto College, 2659-2 Suya, Koshi, Kumamoto, 861-1102, Japan; Hakata, T., National Institute of Technology, Kumamoto College, 2659-2 Suya, Koshi, Kumamoto, 861-1102, Japan","The Japanese government conducted measures for persons with disabilities in 2013 and they reported that about 2.2 million of challenged children aged 17 or younger live in Japan. Those children cannot communicate with others without somebody's help because of severe physical disabilities and they are wholeheartedly keen to become a member of a society and crave a device/application to communicate with other people. This study proposes gaze region estimation method that does not require an expensive device nor initial setting ""calibration"". Objective of this study is not only implementation of the application but also enhancement of utilization of the application by teachers in special needs education school who are not good at IT or computers, hence concept of this application is “simple, user friendly but effective”. In order for challenged people to show their intentions, it is considered that detecting gaze with high precision is not necessary and it is sufficient to detect gaze region concisely. Therefore, a typical webcam is adopted to implement for the algorithm. Generally, smartphone or tablet computer have the built-in camera and the proposed method is available for these devices. In order to implement gaze region estimation without calibration, the proposed algorithm detects not only eyes but also whole face of user, then estimate gaze direction using deep learning composed by convolutional neural network. To figure out of applicability of the proposed algorithm, some trial experiments were conducted and the accuracy and usability is evaluated. © 2019 Association for Computing Machinery.","Assistive technology; Convolutional neural network; Deep learning; Gaze region estimation; Special needs education school","Calibration; Convolution; Deep learning; Deep neural networks; Assistive technology; Estimation algorithm; Estimation methods; Gaze direction; Persons with disabilities; Physical disability; Special needs educations; Tablet computer; Convolutional neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85075474779
"Hoshino K., Ono N.","7402671353;57193552287;","A Compact Wearable Eye Movement Measurement System for Support of Safe Driving",2019,"Proceedings - 2019 IEEE International Conference on Mechatronics, ICM 2019",,,"8722890","225","231",,2,"10.1109/ICMECH.2019.8722890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067120128&doi=10.1109%2fICMECH.2019.8722890&partnerID=40&md5=d0a1d83bdac0380b788c516b11f36ac5","Graduate School of Systems and Information Engineering, University of Tsukuba, Tsukuba, 305-8573, Japan","Hoshino, K., Graduate School of Systems and Information Engineering, University of Tsukuba, Tsukuba, 305-8573, Japan; Ono, N., Graduate School of Systems and Information Engineering, University of Tsukuba, Tsukuba, 305-8573, Japan","If the eye movement of a driver who makes a long drive during the day and at night can be measured with less stress to the driver in order to estimate the following items for a long time with high accuracy, it is expected that the information will make a great contribution to safe driving: 1) What the driver is looking at or paying attention to (i.e. gaze), 2) A feeling or sign of unwellness such as dizziness, car sickness, and discomfort (i.e. rotational eye movement). In this study, the authors propose an approach in which an area of high intensity gradients in the white part of the eye, i.e. an area that includes a blood vessel with a distinctive shape, is automatically selected to estimate the gaze and calculate the angle of eye rotation, so that the target ocular blood vessel can be tracked with high accuracy even if blood vessels in the white part of the eye differ considerably between individuals in terms of shape, length, and thickness. An evaluation experiment yielded excellent results under various lighting conditions. Especially, extremely positive results were obtained for the estimation accuracy of rotational eye movement, with an estimation error of -0.69° ± 1.02° (mean and standard deviation). © 2019 IEEE.","gaze estimation; intensity gradients; rotational eye movement measurement; vascular image tracking","Blood vessels; Wearable technology; Estimation errors; Evaluation experiments; Eye movement measurement; Gaze estimation; Image tracking; Intensity gradients; Lighting conditions; Mean and standard deviations; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85067120128
"Wang Y., Huang R., Guo L.","15761455700;57205761321;56510659300;","Eye gaze pattern analysis for fatigue detection based on GP-BCNN with ESM",2019,"Pattern Recognition Letters","123",,,"61","74",,6,"10.1016/j.patrec.2019.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063356339&doi=10.1016%2fj.patrec.2019.03.013&partnerID=40&md5=19d8524951db0d455486b3ebd2624bdf","School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China; Beijing Advanced Innovation Center for Big-Data Based Precision Medicine, Beihang University, Beijing, 100191, China","Wang, Y., School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China, Beijing Advanced Innovation Center for Big-Data Based Precision Medicine, Beihang University, Beijing, 100191, China; Huang, R., School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China; Guo, L., School of Automation Science and Electrical Engineering, Beihang University, Beijing, 100191, China","This paper presents a robust fatigue detection system based on binocular consistency, which integrates artificial modulation into deep learning to guide the learning process and removes the extreme cases of dynamic objects through screening mechanism. Specifically, we first build a dual-stream bidirectional convolutional neural network (BCNN) for eye gaze pattern detection, which uses binocular consistency for information interaction. Then we incorporate vectorized local integral projection features which named projection vectors and Gabor filters into BCNN to construct GP-BCNN that not only enhances the resistance of deep learned features to the orientation and scale changes, but strengthens the learning of texture information. Finally, an eye screening mechanism (ESM) based on pupil distance is proposed to eliminate the detected errors caused by the occluded eyes when the lateral face is detected. Demonstrated by introducing binocular consistency and artificial modulation to convolutional neural network (CNN), GP-BCNN improves the widely used CNNs architectures and yields a 2.9% promotion in the average accuracy rate compared with the results obtained by CNN alone. Our approach obtains the state-of-the-art results in fatigue detection and has the generalization potential in general image recognition tasks. © 2019 Elsevier B.V.","Artificial modulation; Convolutional neural network; Eye gaze pattern; Fatigue detection; Information interaction","Binoculars; Convolution; Deep learning; Gabor filters; Image recognition; Modulation; Neural networks; Textures; Artificial modulation; Convolutional neural network; Eye-gaze; Fatigue detection; Information interaction; Fatigue of materials",Article,"Final","",Scopus,2-s2.0-85063356339
"Kim J., Stengel M., Majercik A., De Mello S., Dunn D., Laine S., McGuire M., Luebke D.","57201935260;42162165500;57209395817;57201314496;57193954919;13008308700;16175848100;57204337568;","NVGaze: An anatomically-informed dataset for low-latency, near-eye gaze estimation",2019,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,34,"10.1145/3290605.3300780","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067624610&doi=10.1145%2f3290605.3300780&partnerID=40&md5=c60a8b332f6abd093916ae8726530399","NVIDIA, United States","Kim, J., NVIDIA, United States; Stengel, M., NVIDIA, United States; Majercik, A., NVIDIA, United States; De Mello, S., NVIDIA, United States; Dunn, D., NVIDIA, United States; Laine, S., NVIDIA, United States; McGuire, M., NVIDIA, United States; Luebke, D., NVIDIA, United States","Quality, diversity, and size of training data are critical factors for learning-based gaze estimators. We create two datasets satisfying these criteria for near-eye gaze estimation under infrared illumination: a synthetic dataset using anatomically-informed eye and face models with variations in face shape, gaze direction, pupil and iris, skin tone, and external conditions (2M images at 1280x960), and a real-world dataset collected with 35 subjects (2.5M images at 640x480). Using these datasets we train neural networks performing with sub-millisecond latency. Our gaze estimation network achieves 2.06(±0.44)◦ of accuracy across a wide 30◦ × 40◦ field of view on real subjects excluded from training and 0.5◦ best-case accuracy (across the same FOV) when explicitly trained for one real subject. We also train a pupil localization network which achieves higher robustness than previous methods. © 2019 Association for Computing Machinery.","Dataset; Eye tracking; Machine learning; Virtual reality","Eye tracking; Human computer interaction; Human engineering; Learning systems; Virtual reality; Critical factors; Dataset; External conditions; Field of views; Gaze direction; Gaze estimation; Infrared illumination; Pupil localization; Vanadium compounds",Conference Paper,"Final","",Scopus,2-s2.0-85067624610
"Zhang X., Sugano Y., Bulling A.","57142162900;7005470045;6505807414;","Evaluation of appearance-based methods and implications for gaze-based applications",2019,"Conference on Human Factors in Computing Systems - Proceedings",,,,"","",,25,"10.1145/3290605.3300646","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067609997&doi=10.1145%2f3290605.3300646&partnerID=40&md5=ec97f2ec63a6c234437b57ae9e1d8362","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Osaka University, Graduate School of Information Science and Technology, Japan; University of Stuttgart, Institute for Visualisation and Interactive Systems, Germany","Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Sugano, Y., Osaka University, Graduate School of Information Science and Technology, Japan; Bulling, A., University of Stuttgart, Institute for Visualisation and Interactive Systems, Germany","Appearance-based gaze estimation methods that only require an off-the-shelf camera have significantly improved but they are still not yet widely used in the human-computer interaction (HCI) community. This is partly because it remains unclear how they perform compared to model-based approaches as well as dominant, special-purpose eye tracking equipment. To address this limitation, we evaluate the performance of state-of-the-art appearance-based gaze estimation for interaction scenarios with and without personal calibration, indoors and outdoors, for different sensing distances, as well as for users with and without glasses. We discuss the obtained findings and their implications for the most important gaze-based applications, namely explicit eye input, attentive user interfaces, gaze-based user modelling, and passive eye monitoring. To democratise the use of appearance-based gaze estimation and interaction in HCI, we finally present OpenGaze (www.opengaze.org), the first software toolkit for appearance-based gaze estimation and interaction. © 2019 Copyright held by the owner/author(s).","Appearance-based gaze estimation; Model-based gaze estimation; OpenGaze; Software toolkit; Tobii eyex","Computer software; Gallium compounds; Human computer interaction; Human engineering; User interfaces; Appearance-based methods; Attentive user interfaces; Gaze estimation; Human computer interaction (HCI); Model based approach; OpenGaze; Software toolkits; Tobii eyex; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85067609997
"Shafti A., Orlov P., Faisal A.A.","56183259000;56319626100;6602900233;","Gaze-based, context-aware robotic system for assisted reaching and grasping",2019,"Proceedings - IEEE International Conference on Robotics and Automation","2019-May",,"8793804","863","869",,11,"10.1109/ICRA.2019.8793804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071479396&doi=10.1109%2fICRA.2019.8793804&partnerID=40&md5=3385e43ba2ecee70465a4f054ab14739","Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, United Kingdom","Shafti, A., Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, United Kingdom; Orlov, P., Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, United Kingdom; Faisal, A.A., Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, United Kingdom","Assistive robotic systems endeavour to support those with movement disabilities, enabling them to move again and regain functionality. Main issue with these systems is the complexity of their low-level control, and how to translate this to simpler, higher level commands that are easy and intuitive for a human user to interact with. We have created a multi-modal system, consisting of different sensing, decision making and actuating modalities, leading to intuitive, human-in-the-loop assistive robotics. The system takes its cue from the user's gaze, to decode their intentions and implement low-level motion actions to achieve high-level tasks. This results in the user simply having to look at the objects of interest, for the robotic system to assist them in reaching for those objects, grasping them, and using them to interact with other objects. We present our method for 3D gaze estimation, and grammars-based implementation of sequences of action with the robotic system. The 3D gaze estimation is evaluated with 8 subjects, showing an overall accuracy of 4.68pm 0.14cm. The full system is tested with 5 subjects, showing successful implementation of 100% of reach to gaze point actions and full implementation of pick and place tasks in 96%, and pick and pour tasks in 76% of cases. Finally we present a discussion on our results and what future work is needed to improve the system. © 2019 IEEE.",,,Conference Paper,"Final","",Scopus,2-s2.0-85071479396
"Li B., Zhang Y., Zheng X., Huang X., Zhang S., He J.","57102055600;57016903800;57187383000;55500177600;57208438931;57210368932;","A Smart Eye Tracking System for Virtual Reality",2019,"IEEE MTT-S 2019 International Microwave Biomedical Conference, IMBioC 2019 - Proceedings",,,"8777841","","",,1,"10.1109/IMBIOC.2019.8777841","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070500169&doi=10.1109%2fIMBIOC.2019.8777841&partnerID=40&md5=12999c31a8f84ef9f11cc74ec6f4559d","Northwest University, School of Information Science and Technology, Xi'an, Shaanxi, China; Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China; Sichuan University, College of Electrical Engineering and Information Technology, Chengdu, Sichuan, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China","Li, B., Northwest University, School of Information Science and Technology, Xi'an, Shaanxi, China; Zhang, Y., Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China; Zheng, X., Sichuan University, College of Electrical Engineering and Information Technology, Chengdu, Sichuan, China; Huang, X., School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China; Zhang, S., Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China; He, J., Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China","Virtual reality (VR) technology provides specific three-dimensional (3D) scenes for users, which could be benefit for the applications in the field of medical diagnosis, psychological analysis, cognitive researches and entertainments. The current VR device provides the virtual 3D images, but cannot synchronously detect the user's eye movements which could be significant for deriving the users' gaze points and interest regions in varied applications. In this paper, we proposed a smart eye tracking system for VR device. Firstly, a smart eye movement detection kit is mounted inside the VR device to capture the human eye movement images with the rate of 30 Hz. Based on the proposed eye detection kit, a gaze detection algorithm is then established for VR devices. Next, a gaze estimation model is proposed for human gaze estimation. The proposed smart eye tracking system can detect the user's eye movements in real time under VR stimulation. Moreover, it also provide a new human-VR interaction mode. © 2019 IEEE.",,"Diagnosis; Eye movements; Eye protection; Virtual reality; Eye tracking systems; Gaze detection; Gaze estimation; Interaction modes; Interest regions; Movement detection; Psychological analysis; Threedimensional (3-d); Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85070500169
"Ogusu R., Yamanaka T.","57217939441;14068171800;","LPM: Learnable pooling module for efficient full-face gaze estimation",2019,"Proceedings - 14th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2019",,,"8756523","","",,2,"10.1109/FG.2019.8756523","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070455767&doi=10.1109%2fFG.2019.8756523&partnerID=40&md5=74d68e43bff69cd87479674866f70541","Department of Information and Communication Sciences, Sophia University, Tokyo, Japan","Ogusu, R., Department of Information and Communication Sciences, Sophia University, Tokyo, Japan; Yamanaka, T., Department of Information and Communication Sciences, Sophia University, Tokyo, Japan","Gaze tracking is an important technology in many domains. Techniques such as Convolutional Neural Networks (CNNs) have allowed the invention of the gaze tracking method that relies only on commodity hardware such as the camera on a personal computer. It has been shown that the full-face region for gaze estimation can provide a better performance than the one obtained from eye image alone. However, a problem with using the full-face image is the heavy computation due to the larger image size. This study tackles this problem through compression of the input full-face image by removing redundant information using a novel learnable pooling module. The module can be trained end-to-end by backpropagation to learn the size of the grid in the pooling filter. The learnable pooling module keeps the resolution of valuable regions high and vice versa. This proposed method preserved the gaze estimation accuracy at a certain level when the image was reduced to a smaller size. © 2019 IEEE.",,"Computer hardware; Gesture recognition; Neural networks; Personal computers; Commodity hardware; Convolutional neural network; End to end; Eye images; Full faces; Gaze estimation; Gaze tracking; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85070455767
"Ahmed M., Laskar R.H.","57206975401;23397200000;","Eye detection and localization in a facial image based on partial geometric shape of iris and eyelid under practical scenarios",2019,"Journal of Electronic Imaging","28","3","033009","","",,1,"10.1117/1.JEI.28.3.033009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069539448&doi=10.1117%2f1.JEI.28.3.033009&partnerID=40&md5=4e7b97dac30c21862966aaa8c2dbf8c2","National Institute of Technology Silchar, Department of ECE, Assam, India","Ahmed, M., National Institute of Technology Silchar, Department of ECE, Assam, India; Laskar, R.H., National Institute of Technology Silchar, Department of ECE, Assam, India","The accurate detection and localization of an eye in a facial image is important for many computer vision applications, such as face recognition, fatigued driving detection, and gaze estimation. Although research on eye detection has matured, most existing eye detection methods produce poor performance in various practical scenarios where there exists variation in facial expressions or illumination, people wearing clear eyeglasses, and so on. We have proposed a method that can locate eyes under the above-mentioned varied environmental conditions. The proposed approach follows two steps: eye candidate detection and eye candidate verification. In the first step, two features, namely semicircular edge shape and semiellipse edge shape features, are proposed to detect the eye candidates. In the second step, all of the selected eye candidates are verified using a support vector machine trained with the fusion of local binary pattern, cell mean intensity, and histogram of oriented gradients features. The proposed method has been tested under different conditions of AR, the Chinese Academy of Sciences Pose, Expression, Accessories, and Lighting, and the facial recognition technology databases. The experimental results suggest that the proposed method provides better performance as compared to the existing methods in terms of precision and recall. © 2019 SPIE and IS&T.","Eye detection; Eye localization; Feature-level fusion; Hit-or-miss transform; Shape analysis","Eye protection; Feature extraction; Object recognition; Support vector machines; Eye detection; Eye localization; Feature level fusion; Hit-or-Miss Transform; Shape analysis; Face recognition",Article,"Final","",Scopus,2-s2.0-85069539448
"Jha S., Busso C.","57193014012;35742852700;","Estimation of Gaze Region Using Two Dimensional Probabilistic Maps Constructed Using Convolutional Neural Networks",2019,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May",,"8683794","3792","3796",,,"10.1109/ICASSP.2019.8683794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069529490&doi=10.1109%2fICASSP.2019.8683794&partnerID=40&md5=a22c0c4b917196cf7ee4fb9919a2be76","Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, Department of Electrical Computer Engineering, Richardson, TX  75080, United States","Jha, S., Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, Department of Electrical Computer Engineering, Richardson, TX  75080, United States; Busso, C., Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, Department of Electrical Computer Engineering, Richardson, TX  75080, United States","Predicting the gaze of a user can have important applications in human computer interactions (HCI). They find applications in areas such as social interaction, driver distraction, human robot interaction and education. Appearance based models for gaze estimation have significantly improved due to recent advances in convolutional neural network (CNN). This paper proposes a method to predict the gaze of a user with deep models purely based on CNNs. A key novelty of the proposed model is that it produces a probabilistic map describing the gaze distribution (as opposed to predicting a single gaze direction). This approach is achieved by converting the regression problem into a classification problem, predicting the probability at the output instead of a single direction. The framework relies in a sequence of downsampling followed by upsampling to obtain the probabilistic gaze map. We observe that our proposed approach works better than a regression model in terms of prediction accuracy. The average mean squared error between the predicted gaze and the true gaze is observed to be 6.89 in a model trained and tested on the MSP-Gaze database, without any calibration or adaptation to the target user. © 2019 IEEE.","Convolutional neural networks; gaze estimation; regression by classification","Audio signal processing; Convolution; Driver training; Forecasting; Human computer interaction; Mean square error; Neural networks; Probability distributions; Regression analysis; Signal sampling; Speech communication; Appearance-based models; Convolutional neural network; Driver distractions; Gaze estimation; Human computer interaction (HCI); Prediction accuracy; Probabilistic maps; Social interactions; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85069529490
"Neoh Y.Z., Ibrahim H.","57209468556;57202677372;","Non-intrusive eye gaze estimation from a system with two remote cameras",2019,"International Journal of Innovative Technology and Exploring Engineering","8","7",,"2666","2674",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067875186&partnerID=40&md5=187fb944e225add0673077303aeac74f","School of Electrical & Electronic Engineering, Universiti Sains Malaysia, Engineering Campus, Seberang Perai Selatan, Penang, Malaysia","Neoh, Y.Z., School of Electrical & Electronic Engineering, Universiti Sains Malaysia, Engineering Campus, Seberang Perai Selatan, Penang, Malaysia; Ibrahim, H., School of Electrical & Electronic Engineering, Universiti Sains Malaysia, Engineering Campus, Seberang Perai Selatan, Penang, Malaysia","Eye gaze is the direction where a person is looking at. It is suitable to be used as a type of natural Human Computer Interface (HCI). Current researches use infrared or LED to locate the iris of the user to have better gaze estimation accuracy compared to researches that does not. Infrared and LED are intrusive to human eyes and might cause damage to the cornea and the retina of the eye. This research suggests a non-intrusive approach to locate the iris of the user. By using two remote cameras to capture the images of the user, a better accuracy gaze estimation system can be achieved. The system uses Haar cascade algorithms to detect the face and eye regions. The iris detection uses Hough Circle Transform algorithm to locate the position of the iris, which is critical for the gaze estimation calculation. To enable the system to track the eye and the iris location of the user in real time, the system uses CAMshift (Continuously Adaptive Meanshift) to track the eye and iris of the user. The parameters of the eye and iris are then collected and are used to calculate the gaze direction of the user. The left and right camera achieves 70.00% and 74.67% accuracy respectively. When two cameras are used to estimate the gaze direction, 88.67% accuracy is achieved. This shows that by using two cameras, the accuracy of gaze estimation is improved. © BEIESP.","Eye gaze; Human computer interaction; Iris detection; Two cameras system",,Article,"Final","",Scopus,2-s2.0-85067875186
"Larrazabal A.J., García Cena C.E., Martínez C.E.","57208260242;7401486209;8118083200;","Video-oculography eye tracking towards clinical applications: A review",2019,"Computers in Biology and Medicine","108",,,"57","66",,12,"10.1016/j.compbiomed.2019.03.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064254646&doi=10.1016%2fj.compbiomed.2019.03.025&partnerID=40&md5=5b6a62cd3a06bfada8b4ee0a6f6db2ba","Research Institute for Signals, Systems and Computational Intelligence, Sinc(i), FICH-UNL/CONICET, Ruta Nac. No 168, Km 472.4 (3000), Santa Fe, Argentina; Centre for Robotics and Automation, UPM-CSIC, José Gutiérrez Abascal Street, Madrid, 28006, Spain","Larrazabal, A.J., Research Institute for Signals, Systems and Computational Intelligence, Sinc(i), FICH-UNL/CONICET, Ruta Nac. No 168, Km 472.4 (3000), Santa Fe, Argentina; García Cena, C.E., Centre for Robotics and Automation, UPM-CSIC, José Gutiérrez Abascal Street, Madrid, 28006, Spain; Martínez, C.E., Research Institute for Signals, Systems and Computational Intelligence, Sinc(i), FICH-UNL/CONICET, Ruta Nac. No 168, Km 472.4 (3000), Santa Fe, Argentina","Most neurological diseases are usually accompanied by a broad spectrum of oculomotor alterations. Being able to record and analyze these different types of eye movements would be a valuable tool to understand the functional integrity of brain structures. Nowadays, video-oculography is the most widely used eye-movements assessing method. This paper presents a study of the existing eye tracking video-oculography techniques and also analyzes the importance of measuring slight head movements for diseases diagnosis. In particular, two types of methods are reviewed and compared, including appearance-based and feature-based methods which are further subdivided into 2D-mapping and 3D model-based approaches. In order to demonstrate the advantages and disadvantages of these different eye tracking methods for disease diagnosis, a series of comparisons are conducted between them, addressing the complexity of the system, the accuracy achieved, the ability to measure head movements and the external conditions for which they have been designed. Lastly, it also highlights the open challenges in this research field and discusses possible future directions. © 2019 Elsevier Ltd","Disease diagnoses; Eye gazing; Eye tracking; Head movements; Saccadic movements","3D modeling; Diagnosis; Eye movements; Video recording; Clinical application; Disease diagnosis; Eye-gazing; Feature-based method; Functional integrities; Head movements; Neurological disease; Saccadic movements; Eye tracking; cornea; eye movement; eye tracking; gaze; head movement; human; human computer interaction; image processing; priority journal; Review; saccadic eye movement; videooculography; eye movement; neurologic disease; pathophysiology; three-dimensional imaging; videorecording; Eye Movements; Humans; Imaging, Three-Dimensional; Nervous System Diseases; Video Recording",Review,"Final","",Scopus,2-s2.0-85064254646
"Zhao T., Yan Y., Shehu I.S., Fu X., Wang H.","57192707963;57204644440;56495148400;7402204912;55986542800;","Purifying naturalistic images through a real-time style transfer semantics network",2019,"Engineering Applications of Artificial Intelligence","81",,,"428","436",,1,"10.1016/j.engappai.2019.02.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063316426&doi=10.1016%2fj.engappai.2019.02.011&partnerID=40&md5=ea175cc61601d7a0a5c429d0e4ea2175","Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Yan, Y., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Shehu, I.S., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Wang, H., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. However, due to the different distribution of synthetic images compared to real images, the desired performance cannot still be achieved. Real images consist of multiple forms of light orientation, while synthetic images consist of a uniform light orientation. These features are considered to be characteristic of outdoor and indoor scenes, respectively. To solve this problem, the previous method learned a model to improve the realism of the synthetic image. Different from the previous methods, this paper takes the first step to purify real images. Through the style transfer task, the distribution of outdoor real images is converted into indoor synthetic images, thereby reducing the influence of light. Therefore, this paper proposes a real-time style transfer network that preserves image content information (e.g., gaze direction, pupil center position) of an input image (real image) while inferring style information (e.g., image color structure, semantic features) of style image (synthetic image). In addition, the network accelerates the convergence speed of the model and adapts to multi-scale images. Experiments were performed using mixed studies (qualitative and quantitative) methods to demonstrate the possibility of purifying real images in complex directions. Qualitatively, it compares the proposed method with the available methods in a series of indoor and outdoor scenarios of the LPW dataset. In quantitative terms, it evaluates the purified image by training a gaze estimation model on the cross data set. The results show a significant improvement over the baseline method compared to the raw real image. © 2019 Elsevier Ltd","Feed-forward network; Gaze estimation; Learning-by-synthesis; Style transfer","Image segmentation; Semantic Web; Semantics; Complex directions; Different distributions; Feed-forward network; Gaze estimation; Material resources; Semantic features; Semantics networks; Style transfer; Image enhancement",Article,"Final","",Scopus,2-s2.0-85063316426
"Su D., Li Y.-F., Chen H.","57193014913;8589964900;57191584510;","Toward Precise Gaze Estimation for Mobile Head-Mounted Gaze Tracking Systems",2019,"IEEE Transactions on Industrial Informatics","15","5","8451912","2660","2672",,8,"10.1109/TII.2018.2867952","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052613903&doi=10.1109%2fTII.2018.2867952&partnerID=40&md5=5b880f78ecb925d92139558b73b18fc9","Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, Hong Kong","Su, D., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, Hong Kong; Li, Y.-F., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, Hong Kong; Chen, H., Department of Mechanical Engineering, City University of Hong Kong, Hong Kong, Hong Kong","The gaze estimation in the mobile scenario often suffers from the extrapolation and parallax errors. In this paper, we propose a novel calibration framework to achieve the precise gaze estimation for head-mounted gaze trackers. Our proposed framework consists of two steps to learn a point-to-point and a point-to-line relations, respectively. The aim of step I is to infer the relation between pupil centers and spatially constrained points of regard. By adopting the 'CalibMe' gaze data acquisition method, a sparse Gaussian Process using pseudo-inputs is used to capture the smooth residual field unmodeled by the polynomial function. Meanwhile, a distraction detection criterion is introduced to identify the moment when user's attention is taken away from the calibration point thereby removing outliers. By combining with the point-to-point relation inferred in step I, the observed parallax errors are leveraged in step II to obtain a point-to-line relation, i.e., each pupil center will correspond to an epipolar line. Thus, the real image gaze point projected from different depths is predicted as the intersection of two epipolar lines inferred from binocular data. The simulation and experimental results show the effectiveness of our proposed calibration framework for head-mounted gaze trackers. © 2005-2012 IEEE.","Head-mounted gaze tracker; parallax error compensation; precise gaze estimation","Calibration; Data acquisition; Error compensation; Geometrical optics; Calibration points; Detection methods; Gaze estimation; Gaze tracker; Gaze tracking system; Point-to-point relations; Polynomial functions; Sparse Gaussian process; Eye tracking",Article,"Final","",Scopus,2-s2.0-85052613903
"O'Reilly J., Khan A.S., Li Z., Cai J., Hu X., Chen M., Tong Y.","57195295644;57189308442;57202807301;57195222772;57208721397;57193831567;8644533500;","A Novel Remote Eye Gaze Tracking System Using Line Illumination Sources",2019,"Proceedings - 2nd International Conference on Multimedia Information Processing and Retrieval, MIPR 2019",,,"8695323","449","454",,2,"10.1109/MIPR.2019.00090","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065616181&doi=10.1109%2fMIPR.2019.00090&partnerID=40&md5=689c8476cdd12a4ac3da3a2cb122be7b","Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; University of Washington, Bothell, United States","O'Reilly, J., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; Khan, A.S., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; Li, Z., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; Cai, J., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; Hu, X., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States; Chen, M., University of Washington, Bothell, United States; Tong, Y., Department of Computer Science and Engineering, University of South Carolina, Columbia, United States","This paper proposes a novel system to estimate the 3D point of gaze using observations of the pupil center and corneal reflections (glints). A mathematical model is developed with two solutions to estimate the corneal center efficiently using lines of LED lights. Differing from existing 3D approaches requiring associating light sources with glints, the model automatically associates glints and LED lines and can handle missing glints well. The new model also enables a user-friendly calibration process, allowing natural head movement. Experiments demonstrate that the proposed system can achieve accurate gaze estimation with natural head movement. The performance is impressive when using the natural calibration, requiring less user cooperation. © 2019 IEEE.","Eye Tracking; Gaze Estimation; Robust; User Friendly Calibration","Calibration; Estimation; Light emitting diodes; Calibration process; Corneal reflection; Eye gaze tracking; Gaze estimation; Illumination sources; Robust; User cooperation; User friendly; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85065616181
"Lyudvichenko V., Erofeev M., Ploshkin A., Vatolin D.","14067590000;55400446400;57204562881;6506528788;","Improving Video Compression with Deep Visual-attention Models",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"88","94",,2,"10.1145/3332340.3332358","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069934752&doi=10.1145%2f3332340.3332358&partnerID=40&md5=4de9dc8216aa6ad08febd60d90f22571","Lomonosov Moscow State University, Russian Federation","Lyudvichenko, V., Lomonosov Moscow State University, Russian Federation; Erofeev, M., Lomonosov Moscow State University, Russian Federation; Ploshkin, A., Lomonosov Moscow State University, Russian Federation; Vatolin, D., Lomonosov Moscow State University, Russian Federation","Recent advances in deep learning have markedly improved the quality of visual-attention modelling. In this work we apply these advances to video compression. We propose a compression method that uses a saliency model to adaptively compress frame areas in accordance with their predicted saliency. We selected three state-of-the-art saliency models, adapted them for video compression and analyzed their results. The analysis includes objective evaluation of the models as well as objective and subjective evaluation of the compressed videos. Our method, which is based on the x264 video codec, can produce videos with the same visual quality as regular x264, but it reduces the bitrate by 25% according to the objective evaluation and by 17% according to the subjective one. Also, both the subjective and objective evaluations demonstrate that saliency models can compete with gaze maps for a single observer. Our method can extend to most video bitstream formats and can improve video compression quality without requiring a switch to a new video encoding standard. © 2019 Copyright is held by the owner/author(s).","AVC; H.264; Saliency; Video compression; Visual-attention; X264","Behavioral research; Deep learning; Medical image processing; Petroleum reservoir evaluation; Quality control; Video signal processing; H.264; Objective and subjective evaluations; Objective evaluation; Saliency; Subjective and objective evaluations; Visual Attention; Visual attention model; X264; Image compression",Conference Paper,"Final","",Scopus,2-s2.0-85069934752
"Liu Y., Lee B.-S., Rajan D., Sluzek A., McKeown M.J.","57192561421;7405441352;7005909381;6701500691;7005375626;","CamType: assistive text entry using gaze with an off-the-shelf webcam",2019,"Machine Vision and Applications","30","3",,"407","421",,4,"10.1007/s00138-018-00997-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061457966&doi=10.1007%2fs00138-018-00997-4&partnerID=40&md5=569480a52c8439f75a2ee8da245a3cb8","Nanyang Institute of Technology in Health and Medicine, Interdisciplinary Graduate School, Nanyang Technological University, 50 Nanyang Avenue, Singapore, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Department of Electrical and Computer Engineering, Khalifa University, Abu Dhabi, United Arab Emirates; Department of Medicine, The University of British Columbia, Vancouver, Canada","Liu, Y., Nanyang Institute of Technology in Health and Medicine, Interdisciplinary Graduate School, Nanyang Technological University, 50 Nanyang Avenue, Singapore, Singapore; Lee, B.-S., School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Rajan, D., School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Sluzek, A., Department of Electrical and Computer Engineering, Khalifa University, Abu Dhabi, United Arab Emirates; McKeown, M.J., Department of Medicine, The University of British Columbia, Vancouver, Canada","As modern assistive technology advances, eye-based text entry systems have been developed to help a subset of physically challenged people to improve their communication ability. However, speed of text entry in early eye-typing system tends to be relatively slow due to dwell time. Recently, dwell-free methods have been proposed which outperform the dwell-based systems in terms of speed and resilience, but the extra eye-tracking device is still an indispensable equipment. In this article, we propose a prototype of eye-typing system using an off-the-shelf webcam without the extra eye tracker, in which the appearance-based method is proposed to estimate people’s gaze coordinates on the screen based on the frontal face images captured by the webcam. We also investigate some critical issues of the appearance-based method, which helps to improve the estimation accuracy and reduce computing complexity in practice. The performance evaluation shows that eye typing with webcam using the proposed method is comparable to the eye tracker under a small degree of head movement. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Appearance-based method; Assistive technology; Dwell-free methods; Eye-typing system","Eye movements; Appearance-based methods; Assistive technology; Computing complexity; Critical issues; Dwell-free methods; Eye tracking devices; Text entry systems; Typing systems; Eye tracking",Article,"Final","",Scopus,2-s2.0-85061457966
"Nguyen A.-D., Kim J., Oh H., Kim H., Lin W., Lee S.","57202506744;57219460790;55267741300;43361429400;57196114741;55746172800;","Deep Visual Saliency on Stereoscopic Images",2019,"IEEE Transactions on Image Processing","28","4","8520871","1939","1953",,14,"10.1109/TIP.2018.2879408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056147202&doi=10.1109%2fTIP.2018.2879408&partnerID=40&md5=f8cb7b9e5af244bbb37f87afe2e6c393","Department of Electrical and Electronic Engineering, Yonsei University, Seoul, 03722, South Korea; Microsoft Research Asia, Beijing, 100080, China; Electronics and Telecommunications Research Institute, Daejeon, 34129, South Korea; Korea Institute of Science and Technology, Seoul, 02792, South Korea; School of Computer Science and Engineering, Nanyang Technological University, Singapore, 639798, Singapore","Nguyen, A.-D., Department of Electrical and Electronic Engineering, Yonsei University, Seoul, 03722, South Korea; Kim, J., Microsoft Research Asia, Beijing, 100080, China; Oh, H., Electronics and Telecommunications Research Institute, Daejeon, 34129, South Korea; Kim, H., Korea Institute of Science and Technology, Seoul, 02792, South Korea; Lin, W., School of Computer Science and Engineering, Nanyang Technological University, Singapore, 639798, Singapore; Lee, S., Department of Electrical and Electronic Engineering, Yonsei University, Seoul, 03722, South Korea","Visual saliency on stereoscopic 3D (S3D) images has been shown to be heavily influenced by image quality. Hence, this dependency is an important factor in image quality prediction, image restoration and discomfort reduction, but it is still very difficult to predict such a nonlinear relation in images. In addition, most algorithms specialized in detecting visual saliency on pristine images may unsurprisingly fail when facing distorted images. In this paper, we investigate a deep learning scheme named Deep Visual Saliency (DeepVS) to achieve a more accurate and reliable saliency predictor even in the presence of distortions. Since visual saliency is influenced by low-level features (contrast, luminance, and depth information) from a psychophysical point of view, we propose seven low-level features derived from S3D image pairs and utilize them in the context of deep learning to detect visual attention adaptively to human perception. During analysis, it turns out that the low-level features play a role to extract distortion and saliency information. To construct saliency predictors, we weight and model the human visual saliency through two different network architectures, a regression and a fully convolutional neural networks. Our results from thorough experiments confirm that the predicted saliency maps are up to 70% correlated with human gaze patterns, which emphasize the need for the hand-crafted features as input to deep neural networks in S3D saliency detection. © 1992-2012 IEEE.","convolutional neural network; deep learning; distorted image; Saliency prediction; stereoscopic image","Behavioral research; Convolution; Deep learning; Deep neural networks; Distortion (waves); Feature extraction; Flow visualization; Forecasting; Image quality; Network architecture; Neural networks; Stereo image processing; Three dimensional computer graphics; Visualization; Convolutional neural network; Distorted images; Image color analysis; Saliency detection; Stereoscopic image; Two-dimensional displays; Image reconstruction",Article,"Final","",Scopus,2-s2.0-85056147202
"Jigang L., Francis B.S.L., Rajan D.","36069339000;57189266609;7005909381;","Free-Head Appearance-Based Eye Gaze Estimation on Mobile Devices",2019,"1st International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2019",,,"8669057","232","237",,7,"10.1109/ICAIIC.2019.8669057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063896858&doi=10.1109%2fICAIIC.2019.8669057&partnerID=40&md5=92ced563b9bc24a52a9b9ca6eb8cf25a","School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore","Jigang, L., School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Francis, B.S.L., School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Rajan, D., School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore","Eye gaze tracking plays an important role in human-computer interaction applications. In recent years, many research have been performed to explore gaze estimation methods to handle free-head movement, most of which focused on gaze direction estimation. Gaze point estimation on the screen is another important application. In this paper, we proposed a two-step training network, called GazeEstimator, to improve the estimation accuracy of gaze location on mobile devices. The first step is to train an eye landmarks localization network on 300W-LP dataset [1], and the second step is to train a gaze estimation network on GazeCapture dataset [2]. Some processing operations are performed between the two networks for data cleaning. The first network is able to localize eye precisely on the image, while the gaze estimation network use only eye images and eye grids as inputs, and it is robust to facial expressions and occlusion.Compared with state-of-The-Art gaze estimation method, iTracker, our proposed deep network achieves higher accuracy and is able to estimate gaze location even in the condition that the full face cannot be detected. © 2019 IEEE.","CNN; Deep learning; Eye gaze estimation; Eye localization; Gaze location","Artificial intelligence; Cleaning; Deep learning; Human computer interaction; Location; Appearance based; Eye gaze tracking; Eye localization; Eye-gaze; Facial Expressions; Gaze point estimations; Processing operations; Two-step training; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85063896858
"Mathur P., Ayyar M.P., Shah R.R., Sharma S.G.","57202938977;57205119811;56121902800;57195624066;","Exploring classification of histological disease biomarkers from renal biopsy images",2019,"Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019",,,"8658665","81","90",,4,"10.1109/WACV.2019.00016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063585521&doi=10.1109%2fWACV.2019.00016&partnerID=40&md5=e52090c2e154db36facc472c88b2036a","MIDAS Lab, IIIT-Delhi, Delhi, India; Arkana Laboratories, Arkansas, United States","Mathur, P., MIDAS Lab, IIIT-Delhi, Delhi, India; Ayyar, M.P., MIDAS Lab, IIIT-Delhi, Delhi, India; Shah, R.R., MIDAS Lab, IIIT-Delhi, Delhi, India; Sharma, S.G., Arkana Laboratories, Arkansas, United States","Identification of diseased kidney glomeruli and fibrotic regions remains subjective and time-consuming due to complete dependence on an expert kidney pathologist. In an attempt to automate the classification of glomeruli into normal and abnormal morphology and classification of fibrosis patches into mild, moderate and severe categories, we investigate three deep learning techniques: traditional transfer learning, pre-trained deep neural networks for feature extraction followed by supervised classification, and a novel Multi-Gaze Attention Network (MGANet) that uses multi-headed self-attention through parallel residual skip connections in a CNN architecture. Empirically, while the transfer learning models such as ResNet50, InceptionRes-NetV2,VGG19 and InceptionV3 acutely under-perform in the classification tasks, the Logistic Regression model augmented with features extracted from the InceptionResNetV2 shows promising results. Additionally, the experiments effectively ascertain that the proposed MGANet architecture outperforms both the former baseline techniques to establish the state of the art accuracy of 87.25% and 81.47% for glomeruli and fibrosis classification, respectively on the Renal Glomeruli Fibrosis Histopathological (RGFH) database. © 2019 IEEE",,"Computer vision; Deep neural networks; Image classification; Network architecture; Regression analysis; Abnormal morphology; Classification tasks; Disease biomarker; Learning techniques; Logistic Regression modeling; State of the art; Supervised classification; Transfer learning; Classification (of information)",Conference Paper,"Final","",Scopus,2-s2.0-85063585521
"Das P.J., Talukdar A.K., Sarma K.K.","57212081899;36019598100;35219168100;","A Framework for Human Behaviour Detection Using Combined Analysis of Facial Expression and Eye Gaze",2019,"Proceedings of 2nd International Conference on Innovations in Electronics, Signal Processing and Communication, IESC 2019",,,"8902367","154","160",,,"10.1109/IESPC.2019.8902367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075859067&doi=10.1109%2fIESPC.2019.8902367&partnerID=40&md5=fe9fa7cce04820c941a6775e344370dc","Gauhati University, Dept. of Electronics and Communication Engineering, Guwahati, 781014, India","Das, P.J., Gauhati University, Dept. of Electronics and Communication Engineering, Guwahati, 781014, India; Talukdar, A.K., Gauhati University, Dept. of Electronics and Communication Engineering, Guwahati, 781014, India; Sarma, K.K., Gauhati University, Dept. of Electronics and Communication Engineering, Guwahati, 781014, India","Facial appearances like a happy, sad, disgust, surprise, angry, fear and Eye Gaze Estimations are the fastest means of communication while conveying any type of information. Expressions not only expose the sensitivity or feelings of any person but can also be used to judge his/her mental states. Therefore, it has become a wide interest area of research due to its applications to the fields like Human Computer Interface (HCI), Man Machine Interface (MMI) etc. In order to get more effective human behavior identification, we have presented a combination of facial expression recognition and eye gaze estimation technique where we have to recognize the real time expressions by using Convolutional Neural Network (CNN) model with transfer learning method. After that, we determine the eye gaze by using Viola Jones, Circular Hough Transform (CHT) and a geometric method. With the combination of both processes, we able to predict the human behavior like drivers concentration levels. In this way experiments are carried out on MUG database for our own trained CNN model based on VGG16 (Visual Geometry Group) pre trained model and gives better performance with accuracy of 93% for training and 94% for overall process. © 2019 IEEE.","eye gaze estimation; Face detection; face recognition; facial expression; transfer learning CNN; VGG16","Behavioral research; Hough transforms; Learning systems; Neural networks; Circular Hough transforms; Convolutional neural network; Eye-gaze; Facial expression recognition; Facial Expressions; Human computer interfaces; Transfer learning; VGG16; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85075859067
"Sai Mounica M., Manvita M., Jyotsna C., Amudha J.","57211140646;57211140498;57193578958;35766448700;","Low cost eye gaze tracker using web camera",2019,"Proceedings of the 3rd International Conference on Computing Methodologies and Communication, ICCMC 2019",,,"8819645","79","85",,1,"10.1109/ICCMC.2019.8819645","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072777976&doi=10.1109%2fICCMC.2019.8819645&partnerID=40&md5=64063ef4ed86dbbc38f51256c379dad6","Dept. of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, India","Sai Mounica, M., Dept. of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, India; Manvita, M., Dept. of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, India; Jyotsna, C., Dept. of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, India; Amudha, J., Dept. of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, India","The traditional gaze tracking systems are of invasive, expensive or not of a standard hardware. To address this problem, research has been focused onto systems with simple hardware with gaze tracking systems. We propose a system which uses web camera and the free open source Computer Vision Library Open CV. Gaze estimation plays a major role in predicting human attention and understanding human activities. It is also used in market analysis, gaze driven interactive displays, medical research, usability research, packaging research, gaming research, psychology research, and other human-machine interfaces. This paper describes how to develop a low cost eye gaze system using a simple web camera. The system captures real time video of the person to detect the eyes in the initial frames and extract the features of eyes. Once the features are extracted, the eyes are tracked in the subsequent frames and the gaze direction is estimated using the computational intelligence techniques. © 2019 IEEE","Calibration; Eye tracking; Gaze estimation; Human computer interaction; Web camera","Calibration; Cameras; Computer hardware; Costs; Human computer interaction; Open systems; Computational intelligence techniques; Computer vision library; Gaze estimation; Gaze tracking system; Human Machine Interface; Interactive display; Usability research; Web camera; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85072777976
"Wong E.T., Yean S., Hu Q., Lee B.S., Liu J., Deepu R.","57209509367;57192383054;57202648497;7405441352;57209506625;57209509412;","Gaze Estimation Using Residual Neural Network",2019,"2019 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2019",,,"8730846","411","414",,5,"10.1109/PERCOMW.2019.8730846","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067968733&doi=10.1109%2fPERCOMW.2019.8730846&partnerID=40&md5=86eebd4a37148352c6baf1e802b92b3f","Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore","Wong, E.T., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore; Yean, S., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore; Hu, Q., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore; Lee, B.S., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore; Liu, J., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore; Deepu, R., Nanyang Technological University, School of Computer Science and Engineering), Singapore, Singapore","Eye gaze tracking has become an prominent research topic in human-computer interaction and computer vision. It is due to its application in numerous fields, such as the market research, medical, neuroscience and psychology. Eye gaze tracking is implemented by estimating gaze (gaze estimation) for each individual frame in offline or real-time video captured. Therefore, in order to produce the secure the accurate tracking, especially in the emerging use in medical and community, innovation on the gaze estimation posts a challenge in research field. In this paper, we explored the use of the deep learning model, Residual Neural Network (ResNet-18), to predict the eye gaze on mobile device. The model is trained using the large-scale eye tracking public dataset called GazeCapture. We aim to innovate by incorporating methods/techniques of removing the blinking data, applying image histogram normalisation, head pose, and face grid features. As a result, we achieved 3.05cm average error, which is better performance than iTracker (4.11cm average error), the recent gaze tracking deep-learning model using AlexNet architecture. Upon observation, adaptive normalisation of the images was found to produce better results compared to histogram normalisation. Additionally, we found that head pose information was useful contribution to the proposed deep-learning network, while face grid information does not help to reduce test error. © 2019 IEEE.","deep learning; eye track; mobile; ResNet","Deep learning; Errors; Graphic methods; Human computer interaction; Large dataset; Neural networks; Ubiquitous computing; Accurate tracking; Eye gaze tracking; Grid information; Learning network; Market researches; mobile; Real time videos; ResNet; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85067968733
"Ariz M., Villanueva A., Cabeza R.","57204069029;7101612861;36763933900;","Robust and accurate 2D-tracking-based 3D positioning method: Application to head pose estimation",2019,"Computer Vision and Image Understanding","180",,,"13","22",,10,"10.1016/j.cviu.2019.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060351964&doi=10.1016%2fj.cviu.2019.01.002&partnerID=40&md5=e07035c188f4c09306738b73cd46a60b","Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, 31006, Spain","Ariz, M., Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, 31006, Spain; Villanueva, A., Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, 31006, Spain; Cabeza, R., Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, 31006, Spain","Head pose estimation (HPE) is currently a growing research field, mainly because of the proliferation of human–computer interfaces (HCI) in the last decade. It offers a wide variety of applications, including human behavior analysis, driver assistance systems or gaze estimation systems. This article aims to contribute to the development of robust and accurate HPE methods based on 2D tracking of the face, enhancing performance of both 2D point tracking and 3D pose estimation. We start with a baseline method for pose estimation based on POSIT algorithm. A novel weighted variant of POSIT is then proposed, together with a methodology to estimate weights for the 2D–3D point correspondences. Further, outlier detection and correction methods are also proposed in order to enhance both point tracking and pose estimation. With the aim of achieving a wider impact, the problem is addressed using a global approach: all the methods proposed are generalizable to any kind of object for which an approximate 3D model is available. These methods have been evaluated for the specific task of HPE using two different head pose video databases; a recently published one that reflects the expected performance of the system in current technological conditions, and an older one that allows an extensive comparison with state-of-the-art HPE methods. Results show that the proposed enhancements improve the accuracy of both 2D facial point tracking and 3D HPE, with respect to the implemented baseline method, by over 15% in normal tracking conditions and over 30% in noisy tracking conditions. Moreover, the proposed HPE system outperforms the state of the art on the two databases. © 2019 The Authors","Facial point detection and tracking; Head tracking; Outlier correction; Pose estimation; POSIT","Automobile drivers; Behavioral research; Statistics; Facial point detections; Head tracking; Outlier correction; Pose estimation; POSIT; Face recognition",Article,"Final","",Scopus,2-s2.0-85060351964
"Gautam G., Mukhopadhyay S.","57202336101;7401807653;","An adaptive localization of pupil degraded by eyelash occlusion and poor contrast",2019,"Multimedia Tools and Applications","78","6",,"6655","6677",,3,"10.1007/s11042-018-6371-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050812159&doi=10.1007%2fs11042-018-6371-0&partnerID=40&md5=4c2388a9ffd242ac9d181af951d10fe9","Indian Institute of Technology, Dhanbad, 826004, India","Gautam, G., Indian Institute of Technology, Dhanbad, 826004, India; Mukhopadhyay, S., Indian Institute of Technology, Dhanbad, 826004, India","The inner boundary of iris represents the pupil’s edge. Hence, to work an Iris Recognition System (IRS) and the gaze tracking system expeditiously it is important to locate it as precisely as possible in a significant amout of time. In the presence of non-ideal constraints e.g. non-uniform illumination, poor contrast, eyelashes, hairs, glasses, off-angle orientation, these systems may not work well. In this paper we present an adaptive pupil localization method based on the roundness criteria. First, it applies a gray level inversion to suppress the reflections, then it performs Gray level co-occurrence matrix (GLCM) based contrast estimation. If this estimated contrast is lower than a certain threshold, the input image is made to undergo gamma correction to adjust the contrast. Subsequently, anisotropic diffusion filtering followed by log transformation is applied, which suppresses the effect of eyelash occlusion, limits the creation of small regions and highlight the dark pixels. Afterwards, a clean binary image with few regions is acquired using adaptive thresholding and some morphological operations. Finally, the roundness metric is computed for each of these regions and the region with largest roundness metric, also being greater than a prescribed threshold, declared as pupil. Experiments were carried out on few well known databases, NICE1, CASIA V3 lamp, MMU, WVU and IITD. The results are grounded upon subjective and objective evaluation; which in turn, indicate that our method outperforms a state-of-the-art approach and a deep learning approach in terms of localization capability in some unconstrained scenarios and shorter processing time. After assessing the performance of the proposed algorithm, it is manifested that it ensures a fast and robust localization of pupil in the presence of corneal reflection, poor contrast, glasses and eyelash occlusion. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Contrast estimation; Gray-level co-occurrence matrix; Iris biometric; Morphological reconstruction; Pupil localization","Binary images; Biometrics; Deep learning; Glass; Image segmentation; Mathematical morphology; Physical addresses; Anisotropic diffusion filtering; Gray level co occurrence matrix(GLCM); Gray level co-occurrence matrix; Iris biometrics; Morphological reconstruction; Pupil localization; State-of-the-art approach; Subjective and objective evaluations; Eye tracking",Article,"Final","",Scopus,2-s2.0-85050812159
"Li P., Hou X., Wei L., Song G., Duan X.","56375954100;57207460293;57207453327;55218895500;55646016600;","Efficient and low-cost deep-learning based gaze estimator for surgical robot control",2019,"2018 IEEE International Conference on Real-Time Computing and Robotics, RCAR 2018",,,"8621810","58","63",,4,"10.1109/RCAR.2018.8621810","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062548732&doi=10.1109%2fRCAR.2018.8621810&partnerID=40&md5=62576ddafef02ee80f8af70f074656b2","School of Mechanical Engineering and Automation, Harbin Institute of Technology (ShenZhen), Shenzhen, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China","Li, P., School of Mechanical Engineering and Automation, Harbin Institute of Technology (ShenZhen), Shenzhen, China; Hou, X., School of Mechanical Engineering and Automation, Harbin Institute of Technology (ShenZhen), Shenzhen, China; Wei, L., School of Mechanical Engineering and Automation, Harbin Institute of Technology (ShenZhen), Shenzhen, China; Song, G., State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Duan, X., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China","Surgical robots are playing more and more important role in modern operating room. However, operations by using surgical robot are not easy to handle by doctors. Vision based human-computer interaction (HCI) is a way to ease the difficulty to control surgical robots. While the problem of this method is that eyes tracking devices are expensive. In this paper, a low cost and robust deep-learning based on gaze estimator is proposed to control surgical robots. By this method, doctors can easily control the robot by specifying the starting point and ending point of the surgical robot using eye gazing. Surgical robots can also be controlled to move in 9 directions using controllers' eyes gazing information. A Densely Connected convolutional Neural Networks (Dense CNN) model for 9-direction/36-direction gaze estimation is built. The Dense CNN architecture has much more less trainable parameters compared to traditional CNN network architecture (AlexNet like/VGG like) which is more feasible to deploy on the Field-Programmable Gate Array (FPGA) and other hardware with limited memories. © 2018 IEEE.","Convolutional Neural Neural; Deep Learning; Gaze estimation; Minimally Invasive Surgery; Surgical robot","Convolution; Cost estimating; Deep learning; Eye movements; Field programmable gate arrays (FPGA); Human computer interaction; Human robot interaction; Network architecture; Neural networks; Robotic surgery; Robotics; Transplantation (surgical); Convolutional neural network; Convolutional Neural Neural; Gaze estimation; Human computer interaction (HCI); Limited memory; Minimally invasive surgery; Tracking devices; Vision based; Surgical equipment",Conference Paper,"Final","",Scopus,2-s2.0-85062548732
"Kaur A., Mustafa A., Mehta L., Dhall A.","57220992105;57212890487;57201698510;35229206900;","Prediction and Localization of Student Engagement in the Wild",2019,"2018 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2018",,,"8615851","","",,25,"10.1109/DICTA.2018.8615851","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062243042&doi=10.1109%2fDICTA.2018.8615851&partnerID=40&md5=71d2cfced8c97983bbb1153d8ad92483","Learning Affect Semantic Imaging Analysis Group, Indian Institute of Technology, Ropar, India","Kaur, A., Learning Affect Semantic Imaging Analysis Group, Indian Institute of Technology, Ropar, India; Mustafa, A., Learning Affect Semantic Imaging Analysis Group, Indian Institute of Technology, Ropar, India; Mehta, L., Learning Affect Semantic Imaging Analysis Group, Indian Institute of Technology, Ropar, India; Dhall, A., Learning Affect Semantic Imaging Analysis Group, Indian Institute of Technology, Ropar, India","Digital revolution has transformed the traditional teaching procedures, students are going online to access study materials. It is realised that analysis of student engagement in an e-learning environment would facilitate effective task accomplishment and learning. Well known social cues of engagement/disengagement can be inferred from facial expressions, body movements and gaze patterns. In this paper, student's response to various stimuli (educational videos) are recorded and cues are extracted to estimate variations in engagement level. We study the association of a subject's behavioral cues with his/her engagement level, as annotated by labelers. We have localized engaging/non-engaging parts in the stimuli videos using a deep multiple instance learning based framework, which can give useful insight into designing Massive Open Online Courses (MOOCs) video material. Recognizing the lack of any publicly available dataset in the domain of user engagement, a new 'in the wild' dataset is curated. The dataset: Engagement in the Wild contains 264 videos captured from 91 subjects, which is approximately 16.5 hours of recording. Detailed baseline results using different classifiers ranging from traditional machine learning to deep learning based approaches are evaluated on the database. Subject independent analysis is performed and the task of engagement prediction is modeled as a weakly supervised learning problem. The dataset is manually annotated by different labelers and the correlation studies between annotated and predicted labels of videos by different classifiers are reported. This dataset creation is an effort to facilitate research in various e-learning environments such as intelligent tutoring systems, MOOCs, and others. © 2018 IEEE.","Dataset for Student Engagement; E-learning Environment; Engagement Detection; Engagement Localization","Classification (of information); Computer aided instruction; Deep learning; Machine learning; Students; E-learning environment; Engagement Localization; Intelligent tutoring system; Learning-based approach; Massive open online course; Multiple instance learning; Student engagement; Weakly supervised learning; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85062243042
"Mitsuzumi Y., Nakazawa A.","57200412643;35807510800;","Eye Contact Detection Algorithms Using Deep Learning and Generative Adversarial Networks",2019,"Proceedings - 2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",,,"8616663","3927","3931",,,"10.1109/SMC.2018.00666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062231712&doi=10.1109%2fSMC.2018.00666&partnerID=40&md5=c9ba54eb9919b14119b30acb0c2f20ab","Graduate School of Infomatics, Kyoto University, Kyoto, Japan","Mitsuzumi, Y., Graduate School of Infomatics, Kyoto University, Kyoto, Japan; Nakazawa, A., Graduate School of Infomatics, Kyoto University, Kyoto, Japan","Eye contact (mutual gaze) is a foundation of human communication and social interactions; therefore, it is studied in many fields such as psychology, social science, and medicine. Our group have been studied wearable vision-based eye contact detection techniques using a first person camera for the purpose of evaluating the gaze skills in the tender dementia care. In this work, we search for deep learning-based eye contact detection techniques from small number of labeled images. We implemented and tested two eye contact detection algorithms: naïve deep-learning-based algorithm and generative adversarial networks (GAN)-based semi supervised learning (SSL) algorithm. These methods are learned and verified by using Columbia Gaze Dataset, Facescrub and our original datasets. The results show the effectiveness and limitations of the deep-learning-based and GAN-based approaches. Interestingly, we found the bilateral difference of the accuracy of eye contact detection with respect to the facial pose with respect to the camera, which is expected to be caused by the learning datasets. © 2018 IEEE.","deep learning; eye contact; generative adversarial network (GAN); mutual gaze; semi supervised learning","Cameras; Learning algorithms; Machine learning; Signal detection; Supervised learning; Adversarial networks; Eye contact; Human communications; Learning-based algorithms; Mutual gazes; Semi- supervised learning; Semi-supervised learning (SSL); Social interactions; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85062231712
"Yin Y., Juan C., Chakraborty J., McGuire M.P.","57201739020;57207114226;23476701400;16203300300;","Classification of Eye Tracking Data Using a Convolutional Neural Network",2019,"Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018",,,"8614110","530","535",,6,"10.1109/ICMLA.2018.00085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062237930&doi=10.1109%2fICMLA.2018.00085&partnerID=40&md5=c1c73d0675ce108dd7192e9e0ae02507","Department of Computer and Information Sciences, Towson University, Towson, United States","Yin, Y., Department of Computer and Information Sciences, Towson University, Towson, United States; Juan, C., Department of Computer and Information Sciences, Towson University, Towson, United States; Chakraborty, J., Department of Computer and Information Sciences, Towson University, Towson, United States; McGuire, M.P., Department of Computer and Information Sciences, Towson University, Towson, United States","Historically, eye tracking analysis has been a useful approach to identify areas of interest (AOIs) where users have specific regions of the user interface (UI) in which they are interested. Many algorithms have been proposed to analyze eye tracking data in order to make user interfaces more effective. The objective of this study is to use convolutional neural networks (CNNs) to classify eye tracking data. First, a CNN was used to classify two different web interfaces for browsing news data. Then in a second experiment, a CNN was used to classify the nationalities of users. In addition, techniques of data-preprocessing and feature-engineering were applied. The algorithm used in this research is convolutional neural network (CNN), which is famous in deep learning field. Keras framework running on top of TensorFlow was used to define and train our CNN model. The purpose of this research is to explore how feature-engineering can affect evaluation metrics about our model. The results of the study show a number of interesting patterns and generally that deep learning shows promise in the analysis of eye tracking data. © 2018 IEEE.","CNN; Deep learning; Eye tracking; Feature-engineering; Gaze point; Keras; TensorFlow","Classification (of information); Convolution; Deep learning; Machine learning; Neural networks; User interfaces; Convolutional neural network; Data preprocessing; Evaluation metrics; Eye-tracking analysis; Feature engineerings; Gaze point; Keras; TensorFlow; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85062237930
"Barbosa Monforte P.H., Matos Araujo G., Azevedo De Lima A.","57207112505;11539108500;24733621100;","Evaluation of a New Kernel-Based Classifier in Eye Pupil Detection",2019,"Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018",,,"8614088","380","385",,1,"10.1109/ICMLA.2018.00063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062230155&doi=10.1109%2fICMLA.2018.00063&partnerID=40&md5=a0859b6c189a69687c57b4b66cd46877","Centro Federal de Educação Tecnolgica Celso Suckow da Fonseca, Estrada de Adrianópolis, 1.317, Nova Iguaçu - RJ, CEP: 26041-271, Brazil","Barbosa Monforte, P.H., Centro Federal de Educação Tecnolgica Celso Suckow da Fonseca, Estrada de Adrianópolis, 1.317, Nova Iguaçu - RJ, CEP: 26041-271, Brazil; Matos Araujo, G., Centro Federal de Educação Tecnolgica Celso Suckow da Fonseca, Estrada de Adrianópolis, 1.317, Nova Iguaçu - RJ, CEP: 26041-271, Brazil; Azevedo De Lima, A., Centro Federal de Educação Tecnolgica Celso Suckow da Fonseca, Estrada de Adrianópolis, 1.317, Nova Iguaçu - RJ, CEP: 26041-271, Brazil","Accurate pupil location is paramount to applications such as gaze estimation, assistive technologies and several man-machine interfaces as the ones found in smartphones and VR applications. We introduce a new classifier stemmed from the Inner Product Detector and investigate its features on the challenging task of pupil localization. IPD (Inner Product Detector) is a classifier with high potential in facial landmarks detection. It is robust to variations in the desired pattern while maintaining good generalization and computational efficiency. However, one possible limitation is its linear behavior, which could be overcome by aggregating non-linear techniques, such as kernel methods. Although kernel classifiers have been exhaustively studied in the past two decades, it was not analyzed or applied with IPD, yet. The proposed KIPD achieves in the worst case an accuracy of 97.41% on the BioID dataset and 93.71% in LFPW dataset both at 10% of the interocular distance. In this paper the KIPD is compared to the state of the art methods, including the ones using deep learning, being competitive in terms of accuracy as well as computational complexity. © 2018 IEEE.","Correlation filtes; Kernel machines; Machine learning","Computational efficiency; Learning systems; Machine learning; Assistive technology; Eye pupil detections; Kernel based classifiers; Kernel classifiers; Kernel machine; Man machine interface; Nonlinear techniques; State-of-the-art methods; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85062230155
"Tong H., Wan Q., Kaszowska A., Panetta K., Taylor H.A., Agaian S.","57211084616;57188747389;57203481497;6507727793;7403057334;35321805400;","ARFurniture: Augmented reality interior decoration style colorization",2019,"IS and T International Symposium on Electronic Imaging Science and Technology","2019","2","175","","",,4,"10.2352/ISSN.2470-1173.2019.2.ERVR-175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080106107&doi=10.2352%2fISSN.2470-1173.2019.2.ERVR-175&partnerID=40&md5=90c8441c262b585d9b61b56c32a32e63","Department of Electrical and Computer Engineering, Tufts University, Medford, MA, United States; Department of Psychology, Tufts University, Medford, MA, United States; Computer Science, City University of New York, New York City, NY, United States","Tong, H., Department of Electrical and Computer Engineering, Tufts University, Medford, MA, United States; Wan, Q., Department of Electrical and Computer Engineering, Tufts University, Medford, MA, United States; Kaszowska, A., Department of Psychology, Tufts University, Medford, MA, United States; Panetta, K., Department of Electrical and Computer Engineering, Tufts University, Medford, MA, United States; Taylor, H.A., Department of Psychology, Tufts University, Medford, MA, United States; Agaian, S., Computer Science, City University of New York, New York City, NY, United States","Augmented Reality (AR) can seamlessly create an illusion of virtual elements blended into the real world scene, which is one of the most fascinating human-machine interaction technologies. AR has been utilized in a variety of real-life applications including immersive collaborative gaming, fashion appreciation, interior design, and assistive devices for individuals with vision impairments. This paper contributes a real-time AR application, ARFurniture, which will allow the users to envision furniture-of-interests in different colors and different styles, all from their smart devices. The core software architecture consists of deep-learning based semantic segmentation and fast-speed color transformation. Our software architecture allows the user prompt the system to colorize the style of the furniture-of-interest within the scene on their mobile devices, and has been successfully deployed on mobile devices. In addition, using eye gaze as a pointing indicator, a head-mounted user-centric augmented reality based indoor decoration style colorization concept is discussed. Related algorithms, system design, and simulation results for ARFurniture are presented. Furthermore, a no-reference image quality measure, Naturalness Image Quality Evaluator (NIQE), was utilized to evaluate the immersiveness and naturalness of ARFuniture. The results demonstrate that ARFurniture has game-changing value to enhance user experience in indoor decoration. © 2019, Society for Imaging Science and Technology.",,"Architectural design; Augmented reality; Deep learning; Image quality; Semantics; Software architecture; User experience; Virtual reality; Collaborative gaming; Color transformation; Human machine interaction; No-reference images; Real-life applications; Related algorithms; Semantic segmentation; Vision impairments; Quality control",Conference Paper,"Final","",Scopus,2-s2.0-85080106107
"Obeso A.M., Benois-Pineau J., Guissous K., Gouet-Brunet V., Garcia Vazquez M.S., Ramirez Acosta A.A.","57016511800;6701750610;56200912800;55883860100;24366173900;57204866232;","Comparative study of visual saliency maps in the problem of classification of architectural images with Deep CNNs",2019,"2018 8th International Conference on Image Processing Theory, Tools and Applications, IPTA 2018 - Proceedings",,,"8608125","","",,4,"10.1109/IPTA.2018.8608125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061920656&doi=10.1109%2fIPTA.2018.8608125&partnerID=40&md5=89e5ea20da4c31b4f5a7ace515f785f8","Université de Bordeaux, LaBRI, Bordeaux, France; Univ. Paris-Est, LASTIG MATIS, IGN, ENSG, Saint-Mande, F-94160, France; Instituto Politécnico Nacional, CITEDI, Tijuana, Mexico; MIRAL RDI, San Diego, United States","Obeso, A.M., Université de Bordeaux, LaBRI, Bordeaux, France, Instituto Politécnico Nacional, CITEDI, Tijuana, Mexico; Benois-Pineau, J., Université de Bordeaux, LaBRI, Bordeaux, France; Guissous, K., Univ. Paris-Est, LASTIG MATIS, IGN, ENSG, Saint-Mande, F-94160, France; Gouet-Brunet, V., Univ. Paris-Est, LASTIG MATIS, IGN, ENSG, Saint-Mande, F-94160, France; Garcia Vazquez, M.S., Instituto Politécnico Nacional, CITEDI, Tijuana, Mexico; Ramirez Acosta, A.A., MIRAL RDI, San Diego, United States","Incorporating Human Visual System (HVS) models into building of classifiers has become an intensively researched field in visual content mining. In the variety of models of HVS we are interested in so-called visual saliency maps. Contrarily to scan-paths they model instantaneous attention assigning the degree of interestingness/saliency for humans to each pixel in the image plane. In various tasks of visual content understanding, these maps proved to be efficient stressing contribution of the areas of interest in image plane to classifiers models. In previous works saliency layers have been introduced in Deep CNNs, showing that they allow reducing training time getting similar accuracy and loss values in optimal models. In case of large image collections efficient building of saliency maps is based on predictive models of visual attention. They are generally bottom-up and are not adapted to specific visual tasks. Unless they are built for specific content, such as »urban images»-targeted saliency maps we also compare in this paper. In present research we propose a »bootstrap» strategy of building visual saliency maps for particular tasks of visual data mining. A small collection of images relevant to the visual understanding problem is annotated with gaze fixations. Then the propagation to a large training dataset is ensured and compared with the classical GBVS model and a recent method of saliency for urban image content. The classification results within Deep CNN framework are promising compared to the purely automatic visual saliency prediction. © 2018 IEEE.","Deep Learning; Mexican Culture; Saliency Maps","Behavioral research; Data mining; Deep learning; Large dataset; Vision; Visualization; Architectural images; Classification results; Comparative studies; Efficient buildings; Human visual system model; Predictive models; Saliency map; Visual data mining; Image classification",Conference Paper,"Final","",Scopus,2-s2.0-85061920656
"Cazzato D., Castro S.M., Agamennoni O., Fernández G., Voos H.","55866556300;55171401100;6604079588;55550165900;6603280387;","A non-invasive tool for attention-deficit disorder analysis based on gaze tracks.",2019,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,"5","","",,,"10.1145/3309772.3309777","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070542762&doi=10.1145%2f3309772.3309777&partnerID=40&md5=dd2cbc528abc85226d38b86ce5bb14bd","Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg, Luxembourg; Universidad Nacional del Sur, Bahía Blanca, Argentina","Cazzato, D., Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg, Luxembourg; Castro, S.M., Universidad Nacional del Sur, Bahía Blanca, Argentina; Agamennoni, O., Universidad Nacional del Sur, Bahía Blanca, Argentina; Fernández, G., Universidad Nacional del Sur, Bahía Blanca, Argentina; Voos, H., Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg, Luxembourg","Attention deficit hyperactivity disorder (ADHD) is a neurodevelop-mental disability characterized by difficulties in keeping concentration, excessive activity and difficulties controlling behaviour not appropriate to the person’s age. It is estimated to affect between 4-9% of youths and 2-5% of adults. Assistive technologies can help people with ADHD to reach goals, stay organized and even fight the urge to succumb to forms of distraction. This work introduces a tool designed for people with ADHD aimed at detecting and training their ability to follow a target in a screen. The tool is based on noninvasive monocular gaze estimation technique without constraints in terms of user dependent calibration or appearance. The system has been employed and validated in a human-computer interaction (HCI) scenario with the aim of evaluating the user visual exploration. Results show that the tool can be used in complex tasks like monitoring a user progress comparing performance after different sessions. © 2019 Association for Computing Machinery.","Attention-deficit hyperactivity disorder (adhd); Classification; Gaze estimation; Random forests","Classification (of information); Decision trees; Diseases; Human computer interaction; Intelligent systems; Random forests; Assistive technology; Attention deficit disorder; Attention deficit hyperactivity disorder; Gaze estimation; Human computer interaction (HCI); Mental disabilities; User-dependent; Visual exploration; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85070542762
"Rabba S., Kyan M., Gao L., Quddus A., Zandi A.S., Guan L.","57196347194;6506086351;56285851900;57212781508;57212855259;55679853000;","Discriminative robust gaze estimation using kernel-DMCCA fusion",2019,"Proceedings - 2018 IEEE International Symposium on Multimedia, ISM 2018",,,"8603305","291","298",,1,"10.1109/ISM.2018.00064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061667099&doi=10.1109%2fISM.2018.00064&partnerID=40&md5=09674f0867455f7bb52d3cc4d967ff5a","Ryerson University, Toronto, Canada; York University, Toronto, Canada; Alcohol Countermeasure Systems, Toronto, Canada; Alcohol Countermeasure systems, Toronto, Canada","Rabba, S., Ryerson University, Toronto, Canada; Kyan, M., York University, Toronto, Canada; Gao, L., Ryerson University, Toronto, Canada; Quddus, A., Alcohol Countermeasure Systems, Toronto, Canada, Alcohol Countermeasure systems, Toronto, Canada; Zandi, A.S., Alcohol Countermeasure Systems, Toronto, Canada, Alcohol Countermeasure systems, Toronto, Canada; Guan, L., Ryerson University, Toronto, Canada","The proposed framework employs discriminative analysis for gaze estimation using kernel discriminative multiple canonical correlation analysis (K-DMCCA), which represents different feature vectors that account for variations of head pose, illumination and occlusion. The feature extraction component of the framework includes spatial indexing, statistical and geometrical elements. Gaze estimation is constructed by feature aggregation and transforming features into a higher dimensional space using the RBF kernel and spread factor. The output of fused features through K-DMCCA is robust to illumination, occlusion and is calibration free. Our algorithm is validated on MPII, CAVE, ACS and EYEDIAP datasets. The two main contributions of the framework are the following: Enhancing the performance of DMCCA with the kernel and introducing quadtree as an iris region descriptor. Spatial indexing using quadtree is a robust method for detecting which quadrant the iris is situated, detecting the iris boundary and it is inclusive of statistical and geometrical indexing that are calibration free. Our method achieved an accurate gaze estimation of 4.8ºusing Cave, 4.6° using MPII, 5.1ºusing ACS and 5.9° using EYEDIAP datasets respectively. The proposed framework provides insight into the methodology of multi-feature fusion for gaze estimation. © 2018 IEEE","Gaze; Head pose; Kernel-DMCCA; Pupil; Quadtree","Calibration; Caves; Gaze; Head pose; Kernel-DMCCA; Pupil; Quad trees; Indexing (of information)",Conference Paper,"Final","",Scopus,2-s2.0-85061667099
"Wang Z., Liu H.","57205198429;54958434200;","A preliminary visual system for assistant diagnosis of ASD",2019,"Proceedings of the 2018 25th International Conference on Mechatronics and Machine Vision in Practice, M2VIP 2018",,,"8600906","","",,,"10.1109/M2VIP.2018.8600906","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061772801&doi=10.1109%2fM2VIP.2018.8600906&partnerID=40&md5=a556fe89dc832ce354d574511aad72b2","State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China","Wang, Z., State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China; Liu, H., State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China","Autism spectrum disorder(ASD) is a kind of developmental disorder which attracted a lot of attention of researchers for its urgency and pervasiveness. The diagnosis and intervention of ASD is still complicated and hard to handle. The rapid development of technology has brought new methods to the auxiliary diagnosis of ASD, such as face detection, gaze estimation, action recognition, etc. The paper proposed a preliminary visual system for assistant diagnosis of ASD in a core clinical testing scenario-response to name. The eye center localization and gaze estimation were applied to measure the responses of the subject. The purpose of this paper is analyzing the feasibility of this system, and optimizing the sensing structure and the evaluation indicator. © 2018 IEEE.","ASD; gaze estimation; Response to name","Face recognition; Action recognition; Autism spectrum disorders; Clinical testing; Developmental disorders; Evaluation indicators; Gaze estimation; Response to name; Visual systems; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85061772801
"Zhang Z., Yu C., Crandall D.","57201864135;16032623800;8732077700;","A self validation network for object-level human attention estimation",2019,"Advances in Neural Information Processing Systems","32",,,"","",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090169513&partnerID=40&md5=481329b6006ee90ae353dc445b9922f6","Luddy School of Informatics, Computing, and Engineering; Department of Psychological and Brain Sciences, Indiana University Bloomington, United States","Zhang, Z., Luddy School of Informatics, Computing, and Engineering; Yu, C., Department of Psychological and Brain Sciences, Indiana University Bloomington, United States; Crandall, D., Luddy School of Informatics, Computing, and Engineering","Due to the foveated nature of the human vision system, people can focus their visual attention on only a small region of their visual field at a time, which usually contains a single object. Estimating this object of attention in first-person (egocentric) videos is useful for many human-centered real-world applications such as augmented reality and driver assistance systems. A straightforward solution for this problem is to first estimate the gaze with a traditional gaze estimator and generate object candidates from an off-the-shelf object detector, and then pick the object that the estimated gaze falls in. However, such an approach can fail because it addresses the where and the what problems separately, despite that they are highly related, chicken-and-egg problems. In this paper, we propose a novel unified model that incorporates both spatial and temporal evidence in identifying as well as locating the attended object in first-person videos. It introduces a novel Self Validation Module that enforces and leverages consistency of the where and the what concepts. We evaluate on two public datasets, demonstrating that the Self Validation Module significantly benefits both training and testing and that our model outperforms the state-of-the-art. © 2019 Neural information processing systems foundation. All rights reserved.",,"Augmented reality; Automobile drivers; Behavioral research; Driver assistance system; Human vision systems; Object detectors; Self validations; State of the art; Training and testing; Unified Modeling; Visual Attention; Object detection",Conference Paper,"Final","",Scopus,2-s2.0-85090169513
"Luo M., Liu X., Wang W., Huang W.","57206482771;57209845642;56948398100;56195325600;","Improved capsule network for gaze estimation in wireless sensor networks",2019,"Proceedings of the 12th EAI International Conference on Mobile Multimedia Communications, MOBIMEDIA 2019",,,,"307","326",,,"10.4108/eai.29-6-2019.2282839","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088231435&doi=10.4108%2feai.29-6-2019.2282839&partnerID=40&md5=e19ef3c672ec5ad449f5be0c591758e6","School of Information Engineering, Nanchang University, Nanchang, China; School of Economics and Management, Chang’an University, Xi’an, China","Luo, M., School of Information Engineering, Nanchang University, Nanchang, China; Liu, X., School of Information Engineering, Nanchang University, Nanchang, China; Wang, W., School of Economics and Management, Chang’an University, Xi’an, China; Huang, W., School of Information Engineering, Nanchang University, Nanchang, China","In this study, aiming at the problem of gaze estimation in the wireless sensor network in the car, we use image-based method to estimate gaze based on the single camera sensor. We use the deep learning model and propose the improved model from three aspects based on the original capsule network. The first is to increase the convolution layer, the second is to increase the capsule layer, and the third is to widen the capsule layer in the network. Through many contrast experiments, it is proved that the appropriate use of the first or second improved method can achieve performance over other comparison models, and the prediction results of gaze estimation are almost no different from the real gaze direction. © 2019 EAI.","Capsule Network; Gaze estimation; Multi-layer Capsule Network","Deep learning; Multimedia systems; Network layers; Comparison models; Contrast experiment; Gaze direction; Gaze estimation; Image-based methods; Learning models; Single cameras; Wireless sensor networks",Conference Paper,"Final","",Scopus,2-s2.0-85088231435
"Pichitwong W., Chamnongthai K.","57191333091;57202765861;","An eye-tracker-based 3D point-of-gaze estimation method using head movement",2019,"IEEE Access","7",,"8764337","99086","99098",,2,"10.1109/ACCESS.2019.2929195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084926489&doi=10.1109%2fACCESS.2019.2929195&partnerID=40&md5=fb008a16311ea8d0e5c61ac9d1d58804","Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, 10140, Thailand","Pichitwong, W., Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, 10140, Thailand; Chamnongthai, K., Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, 10140, Thailand","Eye trackers are currently used to sense the positions of both the centers of the pupils and the point-of-gaze (POG) position on a screen, in keeping with the original objective for which they were designed; however, it remains difficult to measure the positions of three-dimensional (3D) POGs. This paper proposes a method for 3D gaze estimation by using head movement, pupil position data, and POGs on a screen. The method assumes that a person, usually unintentionally, moves his or her head a short distance such that multiple straight lines can be drawn from the center point between the two pupils to the POG. When the person is continuously focusing on a given 3D POG while moving, these lines represent the lines of sight that intersect at a 3D POG . That 3D POG can, therefore, be found from the intersection of several lines of sight formed by head movements. To evaluate the performance of the proposed method, experimental equipment was constructed, and experiments with five male and five female participants were performed in which the participants looked at nine test points in a 3D space for approximately 20 s each. The experimental results reveal that the proposed method can measure 3D POGs with average distance errors of 13.36 cm, 7.58 cm, 5.72 cm, 3.97 cm, and 3.52 cm for head movement distances of 1 cm, 2 cm, 3 cm, 4 cm, and 5 cm, respectively. © 2019 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","3D gaze estimation; Eye tracker; Gaze tracking","Eye movements; Average Distance; Experimental equipments; Gaze estimation; Head movements; Lines-of-sight; Point of gaze; Position data; Threedimensional (3-d); Eye tracking",Article,"Final","",Scopus,2-s2.0-85084926489
"Palmero C., Selva J., Bagheri M.A., Escalera S.","57188829002;57225366949;57139919400;22634035000;","Recurrent CNN for 3D gaze estimation using appearance and shape cues",2019,"British Machine Vision Conference 2018, BMVC 2018",,,,"","",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084012458&partnerID=40&md5=15c1ac9116bcb0a38ee15bd698b0e258","Dept. Mathematics and Informatics Universitat de Barcelona, Spain; Computer Vision Center Campus UAB, Bellaterra, Spain; Dept. Electrical and Computer Eng, University of Calgary, Canada; Dept. Engineering University of Larestan, Iran","Palmero, C., Dept. Mathematics and Informatics Universitat de Barcelona, Spain, Computer Vision Center Campus UAB, Bellaterra, Spain; Selva, J., Dept. Mathematics and Informatics Universitat de Barcelona, Spain; Bagheri, M.A., Dept. Electrical and Computer Eng, University of Calgary, Canada, Dept. Engineering University of Larestan, Iran; Escalera, S., Dept. Mathematics and Informatics Universitat de Barcelona, Spain, Computer Vision Center Campus UAB, Bellaterra, Spain","Gaze behavior is an important non-verbal cue in social signal processing and human-computer interaction. In this paper, we tackle the problem of person- and head pose-independent 3D gaze estimation from remote cameras, using a multi-modal recurrent convolutional neural network (CNN). We propose to combine face, eyes region, and face landmarks as individual streams in a CNN to estimate gaze in still images. Then, we exploit the dynamic nature of gaze by feeding the learned features of all the frames in a sequence to a many-to-one recurrent module that predicts the 3D gaze vector of the last frame. Our multi-modal static solution is evaluated on a wide range of head poses and gaze directions, achieving a significant improvement of 14.6% over the state of the art on EYEDIAP dataset, further improved by 4% when the temporal modality is included. © 2018. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.",,"Computer vision; Human computer interaction; 3D gaze vectors; Convolutional neural network; Gaze direction; Gaze estimation; Social signal processing; State of the art; Static solutions; Temporal modalities; Recurrent neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85084012458
"Liu G., Yu Y., Funes-Mora K.A., Odobez J.-M.","56420692700;57188644020;55336423200;57203103085;","A differential approach for gaze estimation with calibration",2019,"British Machine Vision Conference 2018, BMVC 2018",,,,"","",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084012365&partnerID=40&md5=3472b1ebe6878b41a5ded8ce89a26208","Idiap Research Institute, Switzerland; Eyeware Tech SA","Liu, G., Idiap Research Institute, Switzerland; Yu, Y., Idiap Research Institute, Switzerland; Funes-Mora, K.A., Eyeware Tech SA; Odobez, J.-M., Idiap Research Institute, Switzerland","Gaze estimation methods usually regress gaze directions directly from a single face or eye image. However, due to important variabilities in eye shapes and inner eye structures amongst individuals, universal models obtain limited accuracies and their output usually exhibit high variance as well as biases which are subject dependent. Therefore, increasing accuracy is usually done through calibration, allowing gaze predictions for a subject to be mapped to his/her specific gaze. In this paper, we introduce a novel image differential method for gaze estimation. We propose to directly train a convolutional neural network to predict the gaze differences between two eye input images of the same subject. Then, given a set of subject specific calibration images, we can use the inferred differences to predict the gaze direction of a novel eye sample. The assumption is that by allowing the comparison between two eye images, annoyance factors (alignment, eyelid closing, illumination perturbations) which usually plague single image prediction methods can be much reduced, allowing better prediction altogether. Experiments on 3 public datasets validate our approach which constantly outperforms state-of-the-art methods even when followed by subject specific gaze adaptation. © 2018. The copyright of this document resides with its authors.",,"Calibration; Computer vision; Neural networks; Convolutional neural network; Differential approach; Differential methods; Gaze direction; Gaze estimation; State-of-the-art methods; Subject-specific; Universal model; Forecasting",Conference Paper,"Final","",Scopus,2-s2.0-85084012365
"Lian D., Zhang Z., Luo W., Hu L., Wu M., Li Z., Yu J., Gao S.","57203743979;57204289144;57206747739;57203744059;57207758944;36731281100;8569656400;35224747100;","RGBD based gaze estimation via multi-task CNN",2019,"33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",,,,"2488","2495",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083883072&partnerID=40&md5=749d3864ba090e5b3a101c86731a3312","ShanghaiTech University, China; Nanjing University of Science and Technology, China","Lian, D., ShanghaiTech University, China; Zhang, Z., ShanghaiTech University, China; Luo, W., ShanghaiTech University, China; Hu, L., ShanghaiTech University, China; Wu, M., ShanghaiTech University, China; Li, Z., Nanjing University of Science and Technology, China; Yu, J., ShanghaiTech University, China; Gao, S., ShanghaiTech University, China","This paper tackles RGBD based gaze estimation with Convolutional Neural Networks (CNNs). Specifically, we propose to decompose gaze point estimation into eyeball pose, head pose, and 3D eye position estimation. Compared with RGB image-based gaze tracking, having depth modality helps to facilitate head pose estimation and 3D eye position estimation. The captured depth image, however, usually contains noise and black holes which noticeably hamper gaze tracking. Thus we propose a CNN-based multi-task learning framework to simultaneously refine depth images and predict gaze points. We utilize a generator network for depth image generation with a Generative Neural Network (GAN), where the generator network is partially shared by both the gaze tracking network and GAN-based depth synthesizing. By optimizing the whole network simultaneously, depth image synthesis improves gaze point estimation and vice versa. Since the only existing RGBD dataset (EYEDIAP) is too small, we build a large-scale RGBD gaze tracking dataset for performance evaluation. As far as we know, it is the largest RGBD gaze dataset in terms of the number of participants. Comprehensive experiments demonstrate that our method outperforms existing methods by a large margin on both our dataset and the EYEDIAP dataset. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org).",,"Convolutional neural networks; Image enhancement; Large dataset; Multi-task learning; Black holes; Depth image; Eye position; Gaze estimation; Gaze point estimations; Gaze tracking; Head Pose Estimation; Large margins; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85083883072
"Li W., Dong Q., Jia H., Zhao S., Wang Y., Xie L., Pan Q., Duan F., Liu T.","55657973600;57190214381;57213687667;56014643800;57213689160;55238809400;57213688492;24537140100;57203377182;","Training a camera to perform long-distance eye tracking by another eye-tracker",2019,"IEEE Access","7",,"8880636","155313","155324",,5,"10.1109/ACCESS.2019.2949150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077956670&doi=10.1109%2fACCESS.2019.2949150&partnerID=40&md5=d35b587d72b4dea2035a85dc0e17af07","Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, 300350, China; Cortical Architecture Imaging and Discovery Lab, University of Georgia, Athens, GA  30602, United States; School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Department of Instrument Science and Technology, Zhejiang University, Hangzhou, 310027, China; Xi'an ISoftStone Network Technology Company Ltd., Xi'an, 710100, China","Li, W., Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, 300350, China; Dong, Q., Cortical Architecture Imaging and Discovery Lab, University of Georgia, Athens, GA  30602, United States; Jia, H., Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, 300350, China; Zhao, S., School of Automation, Northwestern Polytechnical University, Xi'an, 710072, China; Wang, Y., Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, 300350, China; Xie, L., Department of Instrument Science and Technology, Zhejiang University, Hangzhou, 310027, China; Pan, Q., Xi'an ISoftStone Network Technology Company Ltd., Xi'an, 710100, China; Duan, F., Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, 300350, China; Liu, T., Cortical Architecture Imaging and Discovery Lab, University of Georgia, Athens, GA  30602, United States","Appearance-based gaze estimation techniques have been greatly advanced in these years. However, using a single camera for appearance-based gaze estimation has been limited to short distance in previous studies. In addition, labeling of training samples has been a time-consuming and unfriendly step in previous appearance-based gaze estimation studies. To bridge these significant gaps, this paper presents a new long-distance gaze estimation paradigm: Train a camera to perform eye tracking by another eye tracker, named Learning-based Single Camera eye tracker (LSC eye-tracker). In the training stage, the LSC eye-tracker simultaneously acquired gaze data by a commercial trainer eye tracker and face appearance images by a long-distance trainee camera, based on which deep convolutional neural network (CNN) models are utilized to learn the mapping from appearance images to gazes. In the application stage, the LSC eye-tracker works alone to predict gazes based on the acquired appearance images by the single camera and the trained CNN models. Our experimental results show that the LSC eye-tracker enables both population-based eye tracking and personalized eye tracking with promising accuracy and performance. © 2013 IEEE.","Eye tracking; gaze estimation; human-computer interaction; machine learning","Cameras; Deep neural networks; Human computer interaction; Learning systems; Neural networks; Appearance based; CNN models; Convolutional neural network; Eye trackers; Gaze estimation; Single cameras; Training sample; Eye tracking",Article,"Final","",Scopus,2-s2.0-85077956670
"Chen H.-H., Hwang B.-J., Hsu W.-H., Kao C.-W., Chen W.-T.","37035935400;7201453946;17434693900;53984348300;57212604646;","Focus on Area Tracking Based on Deep Learning for Multiple Users",2019,"IEEE Access","7",,"8918265","179477","179491",,,"10.1109/ACCESS.2019.2956953","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077187881&doi=10.1109%2fACCESS.2019.2956953&partnerID=40&md5=227dff81a68d4857b8c9487c6794b3a0","Department of Computer and Communication Engineering, Ming Chuan University, Taipei, 111, Taiwan; Department of Information Management, National Taichung University of Science and Technology, Taichung, 404, Taiwan; Department of Applied Information Technology, Hsing Wu University of Science and Technology, Taipei, 24452, Taiwan; Department of Computer Science and Information Engineering, National Central University, Taoyuan, 320, Taiwan","Chen, H.-H., Department of Computer and Communication Engineering, Ming Chuan University, Taipei, 111, Taiwan; Hwang, B.-J., Department of Computer and Communication Engineering, Ming Chuan University, Taipei, 111, Taiwan; Hsu, W.-H., Department of Information Management, National Taichung University of Science and Technology, Taichung, 404, Taiwan; Kao, C.-W., Department of Applied Information Technology, Hsing Wu University of Science and Technology, Taipei, 24452, Taiwan, Department of Computer Science and Information Engineering, National Central University, Taoyuan, 320, Taiwan; Chen, W.-T., Department of Computer and Communication Engineering, Ming Chuan University, Taipei, 111, Taiwan","Most eye-tracking experiments are limited to single subjects because gaze points are difficult to track when multiple users are involved and environmental factors might cause interference. To overcome this problem, this paper proposes a method for gaze tracking that can be applied for multiple users simultaneously. Four models, including FASEM, FAEM and FAFRCM in the signal-user environment, as well as FAEM and FAMAM in the multiple-user environment, are proposed, and we collected raw data of gazing behaviors to train the models. Through a modified VGG19 architecture and adjusting the Number of Convolutional Layers (NoCL), we obtained and compared the accuracy of various models to determine the most suitable architecture. Since data for multiple-users is not easy to obtain, in this paper, we trained the model first with single users, then extended it to multiple users with transfer learning. Finally, we propose an adaptive method to integrate the benefits of FAEM and FAMAM. © 2013 IEEE.","deep learning; gaze-tracking; Multiple users; position clustering","Eye tracking; Network-on-chip; Adaptive methods; Environmental factors; Gaze point; Gaze tracking; Multiple user; position clustering; Single users; Transfer learning; Deep learning",Article,"Final","",Scopus,2-s2.0-85077187881
"Cao Z., Wang G., Guo X.","57207471798;35231538600;25822445600;","Stage-by-stage based design paradigm of two-pathway model for gaze following",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11858 LNCS",,,"644","656",,1,"10.1007/978-3-030-31723-2_55","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076947157&doi=10.1007%2f978-3-030-31723-2_55&partnerID=40&md5=52548a0e65d249f1d1e9e7cc15d04c8f","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, 510006, China; Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Beijing, China","Cao, Z., School of Data and Computer Science, Sun Yat-sen University, Guangzhou, 510006, China, Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Beijing, China; Wang, G., School of Data and Computer Science, Sun Yat-sen University, Guangzhou, 510006, China, Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Beijing, China; Guo, X., School of Data and Computer Science, Sun Yat-sen University, Guangzhou, 510006, China, Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Beijing, China","Gaze, which is an important non-verbal cue of interactions between human beings, can be used to estimate a person’s point of regard as well as deduce his intention. And gaze following is an task to estimate the visual attention of people in a single image. To tackle this challenging problem, earlier state-of-the-art work try to combine the information from image saliency as well as the gaze directions of people, thus demonstrate a deep-learning based two-pathway model. However, previous work do not focus much on why such a two-pathway model works well. Thus, in this paper, we divide the two-pathway model into three stages, compare different mechanisms in those stages to better understand how each stage may influence the model performance. Finally, we find out the best combinations of the mechanism in three stages and evaluate the model on the benchmark GazeFollow. © Springer Nature Switzerland AG 2019.","Deep learning; Gaze following; Two-pathway model","Behavioral research; Computer vision; Different mechanisms; Gaze following; Image saliencies; Model performance; Point of regards; State of the art; Two-pathway model; Visual Attention; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85076947157
"Jarosz M., Nawrocki P., Placzkiewicz L., Sniezynski B., Zielinski M., Indurkhya B.","57212304491;24587357500;57202718933;23393638000;57212313480;6603120668;","Detecting gaze direction using robot-mounted and mobile-device cameras",2019,"Computer Science","20","4",,"455","476",,1,"10.7494/csci.2019.20.4.3435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076427641&doi=10.7494%2fcsci.2019.20.4.3435&partnerID=40&md5=8914fe279cf10738ae0b2ca0865a484c","AGH University of Science and Technology, Faculty of Computer Science, Electronics, and Telecommunications, Department of Computer Science, al. A. Mickiewicza 30, Krakow, 30-059, Poland; AGH University of Science and Technology, Faculty of Computer Science, Electronics, and Telecommunications, Department of Computer Science, al. A. Mickiewicza 30, Krakow, 30-059, Poland","Jarosz, M., AGH University of Science and Technology, Faculty of Computer Science, Electronics, and Telecommunications, Department of Computer Science, al. A. Mickiewicza 30, Krakow, 30-059, Poland; Nawrocki, P., AGH University of Science and Technology, Faculty of Computer Science, Electronics, and Telecommunications, Department of Computer Science, al. A. Mickiewicza 30, Krakow, 30-059, Poland; Placzkiewicz, L., AGH University of Science and Technology, Faculty of Computer Science, Electronics, and Telecommunications, Department of Computer Science, al. A. Mickiewicza 30, Krakow, 30-059, Poland; Sniezynski, B., AGH University of Science and Technology, Faculty of Computer Science, Electronics, and Telecommunications, Department of Computer Science, al. A. Mickiewicza 30, Krakow, 30-059, Poland; Zielinski, M., AGH University of Science and Technology, Faculty of Computer Science, Electronics, and Telecommunications, Department of Computer Science, al. A. Mickiewicza 30, Krakow, 30-059, Poland; Indurkhya, B., AGH University of Science and Technology, Faculty of Computer Science, Electronics, and Telecommunications, Department of Computer Science, al. A. Mickiewicza 30, Krakow, 30-059, Poland","Two common channels through which humans communicate are speech and gaze. Eye gaze is an important mode of communication: it allows people to better understand each others' intentions, desires, interests, and so on. The goal of this research is to develop a framework for gaze-triggered events that can be executed on a robot and mobile devices and allows experiments to be performed. We experimentally evaluate the framework and techniques for extracting gaze direction based on a robot-mounted camera or a mobile-device camera that are implemented in the framework. We investigate the impact of light on the accuracy of gaze estimation and also how the overall accuracy depends on user eye and head movements. Our research shows that light intensity is important and the placement of a light source is crucial. All of the robot-mounted gaze-detection modules we tested were found to be similar with regard to their accuracy. The framework we developed was tested in a human-robot interaction experiment involving a job-interview scenario. The exible structure of this scenario allowed us to test different components of the framework in varied real-world scenarios, which was very useful for progressing towards our long-term research goal of designing intuitive gaze-based interfaces for human-robot communication. © 2019 AGH University of Science and Technology Press.","Eye-tracking; Face detection; Gaze-direction detection; Mobile device; Robot",,Article,"Final","",Scopus,2-s2.0-85076427641
"Patra A., Cai Y., Chatelain P., Sharma H., Drukker L., Papageorghiou A.T., Noble J.A.","57195771466;57202376461;55891653300;56272987100;36241434600;57194082999;56185660000;","Efficient ultrasound image analysis models with sonographer gaze assisted distillation",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11767 LNCS",,,"394","402",,2,"10.1007/978-3-030-32251-9_43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075690650&doi=10.1007%2f978-3-030-32251-9_43&partnerID=40&md5=48fdff2f6f8aa3560fc7dff1ba24458e","University of Oxford, Oxford, OX3 7DQ, United Kingdom","Patra, A., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Cai, Y., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Chatelain, P., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Sharma, H., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Drukker, L., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Papageorghiou, A.T., University of Oxford, Oxford, OX3 7DQ, United Kingdom; Noble, J.A., University of Oxford, Oxford, OX3 7DQ, United Kingdom","Recent automated medical image analysis methods have attained state-of-the-art performance but have relied on memory and compute-intensive deep learning models. Reducing model size without significant loss in performance metrics is crucial for time and memory-efficient automated image-based decision-making. Traditional deep learning based image analysis only uses expert knowledge in the form of manual annotations. Recently, there has been interest in introducing other forms of expert knowledge into deep learning architecture design. This is the approach considered in the paper where we propose to combine ultrasound video with point-of-gaze tracked for expert sonographers as they scan to train memory-efficient ultrasound image analysis models. Specifically we develop teacher-student knowledge transfer models for the exemplar task of frame classification for the fetal abdomen, head, and femur. The best performing memory-efficient models attain performance within 5% of conventional models that are 1000× larger in size. © Springer Nature Switzerland AG 2019.","Expert knowledge; Gaze tracking; Model compression","Decision making; Deep learning; Distillation; Distilleries; Eye tracking; Knowledge management; Medical computing; Medical imaging; Ultrasonics; Conventional models; Expert knowledge; Gaze tracking; Learning architectures; Model compression; Performance metrics; State-of-the-art performance; Ultrasound image analysis; Image analysis",Conference Paper,"Final","",Scopus,2-s2.0-85075690650
"Luo M., Liu X., Huang W.","57206482771;57209845642;56195325600;","Gaze Estimation Based on Neural Network",2019,"Proceedings of 2019 IEEE 2nd International Conference on Electronic Information and Communication Technology, ICEICT 2019",,,"8846281","590","594",,1,"10.1109/ICEICT.2019.8846281","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074977221&doi=10.1109%2fICEICT.2019.8846281&partnerID=40&md5=d9ed159fdb53ce6a1c8ae3cd98686ad0","Nanchang University, School of Information Engineering, Nanchang, China","Luo, M., Nanchang University, School of Information Engineering, Nanchang, China; Liu, X., Nanchang University, School of Information Engineering, Nanchang, China; Huang, W., Nanchang University, School of Information Engineering, Nanchang, China","In this study, we selected some state-of-the-art or classic neural network models for recently popular research problem gaze estimation. Experimental analysis based on eyegaze data set compares different models and suggests that neural networks can achieve excellent performance in gaze estimation problem while original Capsule Network provides the best performance and 18-layers ResNet is second-best in all models. © 2019 IEEE.","Capsule; Gaze estimation; Neural network; ResNet","Capsule; Experimental analysis; Eye-gaze; Gaze estimation; Neural network model; Research problems; ResNet; State of the art; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85074977221
"Lyudvichenko V.A., Vatolin D.S.","14067590000;6506528788;","Predicting video saliency using crowdsourced mouse-tracking data",2019,"CEUR Workshop Proceedings","2485",,,"127","130",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074734896&partnerID=40&md5=5f8a764e6262cb58cccf93474185eeba","Lomonosov Moscow State University, Moscow, Russian Federation","Lyudvichenko, V.A., Lomonosov Moscow State University, Moscow, Russian Federation; Vatolin, D.S., Lomonosov Moscow State University, Moscow, Russian Federation","This paper presents a new way of getting high-quality saliency maps for video, using a cheaper alternative to eye-tracking data. We designed a mouse-contingent video viewing system which simulates the viewers’ peripheral vision based on the position of the mouse cursor. The system enables the use of mouse-tracking data recorded from an ordinary computer mouse as an alternative to real gaze fixations recorded by a more expensive eye-tracker. We developed a crowdsourcing system that enables the collection of such mouse-tracking data at large scale. Using the collected mouse-tracking data we showed that it can serve as an approximation of eye-tracking data. Moreover, trying to increase the efficiency of collected mouse-tracking data we proposed a novel deep neural network algorithm that improves the quality of mouse-tracking saliency maps. Copyright © 2019 for this paper by its authors.","Crowdsourcing; Deep learning; Eye tracking; Mouse tracking; Saliency; Visual attention","Behavioral research; Computer graphics; Computer vision; Crowdsourcing; Deep learning; Deep neural networks; Eye tracking; Computer mouse; Neural network algorithm; Peripheral vision; Saliency; Tracking data; Video saliencies; Viewing systems; Visual Attention; Mammals",Conference Paper,"Final","",Scopus,2-s2.0-85074734896
[无可用作者姓名],[无可用的作者 ID],"18th International Conference on Computer Analysis of Images and Patterns, CAIP 2019",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11679 LNCS",,,"","",1275,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072863329&partnerID=40&md5=f9ff6e1b89ae6308105170fc01516a52",,"","The proceedings contain 106 papers. The special focus in this conference is on Computer Analysis of Images and Patterns. The topics include: DeepNautilus: A Deep Learning Based System for Nautical Engines’ Live Vibration Processing; binary Code for the Compact Palmprint Representation Using Texture Features; handwriting Analysis to Support Alzheimer’s Disease Diagnosis: A Preliminary Study; geometrical and Statistical Properties of the Rational Order Pyramid Transform and Dilation Filtering; Personal Identity Verification by EEG-Based Network Representation on a Portable Device; a System for Controlling How Carefully Surgeons Are Cleaning Their Hands; class-Conditional Data Augmentation Applied to Image Classification; Fabric Classification and Matching Using CNN and Siamese Network for E-commerce; real-Time Style Transfer with Strength Control; analysis of Skill Improvement Process Based on Movement of Gaze and Hand in Assembly Task; faster Visual-Based Localization with Mobile-PoseNet; unsupervised Effectiveness Estimation Through Intersection of Ranking References; Joint Correlation Measurements for PRNU-Based Source Identification; detecting Sub-Image Replicas: Retrieval and Localization of Zoomed-In Images; homogeneity Index as Stopping Criterion for Anisotropic Diffusion Filter; adaptive Image Binarization Based on Multi-layered Stack of Regions; Evaluating Impacts of Motion Correction on Deep Learning Approaches for Breast DCE-MRI Segmentation and Classification; a Two-Step System Based on Deep Transfer Learning for Writer Identification in Medieval Books; restoration of Colour Images Using Backward Stochastic Differential Equations with Reflection; sound Transformation: Applying Image Neural Style Transfer Networks to Audio Spectograms; hybrid Function Sparse Representation Towards Image Super Resolution; timber Tracing with Multimodal Encoder-Decoder Networks; a Challenging Voice Dataset for Robotic Applications in Noisy Environments.",,,Conference Review,"Final","",Scopus,2-s2.0-85072863329
"Jiang J., Zhou X., Chan S., Chen S.","57211817476;55743240400;57188979700;24491760700;","Appearance-based gaze tracking: A brief review",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11745 LNAI",,,"629","640",,4,"10.1007/978-3-030-27529-7_53","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070721293&doi=10.1007%2f978-3-030-27529-7_53&partnerID=40&md5=c12c4495fefdfe42eeb9ff5515532bf9","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Electrical and Information Engineering, Quzhou University, Quzhou, China; School of Computer Communication and Engineering, Tianjin University of Technology, Tianjin, China","Jiang, J., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Zhou, X., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China, College of Electrical and Information Engineering, Quzhou University, Quzhou, China; Chan, S., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Chen, S., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China, School of Computer Communication and Engineering, Tianjin University of Technology, Tianjin, China","Human gaze tracking plays an important role in the field of Human-Computer Interaction. This paper presents a brief review on appearance-based gaze tracking. Based on the appearance of human eyes, input features can be classified into three categories according to the different ways of extracting human eyes features, namely, complete human eye image, pixel-based feature and 3D reconstruction image. The estimation process from human eye feature to fixation point mainly uses different mapping functions. In this paper, common mapping functions and related algorithms are described in detail: k-nearest neighbor (KNN), random forest (RF) regression, gaussian process (GP) regression, support vector machines (SVM) and artificial neural networks (ANN). This paper evaluates the performance of these gaze tracking algorithms using different mapping functions. Based on the results of the evaluation, potential challenges are summarized and the future directions of gaze estimation are prospected. © Springer Nature Switzerland AG 2019.","Appearance-based; Gaze tracking; HCI; Mapping","Decision trees; Human computer interaction; Mapping; Nearest neighbor search; Neural networks; Robotics; Support vector machines; 3-D reconstruction images; Appearance based; Estimation process; Gaussian process; Gaze tracking; K nearest neighbor (KNN); Mapping functions; Related algorithms; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85070721293
"Feidakis M., Rangoussi M., Kasnesis P., Patrikakis C., Kogias D.G., Charitopoulos A.","36998244700;6602664727;57044812500;8244299800;56896083200;36995957700;","Affective assessment in distance learning: A semi-explicit approach",2019,"International Journal of Technologies in Learning","26","1",,"19","34",,,"10.18848/2327-0144/CGP/V26I01/19-34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070311024&doi=10.18848%2f2327-0144%2fCGP%2fV26I01%2f19-34&partnerID=40&md5=8b7b589331a3df6dbaef581c3f5ecb9a","Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece","Feidakis, M., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece; Rangoussi, M., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece; Kasnesis, P., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece; Patrikakis, C., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece; Kogias, D.G., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece; Charitopoulos, A., Department of Electrical and Electronics Engineering, University of West Attica (UWA), Egaleo, Attica, Greece","Modern e-learning and distance learning systems suffer severe lack of affect-aware interaction: the typical system is irresponsive to the affective state of the user, while even an inadequate human tutor would respond to it and even adapt his/her instruction accordingly. The main goal of this paper is to describe a scenario that deploys state-of-theart technologies to ""sense"" or ""gauge"" the affective state of a remote class of learners while they participate in a distance learning course, either synchronous or asynchronous, and provide feedback to all stakeholders (individual learner, peers, class tutor) through intuitive, easy-to-grasp visualisations. Both semi-automated, smart (selfreporting/ explicit) solutions through gestures and fully automated (user-transparent/implicit) solutions are sought through fusion of a number of ""experts"" (monitored features of the learner) that feed a decision-making algorithm after suitable processing. © Common Ground Research Networks, Michalis Feidakis, Maria Rangoussi, Panagiotis Kasnesis, Charalampos Patrikakis, Dimitrios G. Kogias, Angelos Charitopoulos.","Affective; Affective Computing; Analytics; Artificial Intelligence; Assessment; Deep Learning; Distance Learning; Emotion; Eye Gaze; Feedback; Gesture Analysis; Learning; State",,Article,"Final","",Scopus,2-s2.0-85070311024
[无可用作者姓名],[无可用的作者 ID],"32nd International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2019",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11606 LNAI",,,"","",861,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068593562&partnerID=40&md5=1d8b853a9dcc336e0c3b5d327d0543c7",,"","The proceedings contain 73 papers. The special focus in this conference is on Industrial, Engineering and Other Applications of Applied Intelligent Systems. The topics include: Hydropower optimization using deep learning; towards real-time head pose estimation: Exploring parameter-reduced residual networks on in-the-wild datasets; a rule-based smart control for fail-operational systems; autonomous monitoring of air quality through an unmanned aerial vehicle; intelligent parking management by means of capability oriented requirements engineering; neural network control system of motion of the robot in the environment with obstacles; On board autonomy operations for OPS-SAT experiment; Practical obstacle avoidance path planning for agriculture UAVs; A fault-driven combinatorial process for model evolution in XSS vulnerability detection; a taxonomy of event prediction methods; an efficient algorithm for deriving frequent itemsets from lossless condensed representation; discovering stable periodic-frequent patterns in transactional data; graphical event model learning and verification for security assessment; predicting user preference in pairwise comparisons based on emotions and gaze; a classification method of photos in a tourism website by color analysis; improving customer’s flow through data analytics; towards similarity-aware constraint-based recommendation; understand the buying behavior of E-shop customers through appropriate analytical methods; using conformal prediction for multi-label document classification in e-Mail support systems; a posteriori diagnosis of discrete-event systems with symptom dictionary and scenarios; infilling missing rainfall and runoff data for Sarawak, Malaysia using gaussian mixture model based K-nearest neighbor imputation; detecting fraudulent bookings of online travel agencies with unsupervised machine learning; learned constraint ordering for consistency based direct diagnosis; compressing and querying skypattern cubes.",,,Conference Review,"Final","",Scopus,2-s2.0-85068593562
[无可用作者姓名],[无可用的作者 ID],"14th Asian Conference on Computer Vision, ACCV 2018",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11367 LNCS",,,"","",4380,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068434299&partnerID=40&md5=baba8b8953d30a7450cabd5bcb90e067",,"","The proceedings contain 269 papers. The special focus in this conference is on Computer Vision. The topics include: Paying Attention to Style: Recognizing Photo Styles with Convolutional Attentional Units; E2E-MLT - An Unconstrained End-to-End Method for Multi-language Scene Text; an Invoice Reading System Using a Graph Convolutional Network; reading Industrial Inspection Sheets by Inferring Visual Relations; Learning to Clean: A GAN Perspective; deep Reader: Information Extraction from Document Images via Relation Extraction and Natural Language; simultaneous Recognition of Horizontal and Vertical Text in Natural Images; Automatic Retinal and Choroidal Boundary Segmentation in OCT Images Using Patch-Based Supervised Machine Learning Methods; Discrimination Ability of Glaucoma via DCNNs Models from Ultra-Wide Angle Fundus Images Comparing Either Full or Confined to the Optic Disc; synthesizing New Retinal Symptom Images by Multiple Generative Models; localizing the Gaze Target of a Crowd of People; retinal Detachment Screening with Ensembles of Neural Network Models; Recent Developments of Retinal Image Analysis in Alzheimer’s Disease and Potential AI Applications; intermediate Goals in Deep Learning for Retinal Image Analysis; Enhanced Detection of Referable Diabetic Retinopathy via DCNNs and Transfer Learning; Generative Adversarial Networks (GANs) for Retinal Fundus Image Synthesis; AI-based AMD Analysis: A Review of Recent Progress; artificial Intelligence Using Deep Learning in Classifying Side of the Eyes and Width of Field for Retinal Fundus Photographs; OCT Segmentation via Deep Learning: A Review of Recent Work; auto-classification of Retinal Diseases in the Limit of Sparse Data Using a Two-Streams Machine Learning Model; unconstrained Iris Segmentation Using Convolutional Neural Networks.",,,Conference Review,"Final","",Scopus,2-s2.0-85068434299
"Min W., Park K., Wiggins J., Mott B., Wiebe E., Boyer K.E., Lester J.","55790560900;57209640340;55790031200;57203231751;7005357155;57203215959;57203179695;","Predicting dialogue breakdown in conversational pedagogical agents with multimodal LSTMs",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11626 LNAI",,,"195","200",,5,"10.1007/978-3-030-23207-8_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068335512&doi=10.1007%2f978-3-030-23207-8_37&partnerID=40&md5=813401ee166721e6c751d9b8fbcbecc4","Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States; Department of Computer and Information Science and Engineering, University of Florida, Gainsville, FL  32601, United States","Min, W., Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States; Park, K., Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States; Wiggins, J., Department of Computer and Information Science and Engineering, University of Florida, Gainsville, FL  32601, United States; Mott, B., Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States; Wiebe, E., Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States; Boyer, K.E., Department of Computer and Information Science and Engineering, University of Florida, Gainsville, FL  32601, United States; Lester, J., Center for Educational Informatics, North Carolina State University, Raleigh, NC  27606, United States","Recent years have seen a growing interest in conversational pedagogical agents. However, creating robust dialogue managers for conversational pedagogical agents poses significant challenges. Agents’ misunderstandings and inappropriate responses may cause breakdowns in conversational flow, lead to breaches of trust in agent-student relationships, and negatively impact student learning. Dialogue breakdown detection (DBD) is the task of predicting whether an agent’s utterance will cause a breakdown in an ongoing conversation. A robust DBD framework can support enhanced user experiences by choosing more appropriate responses, while also offering a method to conduct error analyses and improve dialogue managers. This paper presents a multimodal deep learning-based DBD framework to predict breakdowns in student-agent conversations. We investigate this framework with dialogues between middle school students and a conversational pedagogical agent in a game-based learning environment. Results from a study with 92 middle school students demonstrate that multimodal long short-term memory network (LSTM)-based dialogue breakdown detectors incorporating eye gaze features achieve high predictive accuracies and recall rates, suggesting that multimodal detectors can play an important role in designing conversational pedagogical agents that effectively engage students in dialogue. © Springer Nature Switzerland AG 2019.","Conversational pedagogical agent; Dialogue breakdown detection; Gaze; Multimodal; Natural language processing","Computer aided instruction; Deep learning; Forecasting; Long short-term memory; Managers; Natural language processing systems; Speech recognition; Breakdown detection; Gaze; Multi-modal; NAtural language processing; Pedagogical agents; Students",Conference Paper,"Final","",Scopus,2-s2.0-85068335512
[无可用作者姓名],[无可用的作者 ID],"20th International Conference on Artificial Intelligence in Education, AIED 2019",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11626 LNAI",,,"","",973,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068309259&partnerID=40&md5=337f73ce1c0f81f3b87411483823bc81",,"","The proceedings contain 121 papers. The special focus in this conference is on Artificial Intelligence in Education. The topics include: Learners’ gaze behaviors and metacognitive judgments with an agent-based multimedia environment; online assessment of belief biases and their impact on the acceptance of fallacious reasoning; early dropout prediction for programming courses supported by online judges; developing a deep learning-based affect recognition system for young children; using exploratory data analysis to support implementation and improvement of education technology product; bayesian diagnosis tracing: Application of procedural misconceptions in knowledge tracing; analysis of gamification elements. A case study in a computer science course; towards adaptive worked-out examples in an intelligent tutoring system; orchestrating class discussion with collaborative kit-build concept mapping; identifying editor roles in argumentative writing from student revision histories; automating the categorization of learning activities, to help improve learning design; identifying the structure of students’ explanatory essays; A systematic approach for analyzing students’ computational modeling processes in C2STEM; intelligent tutoring system for negotiation skills Training; robot lecture for enhancing non-verbal behavior in lecture; design prompts for virtual reality in education; assessing and improving learning outcomes for power management experiments using cognitive graph; does choosing the concept on which to solve each practice problem in an adaptive tutor affect learning?; measuring content complexity of technical texts: Machine learning experiments; should Students use digital scratchpads? Impact of using a digital assistive tool on arithmetic problem-solving; degree curriculum contraction: A vector space approach; what does time tell? Tracing the forgetting curve using deep knowledge tracing.",,,Conference Review,"Final","",Scopus,2-s2.0-85068309259
[无可用作者姓名],[无可用的作者 ID],"15th International Work-Conference on Artificial Neural Networks, IWANN 2019",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11506 LNCS",,,"","",1853,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067433962&partnerID=40&md5=5155633ffb206b118daa6874eca3045a",,"","The proceedings contain 150 papers. The special focus in this conference is on Artificial Neural Networks. The topics include: The Generalized Sleep Spindles Detector: A Generative Model Approach on Single-Channel EEGs; deepTrace: A Generic Framework for Time Series Forecasting; automatic Identification of Interictal Epileptiform Discharges with the Use of Complex Networks; anomaly Detection for Bivariate Signals; a Scalable Long-Horizon Forecasting of Building Electricity Consumption; long-Term Forecasting of Heterogenous Variables with Automatic Algorithm Selection; Automatic Time Series Forecasting with GRNN: A Comparison with Other Models; improving Online Handwriting Text/Non-text Classification Accuracy Under Condition of Stroke Context Absence; Improving Classification of Ultra-High Energy Cosmic Rays Using Spacial Locality by Means of a Convolutional DNN; boosting Wavelet Neural Networks Using Evolutionary Algorithms for Short-Term Wind Speed Time Series Forecasting; model and Feature Aggregation Based Federated Learning for Multi-sensor Time Series Trend Following; robust Echo State Network for Recursive System Identification; random Hyper-parameter Search-Based Deep Neural Network for Power Consumption Forecasting; A First Approximation to the Effects of Classical Time Series Preprocessing Methods on LSTM Accuracy; detecting Driver Drowsiness in Real Time Through Deep Learning Based Object Detection; the Influence of Human Walking Activities on the Doppler Characteristics of Non-stationary Indoor Channel Models; a Neural Network for Stance Phase Detection in Smart Cane Users; closed-Eye Gaze Gestures: Detection and Recognition of Closed-Eye Movements with Cameras in Smart Glasses; RF-Based Human Activity Recognition: A Non-stationary Channel Model Incorporating the Impact of Phase Distortions; improving Wearable Activity Recognition via Fusion of Multiple Equally-Sized Data Subwindows.",,,Conference Review,"Final","",Scopus,2-s2.0-85067433962
[无可用作者姓名],[无可用的作者 ID],"14th Asian Conference on Computer Vision, ACCV 2018",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11366 LNCS",,,"","",4380,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066961321&partnerID=40&md5=89a7ea56dbc42cc329f3149a4ca244a8",,"","The proceedings contain 269 papers. The special focus in this conference is on Computer Vision. The topics include: Style Transfer with Adversarial Learning for Cross-Dataset Person Re-identification; automatic Graphics Program Generation Using Attention-Based Hierarchical Decoder; occlusion Aware Stereo Matching via Cooperative Unsupervised Learning; background Subtraction Based on Fusion of Color and Local Patterns; a Coded Aperture for Watermark Extraction from Defocused Images; towards Locally Consistent Object Counting with Constrained Multi-stage Convolutional Neural Networks; an Improved Learning Framework for Covariant Local Feature Detection; better Guider Predicts Future Better: Difference Guided Generative Adversarial Networks; guided Feature Selection for Deep Visual Odometry; common Self-polar Triangle of Concentric Conics for Light Field Camera Calibration; appearance-Based Gaze Estimation Using Dilated-Convolutions; deep Clustering and Block Hashing Network for Face Image Retrieval; learning Energy Based Inpainting for Optical Flow; learning Background Subtraction by Video Synthesis and Multi-scale Recurrent Networks; universal Bounding Box Regression and Its Applications; continuous-Time Stereo Visual Odometry Based on Dynamics Model; a2A: Attention to Attention Reasoning for Movie Question Answering; TraMNet - Transition Matrix Network for Efficient Action Tube Proposals; shape-Conditioned Image Generation by Learning Latent Appearance Representation from Unpaired Data; dense In Dense: Training Segmentation from Scratch; CubemapSLAM: A Piecewise-Pinhole Monocular Fisheye SLAM System; Single Image Super-Resolution Using Lightweight CNN with Maxout Units; AVID: Adversarial Visual Irregularity Detection; localization-Aware Active Learning for Object Detection; deep Inverse Halftoning via Progressively Residual Learning; dynamic Random Walk for Superpixel Segmentation; BAN: Focusing on Boundary Context for Object Detection; rethinking Planar Homography Estimation Using Perspective Fields.",,,Conference Review,"Final","",Scopus,2-s2.0-85066961321
"Chen Z., Shi B.E.","56808413900;7402547071;","Appearance-Based Gaze Estimation Using Dilated-Convolutions",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11366 LNCS",,,"309","324",,6,"10.1007/978-3-030-20876-9_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066949725&doi=10.1007%2f978-3-030-20876-9_20&partnerID=40&md5=0f7c751e25d068f3df4b284b1949a027","The Hong Kong University of Science and Technology, Kowloon, Hong Kong","Chen, Z., The Hong Kong University of Science and Technology, Kowloon, Hong Kong; Shi, B.E., The Hong Kong University of Science and Technology, Kowloon, Hong Kong","Appearance-based gaze estimation has attracted more and more attention because of its wide range of applications. The use of deep convolutional neural networks has improved the accuracy significantly. In order to improve the estimation accuracy further, we focus on extracting better features from eye images. Relatively large changes in gaze angles may result in relatively small changes in eye appearance. We argue that current architectures for gaze estimation may not be able to capture such small changes, as they apply multiple pooling layers or other downsampling layers so that the spatial resolution of the high-level layers is reduced significantly. To evaluate whether the use of features extracted at high resolution can benefit gaze estimation, we adopt dilated-convolutions to extract high-level features without reducing spatial resolution. In cross-subject experiments on the Columbia Gaze dataset for eye contact detection and the MPIIGaze dataset for 3D gaze vector regression, the resulting Dilated-Nets achieve significant (upÂ to 20.8%) gains when compared to similar networks without dilated-convolutions. Our proposed Dilated-Net achieves state-of-the-art results on both the Columbia Gaze and the MPIIGaze datasets. © 2019, Springer Nature Switzerland AG.","Appearance-based gaze estimation; Dilated-convolutions","Computer vision; Deep neural networks; Image enhancement; Image resolution; Neural networks; Appearance based; Convolutional neural network; Gaze estimation; High resolution; High-level features; Spatial resolution; State of the art; Subject experiment; Convolution",Conference Paper,"Final","",Scopus,2-s2.0-85066949725
"Wang Z., Liu J., Liu H.","57205198429;57211670942;54958434200;","A Preliminary Visual System for Assistant Diagnosis of ASD: Response to Name",2019,"Communications in Computer and Information Science","1005",,,"76","86",,1,"10.1007/978-981-13-7983-3_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065796543&doi=10.1007%2f978-981-13-7983-3_7&partnerID=40&md5=a1643e9b7aa04dfe8188b127f54b1e57","State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China","Wang, Z., State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China; Liu, J., State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China; Liu, H., State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China","Autism spectrum disorder (ASD) is a kind of developmental disorder which attracted a lot of attention for its urgency and pervasiveness. The rapid development of machine vision technologies have brought new ideas to the auxiliary diagnosis of ASD, such as face detection, gaze estimation, action recognition, etc. The paper proposed a preliminary visual system for assistant diagnosis of ASD in a core clinical testing scenario-Response to Name (NTR). The eye center localization and gaze estimation were applied to measure the responses of the subjects. The main contribution of the article is that an experimental paradigm was established from a visual engineering perspective. The results showed that this system could analyze the response of the child for NTR accurately. &#x00A9; 2019, Springer Nature Singapore Pte Ltd.","ASD; Gaze estimation; Response to Name","Face recognition; Action recognition; Autism spectrum disorders; Clinical testing; Developmental disorders; Engineering perspective; Gaze estimation; Machine vision technologies; Response to Name; Cognitive systems",Conference Paper,"Final","",Scopus,2-s2.0-85065796543
"Chinsatit W., Kondo N., Saitoh T.","57200653220;57201279795;13006364300;","Character input system based on gaze input using wearable camera",2019,"Proceedings of SPIE - The International Society for Optical Engineering","11069",,"1106930","","",,,"10.1117/12.2524411","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065789895&doi=10.1117%2f12.2524411&partnerID=40&md5=273e752e9bb25be27f0ccc08f117f9b7","Kyushu Institute of Technology, 680-4 Kawazu, Iizuka, Fukuoka, 820-8502, Japan","Chinsatit, W., Kyushu Institute of Technology, 680-4 Kawazu, Iizuka, Fukuoka, 820-8502, Japan; Kondo, N., Kyushu Institute of Technology, 680-4 Kawazu, Iizuka, Fukuoka, 820-8502, Japan; Saitoh, T., Kyushu Institute of Technology, 680-4 Kawazu, Iizuka, Fukuoka, 820-8502, Japan","This research presents the character input system by using wearable camera based gaze estimation system (GES) for disorders. The GES is the system that can estimate the user's gaze direction. The proposed system uses the inside-out camera to get the user's gaze image. This system uses the gaze image to classify the character and create the word. Proposed system consists of two processes: Gazes estimation process and character recognition process. This research uses the Convolutional Neural Network (CNN) method to archive the good accuracy. By using the proposed method to train the model, the system was confirmed through evaluation experiments. The experimental results show the good accuracy of the proposed system compared with the previous research. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Character input system; CNN; Gaze estimation","Cameras; Character recognition; Neural networks; Wearable technology; Convolutional neural network; Estimation process; Evaluation experiments; Gaze direction; Gaze estimation; Input systems; System use; Wearable cameras; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85065789895
"Yoo S., Jeong D.K., Jang Y.","57192084306;57205652159;36152811100;","The Study of a Classification Technique for Numeric Gaze-Writing Entry in Hands-Free Interface",2019,"IEEE Access","7",,"6287639","49125","49134",,,"10.1109/ACCESS.2019.2909573","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065069026&doi=10.1109%2fACCESS.2019.2909573&partnerID=40&md5=10a3848e6e7ad0290dd9a08ea2e7ea3f","Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea","Yoo, S., Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea; Jeong, D.K., Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea; Jang, Y., Department of Computer Engineering, Sejong University, Seoul, 05006, South Korea","Recently, many applications are developed in numerous domains with various environments. Since some environments require hands-free applications, new technology is needed for the input interfaces other than the mouse and keyboard. Therefore, to meet the needs, many researchers have begun to investigate the gaze and voice for the input technology. In particular, there are many approaches to render virtual keyboards with the gaze. However, since the virtual keyboards hide the screen space, this technique can only be applied in limited environments. In this paper, we propose a classification technique for gaze-written numbers as the hands-free interface. Since the gaze-writing is less accurate compared to the virtual keyboard typing, we apply the convolutional neural network (CNN) deep learning algorithm to recognize the gaze-writing and improve the classification accuracy. Besides, we create new gaze-writing datasets for training, gaze MNIST (gMNIST), by modifying the MNIST data with features of the gaze movement patterns. For the evaluation, we compare our approach with the basic CNN structures using the original MNIST dataset. Our study will allow us to have more options for the input interfaces and expand our choices in hands-free environments. © 2013 IEEE.","Eye tracking; Gaze-writing; Input technique; Machine learning; MNIST","Computer keyboards; Deep learning; Learning algorithms; Learning systems; Mammals; Neural networks; Classification accuracy; Classification technique; Convolutional neural network; Gaze movements; Input interface; Input techniques; MNIST; Virtual Keyboards; Eye tracking",Article,"Final","",Scopus,2-s2.0-85065069026
"Nakata Y., Tabiraki K., Nakata T.","57208125073;57208134624;35305504000;","Development of head direction measuring system using ultrasound for environmental education",2019,"Proceedings of SPIE - The International Society for Optical Engineering","11049",,"1104928","","",,,"10.1117/12.2522028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063894689&doi=10.1117%2f12.2522028&partnerID=40&md5=7e08fc7b763234c9f9bbb23d36601502","Toyama Prefectural University, Information Systems Engineering, Toyama, Japan; Matsumoto University, Tourism Hospitality, Nagano, Japan","Nakata, Y., Toyama Prefectural University, Information Systems Engineering, Toyama, Japan; Tabiraki, K., Matsumoto University, Tourism Hospitality, Nagano, Japan; Nakata, T., Toyama Prefectural University, Information Systems Engineering, Toyama, Japan","Environmental education is educational activities using nature. Since the importance of environmental education is increasing, development of effective leaders training methods and learning programs is required. We consider that by using learner's interests during educational activities for learning evaluation, we can improve the teaching methods of instructors and learning programs. In this research, we use gaze directions to measure interests. In outdoor such as environmental education, it is difficult to apply gaze estimation techniques using conventional cameras due to the effects of sunlight and fog. In addition, since the wearable device imposes a physical burden on the learner, it has a problem that natural state data cannot be measured. we attach a compact and lightweight device to a cap or helmet. In this research, we measure head direction and estimate gaze direction from the data. The head direction measurement system uses a transmitter with three ultrasonic speakers and receiver. We can measure horizontal and vertical head direction by received time difference of ultrasound transmitted from three speakers. By using the M-sequence for the transmission wave, the spectral efficiency increases, enabling simultaneous measurement of the head direction by many people. In the experiment, we measured horizontal head direction using two ultrasonic speakers. In the result, it was possible to identify the M-sequence and received time. The error in the head direction was 5 degrees. © COPYRIGHT SPIE.","Environmental Education; Measuring head direction; Ultrasound","Ultrasonics; Direction measurement; Educational activities; Environmental education; Learning evaluations; Lightweight devices; Measuring heads; Simultaneous measurement; Spectral efficiencies; Binary sequences",Conference Paper,"Final","",Scopus,2-s2.0-85063894689
"Li P., Hou X., Duan X., Yip H., Song G., Liu Y.","56375954100;57207460293;55646016600;54793800500;55218895500;57203136059;","Appearance-Based Gaze Estimator for Natural Interaction Control of Surgical Robots",2019,"IEEE Access","7",,"8648437","25095","25110",,6,"10.1109/ACCESS.2019.2900424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062733925&doi=10.1109%2fACCESS.2019.2900424&partnerID=40&md5=8a8a3370e0ff0ec00fa2ecefea1e8cb3","School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China","Li, P., School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China; Hou, X., School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, 518055, China; Duan, X., School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; Yip, H., Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong; Song, G., State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, 110016, China; Liu, Y., Department of Mechanical and Automation Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong","Robots are playing an increasingly important role in modern surgery. However, conventional human-computer interaction methods, such as joystick control and sound control, have some shortcomings, and medical personnel are required to specifically practice operating the robot. We propose a human-computer interaction model based on eye movement with which medical staff can conveniently use their eye movements to control the robot. Our algorithm requires only an RGB camera to perform tasks without requiring expensive eye-tracking devices. Two kinds of eye control modes are designed in this paper. The first type is the pick and place movement, with which the user uses eye gaze to specify the point where the robotic arm is required to move. The second type is user command movement, with which the user can use eye gaze to select the direction in which the user desires the robot to move. The experimental results demonstrate the feasibility and convenience of these two modes of movement. © 2013 IEEE.","convolutional neural network; Deep learning; gaze estimation; surgical robot","Deep learning; Eye movements; Eye tracking; Human computer interaction; Human robot interaction; Neural networks; Robotic surgery; Surgery; Appearance based; Convolutional neural network; Eye tracking devices; Gaze estimation; Joystick control; Medical personnel; Natural interactions; Pick and place; Surgical equipment",Article,"Final","",Scopus,2-s2.0-85062733925
"Yang B., Zhang X., Li Z., Du S., Wang F.","57205176402;16032700200;57207687484;15073648200;57221087918;","An Accurate and Robust Gaze Estimation Method Based on Maximum Correntropy Criterion",2019,"IEEE Access","7",,"8629993","23291","23302",,1,"10.1109/ACCESS.2019.2896303","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062704122&doi=10.1109%2fACCESS.2019.2896303&partnerID=40&md5=f43f1e0d33e8ebb22448ee327b83ca01","National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China; School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China","Yang, B., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China; Zhang, X., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China; Li, Z., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China; Du, S., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China; Wang, F., National Engineering Laboratory for Visual Information Processing and Applications, Xi'an Jiaotong University, Xi'an, 710049, China, School of Electronic and Information Engineering, Xi'an Jiaotong University, Xi'an, 710049, China","Accurately estimating the user's gaze is important in many applications, such as human-computer interaction. Due to great convenience, appearance-based methods for gaze estimation have been a popular subject of research for many years. However, the greatest challenges in the appearance-based gaze estimation in a desktop environment are how to simplify the calibration process and deal with other issues such as image noise and low resolution. To address the problems, we adopt a mapping relationship between the high-dimensional eye image features space and the low-dimensional gaze positions and propose a robust and accurate method for gaze estimation with a webcam. First, we utilize Kullback-Leibler divergence to reduce feature dimension and keep similarity between the feature space and the gaze space. Then, we construct the objective function using the maximum correntropy criterion instead of mean squared error, which can enhance the anti-noise ability, especially for outliers or pixel corruption. A regularization term is adopted to adaptively select the sparse training samples for gaze estimation. We conducted extensive experiments in a desktop environment, which verified that the proposed method was robust and efficient in dealing with sparse training samples, pixel corruption, and low-resolution problems in gaze estimation. © 2013 IEEE.","Appearance-based method; gaze estimation; human computer interaction; maximum correntropy criterion","Crime; Mean square error; Pixels; Sampling; Appearance-based methods; Calibration process; Correntropy; Gaze estimation; Kullback Leibler divergence; Mapping relationships; Objective functions; Regularization terms; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85062704122
"Li J., Shan Y., Li S., Chen T.","57203736355;57201580351;16202805500;57201241944;","Gaze estimation using a head-mounted single full-view camera",2019,"Journal of Electronic Imaging","28","1","013002","","",,1,"10.1117/1.JEI.28.1.013002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062644140&doi=10.1117%2f1.JEI.28.1.013002&partnerID=40&md5=391ced3d88ac894f57ec02a521a7121e","Southwest University, School of Electronic and Information Engineering, Chongqing Key Laboratory of Nonlinear Circuit and Intelligent Information Processing, Chongqing, China; Hiroshima City University, Graduate School of Information Sciences, Hiroshima, Japan","Li, J., Southwest University, School of Electronic and Information Engineering, Chongqing Key Laboratory of Nonlinear Circuit and Intelligent Information Processing, Chongqing, China; Shan, Y., Hiroshima City University, Graduate School of Information Sciences, Hiroshima, Japan; Li, S., Southwest University, School of Electronic and Information Engineering, Chongqing Key Laboratory of Nonlinear Circuit and Intelligent Information Processing, Chongqing, China, Hiroshima City University, Graduate School of Information Sciences, Hiroshima, Japan; Chen, T., Southwest University, School of Electronic and Information Engineering, Chongqing Key Laboratory of Nonlinear Circuit and Intelligent Information Processing, Chongqing, China","We present a gaze estimation method for a head-mounted full-view egocentric camera that can capture egocentric video together with users' gaze cues. While the conventional gaze recording device has two cameras, an eye camera and a scene camera, the proposed gaze recording device has only a single spherical camera with a full field of view. To determine the point of gaze on full-view images, we present an eye-model-based gaze estimation method by means of three-dimensional iris projection. First, an eye model is built according to the precalibration process of an eyeball center and the eyeball biological parameters; then, we express the 3D iris contour in the eye model and project it back to the spherical camera model. since 2D iris contours can be detected directly on images and can also be expressed under the spherical model, the relationship between the 3D iris contour and the 2D iris contour can be found. By solving this problem, the gaze direction under the camera model is determined; subsequently, the point of gaze on the images can be inferred. since in the proposed method, a single spherical camera can play the role of the conventional two cameras, not only do the proposed method results have a simpler system structure but also the cumbersome operation of the calibration for the conventional two-camera gaze measurement devices becomes unnecessary. The effectiveness of the proposed method is shown by the experimental results. © 2019 SPIE and IS and T.","eye-model-based gaze estimation; head-mounted camera; point of gaze; spherical camera model","3D modeling; Spheres; Biological parameter; Camera model; Gaze estimation; Head mounted Camera; Measurement device; Point of gaze; Recording devices; System structures; Cameras",Article,"Final","",Scopus,2-s2.0-85062644140
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11130 LNCS",,,"","",17784,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061806215&partnerID=40&md5=db3936347c4f70270679845a29c9201a",,"","The proceedings contain 1100 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85061806215
"Yu Y., Liu G., Odobez J.-M.","57188644020;56420692700;57203103085;","Deep multitask gaze estimation with a constrained Landmark-Gaze model",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11130 LNCS",,,"456","474",,8,"10.1007/978-3-030-11012-3_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061782488&doi=10.1007%2f978-3-030-11012-3_35&partnerID=40&md5=a9f7442a575a13638c64cef81bc25f70","Idiap Research Institute, Martigny, Switzerland; EPFL, Lausanne, Switzerland","Yu, Y., Idiap Research Institute, Martigny, Switzerland, EPFL, Lausanne, Switzerland; Liu, G., Idiap Research Institute, Martigny, Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, Switzerland, EPFL, Lausanne, Switzerland","As an indicator of attention, gaze is an important cue for human behavior and social interaction analysis. Recent deep learning methods for gaze estimation rely on plain regression of the gaze from images without accounting for potential mismatches in eye image cropping and normalization. This may impact the estimation of the implicit relation between visual cues and the gaze direction when dealing with low resolution images or when training with a limited amount of data. In this paper, we propose a deep multitask framework for gaze estimation, with the following contributions. (i) we proposed a multitask framework which relies on both synthetic data and real data for end-to-end training. During training, each dataset provides the label of only one task but the two tasks are combined in a constrained way. (ii) we introduce a Constrained Landmark-Gaze Model (CLGM) modeling the joint variation of eye landmark locations (including the iris center) and gaze directions. By relating explicitly visual information (landmarks) to the more abstract gaze values, we demonstrate that the estimator is more accurate and easier to learn. (iii) by decomposing our deep network into a network inferring jointly the parameters of the CLGM model and the scale and translation parameters of eye regions on one hand, and a CLGM based decoder deterministically inferring landmark positions and gaze from these parameters and head pose on the other hand, our framework decouples gaze estimation from irrelevant geometric variations in the eye image (scale, translation), resulting in a more robust model. Thorough experiments on public datasets demonstrate that our method achieves competitive results, improving over state-of-the-art results in challenging free head pose gaze estimation tasks and on eye landmark localization (iris location) ones. © 2019, Springer Nature Switzerland AG.",,"Behavioral research; Computer vision; Geometric variations; Landmark localization; Landmark locations; Low resolution images; Social interactions; State of the art; Translation parameters; Visual information; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85061782488
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11131 LNCS",,,"","",17784,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061759267&partnerID=40&md5=bd7f2ee1ded077990d9cfa73e7c97c20",,"","The proceedings contain 1100 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85061759267
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11129 LNCS",,,"","",17784,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061754099&partnerID=40&md5=112b84638c64c339cb248a464f3133ce",,"","The proceedings contain 1100 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85061754099
"Celiktutan O., Demiris Y.","16229290400;6506125343;","Inferring human knowledgeability from eye gaze in mobile learning environments",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11134 LNCS",,,"193","209",,2,"10.1007/978-3-030-11024-6_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061731770&doi=10.1007%2f978-3-030-11024-6_13&partnerID=40&md5=fd90671b43f0a137ee2e8295f733e1aa","Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom","Celiktutan, O., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom; Demiris, Y., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom","What people look at during a visual task reflects an interplay between ocular motor functions and cognitive processes. In this paper, we study the links between eye gaze and cognitive states to investigate whether eye gaze reveal information about an individual’s knowledgeability. We focus on a mobile learning scenario where a user and a virtual agent play a quiz game using a hand-held mobile device. To the best of our knowledge, this is the first attempt to predict user’s knowledgeability from eye gaze using a noninvasive eye tracking method on mobile devices: we perform gaze estimation using front-facing camera of mobile devices in contrast to using specialised eye tracking devices. First, we define a set of eye movement features that are discriminative for inferring user’s knowledgeability. Next, we train a model to predict users’ knowledgeability in the course of responding to a question. We obtain a classification performance of 59.1% achieving human performance, using eye movement features only, which has implications for (1) adapting behaviours of the virtual agent to user’s needs (e.g., virtual agent can give hints); (2) personalising quiz questions to the user’s perceived knowledgeability. © Springer Nature Switzerland AG 2019.","Analysis of eye movements; Assistive mobile applications; Human knowledgeability prediction; Noninvasive gaze tracking","Computer aided instruction; Computer vision; E-learning; Eye movements; Forecasting; Virtual reality; Classification performance; Cognitive process; Eye tracking devices; Eye tracking methods; Gaze tracking; Human performance; Mobile applications; Mobile learning environment; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85061731770
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11134 LNCS",,,"","",17784,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061729369&partnerID=40&md5=3d31b7c5ed0eb2701908111cab1c8c49",,"","The proceedings contain 1100 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85061729369
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11132 LNCS",,,"","",17784,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061711750&partnerID=40&md5=96ffa047646f2ca5411f0ff6ae2b1b58",,"","The proceedings contain 1100 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85061711750
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11133 LNCS",,,"","",17784,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061691981&partnerID=40&md5=83e7a62f25d9a6f24724b61f3f1aecb6",,"","The proceedings contain 1100 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85061691981
"Chen Z., Ai S., Jia C.","36170296000;57192987046;8919313200;","Structure-aware deep learning for product image classification",2019,"ACM Transactions on Multimedia Computing, Communications and Applications","15","1s","4","","",,15,"10.1145/3231742","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061198077&doi=10.1145%2f3231742&partnerID=40&md5=232489cdca7ac708cee20265ebefa071","Institute of Automation, Chinese Academy of Sciences, No. 95, Zhongguancun East Road, Haidian District, Beijing, 100190, China; Beijing Jiaotong University, No. 3, Shangyuancun Road, Haidian District, Beijing, 100044, China","Chen, Z., Institute of Automation, Chinese Academy of Sciences, No. 95, Zhongguancun East Road, Haidian District, Beijing, 100190, China; Ai, S., Beijing Jiaotong University, No. 3, Shangyuancun Road, Haidian District, Beijing, 100044, China; Jia, C., Beijing Jiaotong University, No. 3, Shangyuancun Road, Haidian District, Beijing, 100044, China","Automatic product image classification is a task of crucial importance with respect to the management of online retailers. Motivated by recent advancements of deep Convolutional Neural Networks (CNN) on image classification, in this work we revisit the problem in the context of product images with the existence of a predefined categorical hierarchy and attributes, aiming to leverage the hierarchy and attributes to improve classification accuracy. With these structure-aware clues, we argue that more advanced deep models could be developed beyond the flat one-versus-all classification performed by conventional CNNs. To this end, novel efforts of this work include a salient-sensitive CNN that gazes into the product foreground by inserting a dedicated spatial attention module; a multiclass regression-based refinement that is expected to predict more accurately by merging prediction scores from multiple preceding CNNs, each corresponding to a distinct classifier in the hierarchy; and a multitask deep learning architecture that effectively explores correlations among categories and attributes for categorical label prediction. Experimental results on nearly 1 million real-world product images basically validate the effectiveness of the proposed efforts individually and jointly, from which performance gains are observed. © 2019 Association for Computing Machinery.","Category hierarchy; Convolutional neural network; Image classification; Multi-class regression; Multi-task learning","Convolution; Deep neural networks; Forecasting; Image enhancement; Neural networks; Categorical hierarchy; Category hierarchy; Classification accuracy; Convolutional neural network; Learning architectures; Multi-class regression; Multitask learning; Product image classifications; Image classification",Article,"Final","",Scopus,2-s2.0-85061198077
[无可用作者姓名],[无可用的作者 ID],"1st International Conference on Artificial Intelligence and Cognitive Computing, AICC 2018",2019,"Advances in Intelligent Systems and Computing","815",,,"","",716,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057071871&partnerID=40&md5=318bd9efe1e4cd892e7e96beb145d1ae",,"","The proceedings contain 68 papers. The special focus in this conference is on Artificial Intelligence and Cognitive Computing. The topics include: Cognitive-based adaptive path planning for mobile robot in dynamic environment; prediction for indian road network images dataset using feature extraction method; Design and simulation of capacitive MEMS accelerometer; bioinformatics and image processing—Detection of plant diseases; hybrid approach for pixel-wise semantic segmentation using SegNet and squeezenet for embedded platforms; software modernization through model transformations; Semi-automatic annotation of images using eye gaze data (SAIGA); optimizing regression test suite reduction; application monitoring—Active learning approach; acquiring best rules that represent datasets; an automated computer vision system for extraction of retail food product metadata; attacks on two-key elliptic curve digital signature algorithm; bioinformatics: An application in information science; leveraging deep learning for anomaly detection in video surveillance; speaker diarization system using hidden Markov toolkit; Survey: Enhanced trust management for improving QoS in MANETs; a generic survey on medical big data analysis using internet of things; mean estimation under post-stratified cluster sampling scheme; privacy-preserving Naive Bayesian classifier for continuous data and discrete data; learning style recognition: A neural network approach; morphological-based localization of an Iris image; an enhanced efficiency of key management method for wireless sensor networks using mobile agents; a survey on emotion’s recognition using internet of things; KAZE feature based passive image forgery detection; efficiency-based analysis of homomorphic encryption implications; Performance and analysis of human attention using single-channel wireless EEG sensor for medical application; smart heartbeat monitoring system using machine learning.",,,Conference Review,"Final","",Scopus,2-s2.0-85057071871
"Khosravan N., Celik H., Turkbey B., Jones E.C., Wood B., Bagci U.","57195066219;57212691442;9435311800;36071904900;7401873523;24176491700;","A collaborative computer aided diagnosis (C-CAD) system with eye-tracking, sparse attentional model, and deep learning",2019,"Medical Image Analysis","51",,,"101","115",,25,"10.1016/j.media.2018.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055895805&doi=10.1016%2fj.media.2018.10.010&partnerID=40&md5=f9f5b157dddcbc6a837b8b7787f1aba3","Center for Research in Computer Vision, University of Central FloridaFL, United States; Clinical Center, National Institutes of Health, Bethesda, MD, United States","Khosravan, N., Center for Research in Computer Vision, University of Central FloridaFL, United States; Celik, H., Clinical Center, National Institutes of Health, Bethesda, MD, United States; Turkbey, B., Clinical Center, National Institutes of Health, Bethesda, MD, United States; Jones, E.C., Clinical Center, National Institutes of Health, Bethesda, MD, United States; Wood, B., Clinical Center, National Institutes of Health, Bethesda, MD, United States; Bagci, U., Center for Research in Computer Vision, University of Central FloridaFL, United States","Computer aided diagnosis (CAD) tools help radiologists to reduce diagnostic errors such as missing tumors and misdiagnosis. Vision researchers have been analyzing behaviors of radiologists during screening to understand how and why they miss tumors or misdiagnose. In this regard, eye-trackers have been instrumental in understanding visual search processes of radiologists. However, most relevant studies in this aspect are not compatible with realistic radiology reading rooms. In this study, we aim to develop a paradigm shifting CAD system, called collaborative CAD (C-CAD), that unifies CAD and eye-tracking systems in realistic radiology room settings. We first developed an eye-tracking interface providing radiologists with a real radiology reading room experience. Second, we propose a novel algorithm that unifies eye-tracking data and a CAD system. Specifically, we present a new graph based clustering and sparsification algorithm to transform eye-tracking data (gaze) into a graph model to interpret gaze patterns quantitatively and qualitatively. The proposed C-CAD collaborates with radiologists via eye-tracking technology and helps them to improve their diagnostic decisions. The C-CAD uses radiologists’ search efficiency by processing their gaze patterns. Furthermore, the C-CAD incorporates a deep learning algorithm in a newly designed multi-task learning platform to segment and diagnose suspicious areas simultaneously. The proposed C-CAD system has been tested in a lung cancer screening experiment with multiple radiologists, reading low dose chest CTs. Promising results support the efficiency, accuracy and applicability of the proposed C-CAD system in a real radiology room setting. We have also shown that our framework is generalizable to more complex applications such as prostate cancer screening with multi-parametric magnetic resonance imaging (mp-MRI). © 2018 Elsevier B.V.","Attention; Eye-tracking; Graph sparsification; Lung cancer screening; Multi-task deep learning; Prostate cancer screening","Biological organs; Clustering algorithms; Computer aided instruction; Deep learning; Diseases; Efficiency; Eye movements; Eye tracking; Graphic methods; Learning algorithms; Magnetic resonance imaging; Radiation; Radiology; Tumors; Urology; Attention; Computer Aided Diagnosis(CAD); Eye tracking technologies; Graph sparsification; Graph-based clustering; Lung cancer screening; Prostate cancers; Radiology reading rooms; Computer aided diagnosis; Article; cancer screening; collaborative learning; computer assisted diagnosis; computer assisted tomography; conceptual framework; diagnostic accuracy; eye tracking; learning algorithm; lung cancer; machine learning; multiparametric magnetic resonance imaging; priority journal; prostate cancer; qualitative analysis; quantitative analysis; radiologist; technology; algorithm; computer assisted diagnosis; diagnostic error; diagnostic imaging; early cancer diagnosis; eye movement; female; human; lung tumor; male; nuclear magnetic resonance imaging; prevention and control; procedures; prostate tumor; x-ray computed tomography; Algorithms; Deep Learning; Diagnosis, Computer-Assisted; Diagnostic Errors; Early Detection of Cancer; Eye Movements; Female; Humans; Lung Neoplasms; Magnetic Resonance Imaging; Male; Prostatic Neoplasms; Tomography, X-Ray Computed",Article,"Final","",Scopus,2-s2.0-85055895805
"Barbara N., Camilleri T.A., Camilleri K.P.","57193677149;56041755400;8301303700;","EOG-based eye movement detection and gaze estimation for an asynchronous virtual keyboard",2019,"Biomedical Signal Processing and Control","47",,,"159","167",,9,"10.1016/j.bspc.2018.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052997683&doi=10.1016%2fj.bspc.2018.07.005&partnerID=40&md5=9a9f703b1daf642861c7a693fb88cf3e","Department of Systems and Control Engineering, Faculty of Engineering, University of Malta, Msida, MSD2080, Malta","Barbara, N., Department of Systems and Control Engineering, Faculty of Engineering, University of Malta, Msida, MSD2080, Malta; Camilleri, T.A., Department of Systems and Control Engineering, Faculty of Engineering, University of Malta, Msida, MSD2080, Malta; Camilleri, K.P., Department of Systems and Control Engineering, Faculty of Engineering, University of Malta, Msida, MSD2080, Malta","This work aims to develop a novel electrooculography (EOG)-based virtual keyboard with a standard QWERTY layout which, unlike similar state-of-the-art systems, allows users to reach any icon from any location directly and asynchronously. The saccadic EOG potential displacement is mapped to angular gaze displacement using a novel two-channel input linear regression model, which considers features extracted from both the horizontal and vertical EOG signal components jointly. Using this technique, a gaze displacement estimation error of 1.32 ± 0.26° and 1.67 ± 0.26° in the horizontal and vertical directions respectively was achieved, a performance which was also found to be generally statistically significantly better than the performance obtained using one model for each EOG component to model the relationship in the horizontal and vertical directions separately, as typically used in the literature. Furthermore, this work also proposes a threshold-based method to detect eye movements from EOG signals in real-time, which are then classified as saccades or blinks using a novel cascade of a parametric and a signal-morphological classifier based on the EOG peak and gradient features. This resulted in an average saccade and blink labelling accuracy of 99.92% and 100.00% respectively, demonstrating that these two eye movements could be reliably detected and discriminated in real-time using the proposed algorithms. When these techniques were used to interface with the proposed asynchronous EOG-based virtual keyboard, an average writing speed across subjects of 11.89 ± 4.42 characters per minute was achieved, a performance which has been shown to improve substantially with user experience. © 2018 Elsevier Ltd","Blinks; Electrooculography; Eye movement detection; Gaze estimation; Saccades; Virtual keyboard","Biomedical signal processing; Classification (of information); Computer keyboards; Electrooculography; Linear regression; Motion analysis; Blinks; Displacement estimation; Gaze estimation; Linear regression models; Movement detection; State-of-the-art system; Vertical direction; Virtual Keyboards; Eye movements; Article; classifier; electrooculogram; electrooculography; eye movement; eyelid reflex; feature extraction; gaze; human; human experiment; linear regression analysis; priority journal; saccadic eye movement; signal processing; virtual reality; writing",Article,"Final","",Scopus,2-s2.0-85052997683
"Zhang X., Sugano Y., Fritz M., Bulling A.","57142162900;7005470045;14035495500;6505807414;","MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation",2019,"IEEE Transactions on Pattern Analysis and Machine Intelligence","41","1","8122058","162","175",,116,"10.1109/TPAMI.2017.2778103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037648078&doi=10.1109%2fTPAMI.2017.2778103&partnerID=40&md5=07963eacac14fe52a5800aa8c93b3c84","Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, 66123, Germany; Graduate School of Information Science and Technology, Osaka University, Osaka, 565-0871, Japan","Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, 66123, Germany; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Osaka, 565-0871, Japan; Fritz, M., Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, 66123, Germany; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, 66123, Germany","Learning-based methods are believed to work well for unconstrained gaze estimation, i.e. gaze estimation from a monocular RGB camera without assumptions regarding user, environment, or camera. However, current gaze datasets were collected under laboratory conditions and methods were not evaluated across multiple datasets. Our work makes three contributions towards addressing these limitations. First, we present the MPIIGaze dataset, which contains 213,659 full face images and corresponding ground-truth gaze positions collected from 15 users during everyday laptop use over several months. An experience sampling approach ensured continuous gaze and head poses and realistic variation in eye appearance and illumination. To facilitate cross-dataset evaluations, 37,667 images were manually annotated with eye corners, mouth corners, and pupil centres. Second, we present an extensive evaluation of state-of-the-art gaze estimation methods on three current datasets, including MPIIGaze. We study key challenges including target gaze range, illumination conditions, and facial appearance variation. We show that image resolution and the use of both eyes affect gaze estimation performance, while head pose and pupil centre information are less informative. Finally, we propose GazeNet, the first deep appearance-based gaze estimation method. GazeNet improves on the state of the art by 22 percent (from a mean error of 13.9 degrees to 10.8 degrees) for the most challenging cross-dataset evaluation. © 1979-2012 IEEE.","convolutional neural network; cross-dataset evaluation; deep learning; Unconstrained gaze estimation","Cameras; Data structures; Estimation; Image resolution; Lighting; Magnetic heads; Neural networks; Three dimensional displays; Convolutional neural network; Cross-dataset evaluation; Experience sampling; Gaze estimation; Head; Illumination conditions; Laboratory conditions; Learning-based methods; Deep learning",Article,"Final","",Scopus,2-s2.0-85037648078
"Lathuilière S., Massé B., Mesejo P., Horaud R.","56418486900;57191161185;37108185800;7003505326;","Deep Reinforcement Learning for Audio-Visual Gaze Control",2018,"IEEE International Conference on Intelligent Robots and Systems",,,"8594327","1555","1562",,4,"10.1109/IROS.2018.8594327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063008630&doi=10.1109%2fIROS.2018.8594327&partnerID=40&md5=b809b7afbb49dbf6b1b66b041d4c4385","INRIA Grenoble Rhône-Alpes, Univ. Grenoble Alpes, France","Lathuilière, S., INRIA Grenoble Rhône-Alpes, Univ. Grenoble Alpes, France; Massé, B., INRIA Grenoble Rhône-Alpes, Univ. Grenoble Alpes, France; Mesejo, P., INRIA Grenoble Rhône-Alpes, Univ. Grenoble Alpes, France; Horaud, R., INRIA Grenoble Rhône-Alpes, Univ. Grenoble Alpes, France","We address the problem of audio-visual gaze control in the specific context of human-robot interaction, namely how controlled robot motions are combined with visual and acoustic observations in order to direct the robot head towards targets of interest. The paper has the following contributions: (i) a novel audio-visual fusion framework that is well suited for controlling the gaze of a robotic head; (ii) a reinforcement learning (RL) formulation for the gaze control problem, using a reward function based on the available temporal sequence of camera and microphone observations; and (iii) several deep architectures that allow to experiment with early and late fusion of audio and visual data. We introduce a simulated environment that enables us to learn the proposed deep RL model without the need of spending hours of tedious interaction. By thoroughly experimenting on a publicly available dataset and on a real robot, we provide empirical evidence that our method achieves state-of-the-art performance. © 2018 IEEE.",,"Audio acoustics; Deep learning; Human computer interaction; Human robot interaction; Intelligent robots; Machine learning; Reinforcement learning; Audio-visual fusion; Deep architectures; Reward function; Robot motion; Simulated environment; State-of-the-art performance; Targets of interest; Temporal sequences; Visual servoing",Conference Paper,"Final","",Scopus,2-s2.0-85063008630
"Garcia-Garcia A., Martinez-Gonzalez P., Oprea S., Castro-Vargas J.A., Orts-Escolano S., Garcia-Rodriguez J., Jover-Alvarez A.","56585533100;57191663286;57194596019;57194406679;53864128700;35872525800;57207824561;","The RobotriX: An Extremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions",2018,"IEEE International Conference on Intelligent Robots and Systems",,,"8594495","6790","6797",,7,"10.1109/IROS.2018.8594495","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063004793&doi=10.1109%2fIROS.2018.8594495&partnerID=40&md5=93985a218b14b5e6a6eeb2a16559fa12","University of Alicante, 3D Perception Lab, Spain","Garcia-Garcia, A., University of Alicante, 3D Perception Lab, Spain; Martinez-Gonzalez, P., University of Alicante, 3D Perception Lab, Spain; Oprea, S., University of Alicante, 3D Perception Lab, Spain; Castro-Vargas, J.A., University of Alicante, 3D Perception Lab, Spain; Orts-Escolano, S., University of Alicante, 3D Perception Lab, Spain; Garcia-Rodriguez, J., University of Alicante, 3D Perception Lab, Spain; Jover-Alvarez, A., University of Alicante, 3D Perception Lab, Spain","Enter the RobotriX, an extremely photorealistic indoor dataset designed to enable the application of deep learning techniques to a wide variety of robotic vision problems. The RobotriX consists of hyperrealistic indoor scenes which are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline using UnrealCV to generate raw data and ground truth labels. By taking this approach, we were able to generate a dataset of 38 semantic classes across 512 sequences totaling 8M stills recorded at +60 frames per second with full HD resolution. For each frame, RGB-D and 3D information is provided with full annotations in both spaces. Thanks to the high quality and quantity of both raw information and annotations, the RobotriX will serve as a new milestone for investigating 2D and 3D robotic vision tasks with large-scale data-driven techniques. © 2018 IEEE.",,"Deep learning; Intelligent robots; Large dataset; Robotics; Semantics; Virtual reality; 3D information; Frames per seconds; Large scale data; Learning techniques; Photo-realistic; Robot trajectory; Semantic class; Virtual-reality headsets; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85063004793
"Saran A., Majumdar S., Shor E.S., Thomaz A., Niekum S.","57200951054;57200628922;57207832047;13007283900;15080949400;","Human Gaze Following for Human-Robot Interaction",2018,"IEEE International Conference on Intelligent Robots and Systems",,,"8593580","8615","8621",,9,"10.1109/IROS.2018.8593580","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062987552&doi=10.1109%2fIROS.2018.8593580&partnerID=40&md5=832bf364fcb212900ee990c1d74c491e","University of Texas at Austin, Department of Computer Science, Austin, TX  78712, United States; University of Texas at Austin, Department of Electrical and Computer Engineering, Austin, TX  78712, United States","Saran, A., University of Texas at Austin, Department of Computer Science, Austin, TX  78712, United States; Majumdar, S., University of Texas at Austin, Department of Electrical and Computer Engineering, Austin, TX  78712, United States; Shor, E.S., University of Texas at Austin, Department of Electrical and Computer Engineering, Austin, TX  78712, United States; Thomaz, A., University of Texas at Austin, Department of Electrical and Computer Engineering, Austin, TX  78712, United States; Niekum, S., University of Texas at Austin, Department of Computer Science, Austin, TX  78712, United States","Gaze provides subtle informative cues to aid fluent interactions among people. Incorporating human gaze predictions can signify how engaged a person is while interacting with a robot and allow the robot to predict a human's intentions or goals. We propose a novel approach to predict human gaze fixations relevant for human-robot interaction tasks-both referential and mutual gaze-in real time on a robot. We use a deep learning approach which tracks a human's gaze from a robot's perspective in real time. The approach builds on prior work which uses a deep network to predict the referential gaze of a person from a single 2D image. Our work uses an interpretable part of the network, a gaze heat map, and incorporates contextual task knowledge such as location of relevant objects, to predict referential gaze. We find that the gaze heat map statistics also capture differences between mutual and referential gaze conditions, which we use to predict whether a person is facing the robot's camera or not. We highlight the challenges of following a person's gaze on a robot in real time and show improved performance for referential gaze and mutual gaze prediction. © 2018 IEEE.",,"Deep learning; Forecasting; Intelligent robots; Man machine systems; 2D images; Heat maps; Learning approach; Mutual gazes; Real time; Task knowledge; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85062987552
"Breuers S., Beyer L., Rafi U., Leibel B.","55154876600;57220454562;56272692600;57207816236;","Detection- Tracking for Efficient Person Analysis: The DetTA Pipeline",2018,"IEEE International Conference on Intelligent Robots and Systems",,,"8594335","48","53",,3,"10.1109/IROS.2018.8594335","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062980046&doi=10.1109%2fIROS.2018.8594335&partnerID=40&md5=d5cfa9abd88bdb3b6a52e06317acb70d","RWTH Aachen University, Visual Computing Institute, Germany","Breuers, S., RWTH Aachen University, Visual Computing Institute, Germany; Beyer, L., RWTH Aachen University, Visual Computing Institute, Germany; Rafi, U., RWTH Aachen University, Visual Computing Institute, Germany; Leibel, B., RWTH Aachen University, Visual Computing Institute, Germany","In the past decade many robots were deployed in the wild, and people detection and tracking is an important component of such deployments. On top of that, one often needs to run modules which analyze persons and extract higher level attributes such as age and gender, or dynamic information like gaze and pose. The latter ones are especially necessary for building a reactive, social robot-person interaction. In this paper, we combine those components in a fully modular detection-tracking-analysis pipeline, called DetTA. We investigate the benefits of such an integration on the example of head and skeleton pose, by using the consistent track ID for a temporal filtering of the analysis modules' observations, showing a slight improvement in a challenging real-world scenario. We also study the potential of a so-called 'free-flight' mode, where the analysis of a person attribute only relies on the filter's predictions for certain frames. Here, our study shows that this boosts the runtime dramatically, while the prediction quality remains stable. This insight is especially important for reducing power consumption and sharing precious (GPU-)memory when running many analysis components on a mobile platform, especially so in the era of expensive deep learning methods. © 2018 IEEE.",,"Deep learning; Free flight; Pipelines; Dynamic information; Learning methods; Mobile platform; People detection; Prediction quality; Real-world scenario; Temporal filtering; Tracking analysis; Intelligent robots",Conference Paper,"Final","",Scopus,2-s2.0-85062980046
"Lahiri A., Agarwalla A., Biswas P.K.","56572183500;57195936642;7202443668;","Unsupervised Domain Adaptation for Learning Eye Gaze from a Million Synthetic Images: An Adversarial Approach",2018,"ACM International Conference Proceeding Series",,,"3293423","","",,1,"10.1145/3293353.3293423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098160381&doi=10.1145%2f3293353.3293423&partnerID=40&md5=2843b93eb2f12752a766f484f38bced9","Dept. of EandECE, Iit Kharagpur, India; Microsoft Idc Hyderabad and Iit, Kharagpur, India","Lahiri, A., Dept. of EandECE, Iit Kharagpur, India; Agarwalla, A., Microsoft Idc Hyderabad and Iit, Kharagpur, India; Biswas, P.K., Dept. of EandECE, Iit Kharagpur, India","With contemporary advancements of graphics engines, recent trend in deep learning community is to train models on automatically annotated simulated examples and apply on real data during test time. This alleviates the burden of manual annotation. However, there is an inherent difference of distributions between images coming from graphics engine and real world. Such domain difference deteriorates test time performances of models trained on synthetic examples. In this paper we address this issue with unsupervised adversarial feature adaptation across synthetic and real domain for the special use case of eye gaze estimation which is an essential component for various downstream HCI tasks. We initially learn a gaze estimator on annotated synthetic samples rendered from a 3D game engine and then adapt the features of unannotated real samples via a zero-sum minmax adversarial game against a domain discriminator following the recent paradigm of generative adversarial networks. Such adversarial adaptation forces features of both domains to be indistinguishable which enables us to use regression models trained on synthetic domain to be used on real samples. On the challenging MPIIGaze real life dataset, we outperform recent fully supervised methods trained on manually annotated real samples by appreciable margins and also achieve 13% more relative gain after adaptation compared to the current benchmark method of SimGAN [31]. Codes available at: https://github.com/abhinavagarwalla/adversarial_da_icvgip18. © 2018 ACM.","Adversarial Learning; Domain Adaptation; Domain Adversarial Networks; Gaze Prediction; Generative Adversarial Networks; MPIIGaze; UnityEyes","Computer vision; Regression analysis; Adversarial networks; Domain adaptation; Domain differences; Feature adaptation; Learning community; Manual annotation; Supervised methods; Synthetic images; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85098160381
"Wang K., Zhao R., Ji Q.","56637259500;56461916600;18935108400;","A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation",2018,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",,,"8578151","440","448",,21,"10.1109/CVPR.2018.00053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062837341&doi=10.1109%2fCVPR.2018.00053&partnerID=40&md5=619c2f3f5b07ac60f394d0c872e5fd9d","ECSE, Rensselaer Polytechnic Institute, Troy, NY, United States","Wang, K., ECSE, Rensselaer Polytechnic Institute, Troy, NY, United States; Zhao, R., ECSE, Rensselaer Polytechnic Institute, Troy, NY, United States; Ji, Q., ECSE, Rensselaer Polytechnic Institute, Troy, NY, United States","In this work, we introduce a Hierarchical Generative Model (HGM) to enable realistic forward eye image synthesis, as well as effective backward eye gaze estimation. The proposed HGM consists of a hierarchical generative shape model (HGSM), and a conditional bidirectional generative adversarial network (c-BiGAN). The HGSM encodes eye geometry knowledge and relates eye gaze with eye shape, while c-BiGAN leverages on big data and captures the dependency between eye shape and eye appearance. As an intermediate component, eye shape connects knowledge-based model (HGSM) with data-driven model (c-BiGAN) and enables bidirectional inference. Through a top-down inference, the HGM can synthesize eye images consistent with the given eye gaze. Through a bottom-up inference, HGM can infer eye gaze effectively from a given eye image. Qualitative and quantitative evaluations on benchmark datasets demonstrate our model's effectiveness on both eye image synthesis and eye gaze estimation. In addition, the proposed model is not restricted to eye images only. It can be adapted to face images and any shape-appearance related fields. © 2018 IEEE.",,"Computer vision; Knowledge based systems; Adversarial networks; Benchmark datasets; Data-driven model; Generative model; Intermediate components; Knowledge-based model; Quantitative evaluation; Top-down inference; C (programming language)",Conference Paper,"Final","",Scopus,2-s2.0-85062837341
"Ranjan R., De Mello S., Kautz J.","57212394654;57201314496;7006458237;","Light-weight head pose invariant gaze tracking",2018,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2018-June",,"8575461","2237","2245",,26,"10.1109/CVPRW.2018.00290","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060851979&doi=10.1109%2fCVPRW.2018.00290&partnerID=40&md5=8d0704aa71f4d6f1536afa3152268dd2","University of Maryland, United States; NVIDIA, United States","Ranjan, R., University of Maryland, United States; De Mello, S., NVIDIA, United States; Kautz, J., NVIDIA, United States","Unconstrained remote gaze tracking using off-the-shelf cameras is a challenging problem. Recently, promising algorithms for appearance-based gaze estimation using convolutional neural networks (CNN) have been proposed. Improving their robustness to various confounding factors including variable head pose, subject identity, illumination and image quality remain open problems. In this work, we study the effect of variable head pose on machine learning regressors trained to estimate gaze direction. We propose a novel branched CNN architecture that improves the robustness of gaze classifiers to variable head pose, without increasing computational cost. We also present various procedures to effectively train our gaze network including transfer learning from the more closely related task of object viewpoint estimation and from a large high-fidelity synthetic gaze dataset, which enable our ten times faster gaze network to achieve competitive accuracy to its current state-of-the-art direct competitor. © 2018 IEEE.",,"Computer vision; Gesture recognition; Image enhancement; Large dataset; Learning systems; Neural networks; Appearance based; Computational costs; Convolutional neural network; Gaze direction; Gaze estimation; High-fidelity; State of the art; Transfer learning; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85060851979
"Sun H.-P., Yang C.-H., Lai S.-H.","57205559589;56039197700;7402937330;","A deep learning approach to appearance-based gaze estimation under head pose variations",2018,"Proceedings - 4th Asian Conference on Pattern Recognition, ACPR 2017",,,"8575948","941","946",,,"10.1109/ACPR.2017.155","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060553240&doi=10.1109%2fACPR.2017.155&partnerID=40&md5=befade363acc2cc67a50394704d3b86e","National Tsing Hua Univ., Computer Science Dept., Hsinchu, Taiwan","Sun, H.-P., National Tsing Hua Univ., Computer Science Dept., Hsinchu, Taiwan; Yang, C.-H., National Tsing Hua Univ., Computer Science Dept., Hsinchu, Taiwan; Lai, S.-H., National Tsing Hua Univ., Computer Science Dept., Hsinchu, Taiwan","In this paper, we propose a deep learning based gaze estimation algorithm that estimates the gaze direction from a single face image. The proposed gaze estimation algorithm is based on using multiple convolutional neural networks (CNN) to learn the regression networks for gaze estimation from the eye images. The proposed algorithm can provide accurate gaze estimation for users with different head poses, since it explicitly includes the head pose information into the proposed gaze estimation framework. The proposed algorithm can be widely used for appearance-based gaze estimation in practice. Our experimental results show that the proposed gaze estimation system improves the accuracy of appearance-based gaze estimation under head pose variations compared to the previous methods. © 2017 IEEE.","Convolutional neural network; Deep learning; Gaze estimation","Convolution; Neural networks; Pattern recognition; Appearance based; Convolutional neural network; Eye images; Face images; Gaze direction; Gaze estimation; Head pose; Learning approach; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85060553240
"Ruiz N., Chong E., Rehg J.M.","57204290088;57194267364;7004835775;","Fine-grained head pose estimation without keypoints",2018,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2018-June",,"8575451","2155","2164",,124,"10.1109/CVPRW.2018.00281","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051963262&doi=10.1109%2fCVPRW.2018.00281&partnerID=40&md5=62954cd30ec3feedbca4541457229e13","Georgia Institute of Technology, United States","Ruiz, N., Georgia Institute of Technology, United States; Chong, E., Georgia Institute of Technology, United States; Rehg, J.M., Georgia Institute of Technology, United States","Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models. © 2018 IEEE.",,"3D modeling; Classification (of information); Computer vision; Large dataset; Neural networks; Open systems; Statistical tests; Benchmark datasets; Convolutional neural network; Correspondence problems; Head Pose Estimation; Image intensities; Landmark detection; Pose classifications; Training and testing; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85051963262
"Yamamoto T., Seo M., Kitajima T., Chen Y.-W.","57205504201;35280835900;57205503712;56036268200;","Eye Gaze Correction Using Generative Adversarial Networks",2018,"2018 IEEE 7th Global Conference on Consumer Electronics, GCCE 2018",,,"8574844","431","432",,1,"10.1109/GCCE.2018.8574844","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060291711&doi=10.1109%2fGCCE.2018.8574844&partnerID=40&md5=2c1c1bdeb582d4457b83d5b288d4a465","Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan; Sumsung RD Institute Japan, Osaka, Japan","Yamamoto, T., Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan; Seo, M., Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan; Kitajima, T., Sumsung RD Institute Japan, Osaka, Japan; Chen, Y.-W., Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan","Eye gaze correction is an important topic in video teleconference and video chart in order to keep the eye contact. In this paper, we propose to use a generative adversarial networks for eye gaze correction. We use pairs of front facial image (idea camera setting) and real facial image (real camera setting) to training the network. By using the trained network, we can generate a gaze corrected facial image (front facial image) for any real facial image. Experiments demonstrated the effectiveness of our proposed method. © 2018 IEEE.","Conditional GAN; Deep learning; Gaze correction; Generative Adversarial Net(GAN); Image-to-image translation","Cameras; Adversarial networks; Camera settings; Conditional GAN; Eye contact; Eye-gaze; Facial images; Generative Adversarial Net(GAN); Image translation; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85060291711
"Jha S., Busso C.","57193014012;35742852700;","Probabilistic Estimation of the Gaze Region of the Driver using Dense Classification",2018,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2018-November",,"8569709","697","702",,9,"10.1109/ITSC.2018.8569709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060471915&doi=10.1109%2fITSC.2018.8569709&partnerID=40&md5=a73290ca8a720e2a5a00c70755c5a609","Multimodal Signal Processing Lab, University of Texas, Dallas, Richardson, TX  75080, United States","Jha, S., Multimodal Signal Processing Lab, University of Texas, Dallas, Richardson, TX  75080, United States; Busso, C., Multimodal Signal Processing Lab, University of Texas, Dallas, Richardson, TX  75080, United States","The ability to monitor the visual attention of a driver is a useful feature for smart vehicles to understand the driver's intents and behaviors. The gaze angle of the driver is not deterministically related to his/her head pose due to the interplay between head and eye movements. Therefore, this study aims to establish a probabilistic relationship using deep learning. While probabilistic regression techniques such as Gaussian process regression (GPR) has been previously used to predict the visual attention of a driver, the proposed deep learning framework is a more generic approach that does not make assumptions, learning the relationship between gaze and head pose from the data. In our formulation, the continuous gaze angles are converted into intervals and the grid of the quantized angles is treated as an image for dense prediction. We rely on convolutional neural networks (CNNs) with upsampling to map the six degrees of freedom of the orientation and position of the head into gaze angles. We train and evaluate the proposed network with data collected from drivers who were asked to look at predetermined locations inside a car during naturalistic driving recordings. The proposed model obtains very promising results, where the size of the gaze region with 95% accuracy is only 11.73% of a half sphere centered at the driver, which approximates his/her field of view. The architecture offers an appealing and general solution to convert regression problems into dense classification problems. © 2018 IEEE.",,"Deep learning; Degrees of freedom (mechanics); Driver training; Eye movements; Intelligent systems; Intelligent vehicle highway systems; Neural networks; Regression analysis; Convolutional neural network; Gaussian process regression; General solutions; Learning frameworks; Probabilistic estimation; Probabilistic regression; Regression problem; Six degrees of freedom; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85060471915
"Zheng C., Usagawa T.","57203117051;7003663095;","A Rapid Webcam-Based Eye Tracking Method for Human Computer Interaction",2018,"ICCAIS 2018 - 7th International Conference on Control, Automation and Information Sciences",,,"8570532","133","136",,4,"10.1109/ICCAIS.2018.8570532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060290882&doi=10.1109%2fICCAIS.2018.8570532&partnerID=40&md5=04d4e12b9e461d2e83610406dfdfe709","Kumamoto University, Graduate School of Science and Technology, Kumamoto, Japan","Zheng, C., Kumamoto University, Graduate School of Science and Technology, Kumamoto, Japan; Usagawa, T., Kumamoto University, Graduate School of Science and Technology, Kumamoto, Japan","This study proposes a rapid eye tracking method, to respond to a situation that require a high processing speed but less accuracy. Unlike other studies, this study uses a webcam with a low resolution of 640 × 480, which decreased the cost of devices considerably. We also developed the corresponding algorithm to suit the low-quality image. We use an efficient algorithm to detect the pupils which is based on color intensity change to decrease the calculation load. The processing speed exceeds the requirement of eye tracking for saccade eyeball movement. The result of experiment shows that the proposed method is a fast and low-cost method for eye tracking. © 2018 IEEE.","eye tracking; gaze estimation; OpenCV; webcam","Eye movements; Human computer interaction; Color intensity; Eye tracking methods; Eyeball movements; Gaze estimation; Low cost methods; OpenCV; Processing speed; webcam; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85060290882
"Anwar S., Milanova M., Svetleff Z., Abdulla S.","57193015705;7003785945;57200761694;57205575562;","Real Time Eye Gaze Estimation",2018,"Proceedings - 2017 International Conference on Computational Science and Computational Intelligence, CSCI 2017",,,"8560846","526","531",,1,"10.1109/CSCI.2017.89","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060582787&doi=10.1109%2fCSCI.2017.89&partnerID=40&md5=c898a6e1182551b901dfc753cffdf3e7","Computer Science Department, UA Little RockAR, United States; Salahaddin University, Kurdistan Region/Erbil, Iraq; Department of Education Psychology and Higher Eduaction, University of Nevada, Las Vegas, United States; Information Technolgy Department, Polytechnic University, Kurdistan Region, Erbil, Iraq","Anwar, S., Computer Science Department, UA Little RockAR, United States, Salahaddin University, Kurdistan Region/Erbil, Iraq; Milanova, M., Computer Science Department, UA Little RockAR, United States; Svetleff, Z., Department of Education Psychology and Higher Eduaction, University of Nevada, Las Vegas, United States; Abdulla, S., Information Technolgy Department, Polytechnic University, Kurdistan Region, Erbil, Iraq","In this paper we used a set of searching techniques that allow to gain information about eye movement, its location and point of view in real time. The algorithm determines the point on the monitor at which the user is looking at. To obtain such data it is necessary to determine the relative position of the eye and the head. The first step is the initialization during which a head model is created. After initialization, the tracking phase is started using an Active Appearance Models (AAM) and Pose from Orthography and Scaling with ITerations (POSIT) algorithm for head position estimation. The purpose of eye gaze estimation or eye tracking can be used for testing the effectiveness of the text, game, or advertising message. The aim of this work is to develop and implement a system for real time eye gaze estimation using PC's webcam only without any additional hardware. © 2017 IEEE.","AAM; Active Appearance Model; eye gaze; gaze estimation; POSIT","Artificial intelligence; Eye movements; Image recognition; Active appearance models; Eye-gaze; Gain information; Gaze estimation; Head position; POSIT; Relative positions; Searching techniques; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85060582787
"Cha X., Yang X., Feng Z., Xu T., Fan X., Tian J.","57214798390;55683790100;7403443516;56683649700;57197729874;57208750027;","Calibration-Free Gaze Zone Estimation Using Convolutional Neural Network",2018,"2018 International Conference on Security, Pattern Analysis, and Cybernetics, SPAC 2018",,,"8965441","481","484",,1,"10.1109/SPAC46244.2018.8965441","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079151448&doi=10.1109%2fSPAC46244.2018.8965441&partnerID=40&md5=4aed1162415507b4c3ef9bf1ad22e6c3","University of Jinan, School of Information Science and Engineering, Jinan, 250022, China","Cha, X., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China; Yang, X., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China; Feng, Z., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China; Xu, T., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China; Fan, X., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China; Tian, J., University of Jinan, School of Information Science and Engineering, Jinan, 250022, China","In this paper we propose a gaze zone estimation method using deep learning. Compared with traditional method, our method does not need the procedure of calibration. In the proposed method, a Kinect is used to capture the video of a computer user, which is pre-processed to suppress illumination variations. After that, haar cascade classifier is adopted to detect the face region and eye region. Then, the eye region is used to estimate the gaze zone on the monitor via a trained CNN (Convolution Neural Network). Experimental results show that the proposed method has a high accuracy, which can be applied in human-computer interaction. © 2018 IEEE.","convolutional neural network; deep learning; eye tracking; Gaze estimation","Calibration; Classification (of information); Convolution; Deep learning; Deep neural networks; Eye tracking; Human computer interaction; Calibration free; Computer users; Convolution neural network; Estimation methods; Gaze estimation; Haar cascade classifiers; High-accuracy; Illumination variation; Convolutional neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85079151448
"Ghiass R.S., Laurendeau D.","25654959400;7004448364;","Highly accurate and fully automatic 3D head pose estimation and eye gaze estimation using RGB-3D sensors and 3D morphable models",2018,"Sensors (Switzerland)","18","12","4280","","",,3,"10.3390/s18124280","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058845756&doi=10.3390%2fs18124280&partnerID=40&md5=da7b483286b03a8a4a99066047cd8bba","Computer Vision and Systems Laboratory, Laval University, Universite Laval, 1665 Rue de l’Universite, Quebec City, QC  G1V 0A6, Canada","Ghiass, R.S., Computer Vision and Systems Laboratory, Laval University, Universite Laval, 1665 Rue de l’Universite, Quebec City, QC  G1V 0A6, Canada; Laurendeau, D., Computer Vision and Systems Laboratory, Laval University, Universite Laval, 1665 Rue de l’Universite, Quebec City, QC  G1V 0A6, Canada","This work addresses the problem of automatic head pose estimation and its application in 3D gaze estimation using low quality RGB-D sensors without any subject cooperation or manual intervention. The previous works on 3D head pose estimation using RGB-D sensors require either an offline step for supervised learning or 3D head model construction, which may require manual intervention or subject cooperation for complete head model reconstruction. In this paper, we propose a 3D pose estimator based on low quality depth data, which is not limited by any of the aforementioned steps. Instead, the proposed technique relies on modeling the subject’s face in 3D rather than the complete head, which, in turn, relaxes all of the constraints in the previous works. The proposed method is robust, highly accurate and fully automatic. Moreover, it does not need any offline step. Unlike some of the previous works, the method only uses depth data for pose estimation. The experimental results on the Biwi head pose database confirm the efficiency of our algorithm in handling large pose variations and partial occlusion. We also evaluated the performance of our algorithm on IDIAP database for 3D head pose and eye gaze estimation. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","3D eye gaze estimation; 3D head pose estimation; 3D morphable models; Iterative closest point; RGB-D sensors","Iterative methods; 3D head; 3D Morphable model; Eye-gaze; Iterative Closest Points; Rgb-d sensors; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85058845756
"Del Coco M., Leo M., Carcagni P., Fama F., Spadaro L., Ruta L., Pioggia G., Distante C.","55319876500;7006471658;23003296600;57194548297;57200032563;14822531100;8957312900;55884135100;","Study of Mechanisms of Social Interaction Stimulation in Autism Spectrum Disorder by Assisted Humanoid Robot",2018,"IEEE Transactions on Cognitive and Developmental Systems","10","4","8207589","993","1004",,7,"10.1109/TCDS.2017.2783684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038841574&doi=10.1109%2fTCDS.2017.2783684&partnerID=40&md5=371fc73ef16f8cee2a110ce7d0585059","Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Lecce, 73100, Italy; Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Messina, 98164, Italy","Del Coco, M., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Lecce, 73100, Italy; Leo, M., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Lecce, 73100, Italy; Carcagni, P., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Lecce, 73100, Italy; Fama, F., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Messina, 98164, Italy; Spadaro, L., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Messina, 98164, Italy; Ruta, L., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Messina, 98164, Italy; Pioggia, G., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Messina, 98164, Italy; Distante, C., Institute of Applied Sciences and Intelligent Systems, National Research Council of Italy, Lecce, 73100, Italy","Information and communication technologies (ICTs) have been proved to have a great impact in enhancing social, communicative, and language development in children with autism spectrum disorders (ASDs) as demonstrated by plenty of effective technological tools reported in the literature for diagnosis, assessment, and treatment of such neurological diseases. On the contrary, there are very few works exploiting ICT to study the mechanisms that trigger the behavioral patterns during the specialized sessions of treatment focused on social interaction stimulation. From the study of the literature it emerges that the behavioral outcomes are qualitatively evaluated by the therapists making this way impossible to assess, in a consistent manner, the worth of the supplied ASD treatments that should be based on quantitative metric not available for this purpose yet. Moreover, the rare attempts to use a methodological approach are limited to the study of one (of at least a couple) of the several behavioral cues involved. In order to fill this gap, in this paper a technological framework able to analyze and integrate multiple visual cues in order to capture the behavioral trend along an ASD treatment is introduced. It is based on an algorithmic pipeline involving face detection, landmark extraction, gaze estimation, head pose estimation and facial expression recognition and it has been used to detect behavioral features during the interaction among different children, affected by ASD, and a humanoid robot. Experimental results demonstrated the superiority of the proposed framework in the specific application context with respect to leading approaches in the literature, providing a reliable pathway to automatically build a quantitative report that could help therapists to better achieve either ASD diagnosis or assessment tasks. © 2016 IEEE.","Autism spectrum disorders (ASDs) diagnostic; behavioral imaging; human-machine interaction; optical metrology of behavior","Anthropomorphic robots; Behavioral research; Diagnosis; Face recognition; Human computer interaction; Autism spectrum disorders; Children with autisms; Facial expression recognition; Human machine interaction; Information and Communication Technologies; Methodological approach; Optical Metrology; Technological framework; Diseases",Article,"Final","",Scopus,2-s2.0-85038841574
"Fan X., Wang F., Lu Y., Song D., Liu J.","56460611200;36065073900;57205388984;57205425730;8969748600;","Eye gazing enabled driving behavior monitoring and prediction",2018,"2018 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2018",,,"8551544","","",,1,"10.1109/ICMEW.2018.8551544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059982963&doi=10.1109%2fICMEW.2018.8551544&partnerID=40&md5=a09218514cc2ae426b3a1300b4cf3a40","School of Computing Science, Simon Fraser University, Canada; Department of Computer and Information Science, University of Mississippi, United States","Fan, X., School of Computing Science, Simon Fraser University, Canada; Wang, F., Department of Computer and Information Science, University of Mississippi, United States; Lu, Y., School of Computing Science, Simon Fraser University, Canada; Song, D., School of Computing Science, Simon Fraser University, Canada; Liu, J., School of Computing Science, Simon Fraser University, Canada","Automobiles have become one of the necessities of modern life, but also introduced numerous traffic accidents that threaten drivers and other road users. Most state-of-the-art safety systems are passively triggered, reacting to dangerous road conditions or driving behaviors only after they happen and are observed, which greatly limits the last chances for collision avoidances. Therefore, timely tracking and predicting the driving behaviors calls for a more direct interface beyond the traditional steering wheel/brake/gas pedal. In this paper, we argue that a driver's eyes are the interface, as it is the first and the essential window that gathers external information during driving. Our experiments suggest that a driver's gaze patterns appear prior to and correlate with the driving behaviors for driving behavior prediction. We accordingly propose GazMon, an active driving behavior monitoring and prediction framework for driving assistance applications. GazMon extracts the gaze information through a front-camera and analyzes the facial features, including facial landmarks, head pose, and iris centers, through a carefully constructed deep learning architecture. Our on-road experiments demonstrate the superiority of our GazMon on predicting driving behaviors. It is also readily deployable using RGB cameras and allows reuse of existing smartphones towards more safely driving. © 2018 IEEE.","Deep Learning; Driving Assistant; Gaze; Mobile Computing","Accidents; Cameras; Forecasting; Mobile computing; Roads and streets; Driving assistance; Driving Assistant; Driving behavior; External informations; Facial landmark; Gaze; Learning architectures; State of the art; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85059982963
"Jyoti S., Dhall A.","57202800224;35229206900;","Automatic Eye Gaze Estimation using Geometric Texture-based Networks",2018,"Proceedings - International Conference on Pattern Recognition","2018-August",,"8545162","2474","2479",,3,"10.1109/ICPR.2018.8545162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059762588&doi=10.1109%2fICPR.2018.8545162&partnerID=40&md5=09ed648b4f7ffb8f46c7e2014fa92ef9","Learning Affect and SemantIc Imaging (LASII) Group, Indian Institute of Technology Ropar, India","Jyoti, S., Learning Affect and SemantIc Imaging (LASII) Group, Indian Institute of Technology Ropar, India; Dhall, A., Learning Affect and SemantIc Imaging (LASII) Group, Indian Institute of Technology Ropar, India","Eye gaze estimation is an important problem in automatic human behavior understanding. This paper proposes a deep learning based method for inferring the eye gaze direction. The method is based on the use of ensemble of networks, which capture both the geometric and texture information. Firstly, a Deep Neural Network (DNN) is trained using the geometric features that are extracted from the facial landmark locations. Secondly, for the texture based features, three Convolutional Neural Networks (CNN) are trained i.e. For the patch around the left eye, right eye, and the combined eyes, respectively. Finally, the information from the four channels is fused with concatenation and dense layers are trained to predict the final eye gaze. The experiments are performed on the two publicly available datasets: Columbia eye gaze and TabletGaze. The extensive evaluation shows the superior performance of the proposed framework. We also evaluate the performance of the recently proposed swish activation function as compared to Rectified Linear Unit (ReLU) for eye gaze estimation. © 2018 IEEE.",,"Behavioral research; Deep neural networks; Geometry; Neural networks; Activation functions; Convolutional Neural Networks (CNN); Facial landmark; Geometric feature; Geometric texture; Human behavior understanding; Learning-based methods; Texture information; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85059762588
"Cao L., Gou C., Wang K., Xiong G., Wang F.-Y.","57191250405;56320227000;55901133200;55733323100;57211758869;","Gaze-Aided Eye Detection via Appearance Learning",2018,"Proceedings - International Conference on Pattern Recognition","2018-August",,"8545635","1965","1970",,4,"10.1109/ICPR.2018.8545635","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058784655&doi=10.1109%2fICPR.2018.8545635&partnerID=40&md5=6bc55a11c210cd1b3f430118ccfdd305","Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Qingdao Academy of Intelligent Industries, Qingdao, China; Cloud Computing Center, Chinese Academy of Sciences, Dongguan, China","Cao, L., Institute of Automation, Chinese Academy of Sciences, Beijing, China, University of Chinese Academy of Sciences, Beijing, China; Gou, C., Institute of Automation, Chinese Academy of Sciences, Beijing, China, Qingdao Academy of Intelligent Industries, Qingdao, China; Wang, K., Institute of Automation, Chinese Academy of Sciences, Beijing, China, Qingdao Academy of Intelligent Industries, Qingdao, China; Xiong, G., Institute of Automation, Chinese Academy of Sciences, Beijing, China, Cloud Computing Center, Chinese Academy of Sciences, Dongguan, China; Wang, F.-Y., Institute of Automation, Chinese Academy of Sciences, Beijing, China, Qingdao Academy of Intelligent Industries, Qingdao, China","Image based eye detection and gaze estimation have a wide range of potential applications, such as medical treatment, biometrics recognition, human-computer interaction. Though a large number of researchers have attempted to solve the two problems, they still exist some challenges due to the variation in appearance and lack of annotated images. In addition, most related work perform eye detection first, followed by gaze estimation via appearance learning. In this paper, we propose a unified framework to execute the gaze estimation and the eye detection simultaneously by learning the cascade regression models from appearance around the eye related key points. Intuitively, there is coupled relationship among location of eye center, shape of eye related key points, appearance representation and gaze information. To incorporate these information, at each cascade level, we first learn a model to map the shape and appearance around current eye related key points to the three dimension gaze update. Then, with the help of estimated gaze, we further learn a regression model to map the gaze, shape and appearance information to eye location update. By leveraging the power of cascade learning, the proposed method can alternatively optimize the two tasks of eye detection and gaze estimation. The experiments are conducted on benchmarks of GI4E and MPIIGaze. Experimental results show that our proposed method can achieve preferable results in gaze estimation and outperform the state-of-the-art methods in eye detection. © 2018 IEEE.",,"Human computer interaction; Medical imaging; Optical character recognition; Regression analysis; Appearance learning; Eye detection; Gaze estimation; Medical treatment; Regression model; State-of-the-art methods; Three dimensions; Unified framework; Eye protection",Conference Paper,"Final","",Scopus,2-s2.0-85058784655
"Sasaki M., Nagamatsu T., Takemura K.","57205675368;23398000100;8575290600;","Cross-Ratio Based Gaze Estimation using Polarization Camera System",2018,"ISS 2018 - Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces",,,,"333","338",,3,"10.1145/3279778.3279909","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061049946&doi=10.1145%2f3279778.3279909&partnerID=40&md5=3816150b45f66f2bf0ebc845bf30f3a1","Tokai University, Hiratsuka, Kanagawa, 259-1292, Japan; Kobe University, Higashinada-ku, Kobe, 658-0022, Japan","Sasaki, M., Tokai University, Hiratsuka, Kanagawa, 259-1292, Japan; Nagamatsu, T., Kobe University, Higashinada-ku, Kobe, 658-0022, Japan; Takemura, K., Tokai University, Hiratsuka, Kanagawa, 259-1292, Japan","Eye-based interaction is one of the solutions for achieving intuitive interfaces on surfaces such as a large display, and thus, various eye-tracking methods have been studied. Cross-ratio based gaze estimation, which determines the point-of-gaze on a screen, has been studied actively as a novel eye-tracking method because the method does not require a hardware calibration defining the relationship between a camera and monitor. We expect that the cross-ratio method will be a breakthrough for eye-based interaction under various circumstances such as tabletop devices and digital whiteboards. In eye-tracking, near-infrared light is often emitted, and at least four LEDs are located on display corners for detecting the screen plane in the cross-ratio based method. However, long-time radiation of near-infrared light can make a user fatigued. Therefore, in this study, we attempted to extract the screen area correctly without near-infrared radiation emission. A polarizing filter is included in the display, and thus, visibility of the screen can be controlled by the light's polarization direction of the external polarized light filter. We propose gaze estimation based on the cross-ratio method using a developed polarization camera system, which can capture two polarized images of different angles simultaneously. Further, we confirmed that the point-of-gaze could be estimated using the screen reflection detected by computing the differences between two images without near-infrared emission. © 2018 Authors.","Cross-ratio method; Eye gaze estimation; Polarized image","Cameras; Digital devices; Edge detection; Infrared devices; Infrared radiation; Polarization; Cross-ratios; Eye tracking methods; Eye-gaze; Intuitive interfaces; Near-infrared emissions; Near-infrared radiations; Polarization direction; Polarized image; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85061049946
"Zhao T., Yan Y., Shehu I.S., Wei H., Fu X.","57192707963;57204644440;56495148400;57205302391;7402204912;","Image Purification through Controllable Neural Style Transfer",2018,"9th International Conference on Information and Communication Technology Convergence: ICT Convergence Powered by Smart Intelligence, ICTC 2018",,,"8539637","466","471",,1,"10.1109/ICTC.2018.8539637","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059473471&doi=10.1109%2fICTC.2018.8539637&partnerID=40&md5=a6e147a4ea3699d62dac0439f456c186","Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Yan, Y., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Shehu, I.S., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Wei, H., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Recently, progress in learning-by-synthesis proposed training models on synthetic images that can effectively reduce the cost of human and material resources. However, learning from synthetic images still cannot achieve the desired performance due to the different distribution of synthetic images compared to naturalistic images. Naturalistic images are composed of multiform light distribution, which is a characteristic of the outdoor scene. In an attempt to address this issue, previous methods learn a model to improve the realism of synthetic images. Differently from previous methods, this paper takes the first step towards purifying the naturalistic image to weaken the influence of light and convert the distribution of outdoor naturalistic image through a style transfer task to that of indoor synthetic image. This paper proposes therefore, a controlled neural style transfer network to preserve image structure, accelerate model convergence rate and adapt to multi-scale images. A mixed research approach (qualitative and quantitative) was adopted for the experiments carried out to demonstrate the possibility of purifying naturalistic images of complex distribution. Qualitatively, it compares the proposed method with baseline methods across several indoor and outdoor scenes of the LPW dataset. While quantitatively it evaluates the purified images by training models for gaze estimation on cross-dataset. Results show a significant improvement over using raw naturalistic images and when compared with the baseline methods. © 2018 IEEE.","Cross-subject; Gaze estimation; Image Purification; Learning-by-synthesis; Style Transfer","Purification; Cross-subject; Different distributions; Gaze estimation; Light distribution; Material resources; Model convergence; Research approach; Style Transfer; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85059473471
"Wang Y., Cai W., Gu T., Shao W., Khalil I., Xu X.","57117129900;16232517100;7102898974;57208885593;24830581000;35222643600;","GazeRevealer: Inferring password using smartphone front camera",2018,"ACM International Conference Proceeding Series",,,,"254","263",,,"10.1145/3286978.3287026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060050837&doi=10.1145%2f3286978.3287026&partnerID=40&md5=b0ac4bb1ddb9d14d4a76a27b0999069d","Northwestern Polytechnical University, China; RMIT University, Australia; Hangzhou Dianzi University, China","Wang, Y., Northwestern Polytechnical University, China; Cai, W., Northwestern Polytechnical University, China; Gu, T., RMIT University, Australia; Shao, W., RMIT University, Australia; Khalil, I., RMIT University, Australia; Xu, X., Hangzhou Dianzi University, China","The widespread use of smartphones has brought great convenience to our daily lives, while at the same time we have been increasingly exposed to security threats. Keystroke security is an essential element in user privacy protection. In this paper,we present GazeRevealer, a novel side-channel based keystroke inference framework to infer sensitive inputs on smartphone from video recordings of victim’s eye patterns captured from smartphone front camera. We observe that eye movements typically follow the keystrokes typing on the number-only soft keyboard during password input. By exploiting eye patterns, we are able to infer the passwords being entered. We propose a novel algorithm to extract sensitive eye pattern images from video streams, and classify different eye patterns with Support Vector Classification. We also propose a novel enhanced method to boost the inference accuracy. Compared with prior keystroke detection approaches, GazeRevealer does not require any external auxiliary devices, and it relies only on smartphone front camera. We evaluate the performance of GazeRevealer with three different types of smartphones, and the result shows that GazeRevealer achieves 77.43% detection accuracy for a single key number and 83.33% inference rate for the 6-digit password in the ideal case. © 2018 Association for Computing Machinery.","Gaze estimation; Mobile security; Password inference","Authentication; Cameras; Eye movements; Smartphones; Ubiquitous computing; Video recording; Auxiliary device; Detection accuracy; Detection approach; Essential elements; Gaze estimation; Keystroke inferences; Password inference; Support vector classification; Mobile security",Conference Paper,"Final","",Scopus,2-s2.0-85060050837
"Liu Z., Qiao F., Long H., Li G.","57191691341;57206470374;57206482402;57206472308;","GazeLabel: A cost-free data labeling system with public displays using eye-tracking",2018,"SenSys 2018 - Proceedings of the 16th Conference on Embedded Networked Sensor Systems",,,,"343","344",,1,"10.1145/3274783.3275174","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061700396&doi=10.1145%2f3274783.3275174&partnerID=40&md5=d528419897fa398d87689c6b4c33e0e2","Tsinghua University, Haidian District, Beijing, 100084, China; Columbia University, 2920 Broadway, NewYork, NY, United States; Beijing Jiaotong University, No.3 Shangyuancun, Haidian District Beijing, 100044, China; Megvii Inc. (Face++), No.2 Kexueyuan South Road, Haidian District Beijing, China","Liu, Z., Tsinghua University, Haidian District, Beijing, 100084, China; Qiao, F., Columbia University, 2920 Broadway, NewYork, NY, United States; Long, H., Beijing Jiaotong University, No.3 Shangyuancun, Haidian District Beijing, 100044, China; Li, G., Megvii Inc. (Face++), No.2 Kexueyuan South Road, Haidian District Beijing, China","In this paper, we introduce GazeLabel: a cost-free data labeling system with public displays (at bus stops, metro stations, shopping malls, etc.) using eye-tracking technology. With built-in cameras, the proposed system captures pedestrians’ gaze points on the screen, from which we can extract information and get a substantial amount of free labeled data. Based on this idea, we implemented a prototype eye-tracking data-labeling system for binary image classifications. A pilot study demonstrated the potential and practicability in a data labeling process using GazeLabel. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Cost-free; Crowd sourcing; Data labeling; Eye-tracking; Gaze estimation; Public displays","Binary images; Embedded systems; Subway stations; Data labeling; Extract informations; Eye tracking technologies; Gaze estimation; Labeled data; Metro stations; Pilot studies; Public display; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85061700396
"Brousseau B., Rose J., Eizenman M.","55601672200;35586991000;6701402159;","SmartEye: An Accurate Infrared Eye Tracking System for Smartphones",2018,"2018 9th IEEE Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2018",,,"8796799","951","959",,3,"10.1109/UEMCON.2018.8796799","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071553999&doi=10.1109%2fUEMCON.2018.8796799&partnerID=40&md5=a1147ae572eab983d746dfc11140cf2d","Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Electrical and Computer Engineering, Institute Biomedical Engineering University of Toronto, Toronto, Canada","Brousseau, B., Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Rose, J., Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Eizenman, M., Electrical and Computer Engineering, Institute Biomedical Engineering University of Toronto, Toronto, Canada","The capability to estimate where a user is looking on a screen is known as gaze estimation or eye tracking. It has been used in medical applications including assessment of mood and learning disorders, and brain injury diagnosis. If accurate eye tracking could be integrated into commodity smartphones these diagnostics could be broadly deployed at very low cost. The highest accuracy and most robust eye tracking methods employ infrared cameras and illumination which are not yet available on all standard smartphones. In this paper, we present an accurate infrared eye tracking system on a smartphone, named SmartEye, on an industrial prototype phone equipped with an infrared camera and illumination. The system is accurate in the presence of head pose variation and device movements in the user's hands, and requires only a one-time calibration routine to measure specific parameters of the user's eye. Our system achieves a gaze estimation bias of 0.57° at a 20cm distance from the user, 5 times better than state-of-the art mobile device eye-tracking systems that do not use infrared illumination. Our system also allows for free head movements at distances between 20-40cm with a moderate increase in average gaze bias (to ~1°), and can operate at 12fps. This enhanced accuracy and increased mobility can expand significantly the range of eye-tracking applications that can be supported by smartphones. © 2018 IEEE.","Eye Tracking; Gaze Estimation; Mobile Computing; Mobile Eye-Tracking; Gaze-Based Interaction","Diagnosis; Eye movements; Infrared devices; Medical applications; mHealth; Mobile telecommunication systems; Smartphones; Temperature indicating cameras; Ubiquitous computing; Brain injury; Eye tracking methods; Eye tracking systems; Gaze estimation; Gaze-based interaction; Infra-red cameras; Infrared illumination; State of the art; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85071553999
"Wu J., Zhong S.-H., Ma Z., Heinen S.J., Jiang J.","57189367891;36844960400;57220884718;7003536110;57193403110;","Foveated convolutional neural networks for video summarization",2018,"Multimedia Tools and Applications","77","22",,"29245","29267",,6,"10.1007/s11042-018-5953-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054786171&doi=10.1007%2fs11042-018-5953-1&partnerID=40&md5=477195667b5ade331bdc5aff360fbd4a","The College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States","Wu, J., The College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; Zhong, S.-H., The College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; Ma, Z., Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; Heinen, S.J., Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; Jiang, J., The College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China","With the proliferation of video data, video summarization is an ideal tool for users to browse video content rapidly. In this paper, we propose a novel foveated convolutional neural networks for dynamic video summarization. We are the first to integrate gaze information into a deep learning network for video summarization. Foveated images are constructed based on subjects’ eye movements to represent the spatial information of the input video. Multi-frame motion vectors are stacked across several adjacent frames to convey the motion clues. To evaluate the proposed method, experiments are conducted on two video summarization benchmark datasets. The experimental results validate the effectiveness of the gaze information for video summarization despite the fact that the eye movements are collected from different subjects from those who generated summaries. Empirical validations also demonstrate that our proposed foveated convolutional neural networks for video summarization can achieve state-of-the-art performances on these benchmark datasets. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks; Eye movement; Foveated image; Video summarization","Convolution; Deep learning; Neural networks; Video recording; Benchmark datasets; Convolutional neural network; Empirical validation; Foveated images; Learning network; Spatial informations; State-of-the-art performance; Video summarization; Eye movements",Article,"Final","",Scopus,2-s2.0-85054786171
"Kononenko D., Ganin Y., Sungatullina D., Lempitsky V.","57142551900;56938634700;55821404200;22234735100;","Photorealistic Monocular Gaze Redirection Using Machine Learning",2018,"IEEE Transactions on Pattern Analysis and Machine Intelligence","40","11","8010348","2696","2710",,6,"10.1109/TPAMI.2017.2737423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028454003&doi=10.1109%2fTPAMI.2017.2737423&partnerID=40&md5=5967e9341855f072d35ffd09c0f030c4","Skolkovo Institute of Science and Technology, Moscow, 143026, Russian Federation; Universite de Montreal, Montreal, Quebec  136800, Canada","Kononenko, D., Skolkovo Institute of Science and Technology, Moscow, 143026, Russian Federation; Ganin, Y., Universite de Montreal, Montreal, Quebec  136800, Canada; Sungatullina, D., Skolkovo Institute of Science and Technology, Moscow, 143026, Russian Federation; Lempitsky, V., Skolkovo Institute of Science and Technology, Moscow, 143026, Russian Federation","We propose a general approach to the gaze redirection problem in images that utilizes machine learning. The idea is to learn to re-synthesize images by training on pairs of images with known disparities between gaze directions. We show that such learning-based re-synthesis can achieve convincing gaze redirection based on monocular input, and that the learned systems generalize well to people and imaging conditions unseen during training. We describe and compare three instantiations of our idea. The first system is based on efficient decision forest predictors and redirects the gaze by a fixed angle in real-time (on a single CPU), being particularly suitable for the videoconferencing gaze correction. The second system is based on a deep architecture and allows gaze redirection by a range of angles. The second system achieves higher photorealism, while being several times slower. The third system is based on real-time decision forests at test time, while using the supervision from a 'teacher' deep network during training. The third system approaches the quality of a teacher network in our experiments, and thus provides a highly realistic real-time monocular solution to the gaze correction problem. We present in-depth assessment and comparisons of the proposed systems based on quantitative measurements and a user study. © 1979-2012 IEEE.","deep learning; Gaze redirection; image resynthesis; machine learning; random forest; weakly-supervised learning","Artificial intelligence; Decision trees; Image processing; Learning systems; Gaze direction; Gaze redirection; Imaging conditions; Photo-realistic; Random forests; Resynthesis; Weakly supervised learning; Deep learning; anatomy and histology; artificial neural network; decision tree; eye fixation; eye movement; face; facial expression; factual database; female; human; image processing; machine learning; male; procedures; videoconferencing; Databases, Factual; Decision Trees; Deep Learning; Eye Movements; Face; Facial Expression; Female; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Machine Learning; Male; Neural Networks (Computer); Videoconferencing",Article,"Final","",Scopus,2-s2.0-85028454003
"Kar A., Corcoran P.","56956378200;57190839462;","Gaze Visual-A Graphical Software Tool for Performance Evaluation of Eye Gaze Estimation Systems",2018,"2018 IEEE Games, Entertainment, Media Conference, GEM 2018",,,"8516487","282","287",,,"10.1109/GEM.2018.8516487","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057060134&doi=10.1109%2fGEM.2018.8516487&partnerID=40&md5=9da2e6cf559e2f7ccfdc6ff1d1d45abe","College of Engineering Informatics NUI Galway, Center for Cognitive Connected Computational Imaging, Galway, Ireland","Kar, A., College of Engineering Informatics NUI Galway, Center for Cognitive Connected Computational Imaging, Galway, Ireland; Corcoran, P., College of Engineering Informatics NUI Galway, Center for Cognitive Connected Computational Imaging, Galway, Ireland","The concept of an open source software developed for all round performance evaluation of gaze tracking systems is presented. The capabilities of this software towards quantitative, statistical and visual analysis of gaze data are discussed. Potential utilities of this software are towards understanding a gaze tracker's behavior, gaze data quality, and improving usability of gaze based applications that are currently very popular in augmented/virtual reality, gaming and multimedia domains. © 2018 IEEE.","Accuracy metrics; Eye gaze tracking; Open source; Performance evaluation; User interface; Visualizations","Application programs; Open source software; Open systems; User interfaces; Visualization; Accuracy metrics; Augmented/virtual reality; Eye gaze tracking; Gaze tracking system; Open sources; Performance evaluations; Potential utility; Visual analysis; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85057060134
"Lemley J., Kar A., Corcoran P.","23005117400;56956378200;57190839462;","Eye Tracking in Augmented Spaces: A Deep Learning Approach",2018,"2018 IEEE Games, Entertainment, Media Conference, GEM 2018",,,"8516529","396","401",,4,"10.1109/GEM.2018.8516529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057042612&doi=10.1109%2fGEM.2018.8516529&partnerID=40&md5=812f1f4908c656975d3169bbda75becb","NUI Galway College of Engineering Informatics, Center for Cognitive Connected Computational Imaging, Galway, Ireland","Lemley, J., NUI Galway College of Engineering Informatics, Center for Cognitive Connected Computational Imaging, Galway, Ireland; Kar, A., NUI Galway College of Engineering Informatics, Center for Cognitive Connected Computational Imaging, Galway, Ireland; Corcoran, P., NUI Galway College of Engineering Informatics, Center for Cognitive Connected Computational Imaging, Galway, Ireland","The use of deep learning for estimating eye gaze in augmented spaces is investigated in this work. There are two primary ways of interacting with augmented spaces. The first involves the use of AR/VR systems; the second involves devices that respond to the user's gaze directly. This domain can overlap with AR/VR environments but is not exclusive to them and contains its own unique set of issues. Deep learning methods for eye tracking that are capable of performing with minimal power consumption are investigated for both problems. © 2018 IEEE.","Augmented reality; Convolutional neural networks; Deep learning; Gaze estimation; Smart spaces; Virtual reality","Augmented reality; Eye tracking; Neural networks; Virtual reality; Convolutional neural network; Eye-gaze; Gaze estimation; Learning approach; Learning methods; Smart space; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85057042612
"Xia Y., Lou J., Dong J., Li G., Yu H.","57192671097;57192670261;22634069200;13408307700;56115992300;","SDM-Based means of gradient for eye center localization",2018,"Proceedings - IEEE 16th International Conference on Dependable, Autonomic and Secure Computing, IEEE 16th International Conference on Pervasive Intelligence and Computing, IEEE 4th International Conference on Big Data Intelligence and Computing and IEEE 3rd Cyber Science and Technology Congress, DASC-PICom-DataCom-CyberSciTec 2018",,,"8511989","857","861",,4,"10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00-17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056894893&doi=10.1109%2fDASC%2fPiCom%2fDataCom%2fCyberSciTec.2018.00-17&partnerID=40&md5=ae3f253f305eefa034765cf20d9ece07","1School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Key Laboratory of Metallurgical Equipment and Control Technology, Ministry of Education, Wuhan University of Science and Technology, Wuhan, 430081, China","Xia, Y., 1School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Lou, J., 1School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Dong, J., 1School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Li, G., Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Yu, H., Key Laboratory of Metallurgical Equipment and Control Technology, Ministry of Education, Wuhan University of Science and Technology, Wuhan, 430081, China","For eye gaze estimation and eye tracking, localizing eye center is a crucial requirement. This task is challenging work because of the significant variability of eye appearance in illumination, shape, color and viewing angles. In this paper, we improve the performance of means of gradient method in low resolution images, which could locate the eye center more accurately. The proposed method applies Supervised Descent Method (SDM), which has remarkable achievement in the field of face alignment, to improve the traditional means of gradient method in localizing eye center. We extensively evaluate our method on BioID database which is very challenging and realistic for eye center localization. Moreover, we have com-pared our method with existing state of the art methods and the results of the experiment confirm that the proposed method is an attractive alternative for eye center localization. © 2018 IEEE.","Eye Center Localization; Eye Gaze Estimation; Means of Gradient.","Big data; Gradient methods; Image enhancement; Descent method; Experiment confirm; Eye Center Localization; Eye-gaze; Face alignment; Low resolution images; State-of-the-art methods; Viewing angle; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85056894893
"Song G., Zheng J., Cai J., Zhang J., Cham T.-J., Fuchs H.","57204979749;57194827755;7403153287;25423412200;7003391055;7201917402;","Real-time 3D face-eye performance capture of a person wearing vr headset",2018,"MM 2018 - Proceedings of the 2018 ACM Multimedia Conference",,,,"923","931",,2,"10.1145/3240508.3240570","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058230557&doi=10.1145%2f3240508.3240570&partnerID=40&md5=a7f50144a948cc2cd3d8ce9c2f4638e5","Nanyang Technological University, China; University of Science and Technology of China, China; University of North Carolina, Chapel Hill, United States","Song, G., Nanyang Technological University, China; Zheng, J., Nanyang Technological University, China; Cai, J., Nanyang Technological University, China; Zhang, J., University of Science and Technology of China, China; Cham, T.-J., Nanyang Technological University, China; Fuchs, H., University of North Carolina, Chapel Hill, United States","Teleconference or telepresence based on virtual reality (VR) head-mount display (HMD) device is a very interesting and promising application since HMD can provide immersive feelings for users. However, in order to facilitate face-to-face communications for HMD users, real-time 3D facial performance capture of a person wearing HMD is needed, which is a very challenging task due to the large occlusion caused by HMD. The existing limited solutions are very complex either in setting or in approach as well as lacking the performance capture of 3D eye gaze movement. In this paper, we propose a convolutional neural network (CNN) based solution for real-time 3D face-eye performance capture of HMD users without complex modification to devices. To address the issue of lacking training data, we generate massive pairs of HMD face-label dataset by data synthesis as well as collecting VR-IR eye dataset from multiple subjects. Then, we train a dense-fitting network for facial region and an eye gaze network to regress 3D eye model parameters. Extensive experimental results demonstrate that our system can efficiently and effectively produce in real time a vivid personalized 3D avatar with the correct identity, pose, expression and eye motion corresponding to the HMD user. © 2018 Association for Computing Machinery.","3D facial reconstruction; Gaze estimation; HMDs","Complex networks; Eye movements; Neural networks; Three dimensional computer graphics; Virtual reality; Visual communication; Wear of materials; 3d facial reconstruction; Convolutional Neural Networks (CNN); Data synthesis; Face-to-face communications; Gaze estimation; Head-mount displays; Large occlusion; Performance capture; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-85058230557
"Joo J., Steinert-Threlkeld Z.C., Luo J.","55922771100;57131058800;7404182441;","Social and political event analysis based on rich media",2018,"MM 2018 - Proceedings of the 2018 ACM Multimedia Conference",,,,"2093","2095",,1,"10.1145/3240508.3241470","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058221742&doi=10.1145%2f3240508.3241470&partnerID=40&md5=a25ea6e1d695c98a970529e0f9441d27","UCLA, United States; University of Rochester, United States","Joo, J., UCLA, United States; Steinert-Threlkeld, Z.C., UCLA, United States; Luo, J., University of Rochester, United States","This tutorial aims to provide a comprehensive overview on the applications of rich social media data for real world social and political event analysis, which is a new emerging topic in multimedia research. We will discuss the recent evolution of social media as venues for social and political interaction and their impacts on the real world events using specific examples. We will introduce large scale datasets drawn from social media sources and review concrete research projects that build on computer vision and deep learning based methods. Existing researches in social media have examined various patterns of information diffusion and contagion, user activities and networking, and social media-based predictions of real world events. Most existing works, however, rely on non-content or text based features and do not fully leverage rich multiple modalities - visuals and acoustics - which are prevalent in most online social media. Such approaches underutilize vibrant and integrated characteristics of social media especially because the current audiences are getting more attracted to visual information centric media. This proposal highlights the impacts of rich multimodal data to the real world events and elaborates on relevant recent research projects - the concrete development, data governance, technical details, and their implications to politics and society - on the following topics. 1) Decoding non-verbal content to identify intent and impact in political messages in mass and social media, such as political advertisements, debates, or news footage; 2) Recognition of emotion, expressions, and viewer perception from communicative gestures, gazes, and facial expressions; 3) Geo-coded Twitter image analysis for protest and social movement analysis; 4) Election outcome prediction and voter understanding by using social media post; and 5) Detection of misinformation, rumors, and fake news and analyzing their impacts in major political events such as the U.S. presidential election. © 2018 Copyright held by the owner/author(s).","Election Prediction; Fake News; Misinformation; Political Analysis; Protest and Social Movements; Social Media Analysis","Behavioral research; Concretes; Deep learning; Forecasting; Systems analysis; Fake News; Misinformation; Political analysis; Social media analysis; Social movements; Social networking (online)",Conference Paper,"Final","",Scopus,2-s2.0-85058221742
"Zhao T., Yan Y., Shehu I.S., Fu X.","57192707963;57204644440;56495148400;7402204912;","Image Purification Networks: Real-time Style Transfer with Semantics through Feed-forward Synthesis",2018,"Proceedings of the International Joint Conference on Neural Networks","2018-July",,"8489365","","",,5,"10.1109/IJCNN.2018.8489365","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056521893&doi=10.1109%2fIJCNN.2018.8489365&partnerID=40&md5=5deeb394074e97c1c6955a702ba034a6","Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Yan, Y., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Shehu, I.S., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Image synthesis has been widely accepted as a cost effective way to learn models because it provides training sets that are large, diverse and accurately labeled. However, the realism of the synthetic image is not enough, this affects generalization on naturalistic test image. In an attempt to address this issue, previous methods learn a model to improve the realism of synthetic image. Differently, from previous methods, we take the first step towards purifying the naturalistic image to weaken the influence of light and convert the distribution of an outdoor naturalistic image through a real-time style transfer task to that of indoor synthetic image. This paper proposes, therefore a real-time image purification networks that transfer style information with semantics through a feed-forward synthesis. Results from our experiments demonstrate that images purified through the proposed networks architecture trained models for gaze estimation more accurately on cross-datasets over using raw naturalistic images and when compared to baseline methods. © 2018 IEEE.",,"Cost effectiveness; Purification; Semantic Web; Semantics; Baseline methods; Cost effective; Feed forward; Gaze estimation; Image synthesis; Real time images; Synthetic images; Training sets; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85056521893
"Herurkar D., Dengel A., Ishimaru S.","57205026556;6603764314;55876558900;","Poster: Combining software-based eye tracking and a wide-angle lens for sneaking detection",2018,"UbiComp/ISWC 2018 - Adjunct Proceedings of the 2018 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2018 ACM International Symposium on Wearable Computers",,,,"54","57",,,"10.1145/3267305.3267675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058301836&doi=10.1145%2f3267305.3267675&partnerID=40&md5=f60bf3385c3001e314707ee84a306a32","University of Kaiserslautern, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany","Herurkar, D., University of Kaiserslautern, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Dengel, A., University of Kaiserslautern, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Ishimaru, S., University of Kaiserslautern, German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany","This paper proposes Sneaking Detector, a system which recognizes sneaking on a laptop screen by other people and alerts the owner through several interventions. We utilize a pre-trained deep learning network to estimate eye gaze of sneakers captured by a front-facing camera. Since most of the cameras equipped on laptop computers cannot cover a wide enough range, a commercial wide-angle lens attachment and an image processing are applied in our system. On the dataset involving nine participants following four experiments, it has been realized that our system can estimate the horizontal eye gaze and recognizes whether a sneaker is looking at a screen or not with 78% accuracy. © Copyright held by the owner/author(s).","Deep learning; Eye tracking; Image processing; Sneaking","Cameras; Deep learning; Image processing; Laptop computers; Ubiquitous computing; Wearable computers; Eye-gaze; Learning network; Sneaking; Wide-angle lens; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85058301836
"Nath A.","57204671629;","Responding with sentiment appropriate for the user's current sentiment in dialog as inferred from prosody and gaze patterns",2018,"ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction",,,,"529","533",,1,"10.1145/3242969.3264974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056660136&doi=10.1145%2f3242969.3264974&partnerID=40&md5=09c093ec69cc68edf490708cff016ccc","University of Texas at El Paso, El Paso, TX, United States","Nath, A., University of Texas at El Paso, El Paso, TX, United States","Multi-modal sentiment detection from natural video/audio streams has recently received much attention. I propose to use this multi-modal information to develop a technique, Sentiment Coloring, that utilizes the detected sentiments to generate effective responses. In particular, I aim to produce suggested responses colored with sentiment appropriate for that present in the interlocutor's speech. To achieve this, contextual information pertaining to sentiment, extracted from the past as well as the current speech of both the speakers in a dialog, will be utilized. Sentiment, here, includes the three polarities: positive, neutral and negative, as well as other expressions of stance and attitude. Utilizing only the non-verbal cues, namely, prosody and gaze, I will implement two algorithmic approaches and compare their performance in sentiment detection: a simple machine learning algorithm (neural networks), that will act as the baseline, and a deep learning approach, an end-to-end bidirectional LSTM RNN, which is the state-of-the-art in emotion classification. I will build a responsive spoken dialog system(s) with this Sentiment Coloring technique and evaluate the same with human subjects to measure benefits of the technique in various interactive environments. © 2018 Association for Computing Machinery.","Gaze; Multi-modal sentiment detection; Prosody; Responsiveness; Sentiment coloring","Deep learning; Interactive computer systems; Long short-term memory; Machinery; Contextual information; Emotion classification; Gaze; Interactive Environments; Multi-modal information; Prosody; Responsiveness; Sentiment detections; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85056660136
"Kaur A.","57220992105;","Attention network for engagement prediction in the wild",2018,"ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction",,,,"516","519",,2,"10.1145/3242969.3264972","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056642001&doi=10.1145%2f3242969.3264972&partnerID=40&md5=9926e47e244e150b95150b3297acd84c","Indian Institute of Technology Ropar, India","Kaur, A., Indian Institute of Technology Ropar, India","Analysis of the student engagement in an e-learning environment would facilitate effective task accomplishment and learning. Generally, engagement/disengagement can be estimated from facial expressions, body movements and gaze pattern. The focus of this Ph.D. work is to explore automatic student engagement assessment while watching Massive Open Online Courses (MOOCs) video material in the real-world environment. Most of the work till now in this area has been focusing on engagement assessment in lab-controlled environments. There are several challenges involved in moving from lab-controlled environments to real-world scenarios such as face tracking, illumination, occlusion, and context. The early work in this Ph.D. project explores the student engagement while watching MOOCs. The unavailability of any publicly available dataset in the domain of user engagement motivates to collect dataset in this direction. The dataset contains 195 videos captured from 78 subjects which are about 16.5 hours of recording. This dataset is independently annotated by different labelers and final label is derived from the statistical analysis of the individual labels given by the different annotators. Various traditional machine learning algorithm and deep learning based networks are used to derive baseline of the dataset. Engagement prediction and localization are modeled as Multi-Instance Learning (MIL) problem. In this work, the importance of Hierarchical Attention Network (HAN) is studied. This architecture is motivated from the hierarchical nature of the problem where a video is made up of segments and segments are made up of frames. © 2018 Association for Computing Machinery.",,"Computer aided instruction; Deep learning; E-learning; Human computer interaction; Interactive computer systems; Students; Controlled environment; E-learning environment; Massive open online course; Multi-instance learning; Real world environments; Real-world scenario; Student engagement; Task accomplishment; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85056642001
"Otsuka K., Kasuga K., Köhler M.","35303289200;57204683552;57204663938;","Estimating visual focus of attention in multiparty meetings using deep convolutional neural networks",2018,"ICMI 2018 - Proceedings of the 2018 International Conference on Multimodal Interaction",,,,"191","199",,10,"10.1145/3242969.3242973","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056629148&doi=10.1145%2f3242969.3242973&partnerID=40&md5=39fbff3bfd999bf59480087ff475f905","NTT Communication Science Labs, Atsugi-shi, Japan; Nagaoka University of Technology, NTT Communication Science Labs, Nagaoka-shi, Japan; University of Tübingen, Tübingen, Germany","Otsuka, K., NTT Communication Science Labs, Atsugi-shi, Japan; Kasuga, K., Nagaoka University of Technology, NTT Communication Science Labs, Nagaoka-shi, Japan; Köhler, M., University of Tübingen, Tübingen, Germany","Convolutional neural networks (CNNs) are employed to estimate the visual focus of attention (VFoA), also called gaze direction, in multiparty face-to-face meetings on the basis of multimodal nonverbal behaviors including head pose, direction of the eyeball, and presence/absence of utterance. To reveal the potential of CNNs, we focus on aspects of multimodal and multiparty fusion including individual/group models, early/late fusion, and robustness when using inputs from image-based trackers. In contrast to the individual model that separately targets each person specific to one's seat, the group model aims to jointly estimate the gaze directions of all participants. Experiments confirmed that the group model outperformed the individual model especially in predicting listeners' VFoA when the inputs did not include eyeball directions. This result indicates that the group CNN model can implicitly learn underlying conversation structures, e.g., the listeners' gazes converge on the speaker. When the eyeball direction feature is available, both models outperformed the Bayes models used for comparison. In this case, the individual model was superior to the group model, particularly in estimating the speaker's VFoA. Moreover, it was revealed that in group models, two-stage late fusion, which integrates an individual features first, and multiparty features second, outperformed other structures. Furthermore, our experiment confirmed that image-based tracking can provide a comparable level of performance to that of sensor-based measurements. Overall, the results suggest that the CNN is a promising approach for VFoA estimation. © 2018 Association for Computing Machinery.","Convolutional neural networks; Deep learning; Gaze; Meeting analysis; Multimodal fusion; Visual focus of attention","Bayesian networks; Convolution; Deep learning; Interactive computer systems; Neural networks; Convolutional neural network; Gaze; Meeting analysis; Multi-modal fusion; Visual focus of attentions; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85056629148
"Wachner A., Edinger J., Becker C.","57204629801;56178122500;55683104700;","Towards Gaze-Based Mobile Device Interaction for the Disabled",2018,"2018 IEEE International Conference on Pervasive Computing and Communications Workshops, PerCom Workshops 2018",,,"8480159","397","402",,,"10.1109/PERCOMW.2018.8480159","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056454544&doi=10.1109%2fPERCOMW.2018.8480159&partnerID=40&md5=c57fb9c858400609d16ef9d4d1cfd9a0","University of Mannheim, Schloss, Mannheim, 68161, Germany","Wachner, A., University of Mannheim, Schloss, Mannheim, 68161, Germany; Edinger, J., University of Mannheim, Schloss, Mannheim, 68161, Germany; Becker, C., University of Mannheim, Schloss, Mannheim, 68161, Germany","Manual input is the dominant way to interact with mobile devices in everyday life. Innovations like multi-touch displays and virtual keyboards that allow user input by sliding a finger over the letters make this input more and more convenient. However, these systems assume that users have full control over their hand movements. This assumption excludes millions of people who suffer from loss of extremities, paralyses, or spasticities. In this paper, we motivate the urgent need for input mechanisms that make these technologies available for everybody. We focus on gaze-based interaction as eye movements can be tracked by built-in cameras of unmodified mobile devices without any further hardware requirements. As accurate point of gaze estimation on mobile devices is nontrivial, we propose a middleware that is not based on the absolute position of the pupils but rather takes eye movement patterns into account. This paper has four contributions. First, we motivate the need for alternative inputs methods besides manual input. Second, we discuss existing approaches and reveal their limitations. Third, we propose a middleware that allows gaze-based user interaction with mobile devices. Fourth, we provide a framework that allows developers to easily integrate gaze-based control into mobile applications. © 2018 IEEE.",,"Middleware; Mobile devices; Stereo vision; Ubiquitous computing; Absolute position; Alternative input; Eye movement patterns; Gaze-based interaction; Mobile applications; Mobile device interactions; User interaction; Virtual Keyboards; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85056454544
"Cornia M., Baraldi L., Serra G., Cucchiara R.","57192079235;55919206200;10140344600;7006870483;","Predicting human eye fixations via an LSTM-Based saliency attentive model",2018,"IEEE Transactions on Image Processing","27","10",,"5142","5154",,214,"10.1109/TIP.2018.2851672","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049317191&doi=10.1109%2fTIP.2018.2851672&partnerID=40&md5=efe7efe02932c512fd7e95372030a40a","Department of Engineering 'Enzo Ferrari', University of Modena and Reggio Emilia, Modena, Italy; Department of Computer Science Mathematics and Physics, University of Udine, Udine, Italy","Cornia, M., Department of Engineering 'Enzo Ferrari', University of Modena and Reggio Emilia, Modena, Italy; Baraldi, L., Department of Engineering 'Enzo Ferrari', University of Modena and Reggio Emilia, Modena, Italy; Serra, G., Department of Computer Science Mathematics and Physics, University of Udine, Udine, Italy; Cucchiara, R., Department of Engineering 'Enzo Ferrari', University of Modena and Reggio Emilia, Modena, Italy","Data-driven saliency has recently gained a lot of attention thanks to the use of convolutional neural networks for predicting gaze fixations. In this paper, we go beyond standard approaches to saliency prediction, in which gaze maps are computed with a feed-forward network, and present a novel model which can predict accurate saliency maps by incorporating neural attentive mechanisms. The core of our solution is a convolutional long short-term memory that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map. In addition, to tackle the center bias typical of human eye fixations, our model can learn a set of prior maps generated with Gaussian functions. We show, through an extensive evaluation, that the proposed architecture outperforms the current state-of-the-art on public saliency prediction datasets. We further study the contribution of each key component to demonstrate their robustness on different scenarios. © 1992-2012 IEEE.","convolutional neural networks; deep learning; human eye fixations; Saliency","Convolution; Deep learning; Forecasting; Image segmentation; Convolutional neural network; Feed-forward network; Gaussian functions; Human eye; Proposed architectures; Saliency; Salient regions; State of the art; Long short-term memory",Article,"Final","",Scopus,2-s2.0-85049317191
"Xiao F., Huang K., Qiu Y., Shen H.","57206593016;54581052800;57196198075;13309317500;","Accurate iris center localization method using facial landmark, snakuscule, circle fitting and binary connected component",2018,"Multimedia Tools and Applications","77","19",,"25333","25353",,8,"10.1007/s11042-018-5787-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042348432&doi=10.1007%2fs11042-018-5787-x&partnerID=40&md5=0de6e43eb531ff84fbe273f7237800c6","Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China","Xiao, F., Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China; Huang, K., Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China; Qiu, Y., Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China; Shen, H., Institute of VLSI Design, Zhejiang University, Hangzhou, 310027, China","Iris centers have been widely used in machine vision for face matching, gaze estimation, etc. However, in low resolution eye images, the iris and its surrounding region present a variety of appearance characteristics, which make it difficult to accurately locate the iris center. In this paper, we propose a robust, accurate and real-time iris center localization method by combining the facial landmark, snakuscule, circle fitting and binary connected component. Facial landmarks are used to extract an accurate eye Region of Interest (ROI). Thereafter, a fixed size circle-based active contour snakuscule is used to detect the iris center. Based on the snakuscule center and inner radius, a novel method is proposed to extract accurate iris edges for circle fitting. In addition, the quality of the detected iris center is evaluated by a circle-binary quality evaluation method. Binary connected component method is used to improve the accuracies in those unqualified images. The proposed method is tested on three publicly available databases BioID, GI4E and Talking Face Video. The result shows that it could achieve an accuracy of 94.35% on the BioID database when the normalized error is smaller than 0.05, which outperforms all state-of-the-art methods. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.","Binary connected component; Circle fitting; Facial landmark; Iris centers; Snakuscule","Image enhancement; Image segmentation; Circle fitting; Connected component; Facial landmark; Iris centers; Snakuscule; Quality control",Article,"Final","",Scopus,2-s2.0-85042348432
"Wan Z., Wang X., Yin L., Zhou K.","56105053500;55736887200;57201689515;57194266680;","A method of free-space point-of-regard estimation based on 3D eye model and stereo vision",2018,"Applied Sciences (Switzerland)","8","10","1769","","",,,"10.3390/app8101769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054065503&doi=10.3390%2fapp8101769&partnerID=40&md5=73c68a6f9096eb45a1fc9b7c0d1af322","State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, No. 92Weijin Road, Nankai District, Tianjin, 300072, China; MOEMS Education Ministry Key Laboratory, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin, 300072, China","Wan, Z., State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, No. 92Weijin Road, Nankai District, Tianjin, 300072, China, MOEMS Education Ministry Key Laboratory, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin, 300072, China; Wang, X., State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, No. 92Weijin Road, Nankai District, Tianjin, 300072, China, MOEMS Education Ministry Key Laboratory, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin, 300072, China; Yin, L., State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, No. 92Weijin Road, Nankai District, Tianjin, 300072, China, MOEMS Education Ministry Key Laboratory, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin, 300072, China; Zhou, K., State Key Laboratory of Precision Measurement Technology and Instruments, Tianjin University, No. 92Weijin Road, Nankai District, Tianjin, 300072, China, MOEMS Education Ministry Key Laboratory, Tianjin University, No. 92 Weijin Road, Nankai District, Tianjin, 300072, China","This paper proposes a 3D point-of-regard estimation method based on 3D eye model and a corresponding head-mounted gaze tracking device. Firstly, a head-mounted gaze tracking system is given. The gaze tracking device uses two pairs of stereo cameras to capture the left and right eye images, respectively, and then sets a pair of scene cameras to capture the scene images. Secondly, a 3D eye model and the calibration process are established. Common eye features are used to estimate the eye model parameters. Thirdly, a 3D point-of-regard estimation algorithm is proposed. Three main parts of this method are summarized as follows: (1) the spatial coordinates of the eye features are directly calculated by using stereo cameras; (2) the pupil center normal is used to the initial value for the estimation of optical axis; (3) a pair of scene cameras are used to solve the actual position of the objects being watched in the calibration process, and the calibration for the proposed eye model does not need the assistance of the light source. Experimental results show that the proposed method can output the coordinates of 3D point-of-regard more accurately. © 2018 by the authors.","3D point-of-regard; Binocular eye model; Gaze estimation; Stereo vision",,Article,"Final","",Scopus,2-s2.0-85054065503
"Kar A., Corcoran P.","56956378200;57190839462;","Performance evaluation strategies for eye gaze estimation systems with quantitative metrics and visualizations",2018,"Sensors (Switzerland)","18","9","3151","","",,9,"10.3390/s18093151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053692099&doi=10.3390%2fs18093151&partnerID=40&md5=cc49bf735f0dde8848afc36928fd0312","Department of Electrical & Electronic Engineering, National University of Ireland, Galway, H91 TK33, Ireland","Kar, A., Department of Electrical & Electronic Engineering, National University of Ireland, Galway, H91 TK33, Ireland; Corcoran, P., Department of Electrical & Electronic Engineering, National University of Ireland, Galway, H91 TK33, Ireland","An eye tracker’s accuracy and system behavior play critical roles in determining the reliability and usability of eye gaze data obtained from them. However, in contemporary eye gaze research, there exists a lot of ambiguity in the definitions of gaze estimation accuracy parameters and lack of well-defined methods for evaluating the performance of eye tracking systems. In this paper, a set of fully defined evaluation metrics are therefore developed and presented for complete performance characterization of generic commercial eye trackers, when they operate under varying conditions on desktop or mobile platforms. In addition, some useful visualization methods are implemented, which will help in studying the performance and data quality of eye trackers irrespective of their design principles and application areas. Also the concept of a graphical user interface software named GazeVisual v1.1 is proposed that would integrate all these methods and enable general users to effortlessly access the described metrics, generate visualizations and extract valuable information from their own gaze datasets. We intend to present these tools as open resources in future to the eye gaze research community for use and further advancement, as a contribution towards standardization of gaze research outputs and analysis. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Eye gaze estimation; Eye tracker; Graphical user interface; Metrics; Mobile devices; Open source; Performance evaluation; Standardization; Visualizations","Data visualization; Eye movements; Graphical user interfaces; Mobile devices; Open source software; Standardization; Visualization; Eye trackers; Eye-gaze; Metrics; Open sources; Performance evaluations; Eye tracking; computer; eye fixation; eye movement; human; information processing; measurement accuracy; reproducibility; software; standards; Computers; Data Accuracy; Datasets as Topic; Eye Movements; Fixation, Ocular; Humans; Reproducibility of Results; Software",Article,"Final","",Scopus,2-s2.0-85053692099
"Geisler D., Fox D., Kasneci E.","57189847283;7402074129;56059892600;","Real-time 3D Glint Detection in Remote Eye Tracking Based on Bayesian Inference",2018,"Proceedings - IEEE International Conference on Robotics and Automation",,,"8460800","7119","7126",,2,"10.1109/ICRA.2018.8460800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063155664&doi=10.1109%2fICRA.2018.8460800&partnerID=40&md5=85671ef423e7a1fa14d46221395e7ecb","Department of Perception Engineering, University of Tuebingen, Germany; Department of Computer Science Engineering, University of Washington, Seattle, United States","Geisler, D., Department of Perception Engineering, University of Tuebingen, Germany; Fox, D., Department of Computer Science Engineering, University of Washington, Seattle, United States; Kasneci, E., Department of Perception Engineering, University of Tuebingen, Germany","As human gaze provides information on our cognitive states, actions, and intentions, gaze-based interaction has the potential to enable a fluent and natural human-robot collaboration. In this work, we focus on reliable gaze estimation in remote eye tracking based on calibration-free methods. Although these methods work well in controlled settings, they fail when illumination conditions change or other objects induce noise. We propose a novel, adaptive method based on a probabilistic model, which reliably detects glints from stereo images and evaluate our method using a data set that contains different challenges with regarding to light and reflections. © 2018 IEEE.",,"Bayesian networks; Human robot interaction; Inference engines; Robotics; Stereo image processing; Adaptive methods; Bayesian inference; Calibration-free methods; Gaze estimation; Gaze-based interaction; Human-robot collaboration; Illumination conditions; Probabilistic modeling; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85063155664
"Khamis M., Alt F., Bulling A.","35243028400;27267528900;6505807414;","The Past, Present, and Future of Gaze-enabled Handheld Mobile Devices: Survey and lessons learned",2018,"MobileHCI 2018 - Beyond Mobile: The Next 20 Years - 20th International Conference on Human-Computer Interaction with Mobile Devices and Services, Conference Proceedings",,,"a38","","",,35,"10.1145/3229434.3229452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056672448&doi=10.1145%2f3229434.3229452&partnerID=40&md5=2a0cb36885749b610205677b855abf88","LMU Munich, Germany; University of Glasgow, United Kingdom; Munich University of Applied Sciences, Germany; Bundeswehr University, Munich, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Khamis, M., LMU Munich, Germany, University of Glasgow, United Kingdom; Alt, F., LMU Munich, Germany, Munich University of Applied Sciences, Germany, Bundeswehr University, Munich, Germany; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","While first-generation mobile gaze interfaces required special-purpose hardware, recent advances in computational gaze estimation and the availability of sensor-rich and powerful devices is finally fulfilling the promise of pervasive eye tracking and eye-based interaction on off-the-shelf mobile devices. This work provides the first holistic view on the past, present, and future of eye tracking on handheld mobile devices. To this end, we discuss how research developed from building hardware prototypes, to accurate gaze estimation on unmodified smartphones and tablets. We then discuss implications by laying out 1) novel opportunities, including pervasive advertising and conducting in-the-wild eye tracking studies on handhelds, and 2) new challenges that require further research, such as visibility of the user's eyes, lighting conditions, and privacy implications. We discuss how these developments shape MobileHCI research in the future, possibly the next 20 years. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Eye Tracking; Gaze Estimation; Gaze Interaction; Mobile devices; Smartphones; Tablets","Computer hardware; Hardware; Human computer interaction; Mobile devices; Smartphones; Eye-tracking studies; Gaze estimation; Gaze interaction; Handheld mobile devices; Lighting conditions; Pervasive eye tracking; Special purpose hardware; Tablets; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85056672448
"Vora S., Rangesh A., Trivedi M.M.","57195431819;57148239200;7103153314;","Driver gaze zone estimation using convolutional neural networks: A general framework and ablative analysis",2018,"IEEE Transactions on Intelligent Vehicles","3","3","8370646","254","265",,22,"10.1109/TIV.2018.2843120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076995650&doi=10.1109%2fTIV.2018.2843120&partnerID=40&md5=885f3df84fe49b461378ba92449ef68b","Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, San Diego, CA  92093, United States","Vora, S., Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, San Diego, CA  92093, United States; Rangesh, A., Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, San Diego, CA  92093, United States; Trivedi, M.M., Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, San Diego, CA  92093, United States","Driver gaze has been shown to be an excellent surrogate for driver attention in intelligent vehicles. With the recent surge of highly autonomous vehicles, driver gaze can be useful for determining the handoff time to a human driver. While there has been significant improvement in personalized driver gaze zone estimation systems, a generalized system which is invariant to different subjects, perspectives, and scales is still lacking. We take a step toward this generalized system using convolutional neural networks (CNNs). We finetune four popular CNN architectures for this task, and provide extensive comparisons of their outputs. We additionally experiment with different input image patches, and also examine how the image size affects performance. For training and testing the networks, we collect a large naturalistic driving dataset comprising of 11 long drives, driven by ten subjects in two different cars. Our best performing model achieves an accuracy of 95.18% during cross-subject testing, outperforming current state-of-the-art techniques for this task. Finally, we evaluate our best performing model on the publicly available Columbia gaze dataset comprising of images from 56 subjects with varying head pose and gaze directions. Without any training, our model successfully encodes the different gaze directions on this diverse dataset, demonstrating good generalization capabilities. © 2016 IEEE.","Computer vision; driver assistance system; driver distraction; driver gaze region estimation; driver information systems; driver visual attention; facial feature extraction; gaze classification; gaze estimation; gaze tracking; image classification; intelligent systems; on-road study; pattern recognition","Advanced driver assistance systems; Automobile drivers; Behavioral research; Classification (of information); Computer vision; Convolution; Image classification; Intelligent systems; Large dataset; Neural networks; Pattern recognition; Pattern recognition systems; Statistical tests; Driver assistance system; Driver distractions; Driver information systems; Facial feature extraction; Gaze estimation; Gaze tracking; On-road studies; Visual Attention; Eye tracking",Article,"Final","",Scopus,2-s2.0-85076995650
"Hoonkwon K., Oh H.M., Kim M.Y.","57203978980;56028920100;56739349100;","Multiple RGB-D camera-based user intent position and object estimation",2018,"IEEE/ASME International Conference on Advanced Intelligent Mechatronics, AIM","2018-July",,"8452320","176","180",,1,"10.1109/AIM.2018.8452320","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053906824&doi=10.1109%2fAIM.2018.8452320&partnerID=40&md5=0e3122b18ebec242ff7ce21dfa9d760d","School of Electronics Engineering, IT College, Kyungpook National University, Research Center for Neurosurgical Robotic System, 80 Daehak-ro, Buk-gu, Daegu, 41566, South Korea","Hoonkwon, K., School of Electronics Engineering, IT College, Kyungpook National University, Research Center for Neurosurgical Robotic System, 80 Daehak-ro, Buk-gu, Daegu, 41566, South Korea; Oh, H.M., School of Electronics Engineering, IT College, Kyungpook National University, Research Center for Neurosurgical Robotic System, 80 Daehak-ro, Buk-gu, Daegu, 41566, South Korea; Kim, M.Y., School of Electronics Engineering, IT College, Kyungpook National University, Research Center for Neurosurgical Robotic System, 80 Daehak-ro, Buk-gu, Daegu, 41566, South Korea","Human gaze represents the area of interests of the person. By analyzing a time-series of these areas, it is possible to obtain user behavioral pattern that can be used in various fields. Well-known techniques for estimating human gaze are inconvenient because they require a wearable device, or the measurement area is relatively narrow. In this paper, a method to implement gaze estimation system using 3D view tracking with multi RGB-D camera is proposed. Surround 3D cameras are used to extract the region of interest of the user from 3D gaze estimation without wearable device in living space. To implement proposed method, first, 3D space mapping through multiple RGB-D camera calibration is performed. The resulting 3D map is the measurement area, which depends on the number and specifications of the RGB-D cameras used for this purpose. Then, when a person enters the 3D map, the face region is detected using both 2D/3D data, and 3D view tracking is implemented by detecting the gaze vector using the facial feature point and the head data center point extracted from the 3D map. Finally, when the gaze vector line intersects a specific point within the mapping space, the image coordinates corresponding to that point are extracted to implement user Intent position estimation. Applying object detection and classification algorithm to the extracted image can also estimate the intent object at that time. © 2018 IEEE.",,"Cameras; Data mining; Image segmentation; Intelligent mechatronics; Mapping; Object detection; Vector spaces; Wearable technology; Behavioral patterns; Classification algorithm; Facial feature points; Image coordinates; Object estimation; Position estimation; Region of interest; Wearable devices; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85053906824
"Verma S., Nagar P., Gupta D., Arora C.","57205167288;56539584400;57191959856;35203020100;","Making Third Person Techniques Recognize First-Person Actions in Egocentric Videos",2018,"Proceedings - International Conference on Image Processing, ICIP",,,"8451249","2301","2305",,4,"10.1109/ICIP.2018.8451249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062910865&doi=10.1109%2fICIP.2018.8451249&partnerID=40&md5=0924d6b701a6fe7271aff5c803887a4c","IIIT, Delhi, India","Verma, S., IIIT, Delhi, India; Nagar, P., IIIT, Delhi, India; Gupta, D., IIIT, Delhi, India; Arora, C., IIIT, Delhi, India","We focus on first-person action recognition from egocentric videos. Unlike third person domain, researchers have divided first-person actions into two categories: involving hand-object interactions and the ones without, and developed separate techniques for the two action categories. Further, it has been argued that traditional cues used for third person action recognition do not suffice, and egocentric specific features, such as head motion and handled objects have been used for such actions. Unlike the state-of-the-art approaches, we show that a regular two stream Convolutional Neural Network (CNN) with Long Short-Term Memory (LSTM) architecture, having separate streams for objects and motion, can generalize to all categories of first-person actions. The proposed approach unifies the feature learned by all action categories, making the proposed architecture much more practical. In an important observation, we note that the size of the objects visible in the egocentric videos is much smaller. We show that the performance of the proposed model improves after cropping and resizing frames to make the size of objects comparable to the size of ImageNet's objects. Our experiments on the standard datasets: GTEA, EGTEA Gaze+, HUJI, ADL, UTE, and Kitchen, proves that our model significantly outperforms various state-of-the-art techniques. © 2018 IEEE.","Deep Learning; Egocentric Videos; First-Person Action Recognition","Deep learning; Image enhancement; Network architecture; Action recognition; Convolutional neural network; Egocentric Videos; First person; Object interactions; Proposed architectures; State-of-the-art approach; State-of-the-art techniques; Long short-term memory",Conference Paper,"Final","",Scopus,2-s2.0-85062910865
"Ivorra E., Ortega M., Alcaniz M., Garcia-Aracil N.","55639705900;35488807900;7003335420;15762531900;","Multimodal Computer Vision Framework for Human Assistive Robotics",2018,"2018 Workshop on Metrology for Industry 4.0 and IoT, MetroInd 4.0 and IoT 2018 - Proceedings",,,"8428330","18","22",,7,"10.1109/METROI4.2018.8428330","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052543791&doi=10.1109%2fMETROI4.2018.8428330&partnerID=40&md5=b2121674ae73427c63fc18332307fdb7","Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, Spain; Biomedical Neuroengineering Group, Universidad Miguel Hernandez de Elche, Elche, Spain","Ivorra, E., Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, Spain; Ortega, M., Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, Spain; Alcaniz, M., Institute for Research and Innovation in Bioengineering, Universitat Politècnica de València, Valencia, Spain; Garcia-Aracil, N., Biomedical Neuroengineering Group, Universidad Miguel Hernandez de Elche, Elche, Spain","This paper presents a multimodal computer vision framework for human assistive robotics with the purpose of giving accessibility to persons with disabilities. The user is capable of interacting with the system just by staring. Specifically, it is possible to select the desired object as well as to indicate the intention to grasp it just by staring at it. This gaze information is provided by ©Tobii Glasses 2 that in combination with a deep learning algorithm gives the class id of the desirable object. Later, the object's pose is estimated using a RGB-D camera with a new developed technique. This technique mixes a template based algorithm with a deep learning algorithm giving a precise, realtime method for pose estimation. Once the pose is obtained, it is transformed to a grasping position in the coordinate system of the assistive robot that performs the grasping operation. © 2018 IEEE.","Assistive Robotics; Deep Learning; Eye Tracking; Object Pose Detection","Deep learning; Eye tracking; Gesture recognition; Industry 4.0; Internet of things; Robotics; Assistive robotics; Multi-modal; Object pose; Persons with disabilities; Vision frameworks; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85052543791
"Lin J., Zhou X., Chen S., Lei B.","57210574380;55743240400;24491760700;9036257200;","Simple and accurate iris center localization method based on geometric relationship of eyeball model",2018,"2018 IEEE International Conference on Information and Automation, ICIA 2018",,,"8812598","661","666",,,"10.1109/ICInfA.2018.8812598","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072322265&doi=10.1109%2fICInfA.2018.8812598&partnerID=40&md5=67a5b529354c35dd2b5f6c620fa62e7f","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China","Lin, J., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Zhou, X., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Chen, S., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Lei, B., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China","Precise iris localization plays a significant role on accurate gaze estimation. In this paper, a novel iris center localization method is proposed to deal with the iris contour change caused by the rotation of eyeball using the geometric relationship among iris center, eyeball center and iris contour. First, a face alignment method is employed to detect the face feature points that are used to initialize the iris center and eyeball locations. The three-dimensional rotation information of the eyeball is then described by the positional relationship between the centers of the eyeball and iris at the image level. The new obtained iris contour after the rotation is used to update the location of the iris center iteratively. To reduce the effect of illumination on image binarization, the cumulative histogram is introduced to adaptively get the threshold for binarization. Finally, experiments conducted on the BioID Face dataset and GI4E dataset as well as eight subjects demonstrate the superior performance of the proposed method. © 2018 IEEE.","Iris center; Localization","Cumulative histogram; Geometric relationships; Image binarization; Iris center; Localization; Localization method; Positional relationship; Three-dimensional rotation; Iterative methods",Conference Paper,"Final","",Scopus,2-s2.0-85072322265
"Zhang S., Shen L., Zhang R., Yang Y., Zhang Y.","57203158384;57203161509;57221190940;57203162123;57203155623;","Robust eye detection using deeply-learned gaze shifting path",2018,"Journal of Visual Communication and Image Representation","55",,,"654","659",,1,"10.1016/j.jvcir.2018.07.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050716666&doi=10.1016%2fj.jvcir.2018.07.013&partnerID=40&md5=6970b20e0095b13d779a8c5edbfc1855","State Grid Zhejiang Electric Power Co. Ltd., China; Marketing Department, State Grid Hangzhou Power Supply Company, Hangzhou, China; Regulation and Control Center, State Grid Hangzhou Power Supply Company, Hangzhou, China; School of Electrical Engineering, Guizhou University, Guiyang, China","Zhang, S., State Grid Zhejiang Electric Power Co. Ltd., China; Shen, L., Marketing Department, State Grid Hangzhou Power Supply Company, Hangzhou, China; Zhang, R., Regulation and Control Center, State Grid Hangzhou Power Supply Company, Hangzhou, China; Yang, Y., School of Electrical Engineering, Guizhou University, Guiyang, China; Zhang, Y., School of Electrical Engineering, Guizhou University, Guiyang, China","Eye detection is a very useful technique in many intelligent applications. Since the importance of eyes to human beings, eye detection technique is an indispensable component in intelligent systems, e.g., emotional analysis, iris detection and gaze estimation. Recently, there have been proposed a large number of methods for eye detection, wherein good performances have been achieved. But these methods cannot take human visual perception into account, that is to say, human beings will first pay attention to the eyes when they are communicating with each other, and then nose, then mouth. In addition, their geometric positions are almost fixed, i.e., eyes are above the nose and mouth, and eyes are on both sides of the nose. So in our work, a novel method for eye detection is proposed using human visual perception. More specifically, we first derive object patches from a large quantity of training images. Then, a geometry-preserved object patches ranking method is designed to effectively mimic human visual mechanism when human beings are communicating with each other. After that, these ordered object patches will be fed into CNN to extract patch-level deep features, then patch-level deep features will be represented by deep representations. Finally, eye detection can be achieved using learned deep representation. Experimental results on different database show that our method can achieve high efficiency and accuracy of eye detection. © 2018 Elsevier Inc.","Deep feature; Eye detection; Gaze shifting path","Intelligent systems; Vision; Deep feature; Emotional analysis; Eye detection; Gaze shifting path; Geometric position; Human visual perception; Intelligent applications; Number of methods; Eye protection",Article,"Final","",Scopus,2-s2.0-85050716666
"Zhang C., Yao R., Cai J.","57197780774;23567304500;57197787543;","Efficient eye typing with 9-direction gaze estimation",2018,"Multimedia Tools and Applications","77","15",,"19679","19696",,13,"10.1007/s11042-017-5426-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034743223&doi=10.1007%2fs11042-017-5426-y&partnerID=40&md5=a8b8015930b29f28ca6d2d3af82d31eb","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, 221116, China; College of Engineering & Computer Science, Australian National UniversityACT  2601, Australia","Zhang, C., School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, 221116, China; Yao, R., School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, 221116, China; Cai, J., College of Engineering & Computer Science, Australian National UniversityACT  2601, Australia","Vision based text entry systems aim to help disabled people achieve text communication using eye movement. Most previous methods have employed an existing eye tracker to predict gaze direction and designed an input method based upon that. However, these methods can result in eye tracking quality becoming easily affected by various factors and lengthy amounts of time for calibration. Our paper presents a novel efficient gaze based text input method, which has the advantage of low cost and robustness. Users can type in words by looking at an on-screen keyboard and blinking. Rather than estimate gaze angles directly to track eyes, we introduce a method that divides the human gaze into nine directions. This method can effectively improve the accuracy of making a selection by gaze and blinks. We built a Convolutional Neural Network (CNN) model for 9-direction gaze estimation. On the basis of the 9-direction gaze, we used a nine-key T9 input method which is widely used in candy bar phones. Bar phones were very popular in the world decades ago and have cultivated strong user habits and language models. To train a robust gaze estimator, we created a large-scale dataset with images of eyes sourced from 25 people. According to the results from our experiments, our CNN model is able to accurately estimate different people’s gaze under various lighting conditions. In considering disable people’s needs, we removed the complex calibration process. The input methods can run in screen mode and portable off-screen mode. Moreover, The datasets used in our experiments are made available to the community to allow further research. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural network; Eye tracking; Gaze estimation; Human-computer interaction","Calibration; Convolution; Eye movements; Human computer interaction; Neural networks; Telephone sets; Calibration process; Convolutional neural network; Convolutional Neural Networks (CNN); Gaze estimation; Large-scale dataset; Lighting conditions; Text entry systems; Text input methods; Eye tracking",Article,"Final","",Scopus,2-s2.0-85034743223
"Karakaya M.","35223530800;","Deep learning frameworks for off-angle iris recognition",2018,"2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems, BTAS 2018",,,"8698565","","",,2,"10.1109/BTAS.2018.8698565","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065411692&doi=10.1109%2fBTAS.2018.8698565&partnerID=40&md5=1558a7cd3c57658c7fa960364b694801","University of Central Arkansas, Conway, AR  72035, United States","Karakaya, M., University of Central Arkansas, Conway, AR  72035, United States","Although iris is known as one of the most accurate, distinctive, and reliable biometric identification, the accuracy of iris recognition depends on the image quality and it is negatively affected by several factors such as gaze angle, occlusion, and dilation. Since standoff iris recognition systems are much less constrained than traditional systems, the iris images captured are likely to be non-ideal, off-angle, and dilated. In this paper, we present a deep learning algorithm to improve performance of off-angle iris recognition in traditional and untraditional iris recognition frameworks. As a main contribution, this approach will allow us to study the effect of the gaze angle in ocular/periocular biometrics and fuse the information in different off-angle iris images. Using convolutional neural networks (CNNs), we first investigate traditional iris recognition framework with segmentation, normalization, and CNN based encoding and matching. In nontraditional framework, we use the iris images without segmentation and normalization to investigate the effect of periocular regions. We train and test our deep learning based approach with frontal and off-angle iris images in different sizes and types such as original, cropped, segmented, and masked images. Based on results from our real off-angle iris image dataset with 52 subjects, the proposed method improved the recognition performance compared with traditional off-angle iris recognition approaches. © 2018 IEEE.",,"Biometrics; Image enhancement; Image segmentation; Learning algorithms; Neural networks; Biometric identifications; Convolutional neural network; Improve performance; Iris recognition; Iris recognition systems; Learning frameworks; Learning-based approach; Traditional systems; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85065411692
"Smith J., Legg P., Matovic M., Kinsey K.","7410180217;36026676100;57209691115;6603565273;","Predicting user confidence during visual decision making",2018,"ACM Transactions on Interactive Intelligent Systems","8","2","10","","",,5,"10.1145/3185524","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061741550&doi=10.1145%2f3185524&partnerID=40&md5=c5918c3ff1d24a1b36dd6579e45fc300","Department of Computer Science and Creative Technologies, University of the West of England, Bristol, BS161QY, United Kingdom; Department of Psychology, University of the West of England, Bristol, BS161QY, United Kingdom","Smith, J., Department of Computer Science and Creative Technologies, University of the West of England, Bristol, BS161QY, United Kingdom; Legg, P., Department of Computer Science and Creative Technologies, University of the West of England, Bristol, BS161QY, United Kingdom; Matovic, M., Department of Computer Science and Creative Technologies, University of the West of England, Bristol, BS161QY, United Kingdom; Kinsey, K., Department of Psychology, University of the West of England, Bristol, BS161QY, United Kingdom","People are not infallible consistent “oracles”: their confidence in decision-making may vary significantly between tasks and over time. We have previously reported the benefits of using an interface and algorithms that explicitly captured and exploited users’ confidence: error rates were reduced by up to 50% for an industrial multi-class learning problem; and the number of interactions required in a design-optimisation context was reduced by 33%. Having access to users’ confidence judgements could significantly benefit intelligent interactive systems in industry, in areas such as intelligent tutoring systems and in health care. There are many reasons for wanting to capture information about confidence implicitly. Some are ergonomic, but others are more “social”—such as wishing to understand (and possibly take account of) users’ cognitive state without interrupting them. We investigate the hypothesis that users’ confidence can be accurately predicted from measurements of their behaviour. Eye-tracking systems were used to capture users’ gaze patterns as they undertook a series of visual decision tasks, after each of which they reported their confidence on a 5-point Likert scale. Subsequently, predictive models were built using “conventional” machine learning approaches for numerical summary features derived from users’ behaviour. We also investigate the extent to which the deep learning paradigm can reduce the need to design features specific to each application by creating “gaze maps”—visual representations of the trajectories and durations of users’ gaze fixations—and then training deep convolutional networks on these images. Treating the prediction of user confidence as a two-class problem (confident/not confident), we attained classification accuracy of 88% for the scenario of new users on known tasks, and 87% for known users on new tasks. Considering the confidence as an ordinal variable, we produced regression models with a mean absolute error of ≈0.7 in both cases. Capturing just a simple subset of non-task-specific numerical features gave slightly worse, but still quite high accuracy (e.g., MAE ≈ 1.0). Results obtained with gaze maps and convolutional networks are competitive, despite not having access to longer-term information about users and tasks, which was vital for the “summary” feature sets. This suggests that the gaze-map-based approach forms a viable, transferable alternative to handcrafting features for each different application. These results provide significant evidence to confirm our hypothesis, and offer a way of substantially improving many interactive artificial intelligence applications via the addition of cheap non-intrusive hardware and computationally cheap prediction algorithms. © 2018 ACM","Confidence; Human-centred machine learning","Behavioral research; Computer aided instruction; Convolution; Decision making; Deep learning; Forecasting; Machine learning; Regression analysis; Classification accuracy; Confidence; Convolutional networks; Intelligent interactive systems; Intelligent tutoring system; Machine learning approaches; Prediction algorithms; Visual representations; Eye tracking",Article,"Final","",Scopus,2-s2.0-85061741550
"Wang K., Ji Q.","56637259500;18935108400;","3D gaze estimation without explicit personal calibration",2018,"Pattern Recognition","79",,,"216","227",,18,"10.1016/j.patcog.2018.01.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044650376&doi=10.1016%2fj.patcog.2018.01.031&partnerID=40&md5=398ebfcd4005aa8dd4cd7e59246830a5","Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, 110 Eighth Street, Troy, NY  12180, United States","Wang, K., Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, 110 Eighth Street, Troy, NY  12180, United States; Ji, Q., Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, 110 Eighth Street, Troy, NY  12180, United States","Model-based 3D gaze estimation represents a dominant technique for eye gaze estimation. It allows free head movement and gives good estimation accuracy. But it requires a personal calibration, which may significantly limit its practical utility. Various techniques have been proposed to replace intrusive and subject-unfriendly calibration methods. In this paper, we introduce a new implicit calibration method that takes advantage of four natural constraints during eye gaze tracking. The first constraint is based on two complementary gaze estimation methods. The underlying assumption is that different gaze estimation methods, though based on different principles and mechanisms, ideally predict exactly the same gaze point at the same time. The second constraint is inspired by the well-known center prior principle, it is assumed that most fixations are concentrated on the center of the screen with natural viewing scenarios. The third constraint arises from the fact that for console based eye tracking, human's attention/gaze are always within the screen region. The final constraint comes from eye anatomy, where the value of eye parameters must be within certain regions. The four constraints are integrated jointly and help formulate the implicit calibration as a constrained unsupervised regression problem, which can be effectively solved through the proposed iterative hard EM algorithm. Experiments on two everyday interactions Web-browsing and Video-watching demonstrate the effectiveness of the proposed implicit calibration method. © 2018 Elsevier Ltd","Gaze estimation; Human computer interaction; Implicit calibration; Natural constraints","Calibration; Eye movements; Human computer interaction; Iterative methods; Calibration method; EM algorithms; Eye gaze tracking; Eye parameters; Gaze estimation; Model-based OPC; Natural constraints; Regression problem; Eye tracking",Article,"Final","",Scopus,2-s2.0-85044650376
[无可用作者姓名],[无可用的作者 ID],"ACM International Conference Proceeding Series",2018,"ACM International Conference Proceeding Series",,,,"","",586,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049912703&partnerID=40&md5=b8e963120093d8b10a5ba7d89c3b11e5",,"","The proceedings contain 96 papers. The topics discussed include: user-centered design of instructive assistance systems for manual assembly tasks; application areas for mixed and virtual reality-supported assistive systems; requirements for tracking technologies in industrial MR applications; framework for educating users for industrial human-robot collaboration (HRC); modeling workflows considering human-robot-collaboration; a framework for programming a swarm of UAVs; analysis of concise 'average load' definitions in uniformly random deployed wireless sensor networks; towards deep learning based hand keypoints detection for rapid sequential movements from RGB images; health recommender system design in the context of caregivers PRO-MMD project; designing a gamified social platform for people living with dementia and their live-in family caregivers; activity segmentation and identification based on eye gaze features; multidimensional trajectory similarity estimation via spatial-temporal key frame selection and signal correlation analysis; design thinking: using photo prototyping for a user-centred interface design for pick-by-vision systems; smart tourism in cities: exploring urban destinations with audio augmented reality; unsupervised learning fuzzy finite state machine for human activities recognition; automata classification with convolutional neural networks for use in assistive technologies for the visually impaired; and skim-reading strategies in sighted and visually-impaired individuals: a comparative study.",,,Conference Review,"Final","",Scopus,2-s2.0-85049912703
"Martinikorena I., Villanueva A., Cabeza R., Porta S.","56655118600;7101612861;36763933900;7005292345;","Introducing I2head database",2018,"Proceedings - PETMEI 2018: Pervasive Eye Tracking and Mobile Eye-Based Interaction",,,,"","",,7,"10.1145/3208031.3208033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052022779&doi=10.1145%2f3208031.3208033&partnerID=40&md5=b34e5283333fc19d073b639538ee40f8","Electrical and Eletronics Engineering Department, Public University of Navarre Pamplona, Navarre, Spain","Martinikorena, I., Electrical and Eletronics Engineering Department, Public University of Navarre Pamplona, Navarre, Spain; Villanueva, A., Electrical and Eletronics Engineering Department, Public University of Navarre Pamplona, Navarre, Spain; Cabeza, R., Electrical and Eletronics Engineering Department, Public University of Navarre Pamplona, Navarre, Spain; Porta, S., Electrical and Eletronics Engineering Department, Public University of Navarre Pamplona, Navarre, Spain","I2Head database has been created with the aim to become an optimal reference for low cost gaze estimation. It exhibits the following outstanding characteristics: it takes into account key aspects of low resolution eye tracking technology; it combines images of users gazing at different grids of points from alternative positions with registers of user’s head position and it provides calibration information of the camera and a simple 3D head model for each user. Hardware used to build the database includes a 6D magnetic sensor and a webcam. A careful calibration method between the sensor and the camera has been developed to guarantee the accuracy of the data. Different sessions have been recorded for each user including not only static head scenarios but also controlled displacements and even free head movements. The database is an outstanding framework to test both gaze estimation algorithms and head pose estimation methods. © 2018 Copyright held by the owner/author(s).","Database; Gaze estimation evaluation; Head pose estimation; Low cost gaze estimation","Calibration; Cameras; Cost estimating; Database systems; Image recognition; 3D head model; Calibration information; Calibration method; Eye tracking technologies; Gaze estimation; Head Pose Estimation; Head position; Low resolution; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85052022779
"Elmadjian C., Shukla P., Tula A.D., Morimoto C.H.","57202983651;57189354558;55848005200;7102275798;","3D gaze estimation in the scene volume with a head-mounted eye tracker",2018,"Proceedings - COGAIN 2018: Communication by Gaze Interaction",,,"a3","","",,16,"10.1145/3206343.3206351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050119904&doi=10.1145%2f3206343.3206351&partnerID=40&md5=e8dc8145990a83fb5b3de01102ad534f","Computer Science Department, University of São Paulo, Brazil; Computer Science Department, University of California, Santa Barbara, United States","Elmadjian, C., Computer Science Department, University of São Paulo, Brazil; Shukla, P., Computer Science Department, University of California, Santa Barbara, United States; Tula, A.D., Computer Science Department, University of São Paulo, Brazil; Morimoto, C.H., Computer Science Department, University of São Paulo, Brazil","Most applications involving gaze-based interaction are supported by estimation techniques that find a mapping between gaze data and corresponding targets on a 2D surface. However, in Virtual and Augmented Reality (AR) environments, interaction occurs mostly in a volumetric space, which poses a challenge to such techniques. Accurate point-of-regard (PoR) estimation, in particular, is of great importance to AR applications, since most known setups are prone to parallax error and target ambiguity. In this work, we expose the limitations of widely used techniques for PoR estimation in 3D and propose a new calibration procedure using an uncalibrated headmounted binocular eye tracker coupled with an RGB-D camera to track 3D gaze within the scene volume. We conducted a study to evaluate our setup with real-world data using a geometric and an appearance-based method. Our results show that accurate estimation in this setting still is a challenge, though some gaze-based interaction techniques in 3D should be possible.","3D dataset; Calibration; Gaze estimation; Head-mounted eye tracking","Augmented reality; Calibration; Geometrical optics; Stereo vision; 3D dataset; Appearance-based methods; Calibration procedure; Estimation techniques; Gaze estimation; Gaze-based interaction; Head-mounted eye tracking; Virtual and augmented reality; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85050119904
[无可用作者姓名],[无可用的作者 ID],"Proceedings - COGAIN 2018: Communication by Gaze Interaction",2018,"Proceedings - COGAIN 2018: Communication by Gaze Interaction",,,,"","",72,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050074799&partnerID=40&md5=f19d997b4b0868bb050c89867480128c",,"","The proceedings contain 10 papers. The topics discussed include: advantages of eye-gaze over head-gaze-based selection in virtual and augmented reality under varying field of views; eye movements and viewer's impressions in response to HMD-evoked head movements; 3D gaze estimation in the scene volume with a head-mounted eye tracker; MovEye: gaze control of video playback; playing music with the eyes through an isomorphic interface; context switching eye typing using dynamic expanding targets; a Fitts' law study of click and dwell interaction by gaze, head and mouse with a head-mounted display; a Fitts' Law evaluation of gaze input on large displays compared to touch and mouse inputs; content-based image retrieval based on eye-tracking; and beyond gaze cursor - exploring information-based gaze sharing in chat.",,,Conference Review,"Final","",Scopus,2-s2.0-85050074799
"Santini T., Leder H., Brinkmann H., Rosenberg R., Kasneci E., Reitstätter L., Rosenstiel W.","54881866000;8656461700;56175838100;56176068600;56059892600;57203517905;7006528940;","The art of pervasive eye tracking: Unconstrained eye tracking in the Austrian gallery belvedere",2018,"Proceedings - PETMEI 2018: Pervasive Eye Tracking and Mobile Eye-Based Interaction",,,,"","",,16,"10.1145/3208031.3208032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049690903&doi=10.1145%2f3208031.3208032&partnerID=40&md5=3d0edcdaeb427a7ace5db91072822101","University of Tübingen, Tübingen, Germany; University of Vienna, Vienna, Austria","Santini, T., University of Tübingen, Tübingen, Germany; Leder, H., University of Vienna, Vienna, Austria; Brinkmann, H., University of Vienna, Vienna, Austria; Rosenberg, R., University of Vienna, Vienna, Austria; Kasneci, E., University of Tübingen, Tübingen, Germany; Reitstätter, L., University of Vienna, Vienna, Austria; Rosenstiel, W., University of Tübingen, Tübingen, Germany","Pervasive mobile eye tracking provides a rich data source to investigate human natural behavior, providing a high degree of ecological validity in natural environments. However, challenges and limitations intrinsic to unconstrained mobile eye tracking makes its development and usage to some extent an art. Nonetheless, researchers are pushing the boundaries of this technology to help assess museum visitors’ attention not only between the exhibited works, but also within particular pieces, providing significantly more detailed insights than traditional timing-and-tracking or external observer approaches. In this paper, we present in detail the eye tracking system developed for a large scale fully-unconstrained study in the Austrian Gallery Belvedere, providing useful information for eye-tracking system designers. Furthermore, the study is described, and we report on usability and real-time performance metrics. Our results suggest that, although the system is comfortable enough, further eye tracker improvements are necessary to make it less conspicuous. Additionally, real-time accuracy already suffices for simple applications such as audio guides for the majority of users even in the absence of eye-tracker slippage compensation. © 2018 Association for Computing Machinery.","Calibration; Embedded; Eye tracking; Gaze estimation; Mobile; Pervasive; Pupil detection; Pupil tracking; Real-time; System","Behavioral research; Calibration; Embedded; Gaze estimation; Mobile; Pervasive; Pupil detection; Pupil tracking; Real time; System; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049690903
[无可用作者姓名],[无可用的作者 ID],"Eye Tracking Research and Applications Symposium (ETRA)",2018,"Eye Tracking Research and Applications Symposium (ETRA)","Part F137344",,,"","",574,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049686500&partnerID=40&md5=99781783dcf10a2707ad1f46ea08b27f",,"","The proceedings contain 106 papers. The topics discussed include: an investigation of the effects of n-gram length in scanpath analysis for eye-tracking research; evaluating gender difference on algorithmic problems using eye-tracker; how many words is a picture worth? attention allocation on thumbnails versus title text regions; cross-subject workload classification using pupil-related measures; correlation between gaze and hovers during decision-making interaction; supervised descent method (SDM) applied to accurate pupil detection in off-the-shelf eye tracking systems; CBF: circular binary features for robust and real-time pupil center detection; a novel approach to single camera, glint-free 3D eye model fitting including corneal refraction; SMOOTH-I: smart re-calibration using smooth pursuit eye movements; comparison of mapping algorithms for implicit calibration using probable fixation targets; revisiting data normalization for appearance-based gaze estimation; leveraging eye-gaze and time-series features to predict user interests and build a recommendation model for visual analysis; gaze typing in virtual reality: impact of keyboard design, selection method, and motion; the eye of the Typer: a benchmark and analysis of gaze behavior during typing; and predicting the gaze depth in head-mounted displays using multiple feature regression.",,,Conference Review,"Final","",Scopus,2-s2.0-85049686500
"Bannier K., Jain E., Le Meur O.","57202887183;36715118000;8611330900;","DeepComics: Saliency estimation for comics",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a49","","",,4,"10.1145/3204493.3204560","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049684217&doi=10.1145%2f3204493.3204560&partnerID=40&md5=b963927f783a25624772e93a143b4046","Univ Rennes, CNRS, IRISA, Rennes, F-35000, France; University of Florida, Gainesville, United States","Bannier, K., Univ Rennes, CNRS, IRISA, Rennes, F-35000, France; Jain, E., University of Florida, Gainesville, United States; Le Meur, O., Univ Rennes, CNRS, IRISA, Rennes, F-35000, France","A key requirement for training deep learning saliency models is large training eye tracking datasets. Despite the fact that the accessibility of eye tracking technology has greatly increased, collecting eye tracking data on a large scale for very specific content types is cumbersome, such as comic images, which are different from natural images such as photographs because text and pictorial content is integrated. In this paper, we show that a deep network trained on visual categories where the gaze deployment is similar to comics outperforms existing models and models trained with visual categories for which the gaze deployment is dramatically different from comics. Further, we find that it is better to use a computationally generated dataset on visual category close to comics one than real eye tracking data of a visual category that has different gaze deployment. These findings hold implications for the transference of deep networks to different domains. © 2018 Association for Computing Machinery.","Comics; Deep learning; Eye-movements; Saliency","Deep learning; Eye movements; Comics; Deep networks; Different domains; Eye tracking technologies; Natural images; Saliency; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049684217
"Fuhl W., Geisler D., Santini T., Appel T., Rosenstiel W., Kasneci E.","56770084800;57189847283;54881866000;57191500368;7006528940;56059892600;","CBF: Circular binary features for robust and real-time pupil center detection",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a8","","",,20,"10.1145/3204493.3204559","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049678289&doi=10.1145%2f3204493.3204559&partnerID=40&md5=c49bdcd517ad6ff75612fbb94e723bc5","University Tuebingen, Perception Engineering, Tuebingen, Baden-Wuerttemberg, Germany; University Tuebingen, LEAD Graduate School, Tuebingen, Baden-Wuerttemberg, Germany; University Tuebingen, Technical Computer Science, Tuebingen, Baden-Wuerttemberg, Germany","Fuhl, W., University Tuebingen, Perception Engineering, Tuebingen, Baden-Wuerttemberg, Germany; Geisler, D., University Tuebingen, Perception Engineering, Tuebingen, Baden-Wuerttemberg, Germany; Santini, T., University Tuebingen, Perception Engineering, Tuebingen, Baden-Wuerttemberg, Germany; Appel, T., University Tuebingen, LEAD Graduate School, Tuebingen, Baden-Wuerttemberg, Germany; Rosenstiel, W., University Tuebingen, Technical Computer Science, Tuebingen, Baden-Wuerttemberg, Germany; Kasneci, E., University Tuebingen, Perception Engineering, Tuebingen, Baden-Wuerttemberg, Germany","Modern eye tracking systems rely on fast and robust pupil detection, and several algorithms have been proposed for eye tracking under real world conditions. In this work, we propose a novel binary feature selection approach that is trained by computing conditional distributions. These features are scalable and rotatable, allowing for distinct image resolutions, and consist of simple intensity comparisons, making the approach robust to different illumination conditions as well as rapid illumination changes. The proposed method was evaluated on multiple publicly available data sets, considerably outperforming state-of-the-art methods, and being real-time capable for very high frame rates. Moreover, our method is designed to be able to sustain pupil center estimation even when typical edge-detection-based approaches fail – e.g., when the pupil outline is not visible due to occlusions from reflections or eye lids / lashes. As a consequece, it does not attempt to provide an estimate for the pupil outline. Nevertheless, the pupil center suffices for gaze estimation – e.g., by regressing the relationship between pupil center and gaze point during calibration. © 2018 Association for Computing Machinery.","Conditional distrubution; Machine learning; Pupil detection; Random ferns; Rotatable features; Scaleable features; Statistics","Edge detection; Feature extraction; Image resolution; Learning systems; Statistics; Conditional distrubution; Pupil detection; Random ferns; Rotatable features; Scaleable; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049678289
"Park S., Zhang X., Bulling A., Hilliges O.","57195422868;57142162900;6505807414;14041644100;","Learning to find eye region landmarks for remote gaze estimation in unconstrained settings",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a21","","",,47,"10.1145/3204493.3204545","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049678032&doi=10.1145%2f3204493.3204545&partnerID=40&md5=df2fb633eb35f354c0906b81c91ded45","ETH Zurich, Switzerland; MPI for Informatics, United States","Park, S., ETH Zurich, Switzerland; Zhang, X., MPI for Informatics, United States; Bulling, A., MPI for Informatics, United States; Hilliges, O., ETH Zurich, Switzerland","Conventional feature-based and model-based gaze estimation methods have proven to perform well in settings with controlled illumination and specialized cameras. In unconstrained real-world settings, however, such methods are surpassed by recent appearance-based methods due to difficulties in modeling factors such as illumination changes and other visual artifacts. We present a novel learning-based method for eye region landmark localization that enables conventional methods to be competitive to latest appearance-based methods. Despite having been trained exclusively on synthetic data, our method exceeds the state of the art for iris localization and eye shape registration on real-world imagery. We then use the detected landmarks as input to iterative model-fitting and lightweight learning-based gaze estimation methods. Our approach outperforms existing model-fitting and appearance-based methods in the context of person-independent and personalized gaze estimation. © 2018 Copyright held by the owner/author(s).","Eye region landmark localization; Gaze estimation","Iterative methods; Appearance-based methods; Conventional methods; Gaze estimation; Illumination changes; Landmark localization; Learning-based methods; Person-independent; Remote gaze estimation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049678032
"Zhang X., Sugano Y., Bulling A.","57142162900;7005470045;6505807414;","Revisiting data normalization for appearance-based gaze estimation",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a12","","",,23,"10.1145/3204493.3204548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049676828&doi=10.1145%2f3204493.3204548&partnerID=40&md5=6a4945958ee922517dd62622a2db6c96","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Graduate School of Information Science and Technology, Osaka University, Japan","Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Japan; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Appearance-based gaze estimation is promising for unconstrained real-world settings, but the significant variability in head pose and user-camera distance poses significant challenges for training generic gaze estimators. Data normalization was proposed to cancel out this geometric variability by mapping input images and gaze labels to a normalized space. Although used successfully in prior works, the role and importance of data normalization remains unclear. To fill this gap, we study data normalization for the first time using principled evaluations on both simulated and real data. We propose a modification to the current data normalization formulation by removing the scaling factor and show that our new formulation performs significantly better (between 9.5% and 32.7%) in the different evaluation settings. Using images synthesized from a 3D face model, we demonstrate the benefit of data normalization for the efficiency of the model training. Experiments on real-world images confirm the advantages of data normalization in terms of aze estimation performance. © 2018 Copyright held by the owner/author(s).","Appearance-based gaze estimation; Eye tracking; Machine learning","Learning systems; 3-D face modeling; Appearance based; Data normalization; Estimation performance; Gaze estimation; Real world setting; Real-world image; Scaling factors; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049676828
"Rosengren W., Hammar B., Nystöm M., Stridh M.","57202890180;25635882700;8357720600;6701319244;","Suitability of calibration polynomials for eye-tracking data with simulated fixation inaccuracies",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a66","","",,1,"10.1145/3204493.3204586","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049676721&doi=10.1145%2f3204493.3204586&partnerID=40&md5=063194d961cb8ed55502bcda0f54b1d9","Department of Biomedical Engineering, Lund University, Lund, Sweden; Department of Clinical Sciences, Lund University, Lund, Sweden; Humanities Lab, Lund University, Lund, Sweden","Rosengren, W., Department of Biomedical Engineering, Lund University, Lund, Sweden; Hammar, B., Department of Clinical Sciences, Lund University, Lund, Sweden; Nystöm, M., Humanities Lab, Lund University, Lund, Sweden; Stridh, M., Department of Biomedical Engineering, Lund University, Lund, Sweden","Current video-based eye trackers are not suited for calibration of patients who cannot produce stable and accurate fixations. Reliable calibration is crucial in order to make repeatable recordings, which in turn are important to accurately measure the effects of a medical intervention. To test the suitability of different calibration polynomials for such patients, inaccurate calibration data were simulated using a geometric model of the EyeLink 1000 Plus desktop mode setup. This model is used to map eye position features to screen coordinates, creating screen data with known eye tracker data. This allows for objective evaluation of gaze estimation performance over the entire computer screen. Results show that the choice of calibration polynomial is crucial in order to ensure a high repeatability across measurements from patients who are hard to calibrate. Higher order calibration polynomials resulted in poor gaze estimation even for small simulated fixation inaccuracies. © 2018 Association for Computing Machinery.","Computer simulation; Eye tracking calibration","Calibration; Computer simulation; Eye movements; Polynomials; Calibration data; Computer screens; Eye position; Eye-tracker data; Gaze estimation; Geometric modeling; Medical intervention; Objective evaluation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049676721
"Steil J., Huang M.X., Bulling A.","57170107900;55258532000;6505807414;","Fixation detection for head-mounted eye tracking based on visual similarity of gaze targets",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a23","","",,21,"10.1145/3204493.3204538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049676653&doi=10.1145%2f3204493.3204538&partnerID=40&md5=09072c84bcfdcc700a7bac0de8cc1f8c","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Steil, J., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Huang, M.X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Fixations are widely analysed in human vision, gaze-based interaction, and experimental psychology research. However, robust fixation detection in mobile settings is profoundly challenging given the prevalence of user and gaze target motion. These movements feign a shift in gaze estimates in the frame of reference defined by the eye tracker’s scene camera. To address this challenge, we present a novel fixation detection method for head-mounted eye trackers. Our method exploits that, independent of user or gaze target motion, target appearance remains about the same during a fixation. It extracts image information from small regions around the current gaze position and analyses the appearance similarity of these gaze patches across video frames to detect fixations. We evaluate our method using fine-grained fixation annotations on a five-participant indoor dataset (MPIIEgoFixation) with more than 2,300 fixations in total. Our method outperforms commonly used velocity- and dispersion-based algorithms, which highlights its significant potential to analyse scene image information for eye movement detection. © 2018 Copyright held by the owner/author(s).","Egocentric vision; Mobile eye tracking; Visual focus of attention","Eye movements; Image analysis; Stereo vision; Target tracking; Appearance similarities; Frame of reference; Gaze-based interaction; Head-mounted eye tracking; Image information; Mobile eye-tracking; Movement detection; Visual focus of attentions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049676653
"Dierkes K., Kassner M., Bulling A.","57202889988;56406193700;6505807414;","A novel approach to single camera, glint-free 3D eye model fitting including corneal refraction",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a9","","",,16,"10.1145/3204493.3204525","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049676019&doi=10.1145%2f3204493.3204525&partnerID=40&md5=9f276594f1e85e9ab8561a953d731269","Pupil Labs Research, Berlin, Germany","Dierkes, K., Pupil Labs Research, Berlin, Germany; Kassner, M., Pupil Labs Research, Berlin, Germany; Bulling, A., Pupil Labs Research, Berlin, Germany","Model-based methods for glint-free gaze estimation typically infer eye pose using pupil contours extracted from eye images. Existing methods, however, either ignore or require complex hardware setups to deal with refraction effects occurring at the corneal interfaces. In this work we provide a detailed analysis of the effects of refraction in glint-free gaze estimation using a single near-eye camera, based on the method presented by [Swirski and Dodgson 2013]. We demonstrate systematic deviations in inferred eyeball positions and gaze directions with respect to synthetic ground-truth data and show that ignoring corneal refraction can result in angular errors of several degrees. Furthermore, we quantify gaze direction dependent errors in pupil radius estimates. We propose a novel approach to account for corneal refraction in 3D eye model fitting and by analyzing synthetic and real images show that our new method successfully captures refraction effects and helps to overcome the shortcomings of the state of the art approach. © 2018 Copyright held by the owner/author(s).","3D eye model; Contour-based; Eye tracking; Glint-free; Pupil detection; Refraction","Cameras; Refraction; Systematic errors; 3D eye models; Contour-based; Corneal refraction; Glint-free; Pupil detection; Refraction effects; State-of-the-art approach; Systematic deviation; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049676019
"Wang H., Pi J., Qin T., Shen S., Shi B.E.","56809110800;57195220788;57194712639;55325638900;7402547071;","SLAM-based localization of 3D gaze using a mobile eye tracker",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a65","","",,18,"10.1145/3204493.3204584","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049673814&doi=10.1145%2f3204493.3204584&partnerID=40&md5=d577a55a59834f1957c2cd0ded9a18f7","Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong","Wang, H., Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Pi, J., Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Qin, T., Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Shen, S., Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Shi, B.E., Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong","Past work in eye tracking has focused on estimating gaze targets in two dimensions (2D), e.g. on a computer screen or scene camera image. Three-dimensional (3D) gaze estimates would be extremely useful when humans are mobile and interacting with the real 3D environment. We describe a system for estimating the 3D locations of gaze using a mobile eye tracker. The system integrates estimates of the user’s gaze vector from a mobile eye tracker, estimates of the eye tracker pose from a visual-inertial simultaneous localization and mapping (SLAM) algorithm, a 3D point cloud map of the environment from a RGB-D sensor. Experimental results indicate that our system produces accurate estimates of 3D gaze over a much larger range than remote eye trackers. Our system will enable applications, such as the analysis of 3D human attention and more anticipative human robot interfaces. © 2018 Copyright held by the owner/author(s).","3D Gaze Estimation; Eye Tracker; Human-Robot Interaction; Point Cloud; RGB-D camera; SLAM","Cameras; Eye movements; Human robot interaction; Robotics; Eye trackers; Gaze estimation; Point cloud; Rgb-d cameras; SLAM; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049673814
"Barz M., Daiber F., Sonntag D., Bulling A.","57189847803;34972779200;12241487800;6505807414;","Error-aware gaze-based interfaces for robust mobile gaze interaction",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a24","","",,11,"10.1145/3204493.3204536","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049670792&doi=10.1145%2f3204493.3204536&partnerID=40&md5=4f51a66b84722faeee65f777d2fa226f","German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Barz, M., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Daiber, F., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Sonntag, D., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Gaze estimation error can severely hamper usability and performance of mobile gaze-based interfaces given that the error varies constantly for different interaction positions. In this work, we explore error-aware gaze-based interfaces that estimate and adapt to gaze estimation error on-the-fly. We implement a sample error-aware user interface for gaze-based selection and different error compensation methods: A naïve approach that increases component size directly proportional to the absolute error, a recent model by Feit et al. that is based on the two-dimensional error distribution, and a novel predictive model that shifts gaze by a directional error estimate. We evaluate these models in a 12-participant user study and show that our predictive model significantly outperforms the others in terms of selection rate, particularly for small gaze targets. These results underline both the feasibility and potential of next generation error-aware gaze-based user interfaces. © 2018 Copyright held by the owner/author(s).","Error model; Error-aware; Eye tracking; Gaze interaction; Mobile interaction","Error compensation; User interfaces; Absolute error; Directional errors; Error compensation methods; Error model; Gaze estimation; Gaze interaction; Mobile interaction; Predictive modeling; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049670792
"Müller P., Huang M.X., Zhang X., Bulling A.","57188879130;55258532000;57142162900;6505807414;","Robust eye contact detection in natural multi-person interactions using gaze and speaking behaviour",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a31","","",,10,"10.1145/3204493.3204549","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049668277&doi=10.1145%2f3204493.3204549&partnerID=40&md5=7719245cb1fd6f5a7a8949e779a33cc4","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Müller, P., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Huang, M.X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Eye contact is one of the most important non-verbal social cues and fundamental to human interactions. However, detecting eye contact without specialised eye tracking equipment poses significant challenges, particularly for multiple people in real-world settings. We present a novel method to robustly detect eye contact in natural three- and four-person interactions using off-the-shelf ambient cameras. Our method exploits that, during conversations, people tend to look at the person who is currently speaking. Harnessing the correlation between people’s gaze and speaking behaviour therefore allows our method to automatically acquire training data during deployment and adaptively train eye contact detectors for each target user. We empirically evaluate the performance of our method on a recent dataset of natural group interactions and demonstrate that it achieves a relative improvement over the state-of-the-art method of more than 60%, and also improves over a head pose based baseline. © 2018 Copyright held by the owner/author(s).","Gaze estimation; Gaze signaling; Group interactions; Social gaze; Social signal processing","Signal processing; Gaze estimation; Group interaction; Human interactions; Multiple people; Real world setting; Social gaze; Social signal processing; State-of-the-art methods; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049668277
"Sesma-Sanchez L., Hansen D.W.","55320498400;15063910800;","Binocular model-based gaze estimation with a camera and a single infrared light source",2018,"Eye Tracking Research and Applications Symposium (ETRA)",,,"a47","","",,,"10.1145/3204493.3204557","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049664852&doi=10.1145%2f3204493.3204557&partnerID=40&md5=5f5f0a9d9f580968767ed92ae9b3aa62","Davalor Salud, Universidad Pública de Navarra, Italy; ITU Copenhagen, Denmark","Sesma-Sanchez, L., Davalor Salud, Universidad Pública de Navarra, Italy; Hansen, D.W., ITU Copenhagen, Denmark","We propose a binocular model-based method that only uses a single camera and an infrared light source. Most gaze estimation approaches are based on single eye models and with binocular models they are addressed by averaging the results from each eye. In this work, we propose a geometric model of both eyes for gaze estimation. The proposed model is implemented and evaluated in a simulated environment and is compared to a binocular model-based method and polynomial regression-based method with one camera and two infrared lights that average the results from both eyes. The method performs on par with methods using multiple light sources while maintaining robustness to head movements. The study shows that when using both eyes in gaze estimation models it is possible to reduce the hardware requirements while maintaining robustness. © 2018 Association for Computing Machinery.","Gaze estimation; IR light; Model-based method","Binoculars; Cameras; Infrared radiation; Light sources; Gaze estimation; Geometric modeling; Infrared light sources; Model-based method; Model-based OPC; Multiple light source; Polynomial regression; Simulated environment; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049664852
"Baltrusaitis T., Zadeh A., Lim Y.C., Morency L.-P.","36696075900;57144043100;57201857690;6603047400;","OpenFace 2.0: Facial behavior analysis toolkit",2018,"Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018",,,,"59","66",,396,"10.1109/FG.2018.00019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049395496&doi=10.1109%2fFG.2018.00019&partnerID=40&md5=a0bb1171e618014a5d8c3e3b0545b3da","Microsoft, Cambridge, United Kingdom; Carnegie Mellon University, Pittsburgh, United States","Baltrusaitis, T., Microsoft, Cambridge, United Kingdom, Carnegie Mellon University, Pittsburgh, United States; Zadeh, A., Carnegie Mellon University, Pittsburgh, United States; Lim, Y.C., Carnegie Mellon University, Pittsburgh, United States; Morency, L.-P., Carnegie Mellon University, Pittsburgh, United States","Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace 2.0 - a tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace 2.0 is an extension of OpenFace toolkit and is capable of more accurate facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace 2.0 demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, unlike a lot of modern approaches or toolkits, OpenFace 2.0 source code for training models and running them is freely available for research purposes. © 2018 IEEE.","Eye gaze; Facial behavior analysis; Head pose; Landmark detection","Behavioral research; Computer vision; Image recognition; Learning systems; Behavior analysis; Computer vision algorithms; Eye-gaze; Facial action unit recognition; Facial landmark detection; Head pose; Interactive applications; Landmark detection; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-85049395496
"Rangesh A., Trivedi M.M.","57148239200;7103153314;","When Vehicles See Pedestrians with Phones: A Multicue Framework for Recognizing Phone-Based Activities of Pedestrians",2018,"IEEE Transactions on Intelligent Vehicles","3","2",,"218","227",,10,"10.1109/TIV.2018.2804170","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062288767&doi=10.1109%2fTIV.2018.2804170&partnerID=40&md5=59b20a89d2e27aa25271c7fd2d1bb7d6","Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, San Diego, CA, United States","Rangesh, A., Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, San Diego, CA, United States; Trivedi, M.M., Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, San Diego, CA, United States","The intelligent vehicle community has devoted considerable efforts to model driver behavior, and in particular, to detect and overcome driver distraction in an effort to reduce accidents caused by driver negligence. However, as the domain increasingly shifts toward autonomous and semiautonomous solutions, the driver is no longer integral to the decision-making process, indicating a need to refocus efforts elsewhere. To this end, we propose to study pedestrian distraction instead. In particular, we focus on detecting pedestrians who are engaged in secondary activities involving their cellphones and similar hand-held multimedia devices from a purely vision-based standpoint. To achieve this objective, we propose a pipeline incorporating articulated human pose estimation, followed by a soft object label transfer from an ensemble of exemplar support vector machines trained on the nearest neighbors in pose feature space. We additionally incorporate head gaze features and prior pose information to carry out cellphone related pedestrian activity recognition. Finally, we offer a method to reliably track the articulated pose of a pedestrian through a sequence of images using a particle filter with a Gaussian process dynamical model, which can then be used to estimate sequentially varying activity scores at a very low computational cost. The entire framework is fast (especially for sequential data) and accurate, and easily extensible to include other secondary activities and sources of distraction. © 2016 IEEE.","Articulated pose tracking; computer vision; deep learning; exemplar support vector machines (SVMs); highly autonomous vehicles; panoramic surround behavior analysis; pedestrian activity recognition","Accidents; Behavioral research; Computer vision; Decision making; Deep learning; Gesture recognition; Pattern recognition; Support vector machines; Telephone sets; Vector spaces; Vehicles; Activity recognition; Behavior analysis; Decision making process; Driver distractions; Hand-held multimedia; Human pose estimations; Pose tracking; Support vector machine (SVMs); Pedestrian safety",Article,"Final","",Scopus,2-s2.0-85062288767
"Sharma A., Abrol P.","57169198100;26632764700;","Design and analysis of improved iris-based gaze estimation model",2018,"Journal of Computing Science and Engineering","12","2",,"77","89",,,"10.5626/JCSE.2018.12.2.77","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049211140&doi=10.5626%2fJCSE.2018.12.2.77&partnerID=40&md5=2cc20c9dc98b02543a341bda41e8ee79","Department of Computer Science, GGM Science College, Jammu, India; Department of Computer Science and IT, University of Jammu, Jammu, India","Sharma, A., Department of Computer Science, GGM Science College, Jammu, India; Abrol, P., Department of Computer Science and IT, University of Jammu, Jammu, India","The detection accuracy of gaze direction mainly depends on the performance of features extracted from eye images. Limitations on the estimation of gaze direction include harmful infrared (IR) light, expensive devices, static thresholding, inappropriate and complex segmentation techniques, corneal reflections, etc. In this study, an efficient appearance cum feature-based detection model, namely, iris center-based gaze estimation (ICGE), has been proposed. The model is an extension of the earlier proposed glint-based gaze direction estimation (GDE) model and overcomes the above limitations. The ICGE model has been analyzed for GDE based on iris center coordinates using a local adaptive thresholding technique. An indigenous database using more than two hundred images of different subjects on a five quadrant map screen generates almost 90% accurate results for iris and gaze quadrant detection. The distinguishing features of the low cost, non-intrusive proposed model include a lack of IR and affordable ubiquitous H/W designing, large subject-camera distance and screen dimensions, no glint dependency, and many more. The proposed model also shows significantly better results in the lower periphery corners of the quadrant map than traditional models. In addition, aside from the comparison with the GDE model, the proposed model has also been compared with other existing techniques. © 2018. The Korean Institute of Information Scientists and Engineers.","Adaptive thresholding; Gaze quadrant detection; Glint; Iris center; Iris center based gaze estimation (ICGE) model; Non-intrusive","Image segmentation; Adaptive thresholding; Gaze estimation; Glint; Iris center; Non-intrusive; Feature extraction",Article,"Final","",Scopus,2-s2.0-85049211140
"Li N., Busso C.","55805334600;35742852700;","Calibration free, user-independent gaze estimation with tensor analysis",2018,"Image and Vision Computing","74",,,"10","20",,2,"10.1016/j.imavis.2018.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046809989&doi=10.1016%2fj.imavis.2018.04.001&partnerID=40&md5=68cc0f83b589de7fcaf8bec3f6c62729","The University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States","Li, N., The University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States; Busso, C., The University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States","Human gaze directly signals visual attention, therefore, estimation of gaze has been an important research topic in fields such as human attention modeling and human-computer interaction. Accurate gaze estimation requires user, system and even session dependent parameters, which can be obtained by calibration process. However, this process has to be repeated whenever the parameter changes (head movement, camera movement, monitor movement). This study aims to eliminate the calibration process of gaze estimation by building a user-independent, appearance-based gaze estimation model. The system is ideal for multimodal interfaces, where the gaze is tracked without the cooperation from the users. The main goal is to capture the essential representation of the gaze appearance of the target user. We investigate the tensor analysis framework that decomposes the high dimension gaze data into different factors including individual differences, gaze differences, user-screen distances and session differences. The axis that is representative for a particular subject is automatically chosen in the tensor analysis framework using LASSO regression. The proposed approaches show promising results on capturing the test subject gaze changes. To address the estimation shift caused by the variations in individual heights, or relative position to the monitor, we apply domain adaptation to adjust the gaze estimation, observing further improvements. These promising results suggest that the proposed gaze estimation approach is a feasible and flexible scheme to facilitate gaze-based multimodal interfaces. © 2018 Elsevier B.V.","Domain adaptation; Human computer interaction; LASSO regression; Tensor analysis; User-independent gaze estimation","Behavioral research; Calibration; Interactive computer systems; Regression analysis; Tensors; Calibration process; Domain adaptation; Gaze estimation; Human attention model; Individual Differences; Lasso regressions; Multi-modal interfaces; Tensor analysis; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85046809989
"Zhang X., Xiao D., Li J., Qi J., Lu H.","57193391995;57200522393;57196162382;24598209500;8218163400;","Predicting human gaze with multi-level information",2018,"Signal Processing","147",,,"92","100",,,"10.1016/j.sigpro.2018.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041461035&doi=10.1016%2fj.sigpro.2018.01.003&partnerID=40&md5=991593cdcddcf950995cafc8ddc392c0","School of Information and Communication Engineering, Dalian University of Technology, Dalian, 116023, China","Zhang, X., School of Information and Communication Engineering, Dalian University of Technology, Dalian, 116023, China; Xiao, D., School of Information and Communication Engineering, Dalian University of Technology, Dalian, 116023, China; Li, J., School of Information and Communication Engineering, Dalian University of Technology, Dalian, 116023, China; Qi, J., School of Information and Communication Engineering, Dalian University of Technology, Dalian, 116023, China; Lu, H., School of Information and Communication Engineering, Dalian University of Technology, Dalian, 116023, China","Eye fixation models, which try to quantitatively predict human eye attended areas in visual fields, have received increasing interest in recent years. In this paper, a novel framework is proposed for the detection of eye fixations. First, a multi-channel detection module, which extracts information of color contrast, salient object proposals and center bias from input image, is conducted to introduce various useful information into the subsequent fixations detection. In salient object detection channel, we employ the multi-instance learning (MIL) algorithm to determine which object proposal can attract attention, which avoids the fuzzyness of positive sample selection. Second, an adaptive weighted fusion method achieved by deep learning framework is proposed to fuse the multi-level information (i.e., contrast, objective, center bias) together for the detection of fixations, so that the integration of information between each level becomes more scientific. Finally, the detection result is optimized by embedding semantic information. Experimental results show that the algorithm has achieved competitive results in MIT1003, MIT300 and Toronto120 dataset. © 2018 Elsevier B.V.","Adaptive weight learning; Attention; Eye fixation; Feature","Deep learning; Semantics; Adaptive weighted fusions; Adaptive weights; Attention; Eye fixations; Feature; Multi-instance learning; Multichannel detection; Salient object detection; Object detection",Article,"Final","",Scopus,2-s2.0-85041461035
"Morales A., Costela F.M., Tolosana R., Woods R.L.","24476050500;55908194000;55605251600;7401707048;","Saccade landing point prediction: A novel approach based on recurrent neural networks",2018,"ACM International Conference Proceeding Series",,,,"1","5",,4,"10.1145/3231884.3231890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055434531&doi=10.1145%2f3231884.3231890&partnerID=40&md5=26ead8d3af87186d9e58fbaf801c72fb","BiDA-Lab, Department of Electrical Engineering, Universidad Autonoma de Madrid, Madrid, Spain; Schepens Eye Research Institute, Mass Eye and Ear, Boston, MA, United States; Department of Ophthalmology, Harvard Medical School, Boston, MA, United States","Morales, A., BiDA-Lab, Department of Electrical Engineering, Universidad Autonoma de Madrid, Madrid, Spain, Schepens Eye Research Institute, Mass Eye and Ear, Boston, MA, United States; Costela, F.M., Schepens Eye Research Institute, Mass Eye and Ear, Boston, MA, United States, Department of Ophthalmology, Harvard Medical School, Boston, MA, United States; Tolosana, R., BiDA-Lab, Department of Electrical Engineering, Universidad Autonoma de Madrid, Madrid, Spain; Woods, R.L., Schepens Eye Research Institute, Mass Eye and Ear, Boston, MA, United States, Department of Ophthalmology, Harvard Medical School, Boston, MA, United States","A saccade is a fast eye movement that allows the change of visualfixation from one object of interest to another. These movementsare characterized by very high angular velocity peaks that canreach up to 1,000°/s, making them as one of the fastestneuromotor activities in the human body. Modeling such acomplex movement remains a challenge. Saccadic eye movementscan be defined by initial and landing points, duration, amplitude,and velocity profile. The landing point is important as it definesthe new fixation region and, therefore, the region of interest of theviewer. Its prediction may reduce problems caused by displayupdate latency in gaze-contingent systems that make real-timechanges in the display based on eye tracking. The maincontribution of this work is to propose the use of state-of-the-artmachine learning techniques (i.e., Recurrent Neural Networks) forsaccade landing point prediction in real-world scenarios. Ourmethod was evaluated using 220,000 saccades from 75 subjectsacquired during viewing video from ""Hollywood"" movies. Theresults obtained using our proposed methods outperform existingapproaches with improvements of up to 40% error reduction. Ourresults show that dynamic temporal relationships exploited byRecurrent Neural Networks can improve the performance oftraditional Feed Forward Neural Networks. © 2018 Association for Computing Machinery.","Deep Learning; Eye movement; Gaze-contingent; LSTM; Recurrent Neural Networks; Saccade","Angular velocity; Deep learning; Eye tracking; Forecasting; Image segmentation; Landing; Long short-term memory; Recurrent neural networks; Error reduction; Gaze-contingent; Learning techniques; LSTM; Real-world scenario; Region of interest; Temporal relationships; Velocity profiles; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85055434531
"Gupta P., Gupta S., Jayagopal A., Pal S., Sinha R.","57226374129;57203225029;57203223080;57203223677;14053056000;","Saliency prediction for mobile user interfaces",2018,"Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018","2018-January",,,"1529","1538",,6,"10.1109/WACV.2018.00171","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050986169&doi=10.1109%2fWACV.2018.00171&partnerID=40&md5=8743f4b647c09e8db4f864f0de5fb843","Adobe Research, India; IIT, Kanpur, India; IIT, Madras, India; IIT, Kharagpur, India","Gupta, P., Adobe Research, India; Gupta, S., IIT, Kanpur, India; Jayagopal, A., IIT, Madras, India; Pal, S., IIT, Kharagpur, India; Sinha, R., Adobe Research, India","We introduce models for saliency prediction for mobile user interfaces. A mobile interface may include elements like buttons and text in addition to natural images which enable performing a variety of tasks. Saliency in natural images is a well studied topic. However, given the difference in what constitutes a mobile interface, and the usage context of these devices, we postulate that saliency prediction for mobile interface images requires a fresh approach. Mobile interface design involves operating on elements, the building blocks of the interface. We first collected eye-gaze data from mobile devices for a free viewing task. Using this data, we develop a novel autoencoder based multi-scale deep learning model that provides saliency prediction at the mobile interface element level. Compared to saliency prediction approaches developed for natural images, we show that our approach performs significantly better on a range of established metrics. © 2018 IEEE.",,"Computer vision; Deep learning; Forecasting; Auto encoders; Building blockes; Learning models; Mobile interface; Mobile interface design; Mobile user interface; Natural images; Usage context; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85050986169
"Ogawa T., Nakazawa A., Nishida T.","57195404621;35807510800;35595754400;","Point of gaze estimation using corneal surface reflection and omnidirectional camera image",2018,"IEICE Transactions on Information and Systems","E101D","5",,"1278","1287",,5,"10.1587/transinf.2017MVP0020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046297983&doi=10.1587%2ftransinf.2017MVP0020&partnerID=40&md5=24b92a5a09aaa043ea5610fad4b7a3b1","Graduate School of Informatics, Kyoto University, Kyoto-shi, 606-8501, Japan; RIKEN Center for Advanced Intelligence Project, Wako-shi, 351-0198, Japan","Ogawa, T., Graduate School of Informatics, Kyoto University, Kyoto-shi, 606-8501, Japan; Nakazawa, A., Graduate School of Informatics, Kyoto University, Kyoto-shi, 606-8501, Japan; Nishida, T., Graduate School of Informatics, Kyoto University, Kyoto-shi, 606-8501, Japan, RIKEN Center for Advanced Intelligence Project, Wako-shi, 351-0198, Japan","We present a human point of gaze estimation system using corneal surface reflection and omnidirectional image taken by spherical panorama cameras, which becomes popular recent years. Our system enables to find where a user is looking at only from an eye image in a 360° surrounding scene image, thus, does not need gaze mapping from partial scene images to a whole scene image that are necessary in conventional eye gaze tracking system. We first generate multiple perspective scene images from an omnidirectional (equirectangular) image and perform registration between the corneal reflection and perspective images using a corneal reflection-scene image registration technique. We then compute the point of gaze using a corneal imaging technique leveraged by a 3D eye model, and project the point to an omnidirectional image. The 3D eye pose is estimate by using the particle-filter-based tracking algorithm. In experiments, we evaluated the accuracy of the 3D eye pose estimation, robustness of registration and accuracy of PoG estimations using two indoor and five outdoor scenes, and found that gaze mapping error was 5.546 [deg] on average. © 2018 The Institute of Electronics, Information and Communication Engineers.","Corneal imaging; Corneal reflection; Eye tracking; Point of gaze estimation; Spherical panorama","Cameras; Image segmentation; Mapping; Corneal reflection; Multiple perspectives; Omnidirectional cameras; Omnidirectional image; Point of gaze; Spherical panorama; Surface reflections; Tracking algorithm; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85046297983
"Wang Z., Ren J., Zhang D., Sun M., Jiang J.","24175350400;23398632100;57089259800;8927434200;57193403110;","A deep-learning based feature hybrid framework for spatiotemporal saliency detection inside videos",2018,"Neurocomputing","287",,,"68","83",,106,"10.1016/j.neucom.2018.01.076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042360312&doi=10.1016%2fj.neucom.2018.01.076&partnerID=40&md5=469884286bd9c31352804a0338359809","Media Technology and System (MTS) Lab., School of Computer Software, Tianjin University, Tianjin, 300350, China; Centre for Excellence in Signal and Image Processing, University of Strathclyde, Glasgow, United Kingdom; School of Computer Science and Technology, Tianjin University, Tianjin, 300350, China; Research Institute for Future Media Computing, College of Computer Science & Software Engineering, Shenzhen University, Shenzhen, 518060, China","Wang, Z., Media Technology and System (MTS) Lab., School of Computer Software, Tianjin University, Tianjin, 300350, China; Ren, J., Centre for Excellence in Signal and Image Processing, University of Strathclyde, Glasgow, United Kingdom; Zhang, D., School of Computer Science and Technology, Tianjin University, Tianjin, 300350, China; Sun, M., School of Computer Science and Technology, Tianjin University, Tianjin, 300350, China; Jiang, J., Research Institute for Future Media Computing, College of Computer Science & Software Engineering, Shenzhen University, Shenzhen, 518060, China","Although research on detection of saliency and visual attention has been active over recent years, most of the existing work focuses on still image rather than video based saliency. In this paper, a deep learning based hybrid spatiotemporal saliency feature extraction framework is proposed for saliency detection from video footages. The deep learning model is used for the extraction of high-level features from raw video data, and they are then integrated with other high-level features. The deep learning network has been found extremely effective for extracting hidden features than that of conventional handcrafted methodology. The effectiveness for using hybrid high-level features for saliency detection in video is demonstrated in this work. Rather than using only one static image, the proposed deep learning model take several consecutive frames as input and both the spatial and temporal characteristics are considered when computing saliency maps. The efficacy of the proposed hybrid feature framework is evaluated by five databases with human gaze complex scenes. Experimental results show that the proposed model outperforms five other state-of-the-art video saliency detection approaches. In addition, the proposed framework is found useful for other video content based applications such as video highlights. As a result, a large movie clip dataset together with labeled video highlights is generated. © 2018","Convolutional neural networks; Human gaze; Movie highlight extraction; Spatiotemporal saliency detection; Visual dispersion","Behavioral research; Extraction; Feature extraction; Neural networks; Video recording; Convolutional neural network; High-level features; Human gaze; Saliency detection; Spatiotemporal saliency; Spatiotemporal saliency detections; Temporal characteristics; Visual Attention; Deep learning; Article; artificial neural network; benchmarking; data base; deep learning network; feature extraction; image analysis; image processing; imaging and display; information processing; learning algorithm; mathematical model; neuroscience; priority journal; receiver operating characteristic; salience; three dimensional imaging; two dimensional imaging; videorecording",Article,"Final","",Scopus,2-s2.0-85042360312
"Arsenovic M., Sladojevic S., Stefanovic D., Anderla A.","55342797300;55243552000;57198355699;57191747561;","Deep neural network ensemble architecture for eye movements classification",2018,"2018 17th International Symposium on INFOTEH-JAHORINA, INFOTEH 2018 - Proceedings","2018-January",,,"1","4",,4,"10.1109/INFOTEH.2018.8345537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050950713&doi=10.1109%2fINFOTEH.2018.8345537&partnerID=40&md5=976daaf91428c0913d282ec87203de53","Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia","Arsenovic, M., Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia; Sladojevic, S., Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia; Stefanovic, D., Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia; Anderla, A., Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia","Up to now, eye tracking technologies have been used for different purposes in various industries, from medical to gaming. Eye tracking methods could include predicting fixations, gaze mapping or movement classification. Recent advances in deep learning techniques provide possibilities for solving many computer vision tasks with high accuracy. Authors of this paper propose a novel deep learning based architecture for eye movement classification task. Proposed architecture is an ensemble approach which employs deep convolutional neural networks that run in parallel, for both eyes separately, for visual feature extractions along with recurrent layers for temporal information gathering. Dataset images for training and validation were gathered from standard web camera and pre-processed automatically using dedicated tools. Overall accuracy of developed classifier on the validation set was 92%. Proposed architecture uses relatively small networks which brings the possibility of real time usage (successfully tested on 15-20fps) on regular CPU. Classifier achieved overall accuracy of 88% on the real-time test, using standard laptop and web camera. © 2018 IEEE.","convolutional networks; data deep learning; eye tracking; image classification; recurrent networks; time-series prediction","Cameras; Convolution; Deep neural networks; Eye tracking; Image classification; mHealth; Motion analysis; Network architecture; Recurrent neural networks; Convolutional networks; Deep convolutional neural networks; Eye movement classifications; Eye tracking technologies; Neural network ensembles; Recurrent networks; Time series prediction; Visual feature extraction; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85050950713
"Zhang X., Huang M.X., Sugano Y., Bulling A.","57142162900;55258532000;7005470045;6505807414;","Training person-specific gaze estimators from user interactions with multiple devices",2018,"Conference on Human Factors in Computing Systems - Proceedings","2018-April",,,"","",,22,"10.1145/3173574.3174198","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046965951&doi=10.1145%2f3173574.3174198&partnerID=40&md5=e98b2223c456e377a0b73ae661096fac","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Graduate School of Information Science and Technology, Osaka University, Japan","Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Huang, M.X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Japan; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Learning-based gaze estimation has significant potential to enable attentive user interfaces and gaze-based interaction on the billions of camera-equipped handheld devices and ambient displays. While training accurate person- and device-independent gaze estimators remains challenging, person-specific training is feasible but requires tedious data collection for each target device. To address these limitations, we present the first method to train person-specific gaze estimators across multiple devices. At the core of our method is a single convolutional neural network with shared feature extraction layers and device-specific branches that we train from face images and corresponding on-screen gaze locations. Detailed evaluations on a new dataset of interactions with five common devices (mobile phone, tablet, laptop, desktop computer, smart TV) and three common applications (mobile game, text editing, media center) demonstrate the significant potential of cross-device training. We further explore training with gaze locations derived from natural interactions, such as mouse or touch input. © 2018 Copyright held by the owner/author(s).","Appearance-based gaze estimation; Multi-devices","Computer games; Convolutional neural networks; Human engineering; Mammals; Multilayer neural networks; User interfaces; Ambient displays; Attentive user interfaces; Gaze estimation; Gaze-based interaction; Multi-devices; Multiple devices; Natural interactions; User interaction; Display devices",Conference Paper,"Final","",Scopus,2-s2.0-85046965951
"Khamis M., Baier A., Henze N., Alt F., Bulling A.","35243028400;57202047359;23396769800;27267528900;6505807414;","Understanding face and eye visibility in front-facing cameras of smartphones used in the wild",2018,"Conference on Human Factors in Computing Systems - Proceedings","2018-April",,,"","",,13,"10.1145/3173574.3173854","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046946653&doi=10.1145%2f3173574.3173854&partnerID=40&md5=0035e0d3aa97b6d094bf558216e97066","LMU Munich, Germany; University of Stuttgart, Germany; Munich University of Applied Sciences, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Khamis, M., LMU Munich, Germany; Baier, A., LMU Munich, Germany; Henze, N., University of Stuttgart, Germany; Alt, F., LMU Munich, Germany, Munich University of Applied Sciences, Germany; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Commodity mobile devices are now equipped with highresolution front-facing cameras, allowing applications in biometrics (e.g., FaceID in the iPhone X), facial expression analysis, or gaze interaction. However, it is unknown how often users hold devices in a way that allows capturing their face or eyes, and how this impacts detection accuracy. We collected 25,726 in-the-wild photos, taken from the front-facing camera of smartphones as well as associated application usage logs. We found that the full face is visible about 29% of the time, and that in most cases the face is only partially visible. Furthermore, we identified an influence of users' current activity; for example, when watching videos, the eyes but not the entire face are visible 75% of the time in our dataset. We found that a state-of-the-art face detection algorithm performs poorly against photos taken from front-facing cameras. We discuss how these findings impact mobile applications that leverage face and eye detection, and derive practical implications to address state-of-the art's limitations. © 2018 Copyright is held by owners/author(s).","Eye tracking; Face detection; Front-facing camera; Gaze estimation; In the wild study; Mobile device","Cameras; Eye protection; Eye tracking; Facings; Human engineering; Mobile computing; Smartphones; Detection accuracy; Eye visibilities; Face detection algorithm; Facial expression analysis; Gaze estimation; Gaze interaction; Mobile applications; State of the art; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85046946653
"Liu Z., Wang Z., Yao Y., Zhang L., Shao L.","56177043100;57195973746;57005815900;35231925400;55643855000;","Deep Active Learning with Contaminated Tags for Image Aesthetics Assessment",2018,"IEEE Transactions on Image Processing",,,,"","",,12,"10.1109/TIP.2018.2828326","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045760801&doi=10.1109%2fTIP.2018.2828326&partnerID=40&md5=e042d2bf133d60264f8847d7009018c7","school of CSIE, Hefei University of Technology, Hefei, Anhui, China., China; State Grid Zhejiang Electric Power Company Information &#x0026; Telecommunication Branch,Hangzhou, China.; School of Computing Sciences, University of East Anglia, UK., United Kingdom","Liu, Z., school of CSIE, Hefei University of Technology, Hefei, Anhui, China., China; Wang, Z., school of CSIE, Hefei University of Technology, Hefei, Anhui, China., China; Yao, Y., State Grid Zhejiang Electric Power Company Information &#x0026; Telecommunication Branch,Hangzhou, China.; Zhang, L., school of CSIE, Hefei University of Technology, Hefei, Anhui, China., China; Shao, L., School of Computing Sciences, University of East Anglia, UK., United Kingdom","Image aesthetic quality assessment has becoming an indispensable technique that facilitates a variety of image applications, e.g., photo retargeting and non-realistic rendering. Conventional approaches suffer from the following limitations: 1) the inefficiency of semantically describing images due to the inherent tag noise and incompletion, 2) the difficulty of accurately reflecting how humans actively perceive various regions inside each image, and 3) the challenge of incorporating the aesthetic experiences of multiple users. To solve these problems, we propose a novel semi-supervised deep active learning (SDAL) algorithm, which discovers how humans perceive semantically important regions from a large quantity of images partially assigned with contaminated tags. More specifically, as humans usually attend to the foreground objects before understanding them, we extract a succinct set of BING (binarized normed gradients) &#x005B;60&#x005D;-based object patches from each image. To simulate human visual perception, we propose SDAL which hierarchically learns human gaze shifting path (GSP) by sequentially linking semantically important object patches from each scenery. Noticeably, SDLA unifies the semantically important regions discovery and deep GSP feature learning into a principled framework, wherein only a small proportion of tagged images are adopted. Moreover, based on the sparsity penalty, SDLA can optimally abandon the noisy or redundant low-level image features. Finally, by leveraging the deeply-learned GSP features, a probabilistic model is developed for image aesthetics assessment, where the experience of multiple professional photographers can be encoded. Besides, auxiliary quality-related features can be conveniently integrated into our probabilistic model. Comprehensive experiments on a series of benchmark image sets have demonstrated the superiority of our method. As a byproduct, eye tracking experiments have shown that GSPs generated by our SDAL are about 93&#x0025; consistent with real human gaze shifting paths. IEEE","Computational modeling; deep active learning; Feature extraction; gaze shifting path; Image color analysis; Probabilistic logic; probabilistic model; Quality model; Semantics; Semi-supervised; Visual perception; Visualization","Artificial intelligence; Content based retrieval; Deep learning; Eye tracking; Feature extraction; Flow visualization; Probabilistic logics; Semantics; Vision; Active Learning; Computational model; gaze shifting path; Image color analysis; Probabilistic modeling; Quality modeling; Semi-supervised; Visual perception; Image processing",Article in Press,"Article in Press","",Scopus,2-s2.0-85045760801
"Abbasi M.H., Majidi B., Manzuri M.T.","57212413342;15845948800;6506067964;","Glimpse-gaze deep vision for Modular Rapidly Deployable Decision Support Agent in smart jungle",2018,"2018 6th Iranian Joint Congress on Fuzzy and Intelligent Systems, CFIS 2018","2018-January",,,"75","78",,19,"10.1109/CFIS.2018.8336635","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050077803&doi=10.1109%2fCFIS.2018.8336635&partnerID=40&md5=e4c32d9338e56e491575c7afd89aa75b","Department of Computer Engineering, Khatam University, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","Abbasi, M.H., Department of Computer Engineering, Khatam University, Tehran, Iran; Majidi, B., Department of Computer Engineering, Khatam University, Tehran, Iran; Manzuri, M.T., Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","Visual interpretation of complex visual patterns in non-urban environments is necessary for many applications in smart rural community management, smart farming and smart jungles. In this paper, the Glimpse-Gaze framework for deep learning based visual interpretation of complex rural and jungle environment scenes is proposed. The proposed framework is used for decision support and navigation by a multi-agent robotic system singularly referred to as MOdular RApidly Deployable Decision Support Agent (MORAD DSA). A set of deep con-volutional neural networks are trained for fast and accurate interpretation of jungle scenes. Transfer learning and auxiliary pretraining on salient regions of the jungle scenes are investigated and the hyper parameter tuning and data augmentation for avoiding overfitting for the proposed model are explored. The experimental results show that the Glimpse-Gaze framework is capable of generating accurate visual cues for precise navigation and visual interpretation in the unstructured rural and jungle environments. A series of data sets for smart jungle applications are collected and the proposed framework is evaluated for applications such as detection of fire hazards and illegal grazing in the jungle. © 2018 IEEE.","Computer Vision; Convolutional Neural Networks; Deep Learning; Intelligent Systems; Robotics","Complex networks; Computer vision; Decision support systems; Fire hazards; Intelligent systems; Multi agent systems; Neural networks; Robotics; Robots; Convolutional neural network; Data augmentation; Decision supports; Multi-agent robotic systems; Salient regions; Transfer learning; Urban environments; Visual interpretation; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85050077803
"Mohsin H., Abdullah S.H.","57203240193;57211141626;","Pupil detection algorithm based on feature extraction for eye gaze",2018,"2017 6th International Conference on Information and Communication Technology and Accessbility, ICTA 2017","2017-December",,"8336048","1","4",,5,"10.1109/ICTA.2017.8336048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051021846&doi=10.1109%2fICTA.2017.8336048&partnerID=40&md5=89c0d3dc787c3a4a647dd7ef7b187c2f","Computer Science, University of Technology, Baghdad, Iraq","Mohsin, H., Computer Science, University of Technology, Baghdad, Iraq; Abdullah, S.H., Computer Science, University of Technology, Baghdad, Iraq","Exact real-time pupil tracking is an important step in a live eye gaze. Since pupil centre is a base point's reference, exact eye centre localization is essential for many applications, such as face recognition and eye gaze estimation. A new method proposed in this paper is to extract pupil eye features exactly within different intensity levels of eye images, mostly with localization of determined interest objects and where the human is looking. As application area, the eye localization in the frame of a video-sequence has been chosen with continuing in iris and pupil detection. This method is fast and has a high degree of accuracy to determine the eye gaze after the pupil is detected because it depends on the features in the human eye. The intensity increases in the centre of the eye, and these features are extracted using a multistage algorithm. Firstly, the feature-based algorithm detected the location of the face region and will be used to detect the pupil on the face. Secondly, use the pupil to determine where humans are looking. Proposed algorithm experiments results to the faces show that they are not only robust, but also relatively efficient. It has been tested on the Mackup database, which contains 500 images belonging to 108 females from the Asian region with different indoor illuminations. The image from a real-world indoor setting with lenses, and images from the Internet. The experiment results show 99%. This ratio shows very good robustness and accuracy. © 2017 IEEE.","Eye and pupil detection. Eye gaze detection; Features of human eye; Pupil extraction","Extraction; Face recognition; Signal detection; Application area; Eye gaze detection; Eye localization; Feature-based algorithm; High degree of accuracy; Human eye; Multistage-algorithms; Pupil extractions; Feature extraction",Conference Paper,"Final","",Scopus,2-s2.0-85051021846
"Wirawan C., Qingyao H., Yi L., Yean S., Lee B.-S., Ran F.","57202648085;57202648497;57192561421;57192383054;7405441352;57202644576;","Pholder: An Eye-Gaze Assisted Reading Application on Android",2018,"Proceedings - 13th International Conference on Signal-Image Technology and Internet-Based Systems, SITIS 2017","2018-January",,,"350","353",,2,"10.1109/SITIS.2017.64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048944192&doi=10.1109%2fSITIS.2017.64&partnerID=40&md5=aa40b0412c1d606afb3426637611d5e1","School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore","Wirawan, C., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore; Qingyao, H., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore; Yi, L., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore; Yean, S., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore; Lee, B.-S., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore; Ran, F., School of Computer Science and Engineering, Nanyang Technological Uniersity, Singapore","Eye-gaze has been used extensively in human computer interface design, web layout design and as assistive technology. We successfully built a reading application with automatic scrolling, using the images captured by the in-build camera to determine the eye-gaze. The application, Pholder, uses the appearance-based method for gaze estimation and tracking of gaze movement directions for scrolling of the screen. We used an innovative technique, using the integration of pixel intensity, for gaze movement estimation which is more robust then other techniques. © 2017 IEEE.","Appearance-based method; Gaze movement estimation; Mobile; Saccade","Android (operating system); Eye movements; Human computer interaction; Interfaces (computer); Appearance-based methods; Application on androids; Assistive technology; Gaze movements; Human computer interfaces; Innovative techniques; Mobile; Pixel intensities; Motion estimation",Conference Paper,"Final","",Scopus,2-s2.0-85048944192
"Tayibnapis I.R., Choi M.-K., Kwon S.","57193237403;36805491700;35205652600;","Driver's gaze zone estimation by transfer learning",2018,"2018 IEEE International Conference on Consumer Electronics, ICCE 2018","2018-January",,,"1","5",,9,"10.1109/ICCE.2018.8326308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048820834&doi=10.1109%2fICCE.2018.8326308&partnerID=40&md5=919e1adf428a894507b653660f6a0be8","DGIST, Daegu, South Korea","Tayibnapis, I.R., DGIST, Daegu, South Korea; Choi, M.-K., DGIST, Daegu, South Korea; Kwon, S., DGIST, Daegu, South Korea","Estimating driver's gaze zone has very important role to support advanced driver assistant system (ADAS). The gaze estimation can monitor the driver focus and indirectly control the user interface/user experience (UI/UX) on a windshield using augmented reality-head up display (AR-HUD). However, to train gaze zone estimator as a classification task, someone pays huge costs to gather a large amount of annotated dataset. To reduce the labor work, we used a transfer-learning method using pre-Trained CNN model to project the gaze estimation task by regression on mobile devices that have large and reliable dataset into new classification task to overcome lack of annotated dataset for gaze zone estimation. We tested the proposed method to our own building simulation test bed. The result is shown in validation accuracy around 99.01 % and test accuracy with unseen driver around 60.25 % for estimating 10 gaze zones in-vehicle. © 2018 IEEE.",,"Augmented reality; Automobile drivers; Classification (of information); Head-up displays; Large dataset; Learning systems; Transfer learning; User experience; User interfaces; Advanced driver assistant systems; Building simulation; Classification tasks; CNN models; Gaze estimation; Large amounts; Test accuracy; Transfer learning methods; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-85048820834
"Su D., Li Y.F., Guo Y.","57193014913;8589964900;57188987287;","Precise gaze estimation for mobile gaze trackers based on hybrid two-view geometry",2018,"2017 IEEE International Conference on Robotics and Biomimetics, ROBIO 2017","2018-January",,,"302","307",,1,"10.1109/ROBIO.2017.8324434","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049890114&doi=10.1109%2fROBIO.2017.8324434&partnerID=40&md5=2da777becf4323c00d08b0a50a8c64ac","Department of Mechanical and Biomedical Engineering, University of Hong Kong, Kowloon, Hong Kong; University of Hong Kong, Shenzhen Research Institute, Hong Kong","Su, D., Department of Mechanical and Biomedical Engineering, University of Hong Kong, Kowloon, Hong Kong; Li, Y.F., Department of Mechanical and Biomedical Engineering, University of Hong Kong, Kowloon, Hong Kong, University of Hong Kong, Shenzhen Research Institute, Hong Kong; Guo, Y., Department of Mechanical and Biomedical Engineering, University of Hong Kong, Kowloon, Hong Kong","In this paper, we propose a novel calibration framework for the gaze estimation of mobile gaze tracking systems. In our method, the user's eye and the eye camera are modeled as a central catadioptric camera. Thus the epipolar geometry of the mobile gaze tracker can be described by the hybrid two-view geometry. To calibrate this model, the user is asked to gaze at the calibration points distributed in 3-D space but not all located on one plane. In the light of binocular training data, we apply a 3×6 local hybrid-fundamental matrix to register pupil centers with epipolar lines in the scene image. Thus the image gaze point viewed from different depths can be uniquely determined as the intersection of two epipolar lines calculated by binocular data. The simulation and experimental results show the effectiveness of our proposed calibration framework for mobile gaze trackers. © 2017 IEEE.",,"Binoculars; Biomimetics; Calibration; Cameras; Geometry; Robotics; Calibration points; Central catadioptric cameras; Epipolar geometry; Fundamental matrix; Gaze estimation; Gaze tracking system; Pupil centers; Training data; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85049890114
"Higuch K., Matsuda S., Kamikubo R., Enomoto T., Sugano Y., Yamamoto J., Sato Y.","37055797600;55746376100;57200496484;57200271421;7005470045;7402741813;35230954300;","Visualizing gaze direction to support video coding of social attention for children with autism spectrum disorder",2018,"International Conference on Intelligent User Interfaces, Proceedings IUI",,,,"571","582",,7,"10.1145/3172944.3172960","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045453140&doi=10.1145%2f3172944.3172960&partnerID=40&md5=7bb50275907c429f347e674ca481d032","University of Tokyo, Japan; Tsukuba University, Japan; Keio University, Japan; Osaka University, Japan","Higuch, K., University of Tokyo, Japan; Matsuda, S., Tsukuba University, Japan; Kamikubo, R., University of Tokyo, Japan; Enomoto, T., Keio University, Japan; Sugano, Y., Osaka University, Japan; Yamamoto, J., Keio University, Japan; Sato, Y., University of Tokyo, Japan","This paper presents a novel interface to support video coding of social attention in the assessment of children with autism spectrum disorder. Video-based evaluations of social attention during therapeutic activities allow observers to find target behaviors while handling the ambiguity of attention. Despite the recent advances in computer vision-based gaze estimation methods, fully automatic recognition of social attention under diverse environments is still challenging. The goal of this work is to investigate an approach that uses automatic video analysis in a supportive manner for guiding human judgment. The proposed interface displays visualization of gaze estimation results on videos and provides GUI support to allow users to facilitate agreement between observers by defining social attention labels on the video timeline. Through user studies and expert reviews, we show how the interface helps observers perform video coding of social attention and how human judgment compensates for technical limitations of the automatic gaze analysis. © 2018 ACM.","Children with ASD; Social attention; Video coding support","Codes (symbols); Diseases; User interfaces; Automatic recognition; Children with ASD; Children with autisms; Interface displays; Social attention; Technical limitations; Therapeutic activity; Video-based evaluations; Video signal processing",Conference Paper,"Final","",Scopus,2-s2.0-85045453140
"Matsumoto R., Yoshimura H., Nishiyama M., Iwai Y.","57201579216;7402345397;57191888628;57201788506;","Feature extraction using gaze of participants for classifying gender of pedestrians in images",2018,"Proceedings - International Conference on Image Processing, ICIP","2017-September",,,"3545","3549",,3,"10.1109/ICIP.2017.8296942","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045335509&doi=10.1109%2fICIP.2017.8296942&partnerID=40&md5=0e437ab8146adbde1960c26a9d9d32f6","Department of Information and Electronics, Graduate School of Engineering, Tottori University, Japan","Matsumoto, R., Department of Information and Electronics, Graduate School of Engineering, Tottori University, Japan; Yoshimura, H., Department of Information and Electronics, Graduate School of Engineering, Tottori University, Japan; Nishiyama, M., Department of Information and Electronics, Graduate School of Engineering, Tottori University, Japan; Iwai, Y., Department of Information and Electronics, Graduate School of Engineering, Tottori University, Japan","Human participants look at informative regions when attempting to identify the gender of a pedestrian in images. In our preliminary experiment, participants mainly looked at the head and chest regions when classifying gender in these images. Thus, we hypothesized that the regions in which participants gaze locations were clustered would contain discriminative features for a gender classifier. In this paper, we discuss how to reveal and use gaze locations for the gender classification of pedestrian images. Our method acquired the distribution of gaze locations from various participants while they manually classified gender. We termed this distribution a gaze map. To extract discriminative features, we assigned large weights to regions with clusters of gaze locations in the gaze map. Our experiments show that this gaze-based feature extraction method significantly improved the performance of gender classification when combined with either a deep learning or a metric learning classifier. © 2017 IEEE.","Feature; Gaze; Gender","Deep learning; Extraction; Feature extraction; Location; Discriminative features; Feature; Feature extraction methods; Gaze; Gender; Gender classification; Metric learning; Image classification",Conference Paper,"Final","",Scopus,2-s2.0-85045335509
"Cui W., Cui J., Zha H.","57201582993;15623000600;7006639394;","Specialized gaze estimation for children by convolutional neural network and domain adaptation",2018,"Proceedings - International Conference on Image Processing, ICIP","2017-September",,,"3305","3309",,3,"10.1109/ICIP.2017.8296894","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045304282&doi=10.1109%2fICIP.2017.8296894&partnerID=40&md5=c1e01e55f50d7dd164450d4298a32dfd","Key Laboratory of Machine Perception(MOE), Peking University, Beijing, 100871, China","Cui, W., Key Laboratory of Machine Perception(MOE), Peking University, Beijing, 100871, China; Cui, J., Key Laboratory of Machine Perception(MOE), Peking University, Beijing, 100871, China; Zha, H., Key Laboratory of Machine Perception(MOE), Peking University, Beijing, 100871, China","Children's social gaze behavior modeling and evaluation has obtained increasing attentions in various research areas. In psychology research, eye gaze behavior is very important to developmental disorders diagnosis and assessment. In robotics area, gaze interaction between children and robots also draws more and more attention. However, there exists no specific gaze estimator for children in social interaction context. Current approaches usually use models trained with adults' data to estimate children's gaze. Since gaze behaviors and eye appearances of children are different from those of adults, the current approaches, especially those with free-calibration assumptions which are utilized in usual human-robot interaction systems, will result in big errors. Note that children data is difficult to collect and label, so directly learning from children data is hard to achieve. We propose a new system to solve this problem, which combines a CNN feature extractor trained from adult data and a domain adaptation unit using geodesic flow kernel to adapt the source domain (adults) classifier to the target domain (children). Our system performs well in children's gaze estimation. © 2017 IEEE.","Computer Vision for Automation; Gaze Tracking; Neural Network; Semi-Supervised Learning","Human computer interaction; Human robot interaction; Image processing; Neural networks; Supervised learning; Convolutional neural network; Developmental disorders; Domain adaptation; Feature extractor; Gaze estimation; Gaze interaction; Semi- supervised learning; Social interactions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85045304282
"Ma Z.-H., Liu Z.-X., Ho M.-C., Yen J.-Y., Chen Y.-Y.","57203095907;57203091319;7403080513;7202000033;7601429288;","Long range gaze estimation with multiple near-infrared emitters",2018,"2017 International Automatic Control Conference, CACS 2017","2017-November",,,"1","5",,1,"10.1109/CACS.2017.8284270","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050494837&doi=10.1109%2fCACS.2017.8284270&partnerID=40&md5=ae3cc4d765aa384607d96146c92fb476","Department of Electrical Engineering, Taiwan; Department of Surgery, Taiwan; Department of Mechanical, Taiwan; Institute of Biomedical Electronics and Bioinformatics, National Taiwan University, Taipei, Taiwan","Ma, Z.-H., Department of Electrical Engineering, Taiwan; Liu, Z.-X., Department of Electrical Engineering, Taiwan; Ho, M.-C., Department of Surgery, Taiwan; Yen, J.-Y., Department of Mechanical, Taiwan; Chen, Y.-Y., Department of Electrical Engineering, Taiwan, Institute of Biomedical Electronics and Bioinformatics, National Taiwan University, Taipei, Taiwan","Gaze estimation is used to identify the eye foci of users for intention identification or as the input devi for computer/control systems. A novel long-range gai estimation algorithm is developed in this paper wi interpolations method and head motions compensation Among multiple near-infrared (NIR) emitters, particul sets of NIR's are chosen for different head positions provide the optimal gaze point estimations. Experimen were conducted showing satisfactory results for t method. © 2017 IEEE.",,"Automation; Process control; Estimation algorithm; Gaze estimation; Gaze point estimations; Head motion; Head position; Long ranges; Near infra red; Near infrared emitters; Infrared devices",Conference Paper,"Final","",Scopus,2-s2.0-85050494837
"Xu C.-L., Lin C.-Y.","57203186305;50361492700;","Eye-motion detection system for mnd patients",2018,"IEEE 4th International Conference on Soft Computing and Machine Intelligence, ISCMI 2017","2018-January",,,"99","103",,1,"10.1109/ISCMI.2017.8279606","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050882072&doi=10.1109%2fISCMI.2017.8279606&partnerID=40&md5=a015806c819918567d4a4ff2ae5156e3","Department of Mechanical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan","Xu, C.-L., Department of Mechanical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Lin, C.-Y., Department of Mechanical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan","This paper aims to develop an eye-motion based communication system for motor neuron disease (MND) patients to contact with care providers any time they want when they lie on the bed. This eye-motion detection system involves technical modules of eye-blink detection, gaze estimation and head pose estimation on MND patients. The system comprises a rotating arm with a camera, an infrared light source and a speaker. The arm with the camera will autonomously rotate to track the face of the patient with arbitrary body directions so that necessary eye detections can be properly conducted when it is called. When the patient needs to call out, only designated blinking and eye motions will trigger the call out action for the need of finding the care provider. © 2017 IEEE.","blink detection; eye-motion detection; gaze estimation; head pose estimation; motor neuron disease","Artificial intelligence; Brain; Cameras; Eye protection; Image recognition; Light sources; Neurons; Soft computing; Blink detections; Gaze estimation; Head Pose Estimation; Motion detection; Motor neuron disease; Motion analysis",Conference Paper,"Final","",Scopus,2-s2.0-85050882072
"Cazzato D., Dominio F., Manduchi R., Castro S.M.","55866556300;55914420500;7004297978;55171401100;","Real-time gaze estimation via pupil center tracking",2018,"Paladyn","9","1",,"6","18",,4,"10.1515/pjbr-2018-0002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048051474&doi=10.1515%2fpjbr-2018-0002&partnerID=40&md5=fdb5dc056c66f48f84c0d42313e8fd69","Interdisciplinary Centre for Security, Reliability and Trust, University of LuxembourgL-1359, Luxembourg; Urbana Smart Solutions Pte. Ltd, Singapore; Computer Engineering Department, University of California, Santa Cruz, United States; Universidad Nacional del Sur, Bahía Blanca, Argentina","Cazzato, D., Interdisciplinary Centre for Security, Reliability and Trust, University of LuxembourgL-1359, Luxembourg; Dominio, F., Urbana Smart Solutions Pte. Ltd, Singapore; Manduchi, R., Computer Engineering Department, University of California, Santa Cruz, United States; Castro, S.M., Universidad Nacional del Sur, Bahía Blanca, Argentina","Automatic gaze estimation not based on commercial and expensive eye tracking hardware solutions can enable several applications in the fields of human-computer interaction (HCI) and human behavior analysis. It is therefore not surprising that several related techniques and methods have been investigated in recent years. However, very few camera-based systems proposed in the literature are both real-time and robust. In this work, we propose a real-time user-calibration-free gaze estimation system that does not need person-dependent calibration, can deal with illumination changes and head pose variations, and can work with a wide range of distances from the camera. Our solution is based on a 3-D appearance-based method that processes the images from a built-in laptop camera. Real-time performance is obtained by combining head pose information with geometrical eye features to train a machine learning algorithm. Our method has been validated on a data set of images of users in natural environments, and shows promising results. The possibility of a real-time implementation, combined with the good quality of gaze tracking, make this system suitable for various HCI applications. © 2018 De Gruyter Open Ltd. All rights reserved.","Appearance-based method; Gaze estimation; Pupil tracking; Regression tree",,Article,"Final","",Scopus,2-s2.0-85048051474
"Liu M., Wu W., Gu Z., Yu Z., Qi F., Li Y.","57195672820;56529849100;57204214680;57221260509;56530145300;55936283000;","Deep learning based on Batch Normalization for P300 signal detection",2018,"Neurocomputing","275",,,"288","297",,93,"10.1016/j.neucom.2017.08.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029534055&doi=10.1016%2fj.neucom.2017.08.039&partnerID=40&md5=97fc17035463e8d728ff4888951e0863","School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China","Liu, M., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Wu, W., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Gu, Z., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Yu, Z., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Qi, F., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China; Li, Y., School of Automation Science and Engineering, South China University of Technology, Guangzhou, 510641, China","Detecting P300 signals from electroencephalography (EEG) is the key to establishing a P300 speller, which is a type of brain–computer interface (BCI) system based on the oddball paradigm that allows users to type messages simply by controlling eye-gazes. The convolutional neural network (CNN) is an approach that has achieved good P300 detection performances. However, the standard CNN may be prone to overfitting and the convergence may be slow. To address these issues, we develop a novel CNN, termed BN3, for detecting P300 signals, where Batch Normalization is introduced in the input and convolutional layers to alleviate over-fitting, and the rectified linear unit (ReLU) is employed in the convolutional layers to accelerate training. Since our model is fully data-driven, it is capable of automatically capturing the discriminative spatio-temporal features of the P300 signal. The results obtained on previous BCI competition P300 data sets show that BN3 both achieves the state-of-the-art character recognition performance and that it outperforms existing detection approaches with small flashing epoch numbers. BN3 can be used to improve the character recognition performance in P300 speller systems. © 2017 Elsevier B.V.","Brain–computer interface (BCI); Convolutional neural network (CNN); Deep learning; P300","Biomedical signal processing; Character recognition; Convolution; Deep learning; Electroencephalography; Electrophysiology; Interfaces (computer); Neural networks; Signal detection; Convolutional neural network; Detection approach; Detection performance; Linear units; Oddball paradigms; P300; Spatio temporal features; State of the art; Brain computer interface; Article; artificial neural network; batch normalization; batch normalized neural network; brain computer interface; deep learning; electroencephalography; event related potential; machine learning; mathematical computing; mathematical model; priority journal; recognition; signal detection",Article,"Final","",Scopus,2-s2.0-85029534055
"Lee K., Kim H., Suh C.","55638913300;34770685700;12238909500;","Simulated+unsupervised learning with adaptive data generation and bidirectional mappings",2018,"6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings",,,,"","",,10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083954405&partnerID=40&md5=199b28c1d2238dd35f9cf20d24766cbb","School of Electrical Engineering, KAIST, Daejeon, South Korea","Lee, K., School of Electrical Engineering, KAIST, Daejeon, South Korea; Kim, H., School of Electrical Engineering, KAIST, Daejeon, South Korea; Suh, C., School of Electrical Engineering, KAIST, Daejeon, South Korea","Collecting a large dataset with high quality annotations is expensive and time-consuming. Recently, Shrivastava et al. (2017) propose Simulated+Unsupervised (S+U) learning: It first learns a mapping from synthetic data to real data, translates a large amount of labeled synthetic data to the ones that resemble real data, and then trains a learning model on the translated data. Bousmalis et al. (2017b) propose a similar framework that jointly trains a translation mapping and a learning model. While these algorithms are shown to achieve the state-of-the-art performances on various tasks, it may have a room for improvement, as they do not fully leverage flexibility of data simulation process and consider only the forward (synthetic to real) mapping. Inspired by this limitation, we propose a new S+U learning algorithm, which fully leverage the flexibility of data simulators and bidirectional mappings between synthetic and real data. We show that our approach achieves the improved performance on the gaze estimation task, outperforming (Shrivastava et al., 2017). © Learning Representations, ICLR 2018 - Conference Track Proceedings.All right reserved.",,"Large dataset; Machine learning; Mapping; Bidirectional mapping; Data generation; Data simulation; Data simulators; Gaze estimation; Learning models; State-of-the-art performance; Synthetic and real data; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85083954405
"Shakeri H., Nixon M., DiPaola S.","57197771214;57095522100;14035416700;","Saliency-based artistic abstraction with deep learning and regression trees",2018,"IS and T International Symposium on Electronic Imaging Science and Technology",,,,"0604021","0604029",,,"10.2352/J.ImagingSci.Technol.2017.61.6.060402","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064061394&doi=10.2352%2fJ.ImagingSci.Technol.2017.61.6.060402&partnerID=40&md5=f6d2dde08f3ac2b2181ab6dba538b610","Simon Fraser University, 250—13450 102nd Avenue, Surrey, BC  V3T 0A3, Canada","Shakeri, H., Simon Fraser University, 250—13450 102nd Avenue, Surrey, BC  V3T 0A3, Canada; Nixon, M., Simon Fraser University, 250—13450 102nd Avenue, Surrey, BC  V3T 0A3, Canada; DiPaola, S., Simon Fraser University, 250—13450 102nd Avenue, Surrey, BC  V3T 0A3, Canada","Abstraction in art often reflects human perception—areas of an artwork that hold the observer’s gaze longest will generally be more detailed, while peripheral areas are abstracted, just as they are mentally abstracted by humans’ physiological visual process. The authors’ artistic abstraction tool, Salience Stylize, uses Deep Learning to predict the areas in an image that the observer’s gaze will be drawn to, which informs the system about which areas to keep the most detail in and which to abstract most. The planar abstraction is done by a Random Forest Regressor, splitting the image into large planes and adding more detailed planes as it progresses, just as an artist starts with tonally limited masses and iterates to add fine details, then completed with our stroke engine. The authors evaluated the aesthetic appeal and effectiveness of the detail placement in the artwork produced by Salience Stylize through two user studies with 30 subjects. © 2017 Society for Imaging Science and Technology.",,"Abstracting; Decision trees; Aesthetic appeals; Human perception; Random forests; Regression trees; User study; Visual process; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85064061394
"Cazzato D., Leo M., Distante C., Crifaci G., Bernava G.M., Ruta L., Pioggia G., Castro S.M.","55866556300;7006471658;55884135100;36125910400;35078123000;14822531100;8957312900;55171401100;","An ecological visual exploration tool to support the analysis of visual processing pathways in children with autism spectrum disorders",2018,"Journal of Imaging","4","1","9","","",,3,"10.3390/jimaging4010009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063157795&doi=10.3390%2fjimaging4010009&partnerID=40&md5=7820a6d5e0c6f512facd5a1d4c7ee971","Interdisciplinary Centre for Security Reliability and Trust (SnT), University of Luxembourg, 29, Avenue JF Kennedy, Luxembourg, L-1855, Luxembourg; Institute of Applied Sciences and Intelligence Systems—CNR, Lecce, 73100, Italy; Department of Clinical Physiology, CNR Pisa, Pisa, 56124, Italy; Institute of Applied Sciences and Intelligence Systems—CNR, Messina, 98164, Italy; Universidad Nacional del Sur, Bah a Blanca, 8000, Argentina","Cazzato, D., Interdisciplinary Centre for Security Reliability and Trust (SnT), University of Luxembourg, 29, Avenue JF Kennedy, Luxembourg, L-1855, Luxembourg; Leo, M., Institute of Applied Sciences and Intelligence Systems—CNR, Lecce, 73100, Italy; Distante, C., Institute of Applied Sciences and Intelligence Systems—CNR, Lecce, 73100, Italy; Crifaci, G., Department of Clinical Physiology, CNR Pisa, Pisa, 56124, Italy; Bernava, G.M., Institute of Applied Sciences and Intelligence Systems—CNR, Messina, 98164, Italy; Ruta, L., Institute of Applied Sciences and Intelligence Systems—CNR, Messina, 98164, Italy; Pioggia, G., Institute of Applied Sciences and Intelligence Systems—CNR, Messina, 98164, Italy; Castro, S.M., Universidad Nacional del Sur, Bah a Blanca, 8000, Argentina","Recent improvements in the field of assistive technologies have led to innovative solutions aiming at increasing the capabilities of people with disability, helping them in daily activities with applications that span from cognitive impairments to developmental disabilities. In particular, in the case of Autism Spectrum Disorder (ASD), the need to obtain active feedback in order to extract subsequently meaningful data becomes of fundamental importance. In this work, a study about the possibility of understanding the visual exploration in children with ASD is presented. In order to obtain an automatic evaluation, an algorithm for free (i.e., without constraints, nor using additional hardware, infrared (IR) light sources or other intrusive methods) gaze estimation is employed. Furthermore, no initial calibration is required. It allows the user to freely rotate the head in the field of view of the sensor, and it is insensitive to the presence of eyeglasses, hats or particular hairstyles. These relaxations of the constraints make this technique particularly suitable to be used in the critical context of autism, where the child is certainly not inclined to employ invasive devices, nor to collaborate during calibration procedures.The evaluation of children’s gaze trajectories through the proposed solution is presented for the purpose of an Early Start Denver Model (ESDM) program built on the child’s spontaneous interests and game choice delivered in a natural setting. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.","Activity recognition; Affective computing; Assistive computer vision",,Article,"Final","",Scopus,2-s2.0-85063157795
[无可用作者姓名],[无可用的作者 ID],"20th International Conference on HCI, HCI International 2018",2018,"Communications in Computer and Information Science","850",,,"","",1428,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061540340&partnerID=40&md5=7693ced7eb108c73774c0b1820eb54e0",,"","The proceedings contain 195 papers. The special focus in this conference is on HCI International. The topics include: A compliance method for the design and airworthiness certification of civil aircraft flight deck human factor; research on information architecture based on graphic reasoning and mental model; information at hand – Using wearable devices to display task information in the context of industry 4.0; example based programming and ontology building: A bioinformatic application; visual analysis for overcoming population decline and vitalizing local economy in Japan; user interface for managing and refining related patent terms; development of an interactive evolutionary computation catalog interface with user gaze information; Research on user-centered information design in SVOD service; research on filter naming mechanism based on emotional expression and cognitive integration; whale tracking: Software system for the acquisition, management and processing of data on the blue whale at offshore; study on comprehensibility and influencing factors of universal safety signs; activity based mobile user interface visualization for geo-applications; on gaze estimation using integral projection of eye images; proposal of remote face-to-face communication system with line of sight matching based on pupil detection; ergonomic design of target symbols for fighter aircraft cockpit displays based on usability evaluation; research on information interfaces visual search efficiency and matching mechanism based on similarity theory; knowde: A visual search interface; trademark image similarity search; user-based error verification method of laser beam homogenizer; conversion of player locations from football goal scene videos to a 2D top view; interaction design process oriented by metrics.",,,Conference Review,"Final","",Scopus,2-s2.0-85061540340
"Dung L.-R., Lee Y.-C., Wu Y.-Y.","7004517326;57205882012;57020166000;","On gaze estimation using integral projection of eye images",2018,"Communications in Computer and Information Science","850",,,"159","167",,,"10.1007/978-3-319-92270-6_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061536402&doi=10.1007%2f978-3-319-92270-6_22&partnerID=40&md5=2ffec543b089a9a50898c182c8adc528","National Chiao Tung University, Hsinchu, 30010, Taiwan; National Chung-Shan Institute of Science and Technology, Taoyuan, Taiwan","Dung, L.-R., National Chiao Tung University, Hsinchu, 30010, Taiwan; Lee, Y.-C., National Chiao Tung University, Hsinchu, 30010, Taiwan; Wu, Y.-Y., National Chung-Shan Institute of Science and Technology, Taoyuan, Taiwan","This paper presents a gaze estimation algorithm using integral projection of eye images with advantage of low additional hardware requirement and low computational power. The algorithm needs only a webcam under nature light source and captured eye images in a non-intrusive way. Before integral projection, we used binarization process to eliminate the non-related image information to gaze position. Projected on binary eye images with projection adjustment method to avoid eye tilt makes projection error and defined the accurate integral range of eye ROI images to achieve robust gaze estimation. We analyzed the projection diagram with skewness to describe the variation of different gaze position. In skewness calculation, the pixel coordinate of eye ROI images has been normalized to avoid head moved back and forth makes the size of ROI changed. In horizontal direction, the error angle of our algorithm is 2.29°, maximum error angle is 4.8° and the resolution we defined is 7.5. Because our algorithm is inaccurate in vertical, we could only estimate gaze direction, but to estimate precise angle. The computational power of our algorithm is low, the average execution time of each frame is only 0.01652 s, only 24% of opponent. © Springer International Publishing AG, part of Springer Nature 2018.","Gaze estimation; Integral projection; Skewness","Errors; Higher order statistics; Light sources; Adjustment method; Average Execution Time; Computational power; Gaze estimation; Image information; Integral projections; Projection error; Skewness; Binary images",Conference Paper,"Final","",Scopus,2-s2.0-85061536402
[无可用作者姓名],[无可用的作者 ID],"20th International Conference on HCI, HCI International 2018",2018,"Communications in Computer and Information Science","852",,,"","",1428,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061535327&partnerID=40&md5=c04df9c00ab38a9413f0a66ab08f60e3",,"","The proceedings contain 195 papers. The special focus in this conference is on HCI International. The topics include: A compliance method for the design and airworthiness certification of civil aircraft flight deck human factor; research on information architecture based on graphic reasoning and mental model; information at hand – Using wearable devices to display task information in the context of industry 4.0; example based programming and ontology building: A bioinformatic application; visual analysis for overcoming population decline and vitalizing local economy in Japan; user interface for managing and refining related patent terms; development of an interactive evolutionary computation catalog interface with user gaze information; Research on user-centered information design in SVOD service; research on filter naming mechanism based on emotional expression and cognitive integration; whale tracking: Software system for the acquisition, management and processing of data on the blue whale at offshore; study on comprehensibility and influencing factors of universal safety signs; activity based mobile user interface visualization for geo-applications; on gaze estimation using integral projection of eye images; proposal of remote face-to-face communication system with line of sight matching based on pupil detection; ergonomic design of target symbols for fighter aircraft cockpit displays based on usability evaluation; research on information interfaces visual search efficiency and matching mechanism based on similarity theory; knowde: A visual search interface; trademark image similarity search; user-based error verification method of laser beam homogenizer; conversion of player locations from football goal scene videos to a 2D top view; interaction design process oriented by metrics.",,,Conference Review,"Final","",Scopus,2-s2.0-85061535327
[无可用作者姓名],[无可用的作者 ID],"20th International Conference on HCI, HCI International 2018",2018,"Communications in Computer and Information Science","851",,,"","",1428,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061535290&partnerID=40&md5=02e3ae03e7475d77922701f303c185e8",,"","The proceedings contain 195 papers. The special focus in this conference is on HCI International. The topics include: A compliance method for the design and airworthiness certification of civil aircraft flight deck human factor; research on information architecture based on graphic reasoning and mental model; information at hand – Using wearable devices to display task information in the context of industry 4.0; example based programming and ontology building: A bioinformatic application; visual analysis for overcoming population decline and vitalizing local economy in Japan; user interface for managing and refining related patent terms; development of an interactive evolutionary computation catalog interface with user gaze information; Research on user-centered information design in SVOD service; research on filter naming mechanism based on emotional expression and cognitive integration; whale tracking: Software system for the acquisition, management and processing of data on the blue whale at offshore; study on comprehensibility and influencing factors of universal safety signs; activity based mobile user interface visualization for geo-applications; on gaze estimation using integral projection of eye images; proposal of remote face-to-face communication system with line of sight matching based on pupil detection; ergonomic design of target symbols for fighter aircraft cockpit displays based on usability evaluation; research on information interfaces visual search efficiency and matching mechanism based on similarity theory; knowde: A visual search interface; trademark image similarity search; user-based error verification method of laser beam homogenizer; conversion of player locations from football goal scene videos to a 2D top view; interaction design process oriented by metrics.",,,Conference Review,"Final","",Scopus,2-s2.0-85061535290
"Anwar S., Milanova M., Abdulla S., Svetleff Z.","57193015705;7003785945;57205575562;57200761694;","Emotion recognition and eye gaze estimation system: EREGE",2018,"Communications in Computer and Information Science","851",,,"364","371",,,"10.1007/978-3-319-92279-9_49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061535002&doi=10.1007%2f978-3-319-92279-9_49&partnerID=40&md5=d6849b0102c2216efeb1dd1d313641d4","University of Arkansas at Little Rock, Little Rock, AR  72204, United States; Salahaddin University, Erbil, Iraq; Polytechnic University, Erbil, Iraq; University of Nevada, Las Vegas, United States","Anwar, S., University of Arkansas at Little Rock, Little Rock, AR  72204, United States, Salahaddin University, Erbil, Iraq; Milanova, M., University of Arkansas at Little Rock, Little Rock, AR  72204, United States; Abdulla, S., Polytechnic University, Erbil, Iraq; Svetleff, Z., University of Nevada, Las Vegas, United States","In this paper, we proposed EREGE system, EREGE system considers as a face analysis package including face detection, eye detection, eye tracking, emotion recognition, and gaze estimation. EREGE system consists of two parts; facial emotion recognition that recognizes seven emotions such as neutral, happiness, sadness, anger, disgust, fear, and surprise. In the emotion recognition part, we have implemented an Active Shape Model (ASM) tracker which tracks 116 facial landmarks via webcam input. The tracked landmark points are used to extract face expression features. A support Vector machine (SVM) based classifier is implemented which gives rise to robust our system by recognizing seven emotions. The second part of EREGE system is the eye gaze estimation that starts by creating the head model followed by presenting both Active Shape Model (ASM) and Pose from Orthography and Scaling with Iterations (POSIT) algorithms for head tracking and position estimation. © Springer International Publishing AG, part of Springer Nature 2018.","ASM; Face emotion; Gaze estimation; POSIT; RANSAC; SVM","Eye protection; Eye tracking; Speech recognition; Support vector machines; Active shape model; Emotion recognition; Face emotion; Face expressions; Gaze estimation; POSIT; Position estimation; RANSAC; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85061535002
"González-Garduño A.V., Søgaard A.","57205543879;24336006300;","Learning to predict readability using eye-movement data from natives and learners",2018,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",,,,"5118","5124",,7,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060464573&partnerID=40&md5=ab375a27f974a6826569cd82b80a3f7a","Department of Computer Science, University of Copenhagen, Denmark","González-Garduño, A.V., Department of Computer Science, University of Copenhagen, Denmark; Søgaard, A., Department of Computer Science, University of Copenhagen, Denmark","Readability assessment can improve the quality of assisting technologies aimed at language learners. Eye-tracking data has been used for both inducing and evaluating general-purpose NLP/AI models, and below we show that unsurprisingly, gaze data from language learners can also improve multi-task readability assessment models. This is unsurprising, since the gaze data records the reading difficulties of the learners. Unfortunately, eye-tracking data from language learners is often much harder to obtain than eye-tracking data from native speakers. We therefore compare the performance of deep learning readability models that use native speaker eye movement data to models using data from language learners. Somewhat surprisingly, we observe no significant drop in performance when replacing learners with natives, making approaches that rely on native speaker gaze information, more scalable. In other words, our finding is that language learner difficulties can be efficiently estimated from native speakers, which suggests that, more generally, readily available gaze data can be used to improve educational NLP/AI models targeted towards language learners. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Deep learning; Eye movements; Natural language processing systems; Assessment models; Eye movement datum; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85060464573
"Wang Z., Cai H., Liu H.","57205198429;56763253600;54958434200;","Robust Eye Center Localization Based on an Improved SVR Method",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11307 LNCS",,,"623","634",,5,"10.1007/978-3-030-04239-4_56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059040211&doi=10.1007%2f978-3-030-04239-4_56&partnerID=40&md5=c6b729e661a264e060798af85354bc8e","State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China; Group of Intelligent System and Biomedical Robotics, School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom","Wang, Z., State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China; Cai, H., Group of Intelligent System and Biomedical Robotics, School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Liu, H., State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China","Eye center localization is an important technique in gaze estimation, human computer interaction, virtual reality, etc., which attracts a lot of attention. Although a great deal of progress has been achieved over the past few years, the accuracy declines dramatically due to the low input image resolution, poor lighting conditions, side face, and eyes status such as closed or covered. To handle this issue, this paper proposes an improved support vector regression (SVR) method to detect the eye center based on the facial feature localization. Several image processing techniques were tried to improve the accuracy, and results showed that the SVR combining a Gaussian filter could get a better accuracy. © 2018, Springer Nature Switzerland AG.","Eye center localization; Gaussian filter; SVR","Human computer interaction; Image resolution; Pulse shaping circuits; Virtual reality; Center-based; Eye center localization; Facial feature localization; Gaussian filters; Gaze estimation; Image processing technique; Lighting conditions; Support vector regression (SVR); Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85059040211
[无可用作者姓名],[无可用的作者 ID],"14th International Conference on Information Systems Security, ICISS 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11281 LNCS",,,"","",480,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058998005&partnerID=40&md5=d0e59469a39bdf91faa0b0a8d38226c1",,"","The proceedings contain 24 papers. The special focus in this conference is on Information Systems Security. The topics include: Secure Moderated Bargaining Game; SONICS: A Segmentation Method for Integrated ICS and Corporate System; proxy Re-Encryption Scheme for Access Control Enforcement Delegation on Outsourced Data in Public Cloud; From Cyber Security Activities to Collaborative Virtual Environments Practices Through the 3D CyberCOP Platform; a Deep Learning Based Digital Forensic Solution to Blind Source Identification of Facebook Images; A Digital Forensic Technique for Inter–Frame Video Forgery Detection Based on 3D CNN; Re–compression Based JPEG Tamper Detection and Localization Using Deep Neural Network, Eliminating Compression Factor Dependency; seDiCom: A Secure Distributed Privacy-Preserving Communication Platform; Efficacy of GDPR’s Right-to-be-Forgotten on Facebook; secSmartLock: An Architecture and Protocol for Designing Secure Smart Locks; analysis of Newer Aadhaar Privacy Models; drPass: A Dynamic and Reusable Password Generator Protocol; mySecPol: A Client-Side Policy Language for Safe and Secure Browsing; gaze-Based Graphical Password Using Webcam; (Invited Paper) on the Security of Blockchain Consensus Protocols; a Novel Multi-factor Authentication Protocol for Smart Home Environments; modeling and Analyzing Multistage Attacks Using Recursive Composition Algebra; riskWriter: Predicting Cyber Risk of an Enterprise; proPatrol: Attack Investigation via Extracted High-Level Tasks; SGP: A Safe Graphical Password System Resisting Shoulder-Surfing Attack on Smartphones; towards Accuracy in Similarity Analysis of Android Applications; secret Sharing Schemes on Compartmental Access Structure in Presence of Cheaters.",,,Conference Review,"Final","",Scopus,2-s2.0-85058998005
"Vera-Olmos F.J., Pardo E., Melero H., Malpica N.","57195328347;57195324837;56563935200;6603593205;","DeepEye: Deep convolutional network for pupil detection in real environments",2018,"Integrated Computer-Aided Engineering","26","1",,"85","95",,47,"10.3233/ICA-180584","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058542215&doi=10.3233%2fICA-180584&partnerID=40&md5=6d2e54d8ddd0e1324be6fc536b5f929b","Laboratorio de Análisis de Imagen Médica y Biometría, Universidad Rey Juan Carlos, Calle Tulipán s/n, Madrid, 28933, Spain","Vera-Olmos, F.J., Laboratorio de Análisis de Imagen Médica y Biometría, Universidad Rey Juan Carlos, Calle Tulipán s/n, Madrid, 28933, Spain; Pardo, E., Laboratorio de Análisis de Imagen Médica y Biometría, Universidad Rey Juan Carlos, Calle Tulipán s/n, Madrid, 28933, Spain; Melero, H., Laboratorio de Análisis de Imagen Médica y Biometría, Universidad Rey Juan Carlos, Calle Tulipán s/n, Madrid, 28933, Spain; Malpica, N., Laboratorio de Análisis de Imagen Médica y Biometría, Universidad Rey Juan Carlos, Calle Tulipán s/n, Madrid, 28933, Spain","Robust identification and tracking of the pupil provides key information that can be used in several applications such as controlling gaze-based HMIs (human machine interfaces), designing new diagnostic tools for brain diseases, improving driver safety, detecting drowsiness, performing cognitive research, among others. We propose a deep convolutional neural network for eye-Tracking based on atrous convolutions and spatial pyramids. DeepEye is able to handle real world problems such as varying illumination, blurring and reflections. The proposed network was trained and evaluated on 94,000 images taken from 24 data sets recorded in real world scenarios. DeepEye outperforms previous eye-Tracking methods tested with these data sets. It improves the results of the current state of the art in a 26%, achieving an accuracy of more than 70% in almost every data set in terms of percentage of pupils detected with a distance error lower than 5 pixels. DeepEye can be downloaded at: https://github.com/Fjaviervera/DeepEye. © 2019-IOS Press and the authors. All rights reserved.","atrous; convolution; deep learning; Eye-Tracking; network","Convolution; Deep learning; Deep neural networks; Diagnosis; Human computer interaction; Networks (circuits); Neural networks; atrous; Convolutional networks; Deep convolutional neural networks; Eye tracking methods; Human Machine Interface; Real-world problem; Real-world scenario; Robust identification; Eye tracking",Article,"Final","",Scopus,2-s2.0-85058542215
"Wu J., Zhong S.-H., Ma Z., Heinen S.J., Jiang J.","57189367891;36844960400;57220884718;7003536110;57193403110;","Gaze aware deep learning model for video summarization",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11165 LNCS",,,"285","295",,3,"10.1007/978-3-030-00767-6_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057268424&doi=10.1007%2f978-3-030-00767-6_27&partnerID=40&md5=b0f061ebf99ff6425583ed4cc2ef3f50","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; The Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States","Wu, J., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Zhong, S.-H., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Ma, Z., The Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; Heinen, S.J., The Smith-Kettlewell Eye Research Institute, San Francisco, CA, United States; Jiang, J., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China","Video summarization is an ideal tool for skimming videos. Previous computational models extract explicit information from the input video, such as visual appearance, motion or audio information, in order to generate informative summaries. Eye gaze information, which is an implicit clue, has proved useful for indicating important content and the viewer’s interest. In this paper, we propose a novel gaze-aware deep learning model for video summarization. In our model, the position and velocity of the observers’ raw eye movements are processed by the deep neural network to indicate the users’ preferences. Experiments on two widely used video summarization datasets show that our model is more proficient than state-of-the-art methods in summarizing video for characterizing general preferences as well as for personal preferences. The results provide an innovative and improved algorithm for using gaze information in video summarization. © Springer Nature Switzerland AG 2018.","Convolutional neural networks; Gaze information; Video summarization","Eye movements; Neural networks; Video recording; Audio information; Computational model; Convolutional neural network; Explicit information; Gaze information; State-of-the-art methods; Video summarization; Visual appearance; Deep neural networks",Conference Paper,"Final","",Scopus,2-s2.0-85057268424
"Ralekar C., Saha P., Gandhi T.K., Chaudhury S.","57189240043;57202331136;24343318600;7005182122;","Effect of Devanagari font type in reading comprehension: An eye tracking study",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11278 LNCS",,,"136","147",,3,"10.1007/978-3-030-04021-5_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057128397&doi=10.1007%2f978-3-030-04021-5_13&partnerID=40&md5=d3ae806508cb065da6ea558a5c4ba195","Department of Electrical Engineering, IIT Delhi, New Delhi, 110016, India; Department of Computer Science and Technology, IIEST, Shibpur, Howrah, India; CSIR-Central Electronics Engineering Research Institute (CEERI), Pilani, India","Ralekar, C., Department of Electrical Engineering, IIT Delhi, New Delhi, 110016, India; Saha, P., Department of Computer Science and Technology, IIEST, Shibpur, Howrah, India; Gandhi, T.K., Department of Electrical Engineering, IIT Delhi, New Delhi, 110016, India; Chaudhury, S., Department of Electrical Engineering, IIT Delhi, New Delhi, 110016, India, CSIR-Central Electronics Engineering Research Institute (CEERI), Pilani, India","In this world of digitization, screen reading has grown immensely due to the availability of affordable display devices. Most of the people prefer to read on display devices as compared to the print media. To make the reading experience of the reader pleasant and comfortable, the font designers strive hard to choose suitable typographical properties of the text such as font type, font size etc. Some of the researchers suggest that the typography of the text affects the reading performance of the readers to some extent. However, the research focusing on the effect of typography on the reading behavior of the readers is limited and it is hardly touched upon for the Indian scripts. Therefore, the proposed paper aims to find out the effect of Devanagari font type on the reading performance, especially reading comprehension of the readers. In addition to this, a method to reduce the error in the gaze estimation of the eye tracker is also proposed. In order to understand the reading behavior, an eye tracking experiment is performed on 14 participants asking them to read 22 pages, in 3 different font types, presented on the screen of the eye tracker. The performance of the readers is analyzed in terms of total reading time, comprehension score, number of fixations, fixation duration and number of regressions. Our results show that there is a significant difference in the fixation duration, a number of fixations and the comprehension score, when the same document is read in different font type. Thus, there is a scope for improvement in the reading comprehension, by changing the physical properties of the document without changing its content. These findings might be useful to understand the readers’ preference for the font and to design a proper font type for online reading. © Springer Nature Switzerland AG 2018.","Devanagari script; Eye tracking; Font type; Reading comprehension","Display devices; Human computer interaction; Typesetting; Devanagari script; Eye-tracking studies; Fixation duration; Font type; Gaze estimation; Number of fixations; Reading comprehension; Reading performance; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85057128397
"Kim H., Garrido P., Tewari A., Xu W., Thies J., Niessner M., Pérez P., Richardt C., Zollhöfer M., Theobalt C.","57193155212;57196554116;57200618272;57195993759;56312656700;35772871600;55431049800;35181960300;36245738500;6507027272;","Deep video portraits",2018,"ACM Transactions on Graphics","37","4","163","","",,133,"10.1145/3197517.3201283","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056648137&doi=10.1145%2f3197517.3201283&partnerID=40&md5=f9fb32e6290839356a315d1551cb2867","Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany; Technicolor, 975 Avenue des Champs Blancs, Cesson-Sévigné, 35576, France; Technical University of Munich, Boltzmannstraße 3, Garching, 85748, Germany; University of Bath, Claverton Down, Bath, BA2 7AY, United Kingdom; Stanford University, 353 Serra Mall, Stanford, CA  94305, United States","Kim, H., Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany; Garrido, P., Technicolor, 975 Avenue des Champs Blancs, Cesson-Sévigné, 35576, France; Tewari, A., Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany; Xu, W., Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany; Thies, J., Technical University of Munich, Boltzmannstraße 3, Garching, 85748, Germany; Niessner, M., Technical University of Munich, Boltzmannstraße 3, Garching, 85748, Germany; Pérez, P., Technicolor, 975 Avenue des Champs Blancs, Cesson-Sévigné, 35576, France; Richardt, C., University of Bath, Claverton Down, Bath, BA2 7AY, United Kingdom; Zollhöfer, M., Stanford University, 353 Serra Mall, Stanford, CA  94305, United States; Theobalt, C., Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany","We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted to manipulations of facial expressions only, we are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye blinking from a source actor to a portrait video of a target actor. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor. The realism in this rendering-to-video transfer is achieved by careful adversarial training, and as a result, we can create modified target videos that mimic the behavior of the synthetically-created input. In order to enable source-to-target video re-animation, we render a synthetic target video with the reconstructed head animation parameters from a source video, and feed it into the trained network - thus taking full control of the target. With the ability to freely recombine source and target parameters, we are able to demonstrate a large variety of video rewrite applications without explicitly modeling hair, body or background. For instance, we can reenact the full head using interactive user-controlled editing, and realize high-fidelity visual dubbing. To demonstrate the high quality of our output, we conduct an extensive series of experiments and evaluations, where for instance a user study shows that our video edits are hard to detect. © 2018 Association for Computing Machinery.","Conditional GAN; Deep learning; Dubbing; Facial reenactment; Rendering-to-video translation; Video portraits","Animation; Deep learning; Quality control; Animation parameter; Conditional GAN; Dubbing; Facial reenactment; Photo-realistic video; Space-time architecture; Synthetic rendering; Video portraits; Rendering (computer graphics)",Article,"Final","",Scopus,2-s2.0-85056648137
"Dawood A., Turner S., Perepa P.","57204557628;7402275376;55955334100;","Affective computational model to extract natural affective states of students with asperger syndrome (AS) in computer-based learning environment",2018,"IEEE Access","6",,"8522016","67026","67034",,5,"10.1109/ACCESS.2018.2879619","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056173148&doi=10.1109%2fACCESS.2018.2879619&partnerID=40&md5=19bda06948b39f6b5d41603b3c9ff899","Department of Computing, Faculty of Art, Science, and Technology, University of Northampton, Northampton, NN1 5PH, United Kingdom; Department of Special Education Needs and Inclusion, Faculty of Education and Humanities, University of Northampton, Northampton, NN1 5PH, United Kingdom","Dawood, A., Department of Computing, Faculty of Art, Science, and Technology, University of Northampton, Northampton, NN1 5PH, United Kingdom; Turner, S., Department of Computing, Faculty of Art, Science, and Technology, University of Northampton, Northampton, NN1 5PH, United Kingdom; Perepa, P., Department of Special Education Needs and Inclusion, Faculty of Education and Humanities, University of Northampton, Northampton, NN1 5PH, United Kingdom","This paper was inspired by looking at the central role of emotion in the learning process, its impact on students' performance; as well as the lack of affective computing models to detect and infer affective-cognitive states in real time for students with and without Asperger Syndrome (AS). This model overcomes gaps in other models that were designed for people with autism, which needed the use of sensors or physiological instrumentations to collect data. The model uses a webcam to capture students' affective-cognitive states of confidence, uncertainty, engagement, anxiety, and boredom. These states have a dominant effect on the learning process. The model was trained and tested on a natural-spontaneous affective dataset for students with and without AS, which was collected for this purpose. The dataset was collected in an uncontrolled environment and included variations in culture, ethnicity, gender, facial and hairstyle, head movement, talking, glasses, illumination changes, and background variation. The model structure used deep learning (DL) techniques like convolutional neural network and long short-term memory. The DL is the-state-of-art tool that used to reduce data dimensionality and capturing non-linear complex features from simpler representations. The affective model provides reliable results with accuracy 90.06%. This model is the first model to detected affective states for adult students with AS without physiological or wearable instruments. For the first time, the occlusions in this model, like hand over face or head were considered an important indicator for affective states like boredom, anxiety, and uncertainty. These occlusions have been ignored in most other affective models. The essential information channels in this model are facial expressions, head movement, and eye gaze. The model can serve as an aided-technology for tutors to monitor and detect the behaviors of all students at the same time and help in predicting negative affective states during learning process. © 2013 IEEE.","Affective model; affective-cognitive states; AS; Asperger Syndrome; autism; CNN; deep learning; LSTM","Arsenic; Computer aided instruction; Deep learning; Diseases; E-learning; Eye movements; Physiological models; Students; Affective model; Asperger syndromes; Autism; Cognitive state; LSTM; Long short-term memory",Article,"Final","",Scopus,2-s2.0-85056173148
"Le Minh T., Shimizu N., Miyazaki T., Shinoda K.","57219500324;57188969836;57193230054;7102950364;","Deep learning based multi-modal addressee recognition in visual scenes with utterances",2018,"IJCAI International Joint Conference on Artificial Intelligence","2018-July",,,"1546","1553",,6,"10.24963/ijcai.2018/214","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055714585&doi=10.24963%2fijcai.2018%2f214&partnerID=40&md5=e4e613e45d35c0537b1fdb782025a7e3","Tokyo Institute of Technology, Tokyo, Japan; Yahoo Japan Corporation, United Kingdom","Le Minh, T., Tokyo Institute of Technology, Tokyo, Japan; Shimizu, N., Yahoo Japan Corporation, United Kingdom; Miyazaki, T., Yahoo Japan Corporation, United Kingdom; Shinoda, K., Tokyo Institute of Technology, Tokyo, Japan","With the widespread use of intelligent systems, such as smart speakers, addressee recognition has become a concern in human-computer interaction, as more and more people expect such systems to understand complicated social scenes, including those outdoors, in cafeterias, and hospitals. Because previous studies typically focused only on pre-specified tasks with limited conversational situations such as controlling smart homes, we created a mock dataset called Addressee Recognition in Visual Scenes with Utterances (ARVSU) that contains a vast body of image variations in visual scenes with an annotated utterance and a corresponding addressee for each scenario. We also propose a multi-modal deep-learning-based model that takes different human cues, specifically eye gazes and transcripts of an utterance corpus, into account to predict the conversational addressee from a specific speaker's view in various real-life conversational scenarios. To the best of our knowledge, we are the first to introduce an end-to-end deep learning model that combines vision and transcripts of utterance for addressee recognition. As a result, our study suggests that future addressee recognition can reach the ability to understand human intention in many social situations previously unexplored, and our modality dataset is a first step in promoting research in this field. © 2018 International Joint Conferences on Artificial Intelligence. All right reserved.",,"Automation; Human computer interaction; Intelligent buildings; Intelligent systems; End to end; Human intentions; Image variations; Learning Based Models; Learning models; Multi-modal; Smart homes; Visual scene; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85055714585
"Cheng Y., Lu F., Zhang X.","57220572010;54956194300;57142162900;","Appearance-based gaze estimation via evaluation-guided asymmetric regression",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11218 LNCS",,,"105","121",,15,"10.1007/978-3-030-01264-9_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055706658&doi=10.1007%2f978-3-030-01264-9_7&partnerID=40&md5=bc7304d3c97222eb8761960be77cfe15","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, Beijing, China; Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany","Cheng, Y., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China, Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, Beijing, China; Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany","Eye gaze estimation has been increasingly demanded by recent intelligent systems to accomplish a range of interaction-related tasks, by using simple eye images as input. However, learning the highly complex regression between eye images and gaze directions is nontrivial, and thus the problem is yet to be solved efficiently. In this paper, we propose the Asymmetric Regression-Evaluation Network (ARE-Net), and try to improve the gaze estimation performance to its full extent. At the core of our method is the notion of “two eye asymmetry” observed during gaze estimation for the left and right eyes. Inspired by this, we design the multi-stream ARE-Net; one asymmetric regression network (AR-Net) predicts 3D gaze directions for both eyes with a novel asymmetric strategy, and the evaluation network (E-Net) adaptively adjusts the strategy by evaluating the two eyes in terms of their performance during optimization. By training the whole network, our method achieves promising results and surpasses the state-of-the-art methods on multiple public datasets. © 2018, Springer Nature Switzerland AG.","Asymmetric regression; Eye appearance; Gaze estimation","Computer vision; Intelligent systems; Appearance based; Asymmetric regression; Eye appearance; Eye images; Gaze direction; Gaze estimation; Multi-stream; State-of-the-art methods; Regression analysis",Conference Paper,"Final","",Scopus,2-s2.0-85055706658
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11218 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055684324&partnerID=40&md5=d795f744e59f1521262b6149070bc8a5",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055684324
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11217 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055591885&partnerID=40&md5=7df22e74748776557cd45b207258a6d7",,"","The proceedings contain 49 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055591885
"Park S., Spurr A., Hilliges O.","57195422868;57200213697;14041644100;","Deep pictorial gaze estimation",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11217 LNCS",,,"741","757",,13,"10.1007/978-3-030-01261-8_44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055482000&doi=10.1007%2f978-3-030-01261-8_44&partnerID=40&md5=0035b584bb2613d865aaf249fcb8f24e","AIT Lab, Department of Computer Science, ETH Zurich, Zürich, Switzerland","Park, S., AIT Lab, Department of Computer Science, ETH Zurich, Zürich, Switzerland; Spurr, A., AIT Lab, Department of Computer Science, ETH Zurich, Zürich, Switzerland; Hilliges, O., AIT Lab, Department of Computer Science, ETH Zurich, Zürich, Switzerland","Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality. © Springer Nature Switzerland AG 2018.","Appearance-based gaze estimation; Eye tracking","Computer vision; Deep neural networks; Network architecture; Eye images; Gaze direction; Gaze estimation; Highly accurate; Ill posed problem; Pictorial representation; State of the art; Unobservable; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85055482000
"Brau E., Guan J., Jeffries T., Barnard K.","50561084900;50561334200;57204390437;7101814130;","Multiple-Gaze Geometry: Inferring Novel 3D Locations from Gazes Observed in Monocular Video",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11208 LNCS",,,"641","659",,1,"10.1007/978-3-030-01225-0_38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055458488&doi=10.1007%2f978-3-030-01225-0_38&partnerID=40&md5=d8f94d1c423fd2204692765dac1b4db3","CiBO Technologies, Cambridge, MA  02141, United States; University of Arizona, Tucson, AZ  85711, United States","Brau, E., CiBO Technologies, Cambridge, MA  02141, United States; Guan, J., CiBO Technologies, Cambridge, MA  02141, United States; Jeffries, T., University of Arizona, Tucson, AZ  85711, United States; Barnard, K., University of Arizona, Tucson, AZ  85711, United States","We develop using person gaze direction for scene understanding. In particular, we use intersecting gazes to learn 3D locations that people tend to look at, which is analogous to having multiple camera views. The 3D locations that we discover need not be visible to the camera. Conversely, knowing 3D locations of scene elements that draw visual attention, such as other people in the scene, can help infer gaze direction. We provide a Bayesian generative model for the temporal scene that captures the joint probability of camera parameters, locations of people, their gaze, what they are looking at, and locations of visual attention. Both the number of people in the scene and the number of extra objects that draw attention are unknown and need to be inferred. To execute this joint inference we use a probabilistic data association approach that enables principled comparison of model hypotheses. We use MCMC for inference over the discrete correspondence variables, and approximate the marginalization over continuous parameters using the Metropolis-Laplace approximation, using Hamiltonian (Hybrid) Monte Carlo for maximization. As existing data sets do not provide the 3D locations of what people are looking at, we contribute a small data set that does. On this data set, we infer what people are looking at with 59% precision compared with 13% for a baseline approach, and where those objects are within about 0.58 m. © 2018, Springer Nature Switzerland AG.","3D gaze estimation; 3D temporal scene understanding; Discovering objects; MCMC; Model selection; Monocular video","Cameras; Computer vision; Location; Discovering objects; Gaze estimation; MCMC; Model Selection; Monocular video; Scene understanding; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85055458488
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11219 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055446512&partnerID=40&md5=b6b15138fec0c9a1ffbda7e40d65f6e2",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055446512
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11206 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055438298&partnerID=40&md5=75127d2fde545091e8f078df112a3569",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055438298
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11208 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055435468&partnerID=40&md5=b0e90bf3b0b89c9ee4cf0a8cde120190",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055435468
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11212 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055413615&partnerID=40&md5=ec2b75f39c534848e034c805ebb14798",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055413615
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11210 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055131555&partnerID=40&md5=91164c0e0b65bd0399b938c4f3e4bf77",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055131555
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11216 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055130429&partnerID=40&md5=85a1417a4c69a9bf2dc25b57469cada8",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055130429
"Li Y., Liu M., Rehg J.M.","56938415500;57204285999;7004835775;","In the Eye of Beholder: Joint Learning of Gaze and Actions in First Person Video",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11209 LNCS",,,"639","655",,9,"10.1007/978-3-030-01228-1_38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055119518&doi=10.1007%2f978-3-030-01228-1_38&partnerID=40&md5=62d9aa7f97e71b5e84d241882ffeb02c","Carnegie Mellon University, Pittsburgh, United States; College of Computing and Center for Behavioral Imaging, Georgia Institute of Technology, Atlanta, United States","Li, Y., Carnegie Mellon University, Pittsburgh, United States; Liu, M., College of Computing and Center for Behavioral Imaging, Georgia Institute of Technology, Atlanta, United States; Rehg, J.M., College of Computing and Center for Behavioral Imaging, Georgia Institute of Technology, Atlanta, United States","We address the task of jointly determining what a person is doing and where they are looking based on the analysis of video captured by a headworn camera. We propose a novel deep model for joint gaze estimation and action recognition in First Person Vision. Our method describes the participant’s gaze as a probabilistic variable and models its distribution using stochastic units in a deep network. We sample from these stochastic units to generate an attention map. This attention map guides the aggregation of visual features in action recognition, thereby providing coupling between gaze and action. We evaluate our method on the standard EGTEA dataset and demonstrate performance that exceeds the state-of-the-art by a significant margin of 3.5 %. © 2018, Springer Nature Switzerland AG.",,"Probability distributions; Stochastic models; Stochastic systems; Action recognition; Deep networks; First-person visions; Gaze estimation; Headworn cameras; Joint learning; State of the art; Visual feature; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85055119518
"Fischer T., Chang H.J., Demiris Y.","57190126084;35168664400;6506125343;","RT-GENE: Real-time eye gaze estimation in natural environments",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11214 LNCS",,,"339","357",,17,"10.1007/978-3-030-01249-6_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055116507&doi=10.1007%2f978-3-030-01249-6_21&partnerID=40&md5=534fd9d65c4080b5a9b8d50fd6efd72c","Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom","Fischer, T., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom; Chang, H.J., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom; Demiris, Y., Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom","In this work, we consider the problem of robust gaze estimation in natural environments. Large camera-to-subject distances and high variations in head pose and eye gaze angles are common in such environments. This leads to two main shortfalls in state-of-the-art methods for gaze estimation: hindered ground truth gaze annotation and diminished gaze estimation accuracy as image resolution decreases with distance. We first record a novel dataset of varied gaze and head pose images in a natural environment, addressing the issue of ground truth annotation by measuring head pose using a motion capture system and eye gaze using mobile eyetracking glasses. We apply semantic image inpainting to the area covered by the glasses to bridge the gap between training and testing images by removing the obtrusiveness of the glasses. We also present a new real-time algorithm involving appearance-based deep convolutional neural networks with increased capacity to cope with the diverse images in the new dataset. Experiments with this network architecture are conducted on a number of diverse eye-gaze datasets including our own, and in cross dataset evaluations. We demonstrate state-of-the-art performance in terms of estimation accuracy in all experiments, and the architecture performs well even on lower resolution images. © Springer Nature Switzerland AG 2018.","Convolutional neural network; Eyetracking glasses; Gaze dataset; Gaze estimation; Semantic inpainting","Computer vision; Convolution; Deep neural networks; Eye tracking; Glass; Image resolution; Neural networks; Semantics; Convolutional neural network; Cross-dataset evaluation; Deep convolutional neural networks; Gaze dataset; Gaze estimation; Inpainting; State-of-the-art methods; State-of-the-art performance; Network architecture",Conference Paper,"Final","",Scopus,2-s2.0-85055116507
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11213 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055116359&partnerID=40&md5=e4f6b60491b4cd6fbe91ef201326fc83",,"","The proceedings contain 49 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055116359
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11207 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055114733&partnerID=40&md5=a12e5731126f3b4ffd9a56a2d98dfeb2",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055114733
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11205 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055112746&partnerID=40&md5=ccb030292ab971fa251569c6f23c5b0f",,"","The proceedings contain 49 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055112746
"Chong E., Ruiz N., Wang Y., Zhang Y., Rozga A., Rehg J.M.","57194267364;57204290088;57204288633;57204294656;8776877400;7004835775;","Connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11209 LNCS",,,"397","412",,6,"10.1007/978-3-030-01228-1_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055111439&doi=10.1007%2f978-3-030-01228-1_24&partnerID=40&md5=fa083c9cb66320b4c055e008dcf91492","School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, United States","Chong, E., School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, United States; Ruiz, N., School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, United States; Wang, Y., School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, United States; Zhang, Y., School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, United States; Rozga, A., School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, United States; Rehg, J.M., School of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, United States","This paper addresses the challenging problem of estimating the general visual attention of people in images. Our proposed method is designed to work across multiple naturalistic social scenarios and provides a full picture of the subject’s attention and gaze. In contrast, earlier works on gaze and attention estimation have focused on constrained problems in more specific contexts. In particular, our model explicitly represents the gaze direction and handles out-of-frame gaze targets. We leverage three different datasets using a multi-task learning approach. We evaluate our method on widely used benchmarks for single-tasks such as gaze angle estimation and attention-within-an-image, as well as on the new challenging task of generalized visual attention prediction. In addition, we have created extended annotations for the MMDB and GazeFollow datasets which are used in our experiments, which we will publicly release. © 2018, Springer Nature Switzerland AG.","Gaze estimation; Saliency; Visual attention","Computer vision; Attention estimations; Constrained problem; Gaze estimation; Joint modeling; Multitask learning; Saliency; Social scenarios; Visual Attention; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85055111439
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11220 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055108972&partnerID=40&md5=ba61999d92e48a6bad3eaa0b52a48a35",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055108972
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11211 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055106796&partnerID=40&md5=f733e0e849efe5d28e87e2577a878640",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055106796
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11214 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055097677&partnerID=40&md5=017326f469f5ab65ad5583d7b18aef24",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055097677
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11215 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055097529&partnerID=40&md5=91e44cef4b77876121cdeb28dd711964",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055097529
[无可用作者姓名],[无可用的作者 ID],"15th European Conference on Computer Vision, ECCV 2018",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11209 LNCS",,,"","",12549,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055095740&partnerID=40&md5=2be91f4ea512777a251c54b54584ced9",,"","The proceedings contain 727 papers. The special focus in this conference is on Computer Vision. The topics include: Open Set Domain Adaptation by Backpropagation; deep Feature Pyramid Reconfiguration for Object Detection; goal-Oriented Visual Question Generation via Intermediate Rewards; DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model; estimating the Success of Unsupervised Image to Image Translation; parallel Feature Pyramid Network for Object Detection; joint Map and Symmetry Synchronization; MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics; rethinking the Form of Latent States in Image Captioning; transductive Semi-Supervised Deep Learning Using Min-Max Features; unsupervised Holistic Image Generation from Key Local Patches; SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection; hashing with Binary Matrix Pursuit; maskConnect: Connectivity Learning by Gradient Descent; online Multi-Object Tracking with Dual Matching Attention Networks; connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency; videos as Space-Time Region Graphs; unified Perceptual Parsing for Scene Understanding; synthetically Supervised Feature Learning for Scene Text Recognition; probabilistic Video Generation Using Holistic Attribute Control; learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation; DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency; Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization; mutual Learning to Adapt for Joint Human Parsing and Pose Estimation; DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation; view-Graph Selection Framework for SfM; selfie Video Stabilization; cubeNet: Equivariance to 3D Rotation and Translation; YouTube-VOS: Sequence-to-Sequence Video Object Segmentation; neural Stereoscopic Image Style Transfer.",,,Conference Review,"Final","",Scopus,2-s2.0-85055095740
"Recasens A., Kellnhofer P., Stent S., Matusik W., Torralba A.","57189096003;55250016000;56229805900;56230515000;7005432728;","Learning to zoom: A saliency-based sampling layer for neural networks",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11213 LNCS",,,"52","67",,5,"10.1007/978-3-030-01240-3_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055083447&doi=10.1007%2f978-3-030-01240-3_4&partnerID=40&md5=0fe1602f078822e8d8909aca2bb820b2","Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Toyota Research Institute, Cambridge, MA  02139, United States","Recasens, A., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Kellnhofer, P., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Stent, S., Toyota Research Institute, Cambridge, MA  02139, United States; Matusik, W., Massachusetts Institute of Technology, Cambridge, MA  02139, United States; Torralba, A., Massachusetts Institute of Technology, Cambridge, MA  02139, United States","We introduce a saliency-based distortion layer for convolutional neural networks that helps to improve the spatial sampling of input data for a given task. Our differentiable layer can be added as a preprocessing block to existing task networks and trained altogether in an end-to-end fashion. The effect of the layer is to efficiently estimate how to sample from the original data in order to boost task performance. For example, for an image classification task in which the original data might range in size up to several megapixels, but where the desired input images to the task network are much smaller, our layer learns how best to sample from the underlying high resolution data in a manner which preserves task-relevant information better than uniform downsampling. This has the effect of creating distorted, caricature-like intermediate images, in which idiosyncratic elements of the image that improve task performance are zoomed and exaggerated. Unlike alternative approaches such as spatial transformer networks, our proposed layer is inspired by image saliency, computed efficiently from uniformly downsampled data, and degrades gracefully to a uniform sampling strategy under uncertainty. We apply our layer to improve existing networks for the tasks of human gaze estimation and fine-grained object classification. Code for our method is available in: http://github.com/recasens/Saliency-Sampler. © Springer Nature Switzerland AG 2018.","Attention; Convolutional neural networks; Deep learning; Image sampling; Spatial transformer; Task saliency","Classification (of information); Computer vision; Convolution; Deep learning; Image enhancement; Image sampling; Neural networks; Attention; Convolutional neural network; High resolution data; Intermediate image; Object classification; Spatial transformer; Task saliency; Uniform sampling; Network layers",Conference Paper,"Final","",Scopus,2-s2.0-85055083447
"Wan Q., Rajeev S., Kaszowska A., Panetta K., Taylor H.A., Agaian S.","57188747389;57191541248;57203481497;6507727793;7403057334;35321805400;","Fixation oriented object segmentation using mobile eye tracker",2018,"Proceedings of SPIE - The International Society for Optical Engineering","10668",,"106680D","","",,2,"10.1117/12.2304868","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051846131&doi=10.1117%2f12.2304868&partnerID=40&md5=4a2c95e3e9f146df773a83306b845162","Dept of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Dept of Psychology, Tufts University, Medford, MA  02155, United States; Distinguished Prof. of Computer Science, City University of New York, New York City, NY  10017, United States","Wan, Q., Dept of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Rajeev, S., Dept of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Kaszowska, A., Dept of Psychology, Tufts University, Medford, MA  02155, United States; Panetta, K., Dept of Electrical and Computer Engineering, Tufts University, Medford, MA  02155, United States; Taylor, H.A., Dept of Psychology, Tufts University, Medford, MA  02155, United States; Agaian, S., Distinguished Prof. of Computer Science, City University of New York, New York City, NY  10017, United States","Eye tracking technology allows researchers to monitor position of the eye and infer one's gaze direction, which is used to understand the nature of human attention within psychology, cognitive science, marketing and artificial intelligence. Commercially available head-mounted eye trackers allow researchers to track pupil movements (saccades and fixations) using infrared camera and capture the field of vision by a front-facing scene camera. The wearable eye tracker opened a new way to research in unconstrained environment settings; however, the recorded scene video typically has non-uniform illumination, low quality image frames, and moving scene objects. One of the most important tasks for analyzing the recorded scene video data is finding the boundary between different objects in a single frame. This paper presents a multi-level fixation-oriented object segmentation method (MFoOS) to solve the above challenges in segmenting the scene objects in video data collected by the eye tracker in order to support cognition research. MFoOS shows its advancement in position-invariance, illumination, noise tolerance and is task-driven. The proposed method is tested using real-world case studies designed by our team of psychologists focused on understanding visual attention in human problem solving. The extensive computer simulation demonstrates the method's accuracy and robustness for fixation-oriented object segmentation. Moreover, a deep-learning image semantic segmentation combining MFoOS results as label data was explored to demonstrate the possibility of on-line deployment of eye tracker fixation-oriented object segmentation. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Cognitive science; Eye tracking technology; Image semantic segmentation; Multi-level fixation-oriented object segmentation; Online deployment","Behavioral research; Cameras; Cognitive systems; Deep learning; Eye movements; Image segmentation; Problem solving; Semantics; Video recording; Cognitive science; Eye tracking technologies; Image semantics; Object segmentation; Online deployment; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85051846131
"Wood E., Baltrušaitis T., Morency L.-P., Robinson P., Bulling A.","56145872800;36696075900;6603047400;57205369790;6505807414;","Gazedirector: Fully articulated eye gaze redirection in video",2018,"Computer Graphics Forum","37","2",,"217","225",,17,"10.1111/cgf.13355","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051832117&doi=10.1111%2fcgf.13355&partnerID=40&md5=42137a016fcd71cd98890c91e7b6490b","University of Cambridge, United Kingdom; Carnegie Mellon University, United States; Max Planck Institute for Informatics, Germany; Microsoft, United States","Wood, E., University of Cambridge, United Kingdom, Microsoft, United States; Baltrušaitis, T., Carnegie Mellon University, United States, Microsoft, United States; Morency, L.-P., Carnegie Mellon University, United States; Robinson, P., University of Cambridge, United Kingdom; Bulling, A., Max Planck Institute for Informatics, Germany","We present GazeDirector, a new approach for eye gaze redirection that uses model-fitting. Our method first tracks the eyes by fitting a multi-part eye region model to video frames using analysis-by-synthesis, thereby recovering eye region shape, texture, pose, and gaze simultaneously. It then redirects gaze by 1) warping the eyelids from the original image using a model-derived flow field, and 2) rendering and compositing synthesized 3D eyeballs onto the output image in a photorealistic manner. GazeDirector allows us to change where people are looking without person-specific training data, and with full articulation, i.e. we can precisely specify new gaze directions in 3D. Quantitatively, we evaluate both model-fitting and gaze synthesis, with experiments for gaze estimation and redirection on the Columbia gaze dataset. Qualitatively, we compare GazeDirector against recent work on gaze redirection, showing better results especially for large redirection angles. Finally, we demonstrate gaze redirection on YouTube videos by introducing new 3D gaze targets and by manipulating visual behavior. © 2017 The Authors and The Eurographics Association and John Wiley & Sons Ltd.",,"Behavioral research; Analysis by synthesis; Gaze direction; Gaze estimation; Gaze synthesis; New approaches; Original images; Photo-realistic; Visual behavior; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85051832117
"Arai K.","7403965268;","Mobile phone operations using human eyes only and its applications",2018,"International Journal of Advanced Computer Science and Applications","9","3",,"148","154",,,"10.14569/IJACSA.2018.090322","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049512321&doi=10.14569%2fIJACSA.2018.090322&partnerID=40&md5=d5b201d38f7ec97feb49a455f24cfac4","Information Science Department, Graduate School of Science and Engineering, Saga University, Saga City, Japan","Arai, K., Information Science Department, Graduate School of Science and Engineering, Saga University, Saga City, Japan","Mobile phone operations using human eyes only is proposed together with its applications for cooking with referring to recipes and manufacturing with referring to manuals, production procedure, so on. It is found that most of mobile phone operations can be done without touching the screen of the mobile phone. Also, mobile phone operation success rate based on the proposed method is evaluated for the environmental illumination conditions, visible or near infrared (NIR) cameras, the distance between user and mobile phone, as well as pupil size detection accuracy against the environmental illumination changes. Meanwhile, the functionality of two typical applications of the proposed method is confirmed successfully. © 2015 The Science and Information (SAI) Organization Limited.","Gaze estimation; Line of sight estimation; Mobile phone operations; Pupil detection; Wearable computing",,Article,"Final","",Scopus,2-s2.0-85049512321
"Wang L.","57201277527;","Attention decrease detection based on video analysis in E-learning",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10790 LNCS",,,"166","179",,2,"10.1007/978-3-662-56689-3_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044140509&doi=10.1007%2f978-3-662-56689-3_14&partnerID=40&md5=228e7a42c5d6e2e8303c557a63df4468","Department of Educational Technology, Nanjing Normal University, Nanjing, 210097, China","Wang, L., Department of Educational Technology, Nanjing Normal University, Nanjing, 210097, China","E-learning takes the advantages of lower cost and higher benefit. It becomes one of the educational research focus through learning behavior analysis to promote deep learning. In order to help learners overcome possible disadvantages in e-learning environment such as prone inattention and delayed response, one video analysis algorithm is designed to detect attention decrease situation, then feedback in time or warn early. The algorithm uses head posture, gaze, eye closure and mouth opening, facial expression features as attention observation attributes. Next machine learning classifiers are applied to code behavior features. Finally the time sequential statistics of behavior features evaluate the attention level and emotional pleasure degree. Experiments show that the algorithm is effective to find out the inattention cases to give desirable feedback. It may be applicable in adaptive learning and human computer interaction fields. © 2018, Springer-Verlag GmbH Germany.","Attention decrease; Attention observation model; E-learning; Video analysis","Computer aided instruction; Deep learning; E-learning; Human computer interaction; Adaptive learning; Attention decrease; E-learning environment; Educational research; Facial Expressions; Learning behavior; Observation model; Video analysis; Learning systems",Book Chapter,"Final","",Scopus,2-s2.0-85044140509
"Zhao T., Wang Y., Fu X.","57192707963;55211773900;7402204912;","Refining eye synthetic images via coarse-to-fine adversarial networks for appearance-based gaze estimation",2018,"Communications in Computer and Information Science","819",,,"419","428",,3,"10.1007/978-981-10-8530-7_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043366675&doi=10.1007%2f978-981-10-8530-7_41&partnerID=40&md5=4e6fb13f8556f72404391f6bc1a071b6","Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China","Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Wang, Y., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China, School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Recently, several models have achieved great success in terms of reducing the gap between synthetic and real image distributions with large unlabeled real data. However, collecting such large amounts of real data costs a lot of labouring and training them requires high memory. To reduce the gap with less real data, we propose a coarse-to-fine refine eye image method combining coarse model net and fine model net through adversarial training. Coarse model net is a feed-forward convolutional neural network aiming to transform synthetic eye images into coarse images. Fine model net is a modified Generative Adversarial Networks (GANs) which add realism to coarse images using unlabeled real data. Experimental results show that the proposed method achieves similar distributions as recent work but decreasing real data at least one order of magnitude. In addition, a significant accuracy improvement for gaze estimation with refined synthetic eye images is observed. © Springer Nature Singapore Pte Ltd. 2018.","Feed-forward convolutional neural network; Generative adversarial networks; Image synthesis","Convolution; Neural networks; Accuracy Improvement; Adversarial networks; Appearance based; Coarse to fine; Convolutional neural network; Gaze estimation; Image synthesis; Synthetic images; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85043366675
"Wu X., Li J., Wu Q., Sun J., Yan H.","57200820459;55441752700;57043789700;12645161300;56471078100;","Block-Wise Gaze Estimation Based on Binocular Images",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10749 LNCS",,,"477","487",,,"10.1007/978-3-319-75786-5_38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042520818&doi=10.1007%2f978-3-319-75786-5_38&partnerID=40&md5=5625e45d5597f23ea58e0fb1562cc6f9","School of Information Science and Engineering, Shandong University, Jinan, China; School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, China","Wu, X., School of Information Science and Engineering, Shandong University, Jinan, China; Li, J., School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China; Wu, Q., School of Information Science and Engineering, Shandong University, Jinan, China; Sun, J., School of Information Science and Engineering, Shandong Normal University, Jinan, China; Yan, H., School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, China","Appearance-based gaze estimation methods have been proved to be highly effective. Different from the previous methods that estimate gaze direction based on left or right eye image separately, we propose a binocular-image based gaze estimation method. Considering the challenges in estimating the precise gaze points via regression models, we estimate the block-wise gaze position by classifying the binocular images via convolutional neural network (CNN) in the proposed method. We divide the screen of the desktop computer into 2 × 3 and 6 × 9 blocks respectively, label the binocular images with their corresponding gazed block positions, train a convolutional neural network model to classify the eye images according to their labels, and estimate the gazed block through the CNN-based classification. The experimental results demonstrate that the proposed gaze estimation method based on binocular images can reach higher accuracy than those based on monocular images. And the proposed method shows its great potential in practical touch screen-based applications. © 2018, Springer International Publishing AG, part of Springer Nature.","Appearance-based; Convolutional neural network (CNN); Eye image; Gaze block; Gaze estimation","Binoculars; Bins; Convolution; Estimation; Neural networks; Regression analysis; Touch screens; Appearance based; Convolutional Neural Networks (CNN); Eye images; Gaze block; Gaze estimation; Image classification",Conference Paper,"Final","",Scopus,2-s2.0-85042520818
"Huang M.X., Li J., Ngai G., Leong H.V.","55258532000;56433022500;8915594400;7005127948;","Qick bootstrapping of a Personalized gaze model from real-use interactions",2018,"ACM Transactions on Intelligent Systems and Technology","9","4","a43","","",,2,"10.1145/3156682","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041481678&doi=10.1145%2f3156682&partnerID=40&md5=91c8efcaa6e61362875647ab9483b4e6","Max Planck Institute for Informatics, Saarbrucken Informatics Campus, Germany; South China University of Technology, School of Design, Guangzhou, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong","Huang, M.X., Max Planck Institute for Informatics, Saarbrucken Informatics Campus, Germany; Li, J., South China University of Technology, School of Design, Guangzhou, China; Ngai, G., Department of Computing, Hong Kong Polytechnic University, Hong Kong; Leong, H.V., Department of Computing, Hong Kong Polytechnic University, Hong Kong","Understanding human visual attention is essential for understanding human cognition, which in turn benefits human-computer interaction. Recent work has demonstrated a Personalized, Auto-Calibrating Eye-tracking (PACE) system, which makes it possible to achieve accurate gaze estimation using only an off-the-shelf webcam by identifying and collecting data implicitly from user interaction events. However, this method is constrained by the need for large amounts of well-annotated data. We thus present fast-PACE, an adaptation to PACE that exploits knowledge from existing data from different users to accelerate the learning speed of the personalized model. The result is an adaptive, data-driven approach that continuously ""learns"" its user and recalibrates, adapts, and improves with additional usage by a user. Experimental evaluations of fast-PACE demonstrate its competitive accuracy in iris localization, validity of alignment identification between gaze and interactions, and effectiveness of gaze transfer. In general, fast-PACE achieves an initial visual error of 3.98 degrees and then steadily improves to 2.52 degrees given incremental interaction-informed data. Our performance is comparable to state-of-the-art, but without the need for explicit training or calibration. Our technique addresses the data quality and quantity problems. It therefore has the potential to enable comprehensive gaze-aware applications in the wild. © 2018 ACM.","Data validation; Gaze estimation; Gaze transfer learning; Gaze-interaction alignment; Implicit modeling","Behavioral research; Data validation; Gaze estimation; Gaze interaction; Implicit model; Transfer learning; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85041481678
"Wang Y., Zhao T., Ding X., Peng J., Bian J., Fu X.","55211773900;57192707963;57194286638;57192708626;57200854878;7402204912;","Learning a gaze estimator with neighbor selection from large-scale synthetic eye images",2018,"Knowledge-Based Systems","139",,,"41","49",,11,"10.1016/j.knosys.2017.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031806955&doi=10.1016%2fj.knosys.2017.10.010&partnerID=40&md5=92ec8e308d2b9c5a32ece920cb99fbaa","School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian 116024, China; Information Science and Technology College, Dalian Maritime University, Dalian 116026, China","Wang, Y., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian 116024, China, Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; Ding, X., Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; Peng, J., Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; Bian, J., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian 116024, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian 116026, China","Appearance-based gaze estimation works well in inferring human gaze under real-world condition. But one of the significant limitations in appearance-based methods is the need for huge amounts of training data. Eye image synthesis addresses this problem by generating huge amounts of synthetic eye images with computer graphics. To fully use the large-scale synthetic eye images, a simple-but-effective appearance-based gaze estimation framework with neighbor selection is proposed in this paper. The proposed framework hierarchically fuses multiple k-NN queries (in head pose, pupil center and eye appearance spaces) to choose closest samples with more relevant features. Considering the structure characters of the closet samples, neighbor regression methods then can be applied to predict the gaze directions. Experimental results demonstrate that the representative neighbor regression methods under the proposed framework achieve better performance for within-subject and cross-subject gaze estimation. © 2017 Elsevier B.V.","Cross-subject; Gaze estimation; Learning-by-synthesis; Neighbor selection","Computer graphics; Nearest neighbor search; Appearance based; Appearance-based methods; Cross-subject; Gaze estimation; Neighbor selection; Regression method; Relevant features; Structure character; Regression analysis",Article,"Final","",Scopus,2-s2.0-85031806955
"Deng H., Zhu W.","57200613552;55430100200;","Monocular Free-Head 3D Gaze Tracking with Deep Learning and Geometry Constraints",2017,"Proceedings of the IEEE International Conference on Computer Vision","2017-October",,"8237603","3162","3171",,63,"10.1109/ICCV.2017.341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041922315&doi=10.1109%2fICCV.2017.341&partnerID=40&md5=ee9e8db2c6173ac07937327fda39796e","Nanjing University, China; Tsinghua University, China","Deng, H., Nanjing University, China; Zhu, W., Tsinghua University, China","Free-head 3D gaze tracking outputs both the eye location and the gaze vector in 3D space, and it has wide applications in scenarios such as driver monitoring, advertisement analysis and surveillance. A reliable and low-cost monocular solution is critical for pervasive usage in these areas. Noticing that a gaze vector is a composition of head pose and eyeball movement in a geometrically deterministic way, we propose a novel gaze transform layer to connect separate head pose and eyeball movement models. The proposed decomposition does not suffer from head-gaze correlation overfitting and makes it possible to use datasets existing for other tasks. To add stronger supervision for better network training, we propose a two-step training strategy, which first trains sub-tasks with rough labels and then jointly trains with accurate gaze labels. To enable good cross-subject performance under various conditions, we collect a large dataset which has full coverage of head poses and eyeball movements, contains 200 subjects, and has diverse illumination conditions. Our deep solution achieves state-of-the-art gaze tracking accuracy, reaching 5.6° cross-subject prediction error using a small network running at 1000 fps on a single CPU (excluding face alignment time) and 4.3° cross-subject error with a deeper network. © 2017 IEEE.",,"Computer vision; Tracking (position); Vector spaces; Driver monitoring; Eyeball movements; Geometry constraints; Illumination conditions; Network training; Prediction errors; State of the art; Two-step training; Deep learning",Conference Paper,"Final","",Scopus,2-s2.0-85041922315
"Wang K., Ji Q.","56637259500;18935108400;","Real Time Eye Gaze Tracking with 3D Deformable Eye-Face Model",2017,"Proceedings of the IEEE International Conference on Computer Vision","2017-October",,"8237376","1003","1011",,46,"10.1109/ICCV.2017.114","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041894764&doi=10.1109%2fICCV.2017.114&partnerID=40&md5=7688ca08813a7ad66987dd1f22cacf92","ECSE Department, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States","Wang, K., ECSE Department, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States; Ji, Q., ECSE Department, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY, United States","3D model-based gaze estimation methods are widely explored because of their good accuracy and ability to handle free head movement. Traditional methods with complex hardware systems (Eg. infrared lights, 3D sensors, etc.) are restricted to controlled environments, which significantly limit their practical utilities. In this paper, we propose a 3D model-based gaze estimation method with a single web-camera, which enables instant and portable eye gaze tracking. The key idea is to leverage on the proposed 3D eye-face model, from which we can estimate 3D eye gaze from observed 2D facial landmarks. The proposed system includes a 3D deformable eye-face model that is learned offline from multiple training subjects. Given the deformable model, individual 3D eye-face models and personal eye parameters can be recovered through the unified calibration algorithm. Experimental results show that the proposed method outperforms state-of-the-art methods while allowing convenient system setup and free head movement. A real time eye tracking system running at 30 FPS also validates the effectiveness and efficiency of the proposed method. © 2017 IEEE.",,"Computer vision; Deformation; Tracking (position); Complex hardware; Controlled environment; Deformable modeling; Effectiveness and efficiencies; Eye gaze tracking; Real-time eye tracking; State-of-the-art methods; Unified calibration; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85041894764
[无可用作者姓名],[无可用的作者 ID],"Proceedings - International Electronics Symposium on Knowledge Creation and Intelligent Computing, IES-KCIC 2017",2017,"Proceedings - International Electronics Symposium on Knowledge Creation and Intelligent Computing, IES-KCIC 2017","2017-January",,,"","",315,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046949753&partnerID=40&md5=df91678e82b90bca1413445d7ec27002",,"","The proceedings contain 50 papers. The topics discussed include: linked data for air pollution monitoring; multi-group particle swarm optimization with random redistribution; deep learning algorithm for arrhythmia detection; an implementation of Botnet dataset to predict accuracy based on network flow model; a method for reducing the amounts of training samples for developing AI systems; feature selection software development using artificial bee colony on DNA microarray data; the training of feedforward neural network using the unscented Kalman filter for voice classification application; semantic spatial weighted regression for realizing spatial correlation of deforestation effect on soil degradation; feature selection using genetic algorithm to improve classification in network intrusion detection system; an implementation of data exchange using authenticated attribute-based encryption for environmental monitoring; a fuzzy system for quality assurance of crowdsourced wildlife observation geodata; eye gaze tracking to operate android-based communication helper application; preliminary design of mobile visual programming apps for internet of things applications based on Raspberry Pi 3 platform; and UAV-based multispectral aerial image retrieval using spectral feature and semantic computing.",,,Conference Review,"Final","",Scopus,2-s2.0-85046949753
"Nguyen D.-C., Bailly G., Elisei F.","56428394900;7003803108;7003391715;","Learning off-line vs. on-line models of interactive multimodal behaviors with recurrent neural networks",2017,"Pattern Recognition Letters","100",,,"29","36",,3,"10.1016/j.patrec.2017.09.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030327162&doi=10.1016%2fj.patrec.2017.09.033&partnerID=40&md5=de00f8dccfd2039b07428f27aa6110c1","GIPSA-Lab, Univ. Grenoble-Alpes/CNRS, Speech & Cognition Department, France","Nguyen, D.-C., GIPSA-Lab, Univ. Grenoble-Alpes/CNRS, Speech & Cognition Department, France; Bailly, G., GIPSA-Lab, Univ. Grenoble-Alpes/CNRS, Speech & Cognition Department, France; Elisei, F., GIPSA-Lab, Univ. Grenoble-Alpes/CNRS, Speech & Cognition Department, France","Human interactions are driven by multi-level perception-action loops. Interactive behavioral models are typically built using rule-based methods or statistical approaches such as Hidden Markov Model (HMM), Dynamic Bayesian Network (DBN), etc. In this paper, we present the multimodal interactive data and our behavioral model based on recurrent neural networks, namely Long-Short Term Memory (LSTM) and Bidirectional LSTM (BiLSTM) models. Speech, gaze and gestures of two subjects involved in a collaborative task are here jointly modeled. The results show that the proposed deep neural networks are more effective than the conventional statistical methods in generating appropriate overt actions for both on-line and off-line prediction tasks. © 2017 Elsevier B.V.","Behavioral models; Bi-directional LSTM; Co-verbal behavior; Face-to-face interaction; LSTM; Multi-task RNN; Multimodal behavior","Bayesian networks; Behavioral research; Deep learning; Deep neural networks; E-learning; Hidden Markov models; Long short-term memory; Markov processes; Speech processing; Behavioral model; Bi-directional; Face-to-face interaction; LSTM; Multi-modal; Recurrent neural networks",Article,"Final","",Scopus,2-s2.0-85030327162
"Wu X., Li J., Wu Q., Sun J.","57200820459;55441752700;57043789700;12645161300;","Appearance-Based gaze block estimation via CNN classification",2017,"2017 IEEE 19th International Workshop on Multimedia Signal Processing, MMSP 2017","2017-January",,,"1","5",,9,"10.1109/MMSP.2017.8122270","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043397053&doi=10.1109%2fMMSP.2017.8122270&partnerID=40&md5=fde3a862d69f1bb55d8e4e9b167d6e3a","School of Information Science and Engineering, Shandong University, Jinan, China; School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China","Wu, X., School of Information Science and Engineering, Shandong University, Jinan, China; Li, J., School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China; Wu, Q., School of Information Science and Engineering, Shandong University, Jinan, China; Sun, J., School of Information Science and Engineering, Shandong University, Jinan, China","Appearance-based gaze estimation methods have received increasing attention in the field of human-computer interaction (HCI). These methods tried to estimate the accurate gaze point via Convolutional Neural Network (CNN) model, but the estimated accuracy can't reach the requirement of gazebased HCI when the regression model is used in the output layer of CNN. Given the popularity of button-touch-based interaction, we propose an appearance-based gaze block estimation method, which aims to estimate the gaze block, not the gaze point. In the proposed method, we relax the estimation from point to block, so that the gaze block can be estimated by CNN-based classification instead of the previous regression model. We divide the screen into square blocks to imitate the button-touch interface, and build an eye-image dataset, which contains the eye images labelled by their corresponding gaze blocks on the screen. We train the CNN model according to this dataset to estimate the gaze block by classifying the eye images. The experiments on 6- A nd 54-block classifications demonstrate that the proposed method has high accuracy in gaze block estimation without any calibration, and it is promising in button-touch-based interaction. © 2017 IEEE.","Appearance-Based; Button-Touch-Based Interaction; CNN; Gaze block; Gaze Estimation","Calibration; Classification (of information); Multimedia signal processing; Neural networks; Regression analysis; Signal processing; Touch screens; Appearance based; Block estimations; Convolutional Neural Networks (CNN); Gaze block; Gaze estimation; Human Computer Interaction (HCI); Regression model; Touch based interactions; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85043397053
"Han S.Y., Lee S.H., Cho N.I.","57193417343;36063845300;7201718669;","Gaze Estimation Using 3-D eyeball model under HMD circumstance",2017,"2017 IEEE 19th International Workshop on Multimedia Signal Processing, MMSP 2017","2017-January",,,"1","4",,4,"10.1109/MMSP.2017.8122263","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043354076&doi=10.1109%2fMMSP.2017.8122263&partnerID=40&md5=4506108e2b9d37de05670e9708c20f89","Dept. of ECE, INMC, Seoul National Univ., Seoul, South Korea","Han, S.Y., Dept. of ECE, INMC, Seoul National Univ., Seoul, South Korea; Lee, S.H., Dept. of ECE, INMC, Seoul National Univ., Seoul, South Korea; Cho, N.I., Dept. of ECE, INMC, Seoul National Univ., Seoul, South Korea","This paper presents a gaze estimation algorithm using 3-D eyeball model and 2-D pupil center-inner eye corner(PC-IEC) vector. The conventional methods using feature points in the eye images need lots of calibration markers and long calibration time. However, since the pupil and gaze movements are closely related to the 3-D rotation of eyeball, the long and complicated calibrations are not necessary. This paper derives the relationship between the 3-D eyeball model and 2-D PCIEC vector with a single reference calibration point which is located at the center of the screen. Also, the proposed algorithm compensates for the eyeball movements using the eyelid height against the inner eye corner. According to the experiment, the proposed method estimates the gaze within 2 degree error. © 2017 IEEE.",,"Calibration; Eye movements; Multimedia signal processing; Signal processing; Calibration points; Calibration time; Conventional methods; Eye corners; Eyeball movements; Gaze estimation; Gaze movements; Pupil centers; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85043354076
"Jiang B., Wu W., Liu S., Chen H., He L., Shen Y.","56443199100;55707533700;57190766930;57194771022;57200439762;57200439499;","Subtitle Positioning for E-learning Videos Based on Rough Gaze Estimation and Saliency Detection",2017,"SIGGRAPH Asia 2017 Posters, SA 2017",,,"15","","",,2,"10.1145/3145690.3145735","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041299573&doi=10.1145%2f3145690.3145735&partnerID=40&md5=c8d65a521e42feb9e6b6a48030da665b","School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China; School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China; School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China; School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China; School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China; School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China","Jiang, B., School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China; Wu, W., School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China; Liu, S., School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China; Chen, H., School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China; He, L., School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China; Shen, Y., School of Education Science and Technology, Nanjing University of Posts and Telecommunications, China","Subtitle is very common shown in a variety categories of videos, especially useful as translated subtitles for native speakers. Traditional subtitle is placed at the bottom of videos in order to prevent from occluding essential video contents. However, traversing between important video contents and subtitle frequently will have a negative impact on focusing watching video itself. Recently, some research work try more flexible subtitle positioning strategy. However, these methods are effective with restrictions on the video content and devices adopted. In this work, we propose a novel subtitle content organization and placement framework based on rough gaze estimation and saliency detection.","Gaze estimation; Saliency detection; Subtitle positioning","Interactive computer graphics; Gaze estimation; Saliency detection; Subtitle positioning; Video contents; Video recording",Conference Paper,"Final","",Scopus,2-s2.0-85041299573
"Lander C., Gehring S., Löchtefeld M., Bulling A., Krüger A.","55785449200;36095852400;34881899600;6505807414;35264048900;","EyeMirror: Mobile calibration-free gaze approximation using corneal imaging",2017,"ACM International Conference Proceeding Series",,,,"279","291",,3,"10.1145/3152832.3152839","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040509426&doi=10.1145%2f3152832.3152839&partnerID=40&md5=bb0cd6f23ca37d70241aa6f87228b282","German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Department of Architecture, Design and Media Technology, Aalborg University, Aalborg, Denmark; Max Planck Institute for Informatics, Saarbrücken, Germany","Lander, C., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Gehring, S., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany; Löchtefeld, M., Department of Architecture, Design and Media Technology, Aalborg University, Aalborg, Denmark; Bulling, A., Max Planck Institute for Informatics, Saarbrücken, Germany; Krüger, A., German Research Center for Artificial Intelligence (DFKI), Saarbrücken, Germany","Gaze is a powerful measure of people’s attracted attention and reveals where we are looking within our current FOV. Hence gaze-based interfaces are gaining in importance. However, gaze estimation usually requires extensive hardware and depends on a calibration that has to be renewed regularly. We present EyeMirror, a mobile device for calibration-free gaze approximation on surfaces (e.g., displays). It consists of a head-mounted camera, connected to a wearable minicomputer, capturing the environment reflected on the human cornea. The corneal images are analyzed using natural feature tracking for gaze estimation on surfaces. In two lab studies we compared variations of EyeMirror against established methods for gaze estimation in a display scenario, and investigated the effect of display content (i.e. number of features). EyeMirror achieved 4.03 gaze estimation error, while we found no significant effect of display content. © 2017 ACM","Corneal Image; Feature tracking; Gaze approximation; Mobile device; Pervasive","Calibration; Mobile devices; Calibration free; Corneal images; Feature-tracking; Gaze approximation; Gaze estimation; Head mounted Camera; Natural feature tracking; Pervasive; Display devices",Conference Paper,"Final","",Scopus,2-s2.0-85040509426
"Fornalczyk K., Wojciechowski A.","56495242800;15021546300;","Robust face model based approach to head pose estimation",2017,"Proceedings of the 2017 Federated Conference on Computer Science and Information Systems, FedCSIS 2017",,,"8104720","1291","1295",,13,"10.15439/2017F425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039899670&doi=10.15439%2f2017F425&partnerID=40&md5=0cc7d24cefa414428d2dfedbf3b67169","Lodz University of Technology, Institute of Information Technology, Wolczanska 215, Lodz, 90-924, Poland","Fornalczyk, K., Lodz University of Technology, Institute of Information Technology, Wolczanska 215, Lodz, 90-924, Poland; Wojciechowski, A., Lodz University of Technology, Institute of Information Technology, Wolczanska 215, Lodz, 90-924, Poland","Head pose estimation from camera images is a computational problem that may influence many sociological, cognitive, interaction and marketing researches. It is especially crucial in the process of visual gaze estimation which accuracy depends not only on eye region analysis, but head inferring as well. Presented method exploits a 3d head model for a user head pose estimation as it outperforms, in the context of performance, popular appearance based approaches and assures efficient face head pose analysis. The novelty of the presented approach lies in a default head model refinement according to the selected facial features localisation. The new method not only achieves very high precision (about 4°), but iteratively improves the reference head model. The results of the head pose inferring experiments were verified with professional Vicon motion tracking system and head model refinement accuracy was verified with high precision Artec structural light scanner. © 2017 PTI.",,"Information systems; Iterative methods; Marketing; Appearance based approach; Computational problem; Efficient faces; Gaze estimation; Head Pose Estimation; Marketing research; Motion tracking system; Structural lights; Image recognition",Conference Paper,"Final","",Scopus,2-s2.0-85039899670
"Shrivastava A., Pfister T., Tuzel O., Susskind J., Wang W., Webb R.","57197090723;36598926300;8619883500;15077025900;57201317548;57201314933;","Learning from simulated and unsupervised images through adversarial training",2017,"Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017","2017-January",,,"2242","2251",,693,"10.1109/CVPR.2017.241","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041912723&doi=10.1109%2fCVPR.2017.241&partnerID=40&md5=67b60a2b6177dba9328c2026ea55a252","Apple Inc., United States","Shrivastava, A., Apple Inc., United States; Pfister, T., Apple Inc., United States; Tuzel, O., Apple Inc., United States; Susskind, J., Apple Inc., United States; Wang, W., Apple Inc., United States; Webb, R., Apple Inc., United States","With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulator's output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a 'self-regularization' term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data. © 2017 IEEE.",,"Computer vision; Image annotation; Pattern recognition; Adversarial networks; Gaze estimation; Hand pose estimations; Key modifications; Realistic images; Recent progress; State of the art; Synthetic images; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-85041912723
"Siegfried R., Yu Y., Odobez J.-M.","57195685304;57188644020;57203103085;","Towards the use of social interaction conventions as prior for gaze model adaptation",2017,"ICMI 2017 - Proceedings of the 19th ACM International Conference on Multimodal Interaction","2017-January",,,"154","162",,7,"10.1145/3136755.3136793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046706383&doi=10.1145%2f3136755.3136793&partnerID=40&md5=d18da421a4a24520f35fe674f1cb89f0","Idiap Research Institute Martigny, École Polytechnique Fédéral de Lausanne, Switzerland","Siegfried, R., Idiap Research Institute Martigny, École Polytechnique Fédéral de Lausanne, Switzerland; Yu, Y., Idiap Research Institute Martigny, École Polytechnique Fédéral de Lausanne, Switzerland; Odobez, J.-M., Idiap Research Institute Martigny, École Polytechnique Fédéral de Lausanne, Switzerland","Gaze is an important non-verbal cue involved in many facets of social interactions like communication, attentiveness or attitudes. Nevertheless, extracting gaze directions visually and remotely usually suffers large errors because of lowresolution images, inaccurate eye cropping, or large eye shape variations across the population, amongst others. This paper hypothesizes that these challenges can be addressed by exploiting multimodal social cues for gaze model adaptation on top of an head-pose independent 3D gaze estimation framework. First, a robust eye cropping refinement is achieved by combining a semantic face model with eye landmark detections. Investigations on whether temporal smoothing can overcome instantaneous refinement limitations is conducted. Secondly, to study whether social interaction convention could be used as priors for adaptation, we exploited the speaking status and head pose constraints to derive soft gaze labels and infer person-specific gaze bias using robust statistics. Experimental results on gaze coding in natural interactions from two different settings demonstrate that the two steps of our gaze adaptation method contribute to reduce gaze errors by a large margin over the baseline and can be generalized to several identities in challenging scenarios. © 2017 ACM.","Appearance based model; Bias correction; Gaze estimation; Personinvariance; Rgb-d cameras","Interactive computer systems; Semantics; Appearance-based modeling; Bias correction; Gaze estimation; Personinvariance; Rgb-d cameras; Coding errors",Conference Paper,"Final","",Scopus,2-s2.0-85046706383
"Mussgnug M., Singer D., Lohmeyer Q., Meboldt M.","56100585200;57189454238;43461718100;22835485400;","Automated interpretation of eye–hand coordination in mobile eye tracking recordings: Identifying demanding phases in human–machine interactions",2017,"KI - Kunstliche Intelligenz","31","4",,"331","337",,10,"10.1007/s13218-017-0503-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053461185&doi=10.1007%2fs13218-017-0503-y&partnerID=40&md5=3e69dab45b949769712566a91d708804","ETH Zurich, Leonhardstrasse 21, Zurich, 8092, Switzerland","Mussgnug, M., ETH Zurich, Leonhardstrasse 21, Zurich, 8092, Switzerland; Singer, D., ETH Zurich, Leonhardstrasse 21, Zurich, 8092, Switzerland; Lohmeyer, Q., ETH Zurich, Leonhardstrasse 21, Zurich, 8092, Switzerland; Meboldt, M., ETH Zurich, Leonhardstrasse 21, Zurich, 8092, Switzerland","Mobile eye tracking is beneficial for the analysis of human–machine interactions of tangible products, as it tracks the eye movements reliably in natural environments, and it allows for insights into human behaviour and the associated cognitive processes. However, current methods require a manual screening of the video footage, which is time-consuming and subjective. This work aims to automatically detect cognitive demanding phases in mobile eye tracking recordings. The approach presented combines the user’s perception (gaze) and action (hand) to isolate demanding interactions based upon a multi-modal feature level fusion. It was validated in a usability study of a 3D printer with 40 participants by comparing the usability problems found to a thorough manual analysis. The new approach detected 17 out of 19 problems, while the time for manual analyses was reduced by 63%. More than eye tracking alone, adding the information of the hand enriches the insights into human behaviour. The field of AI could significantly advance our approach by improving the hand-tracking through region proposal CNNs, by detecting the parts of a product and mapping the demanding interactions to these parts, or even by a fully automated end-to-end detection of demanding interactions via deep learning. This could set the basis for machines providing real-time assistance to the machine’s users in cases where they are struggling. © 2017, Springer-Verlag GmbH Deutschland.","Cognitive processes; Event interpretation; Eye-hand coordination; Human–machine interaction; Mobile eye tracking; Usability testing",,Article,"Final","",Scopus,2-s2.0-85053461185
"Shakeri H., Nixon M., DiPaola S.","57197771214;57095522100;14035416700;","Saliency-based artistic abstractionwith deep learning and regression trees",2017,"Journal of Imaging Science and Technology","61","6","060402","","",,1,"10.2352/J.ImagingSci.Technol.2017.61.6.060402","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040190036&doi=10.2352%2fJ.ImagingSci.Technol.2017.61.6.060402&partnerID=40&md5=4f64842e91fb9271711b6fab5b02ab9b","Simon Fraser University, 250-13450 102nd Avenue, Surrey, BC V3T 0A3, Canada","Shakeri, H., Simon Fraser University, 250-13450 102nd Avenue, Surrey, BC V3T 0A3, Canada; Nixon, M., Simon Fraser University, 250-13450 102nd Avenue, Surrey, BC V3T 0A3, Canada; DiPaola, S., Simon Fraser University, 250-13450 102nd Avenue, Surrey, BC V3T 0A3, Canada","Reflects human perception - areas of an artwork that hold the observer's gaze longest will generally be more detailed, while peripheral areas are abstracted, just as they are mentally abstracted by humans' physiological visual process. The authors' artistic abstraction tool, Salience Stylize, uses Deep Learning to predict the areas in an image that the observer's gaze will be drawn to, which informs the system about which areas to keep the most detail in and which to abstract most. The planar abstraction is done by a Random Forest Regressor, splitting the image into large planes and adding more detailed planes as it progresses, just as an artist starts with tonally limited masses and iterates to add fine details, then completed with our stroke engine. The authors evaluated the aesthetic appeal and effectiveness of the detail placement in the artwork produced by Salience Stylize through two user studies with 30 subjects. © 2017 Society for Imaging Science and Technology.",,"Abstracting; Decision trees; Aesthetic appeals; Human perception; Random forests; Regression trees; User study; Visual process; Deep learning",Article,"Final","",Scopus,2-s2.0-85040190036
"Dadkhahi H., Duarte M.F., Marlin B.M.","55151400500;15032834800;6506955008;","Out-of-Sample Extension for Dimensionality Reduction of Noisy Time Series",2017,"IEEE Transactions on Image Processing","26","11","8000338","5435","5446",,2,"10.1109/TIP.2017.2735189","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028809642&doi=10.1109%2fTIP.2017.2735189&partnerID=40&md5=6ac7eeee88bfd6e5d914e4a1831950c4","Electrical and Computer Engineering Department, University of Massachusetts at Amherst, Amherst, MA  01003, United States; College of Information and Computer Sciences, University of Massachusetts, Amherst, MA  01003, United States","Dadkhahi, H., Electrical and Computer Engineering Department, University of Massachusetts at Amherst, Amherst, MA  01003, United States, College of Information and Computer Sciences, University of Massachusetts, Amherst, MA  01003, United States; Duarte, M.F., Electrical and Computer Engineering Department, University of Massachusetts at Amherst, Amherst, MA  01003, United States; Marlin, B.M., College of Information and Computer Sciences, University of Massachusetts, Amherst, MA  01003, United States","This paper proposes an out-of-sample extension framework for a global manifold learning algorithm (Isomap) that uses temporal information in out-of-sample points in order to make the embedding more robust to noise and artifacts. Given a set of noise-free training data and its embedding, the proposed framework extends the embedding for a noisy time series. This is achieved by adding a spatio-temporal compactness term to the optimization objective of the embedding. To the best of our knowledge, this is the first method for out-of-sample extension of manifold embeddings that leverages timing information available for the extension set. Experimental results demonstrate that our out-of-sample extension algorithm renders a more robust and accurate embedding of sequentially ordered image data in the presence of various noise and artifacts when compared with other timing-aware embeddings. Additionally, we show that an out-of-sample extension framework based on the proposed algorithm outperforms the state of the art in eye-gaze estimation. © 1992-2012 IEEE.","dimensionality reduction; Manifold learning; out-of-sample extension; time series","Optimization; Time series; Dimensionality reduction; Extension sets; Manifold learning; Manifold learning algorithm; Out-of-sample extension; State of the art; Temporal information; Timing information; Learning algorithms; algorithm; article; artifact; embedding; gaze; human; human experiment; learning algorithm; noise; time series analysis",Article,"Final","",Scopus,2-s2.0-85028809642
"Cheng H., Liu Y., Fu W., Ji Y., Yang L., Zhao Y., Yang J.","56083885000;57194829338;56022697900;36677523000;55961049500;57194829576;56967287500;","Gazing point dependent eye gaze estimation",2017,"Pattern Recognition","71",,,"36","44",,16,"10.1016/j.patcog.2017.04.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023193603&doi=10.1016%2fj.patcog.2017.04.026&partnerID=40&md5=34d9a4e3f3c34b0ee7f1f3ffde1ecfc1","University of Electronic Science and Technology of China, Chengdu 611731, China; Carnegie Mellon University, Forbes Avenue, Pittsburgh, PA 15213, United States","Cheng, H., University of Electronic Science and Technology of China, Chengdu 611731, China; Liu, Y., University of Electronic Science and Technology of China, Chengdu 611731, China; Fu, W., University of Electronic Science and Technology of China, Chengdu 611731, China; Ji, Y., University of Electronic Science and Technology of China, Chengdu 611731, China; Yang, L., University of Electronic Science and Technology of China, Chengdu 611731, China; Zhao, Y., University of Electronic Science and Technology of China, Chengdu 611731, China; Yang, J., Carnegie Mellon University, Forbes Avenue, Pittsburgh, PA 15213, United States","Cross-ratio invariant is used widely in projective transformations for eye gaze estimation. Establishing a virtual plane projection is an important step to use this property. Most of traditional cross-ratio approaches only used fixed parameters to calculate the gazing point. This paper proposes gazing point dependent eye gazing estimation approach. Our contributions are three-folded. First, we model a dynamic virtual plane projection, which is tangent to the cornea of pupil, to estimate the position of the gazing point. Second, we introduce a two stage approach consisting of rough-to-precise framework for gazing point estimation based on the gazing point dependent virtual plane projection. Third, a heuristic strategy which contains off-line and on-line parameter learning for gazing point estimation is proposed. The experiment results show that our approach can significantly improve the gazing estimation performance with an average accuracy of 0.70°. © 2017 Elsevier Ltd","Cross-ratio; Eye gazing Tracking; Human Robot Interaction; Virtual plane","Human robot interaction; Cross-ratios; Estimation approaches; Estimation performance; Eye-gazing; Heuristic strategy; Projective transformation; Two stage approach; Virtual plane; Virtual reality",Article,"Final","",Scopus,2-s2.0-85023193603
"Kim H.-I., Kim J.-B., Park R.-H.","56115915100;56115923900;7401895798;","Efficient and Fast Iris Localization Using Binary Radial Gradient Features for Human-Computer Interaction",2017,"International Journal of Pattern Recognition and Artificial Intelligence","31","11","1756015","","",,3,"10.1142/S0218001417560158","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022325878&doi=10.1142%2fS0218001417560158&partnerID=40&md5=c53c2432e1c3302fe659fc5403dfe2a9","Department of Electronic Engineering, School of Engineering, Sogang University 35, Backbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 04107, South Korea","Kim, H.-I., Department of Electronic Engineering, School of Engineering, Sogang University 35, Backbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 04107, South Korea; Kim, J.-B., Department of Electronic Engineering, School of Engineering, Sogang University 35, Backbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 04107, South Korea; Park, R.-H., Department of Electronic Engineering, School of Engineering, Sogang University 35, Backbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 04107, South Korea","This paper proposes an efficient and fast iris localization method. It uses support vector machine learning of iris features that represent closed outer and inner iris boundaries encompassing a low-intensity region. In addition, depending on the location of the iris in an eye image, an iris detection method is proposed based on three sub-datasets of eye images (middle, right, and left sub-datasets) with different iris features. The proposed method is implemented using fast sliding window and fast computation of the iris detection score with binary features. Compared with state-of-the-art methods, experimental results show that the proposed method is twice as fast and has comparable accuracy, even when factoring in head rotation, glasses, and highlights. © 2017 World Scientific Publishing Company.","binary feature; fast sliding window; gaze estimation; human-computer interaction; Iris localization; radial gradient","Bins; Feature extraction; Binary features; Fast sliding; Gaze estimation; Iris localization; Radial gradient; Human computer interaction",Article,"Final","",Scopus,2-s2.0-85022325878
"Chaabouni S., Benois-pineau J., Tison F., Ben Amar C., Zemmari A.","57190285991;6701750610;7004381839;25959856600;6507308959;","Prediction of visual attention with deep CNN on artificially degraded videos for studies of attention of patients with Dementia",2017,"Multimedia Tools and Applications","76","21",,"22527","22546",,9,"10.1007/s11042-017-4796-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019183731&doi=10.1007%2fs11042-017-4796-5&partnerID=40&md5=1d0068d6d1c890f44015ed92727be4c1","LaBRI UMR 5800, University of Bordeaux, Talence, 33400, France; REGIM-Lab LR11ES48, University of Sfax, Sfax, 3029, Tunisia; CHU de Bordeaux-GH Pellegrin, Bordeaux, France","Chaabouni, S., LaBRI UMR 5800, University of Bordeaux, Talence, 33400, France, REGIM-Lab LR11ES48, University of Sfax, Sfax, 3029, Tunisia; Benois-pineau, J., LaBRI UMR 5800, University of Bordeaux, Talence, 33400, France; Tison, F., CHU de Bordeaux-GH Pellegrin, Bordeaux, France; Ben Amar, C., REGIM-Lab LR11ES48, University of Sfax, Sfax, 3029, Tunisia; Zemmari, A., LaBRI UMR 5800, University of Bordeaux, Talence, 33400, France","Studies of visual attention of patients with Dementia such as Parkinson’s Disease Dementia and Alzheimer Disease is a promising way for non-invasive diagnostics. Past research showed, that people suffering from dementia are not reactive with regard to degradations on still images. Attempts are being made to study their visual attention relatively to the video content. Here the delays in their reactions on novelty and “unusual” novelty of the visual scene are expected. Nevertheless, large-scale screening of population is possible only if sufficiently robust automatic prediction models can be built. In the medical protocols the detection of Dementia behavior in visual content observation is always performed in comparison with healthy, “normal control” subjects. Hence, it is a research question per see as to develop an automatic prediction models for specific visual content to use in psycho-visual experience involving Patients with Dementia (PwD). The difficulty of such a prediction resides in a very small amount of training data. In this paper the reaction of healthy normal control subjects on degraded areas in videos was studied. Furthermore, in order to build an automatic prediction model for salient areas in intentionally degraded videos for PwD studies, a deep learning architecture was designed. Optimal transfer learning strategy for training the model in case of very small amount of training data was deployed. The comparison with gaze fixation maps and classical visual attention prediction models was performed. Results are interesting regarding the reaction of normal control subjects against degraded areas in videos. © 2017, Springer Science+Business Media New York.","Deep CNN; Degraded video; Dementia diseases; Normal video; Saliency","Diagnosis; Forecasting; Neurodegenerative diseases; Automatic prediction; Deep CNN; Degraded video; Learning architectures; Non-invasive diagnostics; Normal video; Patients with dementia; Saliency; Behavioral research",Article,"Final","",Scopus,2-s2.0-85019183731
"Zhang X., Sugano Y., Bulling A.","57142162900;7005470045;6505807414;","Everyday eye contact detection using unsupervised gaze target discovery",2017,"UIST 2017 - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology",,,,"193","203",,31,"10.1145/3126594.3126614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041540055&doi=10.1145%2f3126594.3126614&partnerID=40&md5=6aa3b167e94bd1667fbc857f26e69354","Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Graduate School of Information Science and Technology, Osaka University, Japan","Zhang, X., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Japan; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Eye contact is an important non-verbal cue in social signal processing and promising as a measure of overt attention in human-object interactions and attentive user interfaces. However, robust detection of eye contact across different users, gaze targets, camera positions, and illumination conditions is notoriously challenging. We present a novel method for eye contact detection that combines a state-of-the-art appearancebased gaze estimator with a novel approach for unsupervised gaze target discovery, i.e. without the need for tedious and time-consuming manual data annotation. We evaluate our method in two real-world scenarios: detecting eye contact at the workplace, including on the main work display, from cameras mounted to target objects, as well as during everyday social interactions with the wearer of a head-mounted egocentric camera. We empirically evaluate the performance of our method in both scenarios and demonstrate its effectiveness for detecting eye contact independent of target object type and size, camera position, and user and recording environment. © 2017 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","Appearance-based gaze estimation; Attentive user interfaces; Eye contact; Social signal processing","Cameras; Human computer interaction; Signal processing; Attentive user interfaces; Eye contact; Gaze estimation; Human-object interaction; Illumination conditions; Recording environment; Social interactions; Social signal processing; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85041540055
"Khamis M., Hoesl A., Klimczak A., Reiss M., Alt F., Bulling A.","35243028400;57056356300;57200530213;57200534739;27267528900;6505807414;","EyeScout: Active eye tracking for position and movement independent gaze interaction with large public displays",2017,"UIST 2017 - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology",,,,"155","166",,25,"10.1145/3126594.3126630","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041498559&doi=10.1145%2f3126594.3126630&partnerID=40&md5=7faf90c949b35a57c931f1c913ecc5a9","Ubiquitous Interactive Systems Group, LMU Munich, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","Khamis, M., Ubiquitous Interactive Systems Group, LMU Munich, Germany; Hoesl, A., Ubiquitous Interactive Systems Group, LMU Munich, Germany; Klimczak, A., Ubiquitous Interactive Systems Group, LMU Munich, Germany; Reiss, M., Ubiquitous Interactive Systems Group, LMU Munich, Germany; Alt, F., Ubiquitous Interactive Systems Group, LMU Munich, Germany; Bulling, A., Max Planck Institute for Informatics, Saarland Informatics Campus, Germany","While gaze holds a lot of promise for hands-free interaction with public displays, remote eye trackers with their confined tracking box restrict users to a single stationary position in front of the display. We present EyeScout, an active eye tracking system that combines an eye tracker mounted on a rail system with a computational method to automatically detect and align the tracker with the user's lateral movement. EyeScout addresses key limitations of current gaze-enabled large public displays by offering two novel gaze-interaction modes for a single user: In ""Walk then Interact"" the user can walk up to an arbitrary position in front of the display and interact, while in ""Walk and Interact"" the user can interact even while on the move. We report on a user study that shows that EyeScout is well perceived by users, extends a public display's sweet spot into a sweet line, and reduces gaze interaction kickoff time to 3.5 seconds - a 62% improvement over state of the art solutions. We discuss sample applications that demonstrate how EyeScout can enable position and movement-independent gaze interaction with large public displays. © 2017 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","Body tracking; Gaze estimation; Gaze-enabled displays","User interfaces; Arbitrary positions; Body tracking; Eye tracking systems; Gaze estimation; Hands-free interactions; Lateral movement; Sample applications; Stationary positions; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85041498559
"Van Der Meulen H., Kun A.L., Shaer O.","57191981102;7004078994;16305412100;","What are we missing? Adding eye-tracking to the hololens to improve gaze estimation accuracy",2017,"Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces, ISS 2017",,,,"396","400",,14,"10.1145/3132272.3132278","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034774158&doi=10.1145%2f3132272.3132278&partnerID=40&md5=e24961f90fec2c9503b4b246dcc44bc2","University College Dublin, Dublin, Ireland; University of New Hampshire, Durham, NH, United States; Wellesley College WellesleyMA, United States","Van Der Meulen, H., University College Dublin, Dublin, Ireland; Kun, A.L., University of New Hampshire, Durham, NH, United States; Shaer, O., Wellesley College WellesleyMA, United States","The Microsoft HoloLens keeps track of its location and rotation relative to the environment but lacks the ability to capture eye gaze data. We assess a novel method to extend the HoloLens with a head mounted eye-tracker. Using a combination of eye gaze data and head rotation we compared gaze behavior between real and virtual objects. Results indicate that eye-tracking plays an important role in accurately determining a userâ€™s gaze for real objects in contrast to virtual objects. © 2017 Copyright is held by the owner/author(s).","Augmented Reality; Eye-tracking","Eye trackers; Eye-tracking; Gaze behavior; Gaze estimation; Head rotation; MicroSoft; Real objects; Virtual objects; Augmented reality",Conference Paper,"Final","",Scopus,2-s2.0-85034774158
"Zhang Y., Li Y., Xie B., Li X., Zhu J.","55739827400;57199057151;57199049098;57089954500;55834300600;","Pupil localization algorithm combining convex area voting and model constraint",2017,"Pattern Recognition and Image Analysis","27","4",,"846","854",,4,"10.1134/S1054661817040216","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037542663&doi=10.1134%2fS1054661817040216&partnerID=40&md5=8f40b0fe7aa9385c287fe567037de8a2","College of Mechanical and Electrical Engineering, China Jiliang University, Zhejiang, Hangzhou  310018, China","Zhang, Y., College of Mechanical and Electrical Engineering, China Jiliang University, Zhejiang, Hangzhou  310018, China; Li, Y., College of Mechanical and Electrical Engineering, China Jiliang University, Zhejiang, Hangzhou  310018, China; Xie, B., College of Mechanical and Electrical Engineering, China Jiliang University, Zhejiang, Hangzhou  310018, China; Li, X., College of Mechanical and Electrical Engineering, China Jiliang University, Zhejiang, Hangzhou  310018, China; Zhu, J., College of Mechanical and Electrical Engineering, China Jiliang University, Zhejiang, Hangzhou  310018, China","Locating the center of the eyes plays a significant role in many computer vision applications and research, such as face alignment, face recognition, human-computer interaction, control devices for disabled people, user attention and gaze estimation. The disturbances such as occlusions by eyelashes or eyelids, uneven spots and spectacle frames of glasses affect the accuracy and stability of eye center location. This paper presents a hybrid eye center locating methodology for infrared eye images. The pupil edge points are extracted by Starburst algorithm, and when we get the position and the gradient of the edge points, the approximate pupil boundary is determined by a convex region voting methods. After that, the boundary edge points are iteratively optimized by fitting an ellipses modeling constraint. Finally, the pupil is located correctly. Experiment shows that this algorithm has performance advantages compared with some state of the art approaches in pupil localization accuracy, iteration times and their performance. This algorithm combining convex area voting and model constraint has strong robustness, high accuracy and speed in real environments with occlusions and distortion pupil. © 2017, Pleiades Publishing, Ltd.","convex area voting; gradient feature; infrared image processing; model constraint; outliers eliminate","Face recognition; Human computer interaction; Image processing; Infrared imaging; Location; Computer vision applications; convex area voting; Eye center locations; Gradient feature; Model constraints; outliers eliminate; Pupil localization; State-of-the-art approach; Iterative methods",Article,"Final","",Scopus,2-s2.0-85037542663
"Wang H., Antonelli M., Shi B.E.","56809110800;56263003900;7402547071;","Using point cloud data to improve three dimensional gaze estimation",2017,"Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS",,,"8036944","795","798",,7,"10.1109/EMBC.2017.8036944","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032194852&doi=10.1109%2fEMBC.2017.8036944&partnerID=40&md5=5e18b3ee4ef51cb016633f91868ba9dd","Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","Wang, H., Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Antonelli, M., Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Shi, B.E., Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","This paper addresses the problem of estimating gaze location in the 3D environment using a remote eye tracker. Instead of relying only on data provided by the eye tracker, we investigate how to integrate gaze direction with the point-cloud-based representation of the scene provided by a Kinect sensor. The algorithm first combines the gaze vectors for the two eyes provided by the eye tracker into a single gaze vector emanating from a point in between the two eyes. The gaze target in the three dimensional environment is then identified by finding the point in the 3D point cloud that is closest to the gaze vector. Our experimental results demonstrate that the estimate of the gaze target location provided by this method is significantly better than that provided when considering gaze information alone. It is also better than two other methods for integrating point cloud information: (1) finding the 3D point closest to the gaze location as estimated by triangulating the gaze vectors from the two eyes, and (2) finding the 3D point with smallest average distance to the two gaze vectors considered individually. The proposed method has an average error of 1.7 cm in a workspace of 25 × 23 × 24 cm located at a distance of 60 cm from the user. © 2017 IEEE.","3D gaze estimation; Eye tracker; Human computer interaction; Point cloud","algorithm; eye; eye fixation; Algorithms; Eye; Fixation, Ocular",Conference Paper,"Final","",Scopus,2-s2.0-85032194852
[无可用作者姓名],[无可用的作者 ID],"ACM International Conference Proceeding Series",2017,"ACM International Conference Proceeding Series",,,,"","",69,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046054415&partnerID=40&md5=10a2e3c2e86db5cf8e809736c98c64bd",,"","The proceedings contain 13 papers. The topics discussed include: a vision-based attitude/position estimation for the automatic landing of unmanned helicopter on ship; research on video capture scheme and face recognition algorithms in a class attendance system; a semi-fragile watermarking algorithm against geometric transformation for vector geographic data; joint watermarking and lossless JPEG-LS compression for medical image security; blur invariant block based copy-move forgery detection technique using FWHT features; secure textural watermarking methods for printed document protection; fish tracking using acoustical and optical data fusion in underwater environment; cluster based gaze estimation and data visualization supporting diverse environments; an evaluation of synthetic data for deep learning stereo depth algorithms; unsupervised hyperspectral band selection by sequential clustering; SOM based clustering of two-dimensional gel image segments for quantitative validation of changes in proteome; navigation lights online detection method based on improved random Hough transform; and a new method of elastography reconstruction based on modified extrema location algorithm.",,,Conference Review,"Final","",Scopus,2-s2.0-85046054415
"Kao C.-W., Hwang B.-J., Chen H.-H., Fan K.-C., Wu S.-H.","53984348300;7201453946;37035935400;34770534700;55913950300;","Cluster based gaze estimation and data visualization supporting diverse environments",2017,"ACM International Conference Proceeding Series",,,,"37","41",,1,"10.1145/3150978.3150988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040346544&doi=10.1145%2f3150978.3150988&partnerID=40&md5=9267d766c5461877865681dc82476df5","Department of Computer Science and Information Engineering, National Central University,Taiwn, No.300, Jhongda Rd., Jhongli City, Taoyuan County, 32001, Taiwan; Department of Computer and Communication Engineering, Ming-Chuan University Taiwn, 5 De Ming Rd., Gui Shan District, Taoyuan City, 333, Taiwan; Department of Hotel Management, Vanung University, Taiwn, No.1, Wanneng Rd., Zhongli District., Taoyuan City, 32061, Taiwan","Kao, C.-W., Department of Computer Science and Information Engineering, National Central University,Taiwn, No.300, Jhongda Rd., Jhongli City, Taoyuan County, 32001, Taiwan; Hwang, B.-J., Department of Computer and Communication Engineering, Ming-Chuan University Taiwn, 5 De Ming Rd., Gui Shan District, Taoyuan City, 333, Taiwan; Chen, H.-H., Department of Computer and Communication Engineering, Ming-Chuan University Taiwn, 5 De Ming Rd., Gui Shan District, Taoyuan City, 333, Taiwan; Fan, K.-C., Department of Computer Science and Information Engineering, National Central University,Taiwn, No.300, Jhongda Rd., Jhongli City, Taoyuan County, 32001, Taiwan; Wu, S.-H., Department of Hotel Management, Vanung University, Taiwn, No.1, Wanneng Rd., Zhongli District., Taoyuan City, 32061, Taiwan","This paper proposes a method to explore the navigation of audience by estimating gaze points and labeling them to the objects of video or web content. The cluster based gaze estimation and statistics based labeling method are proposed to support the multiple user environments as well as improve the accuracy and usability. In addition, the number of feature clusters is accorded as the amount of areas of target screen. Additionally, statistics based labeling method is proposed to mapping the gaze points to target object by computing the probability. Moreover, this method can provide a good manner to estimate the attentive objects. The visualization of attentive blocks and the amount of probabilities are presented to illustrate the navigation of audience as attentive object. Therefore, this visualization method can exhibit the navigation behavior of audience more real, besides overcomes the problem of difficult labeling objects. The experimental results demonstrate the proposed method provide more robustness for long range as well as diversity environments. © 2017 Association for Computing Machinery.","Cluster; Data; Gaze Estimation; Statistics Based Labeling","Data visualization; Image watermarking; Navigation; Visualization; Cluster; Cluster-based; Data; Gaze estimation; Labeling methods; Multiple user; Navigation behavior; Visualization method; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85040346544
"Bâce M.","44461063200;","Augmenting human interaction capabilities with proximity, natural gestures, and eye gaze",2017,"Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services, MobileHCI 2017",,,"71","","",,1,"10.1145/3098279.3119924","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030308444&doi=10.1145%2f3098279.3119924&partnerID=40&md5=0a85a4ecbb335d048c2d7385739ee734","Department of Computer Science, ETH Zurich, Switzerland","Bâce, M., Department of Computer Science, ETH Zurich, Switzerland","Nowadays, humans are surrounded by many complex computer systems. When people interact among each other, they use multiple modalities including voice, body posture, hand gestures, facial expressions, or eye gaze. Currently, computers can only understand a small subset of these modalities, but such cues can be captured by an increasing number of wearable devices. This research aims to improve traditional human-human and human-machine interaction by augmenting humans with wearable technology and developing novel user interfaces. More specifically, (i) we investigate and develop systems that enable a group of people in close proximity to interact using in-air hand gestures and facilitate effortless information sharing. Additionally, we focus on (ii) eye gaze which can further enrich the interaction between humans and cyber-physical systems.","Augmented human; Collaboration; CPS; Deep learning; Eye gaze; HCI; Proximity detection; Wearable technology","Deep learning; Embedded systems; Mobile devices; User interfaces; Wearable technology; Augmented human; Collaboration; Complex computer systems; Eye-gaze; Human machine interaction; Information sharing; Multiple modalities; Proximity detection; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85030308444
"Alnajar F., Gevers T., Valenti R., Ghebreab S.","35797175900;7003472472;57192175392;6602783975;","Auto-Calibrated Gaze Estimation Using Human Gaze Patterns",2017,"International Journal of Computer Vision","124","2",,"223","236",,8,"10.1007/s11263-017-1014-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019600913&doi=10.1007%2fs11263-017-1014-x&partnerID=40&md5=53adb52c4de6488bf935a73777ce9f62","Informatics Institute, Faculty of Science, University of Amsterdam, Amsterdam, Netherlands; Amsterdam University College, Amsterdam, Netherlands","Alnajar, F., Informatics Institute, Faculty of Science, University of Amsterdam, Amsterdam, Netherlands; Gevers, T., Informatics Institute, Faculty of Science, University of Amsterdam, Amsterdam, Netherlands; Valenti, R., Informatics Institute, Faculty of Science, University of Amsterdam, Amsterdam, Netherlands; Ghebreab, S., Amsterdam University College, Amsterdam, Netherlands","We present a novel method to auto-calibrate gaze estimators based on gaze patterns obtained from other viewers. Our method is based on the observation that the gaze patterns of humans are indicative of where a new viewer will look at. When a new viewer is looking at a stimulus, we first estimate a topology of gaze points (initial gaze points). Next, these points are transformed so that they match the gaze patterns of other humans to find the correct gaze points. In a flexible uncalibrated setup with a web camera and no chin rest, the proposed method is tested on ten subjects and ten images. The method estimates the gaze points after looking at a stimulus for a few seconds with an average error below 4. 5 ∘. Although the reported performance is lower than what could be achieved with dedicated hardware or calibrated setup, the proposed method still provides sufficient accuracy to trace the viewer attention. This is promising considering the fact that auto-calibration is done in a flexible setup , without the use of a chin rest, and based only on a few seconds of gaze initialization data. To the best of our knowledge, this is the first work to use human gaze patterns in order to auto-calibrate gaze estimators. © 2017, The Author(s).","Auto-calibration; Calibration free; Eye gaze estimation","Artificial intelligence; Software engineering; Auto calibration; Average errors; Calibration free; Dedicated hardware; Eye-gaze; Gaze estimation; Uncalibrated; Web camera; Calibration",Article,"Final","",Scopus,2-s2.0-85019600913
"Zhang X., Sugano Y., Fritz M., Bulling A.","57142162900;7005470045;14035495500;6505807414;","It's Written All over Your Face: Full-Face Appearance-Based Gaze Estimation",2017,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2017-July",,"8015018","2299","2308",,114,"10.1109/CVPRW.2017.284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030228048&doi=10.1109%2fCVPRW.2017.284&partnerID=40&md5=f1e83f1920e0afa0235ad6d80256ae3a","Perceptual User Interfaces Group, Germany; Scalable Learning and Perception Group, Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Graduate School of Information Science and Technology, Osaka University, Japan","Zhang, X., Perceptual User Interfaces Group, Germany; Sugano, Y., Graduate School of Information Science and Technology, Osaka University, Japan; Fritz, M., Scalable Learning and Perception Group, Max Planck Institute for Informatics, Saarland Informatics Campus, Germany; Bulling, A., Perceptual User Interfaces Group, Germany","Eye gaze is an important non-verbal cue for human affect analysis. Recent gaze estimation work indicated that information from the full face region can benefit performance. Pushing this idea further, we propose an appearance-based method that, in contrast to a long-standing line of work in computer vision, only takes the full face image as input. Our method encodes the face image using a convolutional neural network with spatial weights applied on the feature maps to flexibly suppress or enhance information in different facial regions. Through extensive evaluation, we show that our full-face method significantly outperforms the state of the art for both 2D and 3D gaze estimation, achieving improvements of up to 14.3% on MPIIGaze and 27.7% on EYEDIAP for person-independent 3D gaze estimation. We further show that this improvement is consistent across different illumination conditions and gaze directions and particularly pronounced for the most challenging extreme head poses. © 2017 IEEE.",,"Computer vision; Image enhancement; Neural networks; Affect analysis; Appearance based; Appearance-based methods; Convolutional neural network; Full face method; Illumination conditions; Person-independent; State of the art; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85030228048
"Kar A., Corcoran P.","56956378200;57190839462;","A review and analysis of eye-gaze estimation systems, algorithms and performance evaluation methods in consumer platforms",2017,"IEEE Access","5",,"8003267","16495","16519",,95,"10.1109/ACCESS.2017.2735633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028939777&doi=10.1109%2fACCESS.2017.2735633&partnerID=40&md5=96624267e8d3cc7c4e96bf3aa9700a00","Center for Cognitive, Connected, and Computational Imaging, Department of Electrical and Electronic Engineering, National University of Ireland, Galway, Ireland","Kar, A., Center for Cognitive, Connected, and Computational Imaging, Department of Electrical and Electronic Engineering, National University of Ireland, Galway, Ireland; Corcoran, P., Center for Cognitive, Connected, and Computational Imaging, Department of Electrical and Electronic Engineering, National University of Ireland, Galway, Ireland","In this paper, a review is presented for the research on eye gaze estimation techniques and applications, which has progressed in diverse ways over the past two decades. Several generic eye gaze use-cases are identified: Desktop, TV, head-mounted, automotive, and handheld devices. Analysis of the literature leads to the identification of several platform specific factors that influence gaze tracking accuracy. A key outcome from this review is the realization of a need to develop standardized methodologies for the performance evaluation of gaze tracking systems and achieve consistency in their specification and comparative evaluation. To address this need, the concept of a methodological framework for practical evaluation of different gaze tracking systems is proposed. © 2013 IEEE.","accuracy; error sources; Eye gaze; gaze estimation; performance evaluation; user platforms","accuracy; Error sources; Eye-gaze; Gaze estimation; performance evaluation; user platforms; Tracking (position)",Review,"Final","",Scopus,2-s2.0-85028939777
"Tamura K., Choi R., Aoki Y.","55375922900;57195573386;35498779700;","Unconstrained and Calibration-Free Gaze Estimation in a Room-Scale Area Using a Monocular Camera",2017,"IEEE Access","6",,,"10896","10908",,5,"10.1109/ACCESS.2017.2734168","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028927970&doi=10.1109%2fACCESS.2017.2734168&partnerID=40&md5=e3862b9dff5a130a54089d3faa93a86b","Graduate School of Integrated Design Engineering, Keio University, Kanagawa, 223-8522, Japan","Tamura, K., Graduate School of Integrated Design Engineering, Keio University, Kanagawa, 223-8522, Japan; Choi, R., Graduate School of Integrated Design Engineering, Keio University, Kanagawa, 223-8522, Japan; Aoki, Y., Graduate School of Integrated Design Engineering, Keio University, Kanagawa, 223-8522, Japan","Gaze estimation using monocular cameras has significant commercial applicability, and many studies have been undertaken on head pose-invariant and calibration-free gaze estimation. The head positions in existing data sets used in these studies are, however, limited to the vicinity of the camera, and methods trained on such data sets are not applicable when subjects are at greater distances from the camera. In this paper, we create a room-scale gaze data set with large variations in head poses to achieve robust gaze estimation across a broader range of widths and depths. The head positions are much farther from the camera, and the resolution of the eye image is lower than in conventional data sets. To address this issue, we propose a likelihood evaluation method based on edge gradients with dense particles for iris tracking, which achieves robust tracking at low-resolution eye images. Cross-validation experiments show that our proposed method is more accurate than conventional methods on all the individuals in our data set. © 2013 IEEE.","Gaze estimation; iris tracking; particle filter; regression","Calibration; Calibration free; Conventional methods; Cross validation; Gaze estimation; Iris tracking; Monocular cameras; Particle filter; Regression; Cameras",Article,"Final","",Scopus,2-s2.0-85028927970
"Huang Q., Veeraraghavan A., Sabharwal A.","57194771951;6506177844;57223450164;","TabletGaze: dataset and analysis for unconstrained appearance-based gaze estimation in mobile tablets",2017,"Machine Vision and Applications","28","5-6",,"445","461",,60,"10.1007/s00138-017-0852-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021953913&doi=10.1007%2fs00138-017-0852-4&partnerID=40&md5=05ae0f1f70ce457e72048aa7807da7ae","ECE Department, Rice University, Houston, TX, United States","Huang, Q., ECE Department, Rice University, Houston, TX, United States; Veeraraghavan, A., ECE Department, Rice University, Houston, TX, United States; Sabharwal, A., ECE Department, Rice University, Houston, TX, United States","We study gaze estimation on tablets; our key design goal is uncalibrated gaze estimation using the front-facing camera during natural use of tablets, where the posture and method of holding the tablet are not constrained. We collected a large unconstrained gaze dataset of tablet users, labeled Rice TabletGaze dataset. The dataset consists of 51 subjects, each with 4 different postures and 35 gaze locations. Subjects vary in race, gender and in their need for prescription glasses, all of which might impact gaze estimation accuracy. We made three major observations on the collected data and employed a baseline algorithm for analyzing the impact of several factors on gaze estimation accuracy. The baseline algorithm is based on multilevel HoG feature and Random Forests regressor, which achieves a mean error of 3.17 cm. We perform extensive evaluation on the impact of various practical factors such as person dependency, dataset size, race, wearing glasses and user posture on the gaze estimation accuracy. © 2017, Springer-Verlag GmbH Germany.","Applications; Dataset; Eye; Gaze estimation/tracking; Mobile device","Applications; Decision trees; Glass; Appearance based; Data set size; Dataset; Design goal; Gaze estimation; Mean errors; Random forests; Uncalibrated; Mobile devices",Article,"Final","",Scopus,2-s2.0-85021953913
"Lanillos P., Ferreira J.F., Dias J.","24076529300;13007113200;56962751600;","A Bayesian hierarchy for robust gaze estimation in human–robot interaction",2017,"International Journal of Approximate Reasoning","87",,,"1","22",,4,"10.1016/j.ijar.2017.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019500472&doi=10.1016%2fj.ijar.2017.04.007&partnerID=40&md5=959d3f7e28504c32ac5c5f7d67c32c73","Institute for Cognitive Systems (ICS), Technische Universität München, Arcisstrasse 21, München, 80333, Germany; AP4ISR team, Institute of Systems and Robotics (ISR), Dept. of Electrical & Computer Eng., University of Coimbra, Pinhal de Marrocos, Pólo II, Coimbra, 3030-290, Portugal; Khalifa University of Science, Technology, and Research, Abu Dhabi, 127788, United Arab Emirates","Lanillos, P., Institute for Cognitive Systems (ICS), Technische Universität München, Arcisstrasse 21, München, 80333, Germany, AP4ISR team, Institute of Systems and Robotics (ISR), Dept. of Electrical & Computer Eng., University of Coimbra, Pinhal de Marrocos, Pólo II, Coimbra, 3030-290, Portugal; Ferreira, J.F., AP4ISR team, Institute of Systems and Robotics (ISR), Dept. of Electrical & Computer Eng., University of Coimbra, Pinhal de Marrocos, Pólo II, Coimbra, 3030-290, Portugal; Dias, J., AP4ISR team, Institute of Systems and Robotics (ISR), Dept. of Electrical & Computer Eng., University of Coimbra, Pinhal de Marrocos, Pólo II, Coimbra, 3030-290, Portugal, Khalifa University of Science, Technology, and Research, Abu Dhabi, 127788, United Arab Emirates","In this text, we present a probabilistic solution for robust gaze estimation in the context of human–robot interaction. Gaze estimation, in the sense of continuously assessing gaze direction of an interlocutor so as to determine his/her focus of visual attention, is important in several important computer vision applications, such as the development of non-intrusive gaze-tracking equipment for psychophysical experiments in neuroscience, specialised telecommunication devices, video surveillance, human–computer interfaces (HCI) and artificial cognitive systems for human–robot interaction (HRI), our application of interest. We have developed a robust solution based on a probabilistic approach that inherently deals with the uncertainty of sensor models, but also and in particular with uncertainty arising from distance, incomplete data and scene dynamics. This solution comprises a hierarchical formulation in the form of a mixture model that loosely follows how geometrical cues provided by facial features are believed to be used by the human perceptual system for gaze estimation. A quantitative analysis of the proposed framework's performance was undertaken through a thorough set of experimental sessions. Results show that the framework performs according to the difficult requirements of HRI applications, namely by exhibiting correctness, robustness and adaptiveness. © 2017 Elsevier Inc.","Bayesian estimation; Gaze estimation; Head pose estimation; HRI; Robustness to distance; Robustness to missing data","Bayesian networks; Behavioral research; Cognitive systems; Human computer interaction; Image recognition; Image segmentation; Robots; Security systems; Tracking (position); Bayesian estimations; Computer vision applications; Gaze estimation; Head Pose Estimation; Missing data; Probabilistic approaches; Psychophysical experiments; Telecommunication devices; Human robot interaction",Article,"Final","",Scopus,2-s2.0-85019500472
"Zhou X., Cai H., Li Y., Liu H.","55743240400;56763253600;8589964900;54958434200;","Two-eye model-based gaze estimation from a Kinect sensor",2017,"Proceedings - IEEE International Conference on Robotics and Automation",,,"7989194","1646","1653",,20,"10.1109/ICRA.2017.7989194","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028021650&doi=10.1109%2fICRA.2017.7989194&partnerID=40&md5=a635e93025c60ae9e98c2eb7ab4e8e7d","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Computing, University of Portsmouth, Portsmouth, United Kingdom; Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Hong Kong, Hong Kong; State Key Laboratory of Mechanical Systems and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China","Zhou, X., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; Cai, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom; Li, Y., Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Hong Kong, Hong Kong; Liu, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom, State Key Laboratory of Mechanical Systems and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China","In this paper, we present an effective and accurate gaze estimation method based on two-eye model of a subject with the tolerance of free head movement from a Kinect sensor. To accurately and efficiently determine the point of gaze, i) we employ two-eye model to improve the estimation accuracy; ii) we propose an improved convolution-based means of gradients method to localize the iris center in 3D space; iii) we present a new personal calibration method that only needs one calibration point. The method approximates the visual axis as a line from the iris center to the gaze point to determine the eyeball centers and the Kappa angles. The final point of gaze can be calculated by using the calibrated personal eye parameters. We experimentally evaluate the proposed gaze estimation method on eleven subjects. Experimental results demonstrate that our gaze estimation method has an average estimation accuracy around 1.99°, which outperforms many leading methods in the state-of-the-art. © 2017 IEEE.",,"Eye movements; Robotics; Calibration method; Calibration points; Eye parameters; Gaze estimation; Gaze point; Kinect sensors; Point of gaze; State of the art; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-85028021650
[无可用作者姓名],[无可用的作者 ID],"2017 3rd IEEE International Conference on Cybernetics, CYBCONF 2017 - Proceedings",2017,"2017 3rd IEEE International Conference on Cybernetics, CYBCONF 2017 - Proceedings",,,,"","",512,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027858616&partnerID=40&md5=b244f573ce45aa160cc4d0a3f179cc73",,"","The proceedings contain 80 papers. The topics discussed include: SCPLBSa smart cooperative platform for load balancing and security on SDN distributed controllers; a novel efficient index model and modified chord protocol for decentralized service repositories; particle swarm optimization based adaptable predictor of glycemia values; robot Pathfinding using vision based obstacle detection; efficient time-to-collision estimation for a braking supervision system with LIDAR; temporal evolution of motion superpixel for video classification; deploying self-organisation to improve task execution in a multi-agent systems; analysis of temporal features in data streams from multiple wearable devices; multi-project scheduling by fuzzy combinatorial auction; decoupling temporal dynamics for naturalistic affect recognition in a two-stage regression framework; building a viable information security management system; optimal setting for coking flue gas denitriation process indices based on PCR-multi-case fusion; towards a new stability criterion for fractional-order perfect control of LTI MIMO discrete-time systems in state-space; personal health indicators by deep learning of smart phone sensor data; vehicle ROI extraction based on area estimation Gaussian mixture model; and gaze modulated disambiguation technique for gesture control in 3D virtual objects selection.",,,Conference Review,"Final","",Scopus,2-s2.0-85027858616
"Liu Y., Lee B.-S., McKeown M.","57192561421;7405441352;7005375626;","A new reconstruction method in gaze estimation with natural head movement",2017,"Proceedings of the 15th IAPR International Conference on Machine Vision Applications, MVA 2017",,,"7986840","219","222",,4,"10.23919/MVA.2017.7986840","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027858530&doi=10.23919%2fMVA.2017.7986840&partnerID=40&md5=f3c1fbff6fc07b3bcf7175c31fd55619","School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Medicine, University of BritishBC, Canada","Liu, Y., School of Computer Science and Engineering, Nanyang Technological University, Singapore; Lee, B.-S., School of Computer Science and Engineering, Nanyang Technological University, Singapore; McKeown, M., Department of Medicine, University of BritishBC, Canada","We present a novel reconstruction method for the appearance-based gaze estimation that allows inferring persons' gaze under natural head movement. We first study that the locally linear combination in the respective manifolds consisting of stable left and right eye appearances is efficient. The local structure of the manifolds is destroyed when there is head movement. This is due to the destruction of the intrinsic relations between the two eyes(left and right) when we do locally linear combinations. We then introduce a new combination of both eye appearances, which maintains the relation embedding into the reconstruction of the training stage. Through comparison with other well known methods, we show that the proposed method achieves an optimal performance with head pose variation. © 2017 MVA Organization All Rights Reserved.",,"Appearance based; Gaze estimation; Intrinsic relation; Linear combinations; Local structure; Optimal performance; Reconstruction method; Relation embedding; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85027858530
"Ogawa T., Nakazawa A., Nishida T.","57195404621;35807510800;35595754400;","Point of gaze estimation using corneal surface reflection and omnidirectional camera image",2017,"Proceedings of the 15th IAPR International Conference on Machine Vision Applications, MVA 2017",,,"7986842","227","230",,1,"10.23919/MVA.2017.7986842","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027843704&doi=10.23919%2fMVA.2017.7986842&partnerID=40&md5=e5ab26253748c27cf33e183250857bfc","Kyoto University, Japan","Ogawa, T., Kyoto University, Japan; Nakazawa, A., Kyoto University, Japan; Nishida, T., Kyoto University, Japan","We present a human point of gaze estimation system using corneal surface reflection and omni-directional image taken by a fish eye. Only capturing an eye image, our system entables to find where a user is looking in 360P surrounding scene image. We first generates multiple perspective scene images from an equi-rectangular image and perform registration between corneal reflection and perspective images. We then compute the point of gaze using a 3D eye model and project the point to an omni-directional image. We evaluated the robustness of registration and accuracy of PoG estimations using two indoor and five outdoor scenes, and found that gaze mapping error was 5.526[deg] on average. This result shows the potential to the marketing and outdoor training system. © 2017 MVA Organization All Rights Reserved.",,"Image segmentation; Corneal reflection; Multiple perspectives; Omni-directional; Omnidirectional cameras; Perspective image; Rectangular image; Surface reflections; Training Systems; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-85027843704
"Wang S., Wang J., Peng H., Gao S., He D.","55768898200;8771439200;55546015500;55497239300;36663601800;","A new calibration-free gaze tracking algorithm based on DE-SLFA",2017,"Proceedings - 2016 8th International Conference on Information Technology in Medicine and Education, ITME 2016",,,"7976505","380","384",,,"10.1109/ITME.2016.0091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027706342&doi=10.1109%2fITME.2016.0091&partnerID=40&md5=65e60487defa8fed92b6263f255e1c2c","School of Telecommunications Engineering, Xidian University, Xi'an, China; School of Mathematics and Statistics, Xidian University, Xi'an, China","Wang, S., School of Telecommunications Engineering, Xidian University, Xi'an, China; Wang, J., School of Telecommunications Engineering, Xidian University, Xi'an, China; Peng, H., School of Telecommunications Engineering, Xidian University, Xi'an, China; Gao, S., School of Mathematics and Statistics, Xidian University, Xi'an, China; He, D., School of Telecommunications Engineering, Xidian University, Xi'an, China","Advanced remote gaze estimation systems use automatic calibration procedure without requiring active user involving into the estimation of subject-specific eye parameters. Though automatic calibration process can simplify the difficulty of calibration task, it still needs time to collect information for completing the eye parameters of users before the gaze tracking system is used. This paper proposes a novel method, free of calibration procedure to extract subject-specific eye parameters. To estimate the real-time angles between the optical and visual axes of each eye before calculating the direction of the visual axes of the both the left and right eyes, differential evolution and Shuffled Frog-leaping Algorithm (DE-SLFA) is used to minimize the distance between the intersections of the visual axes of the left and right eyes with the surface of a display while subjects look naturally at the display. As a consequence, the inconvenient calibration procedure which may produce possible calibration errors can be eliminated. Computer simulation have been performed to confirm the proposed method. © 2016 IEEE.","Calibration-free; Gaze tracking","Calibration; Evolutionary algorithms; Optimization; Automatic calibration; Calibration free; Calibration procedure; Differential Evolution; Gaze tracking; Gaze tracking system; Remote gaze estimation; Shuffled frog leaping algorithm (SFLA); Tracking (position)",Conference Paper,"Final","",Scopus,2-s2.0-85027706342
"Atoum Y., Chen L., Liu A.X., Hsu S.D.H., Liu X.","56487910600;57188665934;8339909200;16197442200;35793096800;","Automated Online Exam Proctoring",2017,"IEEE Transactions on Multimedia","19","7","7828141","1609","1624",,38,"10.1109/TMM.2017.2656064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021323599&doi=10.1109%2fTMM.2017.2656064&partnerID=40&md5=a0a90d9b639bf07a28cec0ddaf1cce57","Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI  48824, United States; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI  48824, United States; Department of Physics and Astronomy, Michigan State University, East Lansing, MI  48824, United States","Atoum, Y., Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI  48824, United States; Chen, L., Department of Computer Science and Engineering, Michigan State University, East Lansing, MI  48824, United States; Liu, A.X., Department of Computer Science and Engineering, Michigan State University, East Lansing, MI  48824, United States; Hsu, S.D.H., Department of Physics and Astronomy, Michigan State University, East Lansing, MI  48824, United States; Liu, X., Department of Computer Science and Engineering, Michigan State University, East Lansing, MI  48824, United States","Massive open online courses and other forms of remote education continue to increase in popularity and reach. The ability to efficiently proctor remote online examinations is an important limiting factor to the scalability of this next stage in education. Presently, human proctoring is the most common approach of evaluation, by either requiring the test taker to visit an examination center, or by monitoring them visually and acoustically during exams via a webcam. However, such methods are labor intensive and costly. In this paper, we present a multimedia analytics system that performs automatic online exam proctoring. The system hardware includes one webcam, one wearcam, and a microphone for the purpose of monitoring the visual and acoustic environment of the testing location. The system includes six basic components that continuously estimate the key behavior cues: user verification, text detection, voice detection, active window detection, gaze estimation, and phone detection. By combining the continuous estimation components, and applying a temporal sliding window, we design higher level features to classify whether the test taker is cheating at any moment during the exam. To evaluate our proposed system, we collect multimedia (audio and visual) data from 24 subjects performing various types of cheating while taking online exams. Extensive experimental results demonstrate the accuracy, robustness, and efficiency of our online exam proctoring system. © 1999-2012 IEEE.","Covariance feature; gaze estimation; online exam proctoring (OEP); phone detection; speech detection; text detection; user verification","Behavioral research; Speech recognition; Telephone sets; Covariance features; Gaze estimation; Online exams; Speech detection; Text detection; User verification; E-learning",Article,"Final","",Scopus,2-s2.0-85021323599
"Kim B.C., Ko D., Jang U., Han H., Lee E.C.","56726972600;56204492400;56728300700;14037180100;14009024200;","3D Gaze tracking by combining eye- and facial-gaze vectors",2017,"Journal of Supercomputing","73","7",,"3038","3052",,1,"10.1007/s11227-016-1817-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978785985&doi=10.1007%2fs11227-016-1817-5&partnerID=40&md5=7b71c750f0b470e5f6524228c93ef07f","Department of Computer Science, Graduate School, Sangmyung University, Seoul, South Korea; Department of Computer Science, Sangmyung University, Seoul, South Korea","Kim, B.C., Department of Computer Science, Graduate School, Sangmyung University, Seoul, South Korea; Ko, D., Department of Computer Science, Graduate School, Sangmyung University, Seoul, South Korea; Jang, U., Department of Computer Science, Graduate School, Sangmyung University, Seoul, South Korea; Han, H., Department of Computer Science, Sangmyung University, Seoul, South Korea; Lee, E.C., Department of Computer Science, Sangmyung University, Seoul, South Korea","We propose a 3D gaze-tracking method that combines accurate 3D eye- and facial-gaze vectors estimated from a Kinect v2 high-definition face model. Using accurate 3D facial and ocular feature positions, gaze positions can be calculated more accurately than with previous methods. Considering the image resolution of the face and eye regions, two gaze vectors are combined as a weighted sum, allocating more weight to facial-gaze vectors. Hence, the facial orientation mainly determines the gaze position, and eye-gaze vectors then perform minor manipulations. The 3D facial-gaze vector is first defined, and the 3D rotational center of the eyeball is then estimated; together, these define the 3D eye-gaze vector. Finally, the intersection point between the 3D gaze vector and the physical display plane is calculated as the gaze position. Experimental results show that the average gaze estimation root-mean-square error was approximately 23 pixels from the desired position at a resolution of 1920 × 1080. © 2016, Springer Science+Business Media New York.","3D gaze tracking; Eye-gaze vector; Facial-gaze vector; HD face model; Kinect v2","Digital television; Image resolution; Mean square error; Tracking (position); Desired position; Eye-gaze; Face modeling; Facial orientations; Gaze tracking; Intersection points; Kinect v2; Root mean square errors; Vectors",Article,"Final","",Scopus,2-s2.0-84978785985
"Wibirama S., Nugroho H.A., Hamamoto K.","26654457700;57210591699;7102699225;","Evaluating 3D gaze tracking in virtual space: A computer graphics approach",2017,"Entertainment Computing","21",,,"11","17",,20,"10.1016/j.entcom.2017.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018286781&doi=10.1016%2fj.entcom.2017.04.003&partnerID=40&md5=d25951f1f76194b6eb2ca447d9a24f0c","Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Department of Information Media Technology, School of Information and Telecommunication Engineering, Tokai UniversityTokyo  108-8619, Japan","Wibirama, S., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Nugroho, H.A., Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia; Hamamoto, K., Department of Information Media Technology, School of Information and Telecommunication Engineering, Tokai UniversityTokyo  108-8619, Japan","Increasing usage of stereoscopic 3D technology in virtual reality, video games, entertainment, and visualization has risen concern on development of gaze-based interaction. To develop intuitive and accurate gaze-based interaction, estimation of 3D gaze in virtual space should be validated experimentally. 3D gaze tracking is generally performed in real space with rigid object as validation target. Thus, researchers in virtual reality are constrained on choosing appropriate evaluation method when 3D gaze tracking has to be performed in virtual space. To fill this research gap, we present design and development of a new evaluation method for 3D gaze tracking in virtual space. We have implemented computer graphics technology to develop virtual plane containing virtual 3D object as validation target. Experimental results show that the proposed evaluation method was able to support real experiment by proving the accuracy of our 3D gaze tracking system with average Euclidean error less than 1 cm (Mean = 0.95 cm; S.D = 0.55 cm) in 74 cm depth of workspace. Compared with evaluation method for 3D gaze tracking in real space, the proposed method even can be implemented when space of experiment room is limited by adjusting the distance of virtual plane programmatically. © 2017 Elsevier B.V.","3D gaze estimation; Eye tracking; Quad-buffer rendering; Stereoscopic 3D; Validation method; Virtual reality","Computer graphics; Stereo image processing; Stereo vision; Target tracking; Tracking (position); Virtual reality; Computer graphics technology; Design and Development; Eye-tracking; Gaze estimation; Gaze-based interaction; Quad-buffer rendering; Stereoscopic 3-d technologies; Validation method; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-85018286781
"El Hafi L., Ding M., Takamatsu J., Ogasawara T.","57188836029;39461142600;35243684200;7201579979;","Gaze tracking and object recognition from eye images",2017,"Proceedings - 2017 1st IEEE International Conference on Robotic Computing, IRC 2017",,,"7926555","310","315",,4,"10.1109/IRC.2017.44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020199058&doi=10.1109%2fIRC.2017.44&partnerID=40&md5=39c4bd663553cd6f2f37fd7d950031be","Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan","El Hafi, L., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Ding, M., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Takamatsu, J., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Ogasawara, T., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan","This paper introduces a method to identify the focused object in eye images captured from a single camera in order to enable intuitive eye-based interactions using wearable devices. Indeed, eye images allow to not only obtain natural user responses from eye movements, but also the scene reflected on the cornea without the need for additional sensors such as a frontal camera, thus making it more socially acceptable. The proposed method relies on a 3D eye model reconstruction to evaluate the gaze direction from the eye images. The gaze direction is then used in combination with deep learning algorithms to classify the focused object reflected on the cornea. Finally, the experimental results using a wearable prototype demonstrate the potential of the proposed method solely based on eye images captured from a single camera. © 2017 IEEE.","Corneal image; Eye model; Gaze tracking; Object recognition; Wearable device","Cameras; Object recognition; Robotics; Three dimensional computer graphics; Tracking (position); Wearable technology; Corneal images; Eye model; Focused object; Gaze direction; Gaze tracking; Single cameras; Wearable devices; Wearable prototypes; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85020199058
"Durna Y., Ari F.","55293417600;9279640600;","Design of a binocular pupil and gaze point detection system utilizing high deﬁnition images",2017,"Applied Sciences (Switzerland)","7","5","498","","",,7,"10.3390/app7050498","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019752104&doi=10.3390%2fapp7050498&partnerID=40&md5=c523b3c27a761e656260c3c2a07afed7","Department of Electrical and Electronics Engineering, Ankara University, Ankara, 06830, Turkey","Durna, Y., Department of Electrical and Electronics Engineering, Ankara University, Ankara, 06830, Turkey; Ari, F., Department of Electrical and Electronics Engineering, Ankara University, Ankara, 06830, Turkey","This study proposes a novel binocular pupil and gaze detection system utilizing a remote full high deﬁnition (full HD) camera and employing LabVIEW. LabVIEW is inherently parallel and has fewer time-consuming algorithms. Many eye tracker applications are monocular and use low resolution cameras due to real-time image processing difﬁculties. We utilized the computer's direct access memory channel for rapid data transmission and processed full HD images with LabVIEW. Full HD images make easier determinations of center coordinates/sizes of pupil and corneal reﬂection. We modiﬁed the camera so that the camera sensor passed only infrared (IR) images. Glints were taken as reference points for region of interest (ROI) area selection of the eye region in the face image. A morphologic ﬁlter was applied for erosion of noise, and a weighted average technique was used for center detection. To test system accuracy with 11 participants, we produced a visual stimulus set up to analyze each eye's movement. Nonlinear mapping function was utilized for gaze estimation. Pupil size, pupil position, glint position and gaze point coordinates were obtained with free natural head movements in our system. This system also works at 2046 × 1086 resolution at 40 frames per second. It is assumed that 280 frames per second for 640 × 480 pixel images is the case. Experimental results show that the average gaze detection error for 11 participants was 0.76◦ for the left eye, 0.89◦ for right eye and 0.83◦ for the mean of two eyes. © 2017 by the authors.","Binocular pupil detection; Eye tracker; Gaze point; Labview",,Article,"Final","",Scopus,2-s2.0-85019752104
"Santini T., Fuhl W., Kasneci E.","54881866000;56770084800;56059892600;","CalibMe: Fast and unsupervised eye tracker calibration for gaze-based pervasive human-computer interaction",2017,"Conference on Human Factors in Computing Systems - Proceedings","2017-May",,,"2594","2605",,31,"10.1145/3025453.3025950","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044849159&doi=10.1145%2f3025453.3025950&partnerID=40&md5=414cf736a6d684dabacf56d05b5d7b7d","University of Tübingen, Tübingen, Germany","Santini, T., University of Tübingen, Tübingen, Germany; Fuhl, W., University of Tübingen, Tübingen, Germany; Kasneci, E., University of Tübingen, Tübingen, Germany","As devices around us become smart, our gaze is poised to become the next frontier of human-computer interaction (HCI). State-of-the-art mobile eye tracker systems typically rely on eye-model-based gaze estimation approaches, which do not require a calibration. However, such approaches require specialized hardware (e.g., multiple cameras and glint points), can be significantly affected by glasses, and, thus, are not fit for ubiquitous gaze-based HCI. In contrast, regression-based gaze estimations are straightforward approaches requiring solely one eye and one scene camera but necessitate a calibration. Therefore, a fast and accurate calibration is a key development to enable ubiquitous gaze-based HCI. In this paper, we introduce CalibMe, a novel method that exploits collection markers (automatically detected fiducial markers) to allow eye tracker users to gather a large array of calibration points, remove outliers, and automatically reserve evaluation points in a fast and unsupervised manner. The proposed approach is evaluated against a nine-point calibration method, which is typically used due to its relatively short calibration time and adequate accuracy. CalibMe reached a mean angular error of 0.59° (σ = 0.23°) in contrast to 0.82° (σ = 0.15°) for a nine-point calibration, attesting for the efficacy of the method. Moreover, users are able to calibrate the eye tracker anywhere and independently in ≈ 10s using a cellphone to display the collection marker. © 2017 ACM.","Calibration; Eye tracking; Fiducial markers; Gaze-based interaction; Usability","Calibration; Cameras; Human computer interaction; Human engineering; Stereo vision; Calibration method; Calibration points; Fiducial marker; Gaze-based interaction; Human Computer Interaction (HCI); Specialized hardware; State of the art; Usability; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85044849159
"Huang M.X., Li J., Ngai G., Va Leong H.","55258532000;56433022500;8915594400;57202035409;","ScreenGlint: Practical, in-situ gaze estimation on smartphones",2017,"Conference on Human Factors in Computing Systems - Proceedings","2017-May",,,"2546","2557",,22,"10.1145/3025453.3025794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041481290&doi=10.1145%2f3025453.3025794&partnerID=40&md5=d916b025aad3cc34c3f0eceab3c64b42","CHI Lab., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong","Huang, M.X., CHI Lab., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Li, J., CHI Lab., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Ngai, G., CHI Lab., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Va Leong, H., CHI Lab., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong","Gaze estimation has widespread applications. However, little work has explored gaze estimation on smartphones, even though they are fast becoming ubiquitous. This paper presents ScreenGlint, a novel approach which exploits the glint (reflection) of the screen on the user's cornea for gaze estimation, using only the image captured by the front-facing camera. We first conduct a user study on common postures during smartphone use. We then design an experiment to evaluate the accuracy of ScreenGlint under varying face-to-screen distances. An in-depth evaluation involving multiple users is conducted and the impact of head pose variations is investigated. ScreenGlint achieves an overall angular error of 2.44° without head pose variations, and 2.94° with head pose variations. Our technique compares favorably to state-of-the-art research works, indicating that the glint of the screen is an effective and practical cue to gaze estimation on the smartphone platform. We believe that this work can open up new possibilities for practical and ubiquitous gaze-aware applications. © 2017 ACM.","Gaze estimation; Glint; Mobile eye tracker; Screen reflection","Human engineering; Smartphones; Angular errors; Depth evaluations; Eye trackers; Gaze estimation; Glint; Multiple user; State of the art; User study; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85041481290
"Fridman L., Toyoda H., Seaman S., Seppelt B., Angell L., Lee J., Mehler B., Reimer B.","57185179400;57190621577;57194965731;15766218300;36907040600;57189577286;57225324023;7003475727;","What can be predicted from six seconds of driver glances?",2017,"Conference on Human Factors in Computing Systems - Proceedings","2017-May",,,"2805","2813",,18,"10.1145/3025453.3025929","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041138136&doi=10.1145%2f3025453.3025929&partnerID=40&md5=70b46eb52b4a60e21a0b36e6dfa1a9d4","Massachusetts Institute of Technology, United States; Toyota Collaborative Safety Research Center, Japan; Touchstone Evaluations, United States","Fridman, L., Massachusetts Institute of Technology, United States; Toyoda, H., Toyota Collaborative Safety Research Center, Japan; Seaman, S., Touchstone Evaluations, United States; Seppelt, B., Touchstone Evaluations, United States; Angell, L., Touchstone Evaluations, United States; Lee, J., Massachusetts Institute of Technology, United States; Mehler, B., Massachusetts Institute of Technology, United States; Reimer, B., Massachusetts Institute of Technology, United States","We consider a large dataset of real-world, on-road driving from a 100-car naturalistic study to explore the predictive power of driver glances and, specifically, to answer the following question: what can be predicted about the state of the driver and the state of the driving environment from a 6-second sequence of macro-glances? The context-based nature of such glances allows for application of supervised learning to the problem of vision-based gaze estimation, making it robust, accurate, and reliable in messy, real-world conditions. So, it's valuable to ask whether such macro-glances can be used to infer behavioral, environmental, and demographic variables? We analyze 27 binary classification problems based on these variables. The takeaway is that glance can be used as part of a multi-sensor real-time system to predict radio-tuning, fatigue state, failure to signal, talking, and several environment variables. © 2017 ACM.","Driver state prediction; Gaze patterns; Hidden Markov models; Naturalistic on-road study","Automobile drivers; Hidden Markov models; Human engineering; Interactive computer systems; Real time systems; Roads and streets; Binary classification problems; Demographic variables; Driving environment; Gaze patterns; Naturalistic study; On-road studies; Predictive power; State prediction; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85041138136
"Yagi Y., Mitsugami I., Shioiri S., Habe H.","55740719200;13106192400;7005878104;22234161800;","Behavior understanding based on intention-gait model",2017,"Human-Harmonized Information Technology","2",,,"139","172",,1,"10.1007/978-4-431-56535-2_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034962855&doi=10.1007%2f978-4-431-56535-2_5&partnerID=40&md5=6c3f1b139dec213b7b6cca836fdf0c68","Osaka University, Osaka, Japan; Tohoku University, Miyagi, Japan; Kindai University, Osaka, Japan","Yagi, Y., Osaka University, Osaka, Japan; Mitsugami, I., Osaka University, Osaka, Japan; Shioiri, S., Tohoku University, Miyagi, Japan; Habe, H., Kindai University, Osaka, Japan","Gait is known as one of biometrics, and there have been many studies on gait authentication. In those studies, it is implicitly assumed that the gait of a certain person is always constant. It is, however, untrue in reality; a person usually walks differently according to their mood and physical/mental conditions, which we call ""inertial states.""Motivated by this fact, we organized the research project ""Behavior Understanding based on Intention-Gait Model"", whichwas supported by JST-CREST from 2010 to 2017. The goal of this project was to map ""gait"", in the broad sense of the term, to inertial states such as attention, social factors, and cognitive ability. In this chapter, we provide an overview of the three kinds of estimation technologies considered in this project: attention, social factors, and cognitive ability. © Springer Japan KK 2017. All rights reserved.","Cognitive level estimation; Dementia diagnosis; Dual-task; Eye-head coordination; Gait analysis; Gaze estimation; Group segmentation; Huge data collection; Visual perception","Behavioral research; Cognitive levels; Data collection; Dual-tasks; Eye-head coordination; Gaze estimation; Visual perception; Gait analysis",Book Chapter,"Final","",Scopus,2-s2.0-85034962855
"Rattarom S., Aunsri N., Uttama S.","25031802300;55764124300;14631163100;","A framework for polynomial model with head pose in low cost gaze estimation",2017,"2nd Joint International Conference on Digital Arts, Media and Technology 2017: Digital Economy for Sustainable Growth, ICDAMT 2017",,,"7904927","24","27",,1,"10.1109/ICDAMT.2017.7904927","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019254185&doi=10.1109%2fICDAMT.2017.7904927&partnerID=40&md5=decae4818ca8f8f17a02da1f13746281","School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand","Rattarom, S., School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand; Aunsri, N., School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand; Uttama, S., School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand","This paper presents the framework for low-cost gaze estimation system to find the polynomial model used in with head pose based system. This framework uses three features - center of pupil, glint, and inner eye corner to create the polynomial model. The pupil-glint vector used to estimate eye gaze while glint-inner eye corner vector used to estimate head pose and adding inner eye corner to the model could improve the accuracy and robustness of the system. The framework consists of five steps ranging from data acquisition to testing the accuracy steps. This process introduces the validation method in order to prove the efficiency of the model. © 2017 IEEE.","gaze tracking; head pose estimation; interpolation based gaze estimation; mapping function; model validation","Data acquisition; Image recognition; Network function virtualization; Polynomials; Tracking (position); Gaze estimation; Gaze tracking; Head Pose Estimation; Mapping functions; Model validation; Cost estimating",Conference Paper,"Final","",Scopus,2-s2.0-85019254185
[无可用作者姓名],[无可用的作者 ID],"2nd Joint International Conference on Digital Arts, Media and Technology 2017: Digital Economy for Sustainable Growth, ICDAMT 2017",2017,"2nd Joint International Conference on Digital Arts, Media and Technology 2017: Digital Economy for Sustainable Growth, ICDAMT 2017",,,,"","",485,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019238867&partnerID=40&md5=df1f9976f3009c7af48d7c9e41bb34b4",,"","The proceedings contain 91 papers. The topics discussed include: estimation of electrical conductivity and pH in hydroponic nutrient mixing system using linear regression algorithm; musical chords transposer for captured image based on optical character recognition; a software based method for improving accuracy of ultrasonic range finder module; automated microaneurysms detection in fundus images using image segmentation; a framework for polynomial model with head pose in low cost gaze estimation; sea surface temperature estimation from satellite observations and in-situ measurements using multifidelity Gaussian process regression; and realization of switched-beam metamaterial-inspired microstrip antenna.",,,Conference Review,"Final","",Scopus,2-s2.0-85019238867
"Tu P., Chang M.-C., Gao T.","8973223500;8321670000;57214632977;","Crowd analytics via one shot learning and agent based inference",2017,"2016 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2016 - Proceedings",,,"7906028","1181","1185",,,"10.1109/GlobalSIP.2016.7906028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019203487&doi=10.1109%2fGlobalSIP.2016.7906028&partnerID=40&md5=bd005d6fe46c9f4a718874d10fd56381","GE Global Research, United States","Tu, P., GE Global Research, United States; Chang, M.-C., GE Global Research, United States; Gao, T., GE Global Research, United States","For the purposes of inferring social behavior in crowded conditions, three concepts have been explored: 1) the GE Sherlock system which makes use of computer vision algorithms for the purposes of the opportunistic capture of various of social cues, 2) a one shot learning paradigm where behaviors can be identified based on as few as a single example and 3) an agent based approach to inference where generative models become the basis for social behavior recognition. The Sherlock system makes use of tracking, facial analysis, gaze estimation and upper body motion analysis. The one-shot learning paradigm makes use of semantically meaningful affects as descriptors. The agent based inference methods allows for the incorporation of cognitive models as a basis for inference. © 2016 IEEE.","Agent; Crowds; Expression; Inference; Learning; Tracking","Agents; Inference engines; Surface discharges; Agent-based approach; Computer vision algorithms; Crowds; Expression; Inference; Inference methods; Learning; One-shot learning; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-85019203487
"Nakazawa A., Kato H., Nitschke C., Nishida T.","35807510800;57193085205;36947002100;35595754400;","Eye gaze tracking using corneal imaging and active illumination devices",2017,"Advanced Robotics","31","8",,"413","427",,3,"10.1080/01691864.2016.1277552","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010685410&doi=10.1080%2f01691864.2016.1277552&partnerID=40&md5=53d35482ad6f4510ffa809c26b20e569","Graduate School of Informatics, Kyoto University, Kyoto, Japan","Nakazawa, A., Graduate School of Informatics, Kyoto University, Kyoto, Japan; Kato, H., Graduate School of Informatics, Kyoto University, Kyoto, Japan; Nitschke, C., Graduate School of Informatics, Kyoto University, Kyoto, Japan; Nishida, T., Graduate School of Informatics, Kyoto University, Kyoto, Japan","This paper shows a novel eye gaze tracking (EGT) technique using the corneal imaging technique. Compared to the existing pupil center and pupil reflection techniques, our approach directly finds PoG in the reflected scene image at the human corneal surface. As a result, it does not suffer from the parallax issue and does not require per-setup system calibrations. To achieve this system, we develop following techniques: First, we use the idea of the gaze-reflection point (GRP), where light from the PoG in the scene reflects at the corneal surface into an eye image. Second, illuminating the whole scene or particular objects using coded structured light enables robust and accurate matching at the GRP to obtain the PoG in a scene image. For this purpose, we show two implementations: a special high-power IR LED-array projector and active LED markers. Experimental evaluation shows that the proposed scheme achieves considerable accuracy and successfully supports depth-varying environments as well as practical applications including observation in a conversation scene. We believe the proposed EGT technique has considerably large potential to solvemajor issues of current EGT systems, expands the application fields of the EGT and increases the usability of interactive systems. © 2017 Taylor & Francis and The Robotics Society of Japan.","corneal imaging; corneal reflection; eye gaze tracking; Point of gaze estimation","Geometrical optics; Gesture recognition; Imaging techniques; Tracking (position); Active illumination; Coded structured light; Corneal reflection; Experimental evaluation; Eye gaze tracking; Interactive system; Point of gaze; System calibration; Light emitting diodes",Article,"Final","",Scopus,2-s2.0-85010685410
"Lu F., Chen X., Sato Y.","54956194300;13410318100;35230954300;","Appearance-based gaze estimation via uncalibrated gaze pattern recovery",2017,"IEEE Transactions on Image Processing","26","4","7833091","1543","1553",,16,"10.1109/TIP.2017.2657880","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015728206&doi=10.1109%2fTIP.2017.2657880&partnerID=40&md5=16cea1ea9da7fe9964809b6957bdcaf9","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; Institute of Industrial Science, University of Tokyo, Tokyo, Japan","Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China, International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; Chen, X., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; Sato, Y., Institute of Industrial Science, University of Tokyo, Tokyo, Japan","Aiming at reducing the restrictions due to person/scene dependence, we deliver a novel method that solves appearance-based gaze estimation in a novel fashion. First, we introduce and solve an 'uncalibrated gaze pattern' solely from eye images independent of the person and scene. The gaze pattern recovers gaze movements up to only scaling and translation ambiguities, via nonlinear dimension reduction and pixel motion analysis, while no training/calibration is needed. This is new in the literature and enables novel applications. Second, our method allows simple calibrations to align the gaze pattern to any gaze target. This is much simpler than conventional calibrations which rely on sufficient training data to compute person and scene-specific nonlinear gaze mappings. Through various evaluations, we show that: 1) the proposed uncalibrated gaze pattern has novel and broad capabilities; 2) the proposed calibration is simple and efficient, and can be even omitted in some scenarios; and 3) quantitative evaluations produce promising results under various conditions. © 1992-2012 IEEE.","dimension reduction; Gaze estimation; uncalibrated gaze pattern","Image processing; Mathematical models; Appearance based; Dimension reduction; Gaze estimation; Nonlinear dimension; Novel applications; Quantitative evaluation; Translation ambiguities; Uncalibrated; Calibration; algorithm; anatomy and histology; automated pattern recognition; eye; eye fixation; human; image processing; physiology; procedures; Algorithms; Eye; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated",Article,"Final","",Scopus,2-s2.0-85015728206
"Yafei W., Zhao T., Xueyan D., Bian J., Fu X.","55211773900;57192707963;57193958369;57200854878;7402204912;","Head pose-free eye gaze prediction for driver attention study",2017,"2017 IEEE International Conference on Big Data and Smart Computing, BigComp 2017",,,"7881713","42","46",,10,"10.1109/BIGCOMP.2017.7881713","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017628051&doi=10.1109%2fBIGCOMP.2017.7881713&partnerID=40&md5=a2387e8e2b90e3d95f07d77bd538bfdc","School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Yafei, W., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China, Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Zhao, T., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Xueyan, D., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Bian, J., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Driver's gaze direction is an indicator of driver state and plays a significantly role in driving safety. Traditional gaze zone estimation methods based on eye model have disadvantages due to the vulnerability under large head movement. Different from these methods, an appearance-based head pose-free eye gaze prediction method is proposed in this paper, for driver gaze zone estimation under free head movement. To achieve this goal, a gaze zone classifier is trained with head vectors and eye image features by random forest. The head vector is calculated by Pose from Orthography and Scaling with ITerations (POSIT) where a 3D face model is combined with facial landmark detection. And the eye image features are derived from eye images which extracted through eye region localization. These features are presented as the combination of sparse coefficients by sparse encoding with eye image dictionary, having good potential to carry information of the eye images. Experimental results show that the proposed method is applicable in real driving environment. © 2017 IEEE.","Dictionary learning; Driver state; Gaze zone; Head pose-free; Random forest","Automobile drivers; Big data; Decision trees; Image coding; 3-D face modeling; Dictionary learning; Driver state; Estimation methods; Eye-gaze predictions; Facial landmark detection; Head pose; Random forests; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85017628051
"Iyer S.D., Ramasangu H.","57193645627;56038588200;","Hybrid LASSO and Neural Network estimator for gaze estimation",2017,"IEEE Region 10 Annual International Conference, Proceedings/TENCON",,,"7848503","2579","2582",,2,"10.1109/TENCON.2016.7848503","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015435788&doi=10.1109%2fTENCON.2016.7848503&partnerID=40&md5=ba11c460ba660b09a749a5d17b07fc3d","Department of Electronic and Communication Engineering, MSRUAS, Bengaluru, Karnataka, 560058, India","Iyer, S.D., Department of Electronic and Communication Engineering, MSRUAS, Bengaluru, Karnataka, 560058, India; Ramasangu, H., Department of Electronic and Communication Engineering, MSRUAS, Bengaluru, Karnataka, 560058, India","Gaze estimation has wide applications in drowsiness detection, security, and biomedical domains. The challenges in estimating the gaze angle include varying light conditions and subtle movements of the gaze. The Convolutional Neural Network (CNN) has recently been suggested as a potential method for gaze estimation. In this present work, we have proposed a gaze estimator combining a neural network and Least Absolute Shrinkage and Selection Operator (LASSO). The features considered are both eye and head features. The combined estimator neural network-LASSO (NN-LASSO) outperforms the individual performance of neural network and LASSO estimator. The results are validated using MPII Gaze dataset and it has been shown that the proposed NN-LASSO estimator outperforms CNN in mean error sense. © 2016 IEEE.",,"Neural networks; Biomedical domain; Combined estimators; Convolutional neural network; Drowsiness detection; Individual performance; Least absolute shrinkage and selection operators; Neural network estimators; Potential methods; Estimation",Conference Paper,"Final","",Scopus,2-s2.0-85015435788
"Mahmoudi S.A.","55410455500;","Towards a smart selection of resources for low-energy multimedia processing",2017,"Proceedings of 2016 International Conference on Cloud Computing Technologies and Applications, CloudTech 2016",,,"7847705","238","245",,1,"10.1109/CloudTech.2016.7847705","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013854325&doi=10.1109%2fCloudTech.2016.7847705&partnerID=40&md5=0a2f56a8cc00b19ae689e79709c2ef8d","Faculty of Engineering, University of Mons, Computer Science Department, 20, Place du Parc., Belgium","Mahmoudi, S.A., Faculty of Engineering, University of Mons, Computer Science Department, 20, Place du Parc., Belgium","Today, one can find images and videos everywhere, they can come from cameras, mobile phones or from other devices. These images and videos are used to illustrate different objects in a large number of situations (airports, hospitals, public areas, sport events, etc.). This makes the task of processing images and videos a necessary tool that can be used for various domains related to computer vision. The performance of these algorithms have been so reduced due to their high intensive computation and energy consumption. In this work, we propose a new framework that allows users to select in a smart and efficient way the computing units (CPU or/and GPU) in case of processing single image, multiple images, multiple videos or single video in real time. This framework enables to affect the CPU or/and GPU units for calculation depending on the type of media to process and the algorithm complexity. The framework provides several image and video functions on GPU, such as silhouette extraction, points of interest extraction, edges detection, sparse and dense optical flow estimation. These functions are exploited in different applications such as vertebra segmentation in X-ray and MR images, gaze estimation, event detection and localization in real time. Experimental results have been obtained by applying the framework for different use case applications showing a speedup ranging from 5 to 116×, by comparison with sequential CPU implementations. In addition to these performance, the parallel and heterogeneous implementation offered lower power consumption as a result for the fast treatment. © 2016 IEEE.","GPU; Heterogeneous architectures; Image and video processing; Medical imaging; Motion tracking","Cloud computing; Computational complexity; Energy utilization; Extraction; Global system for mobile communications; Graphics processing unit; Image segmentation; Magnetic resonance imaging; Medical imaging; mHealth; Multimedia systems; Video signal processing; Algorithm complexity; Dense optical flow; Heterogeneous architectures; Image and video processing; Lower-power consumption; Motion tracking; Multimedia processing; Silhouette extraction; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85013854325
"Sakurai K., Yan M., Tamura H., Tanno K.","56909413100;55515780300;35600682500;7103288835;","Comparison of two techniques for gaze estimation system using the direction of eyes and head",2017,"2016 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2016 - Conference Proceedings",,,"7844609","2466","2471",,1,"10.1109/SMC.2016.7844609","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015774745&doi=10.1109%2fSMC.2016.7844609&partnerID=40&md5=6e8f30ebc6faf216064f08bb38818e08","Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazaki, Japan; Faculty of Engineering, University of Miyazaki, Miyazaki, Japan","Sakurai, K., Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazaki, Japan; Yan, M., Faculty of Engineering, University of Miyazaki, Miyazaki, Japan; Tamura, H., Faculty of Engineering, University of Miyazaki, Miyazaki, Japan; Tanno, K., Faculty of Engineering, University of Miyazaki, Miyazaki, Japan","Binocular eye-gaze tracking can be used to estimate gaze of a subject in space using the vergence of the eyes. We analyze eye movement and head movement. As for the eye movement, we used the TalkEye which is eye tracking device. The head angle determined by template matching using a camera attached to TalkEye. A purpose of this study is to compare the proposal method using TalkEye with the conventional method to perform a high-precision gaze estimate. Conventional method is a method of using the Electrooculogram (EOG) signal and RGB-D sensor. Experiments were performed in both indoor and outdoor. The subjects could move eyes and a head freely in wide space. From the experiments, we show the result of 30 degrees, 60 degrees, -30 degrees, and -60 degrees gaze estimation is possible with high precision. In addition, we compared proposal technique and the conventional technique using a regression analysis and considered the effectiveness. © 2016 IEEE.","Electrooculogram Signal; Eye Tracker; Gaze Estimation; RGB-D Sensor; Template Matching","Cybernetics; Regression analysis; Template matching; Tracking (position); Conventional methods; Conventional techniques; Electro-oculogram; Eye gaze tracking; Eye trackers; Eye tracking devices; Gaze estimation; Rgb-d sensors; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85015774745
"Kar A., Corcoran P.","56956378200;57190839462;","Towards the development of a standardized performance evaluation framework for eye gaze estimation systems in consumer platforms",2017,"2016 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2016 - Conference Proceedings",,,"7844543","2061","2066",,5,"10.1109/SMC.2016.7844543","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015761809&doi=10.1109%2fSMC.2016.7844543&partnerID=40&md5=ac80bb1036883f63fd18069a44d7df0b","Center for Cognitive, Connected and Computational Imaging, College of Engineering and Informatics, NUI Galway, Galway, Ireland","Kar, A., Center for Cognitive, Connected and Computational Imaging, College of Engineering and Informatics, NUI Galway, Galway, Ireland; Corcoran, P., Center for Cognitive, Connected and Computational Imaging, College of Engineering and Informatics, NUI Galway, Galway, Ireland","There is a need to standardize the performance of eye gaze estimation (EGE) methods in various platforms for human computer interaction (HCI). Because of lack of consistent schemes or protocols for summative evaluation of EGE systems, performance results in this field can neither be compared nor reproduced with any consistency. In contemporary literature, gaze tracking accuracy is measured under non-identical sets of conditions, with variable metrics and most results do not report the impact of system meta-parameters that significantly affect tracking performances. In this work, the diverse nature of these research outcomes and system parameters which affect gaze tracking in different platforms is investigated and their error contributions are estimated quantitatively. Then the concept and development of a performance evaluation framework is proposed- that can define design criteria and benchmark quality measures for the eye gaze research community. © 2016 IEEE.",,"Benchmarking; Cybernetics; Human computer interaction; Tracking (position); Design criteria; Human computer interaction (HCI); Meta-parameters; Performance evaluation frameworks; Quality measures; Research communities; Research outcome; Tracking performance; Quality control",Conference Paper,"Final","",Scopus,2-s2.0-85015761809
"Gomez S.R., Jianu R., Cabeen R., Guo H., Laidlaw D.H.","36650503400;55921296000;36444979200;55415876300;56652601600;","Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks",2017,"IEEE Transactions on Visualization and Computer Graphics","23","2","7414495","1042","1055",,6,"10.1109/TVCG.2016.2532331","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009135121&doi=10.1109%2fTVCG.2016.2532331&partnerID=40&md5=d80c81b55e886084677dadc6bc89b8b9","Department of Computer Science, Brown University, Providence, RI  02912, United States; School of Computing and Information Sciences, Florida International University, Miami, FL  33199, United States","Gomez, S.R., Department of Computer Science, Brown University, Providence, RI  02912, United States; Jianu, R., School of Computing and Information Sciences, Florida International University, Miami, FL  33199, United States; Cabeen, R., Department of Computer Science, Brown University, Providence, RI  02912, United States; Guo, H., Department of Computer Science, Brown University, Providence, RI  02912, United States; Laidlaw, D.H., Department of Computer Science, Brown University, Providence, RI  02912, United States","We present the design and evaluation of a method for estimating gaze locations during the analysis of static visualizations using crowdsourcing. Understanding gaze patterns is helpful for evaluating visualizations and user behaviors, but traditional eye-tracking studies require specialized hardware and local users. To avoid these constraints, we developed a method called Fauxvea, which crowdsources visualization tasks on the Web and estimates gaze fixations through cursor interactions without eye-tracking hardware. We ran experiments to evaluate how gaze estimates from our method compare with eye-tracking data. First, we evaluated crowdsourced estimates for three common types of information visualizations and basic visualization tasks using Amazon Mechanical Turk (MTurk). In another, we reproduced findings from a previous eye-tracking study on tree layouts using our method on MTurk. Results from these experiments show that fixation estimates using Fauxvea are qualitatively and quantitatively similar to eye tracking on the same stimulus-task pairs. These findings suggest that crowdsourcing visual analysis tasks with static information visualizations could be a viable alternative to traditional eye-tracking studies for visualization research and design. © 2016 IEEE.","crowdsourcing; Eye tracking; focus window; information visualization; user studies; visual analysis","Crowdsourcing; Eye movements; Hardware; Information analysis; Information science; Information systems; Visualization; Eye-tracking; Focus window; Information visualization; User study; Visual analysis; Behavioral research; adult; attention; crowdsourcing; eye fixation; female; human; Internet; male; oculography; physiology; procedures; task performance; Adult; Attention; Crowdsourcing; Eye Movement Measurements; Female; Fixation, Ocular; Humans; Internet; Male; Task Performance and Analysis",Article,"Final","",Scopus,2-s2.0-85009135121
"Han S.Y., Hwang I., Lee S.H., Cho N.I.","57193417343;57207374642;36063845300;7201718669;","Gaze estimation using 3-D eyeball model and eyelid shapes",2017,"2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA 2016",,,"7820784","","",,2,"10.1109/APSIPA.2016.7820784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013818055&doi=10.1109%2fAPSIPA.2016.7820784&partnerID=40&md5=4d9846b2050d060e0c2573a5bfa80001","Institute of New Media and Communications, Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea","Han, S.Y., Institute of New Media and Communications, Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea; Hwang, I., Institute of New Media and Communications, Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea; Lee, S.H., Institute of New Media and Communications, Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea; Cho, N.I., Institute of New Media and Communications, Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea","This paper proposes a gaze estimation algorithm using 3-D eyeball model and eyelid shape. The gaze estimation suffers from differences of eye shapes and individual behaviors, and requires user-specific gaze calibration. The proposed method exploits the usual 3-D eyeball model and shapes of the eyelid to estimate gaze without user-specific calibration and learning. Since the gaze is closely related to the 3-D rotation of eyeball, this paper first derives the relation between 2-D pupil location extracted in the eye image and 3-D rotation of eyeball. This paper also models the shapes of the eyelid to adjust gaze based on the observation that the shapes of the eyelid are deformed with respect to the gaze. This paper models the curvature of eyelid curve to compensate for the gaze. According to the various experiments, the proposed method shows good results in gaze estimation. The proposed method does not need user-specific calibration or gaze learning since the general 3-D eyeball and eyelid models are exploited in the localized eye region. Therefore, it is expected that the proposed gaze estimation algorithm is suitable for various applications such as VR/AR devices, driver gaze tracking, gaze-based interfaces, and so on. © 2016 Asia Pacific Signal and Information Processing Association.",,"Calibration; Tracking (position); Eye images; Eye regions; Gaze estimation; Gaze tracking; Individual behavior; Paper models; Pupil locations; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85013818055
"Soliman M., Tavakoli H.R., Laaksonen J.","57193344768;55765000935;56253107200;","Towards gaze-based video annotation",2017,"2016 6th International Conference on Image Processing Theory, Tools and Applications, IPTA 2016",,,"7821028","","",,3,"10.1109/IPTA.2016.7821028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013170705&doi=10.1109%2fIPTA.2016.7821028&partnerID=40&md5=176b65c1aa781f4c3559a31a7e19dd00","Aalto University, Finland","Soliman, M., Aalto University, Finland; Tavakoli, H.R., Aalto University, Finland; Laaksonen, J., Aalto University, Finland","This paper presents our efforts towards a framework for video annotation using gaze. In computer vision, video annotation (VA) is an essential step in providing a ground truth for the evaluation of object detection and tracking techniques. VA is a demanding element in the development of video processing algorithms, where each object of interest should be manually labelled. Although the community has handled VA for a long time, the size of new data sets and the complexity of the new tasks pushes us to revisit it. A barrier towards automated video annotation is the recognition of the object of interest and tracking it over image sequences. To tackle this problem, we employ the concept of visual attention for enhancing video annotation. In an image, human attention naturally grasps interesting areas that provide valuable information for extracting the objects of interest, which can be exploited to annotate videos. Under task-based gaze recording, we utilize an observer's gaze to filter seed object detector responses in a video sequence. The filtered boxes are then passed to an appearance-based tracking algorithm. We evaluate the gaze usefulness by comparing the algorithm with gaze and without it. We show that eye gaze is an influential cue for enhancing the automated video annotation, improving the annotation significantly. © 2016 IEEE.","Eye gaze; Object detection and tracking; Video annotation; Video processing; Visual attenstion","Behavioral research; Image processing; Object detection; Object recognition; Tracking (position); Eye-gaze; Object detection and tracking; Video annotations; Video processing; Visual attenstion; Video signal processing",Conference Paper,"Final","",Scopus,2-s2.0-85013170705
"Divya V., Amudha J., Jyotsna C.","57217409211;35766448700;57193578958;","Developing an application using eye tracker",2017,"2016 IEEE International Conference on Recent Trends in Electronics, Information and Communication Technology, RTEICT 2016 - Proceedings",,,"7808086","1518","1522",,19,"10.1109/RTEICT.2016.7808086","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015058250&doi=10.1109%2fRTEICT.2016.7808086&partnerID=40&md5=b9133c0e375f86868cade2e68eb8af6d","Dept. of Computer Science and Engineering, Amrita School of Engineering, Amrita University, Bengaluru, India","Divya, V., Dept. of Computer Science and Engineering, Amrita School of Engineering, Amrita University, Bengaluru, India; Amudha, J., Dept. of Computer Science and Engineering, Amrita School of Engineering, Amrita University, Bengaluru, India; Jyotsna, C., Dept. of Computer Science and Engineering, Amrita School of Engineering, Amrita University, Bengaluru, India","Eye tracking measures where the eye is focused or the movement of eye with respect to the head. The eye tracker will track the eye positions and eye movement for the visual stimulus presented on the computer system. Various features like gaze point, pupil size and mouse position can be extracted and it can be represented using visualization techniques such as fixation, saccade, scanpath and heat map. The features obtained from eye tracker can be extended to real life applications. Using this technology companies could be able to analyze thousands of customer's eye patterns in real-Time, and make decisions on marketing based on the data. The technology can analyze the stress level of patients, employees in IT, BPO, accounting, banking, front office etc. Here we are illustrating the advantages and applications of eye tracking, its usability and how to develop an application using a commercial eye tracker. © 2016 IEEE.","Calibration; Eye-Tracking; Feature extraction; Gaze estimation; Visualization","Calibration; Feature extraction; Flow visualization; Visualization; Eye trackers; Eye-tracking; Gaze estimation; Real-life applications; Stress levels; Technology companies; Visual stimulus; Visualization technique; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85015058250
"Tian D., He G., Wu J., Chen H., Jiang Y.","57193132566;35931805200;57193141560;57193141782;57193141148;","An accurate eye pupil localization approach based on adaptive gradient boosting decision tree",2017,"VCIP 2016 - 30th Anniversary of Visual Communication and Image Processing",,,"7805483","","",,12,"10.1109/VCIP.2016.7805483","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011085740&doi=10.1109%2fVCIP.2016.7805483&partnerID=40&md5=25737777e737c7fadfcf737631f36c55","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Dongchuan Road 800, Minhang District, Shanghai, 200240, China; Electric Power Research Institute, State Grid Shanghai Electric Power Company, Shanghai, 200093, China","Tian, D., School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Dongchuan Road 800, Minhang District, Shanghai, 200240, China; He, G., School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Dongchuan Road 800, Minhang District, Shanghai, 200240, China; Wu, J., School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Dongchuan Road 800, Minhang District, Shanghai, 200240, China; Chen, H., Electric Power Research Institute, State Grid Shanghai Electric Power Company, Shanghai, 200093, China; Jiang, Y., Electric Power Research Institute, State Grid Shanghai Electric Power Company, Shanghai, 200093, China","Eye pupil localization is an important part in computer vision applications such as face recognition, gaze estimation and so on. In this paper, we propose an improved method for precise and fast eye pupil localization. Based on gradient boosting decision tree(GBDT) algorithm, a more accurate localization is achieved by increasing the weight of the training samples with larger errors in a moderate rate. Furthermore, a pruning strategy is utilized to avoid overfitting and reduce the localization time without accuracy loss. Experimental results show that the improved method achieves an accuracy of 92.39% at a speed as fast as 1.7ms to locate in the range of eye pupil on BioID database. The proposed method outperforms most state-of-the-art methods in terms of localization accuracy and consumed time. © 2016 IEEE.","adaptive; computer vision; eye pupil localization; gradient boosting; Pruning method","Computer vision; Data mining; Decision trees; Face recognition; Trees (mathematics); Visual communication; adaptive; Computer vision applications; Gradient boosting; Localization accuracy; Pruning methods; Pruning strategy; Pupil localization; State-of-the-art methods; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85011085740
"Tokunaga T., Nishikawa H., Iwakura T.","22434161600;37012639700;23389313800;","An eye-tracking study of named entity annotation",2017,"International Conference Recent Advances in Natural Language Processing, RANLP","2017-September",,,"758","764",,5,"10.26615/978-954-452-049-6-097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045764082&doi=10.26615%2f978-954-452-049-6-097&partnerID=40&md5=b439a472e4f23a9af78d56f355708c4c","Tokyo Institute of Technology, Japan; Fujitsu Laboratories Ltd., Japan","Tokunaga, T., Tokyo Institute of Technology, Japan; Nishikawa, H., Tokyo Institute of Technology, Japan; Iwakura, T., Fujitsu Laboratories Ltd., Japan","Utilising effective features in machine learning-based natural language processing (NLP) is crucial in achieving good performance for a given NLP task. The paper describes a pilot study on the analysis of eye-ttacking data during named entity (NE) annotation, aiming at obtaining insights into effective features for the NE recognition task. The eye gaze data were collected from 10 annotators and analysed regarding working time and fixation distribution. The results of the preliminary qualitative analysis showed that human annotators tend to look at broader contexts around the target NE than recent state-of-the-art automatic NE recognition systems and to use predicate argument relations to identify the NE categories. © 2018 Association for Computational Linguistics (ACL). All rights reserved.",,"Deep learning; Learning algorithms; Natural language processing systems; Eye-tracking studies; Fixation distributions; Named entities; Pilot studies; Qualitative analysis; Recent state; Recognition systems; Working time; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85045764082
"Rohanian O., Taslimipoor S., Yaneva V., Ha L.A.","57197737309;55485832700;57003253500;13612155200;","Using gaze data to predict multiword expressions",2017,"International Conference Recent Advances in Natural Language Processing, RANLP","2017-September",,,"601","609",,9,"10.26615/978-954-452-049-6-078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045749173&doi=10.26615%2f978-954-452-049-6-078&partnerID=40&md5=290189adb0ec6c2f0c7a7056fab5a4ba","Research Group in Computational Linguistics, University of Wolverhampton, United Kingdom","Rohanian, O., Research Group in Computational Linguistics, University of Wolverhampton, United Kingdom; Taslimipoor, S., Research Group in Computational Linguistics, University of Wolverhampton, United Kingdom; Yaneva, V., Research Group in Computational Linguistics, University of Wolverhampton, United Kingdom; Ha, L.A., Research Group in Computational Linguistics, University of Wolverhampton, United Kingdom","In recent years gaze data has been increasingly used to improve and evaluate NLP models due to the fact that it carries information about the cognitive processing of linguistic phenomena. In this paper we conduct a preliminary study towards the automatic identification of multiword expressions based on gaze features from native and non-native speakers of English. We report comparisons between a part-of-speech (POS) and frequency baseline to: i) a prediction model based solely on gaze data and ii) a combined model of gaze data, POS and frequency. In spite of the challenging nature of the task, best performance was achieved by the latter. Furthermore, we explore how the type of gaze data (from native versus non-native speakers) affects the prediction, showing that data from the two groups is discriminative to an equal degree. Finally, we show that late processing measures are more predictive than early ones, which is in line with previous research on idioms and other formulaic structures. © 2018 Association for Computational Linguistics (ACL). All rights reserved.",,"Automation; Deep learning; Forecasting; Natural language processing systems; Automatic identification; Cognitive processing; Combined model; Linguistic phenomena; Multi-word expressions; Non-native speakers; Part Of Speech; Prediction model; Data reduction",Conference Paper,"Final","",Scopus,2-s2.0-85045749173
"Santini T., Fuhl W., Geisler D., Kasneci E.","54881866000;56770084800;57189847283;56059892600;","EyeRecToo: Open-source software for real-Time pervasive head-mounted eye tracking",2017,"VISIGRAPP 2017 - Proceedings of the 12th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","6",,,"96","101",,22,"10.5220/0006224700960101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044870684&doi=10.5220%2f0006224700960101&partnerID=40&md5=a70e5ca6fa7d259b63f3379c750fd1c0","Perception Engineering, University of Tübingen, Tübingen, Germany","Santini, T., Perception Engineering, University of Tübingen, Tübingen, Germany; Fuhl, W., Perception Engineering, University of Tübingen, Tübingen, Germany; Geisler, D., Perception Engineering, University of Tübingen, Tübingen, Germany; Kasneci, E., Perception Engineering, University of Tübingen, Tübingen, Germany","Head-mounted eye tracking offers remarkable opportunities for research and applications regarding pervasive health monitoring, mental state inference, and human computer interaction in dynamic scenarios. Although a plethora of software for the acquisition of eye-Tracking data exists, they often exhibit critical issues when pervasive eye tracking is considered, e.g., closed source, costly eye tracker hardware dependencies, and requiring a human supervisor for calibration. In this paper, we introduce EyeRecToo, an open-source software for real-Time pervasive head-mounted eye-Tracking. Out of the box, EyeRecToo offers multiple real-Time state-ofthe-art pupil detection and gaze estimation methods, which can be easily replaced by user implemented algorithms if desired. A novel calibration method that allows users to calibrate the system without the assistance of a human supervisor is also integrated. Moreover, this software supports multiple head-mounted eye-Tracking hardware, records eye and scene videos, and stores pupil and gaze information, which are also available as a real-Time stream. Thus, EyeRecToo serves as a framework to quickly enable pervasive eye-Tracking researchand applications. Available at: www.ti.uni-Tuebingen.de/perception. © 2017 by SCITEPRESS - Science and Technology Publications, Lda.","Calibration; Data Acquisition; Eye Movements; Eye Tracking; Gaze Estimation; Human-computer Interaction; Open-source; Pervasive; Pupil Detection; Real-Time","Calibration; Computer graphics; Computer hardware; Computer hardware description languages; Computer vision; Data acquisition; Eye movements; Human computer interaction; Open source software; Open systems; Supervisory personnel; Gaze estimation; Open sources; Pervasive; Pupil detection; Real time; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-85044870684
"Chinsatit W., Saitoh T.","57200653220;13006364300;","CNN-Based Pupil Center Detection for Wearable Gaze Estimation System",2017,"Applied Computational Intelligence and Soft Computing","2017",,"8718956","","",,21,"10.1155/2017/8718956","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042094141&doi=10.1155%2f2017%2f8718956&partnerID=40&md5=9e4c2f66a02b153bcce10e7c4c8f74b1","Graduate School of Computer Science and Systems Engineering, Kyushu Institute of Technology, 680-4 Kawazu, Iizuka-shi, Fukuoka, 820-8502, Japan","Chinsatit, W., Graduate School of Computer Science and Systems Engineering, Kyushu Institute of Technology, 680-4 Kawazu, Iizuka-shi, Fukuoka, 820-8502, Japan; Saitoh, T., Graduate School of Computer Science and Systems Engineering, Kyushu Institute of Technology, 680-4 Kawazu, Iizuka-shi, Fukuoka, 820-8502, Japan","This paper presents a convolutional neural network- (CNN-) based pupil center detection method for a wearable gaze estimation system using infrared eye images. Potentially, the pupil center position of a user's eye can be used in various applications, such as human-computer interaction, medical diagnosis, and psychological studies. However, users tend to blink frequently; thus, estimating gaze direction is difficult. The proposed method uses two CNN models. The first CNN model is used to classify the eye state and the second is used to estimate the pupil center position. The classification model filters images with closed eyes and terminates the gaze estimation process when the input image shows a closed eye. In addition, this paper presents a process to create an eye image dataset using a wearable camera. This dataset, which was used to evaluate the proposed method, has approximately 20,000 images and a wide variation of eye states. We evaluated the proposed method from various perspectives. The result shows that the proposed method obtained good accuracy and has the potential for application in wearable device-based gaze estimation. © 2017 Warapon Chinsatit and Takeshi Saitoh.",,,Article,"Final","",Scopus,2-s2.0-85042094141
"Wan Z., Wang X.","56105053500;55736887200;","A method of gaze estimation for wearable multi-camera devices using stereo vision",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10462",,"104620G","","",,2,"10.1117/12.2282347","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040723937&doi=10.1117%2f12.2282347&partnerID=40&md5=41e26a2755116a99e589ce9c9ad1e835","MOEMS Education Ministry Lab, Tianjin University, Tianjin, 300072, China; State Key Laboratory of Precision Measuring Technology and Instruments, Tianjin University, Tianjin, 300072, China","Wan, Z., MOEMS Education Ministry Lab, Tianjin University, Tianjin, 300072, China; Wang, X., MOEMS Education Ministry Lab, Tianjin University, Tianjin, 300072, China, State Key Laboratory of Precision Measuring Technology and Instruments, Tianjin University, Tianjin, 300072, China","An approach to gaze estimation for wearable devices is proposed and its effectiveness is demonstrated. The proposed approach includes composing stereo vision system which is comprised of half transparent and half reflecting mirror and multi-camera, and its calibration procedure is easy and practical. This whole system can also run online and with little intervention from user by using half transparent and half reflecting mirror, and cameras above the eyes. Because of the application of stereo vision system, some feature points in the region around eyes can be solved. Thus, eye gaze is estimated directly by calculating the spatial coordinates of feature points which are extracted from pupil and eye corner. The calibration procedure is general, as no complex model of the human eye is utilized in this work, and it can be demonstrated as how to calibrate the angle between optical axis and directly measured gaze. The proposed approach is effectively to study visual attention with the help of gaze estimation experiments, and it is useful and practical for people operating in unstructured scenarios. © 2017 SPIE.","Gaze estimation; Stereo vision","Behavioral research; Calibration; Cameras; Imaging techniques; Mirrors; Stereo image processing; Wearable technology; Calibration procedure; Gaze estimation; N-O complexes; Reflecting mirrors; Spatial coordinates; Stereo vision system; Visual Attention; Wearable devices; Stereo vision",Conference Paper,"Final","",Scopus,2-s2.0-85040723937
[无可用作者姓名],[无可用的作者 ID],"Proceedings of SPIE - The International Society for Optical Engineering",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10462",,,"","",1488,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040712098&partnerID=40&md5=18b2e0466e70794e56ea2bcda201e0b5",,"","The proceedings contain 200 papers. The topics discussed include: long focal length large aperture optical passive athermalization MWIR optical system; the improved deconvolution video restoration technology based on the infrared detection system; discussion on the optimal deconvolution for electro-optical imaging systems; design and implementation of multi-spectral multi-axis parallel calibration system; a fast approach against large foreground motion in real-time image stabilization; denoising processing of MIE-polarization lidar signal using wavelet; hyperspectral face recognition based on spatio-spectral fusion and local binary pattern; identification of conveyor belt injury based on image texture SVM classification method; target interpretation of visible light image and infrared image fusion method; objective lens design of polarization imaging in haze environment; Bayesian multi-frame super-resolution of differently exposed images; dual channel and fast response optical fiber temperature detection system based on Raman scattering; a method of gaze estimation for wearable multi-camera devices using stereo vision; salt-and-pepper noise removal in polarization optics imaging; design of inspection system for surface defects on industrial parts under complex background; LOS stabilization model for ship swaying based on subdivision iterative algorithm; and design of a visible optical passive ranging system based on micro main board.",,,Conference Review,"Final","",Scopus,2-s2.0-85040712098
"Wen W., Chen T., Yang M.","57213009564;57195606499;55703267800;","The Android-Based Acquisition and CNN-Based Analysis for Gaze Estimation in Eye Tracking",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10568 LNCS",,,"562","571",,,"10.1007/978-3-319-69923-3_61","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032657680&doi=10.1007%2f978-3-319-69923-3_61&partnerID=40&md5=18292c094d3796c5efc2781ca0aacffd","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, 518000, China; School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, 510000, China","Wen, W., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, 518000, China; Chen, T., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, 518000, China; Yang, M., College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, 518000, China, School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, 510000, China","Over the past several years, the demand for eye tracking is increasing across fields of computer vision and pattern recognition, especially in commercial applications. However, the low prediction accuracy and the restriction of datasets and methods for special eye tracking equipment have been obstacles of the wide application of gaze estimation. In this paper, we develop an Android-based acquisition software named EyeTracker, to collect the first Chinese gaze dataset. And then we proposed a convolutional neural network framework for gaze estimation in eye tracking based on a single image. We evaluate our proposed analysis model on our dataset-EyeTrackD (tablet) and Gazecapture (part of phone data). Our model achieves a prediction error of 4.33, cm and 2.25, cm on these two datasets respectively, which are better than the previous method using the same data. Extensive experiments under different network settings show the effectiveness of our convolutional neural network framework. © 2017, Springer International Publishing AG.","Convolutional neural network (CNN); Eye tracking; Gaze estimation","Biometrics; Convolution; Neural networks; Pattern recognition; Analysis models; Commercial applications; Convolutional neural network; Eye-tracking; Gaze estimation; Network settings; Prediction accuracy; Prediction errors; Android (operating system)",Conference Paper,"Final","",Scopus,2-s2.0-85032657680
"González-Ortega D., González-Díaz J., Díaz-Pernas F.J., Martínez-Zarzuela M., Antón-Rodríguez M.","13408419500;57196052918;6507870236;23393198300;25122110400;","3D kinect-based gaze region estimation in a driving simulator",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10586 LNCS",,,"789","795",,,"10.1007/978-3-319-67585-5_76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031409301&doi=10.1007%2f978-3-319-67585-5_76&partnerID=40&md5=29ea6fb5852ed9aaa0b66bd52fbd92d6","Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain","González-Ortega, D., Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain; González-Díaz, J., Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain; Díaz-Pernas, F.J., Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain; Martínez-Zarzuela, M., Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain; Antón-Rodríguez, M., Department of Signal Theory, Communications and Telematics Engineering, Telecommunications Engineering School, University of Valladolid, Valladolid, Spain","In this paper, we present a 3D Kinect-based gaze region estimation module to add gaze pattern information in a driving simulator. Gaze region is estimated using only face orientation cues, similarly to other previous approaches in the literature. An initial user-based calibration stage is included in our approach. The module is able to detect the region, out of 7 in which the driving scene was divided, that a driver is gazing on route every processed frame. 8 people tested the module, which achieved an accuracy of 88.23%. The information provided by the gaze estimation module enriches the driving simulator data and makes it possible a multimodal driving performance analysis. © 2017, Springer International Publishing AG.","3D computer vision; Confusion matrix; Driving simulator; Face tracking; Gaze estimation; Kinect device","Artificial intelligence; Automobile drivers; Automobile simulators; Simulators; Ubiquitous computing; 3D computer vision; Confusion matrices; Driving simulator; Face Tracking; Gaze estimation; Kinect device; Ambient intelligence",Conference Paper,"Final","",Scopus,2-s2.0-85031409301
"Yogeswaran A., Payeur P.","15129067200;6602387260;","Leveraging saccades to learn smooth pursuit: A self-organizing motion tracking model using restricted boltzmann machines",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,,,"4313","4319",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030460286&partnerID=40&md5=df2dc5bc47804caf0ee703a6746a66ce","Department of Electrical Engineering and Computer Science, University of Ottawa, 800 King Edward Ave, Ottawa, Canada","Yogeswaran, A., Department of Electrical Engineering and Computer Science, University of Ottawa, 800 King Edward Ave, Ottawa, Canada; Payeur, P., Department of Electrical Engineering and Computer Science, University of Ottawa, 800 King Edward Ave, Ottawa, Canada","In this paper, we propose a biologically-plausible model to explain the emergence of motion tracking behaviour in early development using unsupervised learning. The model's training is biased by a concept called retinal constancy, which measures how similar visual contents are between successive frames. This biasing is similar to a reward in reinforcement learning, but is less explicit, as it modulates the model's learning rate instead of being a learning signal itself. The model is a two-layer deep network. The first layer learns to encode visual motion, and the second layer learns to relate that motion to gaze movements, which it perceives and creates through bi-directional nodes. By randomly generating gaze movements to traverse the local visual space, desirable correlations are developed between visual motion and the appropriate gaze to nullify that motion such that maximal retinal constancy is achieved. Biologically, this is similar to using saccades to look around and learning from moments where a target and the saccade move together such that the image stays the same on the retina, and developing smooth pursuit behaviour to perform this action in the future. Restricted Boltzmann machines are used to implement this model because they can form a deep belief network, perform online learning, and act generatively. These properties all have biological equivalents and coincide with the biological plausibility of using saccades as leverage to learn smooth pursuit. This method is unique because it uses general machine learning algorithms, and their inherent generative properties, to learn from real-world data. It also implements a biological theory, uses motion instead of recognition via local searches, without temporal filtering, and learns in a fully unsupervised manner. Its tracking performance after being trained on real-world images with simulated motion is compared to its tracking performance after being trained on natural video. Results show that this model is able to successfully follow targets in natural video, despite partial occlusions, scale changes, and nonlinear motion. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Aldehydes; Artificial intelligence; Deep learning; Eye movements; Learning systems; Motion analysis; Network layers; Ophthalmology; Reinforcement learning; Vision; Deep belief networks; Motion tracking; Non-linear motions; Partial occlusions; Real-world image; Restricted boltzmann machine; Temporal filtering; Tracking performance; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-85030460286
"Sakurai K., Yan M., Tanno K., Tamura H.","56909413100;55515780300;7103288835;35600682500;","Gaze Estimation Method Using Analysis of Electrooculogram Signals and Kinect Sensor",2017,"Computational Intelligence and Neuroscience","2017",,"2074752","","",,3,"10.1155/2017/2074752","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029155857&doi=10.1155%2f2017%2f2074752&partnerID=40&md5=f38f10b2ccce807e7815951bf0d4a2a8","Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazaki, Japan; Faculty of Engineering, University of Miyazaki, Miyazaki, Japan","Sakurai, K., Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazaki, Japan; Yan, M., Faculty of Engineering, University of Miyazaki, Miyazaki, Japan; Tanno, K., Faculty of Engineering, University of Miyazaki, Miyazaki, Japan; Tamura, H., Faculty of Engineering, University of Miyazaki, Miyazaki, Japan","A gaze estimation system is one of the communication methods for severely disabled people who cannot perform gestures and speech. We previously developed an eye tracking method using a compact and light electrooculogram (EOG) signal, but its accuracy is not very high. In the present study, we conducted experiments to investigate the EOG component strongly correlated with the change of eye movements. The experiments in this study are of two types: Experiments to see objects only by eye movements and experiments to see objects by face and eye movements. The experimental results show the possibility of an eye tracking method using EOG signals and a Kinect sensor. © 2017 Keiko Sakurai et al.",,"Signal analysis; Speech communication; Communication method; Disabled people; Electro-oculogram; EOG signal; Eye tracking methods; Gaze estimation; Kinect sensors; Eye movements; disabled person; electrooculography; eye fixation; eye movement; face; human; physiology; procedures; Disabled Persons; Electrooculography; Eye Movements; Face; Fixation, Ocular; Humans",Article,"Final","",Scopus,2-s2.0-85029155857
"Wan Z., Xiong C.","57195490237;57211738191;","Estimating 3D gaze point on object using stereo scene cameras",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10462 LNAI",,,"323","329",,,"10.1007/978-3-319-65289-4_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028461642&doi=10.1007%2f978-3-319-65289-4_31&partnerID=40&md5=1e437877023832a9a9f7d19fdf8afcca","State Key Lab of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China","Wan, Z., State Key Lab of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China; Xiong, C., State Key Lab of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China","3D eye gaze estimation in real environment is still challenging. A novel method of scene-based 3D gaze estimation is proposed in this paper. As this model combines two models, the 2D nonlinear polynomial mapping model of traditional regression-based gaze estimation and the 3D visual axis linear ray model of traditional geometry-based gaze estimation, it includes two steps. The first step is to estimate the visual axis from the pupil center in an eye camera image. The second one is to estimate the 3D gaze point which is the intersection between the visual axis and the scene object, which can be obtained by stereo scene cameras. As the 3D gaze points are on the object, rather than outside or inside the object like geometry-based 3D gaze estimation, this method is potential for human robot interaction in real environment. Through a simple test, the accuracy of our 3D gaze estimation system is acceptable. © Springer International Publishing AG 2017.","3D gaze estimation; Eye tracking; Stereo scene cameras","Cameras; Robotics; Robots; Eye-tracking; Gaze estimation; Nonlinear polynomials; Pupil centers; Real environments; Scene object; Simple tests; Stereo scene; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85028461642
"Chen H., Yan M., Liu S., Jiang B.","57194771022;57194766650;57190766930;56443199100;","Gaze inspired subtitle position evaluation for MOOCs videos",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10443",,"1044318","","",,1,"10.1117/12.2280281","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021934904&doi=10.1117%2f12.2280281&partnerID=40&md5=bb68e55ef53bbca91e9983b9e0998255","School of Education Science and Technology, Nanjing University of Posts and Telecommunications, Nanjing, China","Chen, H., School of Education Science and Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; Yan, M., School of Education Science and Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; Liu, S., School of Education Science and Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; Jiang, B., School of Education Science and Technology, Nanjing University of Posts and Telecommunications, Nanjing, China","Online educational resources, such as MOOCs, is becoming increasingly popular, especially in higher education field. One most important media type for MOOCs is course video. Besides traditional bottom-position subtitle accompany to the videos, in recent years, researchers try to develop more advanced algorithms to generate speaker-following style subtitles. However, the effectiveness of such subtitle is still unclear. In this paper, we investigate the relationship between subtitle position and the learning effect after watching the video on tablet devices. Inspired with image based human eye tracking technique, this work combines the objective gaze estimation statistics with subjective user study to achieve a convincing conclusion-speaker-following subtitles are more suitable for online educational videos. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Gaze estimation; MOOCs; Tablet; Video subtitles","Education; Pattern recognition; Educational resource; Educational videos; Gaze estimation; Higher education; MOOCs; Position evaluation; Tablet; Video subtitles; E-learning",Conference Paper,"Final","",Scopus,2-s2.0-85021934904
"Grigorescu S.M., Macesanu G.","55382394500;36601194600;","Human–robot interaction through robust gaze following",2017,"Advances in Intelligent Systems and Computing","462",,,"165","178",,,"10.1007/978-3-319-44260-0_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021205307&doi=10.1007%2f978-3-319-44260-0_10&partnerID=40&md5=92e1d6e9fd3453aa00f1fa10a5947ddf","Department of Automation, Transilvania University of Brasov, Mihai Viteazu 5, Braşov, 500174, Romania","Grigorescu, S.M., Department of Automation, Transilvania University of Brasov, Mihai Viteazu 5, Braşov, 500174, Romania; Macesanu, G., Department of Automation, Transilvania University of Brasov, Mihai Viteazu 5, Braşov, 500174, Romania","In this paper, a probabilistic solution for gaze following in the context of joint attention will be presented. Gaze following, in the sense of continuously measuring (with a greater or a lesser degree of anticipation) the head pose and gaze direction of an interlocutor so as to determine his/her focus of attention, is important in several important areas of computer vision applications, such as the development of nonintrusive gaze-tracking equipment for psychophysical experiments in Neuroscience, specialized telecommunication devices, Human–Computer Interfaces (HCI) and artificial cognitive systems for Human–Robot Interaction (HRI). We have developed a probabilistic solution that inherently deals with sensor models uncertainties and incomplete data. This solution comprises a hierarchical formulation of a set of detection classifiers that loosely follows how geometrical cues provided by facial features are used by the human perceptual system for gaze estimation. A quantitative analysis of the proposed architectures performance was undertaken through a set of experimental sessions. In these sessions, temporal sequences of moving human agents fixating a well-known point in space were grabbed by the stereovision setup of a robotic perception system, and then processed by the framework. © Springer International Publishing Switzerland 2017.",,"Cognitive systems; Human computer interaction; Computer vision applications; Focus of Attention; Human perceptual system; Perception systems; Probabilistic solution; Proposed architectures; Psychophysical experiments; Telecommunication devices; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85021205307
"Jain S., Kamath S.S.","57208446516;57216704186;","Saliency prediction for visual regions of interest with applications in advertising",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10165 LNCS",,,"48","60",,1,"10.1007/978-3-319-56687-0_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018586343&doi=10.1007%2f978-3-319-56687-0_5&partnerID=40&md5=1ae4b430430f695281f5080414dc016c","National Institute of Technology Karnataka, Surathkal, 575025, India","Jain, S., National Institute of Technology Karnataka, Surathkal, 575025, India; Kamath, S.S., National Institute of Technology Karnataka, Surathkal, 575025, India","Human visual fixations play a vital role in a plethora of genres, ranging from advertising design to human-computer interaction. Considering saliency in images thus brings significant merits to Computer Vision tasks dealing with human perception. Several classification models have been developed to incorporate various feature levels and estimate free eye-gazes. However, for real-time applications (Here, real-time applications refer to those that are time, and often resource-constrained, requiring speedy results. It does not imply on-line data analysis), the deep convolution neural networks are either difficult to deploy, given current hardware limitations or the proposed classifiers cannot effectively combine image semantics with low-level attributes. In this paper, we propose a novel neural network approach to predict human fixations, specifically aimed at advertisements. Such analysis significantly impacts the brand value and assists in audience measurement. A dataset containing 400 print ads across 21 successful brands was used to successfully evaluate the effectiveness of advertisements and their associated fixations, based on the proposed saliency prediction model. © Springer International Publishing AG 2017.","Advertising; Free eye-gaze estimation; Machine Learning; Neural networks; Support Vector Machines; Visual saliency","Deep neural networks; Face recognition; Forecasting; Learning systems; Marketing; Neural networks; Semantics; Support vector machines; Classification models; Convolution neural network; Eye-gaze; Low-level attributes; Novel neural network; Real-time application; Regions of interest; Visual saliency; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85018586343
"Kacete A., Séguier R., Collobert M., Royan J.","57190134112;6505976299;6506362460;23101034500;","Unconstrained gaze estimation using random forest regression voting",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10113 LNCS",,,"419","432",,4,"10.1007/978-3-319-54187-7_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016032758&doi=10.1007%2f978-3-319-54187-7_28&partnerID=40&md5=be1345d30c3171f7af986bb93aff4310","Institute of Research and Technology B-com, Cesson-Sévigné, France","Kacete, A., Institute of Research and Technology B-com, Cesson-Sévigné, France; Séguier, R., Institute of Research and Technology B-com, Cesson-Sévigné, France; Collobert, M., Institute of Research and Technology B-com, Cesson-Sévigné, France; Royan, J., Institute of Research and Technology B-com, Cesson-Sévigné, France","In this paper we address the problem of automatic gaze estimation using a depth sensor under unconstrained head pose motion and large user-sensor distances. To achieve robustness, we formulate this problem as a regression problem. To solve the task in hand, we propose to use a regression forest according to their high ability of generalization by handling large training set. We train our trees on an important synthetic training data using a statistical model of the human face with an integrated parametric 3D eyeballs. Unlike previous works relying on learning the mapping function using only RGB cues represented by the eye image appearances, we propose to integrate the depth information around the face to build the input vector. In our experiments, we show that our approach can handle real data scenarios presenting strong head pose changes even though it is trained only on synthetic data, we illustrate also the importance of the depth information on the accuracy of the estimation especially in unconstrained scenarios. © Springer International Publishing AG 2017",,"Computer vision; Decision trees; Regression analysis; Depth information; Gaze estimation; Mapping functions; Regression forests; Regression problem; Statistical modeling; Synthetic data; Synthetic training data; Problem solving",Conference Paper,"Final","",Scopus,2-s2.0-85016032758
"Kacete A., Séguier R., Collobert M., Royan J.","57190134112;6505976299;6506362460;23101034500;","Head pose free 3D gaze estimation using RGB-D camera",2017,"Proceedings of SPIE - The International Society for Optical Engineering","10225",,"102251S","","",,2,"10.1117/12.2266091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015075842&doi=10.1117%2f12.2266091&partnerID=40&md5=e92db534544a4514f98190ded7bb7949","Institute of Research and Technology B-com, France","Kacete, A., Institute of Research and Technology B-com, France; Séguier, R., Institute of Research and Technology B-com, France; Collobert, M., Institute of Research and Technology B-com, France; Royan, J., Institute of Research and Technology B-com, France","In this paper, we propose an approach for 3D gaze estimation under head pose variation using RGB-D camera. Our method uses a 3D eye model to determine the 3D optical axis and infer the 3D visual axis. For this, we estimate robustly user head pose parameters and eye pupil locations with an ensembles of randomized trees trained with an important annotated training sets. After projecting eye pupil locations in the sensor coordinate system using the sensor intrinsic parameters and a one-time simple calibration by gazing a known 3D target under different directions, the 3D eyeball centers are determined for a specific user for both eyes yielding the determination of the visual axis. Experimental results demonstrate that our method shows a good gaze estimation accuracy even if the environment is highly unconstrained namely large user-sensor distances (> 1m50) unlike state-of-the-art methods which deal with relatively small distances (<1m). © 2017 SPIE.","3D eyeball; Gaze estimation; head pose estimation; pupil localization; Random Forest; RGB-D camera","Cameras; Decision trees; Image recognition; Optical data processing; 3D eyeball; Gaze estimation; Head Pose Estimation; Pupil localization; Random forests; Rgb-d cameras; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85015075842
[无可用作者姓名],[无可用的作者 ID],"25th International Conference on Robotics in Alpe-Adria-Danube Region, RAAD 2016",2017,"Advances in Intelligent Systems and Computing","540",,,"1","648",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007399614&partnerID=40&md5=1b9210039f7ed2e6d402a1ccb120ec33",,"","The proceedings contain 70 papers. The special focus in this conference is on Intelligent Robot Motion Control, Robot Vision Sensory Processing, Novel Design of Robot Manipulators, Grippers, Robot Applications in Manufacturing, Services, Autonomous Systems, Humanoid, Walking Robots, Evolution, Education, Legal and Social Issues of Robotics. The topics include: Kinematic control of redundant robots in changing task space; virtual compliance control of a kinematically redundant serial manipulator with 9 DoF; minimal energy Cartesian impedance control of robot with bidirectional antagonistic drives; design and shape optimization of novel load cell; 3-axis contact force fingertip sensor based on hall effect sensor; computer simulation of bounded error algorithm for iterative learning control; dynamical modeling and swing-up control of a self-balancing cube; about the accuracy of fast moving robotic devices based on compliant mechanisms; design, modelling and prototyping of a mechanical hand for prosthetic purposes; robot programming with flexible geometric relations; calibration of a robotized bending system; weed segmentation from grayscale tobacco seedling images; path planning for formation control of autonomous vehicles; vision-guided autonomous forklift; gaze-based human-smarthome-interaction by augmented reality controls; toward an active protection for robot arms; action unit based facial expression recognition using deep learning; mechanical design of a prosthetic human arm and its dynamic simulation; prospects of robotics development for restorative medicine; influence of an assistive hip orthosis on gait; model-based development of robotic systems and services in construction robotics and at the crossroads of architecture and robotics.",,,Conference Review,"Final","",Scopus,2-s2.0-85007399614
"Guo Z., Zhou Q., Liu Z.","56304391000;9632964600;55553990400;","Appearance-based gaze estimation under slight head motion",2017,"Multimedia Tools and Applications","76","2",,"2203","2222",,11,"10.1007/s11042-015-3182-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953392336&doi=10.1007%2fs11042-015-3182-4&partnerID=40&md5=4f6cc1b24fc3f19e41c8ef3a51d3ba6a","School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China","Guo, Z., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Zhou, Q., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Liu, Z., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China","At present a lot of gaze estimation methods can get accurate result under ideal conditions, but some practical issues are still the biggest challenges affect the accuracy such as head motion and eye blinking. Improving the accuracy of gaze estimation and the tolerance of head motion are common tasks in the field of gaze estimation. Therefore, this paper aims to propose an accurate gaze estimation method without fixed head pose. The core problem is how to build the mapping relationship between image features and gaze position, and how to resist the head motion through the training samples. To this end, at first, a new input feature, which can well reflect the change of eye image features with different gaze positions, is proposed and it is based on appearance feature and distance feature. So the number of training samples in the process of calibration is significantly reduced. Then ℓ1-optimization is used to select an optimal set, which represents the mapping relationship between input feature and gaze position. At last, a linear equation is fitted to correct the initial estimation bias which is brought by head motion. In this paper, the experimental results demonstrate that our system achieves accurate result with one camera and a small number of calibration points. The accuracy of final gaze estimation is improved by 22 % through compensation equation. In addition, our system is robustness to eye blink and distance change. © 2016, Springer Science+Business Media New York.","Blink recognition; Compensation equation; Feature extraction; Gaze estimation","Feature extraction; Mapping; Sampling; Appearance based; Blink recognition; Calibration points; Distance feature; Gaze estimation; Initial estimation; Mapping relationships; Practical issues; Calibration",Article,"Final","",Scopus,2-s2.0-84953392336
"Vasli B., Martin S., Trivedi M.M.","57193015176;55510668500;7103153314;","On Driver gaze estimation: Explorations and Fusion of geometric and data driven approaches",2016,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC",,,"7795623","655","660",,13,"10.1109/ITSC.2016.7795623","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010064966&doi=10.1109%2fITSC.2016.7795623&partnerID=40&md5=ee44b4bd3bd701aed9dd382302d466f8","Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, San Diego, CA  92092, United States","Vasli, B., Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, San Diego, CA  92092, United States; Martin, S., Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, San Diego, CA  92092, United States; Trivedi, M.M., Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, San Diego, CA  92092, United States","Gaze direction is important in a number of applications such as active safety and driver's activity monitoring. However, there are challenges in estimating gaze robustly in real world driving situations. While performance of personalized gaze estimation models has improved significantly, performance improvement of universal gaze estimation is lagging behind; one reason being, learning based methods do not exploit the physical constraints of the car. In this paper, we propose a system to estimate driver's gaze from head and eye cues projected on a multi-plane geometrical environment and a system which fuses the geometric with data driven learning method. Evaluations are conducted on naturalistic driving data containing different drivers in different vehicles in order to test the generalization of the methods. Systematic evaluations on this data set are presented for the proposed geometric based gaze estimation method and geometric plus learning based hybrid gaze estimation framework, where exploiting the geometrical constraints of the car shows promising results of generalization. Index Terms-In Cabin Activity Analysis, Human-vehicle Interaction, Gaze Estimation, Take-over, Highly Automated Vehicles. © 2016 IEEE.",,"Estimation; Intelligent systems; Intelligent vehicle highway systems; Transportation; Vehicles; Activity monitoring; Data-driven approach; Geometrical constraints; Human vehicle interactions; Learning-based methods; Physical constraints; Real-world drivings; Systematic evaluation; Geometry",Conference Paper,"Final","",Scopus,2-s2.0-85010064966
"Wang Y., Zeng H., Liu J.","57193018119;7401472202;56066228900;","Low-cost eye-tracking glasses with real-time head rotation compensation",2016,"Proceedings of the International Conference on Sensing Technology, ICST",,,"7796336","","",,3,"10.1109/ICSensT.2016.7796336","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010023420&doi=10.1109%2fICSensT.2016.7796336&partnerID=40&md5=8d8877115f78c4d1e6da62929b834922","School of Instrument Science and Engineering, Southeast University, Nanjing, 210096, China; Jiangsu Collab. Innovat. Ctr. of Atmosph. Environ./Equip. Technol., Nanjing University of Information Science and Technology, Jiangsu, 210044, China","Wang, Y., School of Instrument Science and Engineering, Southeast University, Nanjing, 210096, China; Zeng, H., School of Instrument Science and Engineering, Southeast University, Nanjing, 210096, China; Liu, J., Jiangsu Collab. Innovat. Ctr. of Atmosph. Environ./Equip. Technol., Nanjing University of Information Science and Technology, Jiangsu, 210044, China","Nowadays capturing the image of eyeball with camera and estimating gaze points using pupil positions is the most common method for eye-tracking devices. Head-mounted eye-tracking equipment has a wide range of application because of its flexibility and high gaze estimation accuracy. But the user's head should keep still using a chin rest or other tools, or will lead to enormous gaze estimation errors. In this paper, a head rotation compensation method is proposed for head-mounted eye-tracking glasses. The whole cost of our system is 38 USD. According to our preliminary result, the mean error of gaze estimation is reduced from 8.0162° to 0.5748°, which shows that our method can compensate head rotation effectively. The data in the task based experiment reveal that our system can be used as an alternative input device to mouse. © 2016 IEEE.","eye-tracking; gaze estimation; head-rotation compensation; human-computer interface","Glass; Human computer interaction; Rotation; Alternative input devices; Eye tracking devices; Eye-tracking; Gaze estimation; Head rotation; Head-mounted eye tracking; Human computer interfaces; Low cost eye tracking; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85010023420
"Jeni L.A., Cohn J.F.","24474645300;55423536000;","Person-Independent 3D Gaze Estimation Using Face Frontalization",2016,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,,"7789594","792","800",,17,"10.1109/CVPRW.2016.104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010223613&doi=10.1109%2fCVPRW.2016.104&partnerID=40&md5=47c9218fb3853c66a2cbf47dbd9c58d9","Carnegie Mellon University, Pittsburgh, PA, United States; University of Pittsburgh, Pittsburgh, PA, United States","Jeni, L.A., Carnegie Mellon University, Pittsburgh, PA, United States; Cohn, J.F., University of Pittsburgh, Pittsburgh, PA, United States","Person-independent and pose-invariant estimation of eye-gaze is important for situation analysis and for automated video annotation. We propose a fast cascade regression based method that first estimates the location of a dense set of markers and their visibility, then reconstructs face shape by fitting a part-based 3D model. Next, the reconstructed 3D shape is used to estimate a canonical view of the eyes for 3D gaze estimation. The model operates in a feature space that naturally encodes local ordinal properties of pixel intensities leading to photometric invariant estimation of gaze. To evaluate the algorithm in comparison with alternative approaches, three publicly-available databases were used, Boston University Head Tracking, Multi-View Gaze and CAVE Gaze datasets. Precision for head pose and gaze averaged 4 degrees or less for pitch, yaw, and roll. The algorithm outperformed alternative methods in both datasets. © 2016 IEEE.",,"Computer vision; Pattern recognition; Automated video; Boston University; Gaze estimation; Person-independent; Photometric invariants; Pixel intensities; Pose invariant; Situation analysis; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-85010223613
[无可用作者姓名],[无可用的作者 ID],"Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2016",2016,"Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2016",,,,"","",157,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010411113&partnerID=40&md5=c7431427ac67f2c15f5a04210e293798",,"","The proceedings contain 16 papers. The topics discussed include: robust keyframe-based monocular SLAM for augmented reality; leveraging the user's face for absolute scale estimation in handheld monocular SLAM; instant mixed reality lighting from casual scanning; an empirical model for specularity prediction with application to dynamic retexturing; edge snapping-based depth enhancement for dynamic occlusion handling in augmented reality; spatio-temporal point path analysis and optimization of a galvanoscopic scanning laser projector; practical and precise projector-camera calibration; towards kilo-Hertz 6-DoF visual tracking using an egocentric cluster of rolling shutter cameras; learning to fuse: a deep learning approach to visual-inertial camera pose estimation; PPV: pixel-point-volume segmentation for object referencing in collaborative augmented reality; do you see what i see? the effect of gaze tracking on task space remote collaboration; analysis of medium wrap freehand virtual object grasping in exocentric mixed reality; and automated spatial calibration of HMD systems with unconstrained eye-cameras.",,,Conference Review,"Final","",Scopus,2-s2.0-85010411113
"Yu P., Zhou J., Wu Y.","57191071774;57191078967;36747752300;","Learning Reconstruction-Based Remote Gaze Estimation",2016,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2016-December",,"7780744","3447","3455",,4,"10.1109/CVPR.2016.375","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986333979&doi=10.1109%2fCVPR.2016.375&partnerID=40&md5=57a50788f13acb32af903dd35d0e3d39","Northwestern University, 2145 Sheridan Road, Evanston, IL  60208, United States","Yu, P., Northwestern University, 2145 Sheridan Road, Evanston, IL  60208, United States; Zhou, J., Northwestern University, 2145 Sheridan Road, Evanston, IL  60208, United States; Wu, Y., Northwestern University, 2145 Sheridan Road, Evanston, IL  60208, United States","It is a challenging problem to accurately estimate gazes from low-resolution eye images that do not provide fine and detailed features for eyes. Existing methods attempt to establish the mapping between the visual appearance space to the gaze space. Different from the direct regression approach, the reconstruction-based approach represents appearance and gaze via local linear reconstruction in their own spaces. A common treatment is to use the same local reconstruction in the two spaces, i.e., the reconstruction weights in the appearance space are transferred to the gaze space for gaze reconstruction. However, this questionable treatment is taken for granted but has never been justified, leading to significant errors in gaze estimation. This paper is focused on the study of this fundamental issue. It shows that the distance metric in the appearance space needs to be adjusted, before the same reconstruction can be used. A novel method is proposed to learn the metric, such that the affinity structure of the appearance space under this new metric is as close as possible to the affinity structure of the gaze space under the normal Euclidean metric. Furthermore, the local affinity structure invariance is utilized to further regularize the solution to the reconstruction weights, so as to obtain a more robust and accurate solution. Effectiveness of the proposed method is validated and demonstrated through extensive experiments on different subjects. © 2016 IEEE.",,"Computer vision; Distance metrics; Euclidean metrics; Eye images; Gaze estimation; Local linear; Low resolution; Remote gaze estimation; Visual appearance; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-84986333979
"Bruce N.D.B., Catton C., Janjic S.","8347469300;57191074108;57191077140;","A deeper look at saliency: Feature contrast, semantics, and beyond",2016,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2016-December",,"7780431","516","524",,40,"10.1109/CVPR.2016.62","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986246966&doi=10.1109%2fCVPR.2016.62&partnerID=40&md5=7783a83936b5ad5f31a15b7b197e891e","University of Manitoba, Winnipeg, MB, Canada","Bruce, N.D.B., University of Manitoba, Winnipeg, MB, Canada; Catton, C., University of Manitoba, Winnipeg, MB, Canada; Janjic, S., University of Manitoba, Winnipeg, MB, Canada","In this paper we consider the problem of visual saliency modeling, including both human gaze prediction and salient object segmentation. The overarching goal of the paper is to identify high level considerations relevant to deriving more sophisticated visual saliency models. A deep learning model based on fully convolutional networks (FCNs) is presented, which shows very favorable performance across a wide variety of benchmarks relative to existing proposals. We also demonstrate that the manner in which training data is selected, and ground truth treated is critical to resulting model behaviour. Recent efforts have explored the relationship between human gaze and salient objects, and we also examine this point further in the context of FCNs. Close examination of the proposed and alternative models serves as a vehicle for identifying problems important to developing more comprehensive models going forward. © 2016 IEEE.",,"Benchmarking; Computer vision; Image segmentation; Semantics; Visualization; Comprehensive model; Convolutional networks; Deep learning; Feature contrasts; Modeling behaviour; Salient objects; Training data; Visual saliency model; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-84986246966
"Yan B., Yu S., Liu M., Liu X.","35072097900;57193063028;57193058171;57193060016;","Gaze estimation method based on EOG signals",2016,"Proceedings - 2016 6th International Conference on Instrumentation and Measurement, Computer, Communication and Control, IMCCC 2016",,,"7774817","443","446",,,"10.1109/IMCCC.2016.51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010426748&doi=10.1109%2fIMCCC.2016.51&partnerID=40&md5=88d490591cf01e697da966668d80600c","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Reliability and Systems Engineering, Beihang University, Beijing, China","Yan, B., School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Yu, S., School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Liu, M., School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Liu, X., School of Reliability and Systems Engineering, Beihang University, Beijing, China","Electrooculogram (EOG) is an important bioelectricity signal generated by physical activities of eyeballs. Information about gaze position can be obtained by analyzing EOG signals, which can lay a foundation for a novel way of human-computer interaction (HCI) based on EOG signals. In this paper, we propose a gaze estimation method based on EOG signals. After filtering and amplifying the acquired EOG signals, framing and normalization is done to make the signals easier to classify. The wavelet coefficients are extracted as the feature vectors, and BP classifier is trained to classify EOG signals of different directions. The results show that this method works well with different people and the average accuracy achieves 96% © 2016 IEEE.","BP neural work; EOG signal; Gaze estimation method; Wavelet coefficient","Electrophysiology; Human computer interaction; Wavelet transforms; BP neural; Electro-oculogram; EOG signal; Feature vectors; Gaze estimation; Human computer interaction (HCI); Physical activity; Wavelet coefficients; Signal processing",Conference Paper,"Final","",Scopus,2-s2.0-85010426748
"Narayanan A., Kaimal R.M., Bijlani K.","57205486682;6603335048;35721404600;","Estimation of driver head yaw angle using a generic geometric model",2016,"IEEE Transactions on Intelligent Transportation Systems","17","12","7464887","3446","3460",,14,"10.1109/TITS.2016.2551298","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966318465&doi=10.1109%2fTITS.2016.2551298&partnerID=40&md5=879ad99044acc96de45b1a0047ce7ba3","Amrita E-Learning Research Laboratory, Department of Computer Science, Amrita Vishwa Vidyapeetham, Amritapuri Campusc, Kollam, 690 525, India; Department of Computer Science, Amrita Vishwa Vidyapeetham, Amritapuri Campus, Kollam, 690 525, India; Amrita E-Learning Research Laboratory, Amrita Vishwa Vidyapeetham, Amritapuri, Kollam, 690 525, India","Narayanan, A., Amrita E-Learning Research Laboratory, Department of Computer Science, Amrita Vishwa Vidyapeetham, Amritapuri Campusc, Kollam, 690 525, India; Kaimal, R.M., Department of Computer Science, Amrita Vishwa Vidyapeetham, Amritapuri Campus, Kollam, 690 525, India; Bijlani, K., Amrita E-Learning Research Laboratory, Amrita Vishwa Vidyapeetham, Amritapuri, Kollam, 690 525, India","Head yaw angle is an important indicator of a driver's distraction. Forward collision warning systems incorporate head yaw angle estimators for detecting driver's head-off-road glances. This paper presents a generic geometric model for head yaw estimation. The generic model is customized into 12 different models. The performance of the cylindrical and ellipsoidal models is improved by introducing nose projection and truncation. The yaw estimation problem is formulated using a single variable. This formulation provides better visualization of the cylindrical and ellipsoidal models. It is shown that the ellipsoidal model in literature is indeed a cylindrical model with nose projection. We have also proved that, under the ellipsoidal/cylindrical framework, the estimated yaw angle is independent of shifting the center of rotation along depth. Four previous works are shown as the customization of the proposed generic model. The performance of the proposed models are evaluated using four standard head pose datasets and an ADAS dataset. The proposed nose-projected truncated ellipsoidal model outperforms the state-of-the-art geometric models. Further, we have studied the impact of model parameters and inaccurate facial feature localization through simulation experiments. © 2000-2011 IEEE.","Driver inattention detection; gaze estimation; geometric models; yaw angle estimation","Alarm systems; Center of rotation; Cylindrical models; Ellipsoidal model; Estimation problem; Facial feature localization; Forward collision warning system; Generic modeling; Geometric modeling; Geometry",Article,"Final","",Scopus,2-s2.0-84966318465
"Yilmaz C.M., Kose C.","56246395000;6602451231;","Local binary pattern histogram features for on-screen eye-gaze direction estimation and a comparison of appearance based methods",2016,"2016 39th International Conference on Telecommunications and Signal Processing, TSP 2016",,,"7760973","693","696",,3,"10.1109/TSP.2016.7760973","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006699745&doi=10.1109%2fTSP.2016.7760973&partnerID=40&md5=f7c6e65889e0218b3f2ffaefd0c4af81","Department of Computer Engineering, Karadeniz Technical University, Trabzon, Turkey","Yilmaz, C.M., Department of Computer Engineering, Karadeniz Technical University, Trabzon, Turkey; Kose, C., Department of Computer Engineering, Karadeniz Technical University, Trabzon, Turkey","Human Computer Interaction (HCI) has become an important focus of both computer science researches and industrial applications. And, on-screen gaze estimation is one of the hottest topics in this rapidly growing field. Eye-gaze direction estimation is a sub-research area of on-screen gaze estimation and the number of studies that focused on the estimation of on-screen gaze direction is limited. Due to this, various appearance-based video-oculography methods are investigated in this work. Firstly, a new dataset is created via user images taken from daylight censored cameras located at computer screen. Then, Local Binary Pattern Histogram (LBPH), which is used in this work for the first time to obtain on-screen gaze direction information, and Principal Component Analysis (PCA) methods are employed to extract image features. And, parameter optimized Support Vector Machine (SVM), Artificial Neural Networks (ANNs) and k-Nearest Neighbor (k-NN) learning methods are adopted in order to estimate on-screen gaze direction. Finally, these methods' abilities to correctly estimate the on-screen gaze direction are compared using the resulting classification accuracies of applied methods and previous works. The best classification accuracy of 96.67% is obtained when using LBPH and SVM method pair which is better than previous works. The results also show that appearance based methods are pretty applicable for estimating on-screen gaze direction. © 2016 IEEE.","Appearance-based methods; Gaze direction estimation; Gaze estimation; Human computer interaction; Video-oculography","Binary images; Graphic methods; Nearest neighbor search; Neural networks; Principal component analysis; Signal processing; Support vector machines; Video recording; Appearance-based methods; Classification accuracy; Computer science research; Gaze direction; Gaze estimation; Human computer interaction (HCI); Local binary patterns; Video oculography; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-85006699745
"Palinko O., Rea F., Sandini G., Sciutti A.","35099082400;54386065400;35280694500;36957816700;","A Robot reading human gaze: Why eye tracking is better than head tracking for human-robot collaboration",2016,"IEEE International Conference on Intelligent Robots and Systems","2016-November",,"7759741","5048","5054",,44,"10.1109/IROS.2016.7759741","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006489261&doi=10.1109%2fIROS.2016.7759741&partnerID=40&md5=d41175f0784ddea0b425e8f07a0c8dbe","Robotics, Brain and Cognitive Sciences Department, Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy","Palinko, O., Robotics, Brain and Cognitive Sciences Department, Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Rea, F., Robotics, Brain and Cognitive Sciences Department, Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Sandini, G., Robotics, Brain and Cognitive Sciences Department, Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Sciutti, A., Robotics, Brain and Cognitive Sciences Department, Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy","Robots are at the position to become our everyday companions in the near future. Still, many hurdles need to be cleared to achieve this goal. One of them is the fact that robots are still not able to perceive some important communication cues naturally used by humans, e.g. gaze. In the recent past, eye gaze in robot perception was substituted by its proxy, head orientation. Such an approach is still adopted in many applications today. In this paper we introduce performance improvements to an eye tracking system we previously developed and use it to explore if this approximation is appropriate. More precisely, we compare the impact of the use of eye- or head-based gaze estimation in a human robot interaction experiment with the iCub robot and naïve subjects. We find that the possibility to exploit the richer information carried by eye gaze has a significant impact on the interaction. As a result, our eye tracking system allows for a more efficient human-robot collaboration than a comparable head tracking approach, according to both quantitative measures and subjective evaluation by the human participants. © 2016 IEEE.",,"Human computer interaction; Intelligent robots; Robots; Tracking (position); Eye tracking systems; Eye-tracking; Gaze estimation; Head tracking; Human-robot collaboration; Quantitative measures; Robot perception; Subjective evaluations; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-85006489261
"Maralappanavar S., Behera R., Mudenagudi U.","57217925275;56747543400;6508142222;","Driver's distraction detection based on gaze estimation",2016,"2016 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2016",,,"7732431","2489","2494",,8,"10.1109/ICACCI.2016.7732431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007392552&doi=10.1109%2fICACCI.2016.7732431&partnerID=40&md5=388b1f461e01924ce142170427d30ef7","Department of Electronics and Communication, B.V.B College of Engineeering, Hubli, India; KPIT Technologies Ltd., Pune, India","Maralappanavar, S., Department of Electronics and Communication, B.V.B College of Engineeering, Hubli, India; Behera, R., KPIT Technologies Ltd., Pune, India; Mudenagudi, U., Department of Electronics and Communication, B.V.B College of Engineeering, Hubli, India","Drivers easily get distracted by the activities happening around them such as texting, talking on mobile phone or talking to the neighbouring person. All these activities take driver's attention away from the road which may lead to accidents, cause harm to the driver, pedestrians and also other vehicles on the road. In this paper, a method is proposed to estimate the gaze of the driver and determine whether the driver is distracted or not. Driver's gaze direction is estimated as an indicator of his attentiveness. The driver's gaze estimation is done by detecting the gaze with the help of face, eye, pupil, eye corners and then the detected gaze is then categorized as whether the driver is distracted or not. The algorithm is developed in OpenCV and tested on a CPU platform (Intel core with 4 GB RAM). The processing time taken for the execution of a single frame is around one second. The gaze detection accuracy obtained is 75%. © 2016 IEEE.","Driver's Distraction; Eye detection; Gaze Estimation; Viola-Jones","Accidents; Eye protection; Information science; Roads and streets; Driver's Distraction; Eye detection; Gaze detection; Gaze direction; Gaze estimation; Processing time; Single frames; Viola jones; Edge detection",Conference Paper,"Final","",Scopus,2-s2.0-85007392552
"Xiao D., Feng C.","23398884000;57192109761;","Detection of drivers visual attention using smartphone",2016,"2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery, ICNC-FSKD 2016",,,"7603247","630","635",,10,"10.1109/FSKD.2016.7603247","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997684047&doi=10.1109%2fFSKD.2016.7603247&partnerID=40&md5=c9733af41b8d9accb28cb591674ae66d","College of Computer Science and Electronic Engineering, Hunan University, Changsha, China","Xiao, D., College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; Feng, C., College of Computer Science and Electronic Engineering, Hunan University, Changsha, China","A lot of traffic accidents are caused by driver distraction, while most existing driver attention assistance solutions need specialized hardware, which limit their wide accessibility. We present a convenient driver attention detecting system based on smartphones with dual cameras. Firstly, a feature detector describing pupil location, yaw and pitch angle of eyes, position and size of face detected with the front camera is proposed to track drivers gaze direction. Secondly, nine safe gaze areas and one unsafe gaze area are defined. Then a SVM classifier is used to estimate gaze area. At the same time, motion objects in the view of road are detected with the rear camera using optical flow methodology combined dynamic background compensation. At last, an inference module is proposed to quantify the level of drivers visual attention by inferring whether the motion objects are located in the drivers gaze areas. Experimental results on an android pad show that the proposed system can detect driver visual attention in real driving scenarios efficiently. © 2016 IEEE.","Driver assistance; Dual camera; Dynamic background compensation; Gaze estimate; Optical flow; Visual attention","Alpha particles; Automobile drivers; Cameras; Feature extraction; Fuzzy systems; Motion compensation; Motion estimation; Optical flows; Smartphones; Driver assistance; Dual cameras; Dynamic background; Gaze estimate; Visual Attention; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-84997684047
"Sugano Y., Zhang X., Bulling A.","7005470045;57142162900;6505807414;","AggreGaze: Collective estimation of audience attention on public displays",2016,"UIST 2016 - Proceedings of the 29th Annual Symposium on User Interface Software and Technology",,,,"821","831",,31,"10.1145/2984511.2984536","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995812825&doi=10.1145%2f2984511.2984536&partnerID=40&md5=feaef9c16517b376f62189156febd637","Max Planck Institute for Informatics, Germany","Sugano, Y., Max Planck Institute for Informatics, Germany; Zhang, X., Max Planck Institute for Informatics, Germany; Bulling, A., Max Planck Institute for Informatics, Germany","Gaze is frequently explored in public display research given its importance for monitoring and analysing audience attention. However, current gaze-enabled public display interfaces require either special-purpose eye tracking equipment or explicit personal calibration for each individual user. We present AggreGaze, a novel method for estimating spatio-temporal audience attention on public displays. Our method requires only a single off-the-shelf camera attached to the display, does not require any personal calibration, and provides visual attention estimates across the full display. We achieve this by 1) compensating for errors of state-of-the-art appearance-based gaze estimation methods through on-site training data collection, and by 2) aggregating uncalibrated and thus inaccurate gaze estimates of multiple users into joint attention estimates. We propose different visual stimuli for this compensation: a standard 9-point calibration, moving targets, text and visual stimuli embedded into the display content, as well as normal video content. Based on a two-week deployment in a public space, we demonstrate the effectiveness of our method for estimating attention maps that closely resemble ground-truth audience gaze distributions.","Eye tracking; Gaze estimation; Public displays; Visual attention","Behavioral research; Calibration; Estimation; Appearance based; Eye-tracking; Gaze estimation; Joint attention; On-site training; Public display; State of the art; Visual Attention; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-84995812825
"Wang Y., Shen T., Yuan G., Bian J., Fu X.","55211773900;57191893663;36877169400;57200854878;7402204912;","Appearance-based gaze estimation using deep features and random forest regression",2016,"Knowledge-Based Systems","110",,,"293","301",,33,"10.1016/j.knosys.2016.07.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994551714&doi=10.1016%2fj.knosys.2016.07.038&partnerID=40&md5=cc5b8620708ea820bcf350fce6d7eb1e","School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Wang, Y., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China, Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Shen, T., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Yuan, G., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Bian, J., School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian, 116024, China; Fu, X., Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","Conventional appearance-based gaze estimation methods employ local or global features as eye gaze appearance descriptor. But these methods don't work well under natural light with free head movement. To solve this problem, we present an appearance-based gaze estimation method using deep feature representation and feature forest regression. The deep feature is learned through hierarchical extraction of deep Convolutional Neural Network (CNN). And random forest regression with cluster-to-classify node splitting rules is used to take advantage of data distribution in sparse feature space. Experimental results demonstrate that the deep feature has a better performance than local features on calibrated gaze regression. The combination of deep features and random forest regression provides an effective solution for gaze estimation in a natural environment. © 2016","Appearance; CNN; Deep features; Gaze estimation; Random forest","Decision trees; Neural networks; Appearance; Convolutional neural network; Deep features; Feature representation; Gaze estimation; Hierarchical extraction; Natural environments; Random forests; Regression analysis",Article,"Final","",Scopus,2-s2.0-84994551714
"Cazzato D., Evangelista A., Leo M., Carcagnì P., Distante C.","55866556300;56967728900;7006471658;23003296600;55884135100;","A low-cost and calibration-free gaze estimator for soft biometrics: An explorative study",2016,"Pattern Recognition Letters","82",,,"196","206",,7,"10.1016/j.patrec.2015.10.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947565345&doi=10.1016%2fj.patrec.2015.10.015&partnerID=40&md5=ba05ff17fe68d6ed62010fff95c84ede","Faculty of Engineering, University of Salento, Lecce 73100, Italy; National Research Council of Italy – Institute of Optics, Arnesano (LE) 73010, Italy","Cazzato, D., Faculty of Engineering, University of Salento, Lecce 73100, Italy; Evangelista, A., Faculty of Engineering, University of Salento, Lecce 73100, Italy; Leo, M., National Research Council of Italy – Institute of Optics, Arnesano (LE) 73010, Italy; Carcagnì, P., National Research Council of Italy – Institute of Optics, Arnesano (LE) 73010, Italy; Distante, C., National Research Council of Italy – Institute of Optics, Arnesano (LE) 73010, Italy","Soft biometric systems have spread among recent years, both for powering classical biometrics, as well as stand alone solutions with several application scopes ranging from digital signage to human-robot interaction. Among all, in the recent years emerged the possibility to consider as a soft biometrics also the temporal evolution of the human gaze and some recent works in the literature explored this exciting research line by using expensive and (perhaps) unsafe devices which require user cooperation to be calibrated. This work is instead the first attempt to perform biometric identification of individuals on the basis of data acquired by a low-cost, non-invasive, safe and calibration-free gaze estimation framework consisting of two main components conveniently combined and performing user's head pose estimation and eyes’ pupil localization on data acquired by a RGB-D device. The experimental evidence of the feasibility of using the proposed framework as soft-biometrics is given on a set of users watching three benchmark heterogeneous videos in an unconstrained environment. © 2015 Elsevier B.V.","Free gaze estimation; Head pose estimation; Pupil detection; Soft-biometrics","Calibration; Cost estimating; Human robot interaction; Image recognition; Motion estimation; Robots; Biometric identifications; Experimental evidence; Gaze estimation; Head Pose Estimation; Pupil detection; Soft biometrics; Temporal evolution; Unconstrained environments; Biometrics",Article,"Final","",Scopus,2-s2.0-84947565345
"Xiong C., Huang L., Liu C.","56097664100;56566995800;9637739900;","Remote gaze estimation based on 3D face structure and iris centers under natural light",2016,"Multimedia Tools and Applications","75","19",,"11785","11799",,4,"10.1007/s11042-015-2600-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928330693&doi=10.1007%2fs11042-015-2600-y&partnerID=40&md5=9a7a50ca00e9cae33c371a2e054c4691","Institute of Automation, Chinese Academy of Sciences, No. 95, Zhongguancun East Road, Beijing, Haidian District  100190, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","Xiong, C., Institute of Automation, Chinese Academy of Sciences, No. 95, Zhongguancun East Road, Beijing, Haidian District  100190, China; Huang, L., Institute of Automation, Chinese Academy of Sciences, Beijing, China; Liu, C., Institute of Automation, Chinese Academy of Sciences, Beijing, China","Remote gaze estimation under natural light is still a challenging problem. Appearance based methods are seriously sensitive to illumination variation in the visual spectrum and usually can hardly handle the problem of head movements. And most existing feature-based gaze estimation methods strongly rely on cornea reflections, which are unstable to glasses, head movements and especially useless for natural light condition. In this paper, we propose a novel feature based gaze estimation method without use of cornea reflections. A stereo camera system is built for the proposed method. Firstly, 3D Active Shape Models (ASM) is reconstructed using stereo vision to represent 3D face structure. Then, without use of cornea reflections, a 3D Iris-Eye-Contours based descriptor is proposed to represent human gaze information. Iris centers are used in natural light just like the pupil centers in condition of near-infrared light. What’s more, precise estimation of head poses based on 3D face structure is employed to rectify the 3D iris centers and eye contours for improving the ability of tolerance to head movements. Experiments on several subjects show that the system is accurate and allows natural head movements under natural light. © 2015, Springer Science+Business Media New York.","3D face structure; Gaze estimation; Head pose estimation; Iris center location","Eye movements; Face recognition; Image recognition; Infrared devices; Stereo image processing; Stereo vision; Three dimensional computer graphics; 3d active shape models; 3D faces; Appearance-based methods; Center locations; Gaze estimation; Head Pose Estimation; Illumination variation; Remote gaze estimation; Motion estimation",Article,"Final","",Scopus,2-s2.0-84928330693
"Appel T., Santini T., Kasneci E.","57191500368;54881866000;56059892600;","Brightness-And motion-Based blink detection for head-mounted eye trackers",2016,"UbiComp 2016 Adjunct - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing",,,,"1726","1735",,11,"10.1145/2968219.2968341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991094178&doi=10.1145%2f2968219.2968341&partnerID=40&md5=9fa8aaefc7c722453ab482102bcecc9c","Perception Engineering, University of Töbingen, Germany","Appel, T., Perception Engineering, University of Töbingen, Germany; Santini, T., Perception Engineering, University of Töbingen, Germany; Kasneci, E., Perception Engineering, University of Töbingen, Germany","Blinks are an indicator for fatigue or drowsiness and can assis in the diagnose of mental disorders, such as schizophrenia Additionally, a blink that obstructs the pupil impairs th performance of other eye-Tracking algorithms, such as pupi detection, and often results in noise to the gaze estimation In this paper, we present a blink detection algorithm that i tailored towards head-mounted eye trackers and is robust t calibration-based variations like translation or rotation of th eye. The proposed approach reached 96,35% accuracy fo a realistic and challenging data set and in real-Time even o low-end devices, rendering the proposed method suited for pervasive eye tracking. © 2016 ACM.","Blink detection; Image processing; Pervasive eye tracking; Real time","Eye movements; Image processing; Blink detections; End-devices; Eye trackers; Eye-tracking; Gaze estimation; Mental disorders; Pervasive eye tracking; Real time; Ubiquitous computing",Conference Paper,"Final","",Scopus,2-s2.0-84991094178
"Barz M., Sonntag D.","57189847803;12241487800;","Gaze-guided object classification using deep neural networks for attention-based computing",2016,"UbiComp 2016 Adjunct - Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing",,,,"253","256",,15,"10.1145/2968219.2971389","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991069525&doi=10.1145%2f2968219.2971389&partnerID=40&md5=381bb812a59fb68a93bfb4b8d764f6bb","German Research Center for Artificial Intelligence, Stuhlsatzenhausweg 3, Saarbruecken, 66123, Germany","Barz, M., German Research Center for Artificial Intelligence, Stuhlsatzenhausweg 3, Saarbruecken, 66123, Germany; Sonntag, D., German Research Center for Artificial Intelligence, Stuhlsatzenhausweg 3, Saarbruecken, 66123, Germany","Recent advances in eye tracking technologies opened the way to design novel attention-based user interfaces. This is promising for pro-Active and assistive technologies for cyber-physical systems in the domains of, e.g., healthcare and industry 4.0. Prior approaches to recognize a user's attention are usually limited to the raw gaze signal or sensors in instrumented environments. We propose a system that (1) incorporates the gaze signal and the egocentric camera of the eye tracker to identify the objects the user focuses at; (2) employs object classification based on deep learning which we recompiled for our purposes on a GPU-based image classification server; (3) detects whether the user actually draws attention to that object; and (4) combines these modules for constructing episodic memories of egocentric events in real-Time. © 2016 ACM.","Eye Tracking; Gaze-Based Interaction; Object Classification; Visual Attention","Behavioral research; Embedded systems; Stereo vision; Ubiquitous computing; User interfaces; Assistive technology; Cyber physical systems (CPSs); Eye tracking technologies; Eye-tracking; Gaze-based interaction; Instrumented environments; Object classification; Visual Attention; Image classification",Conference Paper,"Final","",Scopus,2-s2.0-84991069525
"Lander C.","55785449200;","Methods for calibration free and multi-user eye tracking",2016,"Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct, MobileHCI 2016",,,,"899","900",,5,"10.1145/2957265.2963116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991101939&doi=10.1145%2f2957265.2963116&partnerID=40&md5=e021f95674ada55ac8ad2baf1ffa90a8","DFKI GmbH, Saarland Informatics Campus, Stuhlsatzenhausweg 3, Saarbrücken, 66123, Germany","Lander, C., DFKI GmbH, Saarland Informatics Campus, Stuhlsatzenhausweg 3, Saarbrücken, 66123, Germany","Human beings sense and perceive most of the world through their eyes. The point of gaze clearly reflects our visual attention indicating our interests. Hence gaze can be used as a powerful tool in different research areas (e.g., marketing, psychology). The progress made over the years in eye tracking enables the creation of gaze-based interactive interfaces. However, these interfaces lack of generic usability outside a controlled environment in a spontaneous pervasive way. The main objective of this research is to investigate eye-tracking technologies by means of calibration. Since calibration is user, location, orientation and target dependent, it prevents from Multi-User interaction and gaze estimation on multiple various objects (e.g., multiple screens of different sizes). Tackling these issues, new mobile as well as remote interfaces are explored and new design spaces are opened.","Calibration; Eye tracking; Gaze; Multi-user","Behavioral research; Calibration; Mobile devices; Controlled environment; Eye tracking technologies; Eye-tracking; Gaze; Interactive interfaces; Multi-user; Multi-user interaction; Remote interfaces; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84991101939
"Pichitwong W., Chamnongthai K.","57191333091;57202765861;","3-D gaze estimation by stereo gaze direction",2016,"2016 13th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, ECTI-CON 2016",,,"7561491","","",,3,"10.1109/ECTICon.2016.7561491","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988847750&doi=10.1109%2fECTICon.2016.7561491&partnerID=40&md5=9123075e82e51c0fc1e0e19170a97434","Dept. Electronic and Telecommunication Engineering, King Mongkut's University of Technology Thonburi, Bangkok, Thailand","Pichitwong, W., Dept. Electronic and Telecommunication Engineering, King Mongkut's University of Technology Thonburi, Bangkok, Thailand; Chamnongthai, K., Dept. Electronic and Telecommunication Engineering, King Mongkut's University of Technology Thonburi, Bangkok, Thailand","The propose of this study aims to use eyeball model approach and algebra-geometric to analyze 3-D gaze estimation. Which stereo gaze directions calculate from 3-D coordinates of the value of Cornea Center and Pupil Center of left and right eyes. Then stereo gaze directions are used to get point of gaze (POG) of 3-D coordination. The experimental is designed in three conditions, (i) no head movement (ii) yaw head movement and (iii) roll head movement. The results from simulation of 3-D gaze estimation show POG accuracy. © 2016 IEEE.","3-D gaze estimation; stereo gaze direction","Computers; Electrical engineering; Mathematical techniques; Gaze direction; Gaze estimation; Head movements; Model approach; Point of gaze; Pupil centers; Computer science",Conference Paper,"Final","",Scopus,2-s2.0-84988847750
"Lu F., Gao Y., Chen X.","54956194300;35298851400;13410318100;","Estimating 3D gaze directions using unlabeled eye images via synthetic iris appearance fitting",2016,"IEEE Transactions on Multimedia","18","9","7484318","1772","1782",,14,"10.1109/TMM.2016.2576284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983441879&doi=10.1109%2fTMM.2016.2576284&partnerID=40&md5=4d505866d1cf97548588da92a35c6766","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; School of Software, Tsinghua University, Beijing, 100084, China","Lu, F., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China, International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; Gao, Y., School of Software, Tsinghua University, Beijing, 100084, China; Chen, X., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China","Estimating three-dimensional (3D) human eye gaze by capturing a single eye image without active illumination is challenging. Although the elliptical iris shape provides a useful cue, existing methods face difficulties in ellipse fitting due to unreliable iris contour detection. These methods may fail frequently especially with low resolution eye images. In this paper, we propose a synthetic iris appearance fitting (SIAF) method that is model-driven to compute 3D gaze direction from iris shape. Instead of fitting an ellipse based on exactly detected iris contour, our method first synthesizes a set of physically possible iris appearances and then optimizes inside this synthetic space to find the best solution to explain the captured eye image. In this way, the solution is highly constrained and guaranteed to be physically feasible. In addition, the proposed advanced image analysis techniques also help the SIAF method be robust to the unreliable iris contour detection. Furthermore, with multiple eye images, we propose a SIAF-joint method that can further reduce the gaze error by half, and it also resolves the binary ambiguity which is inevitable in conventional methods based on simple ellipse fitting. © 1999-2012 IEEE.","Gaze estimation; iris fitting; three-dimensional (3D) human gaze; unlabeled eye images","Multimedia systems; Signal processing; Active illumination; Contour detection; Conventional methods; Eye images; Gaze estimation; Image analysis techniques; iris fitting; Threedimensional (3-d); Geometry",Article,"Final","",Scopus,2-s2.0-84983441879
"Chinsatitf W., Saitoh T.","57200653220;13006364300;","Improvement of eye detection performance for inside-out camera",2016,"2016 IEEE/ACIS 15th International Conference on Computer and Information Science, ICIS 2016 - Proceedings",,,"7550794","","",,,"10.1109/ICIS.2016.7550794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987958832&doi=10.1109%2fICIS.2016.7550794&partnerID=40&md5=d7e264afc540050f752cfd6955ba1329","Kyushu Institute of Technology, Iizuka, Japan","Chinsatitf, W., Kyushu Institute of Technology, Iizuka, Japan; Saitoh, T., Kyushu Institute of Technology, Iizuka, Japan","This paper presents a fast and precious eye detection technique by using gradient value for improve the performance of inside-out camera. The propose methods using gradient vector to estimate a temporary pupil position before using ellipse fitting to detect actual eye position. We collected 280 eye images that capture from our inside-out camera, and apply the proposed method and previous method to the dataset. The experimental results show that the propose method is improving performance of wearable gaze estimation system. © 2016 IEEE.","eye detection; Eye image; gaze estimation; gradient value; inside-out camera","Eye protection; Gesture recognition; Information science; Ellipse fitting; Eye detection; Eye images; Eye position; Gaze estimation; Gradient vectors; Improving performance; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-84987958832
"Guojun Y., Saniie J.","56927104100;7004038692;","Eye tracking using monocular camera for gaze estimation applications",2016,"IEEE International Conference on Electro Information Technology","2016-August",,"7535254","292","296",,4,"10.1109/EIT.2016.7535254","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984636522&doi=10.1109%2fEIT.2016.7535254&partnerID=40&md5=9767dd252cfdac0f2ef2f7c8d246a18e","Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, IL, United States","Guojun, Y., Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, IL, United States; Saniie, J., Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, IL, United States","Gaze estimation reveals valuable information in terms of human behavior. For example, gaze location can be used as a mean of human computer interaction. In practice, locating the eyes position is a challenging task. Moreover, it is more challenging to estimate the location of the gaze. It requires a system to extract the information of the human eye location then estimate the direction of the gaze. There are existing systems that achieve similar tasks. Many of these systems rely on complex systems, such as wearable sensors or an infrared camera to find the exact focus of eyes gaze. Consequently, these systems are not only complex but also suitable for limited applications. In this paper, we present efficient eye focus methods using a monocular camera, which brings more flexibility and convenience to the system. Three different algorithms for locating the eye's focus have been examined and compared. These algorithms are highly efficient and able to discern the location of the eye's focus in real-time using a regular laptop or desktop computers. © 2016 IEEE.",,"Behavioral research; Cameras; Location; Personal computers; Existing systems; Eye-tracking; Gaze estimation; Human behaviors; Human eye; Infra-red cameras; Monocular cameras; Real time; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84984636522
"Li J., Li S.","57203736355;16202805500;","Two-phase approach - Calibration and iris contour estimation - For gaze tracking of head-mounted eye camera",2016,"Proceedings - International Conference on Image Processing, ICIP","2016-August",,"7532937","3136","3140",,4,"10.1109/ICIP.2016.7532937","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006837027&doi=10.1109%2fICIP.2016.7532937&partnerID=40&md5=7bec9567395310f36155a6e61f21616c","Graduate School of Engineering, Tottori University, Tottori, Japan; Graduate School of Information Sciences, Hiroshima City University, Hiroshima, Japan","Li, J., Graduate School of Engineering, Tottori University, Tottori, Japan; Li, S., Graduate School of Information Sciences, Hiroshima City University, Hiroshima, Japan","The fitting of an ellipse to the iris contour is usually performed using five unknown parameters. In this study, we divide the continuous gaze estimation of a head-mounted eye camera into two phases. One phase, known as the calibration phase, is used to estimate the eyeball center position in relation to the coordinate system of the head-mounted eye camera. The other phase is used to fit the iris contour in 2D images employing only two parameters for gaze estimation. As seen from the experimental results, the proposed method demonstrates both credible eyeball center estimation and an accurate iris contour estimation in comparison with the conventional five unknown parameter approach. © 2016 IEEE.","Eyeball calibration; Gaze estimation; Head-mounted camera; Iris fitting","Calibration; Cameras; Image processing; Observability; Tracking (position); Center estimations; Co-ordinate system; Contour estimation; Gaze estimation; Gaze tracking; Head mounted Camera; Iris fitting; Two parameter; Parameter estimation",Conference Paper,"Final","",Scopus,2-s2.0-85006837027
"Chaabouni S., Benois-Pineau J., Ben Amar C.","57190285991;6701750610;25959856600;","Transfer learning with deep networks for saliency prediction in natural video",2016,"Proceedings - International Conference on Image Processing, ICIP","2016-August",,"7532629","1604","1608",,28,"10.1109/ICIP.2016.7532629","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006826726&doi=10.1109%2fICIP.2016.7532629&partnerID=40&md5=29e85a4a39094fb4471f43cd2762d787","LaBRI UMR 5800, University of Bordeaux, 351 crs de la Liberation, Talence Cedex, F33405, France; REGIM-Lab LR11ES48, University of Sfax National Engineering School of Sfax, BP1173, Sfax, 3038, Tunisia","Chaabouni, S., LaBRI UMR 5800, University of Bordeaux, 351 crs de la Liberation, Talence Cedex, F33405, France; Benois-Pineau, J., LaBRI UMR 5800, University of Bordeaux, 351 crs de la Liberation, Talence Cedex, F33405, France; Ben Amar, C., REGIM-Lab LR11ES48, University of Sfax National Engineering School of Sfax, BP1173, Sfax, 3038, Tunisia","The main purpose of transfer learning is to resolve the problem of different data distribution, generally, when the training samples of source domain are different from the training samples of the target domain. Prediction of salient areas in natural video suffers from the lack of large video benchmarks with human gaze fixations. Different databases only provide dozens up to one or two hundred of videos. The only public large database is HOLLYWOOD with 1707 videos available with gaze recordings. The main idea of this paper is to transfer the knowledge learned with the deep network on a large dataset to train the network on a small dataset to predict salient areas. The results show an improvement on two small publicly available video datasets. © 2016 IEEE.","Deep learning; Residual motion; Saliency map; Transfer learning; Visual attention","Behavioral research; Forecasting; Sampling; Deep learning; Residual motion; Saliency map; Transfer learning; Visual Attention; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85006826726
"Anantrasirichai N., Gilchrist I.D., Bull D.R.","15924749200;7005085823;57203057734;","Visual salience and priority estimation for locomotion using a deep convolutional neural network",2016,"Proceedings - International Conference on Image Processing, ICIP","2016-August",,"7532628","1599","1603",,1,"10.1109/ICIP.2016.7532628","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006821412&doi=10.1109%2fICIP.2016.7532628&partnerID=40&md5=176091d508857d7640ab641d3e361a4b","Bristol Vision Institute, University of Bristol, Bristol, BS8 1UB, United Kingdom","Anantrasirichai, N., Bristol Vision Institute, University of Bristol, Bristol, BS8 1UB, United Kingdom; Gilchrist, I.D., Bristol Vision Institute, University of Bristol, Bristol, BS8 1UB, United Kingdom; Bull, D.R., Bristol Vision Institute, University of Bristol, Bristol, BS8 1UB, United Kingdom","This paper presents a novel method of salience and priority estimation for the human visual system during locomotion. This visual information contains dynamic content derived from a moving viewpoint. The priority map, ranking key areas on the image, is created from probabilities of gaze fixations, merged from bottom-up features and top-down control on the locomotion. Two deep convolutional neural networks (CNNs), inspired by models of the primate visual system, are employed to capture local salience features and compute probabilities. The first network operates through the foveal and peripheral areas around the eye positions. The second network obtains the importance of fixated points that have long durations or multiple visits, of which such areas need more times to process or to recheck to ensure smooth locomotion. The results show that our proposed method outperforms the state-of-the-art by up to 30 %, computed from average of four well known metrics for saliency estimation. © 2016 IEEE.","Convolutional neural network; Deep learning; Locomotion; Salience","Biped locomotion; Convolution; Neural networks; Convolutional neural network; Deep learning; Dynamic content; Human Visual System; Salience; State of the art; Top-down control; Visual information; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85006821412
"Vater S., León F.P.","56771184400;6602431857;","Combining isophote and cascade classifier information for precise pupil localization",2016,"Proceedings - International Conference on Image Processing, ICIP","2016-August",,"7532425","589","593",,3,"10.1109/ICIP.2016.7532425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006817524&doi=10.1109%2fICIP.2016.7532425&partnerID=40&md5=f3c2e1979930bcafbcafdc4e3a9d422e","Institute of Industrial Information Technology (IIIT), Karlsruhe Institute of Technology (KIT), Karlsruhe, 76187, Germany","Vater, S., Institute of Industrial Information Technology (IIIT), Karlsruhe Institute of Technology (KIT), Karlsruhe, 76187, Germany; León, F.P., Institute of Industrial Information Technology (IIIT), Karlsruhe Institute of Technology (KIT), Karlsruhe, 76187, Germany","This paper investigates precise pupil center localization in low-resolution images. Being an essential preprocessing step in many applications such as gaze estimation, face alignment as well as human-computer interaction, robust, precise, and efficient methods are necessary. We present a method for accurate eye center localization operating with images from simple off-the-shelf hardware such as webcams. The proposed method utilizes the isophote representation that allows to find pupil center candidates by introducing a novel voting mechanism for pixel weights. To cope with multiple local maxima resulting from the isophote voting map, we combine this information with quasi-continuous responses of a modified cascade classifier framework utilizing appearance-based features. We conduct experiments on the BioID database and show that the presented method outperforms results of existing methods within an error range of the pupil diameter while running at 10 fps on a standard CPU with 3.3 GHz in a Matlab implementation. © 2016 IEEE.","Eye localization; Feature extraction; Image processing; Machine vision; Object detection","Computer vision; Face recognition; Feature extraction; Human computer interaction; Image processing; Object detection; Cascade classifiers; Eye localization; Low resolution images; Off-the-shelf hardwares; Pre-processing step; Pupil center localizations; Pupil localization; Voting mechanism; Classification (of information)",Conference Paper,"Final","",Scopus,2-s2.0-85006817524
"Ye N., Tao X., Dong L., Ge N.","57202057254;57208894708;56419177800;7005864121;","Mouse calibration aided real-time gaze estimation based on boost Gaussian Bayesian learning",2016,"Proceedings - International Conference on Image Processing, ICIP","2016-August",,"7532869","2797","2801",,3,"10.1109/ICIP.2016.7532869","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006804262&doi=10.1109%2fICIP.2016.7532869&partnerID=40&md5=029815eb17b96e35c12132683bb1625f","Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China","Ye, N., Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China; Tao, X., Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China; Dong, L., Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China; Ge, N., Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, China","In this paper, we propose a novel gaze estimation method to evaluate the attention span of users upon on-screen content via a single webcam. Our method is based on supervised descent method for eye region of interest (ROI) extraction. Then, boost Gaussian Bayesian regressors are applied to learn a robust mapping from the input eye ROI to gaze coordinates. To get enough training samples, we implant our scheme as a plug-in into web browsers for data collection from users without bothering. To improve accuracy, we also introduce mouse click to help train the regressors. Experiment results show that our method outperforms the existing method and can provide gaze estimation data for user behaviour analysis in real-time implementation. © 2016 IEEE.","Boost Gaussian Bayesian learning; Gaze estimation; Single webcam","Behavioral research; Gaussian distribution; Image segmentation; Mammals; Real time control; Web browsers; Bayesian learning; Data collection; Descent method; Gaze estimation; Real-time implementations; Single webcam; Training sample; User behaviour; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-85006804262
"Jariwala K., Dalal U., Vincent A.","57191608233;56032021200;57203514766;","A robust eye gaze estimation using geometric eye features",2016,"2016 3rd International Conference on Digital Information Processing, Data Mining, and Wireless Communications, DIPDMWC 2016",,,"7529379","142","147",,1,"10.1109/DIPDMWC.2016.7529379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992036178&doi=10.1109%2fDIPDMWC.2016.7529379&partnerID=40&md5=3bbdf6438fe8cc122752cce20a5f2821","Computer Engineering, SVNIT, Surat, India; Electronics Engineering, SVNIT, Surat, India","Jariwala, K., Computer Engineering, SVNIT, Surat, India; Dalal, U., Electronics Engineering, SVNIT, Surat, India; Vincent, A., Computer Engineering, SVNIT, Surat, India","Gaze estimation is the process of determining the point of gaze in the space, or the visual axis of an eye. It plays an important role in representing human attention; therefore, it can be most appropriately used in Human Computer Interaction as a means of an advance computer input. Here, the focus is to develop a gaze estimation method for Human Computer Interaction using an ordinary webcam mounted on the top of the computer screen without any additional or specialized hardware. The eye center coordinates are obtained with the geometrical eye model and edge gradients. To improve the reliability, the estimates from two eye centers are combined to reduce the noise and improve the accuracy. Facial land marking is done to identify a precise reference point on the face between the nose. The ellipse fitting and RANSAC method is used to estimate the gaze coordinates and to reject the outliers. This approach can estimate the gaze coordinates with high degree of accuracy even when significant numbers of outliers are present in the data set. Several refinements such as feedback and masking, queuing and averaging are proposed to make the system more stable and useful practically. The results show that the proposed method can be successfully applied to commercial gaze tracking systems using ordinary webcams. © 2016 IEEE.","computer vision; eye tracking; image processing","Computer hardware; Computer vision; Data handling; Data mining; Image processing; Information science; Statistics; Tracking (position); Wireless telecommunication systems; Computer screens; Eye-tracking; Gaze estimation; Gaze tracking system; High degree of accuracy; Human attention; Reference points; Specialized hardware; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84992036178
"Cristina S., Camilleri K.P.","49963155000;8301303700;","Model-based head pose-free gaze estimation for assistive communication",2016,"Computer Vision and Image Understanding","149",,,"157","170",,13,"10.1016/j.cviu.2016.02.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990913598&doi=10.1016%2fj.cviu.2016.02.012&partnerID=40&md5=1f48048a8c4e1f8c02b4b6723b21f836","Department of Systems and Control Engineering, University of Malta, Msida, MSD 2080, Malta","Cristina, S., Department of Systems and Control Engineering, University of Malta, Msida, MSD 2080, Malta; Camilleri, K.P., Department of Systems and Control Engineering, University of Malta, Msida, MSD 2080, Malta","The significance of employing video-based eye-gaze tracking as an assistive tool has long been recognised, especially in the domain of human–computer interaction to assist physically challenged individuals in operating a computer by the eye movements alone. Nonetheless, several operating conditions typically associated with existing eye-gaze tracking methods, relating to constraints on the head movement and prolonged user-calibration prior to gaze estimation, need to be alleviated in order to better assist individuals with motor disabilities. In this paper, we propose a method that is based on a cylindrical head and spherical eyeballs model to estimate the three-dimensional eye-gaze under free head movement from a single camera integrated into a notebook computer, alleviating any assumptions of stationary head movement without requiring prolonged user co-operation prior to gaze estimation. The validity of the proposed method has been investigated on a publicly available data set and real-life data captured through the voluntary collaboration of a group of normal subjects and a person suffering from cerebral palsy. © 2016 Elsevier Inc.","Augmentative and alternative communication; Cerebral palsy; Eye-gaze tracking; Head pose-free; Model-based; Videooculography","Diseases; Gesture recognition; Human computer interaction; Human rehabilitation engineering; Tracking (position); Augmentative-and-alternative communication; Cerebral palsy; Eye gaze tracking; Head pose; Model-based OPC; Video oculography; Eye movements",Article,"Final","",Scopus,2-s2.0-84990913598
"Chang C.-J., Huang C.-W., Hu C.-W.","57190854771;7406880628;57190856316;","Wearable binocular eye tracking targets in 3-D environment using 2-D regression-based gaze estimation",2016,"2016 IEEE International Conference on Consumer Electronics-Taiwan, ICCE-TW 2016",,,"7521001","","",,1,"10.1109/ICCE-TW.2016.7521001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983471896&doi=10.1109%2fICCE-TW.2016.7521001&partnerID=40&md5=8760bbcc953ce44d165f9df29b81ef3e",,"Chang, C.-J.; Huang, C.-W.; Hu, C.-W.","This paper presents a binocular eye tracking device which used 2-D regression-based method to avoid the complicated vector calculations, often applied in geometry-based 3-D tracking, to increase processing speed. It is probably due to the speed increasing that enables us to do the experiments of dynamic estimation errors, which are actually like an experiment to trace a moving object in real 3-D environment if the predefined points, used for evaluating the error estimation, increased to a large number. So far, the preliminary estimations seemed acceptable. It also found 3-D estimation errors mainly caused by the 2-D calibration errors. Therefore, seeking a better 2-D calibration would become one of the important issues for further experiments. © 2016 IEEE.",,"Binoculars; Bins; Calibration; Consumer electronics; Wearable technology; 3-D environments; Calibration error; Dynamic estimation; Estimation errors; Eye tracking devices; Processing speed; Speed increasing; Vector calculations; Errors",Conference Paper,"Final","",Scopus,2-s2.0-84983471896
"Simon D., Sridharan S., Sah S., Ptucha R., Kanan C., Bailey R.","57193235677;25722300800;56131822400;6505949286;35185157400;16641965200;","Automatic scanpath generation with deep recurrent neural networks",2016,"Proceedings of the ACM Symposium on Applied Perception, SAP 2016",,,,"130","",,2,"10.1145/2931002.2948726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011964878&doi=10.1145%2f2931002.2948726&partnerID=40&md5=0d328159c6104d644fe4eeb6cd6b754e","Rochester Institute of Technology, United States","Simon, D., Rochester Institute of Technology, United States; Sridharan, S., Rochester Institute of Technology, United States; Sah, S., Rochester Institute of Technology, United States; Ptucha, R., Rochester Institute of Technology, United States; Kanan, C., Rochester Institute of Technology, United States; Bailey, R., Rochester Institute of Technology, United States","Many computer vision algorithms are biologically inspired and designed based on the human visual system. Convolutional neural networks (CNNs) are similarly inspired by the primary visual cortex in the human brain. However, the key difference between current visual models and the human visual system is how the visual information is gathered and processed. We make eye movements to collect information from the environment for navigation and task performance. We also make specific eye movements to important regions in the stimulus to perform the task-at-hand quickly and efficiently. Researchers have used expert scanpaths to train novices for improving the accuracy of visual search tasks. One of the limitations of such a system is that we need an expert to examine each visual stimuli beforehand to generate the scanpaths. In order to extend the idea of gaze guidance to a new unseen stimulus, there is a need for a computational model that can automatically generate expert-like scanpaths. We propose a model for automatic scanpath generation using a convolutional neural network (CNN) and long short-term memory (LSTM) modules. Our model uses LSTMs due to the temporal nature of eye movement data (scanpaths) where the system makes fixation predictions based on previous locations examined. © 2016 Copyright held by the owner/author(s).","Deep learning; Eye-tracking; Gaze manipulation","Brain; Convolution; Deep learning; Deep neural networks; Image processing; Long short-term memory; Neural networks; Recurrent neural networks; Biologically inspired; Computational model; Computer vision algorithms; Convolutional neural network; Eye-tracking; Gaze manipulations; Human Visual System; Primary visual cortex; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85011964878
"Kim H.-I., Kim J.-B., Lee J.-E., Lee T.-Y., Park R.-H.","56115915100;56115923900;56447152400;56923936300;7401895798;","Gaze estimation using a webcam for region of interest detection",2016,"Signal, Image and Video Processing","10","5",,"895","902",,4,"10.1007/s11760-015-0837-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945272208&doi=10.1007%2fs11760-015-0837-6&partnerID=40&md5=61d692eacd13683aaf1087287f345c0b","Department of Electronic Engineering, School of Engineering, Sogang University, 35 Baekbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 121-742, South Korea","Kim, H.-I., Department of Electronic Engineering, School of Engineering, Sogang University, 35 Baekbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 121-742, South Korea; Kim, J.-B., Department of Electronic Engineering, School of Engineering, Sogang University, 35 Baekbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 121-742, South Korea; Lee, J.-E., Department of Electronic Engineering, School of Engineering, Sogang University, 35 Baekbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 121-742, South Korea; Lee, T.-Y., Department of Electronic Engineering, School of Engineering, Sogang University, 35 Baekbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 121-742, South Korea; Park, R.-H., Department of Electronic Engineering, School of Engineering, Sogang University, 35 Baekbeom-ro (Sinsu-dong), Mapo-gu, Seoul, 121-742, South Korea","In this paper, a real-time gaze estimation system using a webcam is proposed, in which variation of head pose is tracked. At first, variation of head position and pose are estimated by using facial features. Then, an iterative iris center detection method is proposed for tracking iris in eye image. Finally, gaze is estimated by using estimated head pose and position, and iris center position. The proposed gaze estimation system is applied to four different applications. Experimental results show that the proposed iterative iris center detection method has a higher accuracy than conventional ones. Also, the proposed gaze estimation system shows about 98 % accuracy using 640 × 480 resolution webcam and 42-inch monitor that are 0.75 m apart. © 2015, Springer-Verlag London.","Gaze estimation; Human–computer interaction; Information retrieval; Iris center detection; Region of interest","Human computer interaction; Image segmentation; Information retrieval; Iterative methods; Computer interaction; Eye images; Facial feature; Gaze estimation; Head position; Iris center detections; Real time; Region of interest; Gesture recognition",Article,"Final","",Scopus,2-s2.0-84945272208
"Chaabouni S., Tison F., Benois-Pineau J., Ben Amar C.","57190285991;7004381839;6701750610;25959856600;","Prediction of visual attention with Deep CNN for studies of neurodegenerative diseases",2016,"Proceedings - International Workshop on Content-Based Multimedia Indexing","2016-June",,"7500243","","",,4,"10.1109/CBMI.2016.7500243","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978828120&doi=10.1109%2fCBMI.2016.7500243&partnerID=40&md5=c4ca7ad4030e0f4efdb084b5861ea62e","LaBRI UMR 5800, University of Bordeaux, Cours de la Libration, Talence Cedex, 33405, France; CHU de Bordeaux-GH Pellegrin, Place Amelie Raba Leon, Bordeaux Cedex, 33076, France; REGIM-Lab LR11ES48, University of Sfax, National Engineering School of Sfax, Sfax, 3038, Tunisia","Chaabouni, S., LaBRI UMR 5800, University of Bordeaux, Cours de la Libration, Talence Cedex, 33405, France; Tison, F., CHU de Bordeaux-GH Pellegrin, Place Amelie Raba Leon, Bordeaux Cedex, 33076, France; Benois-Pineau, J., LaBRI UMR 5800, University of Bordeaux, Cours de la Libration, Talence Cedex, 33405, France; Ben Amar, C., REGIM-Lab LR11ES48, University of Sfax, National Engineering School of Sfax, Sfax, 3038, Tunisia","As a part of the automatic study of visual attention of affected populations with neurodegenerative diseases and to predict whether new gaze records a complaint of these diseases, we should design an automatic model that predicts salient areas in video. Past research showed, that people suffering form dementia are not reactive with regard to degradations on still images. In this paper we study the reaction of healthy normal control subjects on degraded area in videos. Furthermore, in the goal to build an automatic prediction model for salient areas in intentionally degraded videos, we design a deep learning architecture and measure its performances when predicting salient regions on completely unseen data. The obtained results are interesting regarding the reaction of normal control subjects against a degraded area in video. © 2016 IEEE.",,"Behavioral research; Forecasting; Indexing (of information); Automatic modeling; Automatic prediction; Deep learning; Normal controls; Salient regions; Still images; Visual Attention; Neurodegenerative diseases",Conference Paper,"Final","",Scopus,2-s2.0-84978828120
"Sambrekar U., Ramdasi D.","56825929400;36191144300;","Human computer interaction for disabled using eye motion tracking",2016,"Proceedings - IEEE International Conference on Information Processing, ICIP 2015",,,"7489481","745","750",,4,"10.1109/INFOP.2015.7489481","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979257994&doi=10.1109%2fINFOP.2015.7489481&partnerID=40&md5=8a61216e1f7b8267c0162776d207c7a5","Department of Instrumentation and Control, Cummins College of Engineering for Women, Pune, 411052, India","Sambrekar, U., Department of Instrumentation and Control, Cummins College of Engineering for Women, Pune, 411052, India; Ramdasi, D., Department of Instrumentation and Control, Cummins College of Engineering for Women, Pune, 411052, India","Human Computer Interaction is a trend-in technology. Working in this field, we have developed a system to provide a solution to the limb disabled people to interact with computer. Different algorithms are implemented in this paper, to estimate the gaze of user to recognize the reference key. The first step performed on each input frame is the face detection, which is achieved using Viola-Jones algorithm, whereas Circular Hough Transform is used to locate the pupil in eye image. Similarly glint (a small and intense dot inside the pupil image) is detected by the blob analysis method. Using these pupil location and glint location, gaze direction of the user is estimated. With these functions, the user is able to handle input devices of a computer like keyboard and helps to recognize the key to be pressed. Further the type-in process is continued with the blinking phenomenon which is detected using template matching method. Currently the system is limited to 4-key keypad. The system is also designed with opening some window application softwares like Skype, media player etc. which are useful to the limb disable people. The camera used in this application, to access video is Logitech HD 720p webcam (C310) and the software implementation part is done in MATLAB. The system is tested under various environmental conditions, giving the appreciable results and speed. © 2015 IEEE.","Blink Detection; Circular Hough Transform; Gaze Estimation; Glint Detection; Human Computer Interaction; Key Recognition; Pupil Detection","Application programs; Face recognition; Feature extraction; Hough transforms; Image matching; Information science; MATLAB; Motion analysis; Template matching; Blink detections; Circular Hough transforms; Gaze estimation; Key Recognition; Pupil detection; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84979257994
"Tostado P.M., Abbott W.W., Faisal A.A.","57190131185;55312983800;6602900233;","3D gaze cursor: Continuous calibration and end-point grasp control of robotic actuators",2016,"Proceedings - IEEE International Conference on Robotics and Automation","2016-June",,"7487502","3295","3300",,12,"10.1109/ICRA.2016.7487502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977540191&doi=10.1109%2fICRA.2016.7487502&partnerID=40&md5=3ab359f4d727869f5490c406d66e3cd7","Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Dept. of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; MRC Clinical Sciences Centre, United Kingdom","Tostado, P.M., Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Abbott, W.W., Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom; Faisal, A.A., Brain and Behaviour Lab, Dept. of Bioengineering, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom, Dept. of Computing, Imperial College London, South Kensington Campus, London, SW7 2AZ, United Kingdom, MRC Clinical Sciences Centre, United Kingdom","Eye movements are closely related to motor actions, and hence can be used to infer motor intentions. Additionally, eye movements are in some cases the only means of communication and interaction with the environment for paralysed and impaired patients with severe motor deficiencies. Despite this, eye-tracking technology still has a very limited use as a human-robot control interface and its applicability is highly restricted to 2D simple tasks that operate on screen based interfaces and do not suffice for natural physical interaction with the environment. We propose that decoding the gaze position in 3D space rather than in 2D results into a much richer spatial cursor signal that allows users to perform everyday tasks such as grasping and moving objects via gaze-based robotic teleoperation. Eye tracking in 3D calibration is usually slow - we demonstrate here that by using a full 3D trajectory for system calibration generated by a robotic arm rather than a simple grid of discrete points, gaze calibration in the 3 dimensions can be successfully achieved in short time and with high accuracy. We perform the non-linear regression from eye-image to 3D-end point using Gaussian Process regressors, which allows us to handle uncertainty in end-point estimates gracefully. Our telerobotic system uses a multi-joint robot arm with a gripper and is integrated with our in-house GT3D binocular eye tracker. This prototype system has been evaluated and assessed in a test environment with 7 users, yielding gaze-estimation errors of less than 1cm in the horizontal, vertical and depth dimensions, and less than 2cm in the overall 3D Euclidean space. Users reported intuitive, low-cognitive load, control of the system right from their first trial and were straightaway able to simply look at an object and command through a wink to grasp this object with the robot gripper. © 2016 IEEE.",,"Calibration; Eye movements; Grippers; Human robot interaction; Motion planning; Point contacts; Robotic arms; Robots; Uncertainty analysis; Communication and interaction; Continuous calibrations; Eye tracking technologies; Human-robot controls; Non-linear regression; Physical interactions; Robotic teleoperation; Telerobotic systems; Robotics",Conference Paper,"Final","",Scopus,2-s2.0-84977540191
"Park H., Kim D.","55953924600;24597347100;","Gaze classification on a mobile device by using deep belief networks",2016,"Proceedings - 3rd IAPR Asian Conference on Pattern Recognition, ACPR 2015",,,"7486590","685","689",,3,"10.1109/ACPR.2015.7486590","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978906804&doi=10.1109%2fACPR.2015.7486590&partnerID=40&md5=27f66eee8ebbe63a17ec262d6607537c","POSTECH, Pohang, South Korea","Park, H., POSTECH, Pohang, South Korea; Kim, D., POSTECH, Pohang, South Korea","In this paper, we introduce a gaze classification method which classifies the locations of human gaze on a display of a mobile device. For example, when the user see the upper part of a display, our method classifies the gaze as the upper part among the available choices: upper, middle, and lower parts of the display. Our method uses appearance-based gaze estimation and gray-scale images captured from a camera of a mobile device. This method does not require any personal calibration. We train gaze classifiers by using Deep Belief Networks with considering head poses in general environments for a mobile device. The gaze classification method is applied to human-computer interaction and various application programs. © 2015 IEEE.",,"Application programs; Human computer interaction; Mobile devices; Pattern recognition; Appearance based; Classification methods; Deep belief networks; Gaze estimation; Gray-scale images; Head pose; Display devices",Conference Paper,"Final","",Scopus,2-s2.0-84978906804
"Qi H., Wu Q., Guo B., Li J., Sun J., Yan H.","57190182247;57043789700;57190183106;55441752700;12645161300;56471078100;","Gaze estimation based on camera relay",2016,"6th International Conference on Information Science and Technology, ICIST 2016",,,"7483467","511","514",,,"10.1109/ICIST.2016.7483467","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978187191&doi=10.1109%2fICIST.2016.7483467&partnerID=40&md5=bbd9c7bc4081b83f5016f190dff35ab6","School of Information Science and Engineering, Shandong University, Jinan, China; School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China; School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, China","Qi, H., School of Information Science and Engineering, Shandong University, Jinan, China; Wu, Q., School of Information Science and Engineering, Shandong University, Jinan, China; Guo, B., School of Information Science and Engineering, Shandong University, Jinan, China; Li, J., School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China; Sun, J., School of Information Science and Engineering, Shandong University, Jinan, China; Yan, H., School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, China","Gaze-tracking is a hot technology of human-computer interaction, which requires an explicit personal calibration process before every use for the sake of higher accuracy. When a number of different gaze interactive systems are used continuously, the calibration process would take the trouble. Due to the accuracy of interaction relying on the calibration of the camera which is used, in this paper, we proposed a method to transmit the personal calibration information based on camera calibration. The system devotes to deliver the personal calibration information between gaze interactive systems so that only one calibration is needed for the multiple gaze interactive systems. Given the calibrated cameras in the interactive systems, personalized eye parameters calibrated on one camera can be transmitted to any of the other cameras one by one. When one of the other cameras is used, the personal calibration process can be omitted without lowering the accuracy of the premise of the calculation much, where implements multiple interactive systems to realize the seamless transfer to use. Experiment between the two cameras is explored to validate the method we proposed. The experimental results show that the method is feasible and reliable. © 2016 IEEE.","camera calibration; camera relay; gaze calibration; gaze tracking","Cameras; Human computer interaction; Information science; Tracking (position); Calibrated cameras; Calibration information; Calibration process; Camera calibration; Gaze estimation; Gaze tracking; Interactive system; Seamless transfer; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84978187191
"Sang G.-L., Chen H., Huang G., Zhao Q.-J.","56017828200;57189326786;57190006507;57221157913;","Unseen head pose prediction using dense multivariate label distribution",2016,"Frontiers of Information Technology and Electronic Engineering","17","6",,"516","526",,5,"10.1631/FITEE.1500235","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976340260&doi=10.1631%2fFITEE.1500235&partnerID=40&md5=5915a0e50b9509cc8173aa8a10c711ac","State Key Laboratory of Fundamental Science on Synthetic Vision, College of Computer Science, Sichuan University, Chengdu, 610064, China; College of Mathematics and Information Engineering, Jiaxing University, Jiaxing, 314001, China","Sang, G.-L., State Key Laboratory of Fundamental Science on Synthetic Vision, College of Computer Science, Sichuan University, Chengdu, 610064, China, College of Mathematics and Information Engineering, Jiaxing University, Jiaxing, 314001, China; Chen, H., State Key Laboratory of Fundamental Science on Synthetic Vision, College of Computer Science, Sichuan University, Chengdu, 610064, China; Huang, G., State Key Laboratory of Fundamental Science on Synthetic Vision, College of Computer Science, Sichuan University, Chengdu, 610064, China; Zhao, Q.-J., State Key Laboratory of Fundamental Science on Synthetic Vision, College of Computer Science, Sichuan University, Chengdu, 610064, China","Accurate head poses are useful for many face-related tasks such as face recognition, gaze estimation, and emotion analysis. Most existing methods estimate head poses that are included in the training data (i.e., previously seen head poses). To predict head poses that are not seen in the training data, some regression-based methods have been proposed. However, they focus on estimating continuous head pose angles, and thus do not systematically evaluate the performance on predicting unseen head poses. In this paper, we use a dense multivariate label distribution (MLD) to represent the pose angle of a face image. By incorporating both seen and unseen pose angles into MLD, the head pose predictor can estimate unseen head poses with an accuracy comparable to that of estimating seen head poses. On the Pointing’04 database, the mean absolute errors of results for yaw and pitch are 4.01° and 2.13°, respectively. In addition, experiments on the CAS-PEAL and CMU Multi-PIE databases show that the proposed dense MLD-based head pose estimation method can obtain the state-of-the-art performance when compared to some existing methods. © 2016, Journal of Zhejiang University Science Editorial Office and Springer-Verlag Berlin Heidelberg.","Dense multivariate label distribution; Head pose estimation; Inconsistent labels; Sampling intervals","Estimation; Face recognition; Forecasting; Image recognition; Emotion analysis; Gaze estimation; Head Pose Estimation; Label distribution; Mean absolute error; Sampling interval; State-of-the-art performance; Training data; Motion estimation",Article,"Final","",Scopus,2-s2.0-84976340260
"Funes-Mora K.A., Odobez J.-M.","55336423200;57203103085;","Gaze Estimation in the 3D Space Using RGB-D Sensors: Towards Head-Pose and User Invariance",2016,"International Journal of Computer Vision","118","2",,"194","216",,39,"10.1007/s11263-015-0863-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946944319&doi=10.1007%2fs11263-015-0863-4&partnerID=40&md5=399e81b733b016912bbd5adfd6f179b6","Idiap Research Institute, Martigny, Switzerland; École polytechnique fédéral de, Lausanne (EPFL), Switzerland","Funes-Mora, K.A., Idiap Research Institute, Martigny, Switzerland, École polytechnique fédéral de, Lausanne (EPFL), Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, Switzerland, École polytechnique fédéral de, Lausanne (EPFL), Switzerland","We address the problem of 3D gaze estimation within a 3D environment from remote sensors, which is highly valuable for applications in human–human and human–robot interactions. To the contrary of most previous works, which are limited to screen gazing applications, we propose to leverage the depth data of RGB-D cameras to perform an accurate head pose tracking, acquire head pose invariance through a 3D rectification process that renders head pose dependent eye images into a canonical viewpoint, and computes the line-of-sight in the 3D space. To address the low resolution issue of the eye image resulting from the use of remote sensors, we rely on the appearance based gaze estimation paradigm, which has demonstrated robustness against this factor. In this context, we do a comparative study of recent appearance based strategies within our framework, study the generalization of these methods to unseen individual, and propose a cross-user eye image alignment technique relying on the direct registration of gaze-synchronized eye images. We demonstrate the validity of our approach through extensive gaze estimation experiments on a public dataset as well as a gaze coding task applied to natural job interviews. © 2015, Springer Science+Business Media New York.","Appearance based methods; Gaze estimation; Head-pose invariance; Person invariance; RGB-D cameras","Cameras; Gesture recognition; Human robot interaction; Remote sensing; Appearance-based methods; Comparative studies; Gaze estimation; Head pose; Head-pose tracking; Rectification process; Rgb-d cameras; Robot interactions; Motion estimation",Article,"Final","",Scopus,2-s2.0-84946944319
"Li J., Li S.","57203736355;16202805500;","Gaze Estimation From Color Image Based on the Eye Model With Known Head Pose",2016,"IEEE Transactions on Human-Machine Systems","46","3","7272080","414","423",,25,"10.1109/THMS.2015.2477507","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941921838&doi=10.1109%2fTHMS.2015.2477507&partnerID=40&md5=b07b62c4590e7dfd42145720a8271ae0","Graduate School of Engineering, Tottori University, Tottori, 680-8550, Japan; Graduate School of Information Sciences, Hiroshima City University, Hiroshima, 731-3194, Japan","Li, J., Graduate School of Engineering, Tottori University, Tottori, 680-8550, Japan; Li, S., Graduate School of Information Sciences, Hiroshima City University, Hiroshima, 731-3194, Japan","This paper proposes a novel method of gaze estimation based on an eye model with known head pose. The most crucial factors in the eye-model-based approach to gaze estimation are the 3-D positions of the eyeball and iris centers. In the proposed method, an RGB-D camera, Kinect sensor, is used to obtain the head pose as well as the eye region of the color image. The 3-D position of the eyeball center is determined in the calibration phase by gazing at the center of the color image camera. Then, to estimate the 3-D position of the iris center, the 3-D contour of the iris is projected onto the color image with the known head pose obtained from color and depth cues of an RGB-D camera. Thus, the ellipse of the iris in the image can be described using only two parameters: the yaw and pitch angles of the eyeball in the iris coordinate system, rather than the conventional five parameters of an ellipse. The proposed method can fit an iris that is not complete due to eyelid occlusion. The average errors of vertical and horizontal angles of the gaze estimation for seven subjects are 5.9° and 4.4°, respectively. The processing speed is as high as 330 ms per frame. However, for lower resolution and poor illumination images, as tested on the public database EYEDIAP, the performance of the proposed eye-model-based method is inferior to that of the-state-of-the-art appearance-based method. © 2015 IEEE.","3-D eye model; Color and depth cues; eyeball calibration; gaze estimation; iris fitting","Cameras; Color; Appearance-based methods; Co-ordinate system; Horizontal angles; Illumination images; Lower resolution; Processing speed; Public database; State of the art; Motion estimation",Article,"Final","",Scopus,2-s2.0-84941921838
"Baltrusaitis T., Robinson P., Morency L.-P.","36696075900;57205369790;6603047400;","OpenFace: An open source facial behavior analysis toolkit",2016,"2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016",,,"7477553","","",,632,"10.1109/WACV.2016.7477553","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977660584&doi=10.1109%2fWACV.2016.7477553&partnerID=40&md5=0be2d7086631b6f33fc21965ec7dba80",,"Baltrusaitis, T.; Robinson, P.; Morency, L.-P.","Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system. © 2016 IEEE.",,"Artificial intelligence; Behavioral research; Face recognition; Image recognition; Learning systems; Open systems; Affective Computing; Behavior analysis; Computer vision algorithms; Facial action unit recognition; Facial landmark detection; Head Pose Estimation; Interactive applications; Real time performance; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84977660584
"Huang M.X., Kwok T.C.K., Ngai G., Chan S.C.F., Leong H.V.","55258532000;56432365700;8915594400;7404255433;7005127948;","Building a Personalized, Auto-Calibrating Eyetracker from user interactions",2016,"Conference on Human Factors in Computing Systems - Proceedings",,,,"5169","5179",,33,"10.1145/2858036.2858404","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014773115&doi=10.1145%2f2858036.2858404&partnerID=40&md5=68b5c6352b3b2265dfb74a3622d9f693","Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong","Huang, M.X., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Kwok, T.C.K., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Ngai, G., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Chan, S.C.F., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Leong, H.V., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong","We present PACE, a Personalized, Auto-Calibrating Eyetracking system that identifies and collects data unobtrusively from user interaction events on standard computing systems without the need for specialized equipment. PACE relies on eye/facial analysis of webcam data based on a set of robust geometric gaze features and a two-layer data validation mechanism to identify good training samples from daily interaction data. The design of the system is founded on an in-depth investigation of the relationship between gaze patterns and interaction cues, and takes into consideration user preferences and habits. The result is an adaptive, data-driven approach that continuously recalibrates, adapts and improves with additional use. Quantitative evaluation on 31 subjects across different interaction behaviors shows that training instances identified by the PACE data collection have higher gaze point-interaction cue consistency than those identified by conventional approaches. An in-situ study using real-life tasks on a diverse set of interactive applications demonstrates that the PACE gaze estimation achieves an average error of 2.56°, which is comparable to state-of-theart, but without the need for explicit training or calibration. This demonstrates the effectiveness of both the gaze estimation method and the corresponding data collection mechanism. © 2016 ACM.","Data validation; Gaze estimation; Gazeinteraction correspondence; Implicit modeling","Human computer interaction; Human engineering; User interfaces; Data collection mechanism; Data validation; Gaze estimation; Gazeinteraction correspondence; Implicit model; Interactive applications; Quantitative evaluation; Specialized equipment; Data acquisition",Conference Paper,"Final","",Scopus,2-s2.0-85014773115
"Liu C.-L., Lovell B., Tao D., Tistarelli M.","36064176500;7003583184;7102600334;7003853982;","Pattern Recognition, Part 2",2016,"IEEE Intelligent Systems","31","3","7478487","3","5",,,"10.1109/MIS.2016.55","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973365151&doi=10.1109%2fMIS.2016.55&partnerID=40&md5=e2bffe07c5a3c62665711dea57688db4","Chinese Academy of Sciences, China; University of Queensland, Australia; University of Technology Sydney, Australia; University of Sassari, Italy","Liu, C.-L., Chinese Academy of Sciences, China; Lovell, B., University of Queensland, Australia; Tao, D., University of Technology Sydney, Australia; Tistarelli, M., University of Sassari, Italy","This second part of the special issue on pattern recognition reports the advances in pattern recognition for visual data. The selected articles address visual categorization by cross-domain dictionary learning, facial expression recognition, face sketch-photo matching, heartbeat rate measurement from facial video, driver gaze estimation, nighttime vehicle detection, and overlaid arrow detection in biomedical images, respectively. © 2001-2011 IEEE.","arrow detection; computer vision; cross-domain dictionary learning; driver gaze estimation; facial expression; heartbeat rate measurement; intelligent systems; pattern recognition; sketch-photo matching; vehicle detection","Computer vision; Image matching; Intelligent systems; Object detection; Pattern recognition; Pattern recognition systems; Dictionary learning; Facial Expressions; Gaze estimation; Heart beat rates; sketch-photo matching; Vehicle detection; Face recognition",Review,"Final","",Scopus,2-s2.0-84973365151
"Guo Z., Zhou Q., Liu Z., Liu C.","56304391000;9632964600;55553990400;55264234000;","Accurate Pupil Center Location with the SIFT Descriptor and SVM Classifier",2016,"International Journal of Pattern Recognition and Artificial Intelligence","30","4","1655012","","",,2,"10.1142/S0218001416550120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957641755&doi=10.1142%2fS0218001416550120&partnerID=40&md5=979b6704157855a529292260f6ee0c89","School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China","Guo, Z., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Zhou, Q., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Liu, Z., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Liu, C., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China","Locating the center of the pupils is the most important foundation and the core component of gaze tracking. The accuracy of gaze tracking largely depends on the quality of images, but additional constraints and large amount of calculation make gaze tracking impractical on high-resolution images. Although some eye-gaze trackers can get accurate result, improving the accuracy of pupil feature on low-resolution images and accurately recognizing closed eye images are still common tasks in the field of gaze estimation. Our aim is to get the accurate localization of pupil center on low-resolution image. To this aim, we proposed a simple but effective method which can accurately locate pupil center in real time. The method first gets initial eye center based on improved scale-invariant feature transform (SIFT) descriptor and support vector machine (SVM) classifier, and then gets final position of the pupil center through a size variable correction rectangular block. In this paper, comparing with the reported state-of-the-art methods,the experimental results demonstrate that our system can achieve a more accurate result on low-resolution images. On top of that, our approach shows robustness on closed eye images while some other methods would not recognize the closed eye images. © 2016 World Scientific Publishing Company.","classification; correction rectangular block; Pupil center localization; SIFT feature","Classification (of information); Eye tracking; Support vector machines; Eye gaze trackers; High resolution image; Low resolution images; Pupil center localizations; Rectangular block; Scale invariant feature transforms; SIFT Feature; State-of-the-art methods; Image enhancement",Article,"Final","",Scopus,2-s2.0-84957641755
"Tamaki H., Yoshida R., Ogitsu T., Takemura H., Mizoguchi H., Namatame M., Kusunoki F., Yamaguchi E., Inagaki S., Takeda Y., Sugimoto M., Egusa R.","57007805400;56203975500;36174254300;35305948000;35430648400;22433574600;6701837740;55339464100;7202273286;7404002040;7402376805;55212262100;","Novel application of a range image sensor to eye gaze estimation by using the relationship between face and eye directions",2016,"Proceedings of the International Conference on Sensing Technology, ICST","2016-March",,"7438439","443","446",,1,"10.1109/ICSensT.2015.7438439","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964861663&doi=10.1109%2fICSensT.2015.7438439&partnerID=40&md5=302a6eb533288b2286b5dc991d04b597","Department of Mechanical Engineering, Tokyo University of Science, Yamazaki, Noda-shi, Chiba, 2641, Japan; Faculty of Industrial Technology, Tsukuba University of Technology, 4-3-15 Amakubo, Ibaraki, Tsukuba, Japan; Department of Information Design, Tama Art University, 2-1723 Yarimizu, Hachioji, Tokyo, Japan; Graduate School of Human Development and Environment, Kobe University, Yayoigaoka 6, Sanda-shi, Hyogo, Kobe, Japan; Graduate School of Information Science and Technology, Hokkaido University, Kita 15, Nishi 8, Kita-ku, Sapporo, Hokkaido, Japan; Kobe University, 3-11 Tsurukabuto, Nada-ku, Kobe, 657-8501, Japan","Tamaki, H., Department of Mechanical Engineering, Tokyo University of Science, Yamazaki, Noda-shi, Chiba, 2641, Japan; Yoshida, R., Department of Mechanical Engineering, Tokyo University of Science, Yamazaki, Noda-shi, Chiba, 2641, Japan; Ogitsu, T., Department of Mechanical Engineering, Tokyo University of Science, Yamazaki, Noda-shi, Chiba, 2641, Japan; Takemura, H., Department of Mechanical Engineering, Tokyo University of Science, Yamazaki, Noda-shi, Chiba, 2641, Japan; Mizoguchi, H., Department of Mechanical Engineering, Tokyo University of Science, Yamazaki, Noda-shi, Chiba, 2641, Japan; Namatame, M., Faculty of Industrial Technology, Tsukuba University of Technology, 4-3-15 Amakubo, Ibaraki, Tsukuba, Japan; Kusunoki, F., Department of Information Design, Tama Art University, 2-1723 Yarimizu, Hachioji, Tokyo, Japan; Yamaguchi, E., Graduate School of Human Development and Environment, Kobe University, Yayoigaoka 6, Sanda-shi, Hyogo, Kobe, Japan; Inagaki, S., Graduate School of Human Development and Environment, Kobe University, Yayoigaoka 6, Sanda-shi, Hyogo, Kobe, Japan; Takeda, Y., Graduate School of Human Development and Environment, Kobe University, Yayoigaoka 6, Sanda-shi, Hyogo, Kobe, Japan; Sugimoto, M., Graduate School of Information Science and Technology, Hokkaido University, Kita 15, Nishi 8, Kita-ku, Sapporo, Hokkaido, Japan; Egusa, R., Kobe University, 3-11 Tsurukabuto, Nada-ku, Kobe, 657-8501, Japan","In this study, we estimate gaze direction using the relationship between the face and eyes. The face and eyes are divided laterally and in lengthwise direction. We set up an object laterally and in lengthwise direction and measured the face angle using a Kinect sensor, which is range image sensor, when gazing at an object. The angle of the eyes is fixed at the angle of the object and grasp of the relationship between face and eyes. We calculate the gaze direction using the relation between the face and eyes. Then, the results of present experiment, if based on relationship between face and eyes, has been shown to be effective though the results of the experiment. © 2015 IEEE.","face and eyes directions; range image sensor","Eye direction; Eye-gaze; face and eyes directions; Gaze direction; Kinect sensors; Novel applications; Range image sensor; Image sensors",Conference Paper,"Final","",Scopus,2-s2.0-84964861663
"Lu F., Chen X.","54956194300;13410318100;","Person-independent eye gaze prediction from eye images using patch-based features",2016,"Neurocomputing","182",,,"10","17",,10,"10.1016/j.neucom.2015.07.125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954287829&doi=10.1016%2fj.neucom.2015.07.125&partnerID=40&md5=e111f71cac2eeb4cea3e25901b4ec0b1","School of Computer Science and Engineering, Beihang University, Beijing, 100191, China; International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China","Lu, F., School of Computer Science and Engineering, Beihang University, Beijing, 100191, China, International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; Chen, X., State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, 100191, China","This paper delivers a preliminary attempt towards person-independent appearance-based gaze estimation. Conventional methods need to assume training and test data collected from the same person, otherwise eye shape difference due to individuality will affect the estimation severely. To solve this problem, the key idea in this paper is to extract from eye images more advanced eye features, which helps learn a person-independent relationship between eye gaze change and eye appearance variation. To this end, we propose employing the advantages of recent sparse auto-encoding techniques. We partition any eye image into small patches which can overlap with each other. With patches from many images, we learn a codebook comprising a set of bases, which can reconstruct any eye image patch with sparse coefficients. By examining these coefficients, we can analyze the eye shape more effectively. Finally, we produce the eye features by pooling the coefficients at different scales, and then combine these subfeatures from different codebooks. Experimental results show that the proposed method achieves good accuracy on a public dataset and it also outperforms conventional methods by a large margin. © 2015 Elsevier B.V.","Eye image; Gaze direction classification; Gaze estimation; Sparse auto-encoder","Large dataset; Signal encoding; Auto encoders; Conventional methods; Encoding techniques; Eye images; Eye-gaze predictions; Gaze direction; Gaze estimation; Person-independent; Image processing; accuracy; Article; classification algorithm; correlation coefficient; data base; gaze; gaze direction algorithm; image analysis; image processing; image reconstruction; mathematical analysis; prediction; priority journal; virtual reality",Article,"Final","",Scopus,2-s2.0-84954287829
"Sarkar A.R., Sanyal G., Majumder S.","57214033738;35276165000;57210463855;","A hybrid approach for eye-centre localization for estimation of eye-gazes using low-cost web cam",2016,"Proceedings of 2015 IEEE International Conference on Research in Computational Intelligence and Communication Networks, ICRCICN 2015",,,"7434249","273","278",,2,"10.1109/ICRCICN.2015.7434249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965120149&doi=10.1109%2fICRCICN.2015.7434249&partnerID=40&md5=1e7eb246b23c7e93409accefd2e94b85","Dept. of Computer Sc. and Engg., National Institute of Technology, Durgapur, WB  713209, India; Surface Robotics Laboratory, CSIR, Central Mechanical Engg. Resrach Institute, Durgapur, WB  713209, India","Sarkar, A.R., Dept. of Computer Sc. and Engg., National Institute of Technology, Durgapur, WB  713209, India; Sanyal, G., Dept. of Computer Sc. and Engg., National Institute of Technology, Durgapur, WB  713209, India; Majumder, S., Surface Robotics Laboratory, CSIR, Central Mechanical Engg. Resrach Institute, Durgapur, WB  713209, India","Gaze estimation is one of the recent popular research topics in the domain of computer science/engineering for vision-based human-computer-interaction. It has huge potential for application in assessment of physical condition of drivers while driving, controlling robots and prosthetics, surveillance etc. For successful gaze estimation the eyes have to be detected perfectly and this will lead to efficient eye centre localization. Several approaches have been used by researchers for eye and eye centre detection. The present work uses Haar-like cascade classifier for effective face and eye detection. This method has the advantage of lower computational load, faster processing due to the associated AdaBoost algorithm. High success rate can be easily achieved in several environments using proper training sets. However, for eye centre localization an improved version of Hough transform in two dimensional parametric space has been used as it is very simple to use and implement practically. This hybrid approach has been successfully tested using low-cost webcams in different lighting conditions with and without spectacles. © 2015 IEEE.","Eye tracking; Face detection; Gaze estimation; Haar-like features; Hough circle; Rehabilitation; Stroke","Adaptive boosting; Artificial intelligence; Classification (of information); Computer vision; Eye protection; Face recognition; Hough transforms; Patient rehabilitation; Eye-tracking; Gaze estimation; Haar-like features; Hough circle; Stroke; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84965120149
"Barz M., Daiber F., Bulling A.","57189847803;34972779200;6505807414;","Prediction of gaze estimation error for error-aware gaze-based interfaces",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"275","278",,16,"10.1145/2857491.2857493","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975318756&doi=10.1145%2f2857491.2857493&partnerID=40&md5=ffb02010a2c306d126a5280e11a83af0","German Research Center for Artificial Intelligence (DFKI), Germany; Lancaster University, United Kingdom; Perceptual User Interfaces Group, Max Planck Institute for Informatics, Germany","Barz, M., German Research Center for Artificial Intelligence (DFKI), Germany; Daiber, F., German Research Center for Artificial Intelligence (DFKI), Germany, Lancaster University, United Kingdom; Bulling, A., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Germany","Gaze estimation error is inherent in head-mounted eye trackers and seriously impacts performance, usability, and user experience of gaze-based interfaces. Particularly in mobile settings, this error varies constantly as users move in front and look at different parts of a display. We envision a new class of gaze-based interfaces that are aware of the gaze estimation error and adapt to it in real time. As a first step towards this vision we introduce an error model that is able to predict the gaze estimation error. Our method covers major building blocks of mobile gaze estimation, specifically mapping of pupil positions to scene camera coordinates, marker-based display detection, and mapping of gaze from scene camera to on-screen coordinates. We develop our model through a series of principled measurements of a state-of-the-art head-mounted eye tracker. © 2016 Copyright held by the owner/author(s).","Error Modelling and Prediction; Error-Aware Interfaces; Eye Tracking; Gaze Estimation; Gaze-Based Interaction","Cameras; Eye movements; Forecasting; Mapping; Stereo vision; Building blockes; Eye-tracking; Gaze estimation; Gaze-based interaction; Measurements of; Modelling and predictions; State of the art; User experience; Errors",Conference Paper,"Final","",Scopus,2-s2.0-84975318756
"Lander C., Kerber F., Rauber T., Krüger A.","55785449200;55537692200;55874301700;35264048900;","A time-efficient re-calibration algorithm for improved long-term accuracy of head-worn eye trackers",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"213","216",,9,"10.1145/2857491.2857513","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975318252&doi=10.1145%2f2857491.2857513&partnerID=40&md5=7f4eeddac8f9cee60807276aeca9a47b","DFKI GmbH, Germany; Saarland University, Germany","Lander, C., DFKI GmbH, Germany; Kerber, F., DFKI GmbH, Germany; Rauber, T., Saarland University, Germany; Krüger, A., DFKI GmbH, Germany","Mobile gaze-based interaction has been emerging over the last two decades. Head-mounted eye trackers as well as remote systems are used to determine people's gaze (e.g., on a display). However, most state-of-the-art systems need calibration prior to usage. When using a head-mounted eye tracker, many factors (e.g., changes of eye physiology) can influence the stability of the calibration leading to less accuracy over time. Re-calibrating the system at certain time intervals is cumbersome and time-consuming. We investigate methods to minimize the time needed and optimize the process. In a user study with 16 participants, we compared partial re-calibrations with different numbers of calibration points and types of adaptation strategies. In contrast to a full calibration with nine points, the results show that a re-calibration with only three points results in 60% less time needed and achieves a similar accuracy. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Eye tracking; Gaze estimation; Re-calibration","Algorithms; Eye movements; Stereo vision; Adaptation strategies; Calibration points; Eye-tracking; Gaze estimation; Gaze-based interaction; Recalibrations; Remote systems; State-of-the-art system; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84975318252
"Mansouryar M., Steil J., Sugano Y., Bulling A.","57204207379;57170107900;7005470045;6505807414;","3D gaze estimation from 2D pupil positions on monocular head-mounted eye trackers",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"197","200",,33,"10.1145/2857491.2857530","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975317287&doi=10.1145%2f2857491.2857530&partnerID=40&md5=41bfdfbf2ebc6956f21799fa3e7486cf","Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany","Mansouryar, M., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Steil, J., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Sugano, Y., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Bulling, A., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany","3D gaze information is important for scene-centric attention analysis, but accurate estimation and analysis of 3D gaze in real-world environments remains challenging. We present a novel 3D gaze estimation method for monocular head-mounted eye trackers. In contrast to previous work, our method does not aim to infer 3D eyeball poses, but directly maps 2D pupil positions to 3D gaze directions in scene camera coordinate space. We first provide a detailed discussion of the 3D gaze estimation task and summarize different methods, including our own. We then evaluate the performance of different 3D gaze estimation approaches using both simulated and real data. Through experimental validation, we demonstrate the effectiveness of our method in reducing parallax error, and we identify research challenges for the design of 3D calibration procedures. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D gaze estimation; Head-mounted eye tracking; Parallax error","Geometrical optics; Accurate estimation; Coordinate space; Experimental validations; Gaze estimation; Head-mounted eye tracking; Parallax error; Real world environments; Research challenges; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84975317287
"Ahmed Z., Mardanbegi D., Hansen D.W.","57210013533;42761947400;15063910800;","Pupil center as a function of pupil diameter",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"283","286",,,"10.1145/2857491.2857536","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975303243&doi=10.1145%2f2857491.2857536&partnerID=40&md5=83848a056d5f3397ee5418f90461df21","IT University of Copenhagen, Denmark","Ahmed, Z., IT University of Copenhagen, Denmark; Mardanbegi, D., IT University of Copenhagen, Denmark; Hansen, D.W., IT University of Copenhagen, Denmark","We investigate the gaze estimation error induced by pupil size changes using simulated data. We investigate the influence of pupil diameter changes on estimated gaze point error obtained by two gaze estimation models. Simulation data show that at wider viewing angles and at small eye-camera distances, error increases with increasing pupil sizes. The maximum error recorded for refracted pupil images is 2:4° of visual angle and 1:5° for non-refracted pupil projections. © 2016 Copyright held by the owner/author(s).","Eye tracking; Gaze estimation; Human computer interaction; Image processing; Pupil dynamics","Errors; Image processing; Eye-tracking; Gaze estimation; Maximum error; Pupil centers; Pupil diameter; Pupil dynamics; Simulation data; Viewing angle; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84975303243
"Wood E., Baltrušaitis T., Morency L.-P., Robinson P., Bulling A.","56145872800;36696075900;6603047400;57205369790;6505807414;","Learning an appearance-based gaze estimator from one million synthesised images",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"131","138",,126,"10.1145/2857491.2857492","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975298409&doi=10.1145%2f2857491.2857492&partnerID=40&md5=503376b5766f5a3d6a85fd1319b5c8f5","University of Cambridge, United Kingdom; Carnegie Mellon University, United States; Max Planck Institute for Informatics, Germany","Wood, E., University of Cambridge, United Kingdom; Baltrušaitis, T., Carnegie Mellon University, United States; Morency, L.-P., Carnegie Mellon University, United States; Robinson, P., University of Cambridge, United Kingdom; Bulling, A., Max Planck Institute for Informatics, Germany","Learning-based methods for appearance-based gaze estimation achieve state-of-the-art performance in challenging real-world settings but require large amounts of labelled training data. Learningby-synthesis was proposed as a promising solution to this problem but current methods are limited with respect to speed, appearance variability, and the head pose and gaze angle distribution they can synthesize. We present UnityEyes, a novel method to rapidly synthesize large amounts of variable eye region images as training data. Our method combines a novel generative 3D model of the human eye region with a real-time rendering framework. The model is based on high-resolution 3D face scans and uses real-time approximations for complex eyeball materials and structures as well as anatomically inspired procedural geometry methods for eyelid animation. We show that these synthesized images can be used to estimate gaze in difficult in-the-wild scenarios, even for extreme gaze angles or in cases in which the pupil is fully occluded. We also demonstrate competitive gaze estimation results on a benchmark in-the-wild dataset, despite only using a light-weight nearestneighbor algorithm. We are making our UnityEyes synthesis framework available online for the benefit of the research community. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","3D morphable model; Appearance-based gaze estimation; Learning-bysynthesis; Real-time rendering","Rendering (computer graphics); Three dimensional computer graphics; 3D Morphable model; Gaze estimation; Learning-based methods; Learning-bysynthesis; Nearest-neighbor algorithms; Real-time rendering; Research communities; State-of-the-art performance; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-84975298409
"Sesma-Sanchez L., Zhang Y., Gellersen H., Bulling A.","55320498400;52664598600;6701531333;6505807414;","Gaussian processes as an alternative to polynomial gaze estimation functions",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"229","232",,8,"10.1145/2857491.2857509","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975297486&doi=10.1145%2f2857491.2857509&partnerID=40&md5=e4fa5f858e1a8f4fba01dd06e9a065f5","Public University of Navarra, Spain; Lancaster University, United Kingdom; Max-Planck-Institute for Informatics, Germany","Sesma-Sanchez, L., Public University of Navarra, Spain; Zhang, Y., Lancaster University, United Kingdom; Gellersen, H., Lancaster University, United Kingdom; Bulling, A., Max-Planck-Institute for Informatics, Germany","Interpolation-based methods are widely used for gaze estimation due to their simplicity. In particular, feature-based methods that map the image eye features to gaze, are very popular. The most spread regression function used in this kind of method is the polynomial regression. In this paper, we present an alternative regression function to estimate gaze: the Gaussian regression. We show how the Gaussian processes can better adapt to the non-linear behavior of the eye movement, providing higher gaze estimation accuracies. The Gaussian regression is compared, in a simulated environment, to the polynomial regression, when using the same mapping features, the normalized pupil center-corneal reflection and pupil center-eye corners vectors. This comparison is done for three different screen sizes. The results show that for larger screens, where wider gaze angles are required, i.e., the non-linear behavior of the eye is more present, the outperformance of the Gaussian regression is more evident. Furthermore, we can conclude that, for both types of regressions, the gaze estimation accuracy increases for smaller screens, where the eye movements are more linear. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Gaussian process; Gaze estimation; Polynomials","Gaussian distribution; Gaussian noise (electronic); Polynomials; Regression analysis; Corneal reflection; Feature-based method; Gaussian Processes; Gaussian regression; Gaze estimation; Polynomial regression; Regression function; Simulated environment; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84975297486
"Wang K., Ji Q.","56637259500;18935108400;","Hybrid model and appearance based eye tracking with Kinect",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"331","332",,3,"10.1145/2857491.2888591","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975263425&doi=10.1145%2f2857491.2888591&partnerID=40&md5=8a7b9a0e090db4be36740baa63bec63a","Rensselaer Polytechnic Institute, United States","Wang, K., Rensselaer Polytechnic Institute, United States; Ji, Q., Rensselaer Polytechnic Institute, United States","Existing gaze estimation methods rely mainly on 3D eye model or 2D eye appearance. While both methods have validated their effectiveness in various fields and applications, they are still limited in practice, such as portable and non-intrusive system and robust eye gaze tracking in different environments. To this end, we investigate on combining eye model with eye appearance to perform gaze estimation and eye gaze tracking. Specifically, unlike traditional 3D model based methods which rely on cornea reflections, we plan to retrieve 3D information from depth sensor (Eg, Kinect). Kinect integrates camera sensor and IR illuminations into one single device, thus enable more flexible system settings. We further propose to utilize appearance information to help the basic model based methods. Appearance information can help better detection of gaze related features (Eg, pupil center). Plus, eye model and eye appearance can benefit each other to enable robust and accurate gaze estimation. © 2016 Copyright held by the owner/author(s).","3D eye model; Depth sensor; Eye appearance; Learning","Tracking (position); 3D eye models; Appearance based; Depth sensors; Eye appearance; Eye gaze tracking; Flexible system; Learning; Model-based method; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84975263425
"Duchowski A.T., Jörg S., Allen T.N., Giannopoulos I., Krejtz K.","6701824388;26221294500;57193428896;57117903600;55258716700;","Eye movement synthesis",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"147","154",,18,"10.1145/2857491.2857528","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975252768&doi=10.1145%2f2857491.2857528&partnerID=40&md5=3fc769e31ccbf214786de87c2a19b03a","Clemson University, ETH Zürichz, Switzerland; University of Social Sciences and Humanities, National Information Processing Institute, Switzerland","Duchowski, A.T., Clemson University, ETH Zürichz, Switzerland; Jörg, S., Clemson University, ETH Zürichz, Switzerland; Allen, T.N., Clemson University, ETH Zürichz, Switzerland; Giannopoulos, I., Clemson University, ETH Zürichz, Switzerland; Krejtz, K., University of Social Sciences and Humanities, National Information Processing Institute, Switzerland","A convolution-filtering technique is introduced for the synthesis of eye gaze data. Its purpose is to produce, in a controlled manner, a synthetic stream of raw gaze position coordinates, suitable for: (1) testing event detection filters, and (2) rendering synthetic eye movement animations for testing eye tracking gaze estimation algorithms. Synthetic gaze data is parameterized by sampling rate, microsaccadic jitter, and simulated measurement error. Sampled synthetic gaze data is compared against real data captured by an eye tracker showing similar signal characteristics. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Eye movement synthesis; Signal processing","Signal processing; Convolution filtering; Event detection; Eye-tracking; Gaze estimation; Parameterized; Position coordinates; Sampling rates; Signal characteristic; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84975252768
"Kübler T.C., Rittig T., Kasneci E., Ungewiss J., Krauss C.","55701951700;57189852645;56059892600;56529736700;57189850616;","Rendering refraction and reflection of eyeglasses for synthetic eye tracker images",2016,"Eye Tracking Research and Applications Symposium (ETRA)","14",,,"143","146",,7,"10.1145/2857491.2857494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975250739&doi=10.1145%2f2857491.2857494&partnerID=40&md5=457aa383fcbc571ec9ee18f150338a35","Wilhelm-Schickard-Institute of Computer Science, University of Tübingen, Germany; Study Course Ophthalmic Optics/Audiology, University of Applied Sciences Aalen, Germany","Kübler, T.C., Wilhelm-Schickard-Institute of Computer Science, University of Tübingen, Germany; Rittig, T., Wilhelm-Schickard-Institute of Computer Science, University of Tübingen, Germany; Kasneci, E., Wilhelm-Schickard-Institute of Computer Science, University of Tübingen, Germany; Ungewiss, J., Study Course Ophthalmic Optics/Audiology, University of Applied Sciences Aalen, Germany; Krauss, C., Study Course Ophthalmic Optics/Audiology, University of Applied Sciences Aalen, Germany","While for the evaluation of robustness of eye tracking algorithms the use of real-world data is essential, there are many applications where simulated, synthetic eye images are of advantage. They can generate labelled ground-truth data for appearance based gaze estimation algorithms or enable the development of model based gaze estimation techniques by showing the influence on gaze estimation error of different model factors that can then be simplified or extended. We extend the generation of synthetic eye images by a simulation of refraction and reflection for eyeglasses. On the one hand this allows for the testing of pupil and glint detection algorithms under different illumination and reflection conditions, on the other hand the error of gaze estimation routines can be estimated in conjunction with different eyeglasses. We show how a polynomial function fitting calibration performs equally well with and without eyeglasses, and how a geometrical eye model behaves when exposed to glasses. © 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Calibration; Eye tracking; Eyeglasses; Pupil detection; Reflection; Refraction; Rendering","Algorithms; Calibration; Eyeglasses; Reflection; Refraction; Appearance based; Detection algorithm; Eye-tracking; Ground truth data; Model-based OPC; Polynomial functions; Pupil detection; Rendering; Rendering (computer graphics)",Conference Paper,"Final","",Scopus,2-s2.0-84975250739
"Kar A., Bazrafkan S., Ostache C.C., Corcoran P.","56956378200;56403389300;56956976400;57190839462;","Eye-gaze systems - An analysis of error sources and potential accuracy in consumer electronics use cases",2016,"2016 IEEE International Conference on Consumer Electronics, ICCE 2016",,,"7430628","319","320",,6,"10.1109/ICCE.2016.7430628","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965147135&doi=10.1109%2fICCE.2016.7430628&partnerID=40&md5=62905797e31e5c6f114b780a9640453e","Center for Cognitive, Connected and Computational Imaging, College of Engineering and Informatics, NUI Galway, Galway, Ireland","Kar, A., Center for Cognitive, Connected and Computational Imaging, College of Engineering and Informatics, NUI Galway, Galway, Ireland; Bazrafkan, S., Center for Cognitive, Connected and Computational Imaging, College of Engineering and Informatics, NUI Galway, Galway, Ireland; Ostache, C.C., Center for Cognitive, Connected and Computational Imaging, College of Engineering and Informatics, NUI Galway, Galway, Ireland; Corcoran, P., Center for Cognitive, Connected and Computational Imaging, College of Engineering and Informatics, NUI Galway, Galway, Ireland","Several generic CE use cases and corresponding techniques for eye gaze estimation (EGE) are reviewed. The optimal approaches for each use case are determined from a review of recent literature. In addition, the most probable error sources for EGE are determined and the impact of these error sources is quantified. A discussion and analysis of the research outcome is given and future work outlined. © 2016 IEEE.","eye gaze estimation methods; eye-trackers; gaze estimation error sources; smartphones","Consumer electronics; Estimation; Smartphones; Error sources; Eye trackers; Eye-gaze; Gaze estimation; Optimal approaches; Probable errors; Research outcome; Errors",Conference Paper,"Final","",Scopus,2-s2.0-84965147135
"Zhang Z., Shen Y., Lin W., Zhou B.","57190802490;56896455800;24741037600;55823863000;","Eye corner detection with texture image fusion",2016,"2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2015",,,"7415420","992","995",,2,"10.1109/APSIPA.2015.7415420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986239350&doi=10.1109%2fAPSIPA.2015.7415420&partnerID=40&md5=ce31e920c89099bc50a39ebb22754c0b","Department of Electronic Engineering, Shanghai Jiao Tong University, China; School of Information Engineering, Zhengzhou University, China","Zhang, Z., Department of Electronic Engineering, Shanghai Jiao Tong University, China; Shen, Y., Department of Electronic Engineering, Shanghai Jiao Tong University, China; Lin, W., Department of Electronic Engineering, Shanghai Jiao Tong University, China; Zhou, B., School of Information Engineering, Zhengzhou University, China","The localization of eye corner is of great importance since it offers crucial information in various face-related applications including face tracking, gaze estimation, and facial expression recognition. In this paper, a new approach is proposed which localizes eye corners in a precise and robust way. In our approach, we first estimate a rough location about an eye corner. Then, a set of texture images are derived from regions around the estimated eye corner location. These regions are then fused together to decide the reliability of the estimated eye corner location, and an eye corner's location will be re-estimated if the current estimated eye corner is evaluated as unreliable. Finally, a local refinement process is also applied which refines the location of the estimated eye corner to create a more satisfactory result. Experiments show that our approach can achieve better detection results than the existing methods.1 © 2015 Asia-Pacific Signal and Information Processing Association.",,"Face recognition; Image fusion; Image texture; Information science; Location; Eye corners; Face Tracking; Facial expression recognition; Gaze estimation; Local refinement; New approaches; Rough location; Texture image; Edge detection",Conference Paper,"Final","",Scopus,2-s2.0-84986239350
"Shimizu M., Fukui K.","36445092700;57189522778;","Eye-gaze estimation accuracy and key in human vision",2016,"IEEE 2015 International Conference on Signal and Image Processing Applications, ICSIPA 2015 - Proceedings",,,"7412162","48","53",,1,"10.1109/ICSIPA.2015.7412162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971636068&doi=10.1109%2fICSIPA.2015.7412162&partnerID=40&md5=5c1f974973e6b9bb94877deafb05436f","Graduate School of Science and Technology, Nihon University, 7241, Narashinodai, Funabashi, 2748501, Japan","Shimizu, M., Graduate School of Science and Technology, Nihon University, 7241, Narashinodai, Funabashi, 2748501, Japan; Fukui, K., Graduate School of Science and Technology, Nihon University, 7241, Narashinodai, Funabashi, 2748501, Japan","Eye movement and eye-gaze direction have significant roles in human communications. Eye-gaze direction often indicates what a person is thinking about or trying to do. Empirically, it is possible to recognize which eye (right or left) is being fixated by a person who is standing in front of us at 1 m distance; this indicates that our eye-gaze estimation accuracy is less than 3.7 deg. This paper describes our specific examination of eye-gaze direction estimation accuracy and method in the human vision system. Extensive measurements show that the eye-gaze estimation accuracy is 2-4 deg from 1 m distance. However, they also indicate that individual eye-gaze estimation errors differ in magnitude and tendency. A person who has good eyesight estimates well, in general. Furthermore, measurements reveal that human vision might use an eyeball model-based method that employs the center of the iris or pupil, rather than an iris shape-based method with an elliptical iris shape. © 2015 IEEE.",,"Computer vision; Image processing; Eye-gaze; Human communications; Human vision; Human vision systems; Model-based method; Shape based; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84971636068
"Narcizo F.B., Hansen D.W.","56446936200;15063910800;","Depth Compensation Model for Gaze Estimation in Sport Analysis",2016,"Proceedings of the IEEE International Conference on Computer Vision","2016-February",,"7406456","788","795",,8,"10.1109/ICCVW.2015.107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961997641&doi=10.1109%2fICCVW.2015.107&partnerID=40&md5=da7c028032db75e773c83f1a846cf0f1","IT University of Copenhagen, Rued Langgaards Vej 7, København S., 2300, Denmark","Narcizo, F.B., IT University of Copenhagen, Rued Langgaards Vej 7, København S., 2300, Denmark; Hansen, D.W., IT University of Copenhagen, Rued Langgaards Vej 7, København S., 2300, Denmark","A depth compensation model is presented as a novel approach to reduce the effects of parallax error for head-mounted eye trackers. The method can reduce the parallax error when the distance between the user and the target is prior known. The model is geometrically presented and its performance is tested in a totally controlled environment with aim to check the influences of eye tracker parameters and ocular biometric parameters on its behavior. We also present a gaze estimation method based on epipolar geometry for binocular eye tracking setups. The depth compensation model has shown very promising to the field of eye tracking. It can reduce 10 times less the influence of parallax error in multiple depth planes. © 2015 IEEE.","Calibration; Cameras; Estimation; Gaze tracking; Geometry; Training; Transmission line matrix methods","Calibration; Cameras; Computer vision; Errors; Estimation; Eye movements; Geometrical optics; Geometry; Matrix algebra; Personnel training; Tracking (position); Biometric parameters; Compensation modeling; Controlled environment; Epipolar geometry; Gaze estimation; Gaze tracking; Parallax error; Transmission line matrix methods; Error compensation",Conference Paper,"Final","",Scopus,2-s2.0-84961997641
"El Hafi L., Takemura K., Takamatsu J., Ogasawara T.","57188836029;8575290600;35243684200;7201579979;","Model-based approach for gaze estimation from corneal imaging using a single camera",2016,"2015 IEEE/SICE International Symposium on System Integration, SII 2015",,,"7404959","88","93",,4,"10.1109/SII.2015.7404959","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963760990&doi=10.1109%2fSII.2015.7404959&partnerID=40&md5=08fc7cfb51ec1caad97813df1290475e","Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Department of Applied Computer Engineering, School of Information Science and Technology, Tokai University, 4-1-1 Kitakaname, Hiratsuka, Kanagawa, 259-1292, Japan","El Hafi, L., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Takemura, K., Department of Applied Computer Engineering, School of Information Science and Technology, Tokai University, 4-1-1 Kitakaname, Hiratsuka, Kanagawa, 259-1292, Japan; Takamatsu, J., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan; Ogasawara, T., Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan","This paper describes a method to estimate the gaze direction using cornea images captured by a single camera. The purpose is to develop wearable devices capable of obtaining natural user responses, such as interests and behaviors, from eye movements and scene images reflected on the cornea. From an image of the eye, an ellipse is fitted on the colored iris area. A 3D eye model is reconstructed from the ellipse and rotated to simulate projections of the iris area for different eye poses. The gaze direction is then evaluated by matching the iris area of the current image with the corresponding projection obtained from the model. We finally conducted an experiment using a head-mounted prototype to demonstrate the potential of such an eye-tracking method solely based on cornea images captured from a single camera. © 2015 IEEE.",,"Cameras; Three dimensional computer graphics; 3D eye models; Current image; Eye tracking methods; Gaze direction; Gaze estimation; Model based approach; Single cameras; Wearable devices; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84963760990
"Otani T., Matsuda H., Touyama H., Nakata T.","57188925651;15725355100;15023608800;35305504000;","Gaze Estimation Using Human Joint Rotation Angel",2016,"Proceedings - 2015 International Conference on Cyberworlds, CW 2015",,,"7398439","351","354",,3,"10.1109/CW.2015.59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964216435&doi=10.1109%2fCW.2015.59&partnerID=40&md5=2e560e3cae798b68cd4507642c8d4711","Toyama Prefectural University, Department of Information Systems Engineering, Toyama, Japan","Otani, T., Toyama Prefectural University, Department of Information Systems Engineering, Toyama, Japan; Matsuda, H., Toyama Prefectural University, Department of Information Systems Engineering, Toyama, Japan; Touyama, H., Toyama Prefectural University, Department of Information Systems Engineering, Toyama, Japan; Nakata, T., Toyama Prefectural University, Department of Information Systems Engineering, Toyama, Japan","Currently, eye trackers can estimate with high precision using eye images. This method requires high-quality eye images, and thus cannot estimate using low-quality eye images, which have low resolution, illumination changes, and occlusions. Other methods use pose information, including head direction, body direction, and head motion. This method obtains pose information using the Eigenspace method, and thus requires a calibration phase. In addition, the eye direction using this method is only horizontal. We propose a gaze estimation algorithm that does not require high quality eye images. The purpose is estimation without using eye images, with an error of less than ten degrees. Experimental results show that the method can estimate gaze direction without using eye images. © 2015 IEEE.","gaze estimation; pose estimation; user interface","Image recognition; User interfaces; Eigen space method; Gaze direction; Gaze estimation; High-precision; Illumination changes; Low resolution; Pose estimation; Pose information; Information use",Conference Paper,"Final","",Scopus,2-s2.0-84964216435
"Tamura K., Hashimoto K., Aoki Y.","55375922900;55477157400;35498779700;","Head pose-invariant eyelid and iris tracking method",2016,"Electronics and Communications in Japan","99","2",,"19","27",,1,"10.1002/ecj.11776","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954159736&doi=10.1002%2fecj.11776&partnerID=40&md5=d4aaf0714cdf79f3ae1cadf8b7b2795f","Keio University, Japan","Tamura, K., Keio University, Japan; Hashimoto, K., Keio University, Japan; Aoki, Y., Keio University, Japan","These days, there is more demand for a camera-based gaze estimation method for new interfaces and new marketing measurement tools. Considering these applications, the system should track a new user without any operation such as calibration. It should also admit user's natural head pose changes. Previous methods, however, need a calibration procedure before execution and have less accuracy in a head moving situation. In this paper, we propose a method which tracks the user's eyelids and iris automatically and accurately. Our method is a pretreatment of gaze estimation without any calibration and head pose restraint. First of all, we track the facial feature points from an input face image and estimate its head pose, extracting the eye region image. On the eye region image, we track the eyelid shape based on an eyelid shape model generated beforehand from PCA. Finally we track the iris inside the eyelid based on the eyeball model. The eyelid and iris tracking is processed by Particle Filter. An evaluation against a database including head pose changes confirmed that the accuracy of eyelid and iris tracking was improved compared with previous methods. © 2016 Wiley Periodicals, Inc.","3D reconstruction; ASM; eyelid tracking; gaze estimation; interface; iris tracking","Calibration; Face recognition; Interfaces (materials); 3D reconstruction; ASM; Calibration procedure; Facial feature points; Gaze estimation; Iris tracking; Measurement tools; Particle filter; Motion estimation",Article,"Final","",Scopus,2-s2.0-84954159736
"Pan Y., Steed A.","55549956700;18435050200;","Effects of 3D perspective on head gaze estimation with a multiview autostereoscopic display",2016,"International Journal of Human Computer Studies","86",,,"138","148",,8,"10.1016/j.ijhcs.2015.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946781493&doi=10.1016%2fj.ijhcs.2015.10.004&partnerID=40&md5=ca1e453dfa9ed8a6e6be72b82c8a229b","Virtual Environments and Computer Graphics Group, Department of Computer Science, University College, United Kingdom","Pan, Y., Virtual Environments and Computer Graphics Group, Department of Computer Science, University College, United Kingdom; Steed, A., Virtual Environments and Computer Graphics Group, Department of Computer Science, University College, United Kingdom","Head gaze, or the orientation of the head, is a very important attentional cue in face to face conversation. Some subtleties of the gaze can be lost in common teleconferencing systems, because a single perspective warps spatial characteristics. A recent random hole display is a potentially interesting display for group conversation, as it allows multiple stereo viewers in arbitrary locations, without the restriction of conventional autostereoscopic displays on viewing positions. We represented a remote person as an avatar on a random hole display. We evaluated this system by measuring the ability of multiple observers with different horizontal and vertical viewing angles to accurately and simultaneously judge which targets the avatar is gazing at. We compared three perspective conditions: a conventional 2D view, a monoscopic perspective-correct view, and a stereoscopic perspective-correct views. In the latter two conditions, the random hole display shows three and six views simultaneously. Although the random hole display does not provide high quality view, because it has to distribute display pixels among multiple viewers, the different views are easily distinguished. Results suggest the combined presence of perspective-correct and stereoscopic cues significantly improved the effectiveness with which observers were able to assess the avatar's head gaze direction. This motivates the need for stereo in future multiview displays. © 2015 Elsevier Ltd.","Autostereoscopic display; Avatars; Gaze; Random hole display; Telecommunication","Display devices; Stereo image processing; Telecommunication; Auto-stereoscopic display; Avatars; Face-to-face conversation; Gaze; Multi-view displays; Random holes; Spatial characteristics; Teleconferencing systems; Three dimensional computer graphics",Article,"Final","",Scopus,2-s2.0-84946781493
"Wang X., Liu K., Qian X.","56202868100;55554452700;57188569009;","A survey on gaze estimation",2016,"Proceedings - The 2015 10th International Conference on Intelligent Systems and Knowledge Engineering, ISKE 2015",,,"7383057","260","267",,6,"10.1109/ISKE.2015.12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966460129&doi=10.1109%2fISKE.2015.12&partnerID=40&md5=beaa601831b26052b772a06a52cc6a48","China University of Mining and Technology, Beijing School of Mechanical Electronic and Information Engineering, Beijing, China","Wang, X., China University of Mining and Technology, Beijing School of Mechanical Electronic and Information Engineering, Beijing, China; Liu, K., China University of Mining and Technology, Beijing School of Mechanical Electronic and Information Engineering, Beijing, China; Qian, X., China University of Mining and Technology, Beijing School of Mechanical Electronic and Information Engineering, Beijing, China","Eye orientation contains abundance of significant information for estimating user's intention and purpose. Large number of research on gaze direction has been given and improved in statistics, biometrics, organizational behavior, etc. Details of gaze methods are essential conditions for human-computer interactive control, human intention prediction, expression analysis, cognitive state analysis, etc. Although active and significant progress in last decades, eye detection still remains challenging to deviation of location, variability in scale, individuality of eyes, complexity of cameral, calibration of person and related instruments, fixation of head pose. In this paper, gaze estimation has been discussed and various models of gaze detection and tracking have been presented, which show current theoretical need to be further modified in order to improve general gaze tracking estimation solutions in computer vision and beyond. © 2015 IEEE.","detection and tracking; eye detection; gaze direction; gaze estimation; head pose estimation","Computer vision; Eye protection; Human computer interaction; Image recognition; Intelligent systems; Knowledge engineering; Tracking (position); Detection and tracking; Eye detection; Gaze direction; Gaze estimation; Head Pose Estimation; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84966460129
"Khonglah J.R., Khosla A.","57188986934;56227861800;","A low cost webcam based eye tracker for communicating through the eyes of young children with ASD",2016,"Proceedings on 2015 1st International Conference on Next Generation Computing Technologies, NGCT 2015",,,"7375255","925","928",,4,"10.1109/NGCT.2015.7375255","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964555842&doi=10.1109%2fNGCT.2015.7375255&partnerID=40&md5=24367a499acbb69a7fa8a9510cf7c8bb","Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, Punjab, 144011, India","Khonglah, J.R., Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, Punjab, 144011, India; Khosla, A., Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, Punjab, 144011, India","Evolutions in eye tracking systems and their distinct applications have led to their increased use in almost every field of Human-Computer Interaction (HCI). However, cost, setup and availability hinder the integration of these systems with daily use. This paper presents an infrared based detection technique to resist the ambient light variations, and combines scanning of a neighbourhood of 10×10 pixels and blob analysis to generate a gaze vector to calculate the gaze position. The main goal of this paper is to create a low cost and easy-to-setup webcam-based eye tracker that requires no calibration and using it as an assistive technology for young children with autism as a digital communication medium for a session of Applied Behavior Analysis (ABA). © 2015 IEEE.","autism spectrum disorder; cost; eye tracking; gaze estimation; human computer interaction","Cost benefit analysis; Costs; Digital communication systems; Diseases; Assistive technology; Autism spectrum disorders; Behavior analysis; Digital communications; Eye tracking systems; Eye-tracking; Gaze estimation; Human computer interaction (HCI); Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84964555842
"Wood E., Baltrušaitis T., Morency L.-P., Robinson P., Bulling A.","56145872800;36696075900;6603047400;57205369790;6505807414;","A 3D morphable model of the eye region",2016,"European Association for Computer Graphics - 37th Annual Conference, EUROGRAPHICS 2016 - Posters",,,,"35","36",,3,"10.2312/egp.20161054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046731942&doi=10.2312%2fegp.20161054&partnerID=40&md5=377e57dbb3c432cdf7803638013b7646","Computer Lab, University of Cambridge, United Kingdom; Language Technologies Institute, Carnegie Mellon University, United States; Perceptual User Interfaces, Max Planck Institute for Informatics, Germany","Wood, E., Computer Lab, University of Cambridge, United Kingdom; Baltrušaitis, T., Language Technologies Institute, Carnegie Mellon University, United States; Morency, L.-P., Language Technologies Institute, Carnegie Mellon University, United States; Robinson, P., Computer Lab, University of Cambridge, United Kingdom; Bulling, A., Perceptual User Interfaces, Max Planck Institute for Informatics, Germany","We present the first 3D morphable model that includes the eyes, enabling gaze estimation and gaze re-targetting from a single image. Morphable face models are a powerful tool and are used for a range of tasks including avatar animation and facial expression transfer. However, previous work has avoided the eyes, even though they play an important role in human communication. We built a new morphable model of the facial eye-region from high-quality head scan data, and combined this with a parametric eyeball model constructed from anatomical measurements and iris photos. We fit our models to an input RGB image, solving for shape, texture, pose, and scene illumination simultaneously. This provides us with an estimate of where a person is looking in a 3D scene without per-user calibration - a still unsolved problem in computer vision. It also allows us to re-render a person's eyes with different parameters, thus redirecting their perceived attention. © 2016 The Eurographics Association.",,"Computer graphics; Textures; 3D Morphable model; Facial Expressions; Gaze estimation; Human communications; Morphable face model; Morphable model; Unsolved problems; User calibration; 3D modeling",Conference Paper,"Final","",Scopus,2-s2.0-85046731942
"Montenegro J.M.F., Argyriou V.","57188851647;13806485100;","Gaze estimation using EEG signals for HCI in augmented and virtual reality headsets",2016,"Proceedings - International Conference on Pattern Recognition","0",,"7899793","1159","1164",,3,"10.1109/ICPR.2016.7899793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019054882&doi=10.1109%2fICPR.2016.7899793&partnerID=40&md5=810c69ab0228b35f63ff29cc8eb4f046","Kingston University, London, United Kingdom","Montenegro, J.M.F., Kingston University, London, United Kingdom; Argyriou, V., Kingston University, London, United Kingdom","Augmented and virtual reality have evolved significantly over the last few years providing new ways of entertainment and interaction with the environment. Although many systems and solutions are currently available, still there is much left unsettled and some technologies are missing from many VR/AR devices, such as foveated rendering and HCI. In this paper, a novel approach for coarse gaze estimation using EEG sensors with applications in items selection for HCI or foveated rendering for VR/AR devices is proposed. The suggested method requires only few electroencephalogram sensors that can be easily added to the current virtual and augmented reality headsets. A supervised machine leaning approach was suggested utilising novel features, based on quaternions allowing gaze estimation. Experiments were performed to evaluate the proposed method and a new dataset was designed and captured. Finally, the introduced learning framework was compared with other similar techniques demonstrating further the gain of the proposed descriptors. © 2016 IEEE.",,"Augmented reality; Electroencephalography; Gallium compounds; Virtual reality; Augmented and virtual realities; Descriptors; EEG signals; Gaze estimation; Learning frameworks; Machine leaning; Virtual and augmented reality; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85019054882
"Bartoli F., Lisanti G., Seidenari L., Del Bimbo A.","56462292700;35519750700;36015900900;15018931800;","User interest profiling using tracking-free coarse gaze estimation",2016,"Proceedings - International Conference on Pattern Recognition","0",,"7899904","1839","1844",,1,"10.1109/ICPR.2016.7899904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019053621&doi=10.1109%2fICPR.2016.7899904&partnerID=40&md5=14c199b948b0c2931cbf75e82e9b9bcf","Media Integration and Communication Center, Università Degli Studi di Firenze, Italy","Bartoli, F., Media Integration and Communication Center, Università Degli Studi di Firenze, Italy; Lisanti, G., Media Integration and Communication Center, Università Degli Studi di Firenze, Italy; Seidenari, L., Media Integration and Communication Center, Università Degli Studi di Firenze, Italy; Del Bimbo, A., Media Integration and Communication Center, Università Degli Studi di Firenze, Italy","Understanding where people attention focuses is a challenging and extremely valuable task that can be solved using computer vision technologies. In this paper we address this problem on surveillance-like scenarios, where head and body imagery are usually low resolution. We propose a method to profile the attention of people moving in a known space. We exploit coarse gaze estimation and a novel model based on optical flow to improve attention prediction without the need of a tracker. Removing the tracker dependency makes the method applicable also on highly crowded scenarios. The proposed method is able to obtain comparable performance with respect to state of the art solutions in terms of Mean Average Angular Error (MAAE) on the TownCentre dataset. We also test our approach on the publicly available MuseumVisitors dataset showing an improvement both in terms of MAAE and in terms of accuracy in the estimation of visitors' profile. © 2016 IEEE.",,"Statistical tests; Angular errors; Attention focus; Computer vision technology; Gaze estimation; Low resolution; Model-based OPC; State of the art; User interests; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-85019053621
"Zhou X., Cai H., Shao Z., Yu H., Liu H.","55743240400;56763253600;55921729700;56115992300;54958434200;","3D eye model-based gaze estimation from a depth sensor",2016,"2016 IEEE International Conference on Robotics and Biomimetics, ROBIO 2016",,,"7866350","369","374",,6,"10.1109/ROBIO.2016.7866350","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016724706&doi=10.1109%2fROBIO.2016.7866350&partnerID=40&md5=0d95d92f94cf9eb91bd45969db6eba49","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; City University of Hong Kong, Shenzhen Research Institute, Shenzhen, Hong Kong; School of Computing, University of Portsmouth, Portsmouth, United Kingdom; School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom","Zhou, X., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China, City University of Hong Kong, Shenzhen Research Institute, Shenzhen, Hong Kong; Cai, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom; Shao, Z., College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China, City University of Hong Kong, Shenzhen Research Institute, Shenzhen, Hong Kong; Yu, H., School of Creative Technologies, University of Portsmouth, Portsmouth, United Kingdom; Liu, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom","In this paper, we address the 3D eye gaze estimation problem using a low-cost, simple-setup, and non-intrusive consumer depth sensor (Kinect sensor). We present an effective and accurate method based on 3D eye model to estimate the point of gaze of a subject with the tolerance of free head movement. To determine the parameters involved in the proposed eye model, we propose i) an improved convolution-based means of gradients iris center localization method to accurately and efficiently locate the iris center in 3D space; ii) a geometric constraints-based method to estimate the eyeball center under the constraints that all the iris center points are distributed on a sphere originated from the eyeball center and the sizes of two eyeballs of a subject are identical; iii) an effective Kappa angle calculation method based on the fact that the visual axes of both eyes intersect at a same point with the screen plane. The final point of gaze is calculated by using the estimated eye model parameters. We experimentally evaluate our gaze estimation method on five subjects. The experimental results show the good performance of the proposed method with an average estimation accuracy of 3.78°, which outperforms several state-of-the-arts. © 2016 IEEE.",,"Biomimetics; Robotics; Center points; Depth sensors; Gaze estimation; Geometric constraint; Kinect sensors; Localization method; Point of gaze; State of the art; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-85016724706
"Wang J., Zhang G., Shi J.","55972836500;56991516800;7404496048;","2D gaze estimation based on Pupil-Glint vector using an artificial neural network",2016,"Applied Sciences (Switzerland)","6","6","174","","",,14,"10.3390/app6060174","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010746952&doi=10.3390%2fapp6060174&partnerID=40&md5=a98a31d74af2aa3fc4483ddf6c0fcbcb","School of Mechatronical Engineering, Beijing Institute of Technology, 5 South Zhongguancun Street, Haidian District, Beijing, 100081, China","Wang, J., School of Mechatronical Engineering, Beijing Institute of Technology, 5 South Zhongguancun Street, Haidian District, Beijing, 100081, China; Zhang, G., School of Mechatronical Engineering, Beijing Institute of Technology, 5 South Zhongguancun Street, Haidian District, Beijing, 100081, China; Shi, J., School of Mechatronical Engineering, Beijing Institute of Technology, 5 South Zhongguancun Street, Haidian District, Beijing, 100081, China","Gaze estimation methods play an important role in a gaze tracking system. A novel 2D gaze estimation method based on the pupil-glint vector is proposed in this paper. First, the circular ring rays location (CRRL) method and Gaussian fitting are utilized for pupil and glint detection, respectively. Then the pupil-glint vector is calculated through subtraction of pupil and glint center fitting. Second, a mapping function is established according to the corresponding relationship between pupil-glint vectors and actual gaze calibration points. In order to solve the mapping function, an improved artificial neural network (DLSR-ANN) based on direct least squares regression is proposed. When the mapping function is determined, gaze estimation can be actualized through calculating gaze point coordinates. Finally, error compensation is implemented to further enhance accuracy of gaze estimation. The proposed method can achieve a corresponding accuracy of 1.29°, 0.89°, 0.52°, and 0.39° when a model with four, six, nine, or 16 calibration markers is utilized for calibration, respectively. Considering error compensation, gaze estimation accuracy can reach 0.36°. The experimental results show that gaze estimation accuracy of the proposed method in this paper is better than that of linear regression (direct least squares regression) and nonlinear regression (generic artificial neural network). The proposed method contributes to enhancing the total accuracy of a gaze tracking system.","Direct least squares regression; Gaze estimation; Gaze tracking; Human-computer interaction; Improved artificial neural network; Pupil-glint vector",,Article,"Final","",Scopus,2-s2.0-85010746952
"Papoutsaki A., Daskalova N., Sangkloy P., Huang J., Laskey J., Hays J.","55634314200;56159379000;57190444920;55742390500;57192392824;7102191400;","WebGazer: Scalable webcam eye tracking using user interactions",2016,"IJCAI International Joint Conference on Artificial Intelligence","2016-January",,,"3839","3845",,93,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006149549&partnerID=40&md5=0fa53cfb2ae49a307566deb06c14d892","Brown University, United States; Georgia Institute of Technology, United States","Papoutsaki, A., Brown University, United States; Daskalova, N., Brown University, United States; Sangkloy, P., Georgia Institute of Technology, United States; Huang, J., Brown University, United States; Laskey, J., Brown University, United States; Hays, J., Georgia Institute of Technology, United States","We introduce WebGazer, an online eye tracker that uses common webcams already present in laptops and mobile devices to infer the eye-gaze locations of web visitors on a page in real time. The eye tracking model self-calibrates by watching web visitors interact with the web page and trains a mapping between features of the eye and positions on the screen. This approach aims to provide a natural experience to everyday users that is not restricted to laboratories and highly controlled user studies. WebGazer has two key components: a pupil detector that can be combined with any eye detection library, and a gaze estimator using regression analysis informed by user interactions. We perform a large remote online study and a small in-person study to evaluate WebGazer. The findings show that WebGazer can learn from user interactions and that its accuracy is sufficient for approximating the user's gaze. As part of this paper, we release the first eye tracking library that can be easily integrated in any website for real-time gaze interactions, usability studies, or web research.",,"Artificial intelligence; Eye protection; Regression analysis; Eye detection; Eye trackers; Eye-tracking; Gaze interaction; Online studies; Usability studies; User interaction; Web visitors; Websites",Conference Paper,"Final","",Scopus,2-s2.0-85006149549
"Liu Y., Lee B.S., Sluzek A., Rajan D., McKeown M.","57192561421;7405441352;6701500691;7005909381;7005375626;","Feasibility analysis of eye typing with a standard webcam",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9914 LNCS",,,"254","268",,2,"10.1007/978-3-319-48881-3_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996922096&doi=10.1007%2f978-3-319-48881-3_18&partnerID=40&md5=0f5062eb19a168820d2a2bb7cba29ef9","Nanyang Institute of Technology in Health and Medicine, Singapore, Singapore; SCSE, Nanyang Technological University, Singapore, Singapore; Khalifa University, Abu Dhabi, United Arab Emirates; University of British Columbia, Vancouver, BC, Canada","Liu, Y., Nanyang Institute of Technology in Health and Medicine, Singapore, Singapore, SCSE, Nanyang Technological University, Singapore, Singapore; Lee, B.S., SCSE, Nanyang Technological University, Singapore, Singapore; Sluzek, A., Khalifa University, Abu Dhabi, United Arab Emirates; Rajan, D., SCSE, Nanyang Technological University, Singapore, Singapore; McKeown, M., University of British Columbia, Vancouver, BC, Canada","With the development of assistive technology, eye typing has become an alternative form of text entry for physically challenged people with severe motor disabilities. However, additional eye-tracking devices need to be used to track eye movements which is inconvenient in some cases. In this paper, we propose an appearance-based method to estimate the person’s gaze point using a webcam, and also investigate some practical issues of the method. The experimental results demonstrate the feasibility of eye typing using the proposed method. © Springer International Publishing Switzerland 2016.","Appearance-based method; Assistive technology; Eye typing; Gaze estimation","Computer vision; Appearance-based methods; Assistive technology; Eye tracking devices; Eye typing; Feasibility analysis; Gaze estimation; Motor disability; Practical issues; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84996922096
"Ghiass R.S., Arandjelovic O.","25654959400;8557887000;","Highly accurate gaze estimation using a consumer RGB-D Sensor",2016,"IJCAI International Joint Conference on Artificial Intelligence","2016-January",,,"3368","3374",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994582884&partnerID=40&md5=10fdb5bd83c2bc33a8ce90cdfdf68468","Université Laval, Québec, QC  G1V 0A6, Canada; University of St.Andrews, St Andrews, KY16 9SX, United Kingdom","Ghiass, R.S., Université Laval, Québec, QC  G1V 0A6, Canada; Arandjelovic, O., University of St.Andrews, St Andrews, KY16 9SX, United Kingdom","Determining the direction in which a person is looking is an important problem in a wide range of HCI applications. In this paper we describe a highly accurate algorithm that performs gaze estimation using an affordable and widely available device such as Kinect. The method we propose starts by performing accurate head pose estimation achieved by fitting a person specific morphable model of the face to depth data. The ordinarily competing requirements of high accuracy and high speed are met concurrently by formulating the fitting objective function as a combination of terms which excel either in accurate or fast fitting, and then by adaptively adjusting their relative contributions throughout fitting. Following pose estimation, pose normalization is done by re-rendering the fitted model as a frontal face. Finally gaze estimates are obtained through regression from the appearance of the eyes in synthetic, normalized images. Using EYEDIAP, the standard public dataset for the evaluation of gaze estimation algorithms from RGB-D data, we demonstrate that our method greatly outperforms the state of the art.",,"Artificial intelligence; Competing requirements; Head Pose Estimation; Normalized image; Objective functions; Pose estimation; Pose normalization; Relative contribution; State of the art; Image recognition",Conference Paper,"Final","",Scopus,2-s2.0-84994582884
"Ganin Y., Kononenko D., Sungatullina D., Lempitsky V.","56938634700;57142551900;55821404200;22234735100;","DeepWarp: Photorealistic image resynthesis for gaze manipulation",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9906 LNCS",,,"311","326",,43,"10.1007/978-3-319-46475-6_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990855318&doi=10.1007%2f978-3-319-46475-6_20&partnerID=40&md5=fd3c8c709656ec40d33ef6a915a04f43","Skolkovo Institute of Science and Technology, Skolkovo, Moscow Region, Russian Federation","Ganin, Y., Skolkovo Institute of Science and Technology, Skolkovo, Moscow Region, Russian Federation; Kononenko, D., Skolkovo Institute of Science and Technology, Skolkovo, Moscow Region, Russian Federation; Sungatullina, D., Skolkovo Institute of Science and Technology, Skolkovo, Moscow Region, Russian Federation; Lempitsky, V., Skolkovo Institute of Science and Technology, Skolkovo, Moscow Region, Russian Federation","In this work, we consider the task of generating highlyrealistic images of a given face with a redirected gaze. We treat this problem as a specific instance of conditional image generation and suggest a new deep architecture that can handle this task very well as revealed by numerical comparison with prior art and a user study. Our deep architecture performs coarse-to-fine warping with an additional intensity correction of individual pixels. All these operations are performed in a feed-forward manner, and the parameters associated with different operations are learned jointly in the end-to-end fashion. After learning, the resulting neural network can synthesize images with manipulated gaze, while the redirection angle can be selected arbitrarily from a certain range and provided as an input to the network. © Springer International Publishing AG 2016.","Deep learning; Gaze correction; Spatial transformers; Supervised learning; Warping","Network architecture; Supervised learning; Coarse to fine; Deep architectures; Deep learning; Gaze manipulations; Image generations; Numerical comparison; Photorealistic images; Warping; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84990855318
[无可用作者姓名],[无可用的作者 ID],"14th European Conference on Computer Vision, ECCV 2016",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9906 LNCS",,,"1","886",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990851680&partnerID=40&md5=aea5466949f04701ec419fd80569bd7c",,"","The proceedings contain 53 papers. The special focus in this conference is on Computer Vision. The topics include: Learning visual representations via physical interactions; image co-localization by mimicking a good detector’s confidence score distribution; facilitating and exploring planar homogeneous texture for indoor scene understanding; modeling context in referring expressions; taxonomy-regularized semantic deep convolutional neural networks; ground truth from computer games; revisiting additive quantization; photometric stereo under non-uniform light intensities and exposures; visual motif discovery via first-person vision; a cluster sampling method for image matting via sparse coding; fundamental matrices from moving objects using line motion barcodes; fashion landmark detection in the wild; leveraging visual question answering for image-caption ranking; real-time joint tracking of a hand manipulating an object from RGB-D input; photorealistic image resynthesis for gaze manipulation; head reconstruction from internet photos; accelerating the super-resolution convolutional neural network; peak-piloted deep network for facial expression recognition; coarse-to-fine planar regularization for dense monocular depth estimation; deep attributes driven multi-camera person re-identification; an occlusion-resistant ellipse detection method by joining coelliptic arcs; stereo video deblurring; title generation for user generated videos; natural image matting using deep convolutional neural networks; amodal instance segmentation; an efficient fusion move algorithm for the minimum cost lifted multicut problem; normalized cut Meets MRF; polysemous codes and a deep learning-based approach to progressive vehicle re-identification for urban surveillance.",,,Conference Review,"Final","",Scopus,2-s2.0-84990851680
"Bylinskii Z., Recasens A., Borji A., Oliva A., Torralba A., Durand F.","55906691600;57189096003;23395793600;7102603967;7005432728;57203217631;","Where should saliency models look next?",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9909 LNCS",,,"809","824",,67,"10.1007/978-3-319-46454-1_49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990060936&doi=10.1007%2f978-3-319-46454-1_49&partnerID=40&md5=91ea69ea9a17837f7117c0b67338c4a3","Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, United States; Center for Research in Computer Vision, University of Central Florida, Orlando, United States","Bylinskii, Z., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, United States; Recasens, A., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, United States; Borji, A., Center for Research in Computer Vision, University of Central Florida, Orlando, United States; Oliva, A., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, United States; Torralba, A., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, United States; Durand, F., Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, United States","Recently, large breakthroughs have been observed in saliency modeling. The top scores on saliency benchmarks have become dominated by neural network models of saliency, and some evaluation scores have begun to saturate. Large jumps in performance relative to previous models can be found across datasets, image types, and evaluation metrics. Have saliency models begun to converge on human performance? In this paper, we re-examine the current state-of-the-art using a fine-grained analysis on image types, individual images, and image regions. Using experiments to gather annotations for high-density regions of human eye fixations on images in two established saliency datasets, MIT300 and CAT2000, we quantify up to 60% of the remaining errors of saliency models. We argue that to continue to approach human-level performance, saliency models will need to discover higher-level concepts in images: text, objects of gaze and action, locations of motion, and expected locations of people in images. Moreover, they will need to reason about the relative importance of image regions, such as focusing on the most important person in the room or the most informative sign on the road. More accurately tracking performance will require finer-grained evaluations and metrics. Pushing performance further will require higher-level image understanding. © Springer International Publishing AG 2016.","Deep learning; Eye movements; Image understanding; Saliency estimation; Saliency maps","Deep learning; Eye movements; Image understanding; Evaluation metrics; Fine-grained analysis; High density regions; Human-level performance; Neural network model; Saliency map; Saliency modeling; Tracking performance; Large dataset",Conference Paper,"Final","",Scopus,2-s2.0-84990060936
"Wood E., Baltrušaitis T., Morency L.-P., Robinson P., Bulling A.","56145872800;36696075900;6603047400;57205369790;6505807414;","A 3D morphable eye region model for gaze estimation",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9905 LNCS",,,"297","313",,43,"10.1007/978-3-319-46448-0_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990046250&doi=10.1007%2f978-3-319-46448-0_18&partnerID=40&md5=90d77d6b0461ba87d04f78b2cfec6f8d","University of Cambridge, Cambridge, United Kingdom; Carnegie Mellon University, Pittsburgh, United States; Max Planck Institute for Informatics, Saarbrücken, Germany","Wood, E., University of Cambridge, Cambridge, United Kingdom; Baltrušaitis, T., Carnegie Mellon University, Pittsburgh, United States; Morency, L.-P., Carnegie Mellon University, Pittsburgh, United States; Robinson, P., University of Cambridge, Cambridge, United Kingdom; Bulling, A., Max Planck Institute for Informatics, Saarbrücken, Germany","Morphable face models are a powerful tool, but have previously failed to model the eye accurately due to complexities in its material and motion. We present a new multi-part model of the eye that includes a morphable model of the facial eye region, as well as an anatomy-based eyeball model. It is the first morphable model that accurately captures eye region shape, since it was built from high-quality head scans. It is also the first to allow independent eyeball movement, since we treat it as a separate part. To showcase our model we present a new method for illumination- and head-pose–invariant gaze estimation from a single RGB image. We fit our model to an image through analysis-bysynthesis, solving for eye region shape, texture, eyeball pose, and illumination simultaneously. The fitted eyeball pose parameters are then used to estimate gaze direction. Through evaluation on two standard datasets we show that our method generalizes to both webcam and high-quality camera images, and outperforms a state-of-the-art CNN method achieving a gaze estimation accuracy of 9.44° in a challenging user-independent scenario. © Springer International Publishing AG 2016.","Analysis-by-synthesis; Gaze estimation; Morphable model","Computer vision; Textures; Analysis by synthesis; Eyeball movements; Gaze estimation; Morphable face model; Morphable model; Pose parameters; State of the art; User independents; Quality control",Conference Paper,"Final","",Scopus,2-s2.0-84990046250
"Qodseya M., Sanzari M., Ntouskos V., Pirri F.","57191408208;57189662937;55248262900;56990245000;","A3D: A device for studying gaze in 3D",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9913 LNCS",,,"572","588",,5,"10.1007/978-3-319-46604-0_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989877508&doi=10.1007%2f978-3-319-46604-0_41&partnerID=40&md5=338702dded73c383b32c9752684f68c2","ALCOR Lab, DIAG, Sapienza University of Rome, Rome, Italy","Qodseya, M., ALCOR Lab, DIAG, Sapienza University of Rome, Rome, Italy; Sanzari, M., ALCOR Lab, DIAG, Sapienza University of Rome, Rome, Italy; Ntouskos, V., ALCOR Lab, DIAG, Sapienza University of Rome, Rome, Italy; Pirri, F., ALCOR Lab, DIAG, Sapienza University of Rome, Rome, Italy","A wearable device for capturing 3D gaze information in indoor and outdoor environments is proposed. The hardware and software architecture of the device provides an estimate in quasi-real-time of 2.5D points of regard (POR) and then lift their estimations to 3D, by projecting them into the 3D reconstructed scene. The estimation procedure does not need any external device, and can be used both indoor and outdoor and with the subject wearing it moving, though some smooth constraint in the motion are required. To ensure a great flexibility with respect to depth a novel calibration method is proposed, which provides eye-scene calibration that explicitly takes into account depth information, in so ensuring a quite accurate estimation of the PORs. The experimental evaluation demonstrates that both 2.5D and 3D POR are accurately estimated. © Springer International Publishing Switzerland 2016.","3D gaze estimation; Point of regard in 3D scene; Wearable device","Calibration; Wearable technology; 3D scenes; Accurate estimation; Estimation procedures; Experimental evaluation; Gaze estimation; Hardware and software architectures; Outdoor environment; Wearable devices; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84989877508
"Tősér Z., Rill R.A., Faragó K., Jeni L.A., Lőrincz A.","56298422000;57191287116;56578404500;24474645300;26643373200;","Personalization of gaze direction estimation with deep learning",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9904 LNAI",,,"200","207",,4,"10.1007/978-3-319-46073-4_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988628720&doi=10.1007%2f978-3-319-46073-4_20&partnerID=40&md5=6c954cf47f909c096d818426c02496da","Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Faculty of Mathematics and Informatics, Babes-Bolyai University, Cluj-napoca, Romania","Tősér, Z., Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary; Rill, R.A., Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary, Faculty of Mathematics and Informatics, Babes-Bolyai University, Cluj-napoca, Romania; Faragó, K., Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary; Jeni, L.A., Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States; Lőrincz, A., Faculty of Informatics, Eötvös Loránd University, Budapest, Hungary","There is a growing interest in behavior based biometrics. Although biometric data has considerable variations for an individual and may be faked, yet the combination of such ‘weak experts’ can be rather strong. A remotely detectable component is gaze direction estimation and thus, eye movement patterns. Here, we present a novel personalization method for gaze estimation systems, which does not require a precise calibration setup, can be non-obtrusive, is fast and easy to use. We show that it improves the precision of gaze direction estimation algorithms considerably. The method is convenient; we exploit 3D face model reconstruction for the enrichment of a small number of collected data artificially. © Springer International Publishing AG 2016.",,"Artificial intelligence; Biometrics; Eye movements; 3-D face modeling; Behavior-based; Biometric data; Deep learning; Eye movement patterns; Gaze direction; Gaze estimation; Personalizations; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84988628720
[无可用作者姓名],[无可用的作者 ID],"39th German Conference on Artificial Intelligence, KI 2016",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9904 LNAI",,,"1","316",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988591385&partnerID=40&md5=37102f5480215a5f51d3af1568c27530",,"","The proceedings contain 21 papers. The special focus in this conference is on Advances in Artificial Intelligence. The topics include: Providing built-in counters in a declarative dynamic programming environment; model based augmentation and testing of an annotated hand pose dataset; lifted junction tree algorithm; improved diversity in nested rollout policy adaptation; a fast elimination method for pruning in POMDPs; a neural field approach to obstacle avoidance; robotic home assistant with memory aid functionality; declarative decomposition and dispatching for large-scale job-shop scheduling; learning event time series for the automated quality control of videos; trending topic aggregation by news-based context modeling; image-based identification of plant species using a model-free approach and active learning; using a deep understanding of network activities for workflow mining; personalization of gaze direction estimation with deep learning; exception-enriched rule learning from knowledge graphs; towards clause-learning state space search; learning to recognize dead-ends; a column-oriented datalog reasoner; a new tableau-based satisfiability checker for linear temporal logic; investigating the relationship between argumentation semantics via signatures; decoupled strong stubborn sets; state-dependent cost partitionings for cartesian abstractions in classical planning; group decision making via probabilistic belief merging; simulating human inferences in the light of new information; efficient determination of measurement points for sequential diagnosis and discriminatively trained recurrent neural networks for continuous dimensional emotion recognition from audio.",,,Conference Review,"Final","",Scopus,2-s2.0-84988591385
"Calandra D.M., Mauro D.D., Cutugno F., Martino S.D.","57219462932;56529286800;13104207600;9640416000;","Navigating wall-sized displays with the gaze: A proposal for cultural heritage",2016,"CEUR Workshop Proceedings","1621",,,"36","43",,13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984799722&partnerID=40&md5=a991e653bc84cc5d75adecf6b29bd980","Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, 80127, Italy","Calandra, D.M., Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, 80127, Italy; Mauro, D.D., Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, 80127, Italy; Cutugno, F., Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, 80127, Italy; Martino, S.D., Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, 80127, Italy","New technologies for innovative interactive experience represent a powerful medium to deliver cultural heritage content to a wider range of users. Among them, Natural User Interfaces (NUI), i.e. non-intrusive technologies not requiring to the user to wear devices nor use external hardware (e.g. keys or trackballs), are considered a promising way to broader the audience of specific cultural heritage domains, like the navigation/interaction with digital artworks presented on wall-sized displays. Starting from a collaboration with a worldwide famous Italian designer, we defined a NUI to explore 360 panoramic artworks presented on wall-sized displays, like virtual reconstruction of ancient cultural sites, or rendering of imaginary places. Specifically, we let the user to ""move the head"" as way of natural interaction to explore and navigate through these large digital artworks. To this aim, we developed a system including a remote head pose estimator to catch movements of users standing in front of the wall-sized display: starting from a central comfort zone, as users move their head in any direction, the virtual camera rotates accordingly. With NUIs, it is difficult to get feedbacks from the users about the interest for the point of the artwork he/she is looking at. To solve this issue, we complemented the gaze estimator with a preliminary emotional analysis solution, able to implicitly infer the interest of the user for the shown content from his/her pupil size. A sample of 150 subjects was invited to experience the proposed interface at an International Design Week. Preliminary results show that the most of the subjects were able to properly interact with the system from the very first use, and that the emotional module is an interesting solution, even if further work must be devoted to address specific situations. © 2016 for this paper by its authors. Copying permitted for private and academic purposes.",,"Design; Digital devices; User interfaces; Cultural heritages; Emotional analysis; External hardware; International designs; Natural interactions; Natural user interfaces; Virtual reconstruction; Wall-sized displays; Display devices",Conference Paper,"Final","",Scopus,2-s2.0-84984799722
"Cai H., Yu H., Zhou X., Liu H.","56763253600;56115992300;55743240400;54958434200;","Robust gaze estimation via normalized iris center-eye corner vector",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9834 LNCS",,,"300","309",,5,"10.1007/978-3-319-43506-0_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981165326&doi=10.1007%2f978-3-319-43506-0_26&partnerID=40&md5=1e8913e351f2ea7ea9f347be44f7ac13","School of Computing, University of Portsmouth, Portsmouth, United Kingdom; Shenzhen Research Institute, City University of Hong Kong, Hong Kong","Cai, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom; Yu, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom; Zhou, X., Shenzhen Research Institute, City University of Hong Kong, Hong Kong; Liu, H., School of Computing, University of Portsmouth, Portsmouth, United Kingdom","Gaze estimation plays an important role in many practical scenarios such as human robot interaction. Although high accurate gaze estimation could be obtained in constrained settings with additional IR sources or depth sensors, single web-cam based gaze estimation still remains challenging. This paper propose a normalized iris center-eye corner (NIC-EC) vector based gaze estimation methods using a single, low cost web-cam. Firstly, reliable facial features and pupil centers are extracted. Then, the NIC-EC vector is proposed to enhance the robustness and accuracy for pupil center-eye corner vector based gaze estimations. Finally, an interpolation method is employed for the mapping between constructed vectors and points of regard. Experimental results showed that the proposed method has significantly improved the accuracy over the pupil center-eye corner vector based gaze estimation method with average accuracy of 1.66 ◦ under slight head movements. © Springer International Publishing Switzerland 2016.","Eye tracking; Gaze estimation; Interpolation; Normalized iris center-eye corner vector","Cams; Estimation; Eye movements; Human robot interaction; Interpolation; Negative impedance converters; Robotics; Robots; Depth sensors; Eye corners; Eye-tracking; Facial feature; Gaze estimation; Head movements; Interpolation method; Pupil centers; Vectors",Conference Paper,"Final","",Scopus,2-s2.0-84981165326
[无可用作者姓名],[无可用的作者 ID],"18th International Conference on Human-Computer Interaction, HCI International 2016",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9732",,,"1","433",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978886300&partnerID=40&md5=31ec33d8e3746a4ff042bd6e1cdf6328",,"","The proceedings contain 40 papers. The special focus in this conference is on Gesture, Motion-Based, Eye-gaze Based Interaction, Multimodal, Multisensory and Natural Interaction. The topics include: Combining low-cost eye trackers for dual monitor eye tracking; exploring the throughput potential of in-air pointing; a methodology to introduce gesture-based interaction into existing consumer product; dance biofeedback experiments in apperception; real-time gaze estimation using monocular vision; acceptable dwell time range for densely arranged object selection using video mirror interfaces; analysis of choreographed human movements using depth cameras; finding an efficient threshold for fixation detection in eye gaze tracking; hover detection using active acoustic sensing; identification of gracefulness feature parameters for hand-over motion; virtual reality interaction techniques for individuals with autism spectrum disorder; transition times for manipulation tasks in hybrid interfaces; BCI-related research focus at HCI international conference; optimal user interface parameters for dual-sided transparent screens in layered window conditions; bimodal speech recognition fusing audio-visual modalities; towards enhancing force-input interaction by visual-auditory feedback as an introduction of first use; mirroring book design and navigation in an e-book reader; temporal and spatial design of explanations in a multimodal system; the contribution of a virtual self and vibrotactile feedback to walking through virtual apertures; in-depth analysis of multimodal interaction; a survey of text entry techniques for smartwatches; software keyboard with predictive list for mobile device and effects of holding ring attached to mobile devices on pointing accuracy.",,,Conference Review,"Final","",Scopus,2-s2.0-84978886300
"Guo Z., Zhou Q., Liu Z., Zhang X., Xu Z., Lv Y.","56304391000;9632964600;55553990400;56193336600;56962890300;57190280859;","Real-time gaze estimation using monocular vision",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9732",,,"61","70",,,"10.1007/978-3-319-39516-6_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978870570&doi=10.1007%2f978-3-319-39516-6_6&partnerID=40&md5=9b008f0bf8637d1d71a315d323349c75","School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; China National Institute of Standardization, Beijing, 100191, China","Guo, Z., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Zhou, Q., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Liu, Z., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Zhang, X., China National Institute of Standardization, Beijing, 100191, China; Xu, Z., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China; Lv, Y., School of Biological Science and Medical Engineering, Beihang University, Beijing, 100191, China","Improving the accuracy of gaze estimation and the tolerance of head motion is a common task in the field of gaze estimation. The core problem of gaze estimation is how to accurately build up the mapping relationship between image features and gaze position. To this end, we propose a method to reconstruct input features in the optimized subset as the key to our solution. The HOG feature is considered as the input feature. First, we found the closest calibration point to gaze position and constituted the optimized subset. Then, we get a set of weights that can linear reconstruct test samples in the optimized subset. And this set of weights is used to express the mapping relationship. At last, a linear equation is fitted to solve head motion problem. In this paper, the experiment results demonstrate that our system can achieve high accuracy gaze estimation with one camera. © Springer International Publishing Switzerland 2016.","Feature reconstruction; Gaze estimation; Head move compensation; Optimized subset","Mapping; Set theory; Calibration points; Feature reconstruction; Gaze estimation; Image features; Input features; Mapping relationships; Monocular vision; Optimized subset; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84978870570
"Wijayasinghe I.B., Miller H.L., Das S.K., Bugnariu N.L., Popa D.O.","36679469200;53878078700;56383202800;16833547300;7102861346;","Human-like object tracking and gaze estimation with PKD android",2016,"Proceedings of SPIE - The International Society for Optical Engineering","9859",,"985906","","",,4,"10.1117/12.2224382","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978664196&doi=10.1117%2f12.2224382&partnerID=40&md5=a58f9e840835c179712f46048d12e3bf","University of Louisville, Louisville, KY  40292, United States; University of North Texas Health Science Center, Fort Worth, TX  76107, United States","Wijayasinghe, I.B., University of Louisville, Louisville, KY  40292, United States; Miller, H.L., University of North Texas Health Science Center, Fort Worth, TX  76107, United States; Das, S.K., University of Louisville, Louisville, KY  40292, United States; Bugnariu, N.L., University of North Texas Health Science Center, Fort Worth, TX  76107, United States; Popa, D.O., University of Louisville, Louisville, KY  40292, United States","As the use of robots increases for tasks that require human-robot interactions, it is vital that robots exhibit and understand human-like cues for effective communication. In this paper, we describe the implementation of object tracking capability on Philip K. Dick (PKD) android and a gaze tracking algorithm, both of which further robot capabilities with regard to human communication. PKD's ability to track objects with human-like head postures is achieved with visual feedback from a Kinect system and an eye camera. The goal of object tracking with human-like gestures is twofold: to facilitate better human-robot interactions and to enable PKD as a human gaze emulator for future studies. The gaze tracking system employs a mobile eye tracking system (ETG; SensoMotoric Instruments) and a motion capture system (Cortex; Motion Analysis Corp.) for tracking the head orientations. Objects to be tracked are displayed by a virtual reality system, the Computer Assisted Rehabilitation Environment (CAREN; MotekForce Link). The gaze tracking algorithm converts eye tracking data and head orientations to gaze information facilitating two objectives: to evaluate the performance of the object tracking system for PKD and to use the gaze information to predict the intentions of the user, enabling the robot to understand physical cues by humans. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Gaze estimation; Humanoid robot; Object tracking","Anthropomorphic robots; Human robot interaction; Man machine systems; Robotics; Robots; Virtual reality; Visual communication; Effective communication; Gaze estimation; Gaze tracking system; Human communications; Humanoid robot; Motion capture system; Object Tracking; Virtual reality system; Tracking (position)",Conference Paper,"Final","",Scopus,2-s2.0-84978664196
"Chang Z., Qiu Q., Sapiro G.","57190190445;54956074400;7005450011;","Synthesis-based low-cost gaze analysis",2016,"Communications in Computer and Information Science","618",,,"95","100",,5,"10.1007/978-3-319-40542-1_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978259221&doi=10.1007%2f978-3-319-40542-1_15&partnerID=40&md5=c3d764ad25e022435ffacd839a5ae550","Electrical and Computer Engineering, Duke University, Durham, NC, United States","Chang, Z., Electrical and Computer Engineering, Duke University, Durham, NC, United States; Qiu, Q., Electrical and Computer Engineering, Duke University, Durham, NC, United States; Sapiro, G., Electrical and Computer Engineering, Duke University, Durham, NC, United States","Gaze analysis has gained much popularity over the years due to its relevance in a wide array of applications, including humancomputer interaction, fatigue detection, and clinical mental health diagnosis. However, accurate gaze estimation from low resolution images outside of the lab (in the wild) still proves to be a challenging task. The new Intel low-cost RealSense 3D camera, capable of acquiring submillimeter resolution depth information, is currently available in laptops, and such technology is expected to become ubiquitous in other portable devices. In this paper, we focus on low-cost, scalable and real time analysis of human gaze using this RealSense camera. We exploit the direct measurement of eye surface geometry captured by the RGB-D camera, and perform gaze estimation through novel synthesis-based training and testing. Furthermore, we synthesize different eye movement appearances using a linear approach. From each 3D eye training sample captured by the RealSense camera, we synthesize multiple novel 2D views by varying the view angle to simulate head motions expected at testing. We then learn from the synthesized 2D eye images a gaze regression model using regression forests. At testing, for each captured RGB-D eye image, we first repeat the same synthesis process. For each synthesized image, we estimate the gaze from our gaze regression model, and factor-out the associated camera/head motion. In this way, we obtain multiple gaze estimations for each RGB-D eye image, and the consensus is adopted. We show that this synthesis-based training and testing significantly improves the precision in gaze estimation, opening the door to true low-cost solutions. © Springer International Publishing Switzerland 2016.",,"Abstracting; Cameras; Cost benefit analysis; Costs; Eye movements; Regression analysis; Direct measurement; Low resolution images; Real time analysis; Regression forests; Sub-millimeter resolutions; Surface geometries; Synthesized images; Training and testing; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-84978259221
"Van De Camp F., Gill D., Hild J., Beyerer J.","22235493700;56145496000;44161228300;6603794653;","An analysis of accuracy requirements for automatic eyetracker recalibration at runtime",2016,"Communications in Computer and Information Science","618",,,"149","154",,,"10.1007/978-3-319-40542-1_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978216504&doi=10.1007%2f978-3-319-40542-1_24&partnerID=40&md5=af9677954956090db2b693209345b120","Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Fraunhofer IOSB, Karlsruhe, Germany","Van De Camp, F., Fraunhofer IOSB, Karlsruhe, Germany; Gill, D., Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Hild, J., Fraunhofer IOSB, Karlsruhe, Germany; Beyerer, J., Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany, Fraunhofer IOSB, Karlsruhe, Germany","The initial calibration of an eye-tracker is a crucial step to provide accurate gaze data, often as a position on a screen. Issues influencing the calibration such as the user’s pose can change while using the eye tracker. Hence, recalibration might often be necessary but at the expense of interrupting the user executing the working task. Monitoring interactions such as clicks on a target or detecting salient objects could provide recalibration points without deliberate user interaction. To gain insight into how accurate recalibration points must be localized to ensure that gaze estimation accuracy is improved, we conducted a user study and examined the effect of correct as well as erroneous localization of recalibration points. The results show that even a localization error of 1.2 degrees of visual angle induces an error of less than 0.5◦ to the estimated gaze position on screen. Our results indicate the necessary requirements any method automatically providing recalibration points has to fulfill. © Springer International Publishing Switzerland 2016.",,"Abstracting; Eye trackers; Gain insight; Gaze estimation; Localization errors; Recalibrations; Salient objects; User interaction; Visual angle; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84978216504
"Sharma D., Abrol P.","57198421191;26632764700;","Comparative evaluation of SVD-TR model for eye gaze-based systems",2016,"Advances in Intelligent Systems and Computing","438",,,"189","197",,,"10.1007/978-981-10-0767-5_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974695813&doi=10.1007%2f978-981-10-0767-5_21&partnerID=40&md5=79797d43368931f73f4c68358c7dd240","Department of Computer Science & IT, University of Jammu, Jammu, J&K, India","Sharma, D., Department of Computer Science & IT, University of Jammu, Jammu, J&K, India; Abrol, P., Department of Computer Science & IT, University of Jammu, Jammu, J&K, India","Eye gaze techniques require a number of eye inputs which can be taken by a capturing device like digital camera, webcam, etc. These eye inputs are usually in the form of digital images. With some powerful software, features can be removed or replaced in a digital image without any detectable trace and such operations are called tampering. Tampering also includes the addition of noise which is an unwanted data applied to image to disturb its basic features and results in false information. Therefore, it becomes essential to identify the tampering extent for such images. In this research, SVD-based noise detection and removal (SVD-TR) model has been applied to remove noise (salt-pepper and Gaussian) from eye gaze-based image database. The results show that SVD-TR model removes noise effectively from eye gaze-based image database. To check the efficiency of SVD-TR model, the results obtained are compared with median filter. © Springer Science+Business Media Singapore 2016.","Eye gaze-based systems; Gaussian noise; Gaze estimation; Noise; Salt-pepper; SVD; Tampering","Gaussian noise (electronic); Image processing; Median filters; Singular value decomposition; Comparative evaluations; Digital image; Eye-gaze; Gaze estimation; Image database; Noise; Noise detection; Tampering; Salt removal",Conference Paper,"Final","",Scopus,2-s2.0-84974695813
"Choi I.-H., Hong S.K., Kim Y.-G.","55722762400;7405767196;24081003200;","Real-time categorization of driver's gaze zone using the deep learning techniques",2016,"2016 International Conference on Big Data and Smart Computing, BigComp 2016",,,"7425813","143","148",,45,"10.1109/BIGCOMP.2016.7425813","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964589703&doi=10.1109%2fBIGCOMP.2016.7425813&partnerID=40&md5=87c269778e0f3d0bdea37eb037e327a4","Dept. of Computer Eng., Sejong University, Kwangjin-Gu, Seoul, South Korea; Dept. of Aerospace, Sejong University, Kwangjin-Gu, Seoul, South Korea","Choi, I.-H., Dept. of Computer Eng., Sejong University, Kwangjin-Gu, Seoul, South Korea; Hong, S.K., Dept. of Aerospace, Sejong University, Kwangjin-Gu, Seoul, South Korea; Kim, Y.-G., Dept. of Computer Eng., Sejong University, Kwangjin-Gu, Seoul, South Korea","This paper presents a study in which driver's gaze zone is categorized using new deep learning techniques. Since the sequence of gaze zones of a driver reflects precisely what and how he behaves, it allows us infer his drowsiness, focusing or distraction by analyzing the images coming from a camera. A Haar feature based face detector is combined with a correlation filter based MOSS tracker for the face detection task to handle a tough visual environment in the car. Driving database is a big-data which was constructed using a recording setup within a compact sedan by driving around the urban area. The gaze zones consist of 9 categories depending on where a driver is looking at during driving. A convolutional neural network is trained to categorize driver's gaze zone from a given face detected image using a multi-GPU platform, and then its network parameters are transferred to a GPU within a PC running on Windows to operate in the real-time basis. Result suggests that the correct rate of gaze zone categorization reaches to 95% in average, indicating that our system outperforms the state-of-art gaze zone categorization methods based on conventional computer vision techniques. © 2016 IEEE.","Convolutional neural netwok; Deep learning; Driver distraction and fatigue; Driver's gaze zone; Real-time system","Computer vision; Convolution; Face recognition; Interactive computer systems; Learning algorithms; Learning systems; Neural networks; Real time systems; Categorization methods; Conventional computers; Convolutional neural netwok; Convolutional neural network; Correlation filters; Deep learning; Driver distractions; Visual environments; Big data",Conference Paper,"Final","",Scopus,2-s2.0-84964589703
"Wojke N., Hedrich J., Droege D., Paulus D.","55324976000;55805361800;23396527300;55787519500;","Gaze-estimation for consumer-grade cameras using a Gaussian process latent variable model",2016,"Pattern Recognition and Image Analysis","26","1",,"248","255",,2,"10.1134/S1054661816010296","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964054500&doi=10.1134%2fS1054661816010296&partnerID=40&md5=c2a4fb75102e82e8d1c036d8ed6de129","Active Vision Group, Institute for Computational Visualistics, University of Koblenz-Landau, Koblenz, 56070, Germany","Wojke, N., Active Vision Group, Institute for Computational Visualistics, University of Koblenz-Landau, Koblenz, 56070, Germany; Hedrich, J., Active Vision Group, Institute for Computational Visualistics, University of Koblenz-Landau, Koblenz, 56070, Germany; Droege, D., Active Vision Group, Institute for Computational Visualistics, University of Koblenz-Landau, Koblenz, 56070, Germany; Paulus, D., Active Vision Group, Institute for Computational Visualistics, University of Koblenz-Landau, Koblenz, 56070, Germany","Commercial gaze-tracking devices provide accurate measurements of the visual gaze and are applied to a broad range of problems in marketing, human-computer interaction, and health care technology. In some applications commercial systems are either unavailable or unaffordable. Therefore, developing low cost solutions using off the shelf components is worthwhile. In the paper at hand, we apply a hierarchy of Gaussian processes, a class of probabilistic function regressors, to the problem of visual gaze-tracking for consumer grade cameras. Gaussian process latent variable models lead to a lower dimensional manifold which represents the gaze space. Finally, a Gaussian process mapping from screen coordinates to gaze manifold enables us to seek for the users visual gaze point given a previously unseen eye-patch. In our experiments, we achieve mean errors of approximately 2 cm for a consumer grade webcam that is positioned 30-40 cm in front of the user. © 2016, Pleiades Publishing, Ltd.","eye-tracking; Gaussian process regression; gaze-estimation","Cameras; Case based reasoning; Gaussian noise (electronic); Human computer interaction; Medical computing; Principal component analysis; Tracking (position); Eye-tracking; Gaussian process regression; Gaze estimation; Latent variable modeling; Latent variable models; Lower dimensional manifolds; Off-the-shelf components; Probabilistic functions; Gaussian distribution",Article,"Final","",Scopus,2-s2.0-84964054500
"Ferhat O., Vilarino F.","56095267500;6508105875;","Low Cost Eye Tracking: The Current Panorama",2016,"Computational Intelligence and Neuroscience","2016",,"8680541","","",,29,"10.1155/2016/8680541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962674334&doi=10.1155%2f2016%2f8680541&partnerID=40&md5=9933d25b42cfb7c3f20490d542c100df","Computer Vision Center, Campus UAB, Bellaterra, 08193, Spain; Computer Science Department, Universitat Autonoma de Barcelona, Campus UAB, Bellaterra, 08193, Spain","Ferhat, O., Computer Vision Center, Campus UAB, Bellaterra, 08193, Spain, Computer Science Department, Universitat Autonoma de Barcelona, Campus UAB, Bellaterra, 08193, Spain; Vilarino, F., Computer Vision Center, Campus UAB, Bellaterra, 08193, Spain, Computer Science Department, Universitat Autonoma de Barcelona, Campus UAB, Bellaterra, 08193, Spain","Despite the availability of accurate, commercial gaze tracker devices working with infrared (IR) technology, visible light gaze tracking constitutes an interesting alternative by allowing scalability and removing hardware requirements. Over the last years, this field has seen examples of research showing performance comparable to the IR alternatives. In this work, we survey the previous work on remote, visible light gaze trackers and analyze the explored techniques from various perspectives such as calibration strategies, head pose invariance, and gaze estimation techniques. We also provide information on related aspects of research such as public datasets to test against, open source projects to build upon, and gaze tracking services to directly use in applications. With all this information, we aim to provide the contemporary and future researchers with a map detailing previously explored ideas and the required tools. © 2016 Onur Ferhat and Fernando Vilarinõ.",,"Light; Gaze estimation; Gaze tracker; Gaze tracking; Head pose; Low cost eye tracking; Open source projects; Visible light; Tracking (position); eye fixation; human; software; Fixation, Ocular; Humans; Software",Article,"Final","",Scopus,2-s2.0-84962674334
"Sato Y., Sugano Y., Sugimoto A., Kuno Y., Koike H.","35230954300;7005470045;23391348700;7103275980;7202758799;","Sensing and controlling human gaze in daily living space for human-harmonized information environments",2016,"Human-Harmonized Information Technology, Volume 1: Vertical Impact",,,,"199","237",,5,"10.1007/978-4-431-55867-5_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960810599&doi=10.1007%2f978-4-431-55867-5_8&partnerID=40&md5=f504f3dc2afe25a0196c644451363ea9","Institute of Industrial Science, The University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo, Japan; Max Planck Institute for Informatics, Saarbrücken, 66123, Germany; National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan; Saitama University, 255 Shimo-Okubo, Sakura-ku, Saitama, Japan; Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan","Sato, Y., Institute of Industrial Science, The University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo, Japan; Sugano, Y., Max Planck Institute for Informatics, Saarbrücken, 66123, Germany; Sugimoto, A., National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan; Kuno, Y., Saitama University, 255 Shimo-Okubo, Sakura-ku, Saitama, Japan; Koike, H., Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo, Japan","This chapter introduces new techniques we developed for sensing and guiding human gaze non-invasively in daily living space. Such technologies are the key to realize human-harmonized information systems which can provide us various kinds of supports effectively without distracting our activities. Toward the goal of realizing non-invasive gaze sensing, we developed gaze estimation techniques, which requires very limited or no calibration effort by exploiting various cues such as spontaneous attraction of our visual attention to visual stimuli. For shifting our gaze to desired locations in a non-disturbing and natural way, we exploited two approaches for gaze control: subtle modulation of visual stimuli based on visual saliency models, and non-verbal gestures in human-robot interactions. © Springer Japan 2016.","Appearance-based gaze sensing; Calibration-free gaze estimation; Gaze guidance; Visual saliency","Calibration; Visualization; Appearance based; Gaze estimation; Gaze guidances; Information environment; Visual Attention; Visual saliency; Visual saliency model; Visual stimulus; Human robot interaction",Book Chapter,"Final","",Scopus,2-s2.0-84960810599
"Cazzato D., Adamo F., Palestra G., Crifaci G., Pennisi P., Pioggia G., Ruta L., Leo M., Distante C.","55866556300;56203449200;55859663600;36125910400;57021391300;8957312900;14822531100;7006471658;55884135100;","Non-intrusive and calibration free visual exploration analysis in children with autism spectrum disorder",2016,"Computational Vision and Medical Image Processing V - Proceedings of  5th Eccomas Thematic Conference on Computational Vision and Medical Image Processing, VipIMAGE 2015",,,,"201","208",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959311963&partnerID=40&md5=37d0dae499134c67218d4f2f98391cac","University of Salento, Lecce, Italy; Department of Computer Science, University of Bari, Italy; Insitute of Clinical Physiology, National Research Council of Italy, Messina, Italy; Department of Developmental Neuroscience, Stella Maris Scientific Institute, Calambrone, Pisa, Italy; National Institute of Optics, National Research Council of Italy, Arnesano, LE, Italy","Cazzato, D., University of Salento, Lecce, Italy; Adamo, F., University of Salento, Lecce, Italy; Palestra, G., Department of Computer Science, University of Bari, Italy; Crifaci, G., Insitute of Clinical Physiology, National Research Council of Italy, Messina, Italy; Pennisi, P., Insitute of Clinical Physiology, National Research Council of Italy, Messina, Italy; Pioggia, G., Insitute of Clinical Physiology, National Research Council of Italy, Messina, Italy; Ruta, L., Department of Developmental Neuroscience, Stella Maris Scientific Institute, Calambrone, Pisa, Italy; Leo, M., National Institute of Optics, National Research Council of Italy, Arnesano, LE, Italy; Distante, C., National Institute of Optics, National Research Council of Italy, Arnesano, LE, Italy","Assistive technology is a generic system that is used to increase, help or improve the functional capabilities of people with disability. Recently, its employment has generated innovative solutions also in the field of Autism Spectrum Disorder (ASD), where it is extremely challenging to obtain feedback or to extract meaningful data. In this work, a study about the possibility to understand the visual exploration in children with ASD is presented. In order to obtain an automatic evaluation, an algorithm for free gaze estimation is employed. The proposed gaze estimation method can work without constrains nor using additional hardware, IR light sources or other intrusive methods. Furthermore, no initial calibration is required. These relaxations of the constraints makes the technique particularly suitable to be used in the critical context of autism, where the child is certainly not inclined to employ invasive devices. In particular, the technique is used in a scenario where a closet containing specific toys, that are neatly disposed from the therapist, is opened to the child. After a brief environment exploration, the child will freely choose the desired toy that will be subsequently used during therapy. The video acquisition have been accomplished by a Microsoft Kinect sensor hidden into the closet in order to obtain both RGB and depth images, that can be processed by the estimation algorithm, therefore computing gaze tracking by intersection with data coming from the well-known initial disposition of toys. The system has been tested with children with ASD, allowing to understand their choices and preferences, letting to optimize the toy disposition for cognitive-behavioural therapy. © 2016 Taylor & Francis Group, London.",,"Calibration; Diseases; Image processing; Light sources; Medical computing; Medical imaging; Tracking (position); Visualization; Autism spectrum disorders; Automatic evaluation; Children with autisms; Environment exploration; Functional capabilities; Innovative solutions; Microsoft Kinect sensors; People with disabilities; Medical image processing",Conference Paper,"Final","",Scopus,2-s2.0-84959311963
[无可用作者姓名],[无可用的作者 ID],"Proceedings - 10th International Workshop on Semantic and Social Media Adaptation and Personalization, SMAP 2015",2015,"Proceedings - 10th International Workshop on Semantic and Social Media Adaptation and Personalization, SMAP 2015",,,,"","",85,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964004985&partnerID=40&md5=6fb241900c4285cc168fadcd1cc57b49",,"","The proceedings contain 13 papers. The topics discussed include: from smart cities to smart communities: the case of children's independent mobility; counting and tracking people in a smart room: an IoT approach; query expansion for personalized cross-language information retrieval; interweaving deep learning and semantic techniques for emotion analysis in human-machine interaction; creating domain-specific semantic lexicons for aspect-based sentiment analysis; subspace search for community detection and community outlier mining in attributed graphs; keyword search in structured data and network analysis: a preliminary experiment over DBLP; augmented reality, smart codes and cloud computing for personalized interactive advertising on billboards; user and home appliances pervasive interaction in a sensor driven smart home environment: the SandS approach; gaze-tracked crowdsourcing; and semantic social analytics and linked open data cloud.",,,Conference Review,"Final","",Scopus,2-s2.0-84964004985
"Palinko O., Rea F., Sandini G., Sciutti A.","35099082400;54386065400;35280694500;36957816700;","Eye gaze tracking for a humanoid robot",2015,"IEEE-RAS International Conference on Humanoid Robots","2015-December",,"7363561","318","324",,13,"10.1109/HUMANOIDS.2015.7363561","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962266553&doi=10.1109%2fHUMANOIDS.2015.7363561&partnerID=40&md5=b3c3f7006197991799805d34183e2304","Robotics, Brain and Cognitive Sciences Department, Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy","Palinko, O., Robotics, Brain and Cognitive Sciences Department, Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Rea, F., Robotics, Brain and Cognitive Sciences Department, Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Sandini, G., Robotics, Brain and Cognitive Sciences Department, Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Sciutti, A., Robotics, Brain and Cognitive Sciences Department, Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy","Humans use eye gaze in their daily interaction with other humans. Humanoid robots, on the other hand, have not yet taken full advantage of this form of implicit communication. In this paper we present a passive monocular gaze tracking system implemented on the iCub humanoid robot. The validation of the system proved that it is a viable low-cost, calibration-free gaze tracking solution for humanoid platforms, with a mean absolute error of about 5 degrees on horizontal angle estimates. We also demonstrated the applicability of our system to human-robot collaborative tasks, showing that the eye gaze reading ability can enable successful implicit communication between humans and the robot. Finally, in the conclusion we give generic guidelines on how to improve our system and discuss some potential applications of gaze estimation for humanoid robots. © 2015 IEEE.","Cameras; Face; Feature extraction; Gaze tracking; Humanoid robots","Anthropomorphic robots; Cameras; Feature extraction; Gesture recognition; Tracking (position); Collaborative tasks; Eye gaze tracking; Face; Gaze tracking; Gaze tracking system; Humanoid robot; Implicit communications; Mean absolute error; Robots",Conference Paper,"Final","",Scopus,2-s2.0-84962266553
"Lee H., Seo J., Jo H.","35102576600;7401783766;56645819200;","Gaze tracking system using structure sensor & zoom camera",2015,"International Conference on ICT Convergence 2015: Innovations Toward the IoT, 5G, and Smart Media Era, ICTC 2015",,,"7354677","830","832",,2,"10.1109/ICTC.2015.7354677","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964885013&doi=10.1109%2fICTC.2015.7354677&partnerID=40&md5=f972a9ba0c55cd44e2e412cb9906f603","Realistic Broadcasting Media Research Department, ETRI, Daejeon, South Korea; Image Engineering Lab., Hanyang University, Seoul, South Korea","Lee, H., Realistic Broadcasting Media Research Department, ETRI, Daejeon, South Korea; Seo, J., Realistic Broadcasting Media Research Department, ETRI, Daejeon, South Korea; Jo, H., Image Engineering Lab., Hanyang University, Seoul, South Korea","We propose a gaze tracking system that can be applicable to a large display over 60 inch with 2∼3m viewing distance. The proposed gaze tracking device is designed to obtain high resolution near-infrared (NIR) eye images even though a user is far away from the system. And the proposed gaze tracking algorithm extracts pupil center and glints accurately and shows high gaze estimation performance. © 2015 IEEE.","Depth sensor; Gaze; Structure sensor; Tracking; Zoom camera","Cameras; Infrared devices; Surface discharges; Depth sensors; Gaze; Gaze estimation; Gaze tracking system; High resolution; Near infra red; Viewing distance; Zoom camera; Tracking (position)",Conference Paper,"Final","",Scopus,2-s2.0-84964885013
"Domhof J., Chandarr A., Rudinac M., Jonker P.","57118061400;56829532200;17346764300;55141663100;","Multimodal joint visual attention model for natural human-robot interaction in domestic environments",2015,"IEEE International Conference on Intelligent Robots and Systems","2015-December",,"7353703","2406","2412",,6,"10.1109/IROS.2015.7353703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958154131&doi=10.1109%2fIROS.2015.7353703&partnerID=40&md5=7d0ab7a795240b08bce420f2017c46ba","Delft Robotics Institute, TU-Delft, Netherlands","Domhof, J., Delft Robotics Institute, TU-Delft, Netherlands; Chandarr, A., Delft Robotics Institute, TU-Delft, Netherlands; Rudinac, M., Delft Robotics Institute, TU-Delft, Netherlands; Jonker, P., Delft Robotics Institute, TU-Delft, Netherlands","In this paper, we introduce a non-verbal multimodal joint visual attention model for human-robot interaction in household scenarios. Our model combines the bottom-up saliency and depth-based segmentation with the top-down cues such as pointing and gaze to detect the objects of interest according to the user. For generation of the top-down saliency maps, we have introduced novel methods for object saliency, based on the pointing direction as well as the gaze direction. For gaze estimation, a hybrid model has been introduced which automatically selects keypoint-based matching or back-projection based on the textureness of the object model. The combination of different cues ensures reliable object detection and interaction independent of the relative position between the user, robot and objects. Extensive experiments show good detection results in different interaction scenarios as well as in challenging environmental conditions. © 2015 IEEE.","Cameras; Computational modeling; Estimation; Image color analysis; Robots; Three-dimensional displays; Visualization","Behavioral research; Cameras; Estimation; Flow visualization; Image segmentation; Intelligent robots; Man machine systems; Object detection; Robots; Three dimensional computer graphics; Visualization; Bottom-up saliencies; Computational model; Domestic environments; Environmental conditions; Household scenarios; Image color analysis; Three-dimensional display; Visual attention model; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-84958154131
"Koutras P., Maragos P.","56242401400;35243026700;","Estimation of eye gaze direction angles based on active appearance models",2015,"Proceedings - International Conference on Image Processing, ICIP","2015-December",,"7351237","2424","2428",,10,"10.1109/ICIP.2015.7351237","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956704640&doi=10.1109%2fICIP.2015.7351237&partnerID=40&md5=e4f31058ff09c7f9a75c6051114069c7","School of E.C.E., National Technical University of Athens, Athens, 15773, Greece","Koutras, P., School of E.C.E., National Technical University of Athens, Athens, 15773, Greece; Maragos, P., School of E.C.E., National Technical University of Athens, Athens, 15773, Greece","In this paper we demonstrate efficient methods for continuous estimation of eye gaze angles with application to sign language videos. The difficulty of the task lies on the fact that those videos contain images with low face resolution since they are recorded from distance. First, we proceed to the modeling of face and eyes region by training and fitting Global and Local Active Appearance Models (LAAM). Next, we propose a system for eye gaze estimation based on a machine learning approach. In the first stage of our method, we classify gaze into discrete classes using GMMs that are based either on the parameters of the LAAM, or on HOG descriptors for the eyes region. We also propose a method for computing gaze direction angles from GMM log-likelihoods. We qualitatively and quantitatively evaluate our methods on two sign language databases and compare with a state of the art geometric model of the eye based on LAAM landmarks, which provides an estimate in direction angles. Finally, we further evaluate our framework by getting ground truth data from an eye tracking system Our proposed methods, and especially the GMMs using LAAM parameters, demonstrate high accuracy and robustness even in challenging tasks. © 2015 IEEE.","active appearance models; Eye gaze estimation; eye-tracker estimation; face and eyes modeling; Gaussian mixture models; gaze direction angles; histograms of oriented gradients",,Conference Paper,"Final","",Scopus,2-s2.0-84956704640
"Mukherjee S.S., Baxter R.H., Robertson N.M.","57192180712;36447367100;7102941208;","Instantaneous real-time head pose at a distance",2015,"Proceedings - International Conference on Image Processing, ICIP","2015-December",,"7351449","3471","3475",,,"10.1109/ICIP.2015.7351449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956670875&doi=10.1109%2fICIP.2015.7351449&partnerID=40&md5=57d1675ea3fa2b5fc8a0a177b507c217","Visionlab, Heriot-Watt University, Edinburgh, United Kingdom","Mukherjee, S.S., Visionlab, Heriot-Watt University, Edinburgh, United Kingdom; Baxter, R.H., Visionlab, Heriot-Watt University, Edinburgh, United Kingdom; Robertson, N.M., Visionlab, Heriot-Watt University, Edinburgh, United Kingdom","In this paper we focus on robust, real-time human head pose estimation in low resolution RGB data without any smoothing motion priors e.g. direction of motion. Our main contributions lie in three major areas. First, we show that a generative Deep Belief Network model can be learned on human head data from multiple types of data sources. These sources have similar underlying data that are not necessarily labelled or have the same kind of ground truth. Second, we perform discriminative training using multiple disparate supervisory labels to fine tune the model for head pose estimation. Third, we present state-of-the-art results on two publicly available datasets using this new approach. Our implemetation computes head pose for a head image in 0.8 milliseconds, making it real-time and highly scalable. © 2015 IEEE.","Deep Belief Network; Deep Learning; Gaze; Head Pose; Surveillance; Unsupervised Learning",,Conference Paper,"Final","",Scopus,2-s2.0-84956670875
"Chen B.-C., Wu P.-C., Chien S.-Y.","57089360200;37076421300;7201952344;","Real-time eye localization, blink detection, and gaze estimation system without infrared illumination",2015,"Proceedings - International Conference on Image Processing, ICIP","2015-December",,"7350892","715","719",,13,"10.1109/ICIP.2015.7350892","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956653255&doi=10.1109%2fICIP.2015.7350892&partnerID=40&md5=cb7fc59ff15c4d53cc8fdd350ba67d4d","Media IC and System Lab, Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","Chen, B.-C., Media IC and System Lab, Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Wu, P.-C., Media IC and System Lab, Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Chien, S.-Y., Media IC and System Lab, Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","Gaze tracking systems have high potential to be used as natural user interface devices; however, the mainstream systems are designed with infrared illumination, which may be harmful for human eyes. In this paper, a real-time eye localization, blink detection, and gaze estimation system is proposed without infrared illumination. To deal with various lighting conditions and reflections on the iris, the proposed system is based on a continuously updated color model for robust iris detection. Moreover, the proposed algorithm employs both the simplified and the original eye images to achieve the balance between robustness and accuracy. Experimental results show that the proposed system can achieve the accuracy of 96.8% for blink detection and the accuracy of 1.973 degree for gaze estimation with the processing speed of 10-11fps. The performance is comparable to previous works with infrared illumination. © 2015 IEEE.","blink detection; eye localization; Eye tracking; gaze estimation",,Conference Paper,"Final","",Scopus,2-s2.0-84956653255
"Sugano Y., Matsushita Y., Sato Y., Koike H.","7005470045;35956654700;35230954300;7202758799;","Appearance-Based Gaze Estimation with Online Calibration from Mouse Operations",2015,"IEEE Transactions on Human-Machine Systems","45","6","7050250","750","760",,30,"10.1109/THMS.2015.2400434","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959332125&doi=10.1109%2fTHMS.2015.2400434&partnerID=40&md5=e53c4d6a08b53af59596dd65b7f840dc","Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrucken, 66123, Germany; Microsoft Research Asia, Beijing, 100080, China; Institute of Industrial Science, University of Tokyo, Tokyo, 113-8654, Japan; Graduate School of Information Science and Engineering, Tokyo Institute of Technology, Tokyo, 152-8550, Japan","Sugano, Y., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrucken, 66123, Germany; Matsushita, Y., Microsoft Research Asia, Beijing, 100080, China; Sato, Y., Institute of Industrial Science, University of Tokyo, Tokyo, 113-8654, Japan; Koike, H., Graduate School of Information Science and Engineering, Tokyo Institute of Technology, Tokyo, 152-8550, Japan","This paper presents an unconstrained gaze estimation method using an online learning algorithm. We focus on a desktop scenario, where a user operates a personal computer, and use the mouse-clicked positions to infer, where on the screen the user is looking at. Our method continuously captures the user's head pose and eye images with a monocular camera, and each mouse click triggers learning sample acquisition. In order to handle head pose variations, the samples are adaptively clustered according to the estimated head pose. Then, local reconstruction-based gaze estimation models are incrementally updated in each cluster. We conducted a prototype evaluation in real-world environments, and our method achieved an estimation accuracy of 2.9°. © 2013 IEEE.","Computer vision; eye movement; human-computer interface; tracking","Algorithms; Personal computers; Social networking (online); Appearance based; Gaze estimation; Learning samples; Monocular cameras; Mouse operations; On-line calibration; Online learning algorithms; Real world environments; Mammals",Article,"Final","",Scopus,2-s2.0-84959332125
"Sakurai K., Yan M., Inami K., Tamura H., Tanno K.","56909413100;55515780300;56909633500;35600682500;7103288835;","A study on human interface system using the direction of eyes and face",2015,"Artificial Life and Robotics","20","4",,"291","298",,3,"10.1007/s10015-015-0228-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949313346&doi=10.1007%2fs10015-015-0228-7&partnerID=40&md5=998fb36000b516806670200105b7c401","Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, 1-1 Gakuen Kibanadai-nishi, Miyazaki-shi, Miyazaki  889-2192, Japan; Graduate Course in Electrical and Electronic Engineering, University of Miyazaki, Miyazaki, Japan; Department of Environmental Robotics, University of Miyazaki, Miyazaki, Japan; Department of Electrical and Systems Engineering, University of Miyazaki, Miyazaki, Japan","Sakurai, K., Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, 1-1 Gakuen Kibanadai-nishi, Miyazaki-shi, Miyazaki  889-2192, Japan; Yan, M., Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, 1-1 Gakuen Kibanadai-nishi, Miyazaki-shi, Miyazaki  889-2192, Japan; Inami, K., Graduate Course in Electrical and Electronic Engineering, University of Miyazaki, Miyazaki, Japan; Tamura, H., Department of Environmental Robotics, University of Miyazaki, Miyazaki, Japan; Tanno, K., Department of Electrical and Systems Engineering, University of Miyazaki, Miyazaki, Japan","Establishing an efficient alternative channel for communication without overt speech and hand movements is important for increasing the quality of life for patients lacking correct limb and facial muscular responses. This paper presents the eye movement tracking system using cross-channel electrooculogram signals. In addition, we used Kinect sensor (RGB-D sensor) for face tracking. Thus, gaze estimation system can be established by both eye movement and face tracking. Simulation experiments were designed in order to confirm the effectiveness of the proposed system. As a result of simulation experiments, gaze position estimation is recognized under high accuracy in our system. © 2015, ISAROB.","Electrooculogram signal; Gaze estimation; RGB-D sensor","Gesture recognition; Speech communication; Electro-oculogram; Eye-movement tracking; Gaze estimation; Human Interface; Kinect sensors; Position estimation; Quality of life; Rgb-d sensors; Eye movements",Article,"Final","",Scopus,2-s2.0-84949313346
"Cai H.-B., Yu H., Yao C.-Y., Chen S.-Y., Liu H.-H.","56763253600;56115992300;7201495904;57192606393;54958434200;","Convolution-based means of gradient for fast eye center localization",2015,"Proceedings - International Conference on Machine Learning and Cybernetics","2",,"7340650","759","764",,7,"10.1109/ICMLC.2015.7340650","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020694452&doi=10.1109%2fICMLC.2015.7340650&partnerID=40&md5=16004551d4b301e50ca6179529468969","School of Computing, University of Portsmouth, United Kingdom; State Key of Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, China; College of Computer Science, Zhejiang University of Technology, China; College of Mechanical Engineering, Zhejiang University of Technology, China","Cai, H.-B., School of Computing, University of Portsmouth, United Kingdom, College of Computer Science, Zhejiang University of Technology, China; Yu, H., School of Computing, University of Portsmouth, United Kingdom; Yao, C.-Y., College of Mechanical Engineering, Zhejiang University of Technology, China; Chen, S.-Y., College of Computer Science, Zhejiang University of Technology, China; Liu, H.-H., School of Computing, University of Portsmouth, United Kingdom, State Key of Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, China","Localizing eye center is primary challenge for application-s involving gaze estimation, face recognition and human machine interaction. The challenge is caused by significant variability of eye appearance in illumination, shape, color, viewing angle and dynamics, and computation related issues. In this paper, we propose a convolution-based means of gradient method to efficiently and accurately locate the eye center in low resolution images. Priority of enhancing its computation is achieved by the use of FFT transform and fewer identified pixels of circular boundary of potential eye centres. The proposed algorithm is validated in the research database platform of BioID face database. The experimental results confirm that the proposed outperforms the-state-of-art methods and its potential in real-time eye gaze tracking related applications. © 2015 IEEE.","Convolution; Eye center location; Gradients; Iris center location","Artificial intelligence; Convolution; Cybernetics; Gradient methods; Learning systems; Tracking (position); Center locations; Circular boundaries; Eye center locations; Gradients; Human machine interaction; Low resolution images; Research database; State-of-art methods; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85020694452
"Niu C., Sun J., Li J., Yan H.","57162263600;12645161300;55441752700;56471078100;","A calibration simplified method for gaze interaction based on using experience",2015,"2015 IEEE 17th International Workshop on Multimedia Signal Processing, MMSP 2015",,,"7340846","","",,4,"10.1109/MMSP.2015.7340846","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960352586&doi=10.1109%2fMMSP.2015.7340846&partnerID=40&md5=d0d9a4a5c412155044725904b4a26bd4","School of Information Science, Engineering Shandong University, Jinan, 250100, China; School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, 250100, China; School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, 250014, China","Niu, C., School of Information Science, Engineering Shandong University, Jinan, 250100, China; Sun, J., School of Information Science, Engineering Shandong University, Jinan, 250100, China; Li, J., School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, 250100, China; Yan, H., School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, 250014, China","The step of calibration prevents the gaze interaction from interacting naturally, which usually needs five or more calibration points to obtain the user's calibration information. In this paper, a calibration simplified method is proposed, which is based on user experience and consists of the user experience accumulation stage and the calibration simplified stage. In the user experience accumulation stage, the calibration is still needed as usual before each gaze interaction and the calibration parameters and the position of the user are stored as the calibration experience, where the calibration parameters include the actual coordinates of the calibration points and their estimation errors. In the calibration simplified stage, the user needs to fixate on only one calibration point for calibration before the calibration information can be estimated according to the user experience stored in the first stage. Even the calibration information can be estimated according to only the position of the user and the stored calibration parameters, which means the user can interact with computer via gaze directly without calibration. The simulations show that the gaze estimation accuracy of the proposed calibration simplified method can be 1.4047° with one calibration point and 1.7489° without calibration point, which are much higher than that without the user experience. © 2015 IEEE.","Calibration; Eye Tracking; Gaze Tracking; User Experience","Multimedia signal processing; Parameter estimation; Signal processing; Tracking (position); Calibration information; Calibration parameters; Calibration points; Estimation errors; Eye-tracking; Gaze tracking; Simplified method; User experience; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84960352586
"Sheikhi S., Odobez J.-M.","35093209600;57203103085;","Combining dynamic head pose-gaze mapping with the robot conversational state for attention recognition in human-robot interactions",2015,"Pattern Recognition Letters","66",,,"81","90",,33,"10.1016/j.patrec.2014.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943167644&doi=10.1016%2fj.patrec.2014.10.002&partnerID=40&md5=7e07bdb30417fa5d88d2e8fa60b965db","Idiap Research Institue, Rue Marconi 19, Martigny, 1920, Switzerland; École Polytechnique Fédérale de Lausanne, Route Cantonale, Lausanne, 1015, Switzerland","Sheikhi, S., Idiap Research Institue, Rue Marconi 19, Martigny, 1920, Switzerland, École Polytechnique Fédérale de Lausanne, Route Cantonale, Lausanne, 1015, Switzerland; Odobez, J.-M., Idiap Research Institue, Rue Marconi 19, Martigny, 1920, Switzerland, École Polytechnique Fédérale de Lausanne, Route Cantonale, Lausanne, 1015, Switzerland","The ability to recognize the visual focus of attention (VFOA, i.e. what or whom a person is looking at) of people is important for robots or conversational agents interacting with multiple people, since it plays a key role in turn-taking, engagement or intention monitoring. As eye gaze estimation is often impossible to achieve, most systems currently rely on head pose as an approximation, creating ambiguities since the same head pose can be used to look at different VFOA targets. To address this challenge, we propose a dynamic Bayesian model for the VFOA recognition from head pose, where we make two main contributions. First, taking inspiration from behavioral models describing the relationships between the body, head and gaze orientations involved in gaze shifts, we propose novel gaze models that dynamically and more accurately predict the expected head orientation used for looking in a given gaze target direction. This is a neglected aspect of previous works but essential for recognition. Secondly, we propose to exploit the robot conversational state (when he speaks, objects to which he refers) as context to set appropriate priors on candidate VFOA targets and reduce the inherent VFOA ambiguities. Experiments on a public dataset where the humanoid robot NAO plays the role of an art guide and quiz master demonstrate the benefit of the two contributions. © 2014 Elsevier B.V. Allrights reserved.","Attention recognition; Context; Gaze; Head pose; HRI; Non-verbal behavior","Anthropomorphic robots; Bayesian networks; Gesture recognition; Human computer interaction; Robots; Attention recognition; Context; Gaze; Head pose; HRI; Nonverbal behavior; Human robot interaction",Article,"Final","",Scopus,2-s2.0-84943167644
"Sugano Y., Bulling A.","7005470045;6505807414;","Self-calibrating head-mounted eye trackers using egocentric visual saliency",2015,"UIST 2015 - Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology",,,,"363","372",,52,"10.1145/2807442.2807445","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959297812&doi=10.1145%2f2807442.2807445&partnerID=40&md5=fcf2f38d91306e3a2e2c2628b1b536f1","Perceptual User Interfaces Group, Max Planck Institute for Informatics, Germany","Sugano, Y., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Germany; Bulling, A., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Germany","Head-mounted eye tracking has significant potential for gaze-based applications such as life logging, mental health monitoring, or the quantified self. A neglected challenge for the long-term recordings required by these applications is that drift in the initial person-specific eye tracker calibration, for example caused by physical activity, can severely impact gaze estimation accuracy and thus system performance and user experience. We first analyse calibration drift on a new dataset of natural gaze data recorded using synchronised video-based and Electrooculography-based eye trackers of 20 users performing everyday activities in a mobile setting. Based on this analysis we present a method to automatically self-calibrate head-mounted eye trackers based on a computational model of bottom-up visual saliency. Through evaluations on the dataset we show that our method 1) is effective in reducing calibration drift in calibrated eye trackers and 2) given sufficient data, can achieve gaze estimation accuracy competitive with that of a calibrated eye tracker, without any manual calibration.","Calibration Drift; Electrooculography; Mobile Eye Tracking; User Calibration; Visual Saliency","Electrooculography; Eye movements; User interfaces; Visualization; Calibration drift; Computational model; Head-mounted eye tracking; Long-term recording; Manual calibration; Mobile eye-tracking; User calibration; Visual saliency; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84959297812
"Lander C., Gehring S., Krüger A., Boring S., Bulling A.","55785449200;36095852400;35264048900;18233691700;6505807414;","Gazeprojector: Accurate gaze estimation and seamless gaze interaction across multiple displays",2015,"UIST 2015 - Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology",,,,"395","404",,25,"10.1145/2807442.2807479","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959287354&doi=10.1145%2f2807442.2807479&partnerID=40&md5=cfe34c89f9f6ad57339fb0ee3fa3159c","DFKI GmbH, Saarbrücken, Germany; Departement of Computer Science, University of Copenhagen, Denmark; Max Planck Institute for Informatics, Saarbrücken, Germany","Lander, C., DFKI GmbH, Saarbrücken, Germany; Gehring, S., DFKI GmbH, Saarbrücken, Germany; Krüger, A., DFKI GmbH, Saarbrücken, Germany; Boring, S., Departement of Computer Science, University of Copenhagen, Denmark; Bulling, A., Max Planck Institute for Informatics, Saarbrücken, Germany","Mobile gaze-based interaction with multiple displays may occur from arbitrary positions and orientations. However, maintaining high gaze estimation accuracy in such situations remains a significant challenge. In this paper, we present GazeProjector, a system that combines (1) natural feature tracking on displays to determine the mobile eye tracker?s position relative to a display with (2) accurate point-of-gaze estimation. GazeProjector allows for seamless gaze estimation and interaction on multiple displays of arbitrary sizes independently of the user?s position and orientation to the display. In a user study with 12 participants we compare GazeProjector to established methods (here: visual on-screen markers and a state-of-the-art video-based motion capture system). We show that our approach is robust to varying head poses, orientations, and distances to the display, while still providing high gaze estimation accuracy across multiple displays without re-calibration for each variation. Our system represents an important step towards the vision of pervasive gaze-based interfaces.","Calibration; Eye tracking; Gaze estimation; Large displays; Multi-display environments; Natural feature tracking","Calibration; Human computer interaction; Stereo vision; Eye-tracking; Gaze estimation; Large displays; Multi display environments; Natural feature tracking; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-84959287354
"Mukherjee S.S., Robertson N.M.","57192180712;7102941208;","Deep Head Pose: Gaze-Direction Estimation in Multimodal Video",2015,"IEEE Transactions on Multimedia","17","11","7279167","2094","2107",,84,"10.1109/TMM.2015.2482819","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946574616&doi=10.1109%2fTMM.2015.2482819&partnerID=40&md5=9762bdbd16552f50e0fdc6abcc53aa0e","Visionlab at the Edinburgh Research Partnership in Engineering and Mathematics, Heriot-Watt University, Edinburgh, EH14 4AS, United Kingdom; University of Edinburgh, Edinburgh, EH8 9YL, United Kingdom","Mukherjee, S.S., Visionlab at the Edinburgh Research Partnership in Engineering and Mathematics, Heriot-Watt University, Edinburgh, EH14 4AS, United Kingdom, University of Edinburgh, Edinburgh, EH8 9YL, United Kingdom; Robertson, N.M., Visionlab at the Edinburgh Research Partnership in Engineering and Mathematics, Heriot-Watt University, Edinburgh, EH14 4AS, United Kingdom, University of Edinburgh, Edinburgh, EH8 9YL, United Kingdom","In this paper we present a convolutional neural network (CNN)-based model for human head pose estimation in low-resolution multi-modal RGB-D data. We pose the problem as one of classification of human gazing direction. We further fine-tune a regressor based on the learned deep classifier. Next we combine the two models (classification and regression) to estimate approximate regression confidence. We present state-of-the-art results in datasets that span the range of high-resolution human robot interaction (close up faces plus depth information) data to challenging low resolution outdoor surveillance data. We build upon our robust head-pose estimation and further introduce a new visual attention model to recover interaction with the environment. Using this probabilistic model, we show that many higher level scene understanding like human-human/scene interaction detection can be achieved. Our solution runs in real-time on commercial hardware. © 2015 IEEE.","Convolutional neural networks (CNNs); deep learning; gaze direction; head-pose; RGB-D","Behavioral research; Convolution; Image recognition; Modal analysis; Motion estimation; Neural networks; Robots; Convolutional neural network; Deep learning; Gaze direction; Head pose; Head Pose Estimation; Interaction detection; Probabilistic modeling; Visual attention model; Human robot interaction",Article,"Final","",Scopus,2-s2.0-84946574616
"Lu F., Sugano Y., Okabe T., Sato Y.","54956194300;7005470045;7201390055;35230954300;","Gaze Estimation From Eye Appearance: A Head Pose-Free Method via Eye Image Synthesis",2015,"IEEE Transactions on Image Processing","24","11","7122896","3680","3693",,39,"10.1109/TIP.2015.2445295","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938356895&doi=10.1109%2fTIP.2015.2445295&partnerID=40&md5=39cab13998ba6663c3bdd1bd531c7df3","School of Computer Science and Engineering, International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China; Institute of Industrial Science, University of Tokyo, Tokyo, 153-8505, Japan; Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, 66123, Germany; Kyushu Institute of Technology, Fukuoka, 820-8502, Japan","Lu, F., School of Computer Science and Engineering, International Research Institute for Multidisciplinary Science, Beihang University, Beijing, 100191, China, Institute of Industrial Science, University of Tokyo, Tokyo, 153-8505, Japan; Sugano, Y., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, 66123, Germany; Okabe, T., Kyushu Institute of Technology, Fukuoka, 820-8502, Japan; Sato, Y., Institute of Industrial Science, University of Tokyo, Tokyo, 153-8505, Japan","In this paper, we address the problem of free head motion in appearance-based gaze estimation. This problem remains challenging because head motion changes eye appearance significantly, and thus, training images captured for an original head pose cannot handle test images captured for other head poses. To overcome this difficulty, we propose a novel gaze estimation method that handles free head motion via eye image synthesis based on a single camera. Compared with conventional fixed head pose methods with original training images, our method only captures four additional eye images under four reference head poses, and then, precisely synthesizes new training images for other unseen head poses in estimation. To this end, we propose a single-directional (SD) flow model to efficiently handle eye image variations due to head motion. We show how to estimate SD flows for reference head poses first, and then use them to produce new SD flows for training image synthesis. Finally, with synthetic training images, joint optimization is applied that simultaneously solves an eye image alignment and a gaze estimation. Evaluation of the method was conducted through experiments to assess its performance and demonstrate its effectiveness. © 2015 IEEE.","Eye; Face and gesture recognition; Gaze estimation; Head pose-free; Image synthesis","Gesture recognition; Motion estimation; Eye; Face and gesture recognition; Gaze estimation; Head pose; Image synthesis; Image processing; algorithm; anatomy and histology; automated pattern recognition; eye; eye fixation; face; human; image processing; physiology; procedures; videorecording; Algorithms; Eye; Face; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Pattern Recognition, Automated; Video Recording",Article,"Final","",Scopus,2-s2.0-84938356895
"Sun L., Liu Z., Sun M.-T.","55535024300;7406672531;7403181446;","Real time gaze estimation with a consumer depth camera",2015,"Information Sciences","320",,,"346","360",,38,"10.1016/j.ins.2015.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937519329&doi=10.1016%2fj.ins.2015.02.004&partnerID=40&md5=a8f0c5e7a91497b25a9d398afd0dac3c","College of Computer Science, Zhejiang University, Hangzhou, China; Microsoft Research, Redmond, WA, United States; Department of Electrical Engineering, University of Washington, Seattle, WA, United States","Sun, L., College of Computer Science, Zhejiang University, Hangzhou, China; Liu, Z., Microsoft Research, Redmond, WA, United States; Sun, M.-T., Department of Electrical Engineering, University of Washington, Seattle, WA, United States","Existing eye-gaze-tracking systems typically require multiple infrared (IR) lights and high-quality cameras to achieve good performance and robustness against head movement. This requirement limits the systems' potential for broader applications. In this paper, we present a low-cost, non-intrusive, simple-setup gaze estimation system that can estimate the gaze direction under free head movement. In particular, the proposed system only uses a consumer depth camera (Kinect sensor) positioned at a distance from the subject. We develop a simple procedure to calibrate the geometric relationship between the screen and the camera, and subject-specific parameters. A parameterized iris model is then used to locate the center of the iris for gaze feature extraction, which can handle low-quality eye images. Finally, the gaze direction is determined based on a 3D geometric eye model, where the head movement and deviation of the visual axis from the optical axis are taken into consideration. Experimental results indicate that the system can estimate gaze with an accuracy of 1.4-2.7° and is robust against large head movements. Two real-time human-computer interaction (HCI) applications are presented to demonstrate the potential of the proposed system for wide applications. © 2015 Elsevier Inc. All rights reserved.","Eye tracking; Gaze direction; Gaze estimation; Human-computer interaction; System calibration","Cameras; Eye movements; Human computer interaction; Eye gaze tracking; Gaze direction; Gaze estimation; Geometric relationships; Human computer interaction (HCI); Kinect sensors; Subject-specific; System calibration; Eye tracking",Article,"Final","",Scopus,2-s2.0-84937519329
"Pavelkova A., Herout A., Behun K.","57008086100;6506161493;55248328100;","Usability of Pilot's Gaze in Aeronautic Cockpit for Safer Aircraft",2015,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC","2015-October",,"7313344","1545","1550",,3,"10.1109/ITSC.2015.252","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950283056&doi=10.1109%2fITSC.2015.252&partnerID=40&md5=3ddd3615f7422a14f5c0bd7bdf50f4af","Graph at FIT Brno University of Technology, CZ, Czech Republic","Pavelkova, A., Graph at FIT Brno University of Technology, CZ, Czech Republic; Herout, A., Graph at FIT Brno University of Technology, CZ, Czech Republic; Behun, K., Graph at FIT Brno University of Technology, CZ, Czech Republic","With increasing complexity of aircraft equipment, the demands on the aircraft crew are unprecedented. The modern cockpit must adjust its pace of communication for optimal workload of the crew and ensure that the information presented to the pilot is actually received. We contribute to the solution of these problems by designing a passive (purely video-based and unobtrusive) gaze tracking solution. It allows for detection of the pilot missing a piece of information communicated by the cockpit equipment, and for estimation of pilot's workload and immediate focus. We collected a dataset at an aeronautic simulator by using the OptiTrack tracking equipment for development and evaluation of similar gaze tracking systems and we make it publicly available. The experiments show that proposed algorithms for passive and unobtrusive gaze estimation work satisfactorily for the targeted purposes. © 2015 IEEE.",,"Aircraft parts and equipment; Equipment; Intelligent systems; Intelligent vehicle highway systems; Tracking (position); Transportation; Gaze estimation; Gaze tracking; Gaze tracking system; Cockpits (aircraft)",Conference Paper,"Final","",Scopus,2-s2.0-84950283056
"Bartoli F., Lisanti G., Seidenari L., Karaman S., Del Bimbo A.","56462292700;35519750700;36015900900;36622063700;15018931800;","MuseumVisitors: A dataset for pedestrian and group detection, gaze estimation and behavior understanding",2015,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2015-October",,"7301279","19","27",,8,"10.1109/CVPRW.2015.7301279","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952008565&doi=10.1109%2fCVPRW.2015.7301279&partnerID=40&md5=dfa1dee45a06361af22e1fd738040a03","University of Florence, Firenze, 50121, Italy; Columbia University, Firenze, 50121, United States","Bartoli, F., University of Florence, Firenze, 50121, Italy; Lisanti, G., University of Florence, Firenze, 50121, Italy; Seidenari, L., University of Florence, Firenze, 50121, Italy; Karaman, S., University of Florence, Firenze, 50121, Italy, Columbia University, Firenze, 50121, United States; Del Bimbo, A., University of Florence, Firenze, 50121, Italy","In this paper we describe a new dataset, under construction, acquired inside the National Museum of Bargello in Florence. It was recorded with three IP cameras at a resolution of 1280 × 800 pixels and an average framerate of five frames per second. Sequences were recorded following two scenarios. The first scenario consists of visitors watching different artworks (individuals), while the second one consists of groups of visitors watching the same artworks (groups). This dataset is specifically designed to support research on group detection, occlusion handling, tracking, re-identification and behavior analysis. In order to ease the annotation process we designed a user friendly web interface that allows to annotate: bounding boxes, occlusion area, body orientation and head gaze, group belonging, and artwork under observation. We provide a comparison with other existing datasets that have group and occlusion annotations. In order to assess the difficulties of this dataset we have also performed some tests exploiting seven representative state-of-the-art pedestrian detectors. © 2015 IEEE.","Calibration; Cameras; Computer vision; Detectors; Feature extraction; Head; Videos","Calibration; Cameras; Computer vision; Detectors; Statistical tests; Behavior analysis; Behavior understanding; Frames per seconds; Head; Occlusion handling; Re identifications; State of the art; Videos; Feature extraction",Conference Paper,"Final","",Scopus,2-s2.0-84952008565
[无可用作者姓名],[无可用的作者 ID],"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",2015,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2015-October",,,"","",900,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951968952&partnerID=40&md5=e716cd0d637222435665f632c8c0ed45",,"","The proceedings contain 132 papers. The topics discussed include: multi-scale pyramid pooling for deep convolutional representation; from generic to specific deep representations for visual recognition; subset feature learning for fine-grained category classification; channel-max, channel-drop and stochastic max-pooling; deep learning of binary hash codes for fast image retrieval; self-tuned deep super resolution; object level deep feature pooling for compact image representation; exploiting local features from deep networks for image retrieval; convolutional recurrent neural networks: learning spatial dependencies for image representation; walking and talking: a bilinear approach to multi-label action recognition; MuseumVisitors: a dataset for pedestrian and group detection, gaze estimation and behavior understanding; subject centric group feature for person re-identification; the GRODE metrics: exploring the performance of group detection approaches; and discovering human interactions in videos with limited data labeling.",,,Conference Review,"Final","",Scopus,2-s2.0-84951968952
"Zhang X., Sugano Y., Fritz M., Bulling A.","57142162900;7005470045;14035495500;6505807414;","Appearance-based gaze estimation in the wild",2015,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","07-12-June-2015",,"7299081","4511","4520",,320,"10.1109/CVPR.2015.7299081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959187547&doi=10.1109%2fCVPR.2015.7299081&partnerID=40&md5=c0243ed9d8207c6cc377666f9f490c4d","Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Scalable Learning and Perception Group, Max Planck Institute for Informatics, Saarbrücken, Germany","Zhang, X., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Sugano, Y., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Fritz, M., Scalable Learning and Perception Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Bulling, A., Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany","Appearance-based gaze estimation is believed to work well in real-world settings, but existing datasets have been collected under controlled laboratory conditions and methods have been not evaluated across multiple datasets. In this work we study appearance-based gaze estimation in the wild. We present the MPIIGaze dataset that contains 213,659 images we collected from 15 participants during natural everyday laptop use over more than three months. Our dataset is significantly more variable than existing ones with respect to appearance and illumination. We also present a method for in-the-wild appearance-based gaze estimation using multimodal convolutional neural networks that significantly outperforms state-of-the art methods in the most challenging cross-dataset evaluation. We present an extensive evaluation of several state-of-the-art image-based gaze estimation algorithms on three current datasets, including our own. This evaluation provides clear insights and allows us to identify key research challenges of gaze estimation in the wild. © 2015 IEEE.",,"Computer vision; Neural networks; Controlled laboratories; Convolutional neural network; Cross-dataset evaluation; Multiple data sets; Real world setting; Research challenges; State of the art; State-of-the-art methods; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-84959187547
"Stengel M., Grogorick S., Eisemann M., Eisemann E., Magnor M.","42162165500;57063513000;7007089818;35304781200;57191188427;","An affordable solution for binocular eye tracking and calibration in head-mounted displays",2015,"MM 2015 - Proceedings of the 2015 ACM Multimedia Conference",,,,"15","24",,31,"10.1145/2733373.2806265","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962798473&doi=10.1145%2f2733373.2806265&partnerID=40&md5=0d33c85ac33c5344bafe465d21a20af7","TU Braunschweig, Germany; TH Köln, Germany; TU Delft, Netherlands","Stengel, M., TU Braunschweig, Germany; Grogorick, S., TU Braunschweig, Germany; Eisemann, M., TH Köln, Germany; Eisemann, E., TU Delft, Netherlands; Magnor, M., TU Braunschweig, Germany","Immersion is the ultimate goal of head-mounted displays (HMD) for Virtual Reality (VR) in order to produce a convincing user experience. Two important aspects in this context are motion sickness, often due to imprecise calibration, and the integration of a reliable eye tracking. We propose an affordable hard-and software solution for drift-free eye-Tracking and user-friendly lens calibration within an HMD. The use of dichroic mirrors leads to a lean design that provides the full field-of-view (FOV) while using commodity cameras for eye tracking. Our prototype supports personalizable lens positioning to accommodate for different interocular distances. On the software side, a model-based calibration procedure adjusts the eye tracking system and gaze estimation to varying lens positions. Challenges such as partial occlusions due to the lens holders and eye lids are handled by a novel robust monocular pupil-Tracking approach. We present four applications of our work: Gaze map estimation, foveated rendering for depth of field, gaze-contingent level-of-detail, and gaze control of virtual avatars. © 2015 ACM.","Eye Tracking; Gaze; Head-mounted Display; Mobile; Virtual Reality; Wearable","Calibration; Sensory perception; Street traffic control; Virtual reality; Eye-tracking; Gaze; Head mounted displays; Mobile; Wearable; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-84962798473
"Darshana S., Fernando D., Jayawardena S., Wickramanayake S., De Silva C.","57156593800;57193257323;57197858870;56112601600;23033613700;","Efficient PERCLOS and gaze measurement methodologies to estimate driver attention in real time",2015,"Proceedings - International Conference on Intelligent Systems, Modelling and Simulation, ISMS","2015-September",,"7280923","289","294",,13,"10.1109/ISMS.2014.56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959896866&doi=10.1109%2fISMS.2014.56&partnerID=40&md5=7ce1ed64845a2cea79b42f7151bba244","Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka","Darshana, S., Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Fernando, D., Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Jayawardena, S., Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka; Wickramanayake, S., Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka; De Silva, C., Department of Computer Science and Engineering, University of Moratuwa, Moratuwa, Sri Lanka","Drivers not being cautious enough is one of the major reasons for many of today's fatal road accidents. Drivers being fatigued or distracted have been identified as the main two reasons behind drivers losing their attention. PERCLOS and gaze estimation are two visual cue based parameters which can be used to estimate driver drowsiness and distraction respectively. This paper describes advanced and efficient methodologies for obtaining these two parameters. We use an infrared sensitive camera equipped with infrared LEDs in obtaining visual features of the driver. In PERCLOS estimation, for each frame, edge detected eye images are classified using a linear support vector machine. Exponentially smoothed vertical and horizontal movements of the pupils are taken into consideration in gaze estimation. The proposed methodology for eye state detection achieves real time recognition accuracy 83.64% whereas gaze estimation methodology achieves 80.5% accuracy. © 2014 IEEE.","Binary PCA; Distraction; Drowsiness; Fatigue; Gaze; PERCLOS; SVM","Fatigue of materials; Intelligent systems; Binary PCA; Distraction; Drowsiness; Gaze; PERCLOS; Support vector machines",Conference Paper,"Final","",Scopus,2-s2.0-84959896866
"Stengel M., Grogorick S., Eisemann M., Eisemann E., Magnor M.","42162165500;57063513000;7007089818;35304781200;57191188427;","Non-obscuring binocular eye tracking for wide field-of-view head-mounted-displays",2015,"2015 IEEE Virtual Reality Conference, VR 2015 - Proceedings",,,"7223443","357","358",,,"10.1109/VR.2015.7223443","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954488616&doi=10.1109%2fVR.2015.7223443&partnerID=40&md5=496d9d50dd0e85c150a290da515009aa","TU Braunschweig, Germany; TU Delft, Netherlands","Stengel, M., TU Braunschweig, Germany; Grogorick, S., TU Braunschweig, Germany; Eisemann, M., TU Braunschweig, Germany; Eisemann, E., TU Delft, Netherlands; Magnor, M., TU Braunschweig, Germany","We present a complete hardware and software solution for integrating binocular eye tracking into current state-of-the-art lens-based Head-mounted Displays (HMDs) without affecting the user's wide field-of-view off the display. The system uses robust and efficient new algorithms for calibration and pupil tracking and allows realtime eye tracking and gaze estimation. Estimating the relative gaze direction of the user opens the door to a much wider spectrum of virtual reality applications and games when using HMDs. We show a 3d-printed prototype of a low-cost HMD with eye tracking that is simple to fabricate and discuss a variety of VR applications utilizing gaze estimation. © 2015 IEEE.","Computer Graphics [I.3.7]: Three-Dimensional Graphics and Realism - Virtual reality Computer Graphics [C.3]; Special-Purpose and Application-based Systems - Real-time and embedded systems","3D printers; Binoculars; Bins; Computer graphics; Embedded systems; Helmet mounted displays; Real time systems; Virtual reality; Hardware and software; Head mounted displays; Real-time and embedded systems; Real-time eye tracking; State of the art; VR applications; Wide field of view; [i.3.7]: Three-dimensional graphics and realism; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84954488616
"Ye N., Tao X., Dong L., Li Y., Ge N.","57202057254;57208894708;56419177800;55979172900;7005864121;","Indicating eye contacts in one-to-many video teleconference with one web camera",2015,"Proceedings - APMediaCast: 2015 Asia Pacific Conference on Multimedia and Broadcasting",,,"7210284","116","120",,1,"10.1109/APMediaCast.2015.7210284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960516927&doi=10.1109%2fAPMediaCast.2015.7210284&partnerID=40&md5=06ad6360d9ffb71d2557fd8570951019","Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, 100084, China","Ye, N., Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, 100084, China; Tao, X., Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, 100084, China; Dong, L., Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, 100084, China; Li, Y., Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, 100084, China; Ge, N., Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, 100084, China","In video teleconference systems, one drawback that affecting quality of experience (QoE) is the lack of eye contacts. Current methods on how to improve eye contacts in one-to-many video teleconference systems are far from commercial deployment since related hardware are very sophisticated and expensive. In this paper, we propose a novel appearance-based scheme to estimate gaze direction with accessible equipments. Differing from current geometrical fitting methods, machine learning algorithms are used in our scheme, making it more robust against the size and resolution of the eye segments appearing in images. To be specific, an adaptive filter is designed to smooth the fluctuated training data; meanwhile, support vector machine (SVM) is applied to get robust results under various conditions. The experiment results show that, under different illuminations and distances, our method can achieve better performance on gaze direction estimation than current methods in terms of accuracy. © 2015 IEEE.","Adaptive filter; Gaze estimation; One-to-many video teleconference; QoE; Support vector machine","Adaptive filtering; Artificial intelligence; Learning algorithms; Learning systems; Quality of service; Support vector machines; Teleconferencing; Appearance based; Commercial deployment; Eye contact; Fitting method; Gaze direction; Gaze estimation; Quality of experience (QoE); Training data; Adaptive filters",Conference Paper,"Final","",Scopus,2-s2.0-84960516927
"Rattarom S., Aunsri N., Uttama S.","25031802300;55764124300;14631163100;","Interpolation based polynomial regression for eye gazing estimation: A comparative study",2015,"ECTI-CON 2015 - 2015 12th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology",,,"7207124","","",,2,"10.1109/ECTICon.2015.7207124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957037265&doi=10.1109%2fECTICon.2015.7207124&partnerID=40&md5=1583e4e7f28400c07d0d1e5e6fecdba2","School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand","Rattarom, S., School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand; Aunsri, N., School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand; Uttama, S., School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand","This paper presents a comparative study on the use of interpolation based gaze estimations with low-cost web camera with built in infrared LEDs. Six polynomial models were carefully compared based on at 9 points calibration; the best average accuracy from this study among all participants is found to be 1.10 degree from the center of gaze line. The best model provides better quality when compared to results from number of groups of research with web camera and standard open source gaze tracking software. In particular, the accuracy was acceptable under the reasonable price and very easy to establish. The results also emphasize the performance of the polynomial models that may vary with the environment and must be chosen carefully. © 2015 IEEE.","gaze tracking; interpolation based gaze estimation; mapping function","Calibration; Cameras; Interpolation; Open source software; Open systems; Tracking (position); World Wide Web; Comparative studies; Gaze estimation; Gaze tracking; Infrared leds; Mapping functions; Open sources; Polynomial models; Polynomial regression; Polynomials",Conference Paper,"Final","",Scopus,2-s2.0-84957037265
"Vicente F., Huang Z., Xiong X., De La Torre F., Zhang W., Levi D.","56537992500;57221145400;36132371900;7005596125;8081240000;15129012300;","Driver Gaze Tracking and Eyes off the Road Detection System",2015,"IEEE Transactions on Intelligent Transportation Systems","16","4","7053946","2014","2027",,175,"10.1109/TITS.2015.2396031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027940377&doi=10.1109%2fTITS.2015.2396031&partnerID=40&md5=c7484bbebd4b8ab4ab6aa69e20d74204","Carnegie Mellon University, Pittsburgh, PA  15213-3815, United States; General Motors Company, Warren, MI  48090, United States; General Motors Company, Herzliya, 46733, Israel","Vicente, F., Carnegie Mellon University, Pittsburgh, PA  15213-3815, United States; Huang, Z., Carnegie Mellon University, Pittsburgh, PA  15213-3815, United States; Xiong, X., Carnegie Mellon University, Pittsburgh, PA  15213-3815, United States; De La Torre, F., Carnegie Mellon University, Pittsburgh, PA  15213-3815, United States; Zhang, W., General Motors Company, Warren, MI  48090, United States; Levi, D., General Motors Company, Herzliya, 46733, Israel","Distracted driving is one of the main causes of vehicle collisions in the United States. Passively monitoring a driver's activities constitutes the basis of an automobile safety system that can potentially reduce the number of accidents by estimating the driver's focus of attention. This paper proposes an inexpensive vision-based system to accurately detect Eyes Off the Road (EOR). The system has three main components: 1) robust facial feature tracking; 2) head pose and gaze estimation; and 3) 3-D geometric reasoning to detect EOR. From the video stream of a camera installed on the steering wheel column, our system tracks facial features from the driver's face. Using the tracked landmarks and a 3-D face model, the system computes head pose and gaze direction. The head pose estimation algorithm is robust to nonrigid face deformations due to changes in expressions. Finally, using a 3-D geometric analysis, the system reliably detects EOR. © 2000-2011 IEEE.","Driver monitoring system; eyes off the road detection; gaze estimation; Head pose estimation","Accidents; Amphibious vehicles; Behavioral research; Face recognition; Image recognition; Motion estimation; Oil well flooding; Roads and streets; Tracking (position); Transportation; Video streaming; Driver monitoring system; Facial feature tracking; Gaze estimation; Geometric reasoning; Head Pose Estimation; Road detection; Vehicle collisions; Vision based system; Gesture recognition",Article,"Final","",Scopus,2-s2.0-85027940377
"Kumano S., Otsuka K., Ishii R., Yamato J.","23397242700;35303289200;35810445800;6604064362;","Automatic gaze analysis in multiparty conversations based on collective first-person vision",2015,"2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015","2015-January",,"7284861","","",,2,"10.1109/FG.2015.7284861","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049217070&doi=10.1109%2fFG.2015.7284861&partnerID=40&md5=3374518dfb14216cefdab01d73ac3020","NTT Communication Science Laboratories, Japan","Kumano, S., NTT Communication Science Laboratories, Japan; Otsuka, K., NTT Communication Science Laboratories, Japan; Ishii, R., NTT Communication Science Laboratories, Japan; Yamato, J., NTT Communication Science Laboratories, Japan","—This paper extends the affective computing research field by introducing first-person vision to automatic conversation analysis. We target medium-sized-party face-to-face conversations where each person wears inward-looking and outward-looking cameras. We demonstrate that the fundamental techniques required for group gaze analysis, i.e. speaker detection, face tracking, and gaze estimation, can be accurately and effectively performed via self-training in a unified framework by gathering captured audio-visual signals to a centralized system and using a general conversation rule, i.e. listeners look mainly at the speaker. We visualize the characteristics of participants’ gaze behavior as a gazee-centered heat map, which quantitatively reveals what parts of the gazee’s body and for how long the participant looked at it while the gazer speaks or listens. An experiment involving two groups of six-person conversations demonstrates the potential of the proposed framework. © 2015 IEEE.",,"Gesture recognition; Affective Computing; Centralized systems; Conversation analysis; Face-to-face conversation; First-person visions; Multi-party conversations; Speaker detection; Unified framework; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-85049217070
"Arar N.M., Gao H., Thiran J.-P.","55248082400;8274973800;35554798200;","Robust gaze estimation based on adaptive fusion of multiple cameras",2015,"2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015",,,"7163121","","",,4,"10.1109/FG.2015.7163121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944937448&doi=10.1109%2fFG.2015.7163121&partnerID=40&md5=98eba4164b3bdbf7234c06ca6c1bc3f9","Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Switzerland","Arar, N.M., Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Switzerland; Gao, H., Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Switzerland; Thiran, J.-P., Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Switzerland","Gaze movements play a crucial role in humancomputer interaction (HCI) applications. Recently, gaze tracking systems with a wide variety of applications have attracted much interest by the industry as well as the scientific community. The state-of-the-art gaze trackers are mostly non-intrusive and report high estimation accuracies. However, they require complex setups such as camera and geometric calibration in addition to subject-specific calibration. In this paper, we introduce a multi-camera gaze estimation system which requires less effort for the users in terms of the system setup and calibration. The system is based on an adaptive fusion of multiple independent camera systems in which the gaze estimation relies on simple cross-ratio (CR) geometry. Experimental results conducted on real data show that the proposed system achieves a significant accuracy improvement, by around 25%, over the traditional CR-based single camera systems through the novel adaptive multi-camera fusion scheme. The real-time system achieves <0.9° accuracy error with very few calibration data (5 points) under natural head movements, which is competitive with more complex systems. Hence, the proposed system enables fast and user-friendly gaze tracking with minimum user effort without sacrificing too much accuracy. © 2015 IEEE.",,"Calibration; Cameras; Face recognition; Interactive computer systems; Real time systems; Tracking (position); Accuracy Improvement; Gaze tracking system; Geometric calibrations; Human computer interaction (HCI); Multiple cameras; Scientific community; Single camera systems; Subject-specific; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84944937448
"McMurrough C.D., Lioulemes A., Phan S., Makedon F.","34870197500;56236841700;36458848300;7003437865;","3D mapping of visual attention for smart rehabilitation",2015,"8th ACM International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2015 - Proceedings",,,"a95","","",,3,"10.1145/2769493.2769579","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957039189&doi=10.1145%2f2769493.2769579&partnerID=40&md5=3b704871fd2cdf7a36f886a4a2b40f26","HERACLEIA Human-Centered Computing Laboratory, Department of Computer Science and Engineering, University of Texas, Arlington, United States; Institute of Informatics and Telecommunications, (NCSR) Demokritos, Athens, Greece","McMurrough, C.D., HERACLEIA Human-Centered Computing Laboratory, Department of Computer Science and Engineering, University of Texas, Arlington, United States; Lioulemes, A., HERACLEIA Human-Centered Computing Laboratory, Department of Computer Science and Engineering, University of Texas, Arlington, United States, Institute of Informatics and Telecommunications, (NCSR) Demokritos, Athens, Greece; Phan, S., HERACLEIA Human-Centered Computing Laboratory, Department of Computer Science and Engineering, University of Texas, Arlington, United States; Makedon, F., HERACLEIA Human-Centered Computing Laboratory, Department of Computer Science and Engineering, University of Texas, Arlington, United States","The estimation of human attention as input modality has been suggested as a method for an advanced human-computer interaction. With an increasing interest and development of augmented reality tools, the advent of Microsoft HoloLens glasses and increasingly affordable wearable eye-tracking devices, monitoring the human attention will soon become ubiquitous. Also visual heat-maps have become very popular and simpler to create in the 2D space over the last few years. They are very compelling and can be effective in summarizing and communicating data. The innovation in our work is the implementation of visual 3D heat-maps of the real world combined with advanced Computer Vision libraries. Finally, we have incorporated the visual 3D heatmaps for rehabilitation purposes that deal with the loss of concentration in children with learning disabilities, or disabled patients to select items of interest for them across a room.","Eye-tracking; Gaze estimation and attention; Human-computer interaction","Augmented reality; Behavioral research; Computer vision; Patient rehabilitation; Augmented reality tools; Computer vision library; Eye tracking devices; Eye-tracking; Gaze estimation; Input modalities; Learning disabilities; Visual Attention; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84957039189
"Lander C., Coenen N., Speicher M., Biewer S., Paradowski D., Kruger A.","55785449200;57188711771;56278707200;57188708328;55641285600;35264048900;","Collaborative newspaper demo: Exploring an adaptive scrolling algorithm in a multi-user reading scenario",2015,"PerDis 2015 - Proceedings: 4th ACM International Symposium on Pervasive Displays",,,,"271","272",,,"10.1145/2757710.2776817","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962476229&doi=10.1145%2f2757710.2776817&partnerID=40&md5=3c27cdff6d08f14cdafe5a970c3cc56c","German Research Center for Artificial Intelligence (DFKI), Germany; Saarland University, Germany","Lander, C., German Research Center for Artificial Intelligence (DFKI), Germany; Coenen, N., Saarland University, Germany; Speicher, M., German Research Center for Artificial Intelligence (DFKI), Germany; Biewer, S., Saarland University, Germany; Paradowski, D., German Research Center for Artificial Intelligence (DFKI), Germany; Kruger, A., German Research Center for Artificial Intelligence (DFKI), Germany","We present the Collaborative Newspaper, a prototype that manages scrolling of the same content on a public display for several users simultaneously. Usually it is not possible for passers-by to conveniently interact with public displays, as content is not interactive or responsive. Especially news screens are normally showing one news article after another, reducing the amount of information fitting the screen dimensions. Our prototype relies on an adaptive scrolling algorithm analyzing people's gaze to enable them to read the same text with individual scrolling speed. We are using head-mounted eye trackers for on-screen gaze estimation and a big-sized display presenting news articles to the users. © 2015 ACM.","Adaptive scrolling; Collaborative; Gaze-based interaction; Multi-user reading; Shared content",,Conference Paper,"Final","",Scopus,2-s2.0-84962476229
"Rigas I., Komogortsev O.","55403784800;6506328653;","Single-pixel eye tracking via patterned contact lenses: Design and evaluation in HCI domain",2015,"Conference on Human Factors in Computing Systems - Proceedings","18",,,"1241","1246",,1,"10.1145/2702613.2732745","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954210332&doi=10.1145%2f2702613.2732745&partnerID=40&md5=5f1d04cc3f3b647041bd3152a02c4943","Texas State University, 601 University Drive, San Marcos, TX  78666, United States","Rigas, I., Texas State University, 601 University Drive, San Marcos, TX  78666, United States; Komogortsev, O., Texas State University, 601 University Drive, San Marcos, TX  78666, United States","This paper presents a preliminary study of an eye tracking technique suitable for use in devices with low-power consumption demands, e.g. Google Glass. The method uses a patterned contact lens and a single-pixel imaging sensor. Its applicability is explored via a semi-simulated user study, where real eye movements from 50 subjects are used to animate a 3-D graphics replica of an eye wearing a patterned contact lens. An accurate single-pixel camera simulator is used to perform gaze estimation via capturing of the imprinted pattern. The results show the promising potential of the technique in the field of eye tracking and eye gesture recognition. Copyright is held by the author/owner(s).","Eye gesture interfaces; Eye tracking techniques; Patterned contact lenses; Single-pixel imaging","Contact lenses; Gesture recognition; Human computer interaction; Human engineering; Pixels; Tracking (position); Design and evaluations; Eye-gestures; Eye-tracking; Gaze estimation; Imaging sensors; Low-power consumption; Single pixel; Single-pixel cameras; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84954210332
"Chen J., Ji Q.","22633570700;18935108400;","A probabilistic approach to online eye gaze tracking without explicit personal calibration",2015,"IEEE Transactions on Image Processing","24","3","6990593","1076","1086",,35,"10.1109/TIP.2014.2383326","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923174509&doi=10.1109%2fTIP.2014.2383326&partnerID=40&md5=31756723ff3c448811f8f9527a849201","Computer Vision Laboratory, GE Global Research Center, Schenectady, NY  12309, United States; Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY  12180, United States","Chen, J., Computer Vision Laboratory, GE Global Research Center, Schenectady, NY  12309, United States; Ji, Q., Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY  12180, United States","Existing eye gaze tracking systems typically require an explicit personal calibration process in order to estimate certain person-specific eye parameters. For natural human computer interaction, such a personal calibration is often inconvenient and unnatural. In this paper, we propose a new probabilistic eye gaze tracking system without explicit personal calibration. Unlike the conventional eye gaze tracking methods, which estimate the eye parameter deterministically using known gaze points, our approach estimates the probability distributions of the eye parameter and eye gaze. Using an incremental learning framework, the subject does not need personal calibration before using the system. His/her eye parameter estimation and gaze estimation can be improved gradually when he/she is naturally interacting with the system. The experimental result shows that the proposed system can achieve <3° accuracy for different people without explicit personal calibration. © 2014 IEEE.","dynamic Bayesian network; gaze calibration; Gaze estimation","Bayesian networks; Gesture recognition; Human computer interaction; Parameter estimation; Probability distributions; Tracking (position); Calibration process; Dynamic Bayesian networks; Eye gaze tracking; Eye parameters; Gaze estimation; Incremental learning; Natural human computer interactions; Probabilistic approaches; Calibration; animal; Bayes theorem; calibration; eye fixation; human; image processing; physiology; procedures; videorecording; Animals; Bayes Theorem; Calibration; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Video Recording",Article,"Final","",Scopus,2-s2.0-84923174509
"Arar N.M., Gao H., Thiran J.-P.","55248082400;8274973800;35554798200;","Towards convenient calibration for cross-ratio based gaze estimation",2015,"Proceedings - 2015 IEEE Winter Conference on Applications of Computer Vision, WACV 2015",,,"7045945","642","648",,9,"10.1109/WACV.2015.91","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925400476&doi=10.1109%2fWACV.2015.91&partnerID=40&md5=4dea2160668c7f39dae307b5fd26e049","Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Switzerland","Arar, N.M., Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Switzerland; Gao, H., Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Switzerland; Thiran, J.-P., Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Switzerland","Eye gaze movements are considered as a salient modality for human computer interaction applications. Recently, cross-ratio (CR) based eye tracking methods have attracted increasing interest because they provide remote gaze estimation using a single uncalibrated camera. However, due to the simplification assumptions in CR-based methods, their performance is lower than the model-based approaches [8]. Several efforts have been made to improve the accuracy by compensating for the assumptions with subject specific calibration. This paper presents a CR-based automatic gaze estimation system that accurately works under natural head movements. A subject-specific calibration method based on regularized least-squares regression (LSR) is introduced for achieving higher accuracy compared to other state-of-the-art calibration methods. Experimental results also show that the proposed calibration method generalizes better when fewer calibration points are used. This enables user friendly applications with minimum calibration effort without sacrificing too much accuracy. In addition, we adaptively fuse the estimation of the point of regard (PoR) from both eyes based on the visibility of eye features. The adaptive fusion scheme reduces accuracy error by around 20% and also increases the estimation coverage under natural head movements. © 2015 IEEE.",,"Calibration; Eye movements; Human computer interaction; Least squares approximations; Calibration method; Calibration points; Eye tracking methods; Model based approach; Point of regards; Regularized least squares; Remote gaze estimation; Un-calibrated camera; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-84925400476
"Wood E., Baltruaitis T., Zhang X., Sugano Y., Robinson P., Bulling A.","56145872800;36696075900;57142162900;7005470045;57205369790;6505807414;","Rendering of eyes for eye-shape registration and gaze estimation",2015,"Proceedings of the IEEE International Conference on Computer Vision","2015 International Conference on Computer Vision, ICCV 2015",,"7410785","3756","3764",,133,"10.1109/ICCV.2015.428","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973891601&doi=10.1109%2fICCV.2015.428&partnerID=40&md5=6dd7def4925c07ef120e948f051e1761","University of Cambridge, United Kingdom; Max Planck Institute for Informatics, Germany","Wood, E., University of Cambridge, United Kingdom; Baltruaitis, T., University of Cambridge, United Kingdom; Zhang, X., Max Planck Institute for Informatics, Germany; Sugano, Y., Max Planck Institute for Informatics, Germany; Robinson, P., University of Cambridge, United Kingdom; Bulling, A., Max Planck Institute for Informatics, Germany","Images of the eye are key in several computer vision problems, such as shape registration and gaze estimation. Recent large-scale supervised methods for these problems require time-consuming data collection and manual annotation, which can be unreliable. We propose synthesizing perfectly labelled photo-realistic training data in a fraction of the time. We used computer graphics techniques to build a collection of dynamic eye-region models from head scan geometry. These were randomly posed to synthesize close-up eye images for a wide range of head poses, gaze directions, and illumination conditions. We used our model's controllability to verify the importance of realistic illumination and shape variations in eye-region training data. Finally, we demonstrate the benefits of our synthesized training data (SynthesEyes) by out-performing state-of-the-art methods for eye-shape registration as well as cross-dataset appearance-based gaze estimation in the wild. © 2015 IEEE.",,"Computer graphics; Image processing; Rendering (computer graphics); Appearance based; Computer vision problems; Illumination conditions; Manual annotation; Shape registration; Shape variations; State-of-the-art methods; Supervised methods; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84973891601
"Li D., Hu F., Wang L., Zhang M.","36064669100;56038594800;24483987300;55584777175;","Iris center localization using integral projection and gradients",2015,"ICALIP 2014 - 2014 International Conference on Audio, Language and Image Processing, Proceedings",,,"7009788","211","215",,1,"10.1109/ICALIP.2014.7009788","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922534678&doi=10.1109%2fICALIP.2014.7009788&partnerID=40&md5=06d4bbe7b96e0bd6566f0a9b5f0e066d","Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China; Zhejiang Provincial Key Laboratory of Information Network Technology, Hangzhou, 310027, China","Li, D., Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China, Zhejiang Provincial Key Laboratory of Information Network Technology, Hangzhou, 310027, China; Hu, F., Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China, Zhejiang Provincial Key Laboratory of Information Network Technology, Hangzhou, 310027, China; Wang, L., Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China, Zhejiang Provincial Key Laboratory of Information Network Technology, Hangzhou, 310027, China; Zhang, M., Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, 310027, China, Zhejiang Provincial Key Laboratory of Information Network Technology, Hangzhou, 310027, China","Iris center localization becomes more and more popular in the applications of face alignment, human-computer interaction and gaze estimation. However, techniques for low-resolution and low-contrast eye images which are taken in uncontrolled circumstance still need improvement. This paper presents an accurate and fast algorithm to realize iris center localization, by combining integral projection and image gradients. First, the vertical integral projection function is adopted to get precise boundaries of the eyes and to generate a weight map. Second, a novel method combining dot products of gradients with the weight map is proposed. The iris center is detected by searching the maximum of multiplication between the dot products and the weight from the map. A commonly used evaluation is applied to test our result on the very challenging BioID database for eye center and iris center localization. Experiments show that this method can accurately locate irises under natural light conditions and has characteristic of low computation complexity. © 2014 IEEE.","Eye center localization; image gradients; integral projection; pupil and iris localization","Human computer interaction; Computation complexity; Eye center localization; Face alignment; Fast algorithms; Gaze estimation; Image gradients; Integral projections; Iris localization; Image enhancement",Conference Paper,"Final","",Scopus,2-s2.0-84922534678
"Drawdy C.C., Yanik P.M.","56829553000;23096490100;","Gaze Estimation Technique for Directing Assistive Robotics",2015,"Procedia Manufacturing","3",,,"837","844",,2,"10.1016/j.promfg.2015.07.339","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010022023&doi=10.1016%2fj.promfg.2015.07.339&partnerID=40&md5=b8e6a4343df35fe136f48a87c824e655","Western Carolina University, Cullowhee, NC, United States","Drawdy, C.C., Western Carolina University, Cullowhee, NC, United States; Yanik, P.M., Western Carolina University, Cullowhee, NC, United States","Assistive robotics may extend capabilities for individuals with reduced mobility or dexterity. However, effective use of robotic agents typically requires the user to issue control commands in the form of speech, gesture, or text. Thus, for unskilled or impaired users, the need for a paradigm of intuitive Human-Robot Interaction (HRI) is prevalent. It can be inferred that the most productive interactions are those in which the assistive agent is able to ascertain the intention of the user. Also, to perform a task, the agent must know the user's area of attention in three-dimensional space. Eye gaze tracking can be used as a method to determine a specific Volume of Interest (VOI). However, gaze tracking has heretofore been under-utilized as a means of interaction and control in 3D space. This research aims to determine a practical volume of interest in which an individual's eyes are focused by combining past methods in order to achieve greater effectiveness. The proposed method makes use of eye vergence as a useful depth discriminant to generate a tool for improved robot path planning. This research investigates the accuracy of the Vector Intersection (VI) model when applied to a usably large workspace volume. A neural network is also used in tandem with the VI model to create a combined model. The output of the combined model is a VOI that can be used as an aid in a number of applications including robot path planning, entertainment, ubiquitous computing, and others. © 2015 The Authors","Eye tracking; Gaze; Human-Robot Interaction; Neural networks",,Article,"Final","",Scopus,2-s2.0-85010022023
"Cai L., Xiong C., Huang L., Liu C.","57203145670;56097664100;56566995800;9637739900;","A novel face spoofing detection method based on gaze estimation",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9005",,,"547","561",,5,"10.1007/978-3-319-16811-1_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983646541&doi=10.1007%2f978-3-319-16811-1_36&partnerID=40&md5=88e803a6f817cf79dea1689c2b3f93ad","Institute of Automation, Chinese Academy of Sciences, Beijing, China","Cai, L., Institute of Automation, Chinese Academy of Sciences, Beijing, China; Xiong, C., Institute of Automation, Chinese Academy of Sciences, Beijing, China; Huang, L., Institute of Automation, Chinese Academy of Sciences, Beijing, China; Liu, C., Institute of Automation, Chinese Academy of Sciences, Beijing, China","Since gaze is a kind of behavioral biometrics which is difficult to be detected by the surveillance due to the ambiguity of visual attention process, it can be used as a clue for anti-spoofing. This work provides the first investigation in research literature on the use of gaze estimation for face spoofing detection. Firstly, a gaze estimation model mapping the gaze feature to gaze position is established for tracking user’s gaze trajectory. Secondly, gaze histogram is obtained by quantifying and encoding the gaze trajectory. Finally, information entropy on gaze histogram suggests the uncertainty level of user’s gaze movement and estimates the liveness of the user. Our basic assumption is that the gaze trajectory of genuine access has higher uncertainty level than that of attack. Therefore, the greater the entropy, the more probable the user is genuine. Experimental results show that the proposed method obtains competitive performance in distinguishing attacks from genuine access. © Springer International Publishing Switzerland 2015.",,"Behavioral research; Computer vision; Graphic methods; Trajectories; Uncertainty analysis; Behavioral biometrics; Competitive performance; Distinguishing attacks; Face spoofing detections; Gaze estimation; Gaze movements; Information entropy; Visual Attention; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-84983646541
"Siboska D., Karstoft H.","56203521000;36771565700;","Using illumination changes to synchronize eye tracking in visual paradigms",2015,"Communications in Computer and Information Science","511",,,"289","298",,,"10.1007/978-3-319-26129-4_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955269701&doi=10.1007%2f978-3-319-26129-4_19&partnerID=40&md5=6de1f9099470b990173b53e7262ef50b","Department of Engineering, Aarhus University, Aarhus, Denmark","Siboska, D., Department of Engineering, Aarhus University, Aarhus, Denmark; Karstoft, H., Department of Engineering, Aarhus University, Aarhus, Denmark","This paper presents a novel method for synchronizing the recording of a subject’s gaze from an eye tracker (ET) to the display of visual stimuli. The method consists of embedding a signal used as a common time base in a small area of the visual stimuli, measuring this signal with an optical detector attached to the presentation screen, and modulating the global illumination used by the eye tracker synchronously to this measured signal. The timing signal generated with this method can be used to synchronize other data sources, such as electroencephalography (EEG) to the presentation of the visual stimuli as well. The prototype system where this method was implemented achieved a single sample of jitter for both the EEG and ET data. © Springer International Publishing Switzerland 2015.","Electroencephalography; Eye tracking; Gaze estimation; Synchronization","Biomedical engineering; Electrophysiology; Synchronization; Eye-tracking; Gaze estimation; Global illumination; Illumination changes; Measured signals; Optical detectors; Prototype system; Visual stimulus; Electroencephalography",Conference Paper,"Final","",Scopus,2-s2.0-84955269701
"Yu S., Ou W., You X., Jiang X., Zhu Y., Mou Y., Guo W., Tang Y., Chen C.L.P.","56438265200;55613280300;8937550100;36617431100;56911222500;23392939100;57015921500;7404591899;47861886300;","Webcam-based visual gaze estimation under desktop environment",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9490",,,"457","466",,,"10.1007/978-3-319-26535-3_52","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951792369&doi=10.1007%2f978-3-319-26535-3_52&partnerID=40&md5=367d9dbdb01358823c27e4b4033a07bc","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, United States; School of Mathematics and Computer Science, Guizhou Normal University, Guiyang, Guizhou, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, Hubei, China; Research Institute of Huazhong, University of Science and Technology in Shenzhen, Shenzhen, Guangdong, China; University of Macau, Macau, Macau","Yu, S., Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, United States; Ou, W., School of Mathematics and Computer Science, Guizhou Normal University, Guiyang, Guizhou, China; You, X., School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, Hubei, China, Research Institute of Huazhong, University of Science and Technology in Shenzhen, Shenzhen, Guangdong, China; Jiang, X., School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, Hubei, China; Zhu, Y., Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, United States; Mou, Y., School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, Hubei, China; Guo, W., School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, Hubei, China; Tang, Y., School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, Hubei, China, University of Macau, Macau, Macau; Chen, C.L.P., University of Macau, Macau, Macau","Image-based visual gaze estimation has been widely used in various scientific and application-oriented disciplines. However, the high cost and tedious calibration procedure impede its generalization in real scenarios. In this paper, we develop a low cost yet effective webcam based visual gaze estimation system. Different from previous works, we aim at minimizing the system cost, and at the same time, making the system more flexible and feasible to users. More specifically, only a single ordinary webcam is used in our system. Meanwhile, we also proposed a novel calibration mechanism which takes account binocular feature vectors simultaneously, and uses only four visual target points. We compare our system with the state of the art webcam based visual gaze estimation methods. Experimental results demonstrate that our system can achieve satisfactory performance without the requirements of dedicated hardware or tedious calibration procedure. © Springer International Publishing Switzerland 2015.","Binocular calibration; Desktop environment; Flexible; Low cost; Ordinary webcam; Visual gaze estimation","Binoculars; Calibration; Binocular calibrations; Desktop environment; Flexible; Gaze estimation; Low costs; Ordinary webcam; Cost estimating",Conference Paper,"Final","",Scopus,2-s2.0-84951792369
"Cai L., Huang L., Liu C.","57203145670;56566995800;9637739900;","Person-specific face spoofing detection for replay attack based on gaze estimation",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9428",,,"201","211",,4,"10.1007/978-3-319-25417-3_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84950283760&doi=10.1007%2f978-3-319-25417-3_25&partnerID=40&md5=06e92556410ffe5e4e7326832250e60d","Institute of Automation Chinese Academy of Sciences, Beijing, China","Cai, L., Institute of Automation Chinese Academy of Sciences, Beijing, China; Huang, L., Institute of Automation Chinese Academy of Sciences, Beijing, China; Liu, C., Institute of Automation Chinese Academy of Sciences, Beijing, China","Based on gaze estimation, we propose an effective person-specific spoofing detection method to counter replay attack using a noninvasive challenge and response technique. The points on the computer screen create the challenge, and the gaze positions of the user as they look at the computer screen form the response. Firstly, face identification is conducted to recognize identity. Secondly, gaze estimation model is trained for each subject by adaptive linear regression with incremental learning and used to predict gaze positions when user is looking at the computer screen. Finally, difference between predicted gaze positions and system point locations is used as fake score to evaluate the liveness of user. Our basic assumption is that a genuine access can be attacked by salient objects and follow them. Therefore, the lower the fake score is, the more probable the user is genuine. Experimental results show that proposed method obtains competitive performance in distinguishing replay attacks from genuine accesses. © Springer International Publishing Switzerland 2015.","Face spoofing detection; Gaze estimation; Incremental learning; Replay attack","Biometrics; Challenge and response; Competitive performance; Detection methods; Face identification; Face spoofing detections; Gaze estimation; Incremental learning; Replay attack; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-84950283760
"Cazzato D., Leo M., Evangelista A., Distante C.","55866556300;7006471658;56967728900;55884135100;","Soft biometrics by modeling temporal series of gaze cues extracted in the wild",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9386",,,"391","402",,3,"10.1007/978-3-319-25903-1_34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949257848&doi=10.1007%2f978-3-319-25903-1_34&partnerID=40&md5=07ed490a2ec87c23b48ea2b660368611","National Research Council of Italy - Institute of Optics, Arnesano, LE, Italy; University of Salento, Lecce, Italy","Cazzato, D., University of Salento, Lecce, Italy; Leo, M., National Research Council of Italy - Institute of Optics, Arnesano, LE, Italy; Evangelista, A., University of Salento, Lecce, Italy; Distante, C., National Research Council of Italy - Institute of Optics, Arnesano, LE, Italy","Soft biometric systems have spread among recent years, both for powering classical biometrics, as well as stand alone solutions with several application scopes ranging from digital signage to human-robot interaction. Among all, in the recent years emerged the possibility to consider as a soft biometrics also the temporal evolution of the human gaze and some recent works in the literature explored this exciting research line by using expensive and (perhaps) unsafe devices which, moreover, require user cooperation to be calibrated. By our knowledge the use of a low-cost, non-invasive, safe and calibration-free gaze estimator to get soft-biometrics data has not been investigated yet. This paper fills this gap by analyzing the soft-biometrics performances obtained by modeling the series of gaze estimated by exploiting the combination of head poses and eyes’ pupil locations on data acquired by an off-the-shelf RGB-D device. ©Springer International Publishing Switzerland 2015.",,"Calibration; Human robot interaction; Robots; Calibration free; Digital signage; Head pose; Pupil locations; Soft biometrics; Stand -alone; Temporal evolution; User cooperation; Biometrics",Book Chapter,"Final","",Scopus,2-s2.0-84949257848
"He Q., Hong X., Chai X., Holappa J., Zhao G., Chen X., Pietikäinen M.","56405868000;35179692900;7006781144;56024842600;47661917700;8284171300;7006291661;","OMEG: Oulu multi-pose eye gaze dataset",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9127",,,"418","427",,9,"10.1007/978-3-319-19665-7_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947916886&doi=10.1007%2f978-3-319-19665-7_35&partnerID=40&md5=f7d410f10b608b934049b7f8ee271c64","Department of Computer Science and Engineering, University of Oulu, Oulu, Finland; Key Lab of Intelligent Information Processing of the Chinese Academy of Sciences, The Institute of Computing Technology of the Chinese Academy of Sciences, Beijing, China","He, Q., Department of Computer Science and Engineering, University of Oulu, Oulu, Finland; Hong, X., Department of Computer Science and Engineering, University of Oulu, Oulu, Finland; Chai, X., Key Lab of Intelligent Information Processing of the Chinese Academy of Sciences, The Institute of Computing Technology of the Chinese Academy of Sciences, Beijing, China; Holappa, J., Department of Computer Science and Engineering, University of Oulu, Oulu, Finland; Zhao, G., Department of Computer Science and Engineering, University of Oulu, Oulu, Finland; Chen, X., Department of Computer Science and Engineering, University of Oulu, Oulu, Finland, Key Lab of Intelligent Information Processing of the Chinese Academy of Sciences, The Institute of Computing Technology of the Chinese Academy of Sciences, Beijing, China; Pietikäinen, M., Department of Computer Science and Engineering, University of Oulu, Oulu, Finland","Data is in a very important position for pattern recognition tasks including eye gaze estimation. In the literature, most researchers used normal face datasets, which are not specifically designed for eye gaze estimation. As a result, it is difficult to obtain fine labeled eye gaze direction. Therefore large datasets with well-defined gaze directions are desired. To facilitate related researches, we collect and establish the Oulu Multi-pose Eye Gaze Dataset. Inspired by the psychological observation that gaze direction is intrinsically linked with the head orientation, we are devoted to a new data set of eye gaze images captured under multiple head poses. It finally results in a dataset containing over 40K images from 50 subjects, who were asked to fixate on 10 special points on screen under different poses respectively. We investigate a new eye gaze estimation approach by using the IGO based description, and compare it with other popular eye gaze estimation approaches to provide the baseline results on our dataset. © Springer International Publishing Switzerland 2015.","Dataset; Eye gaze; Head pose","Face recognition; Pattern recognition; Baseline results; Dataset; Eye-gaze; Gaze direction; Head pose; Large datasets; Multi-pose; Special points; Image analysis",Conference Paper,"Final","",Scopus,2-s2.0-84947916886
[无可用作者姓名],[无可用的作者 ID],"2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015",2015,"2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015",,,,"","",619,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944916539&partnerID=40&md5=3ae895454a8f6bb36ca8fac2cb35eeea",,"","The proceedings contain 95 papers. The topics discussed include: expression robust 3D face recognition by matching multi-component local shape descriptors on the nasal and adjoining cheek regions; leveraging geometry and appearance cues for recognizing family photos; realistic inverse lighting from a single 2D image of a face, taken under unknown and complex lighting; detecting bids for eye contact using a wearable camera; hierarchical hybrid statistic based video binary code and its application to face retrieval in TV-series; age estimation via unsupervised neural networks; three-dimensional head pose estimation in-the-wild; block-wise constrained sparse graph for face image representation; fully automated facial picture evaluation using high level attributes; sparse low-rank fusion based deep features for missing modality face recognition; robust gaze estimation based on adaptive fusion of multiple cameras; and face recognition under pose variation with active shape model to adjust Gabor filter kernels and to correct feature extraction location.",,"Cameras; Filters; Geometry; Image Analysis; Pattern Recognition; Three Dimensional Design",Conference Review,"Final","",Scopus,2-s2.0-84944916539
"Hong I., Shin D., Kim Y., Bong K., Park S., Lee K., Yoo H.-J.","52663471300;56110138000;55659695300;55658301500;55659692300;56469959700;7201373390;","A keypoint-level parallel pipelined object recognition processor with gaze activation image sensor for mobile smart glasses system",2015,"IEEE Symposium on Low-Power and High-Speed Chips, COOL Chips XVIII - Proceedings",,,"7158531","","",,,"10.1109/CoolChips.2015.7158531","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943279428&doi=10.1109%2fCoolChips.2015.7158531&partnerID=40&md5=3291ef90946ca76917d76fdfc2a91100","Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, South Korea","Hong, I., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, South Korea; Shin, D., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, South Korea; Kim, Y., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, South Korea; Bong, K., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, South Korea; Park, S., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, South Korea; Lee, K., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, South Korea; Yoo, H.-J., Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, South Korea","In this paper, a low-power real-time gaze-activated object recognition processor is proposed for a battery-powered smart glasses system. For high energy efficiency, we propose keypoint-level pipelined architecture to increase the hardware utilziation which results in significant power reduction of the real-time recognition processor. In addition, low-power gaze-activation image sensor with mixed-mode architecture is proposed for the glass user's gaze estimation. Therefore, only the small image region where the glasses user is seeing needs to be processed by the recognition processor leading to further power reduction. As a result, the proposed object recognition processor shows 30fps real-time performance only with 75mW power consumption, which is 3.5x and 4.4x smaller power than the state-of-the-art works. © 2015 IEEE.","gaze estimation; heterogeneous multi-core; keypoint-level pipeline; smart glasses","Chemical activation; Energy efficiency; Glass; Image processing; Image sensors; Object recognition; Gaze estimation; Heterogeneous Multi-Cores; High energy efficiency; Keypoint; Pipelined architecture; Real time performance; Real time recognition; Smart glass; Pipeline processing systems",Conference Paper,"Final","",Scopus,2-s2.0-84943279428
"Mohapatra S.S., Kinage K.","56890494900;36600595000;","Iris tracking using a single web-cam without IR illumination",2015,"Proceedings - 1st International Conference on Computing, Communication, Control and Automation, ICCUBEA 2015",,,"7155939","706","711",,4,"10.1109/ICCUBEA.2015.144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943272444&doi=10.1109%2fICCUBEA.2015.144&partnerID=40&md5=a68f941881b1584305e9f57f8ce2d65a","MIT College of Engineering, Pune, India","Mohapatra, S.S., MIT College of Engineering, Pune, India; Kinage, K., MIT College of Engineering, Pune, India","Iris Tracking is the process of determining the point of gaze or the motion of an eye. In today's era, the combination of Iris Tracking and gaze estimation shows a person's interest. In this paper, we have focused on a single-camera-based gaze estimation algorithm. The paper also describes the implementation of both iris and movement of cursor according to iris position which can be used and detected for gaze estimation in order to improve accuracy without using IR Illumination or any sensor camera. The system works at different distraction conditions, as the normalized error rate is minimal. © 2015 IEEE.","Behavioral biometrics; Gaze estimation; Human-device interaction; Iris tracking","Computer programming; Behavioral biometrics; Gaze estimation; Human-device interaction; Iris tracking; Normalized errors; Point of gaze; Single cameras; WebCams; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-84943272444
"Skodras E., Kanas V.G., Fakotakis N.","47062010600;54682057400;7003320511;","On visual gaze tracking based on a single low cost camera",2015,"Signal Processing: Image Communication","36",,"14968","29","42",,17,"10.1016/j.image.2015.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939216557&doi=10.1016%2fj.image.2015.05.007&partnerID=40&md5=eca82fd0577057ce9723dcca6a3a1a36","Department of Electrical and Computer Engineering, University of Patras, Patras, Greece","Skodras, E., Department of Electrical and Computer Engineering, University of Patras, Patras, Greece; Kanas, V.G., Department of Electrical and Computer Engineering, University of Patras, Patras, Greece; Fakotakis, N., Department of Electrical and Computer Engineering, University of Patras, Patras, Greece","Abstract Gaze tracking technologies provide an unconventional way of human-computer interaction, envisaged to advance practical applications and industrial products in a multitude of fields. The success of such systems depends on selecting the best calibration setup and image features that correspond to a person's line of sight. The purpose of this study is to estimate eye gaze from a single, low cost web-cam, under natural lighting. Facial traits are extracted from the sensory data, from which distance vectors related to gaze are derived. Different experimental setups are studied to evaluate the robustness of the proposed method with respect to various calibration setups, camera position and head movements. The use of new additional features improves the modeling of the subtle eye movements in the vertical direction, while a new calibration setup is proposed that further enhances the performance. The results demonstrate that the proposed framework is able to track gaze with good accuracy, consolidating the use of inexpensive equipment and techniques towards an ever-expanding range of gaze tracking applications. © 2015 Elsevier B.V.","Eye tracking; Gaze estimation; Gaze tracker; HCI; Human computer interaction","Calibration; Cameras; Cost benefit analysis; Eye movements; Tracking (position); Camera positions; Eye-tracking; Gaze estimation; Gaze tracker; Industrial product; Inexpensive equipment; Natural lighting; Vertical direction; Human computer interaction",Article,"Final","",Scopus,2-s2.0-84939216557
"Ferhat O., Llanza A., Vilariño F.","56095267500;56728892900;6508105875;","A feature-based gaze estimation algorithm for natural light scenarios",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9117",,,"569","576",,3,"10.1007/978-3-319-19390-8_64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937510851&doi=10.1007%2f978-3-319-19390-8_64&partnerID=40&md5=d31c57a384ffef7b9ea32e8bace83782","Universitat Autònoma de Barcelona, Bellaterra, Spain; Computer Vision Center, Bellaterra, Spain","Ferhat, O., Universitat Autònoma de Barcelona, Bellaterra, Spain, Computer Vision Center, Bellaterra, Spain; Llanza, A., Universitat Autònoma de Barcelona, Bellaterra, Spain, Computer Vision Center, Bellaterra, Spain; Vilariño, F., Universitat Autònoma de Barcelona, Bellaterra, Spain, Computer Vision Center, Bellaterra, Spain","We present an eye tracking system that works with regular webcams. We base our work on open source CVC Eye Tracker [7] and we propose a number of improvements and a novel gaze estimation method. The new method uses features extracted from iris segmentation and it does not fall into the traditional categorization of appearance– based/model–based methods. Our experiments show that our approach reduces the gaze estimation errors by 34% in the horizontal direction and by 12% in the vertical direction compared to the baseline system. © Springer International Publishing Switzerland 2015.","Eye tracking; Gaze estimation; Natural light; Webcam","Algorithms; Image analysis; Baseline systems; Eye tracking systems; Eye-tracking; Gaze estimation; Iris segmentation; Natural light; Vertical direction; Webcam; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-84937510851
"Sankar A., Suresh A., Varun Babu P., Baskar A., Vasudevan S.K.","56689589600;57198120486;56690412800;56511853600;55793633700;","An in-depth analysis of applications of object recognition",2015,"Research Journal of Applied Sciences, Engineering and Technology","10","1",,"1","14",,2,"10.19026/rjaset.10.2547","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931340325&doi=10.19026%2frjaset.10.2547&partnerID=40&md5=33ddff395cacc670624734d892acda9a","Department of Computer Science and Engineering, Amrita School of Engineering, Amrita University, India","Sankar, A., Department of Computer Science and Engineering, Amrita School of Engineering, Amrita University, India; Suresh, A., Department of Computer Science and Engineering, Amrita School of Engineering, Amrita University, India; Varun Babu, P., Department of Computer Science and Engineering, Amrita School of Engineering, Amrita University, India; Baskar, A., Department of Computer Science and Engineering, Amrita School of Engineering, Amrita University, India; Vasudevan, S.K., Department of Computer Science and Engineering, Amrita School of Engineering, Amrita University, India","Image processing has become one of the most unavoidable fields of engineering. The way the applications are designed based on Image processing is simply superb. This study is drafted as a study paper aimed at reviewing the object recognition techniques supported in Image Processing Sector. Analyzing object recognition through the applications is a new approach and that is what we have tried through our paper. We have taken effort to check the utilization of Object Recognition techniques in the fields of Industrial applications which includes a. automobiles b. food and beverage sector and c. fabric sector. Then attention is paid towards robotic applications. Remote sensing is also observed to be one of the hottest sectors which deploys objects recognition techniques to a better extent. Finally it is ended up with medicinal applications. © Maxwell Scientific Organization, 2015.","AdaBoost; Appearance-based method; Feature-based method; Filters; Fuzzy clustering; Gaze determination; Genetic algorithm; Haar-like structure; Head pose; Histogram; Hyperspectral; Image stabilization; Machine vision; Model-based method; Motion compensation; Motion estimation; Segmentation; SIFT (scale-invariant feature transform); Spectral; Spectroscopy; Statistical; Texture; Thresholding",,Article,"Final","",Scopus,2-s2.0-84931340325
"Xiong C., Huang L., Liu C.","56097664100;56566995800;9637739900;","Gaze estimation using a hybrid appearance and motion descriptor",2015,"Proceedings of SPIE - The International Society for Optical Engineering","9443",,"944320","","",,,"10.1117/12.2178824","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925436549&doi=10.1117%2f12.2178824&partnerID=40&md5=d7341b7238f4833c8df430554439bf43","Institute of Automation, Chinese Academy of Sciences, China","Xiong, C., Institute of Automation, Chinese Academy of Sciences, China; Huang, L., Institute of Automation, Chinese Academy of Sciences, China; Liu, C., Institute of Automation, Chinese Academy of Sciences, China","It is a challenging problem to realize a robust and low cost gaze estimation system. Existing appearance-based and feature-based methods both have achieved impressive progress in the past several years, while their improvements are still limited by feature representation. Therefore, in this paper, we propose a novel descriptor combining eye appearance and pupil center-cornea reflections (PCCR). The hybrid gaze descriptor represents eye structure from both feature level and topology level. At the feature level, a glints-centered appearance descriptor is presented to capture intensity and contour information of eye, and a polynomial representation of normalized PCCR vector is employed to capture motion information of eyeball. At the topology level, the partial least squares is applied for feature fusion and selection. At last, sparse representation based regression is employed to map the descriptor to the point-of-gaze (PoG). Experimental results show that the proposed method achieves high accuracy and has a good tolerance to head movements. © 2015 SPIE.","Gaze estimation; hybrid appearance and motion descriptor; partial least squares; pupil center-cornea reflections; sparse representation","Gesture recognition; Least squares approximations; Topology; Gaze estimation; Motion descriptors; Partial least square (PLS); Pupil centers; Sparse representation; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-84925436549
"Jafari R., Ziou D.","12791016800;7004105959;","Eye-gaze estimation under various head positions and iris states",2015,"Expert Systems with Applications","42","1",,"510","518",,14,"10.1016/j.eswa.2014.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907522029&doi=10.1016%2fj.eswa.2014.08.003&partnerID=40&md5=b681492dbd5d43cef84276f617c885fb","Departement d'Informatique, Universite de Sherbrooke, Sherbrooke, QC  J1K 2R1, Canada","Jafari, R., Departement d'Informatique, Universite de Sherbrooke, Sherbrooke, QC  J1K 2R1, Canada; Ziou, D., Departement d'Informatique, Universite de Sherbrooke, Sherbrooke, QC  J1K 2R1, Canada","This paper describes a method for eye-gaze estimation under normal head movement. In this method, head position and orientation are acquired by Kinect depth data and eye direction is obtained from high resolution images. We propose the Bayesian multinomial logistic regression based on a variational approximation to construct a gaze mapping function and to verify iris state. Our method eliminates limitation of head movements, eye closure and light source as common drawbacks in most conventional techniques. The efficiency of the proposed method is validated by performance evaluation for multiple people with different distances and poses to the camera under various eye states. © 2014 Elsevier Ltd. All rights reserved.","Bayesian logistic regression; Gaze estimation; Kinect","Light sources; Regression analysis; Conventional techniques; Gaze estimation; High resolution image; Kinect; Logistic regressions; Mapping functions; Multinomial logistic regression; Variational approximation; Eye movements",Review,"Final","",Scopus,2-s2.0-84907522029
"Xiong C., Huang L., Liu C.","56097664100;56566995800;9637739900;","Gaze estimation based on 3D face structure and pupil centers",2014,"Proceedings - International Conference on Pattern Recognition",,,"6976918","1156","1161",,6,"10.1109/ICPR.2014.208","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919904483&doi=10.1109%2fICPR.2014.208&partnerID=40&md5=32b1379476fdeefee1901124ee5c7328","Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing, 100190, China","Xiong, C., Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing, 100190, China; Huang, L., Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing, 100190, China; Liu, C., Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing, 100190, China","It is a challenging problem to realize a robust and low cost gaze estimation system. Most existing feature-based gaze estimation methods strongly rely on cornea reflections, which are unstable to glasses, head movements and natural light. In this paper, we propose a novel gaze estimation method without use of cornea reflections based on a stereo camera system. Firstly, 3D Active Shape Models (ASM) is reconstructed using stereo vision to represent 3D face structure. Then, without use of cornea reflections, a 3D Pupil-Eye-Contours based feature is proposed to represent human gaze information. What's more, precise estimation of head poses based on 3D face structure is employed to rectify the 3D pupil centers and eye contours for improving the ability of tolerance to head movements. Experiments on fifteen subjects show that the system is accurate and allows natural head movements. © 2014 IEEE.","3D face structure; Gaze estimation; Head pose","Eye movements; Pattern recognition; Stereo image processing; Stereo vision; 3d active shape models; 3D faces; Feature-based; Gaze estimation; Head movements; Head pose; Pupil centers; Stereo camera system; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84919904483
"Lai C.-C., Chen Y.-T., Chen K.-W., Chen S.-C., Shih S.-W., Hung Y.-P.","36801904900;56462054200;55884042700;54781137900;7201648778;26643286300;","Appearance-based gaze tracking with free head movement",2014,"Proceedings - International Conference on Pattern Recognition",,,"6977039","1869","1873",,14,"10.1109/ICPR.2014.327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919897444&doi=10.1109%2fICPR.2014.327&partnerID=40&md5=db6afc0bcc8b3132c7cce8d126e756a7","Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan; Dept. of Computer Science and Information Engineering, National Chi Nan University, Taiwan","Lai, C.-C., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan; Chen, Y.-T., Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan; Chen, K.-W., Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan; Chen, S.-C., Dept. of Computer Science and Information Engineering, National Taiwan University, Taiwan; Shih, S.-W., Dept. of Computer Science and Information Engineering, National Chi Nan University, Taiwan; Hung, Y.-P., Graduate Institute of Networking and Multimedia, National Taiwan University, Taiwan","In this work, we develop an appearance-based gaze tracking system allowing user to move their head freely. The main difficulty of the appearance-based gaze tracking method is that the eye appearance is sensitive to head orientation. To overcome the difficulty, we propose a 3-D gaze tracking method combining head pose tracking and appearance-based gaze estimation. We use a random forest approach to model the neighbor structure of the joint head pose and eye appearance space, and efficiently select neighbors from the collected high dimensional data set. Li-optimization is then used to seek for the best solution for regression from the selected neighboring samples. Experiment results shows that it can provide robust binocular gaze tracking results with less constraints but still provides moderate estimation accuracy of gaze estimation. © 2014 IEEE.",,"Clustering algorithms; Decision trees; Pattern recognition; Appearance based; Binocular gazes; Gaze estimation; Gaze tracking; Gaze tracking system; Head-pose tracking; High dimensional data; Neighbor structures; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-84919897444
"Schneider T., Schauerte B., Stiefelhagen R.","57198378579;35234793300;6602180348;","Manifold alignment for person independent appearance-based gaze estimation",2014,"Proceedings - International Conference on Pattern Recognition",,,"6976920","1167","1172",,51,"10.1109/ICPR.2014.210","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919897404&doi=10.1109%2fICPR.2014.210&partnerID=40&md5=84596d78838af6b0a0324ebc6304db58","Computer Vision for Human Computer Interaction Lab, Karlsruhe Institute of Technology, Germany","Schneider, T., Computer Vision for Human Computer Interaction Lab, Karlsruhe Institute of Technology, Germany; Schauerte, B., Computer Vision for Human Computer Interaction Lab, Karlsruhe Institute of Technology, Germany; Stiefelhagen, R., Computer Vision for Human Computer Interaction Lab, Karlsruhe Institute of Technology, Germany","We show that dually supervised manifold embedding can improve the performance of machine learning based person-independent and thus calibration-free gaze estimation. For this purpose, we perform a manifold embedding for each person in the training dataset and then learn a linear transformation that aligns the individual, person-dependent manifolds. We evaluate the effect of manifold alignment on the recently presented Columbia dataset, where we analyze the influence on 6 regression methods and 8 feature variants. Using manifold alignment, we are able to improve the person-independent gaze estimation performance by up to 31.2 % compared to the best approach without manifold alignment. © 2014 IEEE.",,"Embeddings; Linear transformations; Mathematical transformations; Regression analysis; Appearance based; Calibration free; Gaze estimation; Manifold alignments; Person-dependent; Person-independent; Regression method; Training dataset; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-84919897404
"Li N., Busso C.","55805334600;35742852700;","User-independent gaze estimation by exploiting similarity measures in the eye pair appearance eigenspace",2014,"ICMI 2014 - Proceedings of the 2014 International Conference on Multimodal Interaction",,,,"335","338",,3,"10.1145/2663204.2663250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947217328&doi=10.1145%2f2663204.2663250&partnerID=40&md5=d7a0639c0df61d482435606201a28122","Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States","Li, N., Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States; Busso, C., Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States","The design of gaze-based computer interfaces has been an active research area for over 40 years. One challenge of using gaze detectors is the repetitive calibration process required to adjust the parameters of the systems, and the constrained conditions imposed on the user for robust gaze estimation. We envision user-independent gaze detectors that do not require calibration, or any cooperation from the user. Toward this goal, we investigate an appearancebased approach, where we estimate the eigenspace for the gaze using principal component analysis (PCA). The projections are used as features of regression models that estimate the screen's coordinates. As expected, the performance of the approach decreases when the models are trained without data from the target user (i.e., user-independent condition). This study proposes an appealing training approach to bridge the gap in performance between user-dependent and user-independent conditions. Using the projections onto the eigenspace, the scheme identifies samples in training set that are similar to the testing images. We build the sample covariance matrix and the regression models only with these samples. We consider either similar frames or data from subjects with similar eye appearance. The promising results suggest that the proposed training approach is a feasible and convenient scheme for gaze-based multimodal interfaces. Copyright 2014 ACM.","Computer user interface; Eigenspace analysis; Similar subject measurement; User-independent gaze estimation","Calibration; Covariance matrix; Interactive computer systems; Modal analysis; Regression analysis; User interfaces; Appearance-based approaches; Calibration process; Computer users; Constrained conditions; Eigen space analysis; Gaze estimation; Multi-modal interfaces; Sample covariance matrix; Principal component analysis",Conference Paper,"Final","",Scopus,2-s2.0-84947217328
"Li N.","55805334600;","Appearance based user-independent gaze estimation",2014,"ICMI 2014 - Proceedings of the 2014 International Conference on Multimodal Interaction",,,,"379","383",,,"10.1145/2663204.2666288","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947205671&doi=10.1145%2f2663204.2666288&partnerID=40&md5=4db5dcad498b1a2627d6ab3cbbdbf667","Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States","Li, N., Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX  75080, United States","An ideal gaze user interface should be able to accurately estimates the user's gaze direction in a non-intrusive setting. Most studies on gaze estimation focus on the accuracy of the estimation results, imposing important constraints on the user such as no head movement, intrusive head mount setting and repetitive calibration process. Due to these limitations, most graphic user interfaces (GUIs) are reluctant to include gaze as an input modality. We envision user- independent gaze detectors for user computer interaction that do not impose any constraints on the users. We believe the appearance of the eye pairs, which implicitly reveals head pose, provides conclusive information on the gaze direction. More importantly, the relative appearance changes in the eye pairs due to the different gaze direction should be consistent among different human subjects. We collected a multimodal corpus (MSP-GAZE) to study and evaluate user independent, appearance based gaze estimation approaches. This corpus considers important factors that affect the appearance based gaze estimation: The individual difference, the head movement, and the distance between the user and the interface's screen. Using this database, our initial study focused on the eye pair appearance eigenspace approach, where the projections into the eye appearance eigenspace basis are used to build regression models to estimate the gaze position. We compare the results between user dependent (training and testing on the same subject) and user independent (testing subject is not included in the training data) models. As expected, due to the individual differences between subjects, the performance decreases when the models are trained without data from the target user. The study aims to reduce the gap between user dependent and user independent conditions. Copyright 2014 ACM.","Domain adaptation; Eigenspace analysis; Tensor analysis; User-independent gaze estimation","Calibration; Interactive computer systems; Regression analysis; User interfaces; Domain adaptation; Eigen space analysis; Gaze estimation; Graphic user interface; Individual Differences; Tensor analysis; Training and testing; User computer interaction; Modal analysis",Conference Paper,"Final","",Scopus,2-s2.0-84947205671
"Huang M.X., Kwok T.C.K., Ngai G., Leong H.V., Chan S.C.F.","55258532000;56432365700;8915594400;7005127948;7404255433;","Building a self-learning eye gaze model from user interaction data",2014,"MM 2014 - Proceedings of the 2014 ACM Conference on Multimedia",,,,"1017","1020",,13,"10.1145/2647868.2655031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84913526109&doi=10.1145%2f2647868.2655031&partnerID=40&md5=5a81baafe5b9cbbba0824ec3330868bb","Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong","Huang, M.X., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Kwok, T.C.K., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Ngai, G., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Leong, H.V., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong; Chan, S.C.F., Department of Computing, Hong Kong Polytechnic University, Hong Kong, Hong Kong","Most eye gaze estimation systems rely on explicit calibration, which is inconvenient to the user, limits the amount of possible training data and consequently the performance. Since there is likely a strong correlation between gaze and interaction cues, such as cursor and caret locations, a supervised learning algorithm can learn the complex mapping between gaze features and the gaze point by training on incremental data collected implicitly from normal computer interactions. We develop a set of robust geometric gaze features and a corresponding data validation mechanism that identifies good training data from noisy interaction-informed data collected in real-use scenarios. Based on a study of gaze movement patterns, we apply behavior-informed validation to extract gaze features that correspond with the interaction cue, and data-driven validation provides another level of crosschecking using previous good data. Experimental evaluation shows that the proposed method achieves an average error of 4.06°, and demonstrates the effectiveness of the proposed gaze estimation method and corresponding validation mechanism.","Data validation; Gaze estimation; Implicit modeling; Supervised learning","Learning systems; Supervised learning; Computer interaction; Data validation; Experimental evaluation; Gaze estimation; Implicit model; Incremental data; Strong correlation; User interaction; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-84913526109
"Lu F., Sugano Y., Okabe T., Sato Y.","54956194300;7005470045;7201390055;35230954300;","Adaptive Linear Regression for Appearance-Based Gaze Estimation",2014,"IEEE Transactions on Pattern Analysis and Machine Intelligence","36","10","6777326","2033","2046",,117,"10.1109/TPAMI.2014.2313123","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937580410&doi=10.1109%2fTPAMI.2014.2313123&partnerID=40&md5=cbf93063eb841fd0f08f82d993db1a2b","Institute of Industrial Science, University of Tokyo, Tokyo, 153-8505, Japan","Lu, F., Institute of Industrial Science, University of Tokyo, Tokyo, 153-8505, Japan; Sugano, Y., Institute of Industrial Science, University of Tokyo, Tokyo, 153-8505, Japan; Okabe, T., Institute of Industrial Science, University of Tokyo, Tokyo, 153-8505, Japan; Sato, Y., Institute of Industrial Science, University of Tokyo, Tokyo, 153-8505, Japan","We investigate the appearance-based gaze estimation problem, with respect to its essential difficulty in reducing the number of required training samples, and other practical issues such as slight head motion, image resolution variation, and eye blinking. We cast the problem as mapping high-dimensional eye image features to low-dimensional gaze positions, and propose an adaptive linear regression (ALR) method as the key to our solution. The ALR method adaptively selects an optimal set of sparsest training samples for the gaze estimation via ℓ1-optimization. In this sense, the number of required training samples is significantly reduced for high accuracy estimation. In addition, by adopting the basic ALR objective function, we integrate the gaze estimation, sub-pixel alignment and blink detection into a unified optimization framework. By solving these problems simultaneously, we successfully handle slight head motion, image resolution variation and eye blinking in appearance-based gaze estimation. We evaluated the proposed method by conducting experiments with multiple users and variant conditions to verify its effectiveness. © 2014 IEEE.","blink detection; Eye; face and gesture recognition; gaze estimation; sub-pixel alignment","Image resolution; Pixels; Problem solving; Sampling; Blink detections; Face and gesture recognition; Gaze estimation; High-dimensional; Objective functions; Resolution variations; Sub pixels; Unified optimization framework; Gesture recognition",Article,"Final","",Scopus,2-s2.0-84937580410
"Sun L., Song M., Liu Z., Sun M.-T.","55535024300;9333722700;7406672531;7403181446;","Real-time gaze estimation with online calibration",2014,"IEEE Multimedia","21","4",,"28","37",,16,"10.1109/MMUL.2014.54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909969176&doi=10.1109%2fMMUL.2014.54&partnerID=40&md5=11b383fcbe35372577206d4c69581056","Zhejiang University, China; Microsoft Research, United States; University of Washington, Seattle, WA, United States","Sun, L., Zhejiang University, China; Song, M., Zhejiang University, China; Liu, Z., Microsoft Research, United States; Sun, M.-T., University of Washington, Seattle, WA, United States","Gaze-tracking technology is highly valuable in many interactive and diagnostic applications. For many gaze estimation systems, calibration is an unavoidable procedure necessary to determine certain person-specific parameters, either explicitly or implicitly. Recently, several offline implicit calibration methods have been proposed to ease the calibration burden. However, the calibration procedure is still cumbersome, and gaze estimation accuracy needs further improvement. In this article, the authors present a novel 3D gaze estimation system with online calibration. The proposed system is based on a new 3D model-based gaze estimation method using a single consumer depth camera sensor (via Kinect). Unlike previous gaze estimation methods using explicit offline calibration with fixed number of calibration points or implicit calibration, their approach constantly improves person-specific eye parameters through online calibration, which enables the system to adapt gradually to a new user. The experimental results and the human-computer interaction (HCI) application show that the proposed system can work in real time with superior gaze estimation accuracy and minimal calibration burden. © 2014 IEEE.","3D model-based gaze estimation; Calibration; Cameras; Eye tracking; Gaze direction; HCI; Medical diagnostic imaging; Multimedia; Online calibration; Real-time systems; Research and development; Target tracking; Three-dimensional displays","Cameras; Human computer interaction; Target tracking; Eye-tracking; Gaze direction; Gaze estimation; Medical diagnostic imaging; Multimedia; On-line calibration; Research and development; Three-dimensional display; Calibration",Article,"Final","",Scopus,2-s2.0-84909969176
"Narayanan A., Kaimal R.M., Bijlani K.","57205486682;6603335048;35721404600;","Yaw estimation using cylindrical and ellipsoidal face models",2014,"IEEE Transactions on Intelligent Transportation Systems","15","5","6807708","2308","2320",,14,"10.1109/TITS.2014.2313371","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907512418&doi=10.1109%2fTITS.2014.2313371&partnerID=40&md5=03acf1e622f3753a9a9cdab797517e7a","Amrita E-Learning Research Lab, Department of Computer Science, Amrita Vishwa Vidyapeetham, Kollam, 690 525, India; Department of Computer Science, Amrita Vishwa Vidyapeetham, Kollam, 690 525, India; Amrita E-Learning Research Lab, Amrita Vishwa Vidyapeetham, Kollam, 690 525, India","Narayanan, A., Amrita E-Learning Research Lab, Department of Computer Science, Amrita Vishwa Vidyapeetham, Kollam, 690 525, India; Kaimal, R.M., Department of Computer Science, Amrita Vishwa Vidyapeetham, Kollam, 690 525, India; Bijlani, K., Amrita E-Learning Research Lab, Amrita Vishwa Vidyapeetham, Kollam, 690 525, India","Accurate head yaw estimation is necessary for detecting driver inattention in forward collision warning systems. In this paper, we propose three geometric models under the ellipsoidal framework for accurate head yaw estimation. We present theoretical analysis of the cylindrical and ellipsoidal face models used for yaw angle estimation of head rotation. The relationship between cylindrical, ellipsoidal, and proposed models is derived. We provide error functions for all models. Furthermore, for each model, over/under estimation of angle, zero crossings of error, bounds on yaw angle estimate, and bounds on error are presented. Experimental results of the proposed models on four standard head pose data sets yielded a mean absolute error between 4° and 8° demonstrating the efficacy of the proposed models over the state-of-the-art methods. © 2000-2011 IEEE.","Driver monitoring system; gaze estimation; Head-orientation estimation","Driver monitoring system; Face models; Gaze estimation",Article,"Final","",Scopus,2-s2.0-84907512418
"Zhang J., Di L., Chen L.","55763931600;56405367300;55839448900;","Design and implementation of gaze tracking system with iPad",2014,"RTCSA 2014 - 20th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications",,,"6910542","","",,3,"10.1109/RTCSA.2014.6910542","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908627791&doi=10.1109%2fRTCSA.2014.6910542&partnerID=40&md5=f8aaf6d16585473d2ffa06f1caae51e6","School of Science and Information Engineering, Yunnan Agricultural University, Kunming, China; School of Mechanic and Electronic Engineering, Yunnan Agricultural University, Kunming, China","Zhang, J., School of Science and Information Engineering, Yunnan Agricultural University, Kunming, China; Di, L., School of Mechanic and Electronic Engineering, Yunnan Agricultural University, Kunming, China; Chen, L., School of Mechanic and Electronic Engineering, Yunnan Agricultural University, Kunming, China","Gaze tracking is the process of measuring gaze point or the motion of an eye relative to the head. Gaze tracking technique provides us a brand new way of human computer interaction. In addition, eye gaze tracking can be also applied to support seriously disabled people in using computer. In this paper, we explored the use of eye gaze tracking technology on a tablet device, designed and implemented an eye tracking system on an iPad device. In our system, gaze estimation is based on analyzing the appearances of eyes which are retrieved by the built-in camera on the iPad. Artificial neural networks are employed to estimate the location of the user gaze from the eye region image. The results indicate that it is possible to obtain an accuracy of 82.5% with the proposed system. © 2014 IEEE.","artificial neural networks; gaze tracking; region detection; tablet device","Embedded systems; Hand held computers; Human computer interaction; Motion tracking; Neural networks; Real time systems; Design and implementations; Disabled people; Eye gaze tracking; Eye tracking systems; Gaze tracking; Gaze tracking system; Region detection; tablet device; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-84908627791
"Jianfeng L., Shigang L.","54395647600;16202805500;","Eye-model-based gaze estimation by RGB-D camera",2014,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,,"6910042","606","610",,39,"10.1109/CVPRW.2014.93","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908514309&doi=10.1109%2fCVPRW.2014.93&partnerID=40&md5=bc698cfb61cebe1312ff101a5496aa8c","Graduate School of Engineering, Tottori University, Tottori, Japan","Jianfeng, L., Graduate School of Engineering, Tottori University, Tottori, Japan; Shigang, L., Graduate School of Engineering, Tottori University, Tottori, Japan","This paper proposes a method of eye-model-based gaze estimation by RGB-D camera, Kinect sensor. Different from other methods, our method sets up a model to calibrate the eyeball center by gazing at a target in 3D space, not predefined. And then by detecting the pupil center, we can estimate the gaze direction. To achieve this algorithm, we first build a head model relying on Kinect sensor, then obtaining the 3D information of pupil center. As we need to know the eyeball center position in head model, we do a calibration by designing a target to gaze. Because the ray from eyeball center to target and the ray from eyeball center to pupil center should meet a relationship, we can have an equation to solve the real eyeball center position. After calibration, we can have a gaze estimation automatically at any time. Our method allows free head motion and it only needs a simple device, finally it also can run automatically in real-time. Experiments show that our method performs well and still has a room for improvement. © 2014 IEEE.","eyeball calibration; free head motion; gaze estimation; kinect sensor; pupil center detection","Calibration; Cameras; Computer vision; Pattern recognition; 3D information; Free-head; Gaze direction; Gaze estimation; Head model; Kinect sensors; Pupil centers; Rgb-d cameras; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84908514309
"Martin S., Tawari A., Trivedi M.M.","55510668500;36470963300;7103153314;","Balancing privacy and safety: Protecting driver identity in naturalistic driving video data",2014,"AutomotiveUI 2014 - 6th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, in Cooperation with ACM SIGCHI - Proceedings",,,,"","",,2,"10.1145/2667317.2667325","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116180507&doi=10.1145%2f2667317.2667325&partnerID=40&md5=59390e5ac7d1a6d10ff91513ad4252d7","Laboratory of Intelligent and Safe Automobiles, UCSD, San Diego, CA, United States","Martin, S., Laboratory of Intelligent and Safe Automobiles, UCSD, San Diego, CA, United States; Tawari, A., Laboratory of Intelligent and Safe Automobiles, UCSD, San Diego, CA, United States; Trivedi, M.M., Laboratory of Intelligent and Safe Automobiles, UCSD, San Diego, CA, United States","Naturalistic driving dataset is at the heart of automotive user interface research, detecting/measuring driver distraction, and many other driver safety related studies. Recent advances in the collection of large scale naturalistic driving data include the second Strategic Highway Research Program (SHRP2) consisting of more than 3000 subjects and the 100-Car study. Public access to such data, however, is made difficult due to personal identifiable information and protection of privacy. We propose de-identification filters for protecting the privacy of drivers while preserving sufficient details to infer driver behavior, such as the gaze direction, in naturalistic driving videos. Driver's gaze estimation is of particular interest because it is a good indicator of driver's visual attention and a good predictor of driver's intent. We implement and compare de-identification filters, which are made up of a combination of preserving eye regions, superimposing head pose encoded face mask and replacing background with black pixels, and show promising results. Copyright © 2014 ACM.","De-identification; Driver safety; Human factors; Privacy","Behavioral research; Data privacy; Human engineering; Natural language processing systems; De-identification; Driver distractions; Driver identities; Driver safety; Gaze estimation; Protection of privacy; Strategic highway research programs; Visual Attention; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-85116180507
"Sun L., Song M., Liu Z., Sun M.-T.","55535024300;9333722700;7406672531;7403181446;","Realtime gaze estimation with online calibration",2014,"Proceedings - IEEE International Conference on Multimedia and Expo","2014-September","Septmber","6890322","","",,5,"10.1109/ICME.2014.6890322","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937468154&doi=10.1109%2fICME.2014.6890322&partnerID=40&md5=6d96df886d29a65ff0601c312ba09180","Zhejiang University, Hangzhou, 310027, China; Microsoft Research, Redmond, WA  98052, United States; University of Washingtong, Seattle, WA  98195, United States","Sun, L., Zhejiang University, Hangzhou, 310027, China; Song, M., Zhejiang University, Hangzhou, 310027, China; Liu, Z., Microsoft Research, Redmond, WA  98052, United States; Sun, M.-T., University of Washingtong, Seattle, WA  98195, United States","For an eye gaze estimation system, calibration is an unavoidable procedure to determine certain person-specific parameters, either explicitly or implicitly. Although several offline implicit calibration methods have been proposed to ease the calibration burden, the calibration procedure is still cumbersome and the gaze estimation accuracy needs further improvement. In this paper, we propose a novel 3D gaze estimation system with online calibration. The proposed system uses a new 3D model-based gaze estimation method with a single consumer camera (Kinect). Unlike previous gaze estimation methods using explicit offline calibration with fixed number of calibration points or implicit calibration, our approach constantly improves person-specific eye parameters through online calibration, which enables the system to adapt gradually to a new user. The experimental results and the human-computer interaction (HCI) application show that the proposed system can work in realtime with superior gaze estimation accuracy (< 2°) and minimal calibration burden. © 2014 IEEE.","3D model-based; gaze direction; gaze estimation; HCI; online calibration","Gesture recognition; Human computer interaction; Parameter estimation; Social networking (online); 3-d modeling; Calibration points; Calibration procedure; Gaze direction; Gaze estimation; Human computer interaction (HCI); Offline calibrations; On-line calibration; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84937468154
"Wang M., Maeda Y., Takahashi Y.","55701262000;7402845246;55705381800;","Teaching assistant system used eye tracking device based on gaze estimation by neural network and intention recognition by fuzzy inference",2014,"Information (Japan)","17","9B",,"4661","4676",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912567684&partnerID=40&md5=3ea5807b41e290d53651ffbd71956001","Graduate School of Engineering, University of Fukui, Fukui, 910-8507, Japan; Faculty of Engineering, Osaka Institute of Technology, Osaka, 535-8585, Japan","Wang, M., Graduate School of Engineering, University of Fukui, Fukui, 910-8507, Japan; Maeda, Y., Faculty of Engineering, Osaka Institute of Technology, Osaka, 535-8585, Japan; Takahashi, Y., Graduate School of Engineering, University of Fukui, Fukui, 910-8507, Japan","Intention recognition can use multiple factors as inputs such as gestures, face images and eye gaze position. On the other hand, eye tracking technology, with its special advantages of applying to Human-Computer Interaction (HCI), can be utilized to develop assistant systems for people with mobility difficulties. In this paper, we propose gaze estimation position information as input of fuzzy inference to achieve intention recognition based on object recognition and construct an assistant system by using humanoid robot. Our approach is divided into three parts: user's gaze estimation, intention recognition and behavior execution. In gaze estimation part, differing from the previous studies, neural network has been used as the decision making unit, and then gaze position on computer screen is estimated. In intention recognition part, user intention is recognized by using gaze frequency and continuous gaze staying time as input of fuzzy inference after an initial intention region set has been found. At last, by using an autonomous humanoid robot, experiments are performed based on the result of intention recognition. After confirmed by user, the robot was controlled with an assistant task for user precisely. © 2014 International Information Institute.","Eye tracking; Fuzzy inference; Humanoid robot; Intention recognition; Neural network",,Article,"Final","",Scopus,2-s2.0-84912567684
"Shen C., Zhao Q.","53871958600;55743334300;","Learning to predict eye fixations for semantic contents using multi-layer sparse network",2014,"Neurocomputing","138",,,"61","68",,40,"10.1016/j.neucom.2013.09.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899945571&doi=10.1016%2fj.neucom.2013.09.053&partnerID=40&md5=3277228c8f0dcfb176770b839ab4d88e","NUS Graduate School for Integrative Sciences and Engineering (NGS), National University of Singapore, 117456, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, 117576, Singapore","Shen, C., NUS Graduate School for Integrative Sciences and Engineering (NGS), National University of Singapore, 117456, Singapore; Zhao, Q., Department of Electrical and Computer Engineering, National University of Singapore, 117576, Singapore","In this paper, we present a novel model for saliency prediction under a unified framework of feature integration. The model distinguishes itself by directly learning from natural images and automatically incorporating higher-level semantic information in a scalable manner for gaze prediction. Unlike most existing saliency models that rely on specific features or object detectors, our model learns multiple stages of features that mimic the hierarchical organization of the ventral stream in the visual cortex and integrate them by adapting their weights based on the ground-truth fixation data. To accomplish this, we utilize a multi-layer sparse network to learn low-, mid- and high-level features from natural images and train a linear support vector machine (SVM) for weight adaption and feature integration. Experimental results show that our model could learn high-level semantic features like faces and texts and can perform competitively among existing approaches in predicting eye fixations. © 2014 Elsevier B.V.","Deep learning; Gaze prediction; Semantic saliency; Sparse coding","Image retrieval; Semantics; Support vector machines; Deep learning; Feature integration; Hierarchical organizations; High-level features; High-level semantic features; Linear Support Vector Machines; Semantic information; Sparse coding; Forecasting; article; eye fixation; eye tracking; learning theory; mathematical model; multilayer sparse network; prediction; priority journal; saliency model; semantics; support vector machine; system analysis",Article,"Final","",Scopus,2-s2.0-84899945571
"Li Y., Monaghan D.S., O'Connor N.E.","55920633500;9246396700;7103192405;","Real-time gaze estimation using a kinect and a HD webcam",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8325 LNCS","PART 1",,"506","517",,8,"10.1007/978-3-319-04114-8_43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893505240&doi=10.1007%2f978-3-319-04114-8_43&partnerID=40&md5=fcd27c998bea96281a3a6f90b35b915a","CLARITY: Center for Sensor Web Technology, Dublin City University, Ireland","Li, Y., CLARITY: Center for Sensor Web Technology, Dublin City University, Ireland; Monaghan, D.S., CLARITY: Center for Sensor Web Technology, Dublin City University, Ireland; O'Connor, N.E., CLARITY: Center for Sensor Web Technology, Dublin City University, Ireland","In human-computer interaction, gaze orientation is an important and promising source of information to demonstrate the attention and focus of users. Gaze detection can also be an extremely useful metric for analysing human mood and affect. Furthermore, gaze can be used as an input method for human-computer interaction. However, currently real-time and accurate gaze estimation is still an open problem. In this paper, we propose a simple and novel estimation model of the real-time gaze direction of a user on a computer screen. This method utilises cheap capturing devices, a HD webcam and a Microsoft Kinect. We consider that the gaze motion from a user facing forwards is composed of the local gaze motion shifted by eye motion and the global gaze motion driven by face motion. We validate our proposed model of gaze estimation and provide experimental evaluation of the reliability and the precision of the method. © 2014 Springer International Publishing.","Eye tracking; Gaze estimation; Gaze tracking; Kinect","Computer screens; Estimation models; Experimental evaluation; Eye-tracking; Gaze estimation; Gaze tracking; Kinect; Microsoft kinect; Artificial intelligence; Computer science; Computers; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84893505240
"Topal C., Gunal S., Kocdeviren O., Dogan A., Nezih Gerek O.","16231520700;24449964400;56025076700;7101805529;6701450919;","A low-computational approach on gaze estimation with eye touch system",2014,"IEEE Transactions on Cybernetics","44","2","6517478","228","239",,15,"10.1109/TCYB.2013.2252792","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893377697&doi=10.1109%2fTCYB.2013.2252792&partnerID=40&md5=e37df66e6b0831a3a15ce3ac8b7eeb61","Anadolu University, Eskisehir, Turkey","Topal, C., Anadolu University, Eskisehir, Turkey; Gunal, S., Anadolu University, Eskisehir, Turkey; Kocdeviren, O., Anadolu University, Eskisehir, Turkey; Dogan, A., Anadolu University, Eskisehir, Turkey; Nezih Gerek, O., Anadolu University, Eskisehir, Turkey","Among various approaches to eye tracking systems, light-reflection based systems with non-imaging sensors, e.g., photodiodes or phototransistors, are known to have relatively low complexity; yet, they provide moderately accurate estimation of the point of gaze. In this paper, a low-computational approach on gaze estimation is proposed using the Eye Touch system, which is a light-reflection based eye tracking system, previously introduced by the authors. Based on the physical implementation of Eye Touch, the sensor measurements are now utilized in low-computational least-squares algorithms to estimate arbitrary gaze directions, unlike the existing light reflection-based systems, including the initial Eye Touch implementation, where only limited predefined regions were distinguished. The system also utilizes an effective pattern classification algorithm to be able to perform left, right, and double clicks based on respective eye winks with significantly high accuracy. In order to avoid accuracy problems for sensitive sensor biasing hardware, a robust custom microcontroller-based data acquisition system is developed. Consequently, the physical size and cost of the overall Eye Touch system are considerably reduced while the power efficiency is improved. The results of the experimental analysis over numerous subjects clearly indicate that the proposed eye tracking system can classify eye winks with 98% accuracy, and attain an accurate gaze direction with an average angular error of about 0.93°. Due to its lightweight structure, competitive accuracy and low-computational requirements relative to video-based eye tracking systems, the proposed system is a promising human-computer interface for both stationary and mobile eye tracking applications. © 2013 IEEE.","Assistive technology; eye tracking; gaze estimation; human-computer interface","Assistive technology; Classification algorithm; Data acquisition system; Experimental analysis; Eye-tracking; Gaze estimation; Human computer interfaces; Least-squares algorithms; Algorithms; Computational methods; Sensors; Tracking (position); Light reflection; algorithm; article; automated pattern recognition; equipment; equipment design; equipment failure; eye fixation; human; methodology; photometry; physiology; reproducibility; sensitivity and specificity; visual field; Algorithms; Equipment Design; Equipment Failure Analysis; Fixation, Ocular; Humans; Pattern Recognition, Automated; Photometry; Reproducibility of Results; Sensitivity and Specificity; Visual Fields",Article,"Final","",Scopus,2-s2.0-84893377697
"Huang Y., Duan D., Cui J., Davoine F., Wang L., Zha H.","56134745500;56097162900;15623000600;6603967034;57044350200;7006639394;","Joint estimation of head pose and visual focus of attention",2014,"2014 IEEE International Conference on Image Processing, ICIP 2014",,,"7025674","3332","3336",,2,"10.1109/ICIP.2014.7025674","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949927801&doi=10.1109%2fICIP.2014.7025674&partnerID=40&md5=6d753e402c1b6f0dbce8279fbf107efc","Key Lab. of Machine Perception (MoE), School of EECS, China; Psychology Department, Peking University, Beijing, China; CNRS, LIAMA Sino-European Lab., Beijing, China","Huang, Y., Key Lab. of Machine Perception (MoE), School of EECS, China; Duan, D., Key Lab. of Machine Perception (MoE), School of EECS, China; Cui, J., Key Lab. of Machine Perception (MoE), School of EECS, China; Davoine, F., CNRS, LIAMA Sino-European Lab., Beijing, China; Wang, L., Psychology Department, Peking University, Beijing, China; Zha, H., Key Lab. of Machine Perception (MoE), School of EECS, China","Head pose is an important indicator of a person's visual focus of attention (VFoA). A traditional way to recognize VFoA is to consider accurate head pose or gaze estimations. However, these estimations usually degrade drastically in middle or low resolution video data. In this paper, a joint estimation of head pose and VFoA is proposed to address this issue; both head pose and VFoA are iteratively refined until convergence. This approach is evaluated in a specific scenario involving children around a table playing together with toys. Datasets are acquired and annotated by psychologists in Peking university. The experimental results demonstrate the usefulness of the join estimation process to recognize visual focus of attention in middle resolution video sequences. © 2014 IEEE.","Head pose estimation; joint estimation; low resolution; visual focus of attention","Image processing; Image recognition; Video recording; Estimation process; Head Pose Estimation; Joint estimation; Low resolution; Low resolution video; Peking University; Resolution video; Visual focus of attentions; Motion estimation",Conference Paper,"Final","",Scopus,2-s2.0-84949927801
"Choi J., Ahn B., Park J., Kweon I.S.","56126520100;55383022000;54793418600;7003450602;","GMM-based saliency aggregation for calibration-free gaze estimation",2014,"2014 IEEE International Conference on Image Processing, ICIP 2014",,,"7025218","1096","1099",,2,"10.1109/ICIP.2014.7025218","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949926527&doi=10.1109%2fICIP.2014.7025218&partnerID=40&md5=09d33c9dbe0ee9dcc32e04feecc6765d","Korea Advanced Institute of Science and Technology, Daejeon, South Korea","Choi, J., Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Ahn, B., Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Park, J., Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Kweon, I.S., Korea Advanced Institute of Science and Technology, Daejeon, South Korea","A typical gaze estimator needs an explicit personal calibration stage with many discrete fixation points. This limitation can be resolved by mapping multiple eye images and corresponding saliency maps of a video clip during an implicit calibration stage. Compared to previous calibration-free methods, our approach clusters eye images by using Gaussian Mixture Model (GMM) in order to increase calibration accuracy and reduce training redundancy. Eye feature vectors representing eye images undergo soft clustering with GMM as well as the corresponding saliency maps for aggregation. The GMM based soft-clustering boosts the accuracy of Gaussian process regression which maps between eye feature vectors and gaze directions given this constructed data. The experimental results show an increase in gaze estimation accuracy compared to previous works on calibration-free method. © 2014 IEEE.","Gaussian mixture model; Gaussian process regression; Gaze estimation; Saliency","Gaussian distribution; Gaussian noise (electronic); Image processing; Image segmentation; Calibration accuracy; Calibration free; Calibration-free methods; Feature vectors; Gaussian Mixture Model; Gaussian process regression; Gaze estimation; Saliency; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84949926527
"Wang X., Xue K., Nam D., Han J., Wang H.","56177151300;35189758500;43761314400;55646340200;56316180500;","Hierarchical gaze estimation based on adaptive feature learning",2014,"2014 IEEE International Conference on Image Processing, ICIP 2014",,,"7025677","3347","3351",,7,"10.1109/ICIP.2014.7025677","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949926439&doi=10.1109%2fICIP.2014.7025677&partnerID=40&md5=111e668e1867f6a408b1094bdf75162f","Samsung RandD Institute China, Beijing, China; Samsung Advanced Institute of Technology, Yongin, South Korea","Wang, X., Samsung RandD Institute China, Beijing, China; Xue, K., Samsung RandD Institute China, Beijing, China; Nam, D., Samsung Advanced Institute of Technology, Yongin, South Korea; Han, J., Samsung Advanced Institute of Technology, Yongin, South Korea; Wang, H., Samsung RandD Institute China, Beijing, China","Existing appearance-based gaze estimation methods suffer from tedious calibration and appearance variation caused by head movement. In this paper, to handle this problem, we propose a novel appearance-based gaze estimation method by introducing supervised adaptive feature extraction and hierarchical mapping model. Firstly, an adaptive feature learning method is proposed to extract topology-preserving (TOP) feature individually. Then hierarchical mapping method is proposed to localize gaze position based on coarse-to-fine strategy. Appearance synthesis approach is used to increase the refer sample density. Experiments show that under the condition of sparse calibration, proposed method has better performance in accuracy than existing methods under fixed head pose without chinrest. Moreover, our method can be easily extended for head pose-varying gaze estimation. © 2014 IEEE.","appearance synthesis; feature learning; gaze estimation; hierarchical mapping","Feature extraction; Image processing; Mapping; Adaptive feature extractions; Appearance synthesis; Better performance; Coarse-to-fine strategy; Feature learning; Gaze estimation; Hierarchical mapping; Topology preserving; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84949926439
"Stengel M., Grogorick S., Rogge L., Magnor M.","42162165500;57063513000;36242298500;57191188427;","A nonobscuring eye tracking solution for wide field-of-view head-mounted displays",2014,"Computer Graphics Forum","33","2",,"7","8",,1,"10.2312/egp.20141063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991107988&doi=10.2312%2fegp.20141063&partnerID=40&md5=a8e37f2b133659d9f2d2c71c897c5d8e","Institut für Computergraphik, TU Braunschweig, Germany","Stengel, M., Institut für Computergraphik, TU Braunschweig, Germany; Grogorick, S., Institut für Computergraphik, TU Braunschweig, Germany; Rogge, L., Institut für Computergraphik, TU Braunschweig, Germany; Magnor, M., Institut für Computergraphik, TU Braunschweig, Germany","We present a solution for integrating a binocular eye tracker into current state-of-the-art lens-based head-mounted displays (HMDs) without affecting the available field-of-view on the display. Estimating the relative eye gaze of the user opens the door for HMDs to a much wider spectrum of virtual reality applications and games. Further, we present a concept of a low-cost head-mounted display with eye tracking and discuss applications which strongly depend on or benefit from gaze estimation. © The Eurographics Association 2014.",,"Computer graphics; Sensory perception; Street traffic control; Virtual reality; Eye trackers; Eye-tracking; Field of views; Gaze estimation; Head mounted displays; Low costs; State of the art; Wide field of view; Helmet mounted displays",Conference Paper,"Final","",Scopus,2-s2.0-84991107988
"Ishac K., Rozado D.","56926754000;36609200700;","Low cost human-robot gaze estimation system",2014,"Proceedings of the 26th Australian Computer-Human Interaction Conference, OzCHI 2014",,,,"404","413",,1,"10.1145/2686612.2686674","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945427734&doi=10.1145%2f2686612.2686674&partnerID=40&md5=fd491ea4762aa2e96daff094125fe195","Lab of Artificial Intelligence, University of Tsukuba, 1-1-1 Tennodai, Tsukuba, Ibaraki, 305-0006, Japan; Autonomous Systems Lab, Digital Productivity Flagship - CSIRO, CSIRO, 15 College Rd, Sandy Bay, TAS 7005, Australia","Ishac, K., Lab of Artificial Intelligence, University of Tsukuba, 1-1-1 Tennodai, Tsukuba, Ibaraki, 305-0006, Japan; Rozado, D., Autonomous Systems Lab, Digital Productivity Flagship - CSIRO, CSIRO, 15 College Rd, Sandy Bay, TAS 7005, Australia","This work presents the development of a low cost Human-Robot gaze estimation system for the purpose of promoting joint Human-Robot workspaces in daily scenarios. We have developed this system using only monocular eye tracking and the 2D gaze point. Prior to this work there have been many efforts to bridge the gap between human and robots by developing robotic assistants capable of serving humans through command based interaction or manual input. Such systems lack freedom and require constant input whenever interaction is required. The system we have developed allows for both seated and free roaming interaction between a human and the robot. The end software allows the robot to track the gaze point of the human in real-time and fixate on the same point in space. Once the robot is stabilized on this gaze point it may perform a number of different tasks which may assist the human. We have constructed our own pair of gaze tracking glasses using only low cost components to demonstrate effective performance under budget costs. These glasses are coupled with a pan/tilt laser robot which tracks the gaze point of the human on the environment by projecting a red laser. We have designed 2 separate subsystems which track the pose o the user's head in real time and then combine to give an accurate estimate relative to the robot. As well as this we have developed a gaze tracking module which calibrates the gaze point of the human to a relative gaze window on the robot. Data from all 3 subsystems are then combined through a data fusion model and then sent to the robot to adjust its pan/tilt angles to focus on the same point in space as the human. These subsystems combine to provide an accuracy of 94% to the centre of a target object. The system was tested through a user study which involved 12 subjects undergoing 5 different testing scenarios. Copyright 2014 ACM.","Assistant Robot; Eye tracking; Eye tracking glasses; Gaze point; Gaze tracking; HCI; Head mounted tracker; HRI; Joint workspace; Mobile interaction","Budget control; Cost estimating; Costs; Data fusion; Gesture recognition; Glass; Human computer interaction; Interactive computer systems; Robots; Tracking (position); Assistant robot; Eye-tracking; Gaze point; Gaze tracking; Head mounted tracker; HRI; Mobile interaction; Human robot interaction",Conference Paper,"Final","",Scopus,2-s2.0-84945427734
"Rozado D., Stephen L., Kottege N.","36609200700;56926381300;22034958500;","Interacting with objects in the environment using gaze tracking glasses and speech",2014,"Proceedings of the 26th Australian Computer-Human Interaction Conference, OzCHI 2014",,,,"414","417",,1,"10.1145/2686612.2686676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945347787&doi=10.1145%2f2686612.2686676&partnerID=40&md5=dacfbd0b4c34f1101feee89125e05237","Autonomous Systems Lab, Digital Productivity Flagship - CSIRO, CSIRO, 15 College Rd, Sandy Bay, TAS  7005, Australia; Department of Electrical Engineering, University of Queensland, Australia; Autonomous Systems Lab, Digital Productivity Flagship - CSIRO, CSIRO, 1 Technology CourtQLD  4069, Australia","Rozado, D., Autonomous Systems Lab, Digital Productivity Flagship - CSIRO, CSIRO, 15 College Rd, Sandy Bay, TAS  7005, Australia; Stephen, L., Department of Electrical Engineering, University of Queensland, Australia; Kottege, N., Autonomous Systems Lab, Digital Productivity Flagship - CSIRO, CSIRO, 1 Technology CourtQLD  4069, Australia","This work explores the combination of gaze and speech to interact with objects in the environment. A head-mounted wireless gaze tracker in the form of gaze tracking glasses is used for mobile monitoring of a subject's point of regard on the surrounding environment. In our proposed system, a mobile subject gazes at an object of interest in the environment which opens an interaction window with the object being gazed upon, and a specific interaction command is then given to the object using speech commands. The gaze tracking glasses were made from low-cost hardware consisting of a safety glasses' frame and wireless eye tracking and scene cameras. An open source gaze estimation algorithm is used for eye tracking and user's gaze estimation. The Windows Speech Recognition engine is used for recognition of voice commands. A visual markers recognition library is used to identify objects in the environment through the scene camera. When combining all these elements, the emerging system permits a subject to move freely in an environment, select the object he wants to interact with using gaze (identification) and transmit a control command to it by uttering a speech command (control). Copyright 2014 ACM.","Gaze interaction; Gaze tracking; Gaze tracking glasses; HCI; Mobile interaction; Speech interaction","Cameras; Glass; Human computer interaction; Interactive computer systems; Patient monitoring; Safety glass; Speech; Tracking (position); Gaze interaction; Gaze tracking; Mobile interaction; Mobile monitoring; Specific interaction; Speech interaction; Speech recognition engine; Surrounding environment; Speech recognition",Conference Paper,"Final","",Scopus,2-s2.0-84945347787
[无可用作者姓名],[无可用的作者 ID],"1st International Workshop on Video Analytics for Audience Measurement, VAAM 2014",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8811",,,"1","158",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945192642&partnerID=40&md5=584e231648b53e50d66c36284934b0a9",,"","The proceedings contain 14 papers. The special focus in this conference is on Demographics and Modelling Consumer Behaviour. The topics include: The applications of video analytics in media planning, trade and shopper marketing; pervasive retail strategy using a low-cost free gaze estimation system; face re-identification for digital signage applications; evaluation of LBP and hog descriptors for clothing attribute description; features descriptors for demographic estimation; with test results on gender classification task; multi-view face detection with one classifier for video analytics systems; online audience measurement system based on machine learning techniques; modelling in-store consumer behaviour using machine learning and digital signage audience measurement data; shopper behaviour analysis based on 3D situation awareness information and customer activity recognition system using a distributed RGB-D camera network.",,,Conference Review,"Final","",Scopus,2-s2.0-84945192642
"Yan M., Tamura H., Tanno K.","55515780300;35600682500;7103288835;","A study on gaze estimation system using cross-channels electrooculogram signals",2014,"Lecture Notes in Engineering and Computer Science","2209","January",,"112","116",,7,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938255137&partnerID=40&md5=7f606d1d1e022c89bfdf22fe8ed14f40","Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazaki, Japan; Department of Electrical and Electronic Engineering, University of Miyazaki, Miyazaki, Japan","Yan, M., Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazaki, Japan; Tamura, H., Department of Electrical and Electronic Engineering, University of Miyazaki, Miyazaki, Japan; Tanno, K., Department of Electrical and Electronic Engineering, University of Miyazaki, Miyazaki, Japan","The aim of this study is to present electrooculogram signals that can be used as a human computer interface efficiently. Establishing an efficient alternative channel for communication without overt speech and hand movements is important for increasing the quality of life for patients suffering from Amyotrophic Lateral Sclerosis or other illnesses that prevent correct limb and facial muscular responses. In this paper, we introduce gaze estimation system of 24 Angles using cross-channels electrooculogram signals. This system records electrooculogram signals when the patients focused on each direct. All these recorded signals could be analyzed using the fuzzy mathematical model will be set up. From simulation results, the 24 pattern of eyeball movements is recognized under an accuracy of 87% in our system.","Cross-channels; Electrooculogram signal; Fuzzy mathematical model; Gaze estimation","Estimation; Gesture recognition; Speech communication; Amyotrophic lateral sclerosis; Cross-channels; Electro-oculogram; Eyeball movements; Gaze estimation; Human computer interfaces; Quality of life; Recorded signals; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84938255137
"Khorrami P., Le V., Hart J.C., Huang T.S.","56179613400;35076170200;55243117900;35513984600;","A system for monitoring the engagement of remote online students using eye gaze estimation",2014,"2014 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2014",,,"6890573","","",,7,"10.1109/ICMEW.2014.6890573","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937124426&doi=10.1109%2fICMEW.2014.6890573&partnerID=40&md5=ff9cabe5867008a674383c8074d1fb92","Beckman Institute, University of Illinois at Urbana Champaign, United States; Department of Computer Science, University of Illinois at Urbana Champaign, United States","Khorrami, P., Beckman Institute, University of Illinois at Urbana Champaign, United States; Le, V., Beckman Institute, University of Illinois at Urbana Champaign, United States; Hart, J.C., Department of Computer Science, University of Illinois at Urbana Champaign, United States; Huang, T.S., Beckman Institute, University of Illinois at Urbana Champaign, United States","We present a system for monitoring the engagement of remote students in an online educational environment. We introduce a novel gaze estimation system that uses a client-side video-based 3D face tracking algorithm, and expands it to estimate the head pose, and intersects this pose-based gaze line of sight with the screen (coplanar with the camera) to estimate the screen position currently viewed. Our preliminary experiments indicate this approach can accurately determine the coarse location of a student's gaze and use it as a cue for engagement estimation. Our system operates in real-time and achieves comparable performance to state-of-the-art gaze estimation techniques that account for free head motion. © 2014 IEEE.","Face Tracking; Gaze Estimation; Head Pose Estimation; Student Monitoring System","Image recognition; Monitoring; Motion estimation; Social networking (online); Students; Three dimensional computer graphics; 3D face tracking; Educational environment; Face Tracking; Gaze estimation; Head Pose Estimation; Remote students; State of the art; Student monitoring; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84937124426
"Tawari A., Møgelmose A., Martin S., Moeslund T.B., Trivedi M.M.","36470963300;55416349300;55510668500;6507267791;7103153314;","Attention estimation by simultaneous analysis of viewer and view",2014,"2014 17th IEEE International Conference on Intelligent Transportation Systems, ITSC 2014",,,"6957880","1381","1387",,20,"10.1109/ITSC.2014.6957880","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937117486&doi=10.1109%2fITSC.2014.6957880&partnerID=40&md5=a2eb325e5bd2b1a0f27500ac9ad1dbe0","Laboratory for Intelligent and Safe Automobiles, UC San Diego, United States; Visual Analysis of People Lab, Aalborg University, Denmark","Tawari, A., Laboratory for Intelligent and Safe Automobiles, UC San Diego, United States; Møgelmose, A., Laboratory for Intelligent and Safe Automobiles, UC San Diego, United States, Visual Analysis of People Lab, Aalborg University, Denmark; Martin, S., Laboratory for Intelligent and Safe Automobiles, UC San Diego, United States; Moeslund, T.B., Visual Analysis of People Lab, Aalborg University, Denmark; Trivedi, M.M., Laboratory for Intelligent and Safe Automobiles, UC San Diego, United States","This paper introduces a system for estimating the attention of a driver wearing a first person view camera using salient objects to improve gaze estimation. A challenging data set of pedestrians crossing intersections has been captured using Google Glass worn by a driver. A challenge unique to first person view from cars is that the interior of the car can take up a large part of the image. The proposed system automatically filters out the dashboard of the car, along with other parts of the instrumentation. The remaining area is used as a region of interest for a pedestrian detector. Two cameras looking at the driver are used to determine the direction of the driver's gaze, by examining the eye corners and the center of the iris. This coarse gaze estimation is then linked to the detected pedestrians to determine which pedestrian the driver is focused on at any given time. © 2014 IEEE.",,"Image segmentation; Intelligent systems; Transportation; Attention estimations; Crossing intersections; Eye corners; First person; Gaze estimation; Region of interest; Salient objects; Simultaneous analysis; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-84937117486
"Li L., Werber K., Calvillo C.F., Dinh K.D., Guarde A., König A.","56591432400;56592693900;57204698561;56592384200;56593009300;57222565420;","Multi-sensor soft-computing system for driver drowsiness detection",2014,"Advances in Intelligent Systems and Computing","223",,,"129","140",,9,"10.1007/978-3-319-00930-8_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927609932&doi=10.1007%2f978-3-319-00930-8_12&partnerID=40&md5=f31cb5f59a67d647cff3b425eddc3bac","TU Kaiserslautern, Department of Electrical and Computer Engineering, Institute of Integrated Sensor Systems, Erwin-Schrödinger-Str. Gebäude 12, Kaiserslautern, 67663, Germany; University of the Basque Country, Bilbao, Spain","Li, L., TU Kaiserslautern, Department of Electrical and Computer Engineering, Institute of Integrated Sensor Systems, Erwin-Schrödinger-Str. Gebäude 12, Kaiserslautern, 67663, Germany; Werber, K., TU Kaiserslautern, Department of Electrical and Computer Engineering, Institute of Integrated Sensor Systems, Erwin-Schrödinger-Str. Gebäude 12, Kaiserslautern, 67663, Germany; Calvillo, C.F., TU Kaiserslautern, Department of Electrical and Computer Engineering, Institute of Integrated Sensor Systems, Erwin-Schrödinger-Str. Gebäude 12, Kaiserslautern, 67663, Germany; Dinh, K.D., TU Kaiserslautern, Department of Electrical and Computer Engineering, Institute of Integrated Sensor Systems, Erwin-Schrödinger-Str. Gebäude 12, Kaiserslautern, 67663, Germany; Guarde, A., University of the Basque Country, Bilbao, Spain; König, A., TU Kaiserslautern, Department of Electrical and Computer Engineering, Institute of Integrated Sensor Systems, Erwin-Schrödinger-Str. Gebäude 12, Kaiserslautern, 67663, Germany","Advanced sensing systems, sophisticated algorithms and increasing computational resources continuously enhance active safety technology for vehicles. Driver status monitoring belongs to the key components of advanced driver assistance system which is capable of improving car and road safety without compromising driving experience. This paper presents a novel approach to driver status monitoring aimed at drowsiness detection based on depth camera, pulse rate sensor and steering angle sensor. Due to NIR active illumination depth camera can provide reliable head movement information in 3D alongside eye gaze estimation and blink detection in a non-intrusive manner. Multi-sensor data fusion on feature level and multilayer neural network facilitate the classification of driver drowsiness level based on which a warning can be issued to prevent traffic accidents. The presented approach is implemented on an integrated soft-computing system for driving simulation (DeCaDrive) with multi-sensing interfaces. The classification accuracy of 98:9% for up to three drowsiness levels has been achieved based on data sets of five test subjects with 588-min driving sequence. © Springer International Publishing Switzerland 2014.",,"Algorithms; Automobile drivers; Automobile safety devices; Cameras; Classification (of information); Data fusion; Eye movements; Motor transportation; Multilayer neural networks; Sensor data fusion; Social networking (online); Soft computing; World Wide Web; Active illumination; Car and road safety; Classification accuracy; Computational resources; Driving experiences; Drowsiness detection; Multisensor data fusion; Steering-angle sensor; Advanced driver assistance systems",Conference Paper,"Final","",Scopus,2-s2.0-84927609932
"Frigerio E., Marcon M.","54995996800;7004432686;","Iris image correction method from unconstrained images",2014,"Lecture Notes in Computational Vision and Biomechanics","15",,,"201","224",,1,"10.1007/978-3-319-04039-4_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927563222&doi=10.1007%2f978-3-319-04039-4_12&partnerID=40&md5=68e663e31612c27e69158d69e545e414","ISPG, DEIB, Politecnico di Milano, P.zza Leonardo Da Vinci, 32, Milano, 20133, Italy","Frigerio, E., ISPG, DEIB, Politecnico di Milano, P.zza Leonardo Da Vinci, 32, Milano, 20133, Italy; Marcon, M., ISPG, DEIB, Politecnico di Milano, P.zza Leonardo Da Vinci, 32, Milano, 20133, Italy","The use of iris as biometric trait has emerged as one of the most preferred method because of its uniqueness, lifetime stability and regular shape. Moreover it shows public acceptance and new user-friendly capture devices are developed and used in a broadened range of applications. Currently, iris recognition systems work well with frontal iris images from cooperative users. Nonideal iris images are still a challenge for iris recognition and can significantly affect the accuracy of iris recognition systems. Moreover, accurate localization of different eye’s parts from videos or still images is a crucial step in many image processing applications that range from iris recognition in Biometrics to gaze estimation for Human Computer Interaction (HCI), impaired people aid or, even, marketing analysis for products attractiveness. Notwithstanding this, actually, most of available implementations for eye’s parts segmentation are quite invasive, imposing a set of constraints both on the environment and on the user itself limiting their applicability to high security Biometrics or to cumbersome interfaces. In the first part of this Chapter, we propose a novel approach to segment the sclera, the white part of the eye. We concentrate on this area since, thanks to the dissimilarity with other eye’s parts, its identification can be performed in a robust way against light variations, reflections and glasses lens flare. An accurate sclera segmentation is a fundamental step in iris and pupil localization, even in non-frontal noisy images. Furthermore its particular geometry can be fruitfully used for accurate eyeball rotation estimation. The proposed technique is based on a statistical approach (supported by some heuristic assumptions) to extract discriminating descriptors for sclera and non-sclera pixels. A Support Vector Machine (SVM) is then used as a final supervised classifier. Once the eyeball rotation angle respect to the camera optical axis is estimated and the limbus (the boundary between the iris and the sclera) is extracted, we propose amethod to correct off-angle iris image. Taking into account the eye morphology and the reflectance properties of the external transparent layers, we can evaluate the distorting effects that are present in the acquired image. The correction algorithm proposed includes a first modeling phase of the human eye and a simulation phase where the acquisition geometry is reproduced and the distortions are evaluated. Finally we obtain an image which does not contain the distorting effects due to jumps in the refractive index. We show how this correction process reduces the intra-class variations for off-angle iris images. © Springer International Publishing Switzerland 2014.","3D eyemodel; Eikonal equation; Gaze estimation; Iris correction; Sclera segmentation",,Article,"Final","",Scopus,2-s2.0-84927563222
"Wísniewska J., Rezaei M., Klette R.","56494674700;57191998350;7003908797;","Robust eye gaze estimation",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8671",,,"636","644",,4,"10.1007/978-3-319-11331-9_76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921816704&doi=10.1007%2f978-3-319-11331-9_76&partnerID=40&md5=3046b110699d6cc83e5c9e811adca440","Warsaw University of Technology, Plac Politechniki 1, Warsaw, 00-661, Poland; The University of Auckland, Department of Computer Science, Private Bag 92019, Auckland, 1142, New Zealand","Wísniewska, J., Warsaw University of Technology, Plac Politechniki 1, Warsaw, 00-661, Poland; Rezaei, M., The University of Auckland, Department of Computer Science, Private Bag 92019, Auckland, 1142, New Zealand; Klette, R., The University of Auckland, Department of Computer Science, Private Bag 92019, Auckland, 1142, New Zealand","Eye gaze detection under challenging lighting conditions is a non-trivial task. Pixel intensity and the shades around the eye region may change depending on the time of day, location, or due to artificial lighting. This paper introduces a lighting-adaptive solution for robust eye gaze detection. First, we propose a binarization and cropping technique to limit our region of interest. Then we develop a gradient-based method for eye-pupil detection; and finally, we introduce an adaptive eye-corner detection technique that altogether lead to robust eye gaze estimation. Experimental results show the outperformance of the proposed method compared with related techniques. © Springer International Publishing Switzerland 2014.",,"Edge detection; Image segmentation; Lighting; Chemical detection; Computer vision; Adaptive solution; Artificial lighting; Eye gaze detection; Eye pupil detections; Gradient-based method; Lighting conditions; Non-trivial tasks; Region of interest; Chemical detection; Edge detection",Article,"Final","",Scopus,2-s2.0-84921816704
"Cazzato D., Leo M., Spagnolo P., Distante C.","55866556300;7006471658;7006569945;55884135100;","Pervasive retail strategy using a low-cost free gaze estimation system",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8811",,,"23","39",,4,"10.1007/978-3-319-12811-5_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921748076&doi=10.1007%2f978-3-319-12811-5_2&partnerID=40&md5=cb5df74f94074853fd1126fe7040311b","National Research Council of Italy - Institute of Optics, Arnesano, LE, Italy","Cazzato, D., National Research Council of Italy - Institute of Optics, Arnesano, LE, Italy; Leo, M., National Research Council of Italy - Institute of Optics, Arnesano, LE, Italy; Spagnolo, P., National Research Council of Italy - Institute of Optics, Arnesano, LE, Italy; Distante, C., National Research Council of Italy - Institute of Optics, Arnesano, LE, Italy","This paper proposes a pervasive retail architecture based on a free human gaze estimation system. The main aim of the paper is to investigate the possibility to automatically understand the behavior of the persons looking at a shop window: this is done by a gaze estimation technique that uses a RGB-D device in order to extract head pose information from which a fast geometric technique then evaluates the focus of attention of the persons in the scene (even more persons at the same time). The main contribution concerns with the application into this challenging research field of a gaze estimation working without any initial calibration and, in spite of this, able to properly deal with completely unaware persons moving in unconstrained environments. Preliminary experiments were conducted in our lab in order to quantitative validate the accuracy of the gaze estimation on different benchmarks of persons. Then the qualitative evaluation of the effectiveness of the proposed architecture was conducted in a real shop window demonstrating the ability to deal with real challenging environmental conditions. © Springer International Publishing Switzerland 2014.","Digital signage; Focus of attention; Free gaze estimation; Human-computer interaction; Pervasive retail","Human computer interaction; Sales; Costs; Digital signage; Environmental conditions; Focus of Attention; Gaze estimation; Pervasive retail; Proposed architectures; Qualitative evaluations; Unconstrained environments; Geometric techniques; Gesture recognition",Article,"Final","",Scopus,2-s2.0-84921748076
"Wojciechowski A., Fornalczyk K.","15021546300;56495242800;","Exponentially smoothed interactive gaze tracking method",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8671",,,"645","652",,8,"10.1007/978-3-319-11331-9_77","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921714502&doi=10.1007%2f978-3-319-11331-9_77&partnerID=40&md5=bb085ba8aa4a9391181c2817ccafbc38","Institute of Information Technology, Technical University of Łódź, Poland; BinarApps, Łódź, Poland","Wojciechowski, A., Institute of Information Technology, Technical University of Łódź, Poland; Fornalczyk, K., Institute of Information Technology, Technical University of Łódź, Poland, BinarApps, Łódź, Poland","Gaze tracking is an aspect of human-computer interaction still growing in popularity. Tracking human eye fixation points can help control user interfaces and eventually may help in the interface evaluation or optimization. Unfortunately professional eye-trackers are very expensive and thus hardly available for researchers and small companies. The paper presents very effective, exponentially smoothed, low cost, appearance based, improved gaze tracking method. The method achieves very high absolute precision (1 deg) at 20 fps, exploiting a simple HD web camera with reasonable environment restrictions. The paper describes results of experimental tests, both static on absolute gaze point estimation, and dynamic on gaze controlled path following. © Springer International Publishing Switzerland 2014.",,"Human computer interaction; User interfaces; Computer vision; Tracking (position); Appearance based; Experimental test; Eye trackers; Gaze point estimations; Gaze tracking; Interface evaluation; Path following; Small companies; Eye tracking; Human computer interaction",Article,"Final","",Scopus,2-s2.0-84921714502
"Tarayan E., Höffken M., Herta A.S., Kressel U.","56307946400;35147786200;56442599400;6602794359;","Iris and pupil measurement on low resolution images for driver observation",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8888",,,"240","249",,,"10.1007/978-3-319-14364-4_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916608703&doi=10.1007%2f978-3-319-14364-4_23&partnerID=40&md5=5b537892a6e4f8d2a8cc98e87841cb83","Institute of Biomedical Engineering and Informatics, Ilmenau University of Technology, Ilmenau, Germany; Institute of Measurement, Control and Microtechnology, Ulm University, Ulm, Germany; Research and Development, Team Driver Analysis, Daimler AG, Ulm, Germany","Tarayan, E., Institute of Biomedical Engineering and Informatics, Ilmenau University of Technology, Ilmenau, Germany; Höffken, M., Institute of Measurement, Control and Microtechnology, Ulm University, Ulm, Germany; Herta, A.S., Research and Development, Team Driver Analysis, Daimler AG, Ulm, Germany; Kressel, U., Research and Development, Team Driver Analysis, Daimler AG, Ulm, Germany","Given the fact that the eyes are one of the most important clues for the driver’s attention in cases of traffic accidents, we propose a robust method for accurate iris and pupil detection and parameter estimation. The latter can be used to infer the eye gaze. The presented video-based system is calibration-free and does not require any training procedure. We implemented and adapted an eye feature extraction technique using computer-vision methods and estimated pupil and iris parameters with the help of a polar representation of the eye image. In order to detect the glints (reflections on the eye cornea), caused by the infrared illumination mounted on the camera, we apply a corner detection algorithm. The experiment results show high applicability of the presented eye feature extraction on low resolution images. © Springer International Publishing Switzerland 2014.","Corneal reflections; Driver’s gaze estimation; Eye feature extraction; Pupil and iris segmentation","Automobile drivers; Edge detection; Extraction; Feature extraction; Image processing; Corneal reflection; Eye feature extraction; Gaze estimation; Infrared illumination; Iris segmentation; Low resolution images; Polar representation; Video-based systems; Parameter estimation",Conference Paper,"Final","",Scopus,2-s2.0-84916608703
"Zhu R., Sang G., Gao W., Zhao Q.","56016589200;56017828200;56424498700;57221157913;","A novel iterative approach to pupil localization",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8833",,,"155","162",,,"10.1007/978-3-319-12484-1_17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911930474&doi=10.1007%2f978-3-319-12484-1_17&partnerID=40&md5=699bc7c04b6d103c876a729abac8f056","National Key Laboratory of Fundamental Science on Synthetic Vision, School of Computer Science, Sichuan University, Chengdu, China","Zhu, R., National Key Laboratory of Fundamental Science on Synthetic Vision, School of Computer Science, Sichuan University, Chengdu, China; Sang, G., National Key Laboratory of Fundamental Science on Synthetic Vision, School of Computer Science, Sichuan University, Chengdu, China; Gao, W., National Key Laboratory of Fundamental Science on Synthetic Vision, School of Computer Science, Sichuan University, Chengdu, China; Zhao, Q., National Key Laboratory of Fundamental Science on Synthetic Vision, School of Computer Science, Sichuan University, Chengdu, China","This paper proposes a novel method for localizing the center of pupils. Given a face detected in an image, it first empirically initializes the eye regions in the face, and locates the pupils within the eye regions by using an improved isophote curvature based method. It then updates the eye regions according to the detected pupil centers. In the updated eye regions, the pupil centers are also refined. The above process iterates until the detected pupil centers have sufficiently high consistency with the eye regions. Compared with previous methods, the proposed method can better cope with faces with varying pose angles. Evaluation experiments have been done on the public BioID database and a set of self-collected face images which display various pose angles and illumination conditions. The results demonstrate that the proposed method can more accurately locate pupil centers and is robust to illumination and pose variations. © Springer International Publishing Switzerland 2014.","Eye localization; Face recognition; Gaze estimation; Pupil localization","Image enhancement; Iterative methods; Curvature based methods; Evaluation experiments; Eye localization; Gaze estimation; Illumination conditions; Iterative approach; Pose variation; Pupil localization; Face recognition",Article,"Final","",Scopus,2-s2.0-84911930474
"Mora K.A.F., Odobez J.-M.","55336423200;57203103085;","Geometric generative gaze estimation (G3E) for remote RGB-D cameras",2014,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",,,"6909625","1773","1780",,49,"10.1109/CVPR.2014.229","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911439384&doi=10.1109%2fCVPR.2014.229&partnerID=40&md5=56331c38ecb84e637d433a870df515e9","Idiap Research Institute, Martigny, CH-1920, Switzerland; École Polytechnique Fédéral de Lausanne, Lausanne, CH-1015, Switzerland","Mora, K.A.F., Idiap Research Institute, Martigny, CH-1920, Switzerland; Odobez, J.-M., Idiap Research Institute, Martigny, CH-1920, Switzerland, École Polytechnique Fédéral de Lausanne, Lausanne, CH-1015, Switzerland","We propose a head pose invariant gaze estimation model for distant RGB-D cameras. It relies on a geometric understanding of the 3D gaze action and generation of eye images. By introducing a semantic segmentation of the eye region within a generative process, the model (i) avoids the critical feature tracking of geometrical approaches requiring high resolution images, (ii) decouples the person dependent geometry from the ambient conditions, allowing adaptation to different conditions without retraining. Priors in the generative framework are adequate for training from few samples. In addition, the model is capable of gaze extrapolation allowing for less restrictive training schemes. Comparisons with state of the art methods validate these properties which make our method highly valuable for addressing many diverse tasks in sociology, HRI and HCI. © 2014 IEEE.","gaze estimation; generative model; HCI; HHI; HRI; RGB-D; segmentation","Cameras; Computer vision; Human computer interaction; Image segmentation; Pattern recognition; Semantics; Gaze estimation; Generative model; Geometrical approaches; HHI; High resolution image; HRI; Semantic segmentation; State-of-the-art methods; Geometry",Conference Paper,"Final","",Scopus,2-s2.0-84911439384
"Sugano Y., Matsushita Y., Sato Y.","7005470045;35956654700;35230954300;","Learning-by-synthesis for appearance-based 3D gaze estimation",2014,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",,,"6909631","1821","1828",,161,"10.1109/CVPR.2014.235","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911391964&doi=10.1109%2fCVPR.2014.235&partnerID=40&md5=f7def872c22d7afeae5b5bcba7be5d6e","University of Tokyo, Tokyo, Japan; Microsoft Research Asia, Beijing, China","Sugano, Y., University of Tokyo, Tokyo, Japan; Matsushita, Y., Microsoft Research Asia, Beijing, China; Sato, Y., University of Tokyo, Tokyo, Japan","Inferring human gaze from low-resolution eye images is still a challenging task despite its practical importance in many application scenarios. This paper presents a learning-by-synthesis approach to accurate image-based gaze estimation that is person- and head pose-independent. Unlike existing appearance-based methods that assume person-specific training data, we use a large amount of cross-subject training data to train a 3D gaze estimator. We collect the largest and fully calibrated multi-view gaze dataset and perform a 3D reconstruction in order to generate dense training data of eye images. By using the synthesized dataset to learn a random regression forest, we show that our method outperforms existing methods that use low-resolution eye images. © 2014 IEEE.",,"Pattern recognition; 3D reconstruction; Appearance based; Appearance-based methods; Application scenario; Gaze estimation; Low resolution; Practical importance; Regression forests; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84911391964
"Zeng W., Liu Z., Steinbach E.","7203021669;7406672531;7005391508;","Forging a close relationship with multimedia communities",2014,"IEEE Multimedia","21","4","6945277","14","15",,,"10.1109/MMUL.2014.60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910030283&doi=10.1109%2fMMUL.2014.60&partnerID=40&md5=e0843930671cb84c159056c774579021","University of Missouri, Microsoft Research Asia, United States; Princeton University, United States; Microsoft Research, Redmond, United States; Technical University of Munich, Germany; University of Erlangen-Nuremberg, Germany","Zeng, W., University of Missouri, Microsoft Research Asia, United States, Princeton University, United States; Liu, Z., Princeton University, United States, Microsoft Research, Redmond, United States; Steinbach, E., Technical University of Munich, Germany, University of Erlangen-Nuremberg, Germany","In May 2014, the authors of the top 26 papers from the IEEE International Conference on Multimedia & Expo (ICME) 2014 were invited to submit extended versions of their papers to this fast track special issue. After a rigorous peer-review process, eight of those submissions were accepted for this special issue, now titled ""Hot Topics in Multimedia Research."" This is just the beginning of a close collaboration between MM and major multimedia conferences. © 2014 IEEE.","Distributed video coding; Gaze estimation; ICME; Image quality assessment; Learning algorithm; Location propagation; Multimedia; Multimedia research; Object tracking; Visual analysis","Distributed video coding; Gaze estimation; ICME; Image quality assessment; Multimedia; Multimedia research; Object Tracking; Visual analysis; Learning algorithms",Review,"Final","",Scopus,2-s2.0-84910030283
"Kassner M., Patera W., Bulling A.","56406193700;56406475600;6505807414;","Pupil: An open source platform for pervasive eye tracking and mobile gaze-based interaction",2014,"UbiComp 2014 - Adjunct Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing",,,,"1151","1160",,379,"10.1145/2638728.2641695","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908701467&doi=10.1145%2f2638728.2641695&partnerID=40&md5=be813c9cfaab293299619033d324f577","Pupil Labs UG, Berlin, Germany; Max Planck Institute for Informatics, Perceptual User Interfaces Group, Saarbrücken, Germany","Kassner, M., Pupil Labs UG, Berlin, Germany; Patera, W., Pupil Labs UG, Berlin, Germany; Bulling, A., Max Planck Institute for Informatics, Perceptual User Interfaces Group, Saarbrücken, Germany","In this paper we present Pupil - an accessible, affordable, and extensible open source platform for pervasive eye tracking and gaze-based interaction. Pupil comprises 1) a light-weight eye tracking headset, 2) an open source software framework for mobile eye tracking, as well as 3) a graphical user interface to playback and visualize video and gaze data. Pupil features high-resolution scene and eye cameras for monocular and binocular gaze estimation. The software and GUI are platform-independent and include state-of-the-art algorithms for real-time pupil detection and tracking, calibration, and accurate gaze estimation. Results of a performance evaluation show that Pupil can provide an average gaze estimation accuracy of 0.6 degree of visual angle (0.08 degree precision) with a processing pipeline latency of only 0.045 seconds.","Eye movement; Gaze-based interaction; Mobile eye tracking; Wearable computing","Computer programming; Graphical user interfaces; Open source software; Open systems; Software engineering; Ubiquitous computing; Wearable computers; Gaze-based interaction; Mobile eye-tracking; Open source platforms; Pervasive eye tracking; Pipeline latency; Pupil detection; State-of-the-art algorithms; Wearable computing; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84908701467
"Egger B., Schönborn S., Forster A., Vetter T.","57210675221;25123324800;57195462538;57203255741;","Pose normalization for eye gaze estimation and facial attribute description from still images",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8753",,,"317","327",,6,"10.1007/978-3-319-11752-2_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908661750&doi=10.1007%2f978-3-319-11752-2_25&partnerID=40&md5=3673cd30b9dcb1b58c80c9d206b40881","Department for Mathematics and Computer Science, University of Basel, Basel, Switzerland","Egger, B., Department for Mathematics and Computer Science, University of Basel, Basel, Switzerland; Schönborn, S., Department for Mathematics and Computer Science, University of Basel, Basel, Switzerland; Forster, A., Department for Mathematics and Computer Science, University of Basel, Basel, Switzerland; Vetter, T., Department for Mathematics and Computer Science, University of Basel, Basel, Switzerland","Our goal is to obtain an eye gaze estimation and a face description based on attributes (e.g. glasses, beard or thick lips) from still images. An attribute-based face description reflects human vocabulary and is therefore adequate as face description. Head pose and eye gaze play an important role in human interaction and are a key element to extract interaction information from still images. Pose variation is a major challenge when analyzing them. Most current approaches for facial image analysis are not explicitly pose-invariant. To obtain a poseinvariant representation, we have to account the three dimensional nature of a face. A 3D Morphable Model (3DMM) of faces is used to obtain a dense 3D reconstruction of the face in the image. This Analysis-by- Synthesis approach provides model parameters which contain an explicit face description and a dense model to image correspondence. However, the fit is restricted to the model space and cannot explain all variations. Our model only contains straight gaze directions and lacks high detail textural features. To overcome this limitations, we use the obtained correspondence in a discriminative approach. The dense correspondence is used to extract a pose-normalized version of the input image. The warped image contains all information from the original image and preserves gaze and detailed textural information. On the pose-normalized representation we train a regression function to obtain gaze estimation and attribute description. We provide results for pose-invariant gaze estimation on still images on the UUlm Head Pose and Gaze Database and attribute description on the Multi-PIE database. To the best of our knowledge, this is the first pose-invariant approach to estimate gaze from unconstrained still images. © Springer International Publishing Switzerland 2014.",,"Eye-gaze; Pose normalization; Still images",Conference Paper,"Final","",Scopus,2-s2.0-84908661750
"Li X., Li Z.-L., Qin J.-L.","56383697600;36066241800;56303312500;","An improved gaze tracking technique based on eye model",2014,"Proceedings of the 33rd Chinese Control Conference, CCC 2014",,,"6896207","7286","7291",,2,"10.1109/ChiCC.2014.6896207","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907919661&doi=10.1109%2fChiCC.2014.6896207&partnerID=40&md5=70f7849199f8d45327580b88b18bd31c","School of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, 100124, China","Li, X., School of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, 100124, China; Li, Z.-L., School of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, 100124, China; Qin, J.-L., School of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, 100124, China","Aiming at several common drawbacks of existing gaze tracking algorithm which is based on image processing technology: low accuracy, low suitability and high requirement of device, this paper presents an improved gaze tracking method based on the human eye model. The vision scatter model is established by studying the relationship among the center of iris, inner corner of eye and the gaze direction. The model is used for gaze rough estimation. An improved geometric eye model with compensation coefficient is used for gaze precise estimation, and the coefficient depends on the result of rough estimation. The result of experiment shows that the proposed approach has a low average error which is reached 2 ° and good robustness. This approach provides an effective solution for gaze tracking of human-computer interaction under the low pixel condition. © 2014 TCCT, CAA.","gaze estimation; gaze tracking; iris tracking; scatter model","Algorithms; Human computer interaction; Image processing; Compensation coefficients; Effective solution; Gaze estimation; Gaze tracking; Image processing technology; Iris tracking; Low average error; Scatter model; Tracking (position)",Conference Paper,"Final","",Scopus,2-s2.0-84907919661
"Liang K., Chahir Y., Molina M., Tijus C., Jouen F.","55854045600;6506225899;7202281141;6603707134;6603701102;","Appearance-based eye control system by manifold learning",2014,"VISAPP 2014 - Proceedings of the 9th International Conference on Computer Vision Theory and Applications","3",,,"148","155",,1,"10.5220/0004682601480155","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906915626&doi=10.5220%2f0004682601480155&partnerID=40&md5=4c874a1f1163234f30d5d297935772fe","CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus, 75014 Paris, France; PALM Laboratory EA 4649, University of Caen, Caen, France","Liang, K., CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus, 75014 Paris, France; Chahir, Y., CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus, 75014 Paris, France; Molina, M., PALM Laboratory EA 4649, University of Caen, Caen, France; Tijus, C., CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus, 75014 Paris, France; Jouen, F., CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus, 75014 Paris, France","Eye-movements are increasingly employed to study usability issues in HCI (Human-Computer Interacetion) contexts. In this paper we introduce our appearance-based eye control system which utilizes 5 specific eye movements, such as closed-eye movement and eye movements with gaze fixation at the positions (up, down, right, left) for HCI applications. In order to measure these eye movements, we employ a fast appeance-based gaze tracking method with manifold learning technique. First we propose to concatenate local eye appearance Center-Symmetric Local Binary Pattern(CS-LBP) descriptor for each subregion of eye image to form an eye appearance feature vector. The calibration phase is then introduced to construct a trainning samples by spectral clustering. After that, Laplacian Eigenmaps will be applied to the trainning set and unseen input together to get the structure of eye manifolds. Finally we can infer the eye movement of the new input by its distances with the clusters in the trainning set. Experimental results demonstrate that our system with quick 4-points calibration not only can reduce the run-time cost, but also provide another way to mesure eye movements without mesuring gaze coordinates to a HCI application such as our eye control system.","Appearance eye descriptor; Appearance-based method; Humancomputer interaction; Manifold learning; Spectral clustering","Calibration; Clustering algorithms; Computer vision; Control theory; Eye tracking; Human computer interaction; Learning systems; Man machine systems; Response time (computer systems); Appearance based; Appearance-based methods; Center-symmetric local binary patterns; Descriptors; Feature vectors; Laplacian eigenmaps; Manifold learning; Spectral clustering; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84906915626
"Yan M., Tamura H., Tanno K.","55515780300;35600682500;7103288835;","A study on gaze estimation system of the horizontal angle using electrooculogram signals",2014,"IEICE Transactions on Information and Systems","E97-D","9",,"2330","2337",,,"10.1587/transinf.2013LOP0018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906902072&doi=10.1587%2ftransinf.2013LOP0018&partnerID=40&md5=367baf5dfb134def66d44cf56d4d178d","Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazakishi, 889-2192, Japan; Department of Electrical and Electronic Engineering, University of Miyazaki, Miyazaki-shi, 889-2192, Japan","Yan, M., Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazakishi, 889-2192, Japan; Tamura, H., Department of Electrical and Electronic Engineering, University of Miyazaki, Miyazaki-shi, 889-2192, Japan; Tanno, K., Department of Electrical and Electronic Engineering, University of Miyazaki, Miyazaki-shi, 889-2192, Japan","The aim of this study is to present electrooculogram signals that can be used for human computer interface efficiently. Establishing an efficient alternative channel for communication without overt speech and hand movements is important to increase the quality of life for patients suffering from Amyotrophic Lateral Sclerosis or other illnesses that prevent correct limb and facial muscular responses. In this paper, we introduce the gaze estimation system of electrooculogram signals. Using this system, the electrooculogram signals can be recorded when the patients focused on each direct. All these recorded signals could be analyzed using mathmethod and the mathematical model will be set up. Gaze estimation can be recognized using electrooculogram signals follow these models. Copyright © 2014 The Institute of Electronics, Information and Communication Engineers.","Drift; Electrooculogram signal; Gaze estimation; Mathematical model","Gesture recognition; Mathematical models; Speech communication; Amyotrophic lateral sclerosis; Drift; Electro-oculogram; Gaze estimation; Horizontal angles; Human computer interfaces; Quality of life; Recorded signals; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84906902072
"Urano R., Suzuki R., Sasaki T.","56340918500;56340322300;36077602500;","Eye gaze estimation based on ellipse fitting and three-dimensional model of eye for 'Intelligent Poster'",2014,"IEEE/ASME International Conference on Advanced Intelligent Mechatronics, AIM",,,"6878237","1157","1162",,6,"10.1109/AIM.2014.6878237","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906657697&doi=10.1109%2fAIM.2014.6878237&partnerID=40&md5=a8c0c8bba7501605ef51132cd93dcecf","Shibaura Institute of Technology, Minato-ku, Tokyo, Japan","Urano, R., Shibaura Institute of Technology, Minato-ku, Tokyo, Japan; Suzuki, R., Shibaura Institute of Technology, Minato-ku, Tokyo, Japan; Sasaki, T., Shibaura Institute of Technology, Minato-ku, Tokyo, Japan","The designers judge the advertisement design based on subjective assessment only. But we think that they need an objective assessment to evaluate it. In this research, we propose a novel system which estimates our interest in posters by our eye gaze and it is called 'Intelligent Poster'. We hereby get an objective rating of the poster design and knowledge for making good posters. This paper focuses on eye gaze estimation which meets some requirements and has enough accuracy and robustness for 'Intelligent Poster'. Many researchers propose various eye gaze estimation, for example, using corneal reflection, wearing measurement hardware, using only cameras. In these days, we can get high resolution cameras in cheap with developing the technology. So, we can simply construct an eye gaze system by using a camera. We combine two methods based on ellipse fitting and three-dimensional model to realize the robust estimation. The achieved accuracy of our proposed method is 6 deg. © 2014 IEEE.",,"Cameras; Intelligent mechatronics; Advertisement designs; Corneal reflection; High resolution camera; Measurement hardware; Objective assessment; Robust estimation; Subjective assessments; Three-dimensional model; Design",Conference Paper,"Final","",Scopus,2-s2.0-84906657697
"Kocejko T., Bujnowski A., Ruminski J., Bylinska E., Wtorek J.","24824292600;23388608300;6603237944;56635879300;6701488611;","Head movement compensation algorithm in multi-display communication by gaze",2014,"Proceedings - 2014 7th International Conference on Human System Interactions, HSI 2014",,,"6860454","88","94",,15,"10.1109/HSI.2014.6860454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905717523&doi=10.1109%2fHSI.2014.6860454&partnerID=40&md5=ae92cba73a12ace08aa67fa7359f627c","Biomedical Engineering Department, Gdansk University of Technology GUT, Gdansk, Poland","Kocejko, T., Biomedical Engineering Department, Gdansk University of Technology GUT, Gdansk, Poland; Bujnowski, A., Biomedical Engineering Department, Gdansk University of Technology GUT, Gdansk, Poland; Ruminski, J., Biomedical Engineering Department, Gdansk University of Technology GUT, Gdansk, Poland; Bylinska, E., Biomedical Engineering Department, Gdansk University of Technology GUT, Gdansk, Poland; Wtorek, J., Biomedical Engineering Department, Gdansk University of Technology GUT, Gdansk, Poland","An influence of head movements on the gaze estimation accuracy when using a head mounted eye tracking system is discussed in the paper. This issue has been examined for a multi-display environment. It was found that head movement (rotation) to some extent does not influence on the gaze estimation accuracy seriously. Acceptable results were obtained when using eye-tracker to communicate with a computer via in two displays simultaneously. © 2014 IEEE.","eye tracking; gaze tracking; head movement compensation; head rotation; multi display; multi-display environment","Tracking (position); Eye-tracking; Gaze tracking; Head movement compensations; Head rotation; Multi displays; Multi-display environments; Algorithms",Conference Paper,"Final","",Scopus,2-s2.0-84905717523
"Martin S., Tawari A., Trivedi M.M.","55510668500;36470963300;7103153314;","Toward privacy-protecting safety systems for naturalistic driving videos",2014,"IEEE Transactions on Intelligent Transportation Systems","15","4","6776568","1811","1822",,15,"10.1109/TITS.2014.2308543","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905367362&doi=10.1109%2fTITS.2014.2308543&partnerID=40&md5=8cc32cfc99ef9682ba4338736edeb462","Laboratory for Intelligent and Safe Automobiles, University of California San Diego, San Diego, CA 92093, United States","Martin, S., Laboratory for Intelligent and Safe Automobiles, University of California San Diego, San Diego, CA 92093, United States; Tawari, A., Laboratory for Intelligent and Safe Automobiles, University of California San Diego, San Diego, CA 92093, United States; Trivedi, M.M., Laboratory for Intelligent and Safe Automobiles, University of California San Diego, San Diego, CA 92093, United States","A common pool of naturalistic driving data is necessary to develop and compare algorithms that infer driver behavior, in order to improve driving safety. Naturalistic driving data, such as video sequences of looking at a driver, however, cause concern for the privacy of individual drivers. In an ideal situation, a deidentification filter applied to a raw image of looking at a driver would, semantically, protect the identity and preserve the behavior (e.g., eye gaze, head pose, and hand activity) of the driver. Driver gaze estimation is of particular interest because it is a good indicator of a driver's visual attention and a good predictor of a driver's intent. Interestingly, the same facial features that are explicitly or implicitly used for gaze estimation play a key role in recognizing a person's identity. In this paper, we implement a specific deidentification filter on video sequences of looking at a driver from naturalistic driving and present novel findings on its effect on face recognition and driver gaze-zone estimation. © 2014 IEEE.","Active safety; deidentification; driver assistance systems; driver behavior; gaze estimation; head pose; human factors","Algorithms; Face recognition; Human engineering; Video recording; Active safety; De-identification; Driver assistance system; Driver behavior; Gaze estimation; Head pose; Behavioral research",Article,"Final","",Scopus,2-s2.0-84905367362
"Wang M., Maeda Y., Takahashi Y.","55701262000;7402845246;55705381800;","Visual attention region prediction based on eye tracking using fuzzy inference",2014,"Journal of Advanced Computational Intelligence and Intelligent Informatics","18","4",,"499","510",,5,"10.20965/jaciii.2014.p0499","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904575403&doi=10.20965%2fjaciii.2014.p0499&partnerID=40&md5=0d646e4642aa64207e5602d1ebcd3e3f","Department of System Design Engineering, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan; Department of Robotics, Faculty of Engineering, Osaka Institute of Technology, 5-16-1 Omiya, Asahi-ku, Osaka 535-8585, Japan; Department of Human and Artificial Intelligent Systems, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan","Wang, M., Department of System Design Engineering, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan; Maeda, Y., Department of Robotics, Faculty of Engineering, Osaka Institute of Technology, 5-16-1 Omiya, Asahi-ku, Osaka 535-8585, Japan; Takahashi, Y., Department of Human and Artificial Intelligent Systems, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan","Visual attention region prediction has attracted the attention of intelligent systems researchers because it makes the interaction between human beings and intelligent nonhuman agents to be more intelligent. Visual attention region prediction uses multiple input factors such as gestures, face images and eye gaze position. Physically, disabled persons may find it difficult to move in some way. In this paper, we propose using gaze position estimation as input to a prediction system achieved by extracting image features. Our approach is divided into two parts: user gaze estimation and visual attention region inference. The neural network has been used in user gaze estimation as the decision making unit, following which the user gaze position at the computer screen is then estimated. We proposed that prediction in visual attention region inference of the visual attention region be inferred by using fuzzy inference after image feature maps and saliency maps have been extracted and computed. User experiments conducted to evaluate the prediction accuracy of our proposed method surveyed prediction results. These results indicated that the prediction we proposed performs better at the attention regions position prediction level depending on the image.","Eye tracking; Fuzzy inference; Neural network; Saliency map; Visual attention","Behavioral research; Decision making; Disabled persons; Forecasting; Fuzzy inference; Fuzzy neural networks; Image segmentation; Intelligent systems; Neural networks; Decision making unit; Position estimation; Position predictions; Prediction accuracy; Prediction systems; Region predictions; Saliency map; Visual Attention; Eye tracking",Article,"Final","",Scopus,2-s2.0-84904575403
"Zhang Y., Bulling A., Gellersen H.","52664598600;6505807414;6701531333;","Pupil-canthi-ratio: A calibration-free method for tracking horizontal gaze direction",2014,"Proceedings of the Workshop on Advanced Visual Interfaces AVI",,,,"129","132",,20,"10.1145/2598153.2598186","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903612123&doi=10.1145%2f2598153.2598186&partnerID=40&md5=63ea83b87db31b85770ade2628a93c40","Lancaster University, Lancaster, United Kingdom; Max Planck Institute for Informatics, Saarbrücken, Germany","Zhang, Y., Lancaster University, Lancaster, United Kingdom; Bulling, A., Max Planck Institute for Informatics, Saarbrücken, Germany; Gellersen, H., Lancaster University, Lancaster, United Kingdom","Eye tracking is compelling for hands-free interaction with pervasive displays. However, most existing eye tracking systems require specialised hardware and explicit calibrations of equipment and individual users, which inhibit their widespread adoption. In this work, we present a light-weight and calibration-free gaze estimation method that leverages only an off-the-shelf camera to track users' gaze horizontally. We introduce pupil-canthi-ratio (PCR), a novel measure for estimating gaze directions. By using the displacement vector between the inner eye corner and the pupil centre of an eye, PCR is calculated as the ratio of the displacement vectors from both eyes. We establish a mapping between PCR to gaze direction by Gaussian process regression, which inherently infers averted horizontal gaze directions of users. We present a study to identify the characteristics of PCR. The results show that PCR achieved an average accuracy of 3.9 degrees across different people. Finally, we show examples of real-time applications of PCR that allow users to interact with a display by moving only their eyes. © 2014 ACM.","calibration-free; eye tracking; Gaussian regression; gaze-based interaction; pervasive displays; vision-based","Estimation; calibration-free; Eye-tracking; Gaussian regression; Gaze-based interaction; Vision based; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84903612123
"Mayberry A., Hu P., Marlin B., Salthouse C., Ganesan D.","55819678200;55582016300;6506955008;6507078030;10739214500;","iShadow: Design of a wearable, real-time mobile gaze tracker",2014,"MobiSys 2014 - Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services",,,,"82","94",,50,"10.1145/2594368.2594388","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903135094&doi=10.1145%2f2594368.2594388&partnerID=40&md5=cd85fd6c9caabef26f72374327ece6a5","University of Massachusetts Amherst, 140 Governor's Drive, Amherst, MA 01003, United States; University of Massachusetts Amherst, 100 Natural Resources Rd., Amherst, MA 01003, United States","Mayberry, A., University of Massachusetts Amherst, 140 Governor's Drive, Amherst, MA 01003, United States; Hu, P., University of Massachusetts Amherst, 140 Governor's Drive, Amherst, MA 01003, United States; Marlin, B., University of Massachusetts Amherst, 140 Governor's Drive, Amherst, MA 01003, United States; Salthouse, C., University of Massachusetts Amherst, 100 Natural Resources Rd., Amherst, MA 01003, United States; Ganesan, D., University of Massachusetts Amherst, 140 Governor's Drive, Amherst, MA 01003, United States","Continuous, real-time tracking of eye gaze is valuable in a variety of scenarios including hands-free interaction with the physical world, detection of unsafe behaviors, leveraging visual context for advertising, life logging, and others. While eye tracking is commonly used in clinical trials and user studies, it has not bridged the gap to everyday consumer use. The challenge is that a real-time eye tracker is a power-hungry and computation-intensive device which requires continuous sensing of the eye using an imager running at many tens of frames per second, and continuous processing of the image stream using sophisticated gaze estimation algorithms. Our key contribution is the design of an eye tracker that dramatically reduces the sensing and computation needs for eye tracking, thereby achieving orders of magnitude reductions in power consumption and form-factor. The key idea is that eye images are extremely redundant, therefore we can estimate gaze by using a small subset of carefully chosen pixels per frame. We instantiate this idea in a prototype hardware platform equipped with a low-power image sensor that provides random access to pixel values, a low-power ARM Cortex M3 microcontroller, and a bluetooth radio to communicate with a mobile phone. The sparse pixel-based gaze estimation algorithm is a multi-layer neural network learned using a state-of-the-art sparsity-inducing regularization function that minimizes the gaze prediction error while simultaneously minimizing the number of pixels used. Our results show that we can operate at roughly 70mW of power, while continuously estimating eye gaze at the rate of 30 Hz with errors of roughly 3 degrees. © 2014 ACM.","eye tracking; lifelog; neural network","Network layers; Neural networks; Continuous processing; Eye-tracking; Hands-free interactions; Life log; Minimizing the number of; Orders of magnitude; Real time tracking; Regularization function; Pixels",Conference Paper,"Final","",Scopus,2-s2.0-84903135094
"Siboska D., Karstoft H., Pedersen H.","56203521000;36771565700;56045683100;","Synchronization of electroencephalography and eye tracking using global illumination changes",2014,"BIOSIGNALS 2014 - 7th Int. Conference on Bio-Inspired Systems and Signal Processing, Proceedings; Part of 7th Int. Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2014",,,,"55","60",,1,"10.5220/0004800400550060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902336234&doi=10.5220%2f0004800400550060&partnerID=40&md5=04621199c9cc072593359b5cbbab054a","Department of Engineering, Aarhus University, Finlandsgade 22, 8200 Aarhus N, Denmark","Siboska, D., Department of Engineering, Aarhus University, Finlandsgade 22, 8200 Aarhus N, Denmark; Karstoft, H., Department of Engineering, Aarhus University, Finlandsgade 22, 8200 Aarhus N, Denmark; Pedersen, H., Department of Engineering, Aarhus University, Finlandsgade 22, 8200 Aarhus N, Denmark","This paper describes a flexible method for synchronizing electroencephalography (EEG) and eye tracking (ET) recordings to the presentation of visual stimuli. The method consists of embedding a synchronization signal in the visual stimuli, and recording this signal with both the EEG and ET equipment. The signal is recorded by the EEG device as an additional data channel, and with the camera used in the ET equipment by modulating the global illumination of the scene in time with the synchronizing signal. The prototype system where this method was implemented resulted in a single sample of jitter in both the EEG and ET system, while the ET system achieved a spatial resolution of 1.26 degrees. The system will be used in future work with augmented memory applications. Copyright © 2014 SCITEPRESS - Science and Technology Publications. All rights reserved.","Electroencephalography; Eye Tracking; Gaze Estimation; Synchronization","Biomedical engineering; Biomimetics; Electroencephalography; Electrophysiology; Signal processing; Synchronization; Additional datum; Gaze estimation; Global illumination; Global illumination change; Memory applications; Spatial resolution; Synchronization signals; Synchronizing signal; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-84902336234
"Mukherjee A., Mukherjee J.K.","56192530200;55617715100;","A gaze estimation approach for intelligent viewing in tele-operation",2014,"IEEE TechSym 2014 - 2014 IEEE Students' Technology Symposium",,,"6808049","214","217",,,"10.1109/TechSym.2014.6808049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901814725&doi=10.1109%2fTechSym.2014.6808049&partnerID=40&md5=3c82fb9248a6193fe49dc4b75d5b5b3e","Electronics, Vivekanand Institute of Technology VESIT, Chembur, Mumbai 400074, India; Machine Intelligence Laboratory, EISD, Bhabha Atomic Research Centre, Mumbai 400085, India","Mukherjee, A., Electronics, Vivekanand Institute of Technology VESIT, Chembur, Mumbai 400074, India; Mukherjee, J.K., Machine Intelligence Laboratory, EISD, Bhabha Atomic Research Centre, Mumbai 400085, India","Human like viewing is a major challenge in intelligent remote scenario viewing in tele-robotic applications of mobile as well as fixed frame slaves that are interactively controlled in 'man in loop' type control. The control being mainly based on console-views available to the operator, the HMI system must understand what the operator wants to look at. The presented work, attempts at developing a method for guessing operator interest zone in remote workspace area by understanding operator's reaction to 'console-view'. The approach establishes a linkage for the remote camera control to subtle operator head movement in reaction to console view. The natural operator reaction is interpreted by our gaze estimator through rapidly extracted image clues. © 2014 IEEE.","head motion interpretation; image-processing; Intelligent HMI; remote viewer; robotics; tele-control","Intelligent robots; Remote consoles; Fixed frames; Gaze estimation; Head motion; Head movements; Intelligent HMI; Remote cameras; remote viewer; Tele-operations; Robotics",Conference Paper,"Final","",Scopus,2-s2.0-84901814725
"Paletta L., Neuschmied H., Schwarz M., Lodron G., Pszeida M., Luley P., Ladstätter S., Deutsch S., Bobeth J., Tscheligi M.","6602696802;56160068200;56684019900;36168421000;56160335300;8572593700;36623838000;35071236500;55329443500;56948770700;","Attention in mobile interactions: Gaze recovery for large scale studies",2014,"Conference on Human Factors in Computing Systems - Proceedings",,,,"1717","1722",,3,"10.1145/2559206.2581235","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900552648&doi=10.1145%2f2559206.2581235&partnerID=40&md5=2e4bd24a0e7ee881367b7aa6bc940733","DIGITAL, Inst. Information and Comm. Techn., Joanneum Research FgesmbH, Austria; CURE - Center for Usability Research and Eng., Modecenterstr. 17, Obj. 2, Vienna, Austria","Paletta, L., DIGITAL, Inst. Information and Comm. Techn., Joanneum Research FgesmbH, Austria; Neuschmied, H., DIGITAL, Inst. Information and Comm. Techn., Joanneum Research FgesmbH, Austria; Schwarz, M., DIGITAL, Inst. Information and Comm. Techn., Joanneum Research FgesmbH, Austria; Lodron, G., DIGITAL, Inst. Information and Comm. Techn., Joanneum Research FgesmbH, Austria; Pszeida, M., DIGITAL, Inst. Information and Comm. Techn., Joanneum Research FgesmbH, Austria; Luley, P., DIGITAL, Inst. Information and Comm. Techn., Joanneum Research FgesmbH, Austria; Ladstätter, S., DIGITAL, Inst. Information and Comm. Techn., Joanneum Research FgesmbH, Austria; Deutsch, S., CURE - Center for Usability Research and Eng., Modecenterstr. 17, Obj. 2, Vienna, Austria; Bobeth, J., CURE - Center for Usability Research and Eng., Modecenterstr. 17, Obj. 2, Vienna, Austria; Tscheligi, M., CURE - Center for Usability Research and Eng., Modecenterstr. 17, Obj. 2, Vienna, Austria","Understanding human attention in mobile interaction is a relevant part of human computer interaction, indicating focus of task, emotion and communication. Lack of large scale studies enabling statistically significant results is due to high costs of manual penetration in eye tracking analysis. With high quality wearable cameras for eye-tracking and Google glasses, video analysis for visual attention analysis will become ubiquitous for automated large scale annotation. We describe for the first time precise gaze estimation on mobile displays and surrounding, its performance and without markers. We demonstrate accurate POR (point of regard) recovery on the mobile device and enable heat mapping of visual tasks. In a benchmark test we achieve a mean accuracy in the POR localization on the display by ≈1.5 mm, and the method is very robust to illumination changes. We conclude from these results that this system may open new avenues in eye tracking research for behavior analysis in mobile applications.","Gaze recovery; Human attention; Mobile interaction heat maps","Behavioral research; Benchmarking; Human engineering; Mobile devices; Recovery; Vision; Behavior analysis; Eye-tracking analysis; Heat maps; Human attention; Illumination changes; Large-scale studies; Mobile applications; Mobile interaction; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84900552648
"Wang R.I., Pelfrey B., Duchowski A.T., House D.H.","55547508300;36716457200;6701824388;7006174095;","Online 3D gaze localization on stereoscopic displays",2014,"ACM Transactions on Applied Perception","11","1","3","","",,19,"10.1145/2593689","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899744100&doi=10.1145%2f2593689&partnerID=40&md5=91f17728d0913ccbcf230b442bca4a53","School of Computing, Clemson University, 100 McAdams Hall, Clemson, SC 29634, United States","Wang, R.I., School of Computing, Clemson University, 100 McAdams Hall, Clemson, SC 29634, United States; Pelfrey, B., School of Computing, Clemson University, 100 McAdams Hall, Clemson, SC 29634, United States; Duchowski, A.T., School of Computing, Clemson University, 100 McAdams Hall, Clemson, SC 29634, United States; House, D.H., School of Computing, Clemson University, 100 McAdams Hall, Clemson, SC 29634, United States","This article summarizes our previous work on developing an online system to allow the estimation of 3D gaze depth using eye tracking in a stereoscopic environment. We report on recent extensions allowing us to report the full 3D gaze position. Our system employs a 3D calibration process that determines the parameters of a mapping from a naive depth estimate, based simply on triangulation, to a refined 3D gaze point estimate tuned to a particular user. We show that our system is an improvement on the geometry-based 3D gaze estimation returned by a proprietary algorithm provided with our tracker. We also compare our approach with that of the Parameterized Self-Organizing Map (PSOM) method, due to Essig and colleagues, which also individually calibrates to each user. We argue that our method is superior in speed and ease of calibration, is easier to implement, and does not require an iterative solver to produce a gaze position, thus guaranteeing computation at the rate of tracker acquisition. In addition, we report on a user study that indicates that, compared with PSOM, our method more accurately estimates gaze depth, and is nearly as accurate in estimating horizontal and vertical position. Results are verified on two different 4D eye tracking systems, a high accuracy Wheatstone haploscope and a medium accuracy active stereo display. Thus, it is the recommended method for applications that primarily require gaze depth information, while its ease of use makes it suitable for many applications requiring full 3D gaze position. © 2014 ACM.","3D eye tracking; Eye tracking; Stereoscopic displays; Vergence","Conformal mapping; Display devices; Estimation; Iterative methods; 3D gaze positions; Depth information; Eye tracking systems; Eye-tracking; Parameterized self-organizing maps; Stereoscopic display; Vergences; Vertical positions; Three dimensional",Article,"Final","",Scopus,2-s2.0-84899744100
"Lidegaard M., Hansen D.W., Krüger N.","56145694300;15063910800;7004442773;","Head mounted device for point-of-gaze estimation in three dimensions",2014,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"83","86",,11,"10.1145/2578153.2578163","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899701590&doi=10.1145%2f2578153.2578163&partnerID=40&md5=1c3a8b54b3c2401b754be0c31999b245","University of Southern Denmark, Denmark; IT University of Copenhagen, Denmark","Lidegaard, M., University of Southern Denmark, Denmark; Hansen, D.W., IT University of Copenhagen, Denmark; Krüger, N., University of Southern Denmark, Denmark","This paper presents a fully calibrated extended geometric approach for gaze estimation in three dimensions (3D). The methodology is based on a geometric approach utilising a fully calibrated binocular setup constructed as a head-mounted system. The approach is based on utilisation of two ordinary web-cameras for each eye and 6D magnetic sensors allowing free head movements in 3D. Evaluation of initial experiments indicate comparable results to current state-of-the-art on estimating gaze in 3D. Initial results show an RMS error of 39-50 mm in the depth dimension and even smaller in the horizontal and vertical dimensions regarding fixations. However, even though the workspace is limited, the fact that the system is designed as a head-mounted device, the workspace volume is relatively positioned to the pose of the device. Hence gaze can be estimated in 3D with relatively free head-movements with external reference to a world coordinate system and is therefore offering flexibility and movability within certain constraints.","Eye tracking; Fully calibrated geometric approach; Gaze estimation; Three dimensional (3D) gaze tracking","Eye movements; Geometry; Eye-tracking; Gaze estimation; Gaze tracking; Geometric approaches; Head-mounted systems; Three dimensions; Vertical dimensions; World coordinate systems; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-84899701590
"Sesma-Sanchez L., Villanueva A., Cabeza R.","55320498400;7101612861;36763933900;","Design issues of remote eye tracking systems with large range of movement",2014,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"243","246",,2,"10.1145/2578153.2578193","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899694360&doi=10.1145%2f2578153.2578193&partnerID=40&md5=67641b6cfb6165c3c4b6cc67ca2c5ea4","Public University of Navarra, Spain","Sesma-Sanchez, L., Public University of Navarra, Spain; Villanueva, A., Public University of Navarra, Spain; Cabeza, R., Public University of Navarra, Spain","One of the goals of the eye tracking community is to build systems that allow users to move freely. In general, there is a trade-off between the field of view of an eye tracking system and the gaze estimation accuracy. We aim to study how much the field of view of an eye tracking system can be increased, while maintaining acceptable accuracy. In this paper, we investigate all the issues concerning remote eye tracking systems with large range of movement in a simulated environment and we give some guidelines that can facilitate the process of designing an eye tracker. Given a desired range of movement and a working distance, we can calculate the camera focal length and sensor size or given a certain camera, we can determine the user's range of movement. The robustness against large head movement of two gaze estimation methods based on infrared light is analyzed: an interpolation and a geometrical method. We relate the accuracy of the gaze estimation methods with the image resolution around the eye area for a certain feature detector's accuracy and provide possible combinations of pixel size and focal length for different gaze estimation accuracies. Finally, we give the gaze estimation accuracy as a function of a new defined eye error, which is independent of any design parameters.","Eye tracking technology; Gaze estimation; Interpolation methods; Large head movements","Feature extraction; Image resolution; Interpolation; Tracking (position); Camera focal length; Eye tracking systems; Eye tracking technologies; Gaze estimation; Geometrical methods; Head movements; Interpolation method; Simulated environment; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84899694360
"Huang J.-B., Cai Q., Liu Z., Ahuja N., Zhang Z.","21742461700;36469764800;7406672531;35515078200;13609600600;","Towards accurate and robust cross-ratio based gaze trackers through learning from simulation",2014,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"75","82",,19,"10.1145/2578153.2578162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899693799&doi=10.1145%2f2578153.2578162&partnerID=40&md5=e917cabe930b7897d14da47f7d67e8c0","University of Illinois, Urbana-Champaign, United States; Microsoft Research, United States","Huang, J.-B., University of Illinois, Urbana-Champaign, United States; Cai, Q., Microsoft Research, United States; Liu, Z., Microsoft Research, United States; Ahuja, N., University of Illinois, Urbana-Champaign, United States; Zhang, Z., University of Illinois, Urbana-Champaign, United States","remote gaze estimation using a single camera in an uncalibrated setup by exploiting invariance of a plane projectivity. Unfortunately, due to several simplification assumptions, the performance of CR-based eye gaze trackers decays significantly as the subject moves away from the calibration position. In this paper, we introduce an adaptive homography mapping for achieving gaze prediction with higher accuracy at the calibration position and more robustness under head movements. This is achieved with a learningbased method for compensating both spatially-varying gaze errors and head pose dependent errors simultaneously in a unified framework. The model of adaptive homography is trained offline using simulated data, saving a tremendous amount of time in data collection. We validate the effectiveness of the proposed approach using both simulated and real data from a physical setup. We show that our method compares favorably against other state-of-the-art CR based methods.","Cross-ratio; Eye gaze tracking; Simulation","Calibration; Errors; Gesture recognition; Cross-ratios; Data collection; Eye gaze trackers; Eye gaze tracking; Learning-based methods; Remote gaze estimation; Simulation; Unified framework; Computer simulation",Conference Paper,"Final","",Scopus,2-s2.0-84899693799
"Mazzei A., Eivazi S., Marko Y., Kaplan F., Dillenbourg P.","55843510100;37019970200;36697950800;56268256700;8912010400;","3D model-based gaze estimation in natural reading: A systematic error correction procedure based on annotated texts",2014,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"87","90",,8,"10.1145/2578153.2578164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899693382&doi=10.1145%2f2578153.2578164&partnerID=40&md5=6a44df1de49bc48dc94689e467b15cc8","School of Computer and Communication Sciences, École Polytechnique Fédérale de Lausanne, Switzerland; School of Computing, University of Eastern Finland, Finland","Mazzei, A., School of Computer and Communication Sciences, École Polytechnique Fédérale de Lausanne, Switzerland, School of Computing, University of Eastern Finland, Finland; Eivazi, S., School of Computer and Communication Sciences, École Polytechnique Fédérale de Lausanne, Switzerland, School of Computing, University of Eastern Finland, Finland; Marko, Y., School of Computer and Communication Sciences, École Polytechnique Fédérale de Lausanne, Switzerland, School of Computing, University of Eastern Finland, Finland; Kaplan, F., School of Computer and Communication Sciences, École Polytechnique Fédérale de Lausanne, Switzerland, School of Computing, University of Eastern Finland, Finland; Dillenbourg, P., School of Computer and Communication Sciences, École Polytechnique Fédérale de Lausanne, Switzerland, School of Computing, University of Eastern Finland, Finland","Studying natural reading and its underlying attention processes requires devices that are able to provide precise measurements of gaze without rendering the reading activity unnatural. In this paper we propose an eye tracking system that can be used to conduct analyses of reading behavior in low constrained experimental settings. The system is designed for dual-camera-based head-mounted eye trackers and allows free head movements and note taking. The system is composed of three different modules. First, a 3D model-based gaze estimation method computes the reader's gaze trajectory. Second, a document image retrieval algorithm is used to recognize document pages and extract annotations. Third, a systematic error correction procedure is used to post-calibrate the system parameters and compensate for spatial drifts. The validation results show that the proposed method is capable of extracting reliable gaze data when reading in low constrained experimental conditions.","3D Gaze Estimation; Systematic Error Correction","Error correction; Systematic errors; Document image retrieval; Experimental conditions; Eye trackers; Eye tracking systems; Gaze estimation; Precise measurements; Reading activities; Validation results; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-84899693382
"Wood E., Bulling A.","56145872800;6505807414;","EyeTab: Model-based gaze estimation on unmodified tablet computers",2014,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"207","210",,145,"10.1145/2578153.2578185","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899692476&doi=10.1145%2f2578153.2578185&partnerID=40&md5=dc6b4bbca9abb41f808c1b5b955c211c","University of Cambridge, Cambridge, United Kingdom; Max Planck Institute for Informatics, Saarbrücken, Germany","Wood, E., University of Cambridge, Cambridge, United Kingdom; Bulling, A., Max Planck Institute for Informatics, Saarbrücken, Germany","Despite the widespread use of mobile phones and tablets, hand-held portable devices have only recently been identified as a promising platform for gaze-aware applications. Estimating gaze on portable devices is challenging given their limited computational resources, low quality integrated front-facing RGB cameras, and small screens to which gaze is mapped. In this paper we present EyeTab, a modelbased approach for binocular gaze estimation that runs entirely on an unmodified tablet. EyeTab builds on set of established image processing and computer vision algorithms and adapts them for robust and near-realtime gaze estimation. A technical prototype evaluation with eight participants in a normal indoors office setting shows that EyeTab achieves an average gaze estimation accuracy of 6:88° of visual angle at 12 frames per second.","Attentive user interfaces; Eye tracking; Gaze estimation; Gaze-based interfaces; Portable devices","Computer vision; User interfaces; Attentive user interfaces; Computational resources; Eye-tracking; Frames per seconds; Gaze estimation; Image processing and computer vision; Model based approach; Portable device; Portable equipment",Conference Paper,"Final","",Scopus,2-s2.0-84899692476
[无可用作者姓名],[无可用的作者 ID],"Proceedings of the Symposium on Eye Tracking Research and Applications, ETRA 2014",2014,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",396,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899690037&partnerID=40&md5=dba61e3d0d6e75f3dd28fb5d12f7a07a",,"","The proceedings contain 75 papers. The topics discussed include: cross-device gaze-supported point-to-point content transfer; look and lean: accurate head-assisted eye pointing; saliency-based Bayesian modeling of dynamic viewing of static scenes; creating a new dynamic measure of the useful field view using gaze-contingent displays; towards accurate and robust cross-ratio based gaze trackers through learning from simulation; head mounted device for point-of-gaze estimation in three dimensions; comparing estimated gaze depth in virtual and physical environments; the effects of fast disparity adjustment in gaze-controlled stereoscopic applications; eye-movement sequence statistics and hypothesis-testing with classical recurrence analysis; EyeSee3D: a low-cost approach for analyzing mobile 3D eye tracking data using computer vision and augmented reality technology; and attentional processes in natural reading: the effect of margin annotations on reading behavior and comprehension.",,,Conference Review,"Final","",Scopus,2-s2.0-84899690037
"Mora K.A.F., Monay F., Odobez J.-M.","55336423200;56145184900;57203103085;","EYEDIAP: A database for the development and evaluation of gaze estimation algorithms from RGB and RGB-D cameras",2014,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"255","258",,109,"10.1145/2578153.2578190","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899680058&doi=10.1145%2f2578153.2578190&partnerID=40&md5=438726f0a83d1d6f0190bbc6762e2e06","Idiap Research Institute, Switzerland; École Polytechnique Fédérale de Lausanne (EPFL), Switzerland","Mora, K.A.F., Idiap Research Institute, Switzerland, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland; Monay, F., Idiap Research Institute, Switzerland; Odobez, J.-M., Idiap Research Institute, Switzerland, École Polytechnique Fédérale de Lausanne (EPFL), Switzerland","The lack of a common benchmark for the evaluation of the gaze estimation task from RGB and RGB-D data is a serious limitation for distinguishing the advantages and disadvantages of the many proposed algorithms found in the literature. This paper intends to overcome this limitation by introducing a novel database along with a common framework for the training and evaluation of gaze estimation approaches. In particular, we have designed this database to enable the evaluation of the robustness of algorithms with respect to the main challenges associated to this task: i) Head pose variations; ii) Person variation; iii) Changes in ambient and sensing conditions and iv) Types of target: screen or 3D object.","Database; Depth; Gaze estimation; Head pose; Natural-light; Remote sensing; RGB; RGB-D","Algorithms; Remote sensing; Depth; Gaze estimation; Head pose; Natural-light; RGB; RGB-D; Database systems",Conference Paper,"Final","",Scopus,2-s2.0-84899680058
"Świrski L., Dodgson N.","53867304000;6603866623;","Rendering synthetic ground truth images for eye tracker evaluation",2014,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"219","222",,33,"10.1145/2578153.2578188","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899670436&doi=10.1145%2f2578153.2578188&partnerID=40&md5=7035ca6d134ad2c259433b0bbce3e66b","University of Cambridge, United Kingdom","Świrski, L., University of Cambridge, United Kingdom; Dodgson, N., University of Cambridge, United Kingdom","When evaluating eye tracking algorithms, a recurring issue is what metric to use and what data to compare against. User studies are informative when considering the entire eye tracking system, however they are often unsatisfactory for evaluating the gaze estimation algorithm in isolation. This is particularly an issue when evaluating a system's component parts, such as pupil detection, pupil-to-gaze mapping or head pose estimation. Instead of user studies, eye tracking algorithms can be evaluated using simulated input video. We describe a computer graphics approach to creating realistic synthetic eye images, using a 3D model of the eye and head and a physically correct rendering technique. By using rendering, we have full control over the parameters of the scene such as the gaze vector or camera position, which allows the calculation of ground truth data, while creating a realistic input for a video-based gaze estimator.","Eye tracking; Ground truth; Pupil detection; Rendering","Algorithms; Image recognition; Tracking (position); Camera positions; Eye tracking systems; Eye-tracking; Ground truth; Ground truth data; Head Pose Estimation; Pupil detection; Rendering; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84899670436
"Mayberry A., Hu P., Marlin B., Salthouse C., Ganesan D.","55819678200;55582016300;6506955008;6507078030;10739214500;","IShadow: The computational eyeglass system",2014,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"359","360",,2,"10.1145/2578153.2582177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899670096&doi=10.1145%2f2578153.2582177&partnerID=40&md5=c183d28c391aebbc8ad7723f99c5872a","University of Massachusetts, United States","Mayberry, A., University of Massachusetts, United States; Hu, P., University of Massachusetts, United States; Marlin, B., University of Massachusetts, United States; Salthouse, C., University of Massachusetts, United States; Ganesan, D., University of Massachusetts, United States","Continuous, real-time tracking of eye gaze is valuable in a variety of scenarios including hands-free interaction with the physical world, detection of unsafe behaviors, leveraging visual context for advertising, life logging, and others. While eye tracking is commonly used in clinical trials and user studies, it has not bridged the gap to everyday consumer use. The challenge is that a real-time eye tracker is a power-hungry and computation-intensive device which requires continuous sensing of the eye using an imager running at many tens of frames per second, and continuous processing of the image stream using sophisticated gaze estimation algorithms. Our key contribution is the design of an eye tracker that dramatically reduces the sensing and computation needs for eye tracking, thereby achieving orders of magnitude reductions in power consumption and form-factor. The key idea is that eye images are extremely redundant, therefore we can estimate gaze by using a small subset of carefully chosen pixels per frame. We use a sparse pixel-based gaze estimation algorithm that is a multi-layer neural network learned using a state-of-the-art sparsity-inducing regularization function which minimizes the gaze prediction error while simultaneously minimizing the number of pixels used. Our results show that we can operate at roughly 70mW of power, while continuously estimating eye gaze at the rate of 30 Hz with errors of roughly 4 degrees.","Eye tracking; Gaze estimation; MHealth; Neural network","Algorithms; Health care; Neural networks; Continuous processing; Eye-tracking; Gaze estimation; Hands-free interactions; mHealth; Minimizing the number of; Orders of magnitude; Regularization function; Pixels",Conference Paper,"Final","",Scopus,2-s2.0-84899670096
"Ackland S., Istance H., Coupland S., Vickers S.","56145688300;6602493995;16052310400;7005694667;","An investigation into determining head pose for gaze estimation on unmodified mobile devices",2014,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"203","206",,2,"10.1145/2578153.2578184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899668603&doi=10.1145%2f2578153.2578184&partnerID=40&md5=b71b946146a2264eb19e735837b37a91","De Montfort University, United Kingdom","Ackland, S., De Montfort University, United Kingdom; Istance, H., De Montfort University, United Kingdom; Coupland, S., De Montfort University, United Kingdom; Vickers, S., De Montfort University, United Kingdom","Traditionally, devices which are able to determine a users gaze are large, expensive and often restrictive. We investigate the prospect of using common webcams and mobile devices such as laptops, tablets and phones without modification as an alternative means for obtaining a users gaze. A person's gaze can be fundamentally determined by the pose of the head as well as the orientation of the eyes. This initial work investigates the first of these factors - an estimate of the 3D head pose (and subsequently the positions of the eye centres) relative to a camera device. Specifically, we seek a low cost algorithm that requires only a one-time calibration for an individual user, that can run in real-time on the aforementioned mobile devices with noisy camera data. We use our head tracker to estimate the 4 eye corners of a user over a 10 second video. We present the results at several different frames per second (fps) to analyse the impact on the tracker with lower quality cameras. We show that our algorithm is efficient enough to run at 75fps on a common laptop, but struggles with tracking loss when the fps is lower than 10fps.","3D Head Pose; Computer vision; Webcam systems","Algorithms; Cameras; Computer vision; Laptop computers; Three dimensional; 3D head; Camera devices; Eye corners; Frames per seconds; Gaze estimation; Head pose; Head trackers; Low costs; Mobile devices",Conference Paper,"Final","",Scopus,2-s2.0-84899668603
"Chen J., Chen D., Li X., Zhang K.","55139504300;7405449711;57192497946;56602062900;","Towards improving social communication skills with multimodal sensory information",2014,"IEEE Transactions on Industrial Informatics","10","1","6553174","323","330",,27,"10.1109/TII.2013.2271914","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898427098&doi=10.1109%2fTII.2013.2271914&partnerID=40&md5=0af429d5fdd0c6fb3ba296ff00d4d1fa","School of Computer Science, China University of Geosciences, Wuhan 430074, China; National Engineering Research Center for E-Learning, Central China Normal University, Wuhan 430079, China; National Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing 100875, China","Chen, J., National Engineering Research Center for E-Learning, Central China Normal University, Wuhan 430079, China; Chen, D., School of Computer Science, China University of Geosciences, Wuhan 430074, China; Li, X., National Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing 100875, China; Zhang, K., National Engineering Research Center for E-Learning, Central China Normal University, Wuhan 430079, China","How to improve social communication skills for children, especially those with social communication difficulties such as attention deficit/hyperactivity disorder, has long been a challenge faced by researchers and therapists. Recent research indicates that computer-assisted approaches may be effective in addressing this issue. This study aimed to understand children's behaviors and then provide appropriate support to improve their social communication skills. We have established an intelligent system, inside which a child can freely play interactive social skills games with virtual characters. The virtual characters can adjust their own behaviors by adapting to the child's cognitive state (e.g., focus of attention) and affective state (e.g., happiness or surprise). The child's behavior is identified in real-time by recognition of multimodal sensory information, which includes head pose and eye gaze estimation, gesture detection, and affective state detection supported by a series of algorithms proposed in this study. Furthermore, this intelligent system has been enabled in a nonintrusive manner using a novel approach of multicamera surveillance to provide the child with natural interaction with the system. Experimental results show the system can estimate a user's attention and affective states with correctness rates of 93% and 91.3%, respectively. The results obtained suggest that the methods have strong potential as alternative methods for sensing human behavior and providing appropriate support. © 2005-2012 IEEE.","Behavior detection; intelligent systems; multimodal sensory information; social communication skills","Communication; Gesture recognition; Intelligent systems; Affective state detection; Behavior detection; Focus of Attention; Gesture detections; Multi-camera surveillance; Natural interactions; Sensory information; Social communications; Education",Article,"Final","",Scopus,2-s2.0-84898427098
"Karakaya M., Bolme D., Boehnen C.","35223530800;8410729500;15622900000;","Eye gaze tracking using correlation filters",2014,"Proceedings of SPIE - The International Society for Optical Engineering","9024",,"90240U","","",,,"10.1117/12.2040266","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897439455&doi=10.1117%2f12.2040266&partnerID=40&md5=4d246b20c42e8edf1eeffaaa10991695","Meliksah Univ., Mevlana Mahallesi, Talas/Kayseri 38030, Turkey; Oak Ridge National Laboratory, 1 Bethel Valley Road, Oak Ridge, TN 37831, United States","Karakaya, M., Meliksah Univ., Mevlana Mahallesi, Talas/Kayseri 38030, Turkey; Bolme, D., Oak Ridge National Laboratory, 1 Bethel Valley Road, Oak Ridge, TN 37831, United States; Boehnen, C., Oak Ridge National Laboratory, 1 Bethel Valley Road, Oak Ridge, TN 37831, United States","In this paper, we studied a method for eye gaze tracking that provide gaze estimation from a standard webcam with a zoom lens and reduce the setup and calibration requirements for new users. Specifically, we have developed a gaze estimation method based on the relative locations of points on the top of the eyelid and eye corners. Gaze estimation method in this paper is based on the distances between top point of the eyelid and eye corner detected by the correlation filters. Advanced correlation filters were found to provide facial landmark detections that are accurate enough to determine the subjects gaze direction up to angle of approximately 4-5 degrees although calibration errors often produce a larger overall shift in the estimates. This is approximately a circle of diameter 2 inches for a screen that is arm's length from the subject. At this accuracy it is possible to figure out what regions of text or images the subject is looking but it falls short of being able to determine which word the subject has looked at.","correlation filters; Eye tracking; gaze estimation","Calibration; Computer vision; Edge detection; Face recognition; Calibration error; Correlation filters; Eye gaze tracking; Eye-tracking; Facial landmark detection; Gaze direction; Gaze estimation; Relative location; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84897439455
"Asteriadis S., Karpouzis K., Kollias S.","55936774500;6602860576;57193712526;","Visual focus of attention in non-calibrated environments using gaze estimation",2014,"International Journal of Computer Vision","107","3",,"293","316",,37,"10.1007/s11263-013-0691-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897415257&doi=10.1007%2fs11263-013-0691-3&partnerID=40&md5=5d56bc07d6ab9181735f22bdd5a771ea","Image, Video and Multimedia Systems Lab, National Technical University of Athens, 9 Iroon Polytechniou str, 157 80 Athens, Greece","Asteriadis, S., Image, Video and Multimedia Systems Lab, National Technical University of Athens, 9 Iroon Polytechniou str, 157 80 Athens, Greece; Karpouzis, K., Image, Video and Multimedia Systems Lab, National Technical University of Athens, 9 Iroon Polytechniou str, 157 80 Athens, Greece; Kollias, S., Image, Video and Multimedia Systems Lab, National Technical University of Athens, 9 Iroon Polytechniou str, 157 80 Athens, Greece","Estimating the focus of attention of a person highly depends on her/his gaze directionality. Here, we propose a new method for estimating visual focus of attention using head rotation, as well as fuzzy fusion of head rotation and eye gaze estimates, in a fully automatic manner, without the need for any special hardware or a priori knowledge regarding the user, the environment or the setup. Instead, we propose a system aimed at functioning under unpretending conditions, only with the usage of simple hardware, like a normal web-camera. Our system is aimed at functioning in a human-computer interaction environment, considering a person is facing a monitor with a camera adjusted on top. To this aim, we propose in this paper two novel techniques, based on local and appearance information, estimating head rotation, and we adaptively fuse them in a common framework. The system is able to recognize head rotational movement, under translational movements of the user towards any direction, without any knowledge or a-priori estimate of the user's distance from the camera or camera intrinsic parameters. © 2013 Springer Science+Business Media New York.","Eye Gaze estimation; Face tracking; Facial feature detection; Facial feature tracking; Head pose estimation; Human computer interaction; User attention estimation","Cameras; Hardware; Human computer interaction; Image recognition; Image segmentation; Eye-gaze; Face Tracking; Facial feature detection; Facial feature tracking; Head Pose Estimation; User attention; Estimation",Article,"Final","",Scopus,2-s2.0-84897415257
"Lu F., Okabe T., Sugano Y., Sato Y.","54956194300;7201390055;7005470045;35230954300;","Learning gaze biases with head motion for head pose-free gaze estimation",2014,"Image and Vision Computing","32","3",,"169","179",,59,"10.1016/j.imavis.2014.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894246243&doi=10.1016%2fj.imavis.2014.01.005&partnerID=40&md5=1da8144e961c1781c702a28fd3830f9d","University of Tokyo, Japan; Kyushu Institute of Technology, Japan; Institute of Industrial Science, University of Tokyo, Japan","Lu, F., University of Tokyo, Japan; Okabe, T., Kyushu Institute of Technology, Japan; Sugano, Y., University of Tokyo, Japan, Kyushu Institute of Technology, Japan; Sato, Y., University of Tokyo, Japan, Institute of Industrial Science, University of Tokyo, Japan","When estimating human gaze directions from captured eye appearances, most existing methods assume a fixed head pose because head motion changes eye appearance greatly and makes the estimation inaccurate. To handle this difficult problem, in this paper, we propose a novel method that performs accurate gaze estimation without restricting the user's head motion. The key idea is to decompose the original free-head motion problem into subproblems, including an initial fixed head pose problem and subsequent compensations to correct the initial estimation biases. For the initial estimation, automatic image rectification and joint alignment with gaze estimation are introduced. Then compensations are done by either learning-based regression or geometric-based calculation. The merit of using such a compensation strategy is that the training requirement to allow head motion is not significantly increased; only capturing a 5-s video clip is required. Experiments are conducted, and the results show that our method achieves an average accuracy of around 3 by using only a single camera. © 2014 Elsevier B.V.","Appearance-based approach; Free head motion; Gaze estimation; Head pose compensation","Electrical engineering; Appearance based approach; Compensation strategy; Free-head; Gaze estimation; Head pose; Image rectification; Initial estimation; Training requirement; Computer applications",Article,"Final","",Scopus,2-s2.0-84894246243
"Tseng P.-H., Paolozza A., Munoz D.P., Reynolds J.N., Itti L.","56835183800;55650529600;7103163217;7403603667;7003911079;","Deep learning on natural viewing behaviors to differentiate children with fetal alcohol spectrum disorder",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8206 LNCS",,,"178","185",,5,"10.1007/978-3-642-41278-3_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890863329&doi=10.1007%2f978-3-642-41278-3_22&partnerID=40&md5=db40506c14f29ccb2f7699adc51b41ea","Department of Computer Science, University of Southern California, United States; Centre for Neuroscience Studies, Queen's University, Canada","Tseng, P.-H., Department of Computer Science, University of Southern California, United States; Paolozza, A., Centre for Neuroscience Studies, Queen's University, Canada; Munoz, D.P., Centre for Neuroscience Studies, Queen's University, Canada; Reynolds, J.N., Centre for Neuroscience Studies, Queen's University, Canada; Itti, L., Department of Computer Science, University of Southern California, United States","Computational models of visual attention have attracted strong interest by accurately predicting how humans deploy attention. However, little research has utilized these models to detect clinical populations whose attention control has been affected by neurological disorders. We designed a framework to decypher disorders from the joint analysis of video and patients' natural eye movement behaviors (watch television for 5 minutes). We employ convolutional deep neural networks to extract visual features in real-time at the point of gaze, followed by SVM and Adaboost to classify typically developing children vs. children with fetal alcohol spectrum disorder (FASD), who exhibit impaired attentional control. The classifier achieved 74.1% accuracy (ROC: 0.82). Our results demonstrate that there is substantial information about attentional control in even very short recordings of natural viewing behavior. Our new method could lead to high-throughput, low-cost screening tools for identifying individuals with deficits in attentional control. © 2013 Springer-Verlag.","clinical populations; deep learning; eye movement; natural scenes; sparse representation","Clinical population; Computational model; Deep learning; Deep neural networks; Movement behavior; Natural scenes; Neurological disorders; Sparse representation; Adaptive boosting; Eye movements; Engineering education",Conference Paper,"Final","",Scopus,2-s2.0-84890863329
"Quintas J., Menezes P., Dias J.","37005875400;56592011600;56962751600;","Context-based perception and understanding of human intentions",2013,"Proceedings - IEEE International Workshop on Robot and Human Interactive Communication",,,"6628489","346","347",,3,"10.1109/ROMAN.2013.6628489","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889598686&doi=10.1109%2fROMAN.2013.6628489&partnerID=40&md5=c52585790e1ecff89c58c89abf60f519","Instituto Pedro Nunes, Coimbra, Portugal; University of Coimbra, Coimbra, Portugal; Khalifa University, Abu Dhabi, United Arab Emirates","Quintas, J., Instituto Pedro Nunes, Coimbra, Portugal; Menezes, P., University of Coimbra, Coimbra, Portugal; Dias, J., University of Coimbra, Coimbra, Portugal, Khalifa University, Abu Dhabi, United Arab Emirates","This work focus in the importance of context awareness and intention understanding capabilities in modern robots when faced with different situations. The objective is to be capable of providing new features for robots, which enable new real-world applications, and extend their autonomy, in terms of self-management and cooperation with humans or other systems. Gaze estimation and gesture interpretation are modalities, closely related with context-dependent human intention understanding, that are addressed in this work. © 2013 IEEE.",,"Context dependent; Context- awareness; Context-based; Cooperation with human; Gaze estimation; Human intentions; Intention understanding; Self management; Communication; Robots",Conference Paper,"Final","",Scopus,2-s2.0-84889598686
"Mora K.A.F., Odobez J.-M.","55336423200;57203103085;","Person independent 3D gaze estimation from remote RGB-D cameras",2013,"2013 IEEE International Conference on Image Processing, ICIP 2013 - Proceedings",,,"6738574","2787","2791",,40,"10.1109/ICIP.2013.6738574","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897817928&doi=10.1109%2fICIP.2013.6738574&partnerID=40&md5=0e5b8c828ac60f03d4d29567055c21b3","Idiap Research Institute, CH-1920, Martigny, Switzerland; École Polytechnique Fédérale de Lausanne, CH-1015, Lausanne, Switzerland","Mora, K.A.F., Idiap Research Institute, CH-1920, Martigny, Switzerland, École Polytechnique Fédérale de Lausanne, CH-1015, Lausanne, Switzerland; Odobez, J.-M., Idiap Research Institute, CH-1920, Martigny, Switzerland, École Polytechnique Fédérale de Lausanne, CH-1015, Lausanne, Switzerland","We address the problem of person independent 3D gaze estimation using a remote, low resolution, RGB-D camera. The approach relies on a sparse technique to reconstruct normalized eye test images from a gaze appearance model (a set of eye image/gaze pairs) and infer their gaze accordingly. In this context, the paper makes three contributions: (i) unlike most previous approaches, we exploit the coupling (and constraints) between both eyes to infer their gaze jointly; (ii) we show that a generic gaze appearance model built from the aggregation of person-specific models can be used to handle unseen users and compensate for appearance variations across people, since a test user eyes' appearance will be reconstructed from similar users within the generic model. (iii) we propose an automatic model selection method that leads to comparable performance with a reduced computational load. © 2013 IEEE.","3D gaze estimation; appearance based methods; person-independence; sparse reconstruction","Appearance modeling; Appearance-based methods; Automatic model selection; Computational loads; Gaze estimation; person-independence; Person-independent; Sparse reconstruction; Cameras; Face recognition; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-84897817928
"Sim N., Gavriel C., Abbott W.W., Faisal A.A.","57204251671;56102634300;55312983800;6602900233;","The head mouse - Head gaze estimation 'In-the-Wild' with low-cost inertial sensors for BMI use",2013,"International IEEE/EMBS Conference on Neural Engineering, NER",,,"6696039","735","738",,22,"10.1109/NER.2013.6696039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897734579&doi=10.1109%2fNER.2013.6696039&partnerID=40&md5=0c7696b159691b3f8405f73086287993","Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom; Department of Computing, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom; MRC Clinical Sciences Centre, Hammersmith Hospital Campus, W12 0NN London, United Kingdom","Sim, N., Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom; Gavriel, C., Department of Computing, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom; Abbott, W.W., Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom; Faisal, A.A., Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom, Department of Computing, Imperial College London, South Kensington Campus, London SW7 2AZ, United Kingdom, MRC Clinical Sciences Centre, Hammersmith Hospital Campus, W12 0NN London, United Kingdom","We present a wearable head-tracking device using inexpensive inertial sensors as an alternative head movement tracking system. This can be used as indicator of human movement intentions for Brain-Machine Interface (BMI) applications. Our system is capable of tracking head movements at high rates (100 Hz) and achieves R2 = 0.99 with a 2.5° RMSE against a ground-truth motion tracking system. The system tracks head movements over periods in the order of tens of minutes with little drift. The accuracy and precision of our system, together with its low response latency of ≈ 20 ms make it an unconventional but effective system for human-computer interfacing: the 'head mouse' controls the mouse cursor on a display based on head orientation alone, so that it matches the centre of a straight-onward looking user. Our head mouse is suitable for amputees and spinal chord injury patients who have lost control of their upper extremities. We show that naive test subjects are capable to write text using our system and an on-screen keyboard at a rate of 4.65 words/minute, compared to able bodied users using a physical computer mouse which reached 7.85 words/minute. Crucially we measure the natural head movements of able bodied computer users, and show that our approach falls within the range of natural head movement parameters. © 2013 IEEE.",,"Accuracy and precision; Brain machine interface; Effective systems; Head movement tracking; Human movements; Motion tracking system; On-screen keyboard; Spinal chord injuries; Inertial navigation systems; Tracking (position); Mammals",Conference Paper,"Final","",Scopus,2-s2.0-84897734579
"Kadyrov A., Yu H., Eyles J., Liu H.","56248827600;56115992300;56028581900;54958434200;","Explore new eye tracking and gaze locating methods",2013,"Proceedings - 2013 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2013",,,"6722242","2866","2871",,3,"10.1109/SMC.2013.489","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893566125&doi=10.1109%2fSMC.2013.489&partnerID=40&md5=e60568d8194867c0eaa5cf5a68ab8cd0","School of Creative Technologies, University of Portsmouth, United Kingdom","Kadyrov, A., School of Creative Technologies, University of Portsmouth, United Kingdom; Yu, H., School of Creative Technologies, University of Portsmouth, United Kingdom; Eyles, J., School of Creative Technologies, University of Portsmouth, United Kingdom; Liu, H., School of Creative Technologies, University of Portsmouth, United Kingdom","Eye tracking has been used extensively in research, often for plotting the gaze location of research participants. Many of the existing methods on tracking gaze locations, however, require special equipment. This equipment can be very expensive and/or cumbersome to use, restricting the use of it to specialist labs. By producing a method of eye tracking which requires minimal equipment and set up time, eye tracking might be more widely used as a research tool, especially in exploratory areas where the outcomes of the research are not known. This paper introduces two novel eye tracking methods which use only a webcam feed as input and require no specialist equipment. The camera used in this research is a standard webcam built into a normal laptop; any current commercial webcam could be used. Experiments using the proposed method showed more accurate tracking results than some existing eye trackers. © 2013 IEEE.","Circle detection; Eye tracking; Gaze estimation; Player experience","Accurate tracking; Circle detection; Eye tracking methods; Eye-tracking; Gaze estimation; Player experience; Research tools; Specialist equipments; Cybernetics; Equipment; Research",Conference Paper,"Final","",Scopus,2-s2.0-84893566125
"Funes Mora K.A.","55336423200;","3D head pose and gaze tracking and their application to diverse multimodal tasks",2013,"ICMI 2013 - Proceedings of the 2013 ACM International Conference on Multimodal Interaction",,,,"345","348",,,"10.1145/2522848.2532192","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892590822&doi=10.1145%2f2522848.2532192&partnerID=40&md5=dbdc61b44b2c41a25672d3f266b62cc1","Idiap Research Institute, École Polytechnique Fédérale de Lausanne, Switzerland","Funes Mora, K.A., Idiap Research Institute, École Polytechnique Fédérale de Lausanne, Switzerland","In this PhD thesis the problem of 3D head pose and gaze tracking from minimal user cooperation is addressed. By exploiting characteristics of RGB-D sensors, contributions have been made related to consequent problems of the lack of cooperation: in particular, head pose and inter-person appearance variability; in addition to low resolution handling. The resulting system enabled diverse multimodal applications. In particular, recent work combined multiple RGB-D sensors to detect gazing events in dyadic interactions. The research plan consists of: i) Improving the robustness, accuracy and usability of the head pose and gaze tracking system; ii) To use additional multimodal cues, such as speech and dynamic context, to train and adapt gaze models in an unsupervised manner; iii) To extend the application of 3D gaze estimation to diverse multimodal applications. This includes visual focus of attention tasks involving multiple visual targets, e.g.∼people in a meeting-like setup. © 2013 Author.","gaze estimation; hci; head pose tracking; hhi; hri; speech","Dyadic interaction; Gaze estimation; Gaze tracking system; Head-pose tracking; hhi; hri; Multimodal application; Visual focus of attentions; Gesture recognition; Human computer interaction; Interactive computer systems; Sensors; Speech; Three dimensional; Tracking (position); Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84892590822
"Sheikhi S., Khalidov V., Klotz D., Wrede B., Odobez J.-M.","35093209600;25927242400;55052552700;8297851900;57203103085;","Leveraging the robot dialog state for visual focus of attention recognition",2013,"ICMI 2013 - Proceedings of the 2013 ACM International Conference on Multimodal Interaction",,,,"107","110",,2,"10.1145/2522848.2522881","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892587338&doi=10.1145%2f2522848.2522881&partnerID=40&md5=a45b01efd86dd4ade03d7868d1f1442e","Idiap Research Institute, Martigny, Switzerland; Bielefeld University, Bielefeld, Germany","Sheikhi, S., Idiap Research Institute, Martigny, Switzerland; Khalidov, V., Idiap Research Institute, Martigny, Switzerland; Klotz, D., Bielefeld University, Bielefeld, Germany; Wrede, B., Bielefeld University, Bielefeld, Germany; Odobez, J.-M., Idiap Research Institute, Martigny, Switzerland","The Visual Focus of Attention (what or whom a person is looking at) or VFOA is a fundamental cue in non-verbal communication and plays an important role when designing effective human-machine interaction systems. However, recognizing the VFOA of an interacting person is difficult for a robot, since due to low resolution imaging, eye gaze estimation is not possible. Rather, head pose cue is used as a substitute for gaze, but leads to ambiguities in its interpretation as VFOA indicator. In this paper, we investigate the use of the robot conversational state, which the robot is aware of, as contextual information to improve VFOA recognition from head pose. We propose a dynamic Bayesian model that accounts for the robot state (speaking status, person he addresses, reference to objects) along with a dynamic head-to-gaze mapping function. Experiments on a publicly available human-robot interaction dataset, where a humanoid robot plays the role of an art guide and quiz master, shows that using such conversational context is effective in improving VFOA. © 2013 ACM.","dialog context; gaze recognition; HCI; HRI; VFOA","Contextual information; dialog context; gaze recognition; HRI; Human machine interaction; Non-verbal communications; VFOA; Visual focus of attentions; Anthropomorphic robots; Bayesian networks; Human computer interaction; Image reconstruction; Interactive computer systems; Man machine systems",Conference Paper,"Final","",Scopus,2-s2.0-84892587338
"Li N., Busso C.","55805334600;35742852700;","Evaluating the robustness of an appearance-based gaze estimation method for multimodal interfaces",2013,"ICMI 2013 - Proceedings of the 2013 ACM International Conference on Multimodal Interaction",,,,"91","98",,7,"10.1145/2522848.2522876","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892584903&doi=10.1145%2f2522848.2522876&partnerID=40&md5=472987929671e6b10de4bb41fc360cd2","Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX 75080, United States","Li, N., Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX 75080, United States; Busso, C., Multimodal Signal Processing (MSP) Laboratory, University of Texas at Dallas, 800 W Campbell Rd, Richardson, TX 75080, United States","Given the crucial role of eye movements on visual attention, tracking gaze behaviors is an important research problem in various applications including biometric identification, attention modeling and human-computer interaction. Most of the existing gaze tracking methods require a repetitive system calibration process and are sensitive to the user's head movements. Therefore, they cannot be easily implemented in current multimodal interfaces. This paper investigates an appearance-based approach for gaze estimation that requires minimum calibration and is robust against head motion. The approach consists in building an orthonormal basis, or eigenspace, of the eye appearance with principal component analysis (PCA). Unlike previous studies, we build the eigenspace using image patches displaying both eyes. The projections into the basis are used to train regression models which predict the gaze location. The approach is trained and tested with a new multimodal corpus introduced in this paper. We consider several variables such as the distance between user and the computer monitor, and head movement. The evaluation includes the performance of the proposed gaze estimation system with and without head movement. It also evaluates the results in subject-dependent versus subject-independent conditions under different distances. We report promising results which suggest that the proposed gaze estimation approach is a feasible and flexible scheme to facilitate gaze-based multimodal interfaces. © 2013 ACM.","computer user interface; eigenspace analysis; gaze estimation; multimodal interfaces","Appearance-based approaches; Biometric identifications; Computer users; Eigen space analysis; Gaze estimation; Multi-modal interfaces; Research problems; System calibration; Eye movements; Gesture recognition; Principal component analysis; Regression analysis; User interfaces; Interactive computer systems",Conference Paper,"Final","",Scopus,2-s2.0-84892584903
"Funes-Mora K.A., Nguyen L., Gatica-Perez D., Odobez J.-M.","55336423200;57207122948;6602722700;57203103085;","A semi-automated system for accurate gaze coding in natural dyadic interactions",2013,"ICMI 2013 - Proceedings of the 2013 ACM International Conference on Multimodal Interaction",,,,"87","90",,7,"10.1145/2522848.2522884","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892566620&doi=10.1145%2f2522848.2522884&partnerID=40&md5=7b9ba0bfc3a6f8ae81fa8c6991490e18","Idiap Research Institute, École Polytechnique Fédérale de Lausanne (EPFL), Martigny, Valais, Switzerland","Funes-Mora, K.A., Idiap Research Institute, École Polytechnique Fédérale de Lausanne (EPFL), Martigny, Valais, Switzerland; Nguyen, L., Idiap Research Institute, École Polytechnique Fédérale de Lausanne (EPFL), Martigny, Valais, Switzerland; Gatica-Perez, D., Idiap Research Institute, École Polytechnique Fédérale de Lausanne (EPFL), Martigny, Valais, Switzerland; Odobez, J.-M., Idiap Research Institute, École Polytechnique Fédérale de Lausanne (EPFL), Martigny, Valais, Switzerland","In this paper we propose a system capable of accurately coding gazing events in natural dyadic interactions. Contrary to previous works, our approach exploits the actual continuous gaze direction of a participant by leveraging on remote RGB-D sensors and a head pose-independent gaze estimation method. Our contributions are: i) we propose a system setup built from low-cost sensors and a technique to easily calibrate these sensors in a room with minimal assumptions; ii) we propose a method which, provided short manual annotations, can automatically detect gazing events in the rest of the sequence; iii) we demonstrate on substantially long, natural dyadic data that high accuracy can be obtained, showing the potential of our system. Our approach is non-invasive and does not require collaboration from the interactors. These characteristics are highly valuable in psychology and sociology research. © 2013 ACM.","gaze event detection; RGB-D cameras; social interaction","Dyadic interaction; Event detection; Gaze estimation; Low-cost sensors; Manual annotation; Rgb-d cameras; Semi-automated systems; Social interactions; Automation; Interactive computer systems; Social sciences; Sensors",Conference Paper,"Final","",Scopus,2-s2.0-84892566620
"Han K., Wang X., Zhang Z., Zhao H.","57197058785;55981036800;55787974800;54685658800;","A novel remote eye gaze tracking approach with dynamic calibration",2013,"2013 IEEE International Workshop on Multimedia Signal Processing, MMSP 2013",,,"6659273","111","116",,4,"10.1109/MMSP.2013.6659273","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892503341&doi=10.1109%2fMMSP.2013.6659273&partnerID=40&md5=9e8180853677f06f8b6f0a8184b2a10c","Computer Application Research Center, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen 518055, China","Han, K., Computer Application Research Center, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen 518055, China; Wang, X., Computer Application Research Center, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen 518055, China; Zhang, Z., Computer Application Research Center, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen 518055, China; Zhao, H., Computer Application Research Center, Harbin Institute of Technology Shenzhen Graduate School, Shenzhen 518055, China","Point of gaze estimation is the most important part of remote eye gaze tracking techniques. Though there are various point of gaze detection and estimation methods, few of them can satisfy the expectation of widely use. One primary reason is lack of accurate calibration method. In order to deal with this problem, an adaptive calibration technique is proposed based on cross-ratio. It can compensate estimation bias much better. Also, efficient remote eye gaze tracking parameters estimation procedures are employed in this paper. The eye gaze tracking approach in this paper allows users to have free head motion and has high accuracy rate, and it is based on infrared radiation (IR) light sources reflection on cornea. Experimental results demonstrate that considerable improvement can be achieved. © 2013 IEEE.",,"Accuracy rate; Adaptive calibration; Calibration method; Dynamic calibration; Estimation bias; Eye gaze tracking; Parameters estimation; Point of gaze; Calibration; Infrared radiation; Light sources; Multimedia signal processing; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84892503341
"Reale M.J., Liu P., Yin L., Canavan S.","27868052800;57191070080;7203060635;24490527800;","Art critic: Multisignal vision and speech interaction system in a gaming context",2013,"IEEE Transactions on Cybernetics","43","6","6572826","1546","1559",,6,"10.1109/TCYB.2013.2271606","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890012302&doi=10.1109%2fTCYB.2013.2271606&partnerID=40&md5=ac385f8f82b052b901e48a2b389c5a01","Department of Computer Science, State University of New York at Binghamton, Binghamton, NY 13902, United States","Reale, M.J., Department of Computer Science, State University of New York at Binghamton, Binghamton, NY 13902, United States; Liu, P., Department of Computer Science, State University of New York at Binghamton, Binghamton, NY 13902, United States; Yin, L., Department of Computer Science, State University of New York at Binghamton, Binghamton, NY 13902, United States; Canavan, S., Department of Computer Science, State University of New York at Binghamton, Binghamton, NY 13902, United States","True immersion of a player within a game can only occur when the world simulated looks and behaves as close to reality as possible. This implies that the game must correctly read and understand, among other things, the player's focus, attitude toward the objects/persons in focus, gestures, and speech. In this paper, we proposed a novel system that integrates eye gaze estimation, head pose estimation, facial expression recognition, speech recognition, and text-to-speech components for use in real-time games. Both the eye gaze and head pose components utilize underlying 3-D models, and our novel head pose estimation algorithm uniquely combines scene flow with a generic head model. The facial expression recognition module uses the local binary patterns with three orthogonal planes approach on the 2-D shape index domain rather than the pixel domain, resulting in improved classification. Our system has also been extended to use a pan-tilt-zoom camera driven by the Kinect, allowing us to track a moving player. A test game, Art Critic, is also presented, which not only demonstrates the utility of our system but also provides a template for player/non-player character (NPC) interaction in a gaming context. The player alters his/her view of the 3-D world using head pose, looks at paintings/NPCs using eye gaze, and makes an evaluation based on the player's expression and speech. The NPC artist will respond with facial expression and synthetic speech based on its personality. Both qualitative and quantitative evaluations of the system are performed to illustrate the system's effectiveness. © 2013 IEEE.","Expression recognition; Gaming interaction; Gaze tracking; Head pose estimation; Speech recognition; Text-to-speech","Expression recognition; Gaming interaction; Gaze tracking; Head Pose Estimation; Text to speech; Character recognition; Face recognition; Speech recognition; Speech synthesis; Three dimensional computer graphics; Gesture recognition; algorithm; article; automatic speech recognition; computer interface; eye movement; facial expression; head movement; human; male; methodology; physiology; recreation; sound detection; three dimensional imaging; young adult; Algorithms; Eye Movements; Facial Expression; Head Movements; Humans; Imaging, Three-Dimensional; Male; Sound Spectrography; Speech Recognition Software; User-Computer Interface; Video Games; Young Adult",Article,"Final","",Scopus,2-s2.0-84890012302
"Song F., Tan X., Chen S., Zhou Z.-H.","25928041500;8876261800;7410249270;55365262700;","A literature survey on robust and efficient eye localization in real-life scenarios",2013,"Pattern Recognition","46","12",,"3157","3173",,58,"10.1016/j.patcog.2013.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881062562&doi=10.1016%2fj.patcog.2013.05.009&partnerID=40&md5=6ff424a6c1cf2e26700816ab347ef5b2","Department of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Yudao Street 29, Nanjing 210016, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China","Song, F., Department of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Yudao Street 29, Nanjing 210016, China; Tan, X., Department of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Yudao Street 29, Nanjing 210016, China; Chen, S., Department of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Yudao Street 29, Nanjing 210016, China; Zhou, Z.-H., National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China","Eye localization has gained a wide range of applications in face recognition, gaze estimation, pose estimation, expression analysis, etc. However, due to the high degree of appearance variability of eyes in size, shape, color, texture and various ambient environment changes, this task is challenging. During the last three decades, numerous techniques have been developed to meet these challenges. The goal of this paper is to categorize and evaluate these algorithms in a comprehensive way. We focus on the overall difficulties and challenges in real-life scenarios, and present a detailed review of prominent algorithms from the perspective of learning generalizable, flexible and efficient statistical eye models from a small number of training images. In addition, we organize the discussion of the global aspects of eye localization in uncontrolled environments, towards the development of a robust eye localization system. This paper concludes with several promising directions for future research. © 2013 Elsevier Ltd.","Computer vision; Eye localization; Survey","Ambient environment; Expression analysis; Eye localization; Gaze estimation; Global aspects; Literature survey; Pose estimation; Training image; Algorithms; Computer vision; Surveying; Surveys; Face recognition",Article,"Final","",Scopus,2-s2.0-84881062562
"Smith B.A., Yin Q., Feiner S.K., Nayar S.K.","18038896900;55929059900;7005949369;35560595700;","Gaze locking: Passive eye contact detection for human-object interaction",2013,"UIST 2013 - Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology",,,,"271","280",,146,"10.1145/2501988.2501994","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887581920&doi=10.1145%2f2501988.2501994&partnerID=40&md5=904bec7d4c41ac482a50bfc099fe3139","Department of Computer Science, Columbia University, 450 Comp. Sci. Bldg., 1214 Amsterdam Ave., New York, NY 10027, United States","Smith, B.A., Department of Computer Science, Columbia University, 450 Comp. Sci. Bldg., 1214 Amsterdam Ave., New York, NY 10027, United States; Yin, Q., Department of Computer Science, Columbia University, 450 Comp. Sci. Bldg., 1214 Amsterdam Ave., New York, NY 10027, United States; Feiner, S.K., Department of Computer Science, Columbia University, 450 Comp. Sci. Bldg., 1214 Amsterdam Ave., New York, NY 10027, United States; Nayar, S.K., Department of Computer Science, Columbia University, 450 Comp. Sci. Bldg., 1214 Amsterdam Ave., New York, NY 10027, United States","Eye contact plays a crucial role in our everyday social interactions. The ability of a device to reliably detect when a person is looking at it can lead to powerful human-object interfaces. Today, most gaze-based interactive systems rely on gaze tracking technology. Unfortunately, current gaze tracking techniques require active infrared illumination, calibration, or are sensitive to distance and pose. In this work, we propose a different solution-a passive, appearance-based approach for sensing eye contact in an image. By focusing on gaze locking rather than gaze tracking, we exploit the special appearance of direct eye gaze, achieving a Matthews correlation coefficient (MCC) of over 0.83 at long distances (up to 18 m) and large pose variations (up to ±30° of head yaw rotation) using a very basic classifier and without calibration. To train our detector, we also created a large publicly available gaze data set: 5,880 images of 56 people over varying gaze directions and head poses. We demonstrate how our method facilitates human-object interaction, user analytics, image filtering, and gaze-triggered photography. Copyright © 2013 ACM.","Gaze-based interaction; Gaze-triggered photography; Human vision; Human-object interaction; Image filtering; Passive eye contact detection; User analytics","Eye contact; Gaze-based interaction; Human vision; Human-object interaction; Image filtering; User analytics; Calibration; Face recognition; Locks (fasteners); Photography; Tracking (position); User interfaces; Detectors",Conference Paper,"Final","",Scopus,2-s2.0-84887581920
"Sun Y.-W., Chiang C.-K., Lai S.-H.","57191647344;55462217500;7402937330;","Integrating eye tracking and motion sensor on mobile phone for interactive 3D display",2013,"Proceedings of SPIE - The International Society for Optical Engineering","8856",,"88560F","","",,2,"10.1117/12.2026577","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887003042&doi=10.1117%2f12.2026577&partnerID=40&md5=eebe95e079dcb3a67e74e4d2d88254f7","Dept. of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","Sun, Y.-W., Dept. of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Chiang, C.-K., Dept. of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Lai, S.-H., Dept. of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","In this paper, we propose an eye tracking and gaze estimation system for mobile phone. We integrate an eye detector, cornereye center and iso-center to improve pupil detection. The optical flow information is used for eye tracking. We develop a robust eye tracking system that integrates eye detection and optical-flow based image tracking. In addition, we further incorporate the orientation sensor information from the mobile phone to improve the eye tracking for accurate gaze estimation. We demonstrate the accuracy of the proposed eye tracking and gaze estimation system through experiments on some public video sequences as well as videos acquired directly from mobile phone. © 2013 SPIE.",,"Eye tracking systems; Flow informations; Gaze estimation; Image tracking; Motion sensors; Orientation sensors; Pupil detection; Video sequences; Eye protection; Gesture recognition; Image processing; Mobile phones; Sensors; Cellular telephones",Conference Paper,"Final","",Scopus,2-s2.0-84887003042
"Blignaut P.","6602384906;","A new mapping function to improve the accuracy of a video-based eye tracker",2013,"ACM International Conference Proceeding Series",,,,"56","59",,14,"10.1145/2513456.2513461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886285297&doi=10.1145%2f2513456.2513461&partnerID=40&md5=c60d7b2ddb40229c94fb38692179bc01","Department of Computer Science and Informatics, University of the Free State, South Africa","Blignaut, P., Department of Computer Science and Informatics, University of the Free State, South Africa","The accuracy that can be achieved with a simple one camera, one IR source video-based eye tracker is evaluated for various combinations of calibration set and mapping model. It is shown that, using regression-based gaze estimation, accuracy varies across the display. A good mapping model would be one that results in a small average error as well as a small number of points with larger errors. An average error of less than 0.6° can be achieved when at least 12 calibration points are used in combination with a newly derived mapping model. Copyright 2013 ACM.","Accuracy; Calibration; Eye tracking; Mapping functions","Accuracy; Average errors; Calibration points; Eye trackers; Eye-tracking; Gaze estimation; Mapping functions; Mapping model; Computer science; Engineers; Information technology; Mapping; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84886285297
"Nguyen P., Fleureau J., Chamaret C., Guillotel P.","57198903963;23388852400;26424247200;6506461641;","Calibration-free gaze tracking using particle filter",2013,"Proceedings - IEEE International Conference on Multimedia and Expo",,,"6607532","","",,7,"10.1109/ICME.2013.6607532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885573078&doi=10.1109%2fICME.2013.6607532&partnerID=40&md5=28418f541594022773044e40ec402a32","Technicolor, 975 Ave. des Champs Blancs, 35576 Cesson-Sevigne, France","Nguyen, P., Technicolor, 975 Ave. des Champs Blancs, 35576 Cesson-Sevigne, France; Fleureau, J., Technicolor, 975 Ave. des Champs Blancs, 35576 Cesson-Sevigne, France; Chamaret, C., Technicolor, 975 Ave. des Champs Blancs, 35576 Cesson-Sevigne, France; Guillotel, P., Technicolor, 975 Ave. des Champs Blancs, 35576 Cesson-Sevigne, France","This paper presents a novel approach for gaze estimation using only a low-cost camera and requiring no calibration. The main idea is based on the center-bias property of human gaze distribution to get a coarse estimate of the current gaze position as well as benefit from temporal information to enhance this rough gaze estimate. Firstly, we propose a method for detecting the eye center location and a mapping model based on the center-bias effect to convert it to gaze position. This initial gaze estimate then serves to construct the likelihood model of the eye-appearance. The final gaze position is estimated by fusing the likelihood model with the prior information obtained from previous observations on the basis of the particle filtering framework. Extensive experiments demonstrate the good performance of the proposed system with an average estimation error of 3.43° which outperforms state-of-the-art methods. Furthermore, the low complexity of the proposed system makes it suitable for real-time applications. © 2013 IEEE.","calibration-free; eye center detection; gaze estimation; HCI; particle filter","calibration-free; Center detection; Eye center locations; Gaze estimation; Particle filter; Real-time application; State-of-the-art methods; Temporal information; Calibration; Estimation; Exhibitions; Human computer interaction; Monte Carlo methods; Signal filtering and prediction",Conference Paper,"Final","",Scopus,2-s2.0-84885573078
"Mantiuk R., Markowski M.","22433849100;36241934700;","Gaze-dependent tone mapping",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7950 LNCS",,,"426","433",,11,"10.1007/978-3-642-39094-4_48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884471612&doi=10.1007%2f978-3-642-39094-4_48&partnerID=40&md5=07abd32060822d589f6989043bdb26b2","West Pomeranian University of Technology, Szczecin, Faculty of Computer Science, Zołnierska 52, 71-210, Szczecin, Poland","Mantiuk, R., West Pomeranian University of Technology, Szczecin, Faculty of Computer Science, Zołnierska 52, 71-210, Szczecin, Poland; Markowski, M., West Pomeranian University of Technology, Szczecin, Faculty of Computer Science, Zołnierska 52, 71-210, Szczecin, Poland","In this paper we model the process of temporal adaptation of the human visual system to varying luminance conditions. An eye tracker is used to capture the location of an observer's gaze in a high dynamic range image displayed on the screen. We apply a novel technique of eye tracker data filtering to avoid flickering caused by incorrect gaze estimation. Temporary adaptation luminance is then determined in the area surrounding the gaze point. We use its value to compress the high dynamic range image and display it on the low dynamic range display. The applied tone mapping technique uses a global compression curve in which location is shifted along the luminance axis according to a value of the adaptation luminance. This technique models the natural process of adaptation occurring in the human eyes, also taking into account the time-dependent visual adaptation to dark and bright backgrounds. © 2013 Springer-Verlag.","eye tracking; gaze-dependent tone mapping; high dynamic range images; luminance adaptation; tone mapping operators","Compression curves; Eye-tracking; High dynamic range images; Human Visual System; Low dynamic range; Luminance conditions; Tone mapping; Tone mapping operators; Image analysis; Mapping; Luminance",Conference Paper,"Final","",Scopus,2-s2.0-84884471612
[无可用作者姓名],[无可用的作者 ID],"Proceedings of the 2013 Conference on Eye Tracking South Africa, ETSA 2013",2013,"ACM International Conference Proceeding Series",,,,"","",81,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883872397&partnerID=40&md5=2851ef805dcc47c89cb5ba964c0fda54",,"","The proceedings contain 17 papers. The topics discussed include: shedding light on retail environments; visual perception of international traffic signs: influence of e-learning and culture on eye movements; appearance-based gaze tracking with spectral clustering and semi-supervised Gaussian process regression; reading on-screen text with gaze-based auto-scrolling; a regression-based method for the prediction of the indecisiveness degree through eye movement patterns; the effect of mapping function on the accuracy of a video-based eye tracker; saccade deviation indicators for automated eye tracking analysis; dealing with head-mounted eye-tracking data: comparison of a frame-by-frame and a fixation-based analysis; circular heat map transition diagram; measuring the impact of subtitles on cognitive load: eye tracking and dynamic audiovisual texts; and a new interaction technique involving eye gaze tracker and scanning system.",,,Conference Review,"Final","",Scopus,2-s2.0-84883872397
"Liang K., Chahir Y., Molina M., Tijus C., Jouen F.","55854045600;6506225899;7202281141;6603707134;6603701102;","Appearance-based gaze tracking with spectral clustering and semi-supervised Gaussian process regression",2013,"ACM International Conference Proceeding Series",,,,"17","23",,18,"10.1145/2509315.2509318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883872169&doi=10.1145%2f2509315.2509318&partnerID=40&md5=fa7640503a9eef9fb82b20154b002be5","CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus, 75014 Paris, France; Computer Science Department, GREYC-UMR CNRS 6072, University of Caen, Caen, France; PALM Laboratory EA 4649, University of Caen, Caen, France","Liang, K., CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus, 75014 Paris, France; Chahir, Y., Computer Science Department, GREYC-UMR CNRS 6072, University of Caen, Caen, France; Molina, M., PALM Laboratory EA 4649, University of Caen, Caen, France; Tijus, C., CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus, 75014 Paris, France; Jouen, F., CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus, 75014 Paris, France","Two of the challenges in appearance-based gaze tracking are: 1) prediction accuracy, 2) the efficiency of calibration process, which can be considered as the collection and analysis phase of labelled and unlabelled eye data. In this paper, we introduce an appearance-based gaze tracking model with a rapid calibration. First we propose to concatenate local eye appearance Center-Symmetric Local Binary Pattern(CS-LBP) descriptor for each subregion of eye image to form an eye appearance feature vector. The spectral clustering is then introduced to get the supervision information of eye manifolds on-line. Finally, taking advantage of eye manifold structure, a sparse semi-supervised Gaussian Process Regression(GPR) method is applied to estimate the subject's gaze coordinates. Experimental results demonstrate that our system with an efficient and accurate 5-points calibration not only can reduce the run-time cost but also can lead to a better accuracy result of 0.9°. © 2013 ACM.","appearance-based gaze estimation; Gaussian process regression; spectral clustering","Appearance based; Calibration process; Center-symmetric local binary patterns; Gaussian process regression; Gaze estimation; Manifold structures; Prediction accuracy; Spectral clustering; Gaussian distribution; Gaussian noise (electronic); Regression analysis; Response time (computer systems); Tracking (position); Calibration",Conference Paper,"Final","",Scopus,2-s2.0-84883872169
"Mansanet J., Albiol A., Paredes R., Mossi J.M., Albiol A.","55839953100;7003872793;7007168408;6603278304;7003872794;","Estimating point of regard with a consumer camera at a distance",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7887 LNCS",,,"881","888",,3,"10.1007/978-3-642-38628-2_104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883153993&doi=10.1007%2f978-3-642-38628-2_104&partnerID=40&md5=8702ce0c139ce64985a78a276fe22269","iTEAM - Instituto de Telecomunicaciones Y Aplicaciones Multimedia, Universitat Politècnica de València, Valencia, Spain; ITI - Insitituto Tecnológico de Informática, Universitat Politècnica de València, Valencia, Spain","Mansanet, J., iTEAM - Instituto de Telecomunicaciones Y Aplicaciones Multimedia, Universitat Politècnica de València, Valencia, Spain; Albiol, A., iTEAM - Instituto de Telecomunicaciones Y Aplicaciones Multimedia, Universitat Politècnica de València, Valencia, Spain; Paredes, R., ITI - Insitituto Tecnológico de Informática, Universitat Politècnica de València, Valencia, Spain; Mossi, J.M., iTEAM - Instituto de Telecomunicaciones Y Aplicaciones Multimedia, Universitat Politècnica de València, Valencia, Spain; Albiol, A., iTEAM - Instituto de Telecomunicaciones Y Aplicaciones Multimedia, Universitat Politècnica de València, Valencia, Spain","In this work, we have studied the viability of a novel technique to estimate the POR that only requires video feed from a consumer camera. The system can work under uncontrolled light conditions and does not require any complex hardware setup. To that end we propose a system that uses PCA feature extraction from the eyes region followed by non-linear regression. We evaluated three state of the art non-linear regression algorithms. In the study, we also compared the performance using a high quality webcam versus a Kinect sensor. We found, that despite the relatively low quality of the Kinect images it achieves similar performance compared to the high quality camera. These results show that the proposed approach could be extended to estimate POR in a completely non-intrusive way. © 2013 Springer-Verlag.","eye tracking; gaze estimation; human computer interaction (HCI); point-of-regard","Complex hardware; Eye-tracking; Gaze estimation; Human computer interaction (HCI); Non-linear regression; Pca feature extractions; point-of-regard; State of the art; Estimation; Feature extraction; Image analysis; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-84883153993
"Rozado D.","36609200700;","Mouse and keyboard cursor warping to accelerate and reduce the effort of routine HCI input tasks",2013,"IEEE Transactions on Human-Machine Systems","43","5","6626613","487","493",,13,"10.1109/THMS.2013.2281852","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893070277&doi=10.1109%2fTHMS.2013.2281852&partnerID=40&md5=7ec85fc5231986c173495bcf4a8401eb","Autonomous Systems Lab, CSIRO Computational Informatics, Pullenvale, QLD 4069, Australia","Rozado, D., Autonomous Systems Lab, CSIRO Computational Informatics, Pullenvale, QLD 4069, Australia","Gaze tracking has been suggested as an alternative to traditional computer pointing mechanisms. However, the accuracy limitations of gaze estimation algorithms and the fatigue imposed on users when overloading the visual perceptual channel with a motor control task have prevented the widespread adoption of gaze as a pointing modality. Rather than using gaze as a complete pointingmechanism, this study investigates the usage of gaze to complement traditional keyboard/mouse cursor positioning methods during standard human-computer interaction (HCI). With this approach, bringing the mouse/keyboard cursor to a target still requires a manual action, but the time and effort involved are substantially reduced in terms of mouse movement amplitude incurred or number of keystrokes pressed. This is accomplished by the cursor warping from its original position on the screen to the estimated point of regard of the user on the screen as estimated by video-oculography gaze tracking when a keystroke or mousemovement event is detected.The user adjusts the final fine-grained positioning of the cursor manually. The results of the user study carried out here on the effects of cursor warping in common computer input operations that involve cursor repositioning when using one or severalmonitors as well as on its learning dynamics over time show that cursor warping can speed up and/or reduce the physical effort required to complete tasks such as mouse/trackpad target acquisition, keyboard text cursor positioning, mouse/keyboard based text selection, and drag and drop operations. © 2013 IEEE.","Attentive user interface; Cursor warping; Eye tracking; Gaze aware interfaces; Gaze tracking; Human computer interaction (HCI); Human factors; Repetitive stress injury (RSI)","Attentive user interfaces; Cursor warping; Eye-tracking; Gaze tracking; Human computer interaction (HCI); Repetitive stress injuries; Human computer interaction; Human engineering; User interfaces; Tracking (position)",Article,"Final","",Scopus,2-s2.0-84893070277
"Martinez F., Carbone A., Pissaloux E.","39261924600;16174463200;7003850978;","Combining first-person and third-person gaze for attention recognition",2013,"2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2013",,,"6553735","","",,10,"10.1109/FG.2013.6553735","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881493085&doi=10.1109%2fFG.2013.6553735&partnerID=40&md5=3b41bfe9ae6363bcb4ba1fa6ce2da192","ISIR, Université Pierre et Marie Curie, CNRS UMR 7222, France","Martinez, F., ISIR, Université Pierre et Marie Curie, CNRS UMR 7222, France; Carbone, A., ISIR, Université Pierre et Marie Curie, CNRS UMR 7222, France; Pissaloux, E., ISIR, Université Pierre et Marie Curie, CNRS UMR 7222, France","This paper presents a method to recognize attentional behaviors from a head-mounted binocular eye tracker in triadic interactions. By taking advantage of the first-person view, we simultaneously estimate the first-person and third-person gaze. The first-person gaze is computed using an appearance-based method relying on local features. In parallel, head pose tracking allows determining the coarse gaze of people in the scene camera. Finally, knowing the first- and third-person gaze direction, scores are computed which permit to assign attention patterns to each frame. Our contributions are the followings: (i) head pose estimation based on localized regression, (ii) attention analysis, in particular mutual and shared gaze, including the first-person gaze, (iii) experiments conducted using a head-mounted appearance-based gaze tracker. Experiments on recorded data show encouraging results. © 2013 IEEE.",,"Appearance based; Appearance-based methods; Attention recognition; Gaze direction; Head Pose Estimation; Head-pose tracking; Local feature; Localized regression; Experiments; Image recognition; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84881493085
"Karakaya M., Barstow D., Santos-Villalobos H., Thompson J., Bolme D., Boehnen C.","35223530800;55546623900;16043812000;55492720800;8410729500;15622900000;","Gaze estimation for off-Angle iris recognition based on the biometric eye model",2013,"Proceedings of SPIE - The International Society for Optical Engineering","8712",,"87120F","","",,2,"10.1117/12.2018614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881038636&doi=10.1117%2f12.2018614&partnerID=40&md5=9640b0cf1aeb267fd00431ca482e7016","Oak Ridge National Laboratory, Imaging, Signals, and Machine Learning Group, Oak Ridge, TN 37831, United States","Karakaya, M., Oak Ridge National Laboratory, Imaging, Signals, and Machine Learning Group, Oak Ridge, TN 37831, United States; Barstow, D., Oak Ridge National Laboratory, Imaging, Signals, and Machine Learning Group, Oak Ridge, TN 37831, United States; Santos-Villalobos, H., Oak Ridge National Laboratory, Imaging, Signals, and Machine Learning Group, Oak Ridge, TN 37831, United States; Thompson, J., Oak Ridge National Laboratory, Imaging, Signals, and Machine Learning Group, Oak Ridge, TN 37831, United States; Bolme, D., Oak Ridge National Laboratory, Imaging, Signals, and Machine Learning Group, Oak Ridge, TN 37831, United States; Boehnen, C., Oak Ridge National Laboratory, Imaging, Signals, and Machine Learning Group, Oak Ridge, TN 37831, United States","Iris recognition is among the highest accuracy biometrics. However, its accuracy relies on controlled high quality capture data and is negatively affected by several factors such as angle, occlusion, and dilation. Non-ideal iris recognition is a new research focus in biometrics. In this paper, we present a gaze estimation method designed for use in an off-Angle iris recognition framework based on the ORNL biometric eye model. Gaze estimation is an important prerequisite step to correct an off-Angle iris images. To achieve the accurate frontal reconstruction of an off-Angle iris image, we first need to estimate the eye gaze direction from elliptical features of an iris image. Typically additional information such as well-controlled light sources, head mounted equipment, and multiple cameras are not available. Our approach utilizes only the iris and pupil boundary segmentation allowing it to be applicable to all iris capture hardware. We compare the boundaries with a look-up-table generated by using our biologically inspired biometric eye model and find the closest feature point in the look-up-table to estimate the gaze. Based on the results from real images, the proposed method shows effectiveness in gaze estimation accuracy for our biometric eye model with an average error of approximately 3.5 degrees over a 50 degree range. © 2013 SPIE.","Biometric eye model; Elliptical iris boundary; Gaze estimation; Iris recognition; Off-Angle iris","Elliptical iris boundary; Eye model; Gaze estimation; Iris recognition; Off-angle; Functional assessment; Light sources; Quality control; Biometrics",Conference Paper,"Final","",Scopus,2-s2.0-84881038636
"Yan M., Tamura H., Tanno K.","55515780300;35600682500;7103288835;","Gaze estimation using electrooculogram signals and its mathematical modeling",2013,"Proceedings of The International Symposium on Multiple-Valued Logic",,,"6524633","18","22",,4,"10.1109/ISMVL.2013.31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880708533&doi=10.1109%2fISMVL.2013.31&partnerID=40&md5=95bda816270a9e5d0cb27c6106d8412f","Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazaki, Japan; Department of Electrical and Electronic Engineering, University of Miyazaki, Miyazaki, Japan","Yan, M., Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Miyazaki, Japan; Tamura, H., Department of Electrical and Electronic Engineering, University of Miyazaki, Miyazaki, Japan; Tanno, K., Department of Electrical and Electronic Engineering, University of Miyazaki, Miyazaki, Japan","The aim of this study is to present electrooculogram signals that can be used for human computer interface efficiently. Establishing an efficient alternative channel for communication without overt speech and hand movements is important to increase the quality of life for patients suffering from Amyotrophic Lateral Sclerosis or other illnesses that prevent correct limb and facial muscular responses. In this paper, we introduce the gaze estimation system of electrooculogram signals. Using this system, the electrooculogram signals can be recorded when the patients focused on each direct. All these recorded signals could be analyzed using math-method and the mathematical model will be set up. Gaze estimation can be recognized using electrooculogram signals follow these models. © 2013 IEEE.","Drift; Electrooculogram Signal; Gaze estimation; Mathematical model","Amyotrophic lateral sclerosis; Drift; Electro-oculogram; Gaze estimation; Hand movement; Human computer interfaces; Quality of life; Recorded signals; Gesture recognition; Many valued logics; Mathematical models; Speech communication; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-84880708533
[无可用作者姓名],[无可用的作者 ID],"Proceedings - 2013 IEEE 43rd International Symposium on Multiple-Valued Logic, ISMVL 2013",2013,"Proceedings of The International Symposium on Multiple-Valued Logic",,,,"","",362,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880705423&partnerID=40&md5=30baf957b8c86db628f351edc9fc8cd5",,"","The proceedings contain 59 papers. The topics discussed include: systems health care: health management technology; wearable human activity recognition by electrocardiograph and accelerometer; gaze estimation using electrooculogram signals and its mathematical modeling; fuzzy damage extraction method for ultrasonic nondestructive testing images; fault ordering for automatic test pattern generation of reversible circuits; analysis and improvement of transformation-based reversible logic synthesis; a fuzzy human detection for security system using infrared laser camera; mining multi human locations using thermopile array sensors; on selection of intraocular power formula using support vector machines and genetic algorithm; a broken line classification method of mathematical graphs for automating translation into scalable vector graphic; and remarks on applications of shapes of decision diagrams in classification of multiple-valued logic functions.",,,Conference Review,"Final","",Scopus,2-s2.0-84880705423
"Lin Y.-T., Lin R.-Y., Lin Y.-C., Lee G.C.","16310089500;55331109200;7406582562;18836300000;","Real-time eye-gaze estimation using a low-resolution webcam",2013,"Multimedia Tools and Applications","65","3",,"543","568",,26,"10.1007/s11042-012-1202-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878291017&doi=10.1007%2fs11042-012-1202-1&partnerID=40&md5=4acba0647d420449068697dd818392f2","Graduate Institute of Information and Computer Education, National Taiwan Normal University, Taipei 10610, Taiwan; Department of Computer Science and Information Engineering, National Taiwan Normal University, Taipei 10610, Taiwan; Department of Biomedical Engineering, Yuanpei University, Hsinchu 30015, Taiwan","Lin, Y.-T., Graduate Institute of Information and Computer Education, National Taiwan Normal University, Taipei 10610, Taiwan; Lin, R.-Y., Department of Computer Science and Information Engineering, National Taiwan Normal University, Taipei 10610, Taiwan; Lin, Y.-C., Department of Biomedical Engineering, Yuanpei University, Hsinchu 30015, Taiwan; Lee, G.C., Department of Computer Science and Information Engineering, National Taiwan Normal University, Taipei 10610, Taiwan","Eye detection and gaze estimation play an important role in many applications, e.g., the eye-controlled mouse in the assisting system for disabled or elderly persons, eye fixation and saccade in psychological analysis, or iris recognition in the security system. Traditional research usually achieves eye tracking by employing intrusive infrared-based techniques or expensive eye trackers. Nowadays, there are more and more needs to analyze user behaviors from tracking eye attention in general applications, in which users usually use a consumer-grade computer or even laptop with an inexpensive webcam. To satisfy the requirements of rapid developments of such applications and reduce the cost, it is no more practical to apply intrusive techniques or use expensive/specific equipment. In this paper, we propose a real-time eye-gaze estimation system by using a general low-resolution webcam, which can estimate eye-gaze accurately without expensive or specific equipment, and also without an intrusive detection process. An illuminance filtering approach is designed to remove the influence from light changes so that the eyes can be detected correctly from the low-resolution webcam video frames. A hybrid model combining the position criterion and an angle-based eye detection strategy are also derived to locate the eyes accurately and efficiently. In the eye-gaze estimation stage, we employ the Fourier Descriptor to describe the appearance-based features of eyes compactly. The determination of eye-gaze position is then carried out by the Support Vector Machine. The proposed algorithms have high performances with low computational complexity. The experiment results also show the feasibility of the proposed methodology. © 2012 Springer Science+Business Media, LLC.","Eye detection; Eye tracking; Eye-gaze estimation; Face detection","Eye detection; Eye-gaze; Eye-tracking; Fourier descriptors; General applications; Intrusive detection; Low computational complexity; Psychological analysis; Biometrics; Consumer behavior; Eye controlled devices; Eye protection; Face recognition; Handicapped persons; Laptop computers; Gesture recognition",Article,"Final","",Scopus,2-s2.0-84878291017
"Mohammadi M.R., Raie A.","57202976302;14319409000;","Selection of unique gaze direction based on pupil position",2013,"IET Computer Vision","7","4",,"238","245",,7,"10.1049/iet-cvi.2012.0141","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880341054&doi=10.1049%2fiet-cvi.2012.0141&partnerID=40&md5=3fef3e638f3dba3177c5e91527879458","Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran","Mohammadi, M.R., Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran; Raie, A., Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran","The 'gaze estimation' problem, because of its manifold applications including human-computer interaction especially for the handicapped, has been a topic of research for many years. Recently, thanks to technological advances, non-intrusive methods based on imageprocessing employed in broader applications, are addressed more than before. One of the promising approaches to gaze estimation is based on projective geometry. In projective geometry-based approaches, an ellipse is fitted to the image of iris boundary and from its parameters, two solutions are obtained, only one of which is valid for the gaze direction.Since the ellipse parameters are not adequate for disambiguation, in a previous work, theaccurate coordinates of eye corners in three-dimensional, as complementary information, is obtained through another system increasing the cost and complexity of the overall system. In this article, a new technique to select the valid solution, based on eye geometry and iris image, is developed. In the proposed technique, relative position of the pupil centre with respect to the iris centre is used as the complementary information and a novelalgorithm is proposed for its extraction. The performance of the proposed technique was evaluated on 600 real images and with only one failure on selecting the valid solution demonstrated an accuracy of 99.8% for disambiguation. © The Institution of Engineering and Technology 2013.",,"Eye corners; Gaze direction; Gaze estimation; Non-intrusive method; Projective geometry; Real images; Relative positions; Technological advances; Computer science; Computer vision; Software engineering; Geometry",Article,"Final","",Scopus,2-s2.0-84880341054
"Man Y., Zhao X., Zhang K.","55352204400;55352149700;56537128000;","3D Gaze estimation based on facial feature tracking",2013,"Proceedings of SPIE - The International Society for Optical Engineering","8768",,"87681N","","",,,"10.1117/12.2010781","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880167547&doi=10.1117%2f12.2010781&partnerID=40&md5=ce87e0eca24b19a3eb513162fdcaa583","Shaanxi Provincial Key Lab. of Speech and Image Information Processing, Northwestern Polytechnical University, Xi'an, China","Man, Y., Shaanxi Provincial Key Lab. of Speech and Image Information Processing, Northwestern Polytechnical University, Xi'an, China; Zhao, X., Shaanxi Provincial Key Lab. of Speech and Image Information Processing, Northwestern Polytechnical University, Xi'an, China; Zhang, K., Shaanxi Provincial Key Lab. of Speech and Image Information Processing, Northwestern Polytechnical University, Xi'an, China","A 3D gaze estimation and tracking algorithm based on facial feature tracking is presented in this paper. Firstly, we used the Active Shape Model (ASM) to extract facial feature points with a stereo camera. Then, the full 3D pose of head is estimated by comparing the feature points of current pose with initial head pose. After that, the center of eyeball is obtained based on a 3D eye model which is related to head pose and the middle point of eye corners. Thereby, optical axis was computed as 3D vectors through the center of eyeball to the center of pupil. Finally, visual axis was gotten by adding an angle to optical axis. Here, in our system, a one-time personal calibration is used to determine this angle. The experimental results show the accuracy of our gaze tracking system achieves less than 3 degree. © 2013 SPIE.","Facial feature tracking; Gaze estimation; Gaze tracking; Head pose; Stereo cameras","Facial feature tracking; Gaze estimation; Gaze tracking; Head pose; Stereo cameras; Cameras; Face recognition; Tracking (position); Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-84880167547
"Mohammadi M.R., Raie A.A.","57202976302;14319409000;","Pose-invariant eye gaze estimation using geometrical features of iris and pupil images",2013,"Journal of Information Systems and Telecommunication","1","3",,"143","153",,,"10.7508/jist.2013.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020442392&doi=10.7508%2fjist.2013.03.001&partnerID=40&md5=fa205bc458253930a82f3774c80746a2","Amirkabir University of Technology, Iran","Mohammadi, M.R., Amirkabir University of Technology, Iran; Raie, A.A., Amirkabir University of Technology, Iran","In the cases of severe paralysis in which the ability to control the body movements of a person is limited to the muscles around the eyes, eye movements or blinks are the only way for the person to communicate. Interfaces that assist in such communications often require special hardware or reliance on active infrared illumination. In this paper, we propose a non-intrusive algorithm for eye gaze estimation that works with video input from an inexpensive camera and without special lighting. The main contribution of this paper is proposing a new geometrical model for eye region that only requires the image of one iris for gaze estimation. Essential parameters for this system are the best fitted ellipse of the iris and the pupil center. The algorithms used for both iris ellipse fitting and pupil center localization pose no pre-assumptions on the head pose. All in all, the achievement of this paper is the robustness of the proposed system to the head pose variations. The performance of the method has been evaluated on both synthetic and real images leading to errors of 2.12 and 3.48 degrees, respectively.","Gaze estimation; Iris ellipse fitting; Projective geometry; Pupil center localization; Video-based human-computer interface",,Article,"Final","",Scopus,2-s2.0-85020442392
"Yücel Z., Salah A.A., Meriçli Ç., Mericļi T., Valenti R., Gevers T.","15847245400;7006556254;57209788447;23091748100;57192175392;7003472472;","Joint attention by gaze interpolation and saliency",2013,"IEEE Transactions on Cybernetics","43","3",,"829","842",,40,"10.1109/TSMCB.2012.2216979","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890437287&doi=10.1109%2fTSMCB.2012.2216979&partnerID=40&md5=ce180d7140583f361b25a093483a8ff9","Intelligent Robotics and Communication Laboratories, Advanced Telecommunications Research Institute International, Kyoto 619-0288, Japan; Department of Computer Engineering, Boǧaziçi University, Istanbul 34342, Turkey; Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Intelligent Systems Lab Amsterdam, University of Amsterdam, 1098 Amsterdam, Netherlands; Computer Vision Center, Universitat Autònoma de Barcelona, 08193 Barcelona, Spain","Yücel, Z., Intelligent Robotics and Communication Laboratories, Advanced Telecommunications Research Institute International, Kyoto 619-0288, Japan; Salah, A.A., Department of Computer Engineering, Boǧaziçi University, Istanbul 34342, Turkey; Meriçli, Ç., Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Mericļi, T., Department of Computer Engineering, Boǧaziçi University, Istanbul 34342, Turkey; Valenti, R., Intelligent Systems Lab Amsterdam, University of Amsterdam, 1098 Amsterdam, Netherlands; Gevers, T., Intelligent Systems Lab Amsterdam, University of Amsterdam, 1098 Amsterdam, Netherlands, Computer Vision Center, Universitat Autònoma de Barcelona, 08193 Barcelona, Spain","Joint attention, which is the ability of coordination of a common point of reference with the communicating party, emerges as a key factor in various interaction scenarios. This paper presents an image-based method for establishing joint attention between an experimenter and a robot. The precise analysis of the experimenter's eye region requires stability and high-resolution image acquisition, which is not always available. We investigate regression-based interpolation of the gaze direction from the head pose of the experimenter, which is easier to track. Gaussian process regression and neural networks are contrasted to interpolate the gaze direction. Then, we combine gaze interpolation with image-based saliency to improve the target point estimates and test three different saliency schemes. We demonstrate the proposed method on a human-robot interaction scenario. Cross-subject evaluations, as well as experiments under adverse conditions (such as dimmed or artificial illumination or motion blur), show that our method generalizes well and achieves rapid gaze estimation for establishing joint attention. © 2012 IEEE.","Developmental robotics; Gaze following; Head pose estimation; Joint visual attention; Saliency; Selective attention","Developmental robotics; Gaze following; Head Pose Estimation; Saliency; Selective attention; Visual Attention; Image recognition; Interpolation; algorithm; artificial intelligence; attention; automated pattern recognition; biomimetics; eye fixation; human; interpersonal communication; man machine interaction; physiology; procedures; robotics; article; attention; automated pattern recognition; eye fixation; methodology; physiology; robotics; Algorithms; Artificial Intelligence; Attention; Biomimetics; Communication; Fixation, Ocular; Humans; Man-Machine Systems; Pattern Recognition, Automated; Robotics; Algorithms; Artificial Intelligence; Attention; Biomimetics; Communication; Fixation, Ocular; Humans; Man-Machine Systems; Pattern Recognition, Automated; Robotics",Article,"Final","",Scopus,2-s2.0-84890437287
"Lee E.C., Park M.W.","14009024200;57225850371;","A new eye tracking method as a smartphone interface",2013,"KSII Transactions on Internet and Information Systems","7","4",,"834","848",,6,"10.3837/tiis.2013.04.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877064547&doi=10.3837%2ftiis.2013.04.013&partnerID=40&md5=9d8368a68af6271b31bc62337dc3f52a","Department of Computer Science, College of Software, Sangmyung University, 7 Hongji-Dong, 110-743, Jongno-Gu, Seoul, South Korea; Department of Computer Science, Graduate School, Sangmyung University, 7 Hongji-Dong, 110-743, Jongno-Gu, Seoul, South Korea","Lee, E.C., Department of Computer Science, College of Software, Sangmyung University, 7 Hongji-Dong, 110-743, Jongno-Gu, Seoul, South Korea; Park, M.W., Department of Computer Science, Graduate School, Sangmyung University, 7 Hongji-Dong, 110-743, Jongno-Gu, Seoul, South Korea","To effectively use these functions many kinds of human-phone interface are used such as touch, voice, and gesture. However, the most important touch interface cannot be used in case of hand disabled person or busy both hands. Although eye tracking is a superb human-computer interface method, it has not been applied to smartphones because of the small screen size, the frequently changing geometric position between the user's face and phone screen, and the low resolution of the frontal cameras. In this paper, a new eye tracking method is proposed to act as a smartphone user interface. To maximize eye image resolution, a zoom lens and three infrared LEDs are adopted. Our proposed method has following novelties. Firstly, appropriate camera specification and image resolution are analyzed in order to smartphone based gaze tracking method. Secondly, facial movement is allowable in case of one eye region is included in image. Thirdly, the proposed method can be operated in case of both landscape and portrait screen modes. Fourthly, only two LED reflective positions are used in order to calculate gaze position on the basis of 2D geometric relation between reflective rectangle and screen. Fifthly, a prototype mock-up design module is made in order to confirm feasibility for applying to actual smart-phone. Experimental results showed that the gaze estimation error was about 31 pixels at a screen resolution of 480×800 and the average hit ratio of a 5×4 icon grid was 94.6%. © 2013 KSII.","Eye tracking; Gaze tracking; Smartphone interface","Eye tracking methods; Eye-tracking; Facial movements; Gaze tracking; Geometric position; Geometric relations; Human computer interfaces; Screen resolution; Cameras; Handicapped persons; Image resolution; Light emitting diodes; Signal encoding; Smartphones; Telephone sets; Tracking (position); User interfaces; Eye movements",Article,"Final","",Scopus,2-s2.0-84877064547
"Coutinho F.L., Morimoto C.H.","22233254400;7102275798;","Improving head movement tolerance of cross-ratio based eye trackers",2013,"International Journal of Computer Vision","101","3",,"459","481",,31,"10.1007/s11263-012-0541-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884721011&doi=10.1007%2fs11263-012-0541-8&partnerID=40&md5=8ee8725b98797941f256f862d0f99b5e","Department of Computer Science, University of São Paulo, São Paulo, Brazil","Coutinho, F.L., Department of Computer Science, University of São Paulo, São Paulo, Brazil; Morimoto, C.H., Department of Computer Science, University of São Paulo, São Paulo, Brazil","When first introduced, the cross-ratio (CR) based remote eye tracking method offered many attractive features for natural human gaze-based interaction, such as simple camera setup, no user calibration, and invariance to head motion. However, due to many simplification assumptions, current CR-based methods are still sensitive to head movements. In this paper, we revisit the CR-based method and introduce two new extensions to improve the robustness of the method to head motion. The first method dynamically compensates for scale changes in the corneal reflection pattern, and the second method estimates true coplanar eye features so that the cross-ratio can be applied. We present real-time implementations of both systems, and compare the performance of these new methods using simulations and user experiments. Our results show a significant improvement in robustness to head motion and, for the user experiments in particular, an average reduction of up to 40 % in gaze estimation error was observed. © 2012 Springer Science+Business Media, LLC.","Cross-ratio; Eye tracking; Free-head motion; Gaze tracking; Head movement tolerance; Homography; Remote eye gaze tracking","Cross-ratios; Eye gaze tracking; Eye-tracking; Free-head; Gaze tracking; Head movements; Homographies; Experiments; Gesture recognition; Real time control; Eye movements",Article,"Final","",Scopus,2-s2.0-84884721011
"Sugano Y., Matsushita Y., Sato Y.","7005470045;35956654700;35230954300;","Appearance-based gaze estimation using visual saliency",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence","35","2","6193107","329","341",,114,"10.1109/TPAMI.2012.101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871733927&doi=10.1109%2fTPAMI.2012.101&partnerID=40&md5=ea0b5dd9fa2d62e3c95d540b6ccf7240","Sato Laboratory, Institute of Industrial Science, University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan; Microsoft Research Asia, 13F, Building 2, No. 5 Dan Ling Street, Haidian District, Beijing 100080, China","Sugano, Y., Sato Laboratory, Institute of Industrial Science, University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan; Matsushita, Y., Microsoft Research Asia, 13F, Building 2, No. 5 Dan Ling Street, Haidian District, Beijing 100080, China; Sato, Y., Sato Laboratory, Institute of Industrial Science, University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan","We propose a gaze sensing method using visual saliency maps that does not need explicit personal calibration. Our goal is to create a gaze estimator using only the eye images captured from a person watching a video clip. Our method treats the saliency maps of the video frames as the probability distributions of the gaze points. We aggregate the saliency maps based on the similarity in eye images to efficiently identify the gaze points from the saliency maps. We establish a mapping between the eye images to the gaze points by using Gaussian process regression. In addition, we use a feedback loop from the gaze estimator to refine the gaze probability maps to improve the accuracy of the gaze estimation. The experimental results show that the proposed method works well with different people and video clips and achieves a 3.5-degree accuracy, which is sufficient for estimating a user's attention on a display. © 2012 IEEE.","face and gesture recognition; Gaze estimation; visual attention","Appearance based; Eye images; Face and gesture recognition; Feed-back loop; Gaussian process regression; Gaze estimation; Gaze point; Probability maps; Saliency map; Video clips; Video frame; Visual Attention; Visual saliency; Gesture recognition; Probability distributions; Video cameras; Visualization; Estimation; algorithm; article; artificial intelligence; attention; automated pattern recognition; biological model; biomimetics; computer assisted diagnosis; computer simulation; eye fixation; eye movement; human; methodology; nonlinear system; pattern recognition; physiology; reproducibility; sensitivity and specificity; Algorithms; Artificial Intelligence; Attention; Biomimetics; Computer Simulation; Eye Movements; Fixation, Ocular; Humans; Image Interpretation, Computer-Assisted; Models, Biological; Nonlinear Dynamics; Pattern Recognition, Automated; Pattern Recognition, Visual; Reproducibility of Results; Sensitivity and Specificity",Article,"Final","",Scopus,2-s2.0-84871733927
[无可用作者姓名],[无可用的作者 ID],"15th International Conference on Human-Computer Interaction, HCI International 2013",2013,"Communications in Computer and Information Science","373","PART I",,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015592612&partnerID=40&md5=33545264f37c88d88da611ce47f142e1",,"","The proceedings contain 149 papers. The special focus in this conference is on Human-Computer Interaction, and Human Interface and the Management of Information. The topics include: User Driven Service Design and Innovation Platforms; Affective service design considered informational assimilation of layout factors; the question concerning technology as art; the role of knowledge management in agile software development; issues and understandings for rural HCI systems development; a study on the prototype of focusing on the operability for requirement acquisition; task-oriented M-commerce interface design; towards exploring web interface sign ontology; application of kinect technology in the design of interactive products for Chinese senior citizens; a process to extract knowledge automatic for improving personas; document analysis (DA) as a sociotechnical design framework for HCI; developing a performance-based design system with semantic interoperability; a review on emotional evaluations for smart phone; usability and user acceptance of university web portal interfaces; calculating website's usability metrics using log file information; verification of the questionnaire for the level of mental models building; towards usable and secure natural language processing systems; variables of usability; the convergence of security and usability; a two-step click interaction for mobile internet on Smartphone; a study on the usability testing of gesture tracking-based natural user interface; development of a usability evaluation method based on finger movement; examining the quality in use of web 2.0 applications; desirability methods for evaluating visual design; optimizing usability on video streaming devices and smart TV's; insights from eye movement into dynamic decision-making research and usability testing; a usability study of dynamic geometry software's interfaces; interactive serious gaming for children with auditory processing difficulties in the Arabic language; issues with designing dementia-friendly interfaces; opening data to the great British publictoilet map; applying an approach to develop web applications considering accessibility practices using design rationale; a leaning tool for students with special needs; an accessible chat prototype for screen reader users in mobile devices; an approach to design with people who have special needs; developing a mobile application for language disabled children with user centered design; a study for web site color guideline for universal access for color vision deficiencies; older adults' experiences with technology; providing access to social networking services for elderly people; design of user manuals for elderly people focusing on font types; development of support applications for elderly and handicapped people with ICT infrastructure; interface model for accessible forums for blind, deaf and non-disabled people; evaluation of accessibility with the deaf user; design touch feedback for blind users; development of recognition system of Japanese sign language using 3d image sensor; smart watches for home interaction services; developing a multimedia gallery supporting mid-air gesture-based interaction and control; a shadow touching technique for interactive projector devices; interpret human gestures with a time of flight camera using standard image processing algorithms on a distributed system; calibrating screen coordinates of tabletop display using shadow-cursor; designing interactive sonification for live aquarium exhibits; emotional speech conversion using pitch-synchronous harmonic and non-harmonic modeling of speech; the difference of the emotional communication by movement on the digital contents; a study on the interaction between human and smart devices based on emotion recognition; effects of plane mapping on sound localization in a virtual auditory environment; automatic facial expression recognition using modifiedwavelet-based salient points and Gabor-wavelet filters; virtual flying experience contents using upper-body gesture recognition; adaptive multimodal HCI with uncertain data by collaborative fission and fusion; a design on gestural user interaction techniques for tiled displays using kinects; the shaking screening desktop interaction types based on tablet computer; arm gesture recognition using continuous for user-defined gestures; using EEG biometric feedback devices to investigate interruption impact on multi-tasking task completion; empirical review of challenge design in video game design; emotion-cognition interaction of decision making in the social context; evaluation of subjective and EEG-based measures of mental workload; self soothing by reviewing favorite memories; the effects of information format and reading task on mobile user's reading behavior; experiment on how type a and type b behavior pattern affect decision-making; a novel approach to cognitive engineering; role of metacognition in basic electric circuit problem solving process; evaluating the attention devoted to memory storage using simultaneous measurement of the brain activity and eye movements; tracking attention based on EEG spectrum; measurement of useful field of view during ocular following response; visual perception modeling on sense of material of object surface; eye gaze and mouse cursor relationship in a debugging task; adaptive control elements for navigation systems; differences between a young and older computer users' recognition rate of tactons; enhancing depth perception and the understanding of object relations; how humans search varying-knowledge environments; vibration of the white cane causing a hardness sense of an object; physiological responses to watching 3D on television with active and passive glasses; visual illusion by phase-shifted light projection and its applications; generation of the certain kind of figures using the movement sense of localized sound and its application; analysis of perceived discomfort and EMG for touch locations of a soft keyboard; servo-actuated stylus for post stroke arm and fore arm rehabilitation; permitting fast, precise and user-friendly keyboard-based mouse control; laser pointer interaction and its properties in pointing performance; relationship between surface property and operability of tablet terminal with touch-sensitive screen; interactive pose estimation for active pauses; lower limb musculoskeletal model validation during one legged forward hopping and side jumping in healthy subjects using EMG; comparative study of P300 and motor imagery for typing using dry-electrode EEG devices; eye-controlled games for behavioral therapy of attention deficit disorders; towards an emergent and autopoietic approach to adaptative chord generation through human interaction; proposal of PC input method by combination of gaze detection and head movement detection; brain-computer interfaces for military training; a novel approach for adaptive EEG artefact rejection and EOG gaze estimation; point-and-click interface based on parameter-free eye tracking technique using a single camera; automatic sleep stage classification GUI with a portable EEG device; intelligent workload control for exercise game; measurement of the characteristics for BCI by SSVEP; evaluation of independent component analysis algorithms for electroencephalography source separation; ants can schedule software projects; visualizing software ecosystems as living cities; modeling the portfolio selection problem with constraint programming; adaptive and multilevel approach for constraint solving; a GUI for modeling regular constraints; an interactive approach with four criteria for stochastic weighted Weber problems; a case study comparing the realism of furniture products in E-commerce; designing a service innovation measurement of SMEs; collaborative design support system based on interactive genetic algorithm (IGA); development of brand selection model considering customer service; mobile in-app advertising for tourism; the GUI design for the products of business use by using the business user model; modeling relationship between visual impression of products and their graphical features; case study of design action to alleviate poverty; promoting consumer products with fictional stories; service designs for lifestyle changes; using fuzzy analytic hierarchy process to construct green suppliers assessment criteria and inspection exemption guidelines; optimizing product interface training program for older adults-a pilot study; multimedia interactive display system for retail stores; estimation of dominant features of commodities based on shopping behavior analysis and a study on consumers' emotions evoked by product semantics.",,,Conference Review,"Final","",Scopus,2-s2.0-85015592612
"Duan D., Tian L., Cui J., Wang L., Zha H., Aghajan H.","56097162900;55263470100;15623000600;57044350200;7006639394;18041521100;","Gaze estimation in children's peer-play scenarios",2013,"Proceedings - 2nd IAPR Asian Conference on Pattern Recognition, ACPR 2013",,,"6778426","760","764",,3,"10.1109/ACPR.2013.178","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899119922&doi=10.1109%2fACPR.2013.178&partnerID=40&md5=227e87d934f4ae452e011e2f15ad46d0","Key Laboratory of Machine Perception, Peking University, China; Department of Psychology, Peking University, China; Wireless Sensor Networks Lab, Stanford University, United States","Duan, D., Key Laboratory of Machine Perception, Peking University, China; Tian, L., Key Laboratory of Machine Perception, Peking University, China; Cui, J., Key Laboratory of Machine Perception, Peking University, China; Wang, L., Department of Psychology, Peking University, China; Zha, H., Key Laboratory of Machine Perception, Peking University, China; Aghajan, H., Wireless Sensor Networks Lab, Stanford University, United States","Gaze is a powerful cue for children's social behavior analysis. In this paper, a novel method is proposed to estimate children's gaze orientation in the experimental data of developmental psychology based on head pose estimation. In consideration of the possible errors of head pose estimation results, temporal information and potential targets are both introduced to improve the results of gaze estimation. At last, this method is evaluated by a dataset of children's peer-play scenarios and the results show that this method has a good performance. According to the experimental valuation and analysis, in a certain peer-play scenario, potential targets are powerful spatial cues for children's gaze estimation and temporal information also provides some cues to improve the estimation results. © 2013 IEEE.","Children's social behavior analysis; Gaze estimation; Head pose estimation","Pattern recognition; Software engineering; Developmental psychology; Estimation results; Gaze estimation; Head Pose Estimation; Potential targets; Social behavior; Spatial cues; Temporal information; Image recognition",Conference Paper,"Final","",Scopus,2-s2.0-84899119922
"Nitschke C., Nakazawa A., Nishida T.","36947002100;35807510800;35595754400;","I See what you see: Point of gaze estimation from corneal images",2013,"Proceedings - 2nd IAPR Asian Conference on Pattern Recognition, ACPR 2013",,,"6778329","298","304",,13,"10.1109/ACPR.2013.84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899114386&doi=10.1109%2fACPR.2013.84&partnerID=40&md5=ec74d9acd29bc7f304c77ff597e83460","Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan; PRESTO, Japan Science and Technology Agency (JST), 4-1-8 Honcho, Kawaguchi, Saitama, 332-0012, Japan","Nitschke, C., Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan; Nakazawa, A., Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan, PRESTO, Japan Science and Technology Agency (JST), 4-1-8 Honcho, Kawaguchi, Saitama, 332-0012, Japan; Nishida, T., Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan","Eye-gaze tracking (EGT) is an important problem with a long history and various applications. However, state-of-the-art geometric vision-based techniques still suffer from major limitations, especially (1) the requirement for calibration of a static relationship between eye camera and scene, and (2) a parallax error that occurs when the depth of the scene varies. This paper introduces a novel concept for EGT that overcomes these limitations using corneal imaging. Based on the observation that the cornea reflects the surrounding scene over a wide field of view, it is shown how to extract that information and determine the point of gaze (PoG) directly in an eye image. To realize this, a closed-form solution is developed to obtain the gaze-reflection point (GRP), where light from the PoG reflects at the corneal surface into a camera. This includes compensation for the individual offset between optical and visual axis. Quantitative and qualitative evaluation shows that the strategy achieves considerable accuracy and successfully supports depth-varying environments. The novel approach provides important practical advantages, including reduced intrusiveness and complexity, and support for flexible dynamic setups, non-planar scenes and outdoor application. © 2013 IEEE.","Analytic forward projection; Calibration; Corneal imaging; Eye gaze tracking; Gaze reflection point; Human computer interaction; Point of gaze; Spherical mirror","Calibration; Cameras; Geometrical optics; Human computer interaction; Pattern recognition; Synthetic aperture sonar; Analytic forward projection; Eye gaze tracking; Point of gaze; Reflection points; Spherical mirror; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84899114386
"Hiratani A., Nakashima R., Matsumiya K., Kuriki I., Shioiri S.","56125609700;36613417100;7006069903;6701407070;7005878104;","Considerations of self-motion in motion saliency",2013,"Proceedings - 2nd IAPR Asian Conference on Pattern Recognition, ACPR 2013",,,"6778431","783","787",,3,"10.1109/ACPR.2013.183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899110531&doi=10.1109%2fACPR.2013.183&partnerID=40&md5=fb2447d131f2e48b56e1c67c7ac6281f","Research Institute of Electrical Communication, Tohoku University, Sendai, Japan; Graduate School of Information Sciences, Tohoku University, Sendai, Japan","Hiratani, A., Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Nakashima, R., Research Institute of Electrical Communication, Tohoku University, Sendai, Japan; Matsumiya, K., Research Institute of Electrical Communication, Tohoku University, Sendai, Japan, Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Kuriki, I., Research Institute of Electrical Communication, Tohoku University, Sendai, Japan, Graduate School of Information Sciences, Tohoku University, Sendai, Japan; Shioiri, S., Research Institute of Electrical Communication, Tohoku University, Sendai, Japan, Graduate School of Information Sciences, Tohoku University, Sendai, Japan","Despite a number of studies of computational models of visual saliency, it is still difficult to predict where human would attend in a scene. One of the problems is the effect of self-motion on retinal images. Usually saliency is calculated with local motion signals, and high degree of saliency is found in motion components caused by self-motion. Since human observers usually ignore the motion caused by self-motion, the prediction accuracy of attention locations falls off. We developed a framework that reduces the saliency of the motion components caused self-motion, using a physiological model of optic flow processing that extracts object motion among self-motion signals. © 2013 IEEE.","Gaze estimation; Global motion; Saliency map; Self-motion","Physiological models; Computational model; Gaze estimation; Global motion; Motion components; Motion saliencies; Prediction accuracy; Saliency map; Self motion; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-84899110531
"Okada T., Yamazoe H., Mitsugami I., Yagi Y.","56127030700;8517824300;13106192400;55740719200;","Preliminary analysis of gait changes that correspond to gaze directions",2013,"Proceedings - 2nd IAPR Asian Conference on Pattern Recognition, ACPR 2013",,,"6778432","788","792",,7,"10.1109/ACPR.2013.184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899088124&doi=10.1109%2fACPR.2013.184&partnerID=40&md5=6e02086f8b5b0894e1a7baa897c9cc93","Osaka University, Suita, Osaka, 567-0047, Japan; Osaka University, Toyonaka, Osaka, 560-0043, Japan","Okada, T., Osaka University, Suita, Osaka, 567-0047, Japan; Yamazoe, H., Osaka University, Toyonaka, Osaka, 560-0043, Japan; Mitsugami, I., Osaka University, Suita, Osaka, 567-0047, Japan; Yagi, Y., Osaka University, Suita, Osaka, 567-0047, Japan","Human gait (way of walking) can be changed to correspond to human gaze direction. We aim to realize a gait-based gaze estimation scheme by modeling the relations between gait and gaze. In this paper, as a first step to our goal, we show the preliminary analysis results of the relations between gait and gaze. From the results, we confirmed that arm swing is affected by the gaze direction. We confirmed a tendency for the opposite arm's amplitude in the gazing direction to decrease. For these analysis, we constructed an immersive walking environment in which we measured subject gaits in various gazing situations by a motion capturing system and an eye tracker. The environment consisted of a treadmill and 180 multi-screens for presenting the gazing target. Our analysis results suggest the possibility of gaze estimation based on gait. © 2013 IEEE.","Gait; Gaze; Motion capture","Software engineering; Gait; Gaze; Gaze direction; Gaze estimation; Motion capture; Motion capturing; Preliminary analysis; Walking environments; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-84899088124
"Choi J., Ahn B., Parl J., Kweon I.S.","56126520100;55383022000;56126183400;7003450602;","Appearance-based gaze estimation using kinect",2013,"2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence, URAI 2013",,,"6677362","260","261",,7,"10.1109/URAI.2013.6677362","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899078501&doi=10.1109%2fURAI.2013.6677362&partnerID=40&md5=38b1e2bca08e339ac1f7181d178227bd","Korea Advanced Institute of Science and Technology, Daejeon, 305-701, South Korea","Choi, J., Korea Advanced Institute of Science and Technology, Daejeon, 305-701, South Korea; Ahn, B., Korea Advanced Institute of Science and Technology, Daejeon, 305-701, South Korea; Parl, J., Korea Advanced Institute of Science and Technology, Daejeon, 305-701, South Korea; Kweon, I.S., Korea Advanced Institute of Science and Technology, Daejeon, 305-701, South Korea","Human gaze tracking has gathered much attention due to its capability to detect intuitive attention. Appearance-based methods can work with a single camera in ordinary conditions to track human gaze. An effective way to generate eye appearances is proposed using the Kinect. The head pose information is obtained from the Kinect after a series of calibrations. The Eye Appearance features are collected through ASM and KLT feature tracker. With 23 training samples, the error is found to be 1.07°. This paper proposes an efficient scheme for gaze tracking using a single Kinect device. © 2013 IEEE.","List keywords here. No more than 5","Artificial intelligence; Appearance based; Appearance-based methods; Efficient schemes; Gaze estimation; Klt feature trackers; No more than 5; Single cameras; Training sample; Tracking (position)",Conference Paper,"Final","",Scopus,2-s2.0-84899078501
"Alnajar F., Gevers T., Valenti R., Ghebreab S.","35797175900;7003472472;57192175392;6602783975;","Calibration-free gaze estimation using human gaze patterns",2013,"Proceedings of the IEEE International Conference on Computer Vision",,,"6751126","137","144",,41,"10.1109/ICCV.2013.24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898826721&doi=10.1109%2fICCV.2013.24&partnerID=40&md5=69ad5400541e9ce05d0f034090bbe843","University of Amsterdam, Amsterdam, Netherlands","Alnajar, F., University of Amsterdam, Amsterdam, Netherlands; Gevers, T., University of Amsterdam, Amsterdam, Netherlands; Valenti, R., University of Amsterdam, Amsterdam, Netherlands; Ghebreab, S., University of Amsterdam, Amsterdam, Netherlands","We present a novel method to auto-calibrate gaze estimators based on gaze patterns obtained from other viewers. Our method is based on the observation that the gaze patterns of humans are indicative of where a new viewer will look at. When a new viewer is looking at a stimulus, we first estimate a topology of gaze points (initial gaze points). Next, these points are transformed so that they match the gaze patterns of other humans to find the correct gaze points. In a flexible uncalibrated setup with a web camera and no chin rest, the proposed method was tested on ten subjects and ten images. The method estimates the gaze points after looking at a stimulus for a few seconds with an average accuracy of 4:3°. Although the reported performance is lower than what could be achieved with dedicated hardware or calibrated setup, the proposed method still provides a sufficient accuracy to trace the viewer attention. This is promising considering the fact that auto-calibration is done in a flexible setup, without the use of a chin rest, and based only on a few seconds of gaze initialization data. To the best of our knowledge, this is the first work to use human gaze patterns in order to auto-calibrate gaze estimators. © 2013 IEEE.",,"Calibration; Auto calibration; Dedicated hardware; Gaze estimation; Gaze point; Uncalibrated; Web camera; Estimation",Conference Paper,"Final","",Scopus,2-s2.0-84898826721
"Florea L., Florea C., Vrânceanu R., Vertan C.","57197861133;35619833700;50062133700;6603999152;","Can your eyes tell me how you think? a gaze directed estimation of the mental activity",2013,"BMVC 2013 - Electronic Proceedings of the British Machine Vision Conference 2013",,,,"","",,25,"10.5244/C.27.60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898457718&doi=10.5244%2fC.27.60&partnerID=40&md5=a99dbc88fd8ed28b3d580c0988e6867c","Image Processing and Analysis Laboratory, LAPI University Politehnica of Bucharest Bucharest, Romania","Florea, L., Image Processing and Analysis Laboratory, LAPI University Politehnica of Bucharest Bucharest, Romania; Florea, C., Image Processing and Analysis Laboratory, LAPI University Politehnica of Bucharest Bucharest, Romania; Vrânceanu, R., Image Processing and Analysis Laboratory, LAPI University Politehnica of Bucharest Bucharest, Romania; Vertan, C., Image Processing and Analysis Laboratory, LAPI University Politehnica of Bucharest Bucharest, Romania","Many applications pointed to the informative potential of the human eyes. In this paper we investigate the possibility of estimating the cognitive process used by a person when addressing a mental challenge, according to the Eye Accessing Cue (EAC) model from the Neuro-Linguistic Programming (NLP) theory [3]. This model states that there is a subtle, yet firm, connection between the non-visual gaze direction and the mental representation system used. From the point of view of computer vision, this work deals with gaze estimation under passive illumination. Using a multistage fusion approach, we show that it is possible to achieve highly accurate results in both terms of eye gaze localization or EAC case recognition.",,"Cognitive process; Gaze direction; Gaze estimation; Highly accurate; Mental activity; Mental representations; Multi-stage fusions; Non visuals; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84898457718
"Kanai S., Date H.","7101763644;7101748110;","Homography-based low-cost gaze estimation and its application to the usability assessment of digital prototypes of information appliances",2013,"Proceedings of the ASME Design Engineering Technical Conference","2 B",,"V02BT02A026","","",,1,"10.1115/DETC2013-12931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896988380&doi=10.1115%2fDETC2013-12931&partnerID=40&md5=a8c0694bb5a7a13599a4bd35958bc78a","Graduate School of Information Science and Technology, Hokkaido University, Sapporo 060-0814, Japan","Kanai, S., Graduate School of Information Science and Technology, Hokkaido University, Sapporo 060-0814, Japan; Date, H., Graduate School of Information Science and Technology, Hokkaido University, Sapporo 060-0814, Japan","Recently 3D digital prototypes of information appliances have been proposed for efficient user acceptance tests of user interface (UI) usability. The purpose of this research is to develop a gaze estimation system based on Homography and to fully integrate it with a 3D digital prototype of the information appliances in order to obtain information more useful for usability assessment. The estimation system consists only of four infrared LEDs and a USB camera and is low-cost. The gaze estimation enables the system not only to record a gaze point on the prototype but to identify the UI objects which the user is looking for in real time during the test session. A gazebased index was newly introduced to identify the misleading UI objects and to quantify the irrelevance of the UI design. A case study suggested that the integration of the gaze estimation with the 3D digital prototype and the proposed index were useful for automatically identifying which irrelevant UI objects misled the users' operations which could not yet be captured in previous simple event logging of the user inputs. Copyright © 2013 by ASME.",,"Acceptance tests; Design; Gesture recognition; Three dimensional; User interfaces; Digital prototype; Estimation systems; Event logging; Gaze estimation; Information appliances; Infrared leds; ITS applications; Usability assessment; Digital devices",Conference Paper,"Final","",Scopus,2-s2.0-84896988380
"Hikita S., Seto Y.","7003379210;55982792800;","Point-and-click interface based on parameter-free eye tracking technique using a single camera",2013,"Communications in Computer and Information Science","373","PART I",,"608","612",,,"10.1007/978-3-642-39473-7_121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891545718&doi=10.1007%2f978-3-642-39473-7_121&partnerID=40&md5=9ca5bfcc451b9a75695ae90b26d3f2bb","Graduate School of Information Sciences, Hiroshima City University, 3-4-1 Ozuka-Higashi, Asa-Minami-Ku, Hiroshima, 731-3194, Japan","Hikita, S., Graduate School of Information Sciences, Hiroshima City University, 3-4-1 Ozuka-Higashi, Asa-Minami-Ku, Hiroshima, 731-3194, Japan; Seto, Y., Graduate School of Information Sciences, Hiroshima City University, 3-4-1 Ozuka-Higashi, Asa-Minami-Ku, Hiroshima, 731-3194, Japan","We propose a method for the estimation of point of gaze with neither user- nor environment-dependent parameters. The gaze direction is calculated from the centers of both the pupil and the eye rotation. The center of the eye rotation is determined using the centers of both the pupil and the iris and the edge of the iris when at least four calibration targets, for which only the distances between them are known, are fixated on the screen. The mean horizontal and vertical errors for seven subjects were 0.91 deg and 0.77 deg, respectively. Next, a point-and-click interface, in which a user can move a cursor by a gaze shift and click a computer mouse by a voluntary eye blink or short fixation, was developed. On average, it took 1.2, 0.9, and 0.8 sec to point and click for each target with eye blink, short fixation, and normal hand manipulation, respectively. © Springer-Verlag Berlin Heidelberg 2013.","Center of eye rotation; Eye-gaze estimation; Iris; Pupil","Computer science; Computers; Calibration targets; Computer mouse; Eye-gaze; Hand manipulation; Iris; Point-and-click interface; Pupil; Voluntary eye blink; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84891545718
"Haji Samadi M.R., Cooke N.","56425793900;7102096932;","A novel approach for adaptive EEG artefact rejection and EOG gaze estimation",2013,"Communications in Computer and Information Science","373","PART I",,"603","607",,3,"10.1007/978-3-642-39473-7_120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891527316&doi=10.1007%2f978-3-642-39473-7_120&partnerID=40&md5=14014f842f0686f463676c28d0bad387","School of Electronic, Electrical and Computer Engineering, University of Birmingham, United Kingdom","Haji Samadi, M.R., School of Electronic, Electrical and Computer Engineering, University of Birmingham, United Kingdom; Cooke, N., School of Electronic, Electrical and Computer Engineering, University of Birmingham, United Kingdom","An adaptive system for Electroencephalography (EEG) artefact rejection and Electrooculogrum (EOG) gaze estimation is proposed. The system inputs optical gaze information, and accuracy of the EOG gaze classification into an adaptive Independent Component Analysis (ICA) algorithm, for improving EEG source separation. Finally two evaluation methods based on EOG gaze estimation are suggested to assess the performance of the proposed system. The work will be of use to researchers considering using BCI and eye-tracking paradigms in real life applications. © Springer-Verlag Berlin Heidelberg 2013.","Adaptive ICA; Artefact; BCI; EEG; Gaze","Electroencephalography; Electrophysiology; Artefact; BCI; Evaluation methods; Eye-tracking; Gaze; Gaze estimation; Independent component analysis(ICA); Real-life applications; Independent component analysis",Conference Paper,"Final","",Scopus,2-s2.0-84891527316
"Orozco J., Rudovic O., Gonzàlez J., Pantic M.","57193883839;27868018100;55450715500;56259551600;","Hierarchical on-line appearance-based tracking for 3d head pose, eyebrows, lips, eyelids and irises",2013,"Image and Vision Computing","31","4",,"322","340",,35,"10.1016/j.imavis.2013.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886396979&doi=10.1016%2fj.imavis.2013.02.001&partnerID=40&md5=826541cbdcae314a25af1f322fefeb8a","Department of Computing, Imperial College, London, United Kingdom; EEMCS, University of Twente, Twente, Netherlands; Computer Vision Center, Campus UAB, Barcelona, Spain","Orozco, J., Department of Computing, Imperial College, London, United Kingdom; Rudovic, O., Department of Computing, Imperial College, London, United Kingdom; Gonzàlez, J., Computer Vision Center, Campus UAB, Barcelona, Spain; Pantic, M., Department of Computing, Imperial College, London, United Kingdom, EEMCS, University of Twente, Twente, Netherlands","In this paper, we propose an On-line Appearance-Based Tracker (OABT) for simultaneous tracking of 3D head pose, lips, eyebrows, eyelids and irises in monocular video sequences. In contrast to previously proposed tracking approaches, which deal with face and gaze tracking separately, our OABT can also be used for eyelid and iris tracking, as well as 3D head pose, lips and eyebrows facial actions tracking. Furthermore, our approach applies an on-line learning of changes in the appearance of the tracked target. Hence, the prior training of appearance models, which usually requires a large amount of labeled facial images, is avoided. Moreover, the proposed method is built upon a hierarchical combination of three OABTs, which are optimized using a Levenberg-Marquardt Algorithm (LMA) enhanced with line-search procedures. This, in turn, makes the proposed method robust to changes in lighting conditions, occlusions and translucent textures, as evidenced by our experiments. Finally, the proposed method achieves head and facial actions tracking in real-time. © 2013 Elsevier B.V.","3D face tracking; Eyelid tracking; Facial action tracking; Iris tracking; Levenberg-marquardt algorithm; Line-search optimization; On-line appearance models","Face recognition; Optimization; Target tracking; Textures; Three dimensional computer graphics; 3D face tracking; Appearance models; Facial action; Iris tracking; Levenberg-Marquardt algorithm; Line searches; Eye tracking",Article,"Final","",Scopus,2-s2.0-84886396979
"Li W., Che M., Li F.","55770179700;36098984900;55770717100;","Gaze estimation research with single camera*",2013,"Communications in Computer and Information Science","332",,,"592","599",,1,"10.1007/978-3-642-34447-3_53","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879235259&doi=10.1007%2f978-3-642-34447-3_53&partnerID=40&md5=717ef9f7d69aebe6830be44b5fa08bfc","School of Computer Science and Technology, Tianjin University, Tianjin, China","Li, W., School of Computer Science and Technology, Tianjin University, Tianjin, China; Che, M., School of Computer Science and Technology, Tianjin University, Tianjin, China; Li, F., School of Computer Science and Technology, Tianjin University, Tianjin, China","Gaze estimation research has experienced significant progress in accuracy and tolerance of free head movement since reference light sources were introduced. The problem, however, is that it needs complex setup and restricted light environment to make sure there is no unexpected reflex light spot in the eye. This paper proposes a method that only needs one camera and no reference light source. The trade-off for minimizing the requirement is that only 3 out of 6 degrees-of-freedom of head movement is allowed. The 3D coordinates of feature points under restricted head movement can be determined. With these head movement information, we can compensate the results produced by homography between eye plane and screen plane. The model is simple and practical and the experimental data demonstrates that the RMS error is less than 1cm. © Springer-Verlag Berlin Heidelberg 2012.","Eye tracking; Gaze estimation; HCI; Single camera","Cameras; Degrees of freedom (mechanics); Economic and social effects; Eye movements; Human computer interaction; Light sources; 3D coordinates; Gaze estimation; Head movements; Homographies; Light environment; No references; Reference lights; Single cameras; Eye tracking",Article,"Final","",Scopus,2-s2.0-84879235259
"Bär T., Reuter J.F., Zöllner J.M.","49861130100;54786922500;7003750438;","Driver head pose and gaze estimation based on multi-template ICP 3-D point cloud alignment",2012,"IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC",,,"6338678","1797","1802",,21,"10.1109/ITSC.2012.6338678","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871190475&doi=10.1109%2fITSC.2012.6338678&partnerID=40&md5=f1932032d5d5b92a1f1cad76944bc3d3","Intelligent Systems and Production Engineering (ISPE), FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany","Bär, T., Intelligent Systems and Production Engineering (ISPE), FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; Reuter, J.F., Intelligent Systems and Production Engineering (ISPE), FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; Zöllner, J.M., Intelligent Systems and Production Engineering (ISPE), FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany","Head movements, combined with the line of gaze, play a fundamental role in predicting the driver's actions and in inferring his intention. However, a gaze tracking system for automotive applications needs to satisfy high demands: It must not disturb the driver in his freedom of movements, it must cover large and fast head turns in yaw and pitch, be resistant to changing illumination conditions, be fast enough to recognize fast mirror checks, which are performed almost exclusively through eye rather than head movements, and be accurate and reliable enough to derive high quality information for driver assistance systems relying on their output. In this work a multi-template, ICP-based gaze tracking system is introduced. The system determines the head pose and subsequently estimates the driver's line of gaze by analyzing the angles of the eyes. Due to a fast search of correspondences, and switching between point-to-point and point-to-plane alignment, real-time performance and high accuracy can be achieved. The system is compared with other state of the art head pose estimation systems based on a publicly available benchmark database, where a classification rate of 92% at a tolerance of 10 degrees in yaw could be achieved. We further show in the experiments section, that head rotations up to 4 radians per seconds can be handled. Taking the angles of the eyes into account, rather than the head pose only, the driver's line of sight could be successfully mapped to particular regions of interest. © 2012 IEEE.",,"Automotive applications; Benchmark database; Classification rates; Driver assistance system; Fast search; Freedom of movement; Gaze estimation; Gaze tracking system; Head movements; Head pose; Head Pose Estimation; Head rotation; High demand; High quality information; Illumination conditions; Line of Sight; Point cloud; Real time performance; Regions of interest; State of the art; Intelligent systems; Tracking (position); Image recognition",Conference Paper,"Final","",Scopus,2-s2.0-84871190475
"Quintas J., Almeida L., Brito M., Quintela G., Menezes P., Dias J.","37005875400;56231092000;57200874430;55516782000;56592011600;56962751600;","Context-based understanding of interaction intentions",2012,"Proceedings - IEEE International Workshop on Robot and Human Interactive Communication",,,"6343803","515","520",,6,"10.1109/ROMAN.2012.6343803","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870843528&doi=10.1109%2fROMAN.2012.6343803&partnerID=40&md5=81fc2b07fc6caa32c54e6d11711650d3","Institute of Systems and Robotics, University of Coimbra, Portugal; Institute Pedro Nunes, Coimbra, Portugal; Instituto Politécnico de Tomar, Tomar, Portugal; Khalifa University of Science, Technology and Research (KUSTAR), Abu Dhabi, United Arab Emirates","Quintas, J., Institute of Systems and Robotics, University of Coimbra, Portugal, Institute Pedro Nunes, Coimbra, Portugal; Almeida, L., Institute of Systems and Robotics, University of Coimbra, Portugal, Instituto Politécnico de Tomar, Tomar, Portugal; Brito, M., Institute of Systems and Robotics, University of Coimbra, Portugal; Quintela, G., Institute of Systems and Robotics, University of Coimbra, Portugal; Menezes, P., Institute of Systems and Robotics, University of Coimbra, Portugal; Dias, J., Institute of Systems and Robotics, University of Coimbra, Portugal, Khalifa University of Science, Technology and Research (KUSTAR), Abu Dhabi, United Arab Emirates","This paper focus in the importance of context awareness and intention understanding capabilities in modern robots when faced with different situations. The inclusion of such requirements in robot design aim for more intelligent robots capable to adapt its behaviours to the faced situations. Gaze estimation and gesture interpretation are modalities, closely related with context-depent human intention understanding, that are addressed in this work. © 2012 IEEE.",,"Context- awareness; Context-based; Gaze estimation; Human intentions; Intention understanding; Robot designs; Communication; Robots; Machine design",Conference Paper,"Final","",Scopus,2-s2.0-84870843528
"Martin S., Tawari A., Murphy-Chutorian E., Cheng S.Y., Trivedi M.","55510668500;36470963300;7801402744;7404685598;7103153314;","On the design and evaluation of robust head pose for visual user interfaces: Algorithms, databases, and comparisons",2012,"AutomotiveUI 2012 - 4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, In-cooperation with ACM SIGCHI - Proceedings",,,,"149","154",,29,"10.1145/2390256.2390281","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870493827&doi=10.1145%2f2390256.2390281&partnerID=40&md5=3307486e057b50510f2a3dcbcb01411e","Laboratory of Intelligent and Safe Automobiles, UCSD, San Diego, CA, United States; Google Inc., United States; HRL Laboratories LLC, United States","Martin, S., Laboratory of Intelligent and Safe Automobiles, UCSD, San Diego, CA, United States; Tawari, A., Laboratory of Intelligent and Safe Automobiles, UCSD, San Diego, CA, United States; Murphy-Chutorian, E., Laboratory of Intelligent and Safe Automobiles, UCSD, San Diego, CA, United States, Google Inc., United States; Cheng, S.Y., Laboratory of Intelligent and Safe Automobiles, UCSD, San Diego, CA, United States, HRL Laboratories LLC, United States; Trivedi, M., Laboratory of Intelligent and Safe Automobiles, UCSD, San Diego, CA, United States","An important goal in automotive user interface research is to predict a user's reactions and behaviors in a driving environment. The behavior of both drivers and passengers can be studied by analyzing eye gaze, head, hand, and foot movement, upper body posture, etc. In this paper, we focus on estimating head pose, which has been shown to be a good predictor of driver intent and a good proxy for gaze estimation, and provide a valuable head pose database for future comparative studies. Most existing head pose estimation algorithms are still struggling under large spatial head turns. Our method, however, relies on using facial features that are visible even during large spatial head turns to estimate head pose. The method is evaluated on the LISA-P Head Pose database, which has head pose data from on-road daytime and nighttime drivers of varying age, race, and gender; ground truth for head pose is provided using a motion capture system. In special regards to eye gaze estimation for automotive user interface study, the automatic head pose estimation technique presented in this paper can replace previous eye gaze estimation methods that rely on manual data annotation or be used in conjunction with them when necessary. Copyright © 2012 ACM.","Database; Facial features; Head pose","Body postures; Comparative studies; Data annotation; Driving environment; Eye-gaze; Facial feature; Gaze estimation; Ground truth; Head pose; Head Pose Estimation; Motion capture system; Visual user interfaces; Algorithms; Automobile drivers; Database systems; Estimation; Eye movements; Image recognition; Motion estimation; User interfaces",Conference Paper,"Final","",Scopus,2-s2.0-84870493827
[无可用作者姓名],[无可用的作者 ID],"2012 International Conference on E-Business Technology and Strategy, iCETS 2012",2012,"Communications in Computer and Information Science","332",,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885055283&partnerID=40&md5=834a733c66ce56ccd26b82c106e5d484",,"","The proceedings contain 65 papers. The special focus in this conference is on E-Business Technology and Strategy. The topics include: Consumer online shopping in Canada; integrating spatial decision support system with graph mining technique; concept based orchestration of web services using XOT; a multi-objective programming for web services composition; semi-automatic acquisition and formal representation of OpenAPI; eliciting security requirements method based on safety knowledge base; sale channel selection of information products in the presence of network externality; improving android security with layered structure instrumentation; a threat model-driven security testing approach for web application; a novel architecture on FPGA for face detection using jumping scanning mechanism; optimality of versioning for information products using niching SSGA; dynamic neural network ensemble construction for classification; a secure data exchange protocol for the internet of things; an improved secure communications protocol; a browser extension vulnerability detecting approach based on behavior monitoring and analysis; research on location selection based on genetic and simulated annealing algorithm; a screening method of security functional components based on fuzzy; eliminating false matches using geometric context; online service quality in social commerce websites; an improved algorithm for K-anonymity; a probability based similarity scoring for DNA motifs comparison; sharing and building management system based on handheld mobile devices; research on contingency plan simulation system based on CAS theory; consistent view mapping of large-scale ontology; a multi-objective optimization model for information system design; neighborhood covering based rule learning algorithms; program package for decision making; multi-modal fusion emotion recognition based on HMM and ANN; visual modeling for multi-level parallel computing environment based on DSL; a GPU-based face detection algorithm for low-bandwidth implementation; gaze estimation research with single camera; an improved text feature selection method for transfer learning; comprehensive mining on medical homepage records using Bayesian network approach; a hybrid recommendation model for HTML5 mobile web applications; Thai immigrant entrepreneurs in New Zealand; application-aware storage strategy for scientific data; the design and realization of 3G-based vehicular video surveillance system and a dynamic scanning approach based on trie matching for clustering XML data in E-business.",,,Conference Review,"Final","",Scopus,2-s2.0-84885055283
"Wang M., Maeda Y., Takahashi Y.","55701262000;7402845246;55705381800;","Human intention recognition via eye tracking based on fuzzy inference",2012,"6th International Conference on Soft Computing and Intelligent Systems, and 13th International Symposium on Advanced Intelligence Systems, SCIS/ISIS 2012",,,"6505330","846","851",,3,"10.1109/SCIS-ISIS.2012.6505330","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877824409&doi=10.1109%2fSCIS-ISIS.2012.6505330&partnerID=40&md5=65440f0430417fff7e316dfa98f494bf","Dept. of System Design Engineering, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan; Dept. of Human and Artificial Intelligent Systems, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan","Wang, M., Dept. of System Design Engineering, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan; Maeda, Y., Dept. of Human and Artificial Intelligent Systems, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan; Takahashi, Y., Dept. of Human and Artificial Intelligent Systems, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan","Intention recognition has been paid much attention on by researchers in intelligent systems recent years because it can make the interaction between human and intelligent agents to be more convenient. Intention recognition can use multiple factors as inputs such as gestures, face images and eye gaze position. In this paper, we propose user's eye gaze estimation position as input of a fuzzy system to achieve intention recognition. Our approach can be divided into two parts: user gaze estimation and intention recognition. In the user gaze estimation part, neural network has been used as the decision making unit, and then the user gaze position at the computer screen is estimated. In the intention recognition part, the intention is recognized by using of a fuzzy system after an initial intention region set has been found. An illustrative example of user's intention region recognition is given and discussed in details. © 2012 IEEE.",,"Computer screens; Decision making unit; Gaze estimation; Human intentions; Intention recognition; Multiple factors; Region recognition; User's intentions; Fuzzy systems; Intelligent agents; Soft computing; Intelligent systems",Conference Paper,"Final","",Scopus,2-s2.0-84877824409
"Waizenegger W., Atzpadin N., Schreer O., Feldmann I., Eisert P.","24825840200;23484214100;6603359258;56624704100;55892485300;","Model based 3D gaze estimation for provision of virtual eye contact",2012,"Proceedings - International Conference on Image Processing, ICIP",,,"6467274","1973","1976",,10,"10.1109/ICIP.2012.6467274","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875834405&doi=10.1109%2fICIP.2012.6467274&partnerID=40&md5=a0c40981babc4c0f4495aa2524a8ea90","Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany; Humboldt University, Berlin, Germany","Waizenegger, W., Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany, Humboldt University, Berlin, Germany; Atzpadin, N., Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany; Schreer, O., Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany; Feldmann, I., Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany; Eisert, P., Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany, Humboldt University, Berlin, Germany","In recent years, video communication has received a rapidly increasing interest on the market. Still unsolved is the problem of eye contact. The conferee still needs to decide whether to look into the camera or directly to the screen. Recently, a solution to this problem was presented which is based on a real-time 3D modeling of the conferees [1]. In order to achieve direct eye contact the authors defined a virtual camera directly on the screen in the eyes of the remote conferee. This paper discusses the problem of adequately positioning this virtual camera. A new approach will be presented which performs an eye and gaze tracking directly on the real-time 3D model rather than on the 2D image. Our methods not only provides robust and highly accurate results but is also able to additionally measure the distance between the conferees eye and the display with high precision. © 2012 IEEE.","3D; camera calibration; stereo gaze tracking; video communication","2D images; 3-d modeling; 3D; 3D models; Camera calibration; Eye contact; Gaze estimation; Gaze tracking; High precision; Highly accurate; Model-based OPC; New approaches; Video communications; Virtual camera; Cameras; Communication; Image processing; Three dimensional computer graphics; Tracking (position); Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-84875834405
"Martinez F., Carbone A., Pissaloux E.","39261924600;16174463200;7003850978;","Gaze estimation using local features and non-linear regression",2012,"Proceedings - International Conference on Image Processing, ICIP",,,"6467271","1961","1964",,33,"10.1109/ICIP.2012.6467271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875832894&doi=10.1109%2fICIP.2012.6467271&partnerID=40&md5=e1b88a214f5b325a3639ca65ac71be9f","ISIR, Université Pierre et Marie Curie (UPMC), 4 place Jussieu, 75005 Paris, France","Martinez, F., ISIR, Université Pierre et Marie Curie (UPMC), 4 place Jussieu, 75005 Paris, France; Carbone, A., ISIR, Université Pierre et Marie Curie (UPMC), 4 place Jussieu, 75005 Paris, France; Pissaloux, E., ISIR, Université Pierre et Marie Curie (UPMC), 4 place Jussieu, 75005 Paris, France","In this paper, we present an appearance-based gaze estimation method for a head-mounted eye tracker. The idea is to extract discriminative image descriptors with respect to gaze before applying a regression scheme. We employ multilevel Histograms of Oriented Gradients (HOG) features as our appearance descriptor. To learn the mapping between eye appearance and gaze coordinates, two learning-based approaches are evaluated : Support Vector Regression (SVR) and Relevance Vector Regression (RVR). Experimental results demonstrate that, despite the high dimensionality, our method works well and RVR provides a more efficient and generalized solution than SVR by retaining a low number of basis functions. © 2012 IEEE.","features extraction; gaze estimation; nonlinear regression","Appearance based; Basis functions; Descriptors; Eye trackers; Features extraction; Gaze estimation; Generalized solution; High dimensionality; Histograms of oriented gradients (HoG); Image descriptors; Learning-based approach; Local feature; Non-linear regression; Support vector regression (SVR); Image processing; Regression analysis",Conference Paper,"Final","",Scopus,2-s2.0-84875832894
"Chaudhuri A., Dasgupta A., Routray A.","55638576400;56402020900;55927861800;","Video & EOG based investigation of pure saccades in human subjects",2012,"4th International Conference on Intelligent Human Computer Interaction: Advancing Technology for Humanity, IHCI 2012",,,"6481872","","",,12,"10.1109/IHCI.2012.6481872","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875721083&doi=10.1109%2fIHCI.2012.6481872&partnerID=40&md5=2e3254ceabfd862b2ad36fcbf9d3844c","Department of Electrical Engineering, IIT Kharagpur, Kharagpur, India","Chaudhuri, A., Department of Electrical Engineering, IIT Kharagpur, Kharagpur, India; Dasgupta, A., Department of Electrical Engineering, IIT Kharagpur, Kharagpur, India; Routray, A., Department of Electrical Engineering, IIT Kharagpur, Kharagpur, India","Human Computer Interaction (HCI) is the methodology through which computing systems understand actions of human beings. Some important applications of HCI as reported in literature are eye gaze estimation, eye tracking, alertness and health monitoring etc. Of them major applications are based upon eye movements. However, accurate estimation of these measurements is still a challenge in research community. A particular type of eye movement i.e. saccadic eye movement has potential applications in HCI such as alertness assessment, disease diagnosis etc. Electrooculography (EOG) based measurement of saccadic movements has been reported to be an accurate method. However being a contact based method, alternative method as suggested by literature is image based approach. This paper aims to investigate the correlation between video and EOG based observation of pure horizontal directional saccades to establish a relationship between image and EOG signals. © 2012 IEEE.","correlation of EOG and video; EOG; high speed imaging; pupil position; saccade","Accurate estimation; Alternative methods; EOG; High speed imaging; Human computer interaction (HCI); pupil position; Research communities; Saccadic eye movements; Diagnosis; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84875721083
"Happy S.L., Dasgupta A., George A., Routray A.","55638461500;56402020900;55638380800;55927861800;","A video database of human faces under near Infra-Red illumination for human computer interaction applications",2012,"4th International Conference on Intelligent Human Computer Interaction: Advancing Technology for Humanity, IHCI 2012",,,"6481868","","",,5,"10.1109/IHCI.2012.6481868","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875698135&doi=10.1109%2fIHCI.2012.6481868&partnerID=40&md5=46cd7eefdeebb245b563a5f6c3671fc5","Department of Electrical Engineering, IIT Kharagpur, India","Happy, S.L., Department of Electrical Engineering, IIT Kharagpur, India; Dasgupta, A., Department of Electrical Engineering, IIT Kharagpur, India; George, A., Department of Electrical Engineering, IIT Kharagpur, India; Routray, A., Department of Electrical Engineering, IIT Kharagpur, India","Human Computer Interaction (HCI) is an evolving area of research for coherent communication between computers and human beings. Some of the important applications of HCI as reported in literature are face detection, face pose estimation, face tracking and eye gaze estimation. Development of algorithms for these applications is an active field of research. However, availability of standard database to validate such algorithms is insufficient. This paper discusses the creation of such a database created under Near Infra-Red (NIR) illumination. NIR illumination has gained its popularity for night mode applications since prolonged exposure to Infra-Red (IR) lighting may lead to many health issues. The database contains NIR videos of 60 subjects in different head orientations and with different facial expressions, facial occlusions and illumination variation. This new database can be a very valuable resource for development and evaluation of algorithms on face detection, eye detection, head tracking, eye gaze tracking etc. in NIR lighting. © 2012 IEEE.","face database; face detection; face tracking; head pose; Near Infra-Red(NIR) illumination","Coherent communication; Face database; Face pose estimation; Face Tracking; Head pose; Human computer interaction (HCI); Illumination variation; Near infra red; Algorithms; Database systems; Eye protection; Gesture recognition; Infrared devices; Lighting; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-84875698135
"Hong A.K.A., Pelz J., Cockburn J.","55617686100;7007018556;7102249479;","Lightweight, low-cost, side-mounted mobile eye tracking system",2012,"2012 Western New York Image Processing Workshop, WNYIPW 2012",,,"6466645","1","4",,3,"10.1109/WNYIPW.2012.6466645","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874730330&doi=10.1109%2fWNYIPW.2012.6466645&partnerID=40&md5=db4e4e240a7897e894dd700d9b1f66c0","Rochester Institute of Technology, Center of Imaging Science, United States; Department of Computer Engineering, 83 Lomb Memorial Dr., Rochester, NY 14623, United States","Hong, A.K.A., Rochester Institute of Technology, Center of Imaging Science, United States; Pelz, J., Department of Computer Engineering, 83 Lomb Memorial Dr., Rochester, NY 14623, United States; Cockburn, J., Rochester Institute of Technology, Center of Imaging Science, United States","Commercial mobile eye tracking systems are readily available, but are costly and complex. They have an additional disadvantage in that the eye cameras are placed directly in the field of view of the subject in order to obtain a clear frontal view of the eye. We propose a lightweight, low-cost, side-mounted mobile eye tracking system that uses side-view eye images to estimate the gaze of the subject. Cameras are mounted on the side of the head using curved mirrors to split the captured frames into scene and eye images. A hybrid algorithm using both feature-based models and appearance-based models is designed to accommodate this novel system. Image sequences, consisting of 4339 frames from seven subjects are analyzed by the algorithm, resulting in a successful gaze estimation rate of 95.7%. © 2012 IEEE.","computer vision; Eye tracking; image processing; mobile; side-mounted","Appearance-based models; Captured frame; Curved mirror; Eye camera; Eye images; Eye-tracking; Feature-based model; Field of views; Gaze estimation; Hybrid algorithms; Image sequence; mobile; Mobile eye-tracking; side-mounted; Algorithms; Cameras; Computer vision; Image processing; Tracking (position); Cost benefit analysis",Conference Paper,"Final","",Scopus,2-s2.0-84874730330
"Lu F., Sugano Y., Okabe T., Sato Y.","54956194300;7005470045;7201390055;35230954300;","Head pose-free appearance-based gaze sensing via eye image synthesis",2012,"Proceedings - International Conference on Pattern Recognition",,,"6460306","1008","1011",,37,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874568334&partnerID=40&md5=9eb7e1c19aa4cbac257fca8c18a0e5c5","Institute of Industrial Science, University of Tokyo, Japan","Lu, F., Institute of Industrial Science, University of Tokyo, Japan; Sugano, Y., Institute of Industrial Science, University of Tokyo, Japan; Okabe, T., Institute of Industrial Science, University of Tokyo, Japan; Sato, Y., Institute of Industrial Science, University of Tokyo, Japan","This paper addresses the problem of estimating human gaze from eye appearance under free head motion. Allowing head motion remains challenging because eye appearance changes significantly for different head poses, and thus new head poses require new training images. To avoid repetitive training, we propose to produce synthetic training images for varying head poses. First, we model pixel displacements between head-moving eye images as 1D pixel flows, and then produce such flows to synthesize new training images from the original training images captured under a fixed default head pose. Specifically, we produce all the required 1D flows by using only four additionally captured images. Our method was successfully tested with extensive experiments to demonstrate its effectiveness. © 2012 ICPR Org Committee.",,"Appearance based; Eye images; Free-head; Head motion; Head pose; Pixel displacement; Pixel flow; Repetitive training; Training image; Software engineering; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-84874568334
"Cheok L.-T., Heo S.Y., Mitrani D., Tewari A.","6505870872;55604626800;55605006200;55604513200;","Automatic actor recognition for video services on mobile devices",2012,"Proceedings - 2012 IEEE International Symposium on Multimedia, ISM 2012",,,"6424694","384","385",,1,"10.1109/ISM.2012.80","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874231027&doi=10.1109%2fISM.2012.80&partnerID=40&md5=34b2bd28d1c6e43bd7c22eecd06fb1ef","Dallas Technology Lab., Samsung Telecommunications America, Richardson, TX, United States","Cheok, L.-T., Dallas Technology Lab., Samsung Telecommunications America, Richardson, TX, United States; Heo, S.Y., Dallas Technology Lab., Samsung Telecommunications America, Richardson, TX, United States; Mitrani, D., Dallas Technology Lab., Samsung Telecommunications America, Richardson, TX, United States; Tewari, A., Dallas Technology Lab., Samsung Telecommunications America, Richardson, TX, United States","Face recognition is one of the most promising and successful applications of image analysis and understanding. Applications include biometrics identification, gaze estimation, emotion recognition, human computer interface, among others. A closed system trained to recognize only a predetermined number of faces will become obsolete very easily. In this paper, we describe a demo that we have developed using face detection and recognition algorithms for recognizing actors/actresses in movies. The demo runs on a Samsung tablet to recognize actors/actresses in the video. We also present our proposed method that allows user to interact with the system during training while watching video. New faces are tracked and trained into new face classifiers as video is continuously playing and the face database is updated dynamically. © 2012 IEEE.","Face detection; Face recognition; Image analysis; Image understanding","Closed systems; Emotion recognition; Face classifiers; Face database; Face detection and recognition; Gaze estimation; Human computer interfaces; Samsung; Video services; Biometrics; Human computer interaction; Image analysis; Image understanding; Mobile devices; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-84874231027
"Jafari R., Ziou D.","12791016800;7004105959;","Gaze estimation using Kinect/PTZ camera",2012,"2012 IEEE International Symposium on Robotic and Sensors Environments, ROSE 2012 - Proceedings",,,"6402633","13","18",,15,"10.1109/ROSE.2012.6402633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873556881&doi=10.1109%2fROSE.2012.6402633&partnerID=40&md5=2a098e7626e619eb244e2882d1033e6c","Departement d'Informatique, Universite de Sherbrooke, QC J1K 2R1, Canada","Jafari, R., Departement d'Informatique, Universite de Sherbrooke, QC J1K 2R1, Canada; Ziou, D., Departement d'Informatique, Universite de Sherbrooke, QC J1K 2R1, Canada","This paper describes a novel method for eye-gaze estimation under normal head movement. In this method, head position and orientation are acquired by Kinect while eye direction is obtained by PTZ camera. We propose the Bayesian multinomial logistic regression based on a variational approximation to construct a gaze mapping function from head and eyes features. Our proposed method eliminates stationary head position, awkward personal calibration procedure and active light source as three common drawbacks in most conventional techniques. The efficiency of the proposed method is validated by performance evaluation for different users under varying head position and orientation. © 2012 IEEE.",,"Bayesian; Calibration procedure; Conventional techniques; Eye direction; Eye-gaze; Gaze estimation; Head movements; Head position; Mapping functions; Multinomial logistic regression; Performance evaluation; PTZ camera; Variational approximation; Cameras; Light sources; Logistics; Robotics; Sensors; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84873556881
"McMurrough C., Rich J., Metsis V., Nguyen A., Makedon F.","34870197500;55257997600;25923561200;57198227293;7003437865;","Low-cost head position tracking for gaze point estimation",2012,"ACM International Conference Proceeding Series",,,"22","","",,5,"10.1145/2413097.2413125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871961993&doi=10.1145%2f2413097.2413125&partnerID=40&md5=8d3d68db9144e84b9e60c447f5ed0a4c","Heracleia Human Centered Computing Laboratory, University of Texas at Arlington, United States","McMurrough, C., Heracleia Human Centered Computing Laboratory, University of Texas at Arlington, United States; Rich, J., Heracleia Human Centered Computing Laboratory, University of Texas at Arlington, United States; Metsis, V., Heracleia Human Centered Computing Laboratory, University of Texas at Arlington, United States; Nguyen, A., Heracleia Human Centered Computing Laboratory, University of Texas at Arlington, United States; Makedon, F., Heracleia Human Centered Computing Laboratory, University of Texas at Arlington, United States","In this paper, we present a low-cost solution for real-time tracking of a human user's head position with respect to a video display source for eye gaze estimation in an assistive setting. The solution utilizes a wearable headset equipped with sensors found in commercially available off-the-shelf video gaming devices in order to minimize hardware complexity and expense. A pair of Nintendo Wiimote imaging sensors are used to create a stereo camera for 6DOF position tracking of the headset, while a modified Playstation Eye monocular camera is used to track the pupil position. The resulting tracking hardware is able to measure the 3D position of four infrared LEDs mounted at known locations on the video display using triangulation of the stereo camera data. Integration of the head tracking estimate with a computer vision based pupil tracking solution in order to compute the user's point of gaze is also described.","Epipolar geometry; Eye gaze; Eye tracking; Head tracking; Point of regard; Stereo camera","Epipolar geometry; Eye-gaze; Eye-tracking; Head tracking; Point of regards; Stereo cameras; Cameras; Computer vision; Hardware; Interactive computer graphics; Light emitting diodes; Sensors; Tracking (position)",Conference Paper,"Final","",Scopus,2-s2.0-84871961993
"McMurrough C., Conly C., Athitsos V., Makedon F.","34870197500;55497079800;6602972245;7003437865;","3D point of gaze estimation using head-mounted RGB-D cameras",2012,"ASSETS'12 - Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility",,,,"283","284",,6,"10.1145/2384916.2384994","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869768360&doi=10.1145%2f2384916.2384994&partnerID=40&md5=a554e2416adf309e8d68d0477855d706","Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, United States","McMurrough, C., Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, United States; Conly, C., Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, United States; Athitsos, V., Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, United States; Makedon, F., Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, United States","This paper presents a low-cost, wearable headset for 3D Point of Gaze PoGestimation in assistive applications. The device consists of an eye tracking camera and forward facing RGB-D scene camera which, together, provide an estimate of the user gaze vector and its intersection with a 3D point in space. The resulting system is able to compute the 3D PoG in real-time using inexpensive and readily available hardware components.","Assistive environments; Eyetracking; Human-computer interaction; Multimodal systems","Assistive; Eye-tracking; Hardware components; Multimodal system; Point of gaze; Cameras; Human computer interaction; Vector spaces; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84869768360
"Nakazawa A., Nitschke C.","35807510800;36947002100;","Point of gaze estimation through corneal surface reflection in an active illumination environment",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7573 LNCS","PART 2",,"159","172",,45,"10.1007/978-3-642-33709-3_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867843925&doi=10.1007%2f978-3-642-33709-3_12&partnerID=40&md5=3739a8f19cdddedb5939e37710aed064","Cybermedia Center, Osaka University, 1-32 Machikaneyama, Toyonaka, Osaka 560-0032, Japan; PRESTO, Japan Science and Technology Agency (JST), 4-1-8 Honmachi, Kawaguchi, Saitama 332-0012, Japan","Nakazawa, A., Cybermedia Center, Osaka University, 1-32 Machikaneyama, Toyonaka, Osaka 560-0032, Japan, PRESTO, Japan Science and Technology Agency (JST), 4-1-8 Honmachi, Kawaguchi, Saitama 332-0012, Japan; Nitschke, C., Cybermedia Center, Osaka University, 1-32 Machikaneyama, Toyonaka, Osaka 560-0032, Japan","Eye gaze tracking (EGT) is a common problem with many applications in various fields. While recent methods have achieved improvements in accuracy and usability, current techniques still share several limitations. A major issue is the need for external calibration between the gaze camera system and the scene, which commonly restricts to static planar surfaces and leads to parallax errors. To overcome these issues, the paper proposes a novel scheme that uses the corneal imaging technique to directly analyze reflections from a scene illuminated with structured light. This comprises two major contributions: First, an analytic solution is developed for the forward projection problem to obtain the gaze reflection point (GRP), where light from the point of gaze (PoG) in the scene reflects at the corneal surface into an eye image. We also develop a method to compensate for the individual offset between the optical axis and true visual axis. Second, introducing active coded illumination enables robust and accurate matching at the GRP to obtain the PoG in a scene image, which is the first use of this technique in EGT and corneal reflection analysis. For this purpose, we designed a special high-power IR LED-array projector. Experimental evaluation with a prototype system shows that the proposed scheme achieves considerable accuracy and successfully supports depth-varying environments. © 2012 Springer-Verlag.",,"Active illumination; Analytic solution; Camera systems; Corneal reflection; Experimental evaluation; External calibration; Eye gaze tracking; Eye images; High-power; Optical axis; Parallax error; Planar surface; Point of gaze; Prototype system; Reflection points; Scene image; Structured Light; Surface reflections; Geometrical optics; Gesture recognition; Imaging techniques; Light emitting diodes; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84867843925
[无可用作者姓名],[无可用的作者 ID],"Computational Modelling of Objects Represented in Images: Fundamentals, Methods and Applications III - Proceedings of the International Symposium, CompIMAGE 2012",2012,"Computational Modelling of Objects Represented in Images: Fundamentals, Methods and Applications III - Proceedings of the International Symposium, CompIMAGE 2012",,,,"","",489,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867739763&partnerID=40&md5=e7bc960a853ae4067c10717f531e0ac6",,"","The proceedings contain 84 papers. The topics discussed include: detection system from the driver's hands based on address event representation (AER); sclera segmentation for gaze estimation and iris localization in unconstrained images; morphing billboards for accurate reproduction of shape and shading of articulated objects with an application to real-time hand tracking; flow measurement in open channels based in digital image processing to debris flow study; evaluation of the Menzies method potential for automatic dermoscopic image analysis; texture image segmentation with smooth gradients and local information; automated shape analysis landmarks detection for medical image processing; Wishart classification comparison between compact and quad-polarimetric SAR imagery using RADARSAT2 data; comparing land cover maps obtained from remote sensing for deriving urban indicators; and mandibular nerve canal identification for preoperative planning in oral implantology.",,,Conference Review,"Final","",Scopus,2-s2.0-84867739763
"Marcon M., Frigerio E., Tubaro S.","7004432686;54995996800;7003411765;","Sclera segmentation for gaze estimation and iris localization in unconstrained images",2012,"Computational Modelling of Objects Represented in Images: Fundamentals, Methods and Applications III - Proceedings of the International Symposium, CompIMAGE 2012",,,,"25","29",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867707282&partnerID=40&md5=3fd0846924fb92bac25f41f7d7ebda50","Politecnico di Milano, Dipartimento di Elettronica e Informazione, Milano, Italy","Marcon, M., Politecnico di Milano, Dipartimento di Elettronica e Informazione, Milano, Italy; Frigerio, E., Politecnico di Milano, Dipartimento di Elettronica e Informazione, Milano, Italy; Tubaro, S., Politecnico di Milano, Dipartimento di Elettronica e Informazione, Milano, Italy","Accurate localization of different eye's parts from videos or still images is a crucial step in many image processing applications that range from iris recognition in Biometrics to gaze estimation for Human Computer Interaction (HCI), impaired people aid or, even, marketing analysis for products attractiveness. Notwithstanding this, actually, most of available implementations for eye's parts segmentation are quite invasive, imposing a set of constraints both on the environment and on the user itself limiting their applicability to high security Biometrics or to cumbersome interfaces. In this paper we propose a novel approach to segment the sclera, the white part of the eye. We concentrated on this area since, thanks to the dissimilarity with other eye's parts, its identification can be performed in a robust way against light variations, reflections and glasses lens flare. An accurate sclera segmentation is a fundamental step in iris and pupil localization with respect to the eyeball center and to its relative rotation with respect to the head orientation. Once the sclera is correctly defined, iris, pupil and relative eyeball rotation can be found with high accuracy even in non-frontal noisy images. Furthermore its particular geometry, resembling in most of cases a triangle with bent sides, both on the left and on the right of the iris, can be fruitfully used for accurate eyeball rotation estimation. The proposed technique is based on a statistical approach (supported by some heuristic assumptions) to extract discriminating descriptors for sclera and non-sclera pixels. A Support Vector Machine (SVM) is then used as a final supervised classifier. © 2012 Taylor & Francis Group.",,"Descriptors; Gaze estimation; Image processing applications; Impaired people; Iris localization; Iris recognition; Marketing analysis; Noisy image; Pupil localization; Relative rotation; Statistical approach; Still images; Supervised classifiers; Biometrics; Support vector machines; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-84867707282
"Chaaraoui A.A., Climent-Pérez P., Flórez-Revuelta F.","55122683300;55123271700;13106226300;","A review on vision techniques applied to Human Behaviour Analysis for Ambient-Assisted Living",2012,"Expert Systems with Applications","39","12",,"10873","10888",,142,"10.1016/j.eswa.2012.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861190873&doi=10.1016%2fj.eswa.2012.03.005&partnerID=40&md5=806f740b734f219fb450baad3acd17c9","Department of Computing Technology, University of Alicante, P.O. Box 99, E-03080 Alicante, Spain","Chaaraoui, A.A., Department of Computing Technology, University of Alicante, P.O. Box 99, E-03080 Alicante, Spain; Climent-Pérez, P., Department of Computing Technology, University of Alicante, P.O. Box 99, E-03080 Alicante, Spain; Flórez-Revuelta, F., Department of Computing Technology, University of Alicante, P.O. Box 99, E-03080 Alicante, Spain","Human Behaviour Analysis (HBA) is more and more being of interest for computer vision and artificial intelligence researchers. Its main application areas, like Video Surveillance and Ambient-Assisted Living (AAL), have been in great demand in recent years. This paper provides a review on HBA for AAL and ageing in place purposes focusing specially on vision techniques. First, a clearly defined taxonomy is presented in order to classify the reviewed works, which are consequently presented following a bottom-up abstraction and complexity order. At the motion level, pose and gaze estimation as well as basic human movement recognition are covered. Next, the mainly used action and activity recognition approaches are presented with examples of recent research works. Increasing the degree of semantics and the time interval involved in the HBA, finally the behaviour level is reached. Furthermore, useful tools and datasets are analysed in order to provide help for initiating projects. © 2012 Elsevier Ltd. All rights reserved.","Action recognition; Activities of daily living (ADLs); Activity recognition; Ambient-Assisted Living; Computer vision; Human behaviour; Motion analysis","Artificial intelligence; Assisted living; Computer vision; Image recognition; Motion analysis; Motion estimation; Security systems; Semantics; Action recognition; Activities of daily living (ADLs); Activity recognition; Ambient assisted living; Human behaviours; Behavioral research",Article,"Final","",Scopus,2-s2.0-84861190873
"Kang S., Kim S., Lee Y.-S., Jeon G.","8977553900;56972759400;57199021757;15022497800;","Analysis of screen resolution according to gaze estimation in the 3D space",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7425 LNCS",,,"271","277",,1,"10.1007/978-3-642-32645-5_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866051116&doi=10.1007%2f978-3-642-32645-5_35&partnerID=40&md5=c284e638361bb6162f5090b4abd8c214","Dept. of Embedded Systems Engineering, Univ. of Incheon, 12-1 Songdo-dong, Yeonsu-gu, Incheon, 406-772, South Korea","Kang, S., Dept. of Embedded Systems Engineering, Univ. of Incheon, 12-1 Songdo-dong, Yeonsu-gu, Incheon, 406-772, South Korea; Kim, S., Dept. of Embedded Systems Engineering, Univ. of Incheon, 12-1 Songdo-dong, Yeonsu-gu, Incheon, 406-772, South Korea; Lee, Y.-S., Dept. of Embedded Systems Engineering, Univ. of Incheon, 12-1 Songdo-dong, Yeonsu-gu, Incheon, 406-772, South Korea; Jeon, G., Dept. of Embedded Systems Engineering, Univ. of Incheon, 12-1 Songdo-dong, Yeonsu-gu, Incheon, 406-772, South Korea","This paper proposes the optimum interpretation of screen resolution of gaze estimation according to the position in the 3D space between a user and a camera. The screen resolution is directly related to the moving distance of the pupil by pixels from end to end of a display. Also, the relationship of the 3D position between a user and a camera influences on the moving distance, and the screen resolution is decided. In this paper, by the technology of gaze estimation using single camera, the maximum screen resolution by the 3D position is shown. The accuracy of gaze estimation using the maximum screen resolution is the accuracy of average 68.39%. This paper suggests the maximum screen resolution that shows more than 90% of accuracy by the control of screen resolution. © 2012 Springer-Verlag.","3D location; Gaze estimation; human computer interaction; screen resolution","3-D space; 3D positions; End to end; Gaze estimation; Screen resolution; Single cameras; Cameras; Human computer interaction; Information technology; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-84866051116
"Mora K.A.F., Odobez J.-M.","55336423200;57203103085;","Gaze estimation from multimodal kinect data",2012,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops",,,"6239182","25","30",,85,"10.1109/CVPRW.2012.6239182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865005447&doi=10.1109%2fCVPRW.2012.6239182&partnerID=40&md5=5bf90adf5215198d2729ff72d1e459ea","Idiap Research Institute, CH-1920, Martigny, Switzerland; École Polytechnique Fédéral de Lausanne, CH-1015, Lausanne, Switzerland","Mora, K.A.F., Idiap Research Institute, CH-1920, Martigny, Switzerland, École Polytechnique Fédéral de Lausanne, CH-1015, Lausanne, Switzerland; Odobez, J.-M., Idiap Research Institute, CH-1920, Martigny, Switzerland, École Polytechnique Fédéral de Lausanne, CH-1015, Lausanne, Switzerland","This paper addresses the problem of free gaze estimation under unrestricted head motion. More precisely, unlike previous approaches that mainly focus on estimating gaze towards a small planar screen, we propose a method to estimate the gaze direction in the 3D space. In this context the paper makes the following contributions: (i) leveraging on Kinect device, we propose a multimodal method that rely on depth sensing to obtain robust and accurate head pose tracking even under large head pose, and on the visual data to obtain the remaining eye-in-head gaze directional information from the eye image; (ii) a rectification scheme of the image that exploits the 3D mesh tracking, allowing to conduct a head pose free eye-in-head gaze directional estimation; (iii) a simple way of collecting ground truth data thanks to the Kinect device. Results on three users demonstrate the great potential of our approach. © 2012 IEEE.",,"3-D space; 3D meshes; Depth sensing; Directional information; Eye images; Gaze direction; Gaze estimation; Ground truth data; Head motion; Head pose; Head-pose tracking; Multi-modal; Planar screen; Visual data; Estimation; Gesture recognition; Three dimensional; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84865005447
"Cho C.W., Lee J.W., Shin K.Y., Lee E.C., Park K.R., Lee H., Cha J.","36056134600;56557988600;35812577400;14009024200;8983316300;35102576600;11339697400;","Gaze detection by wearable eye-tracking and NIR LED-based head-tracking device based on SVR",2012,"ETRI Journal","34","4",,"542","552",,21,"10.4218/etrij.12.0111.0193","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864921485&doi=10.4218%2fetrij.12.0111.0193&partnerID=40&md5=f7a832aa86266c54e808652335a0ad6f","Department of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea; Division of Computer Science, Sangmyung University, Seoul, South Korea; Division of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea; Broadcasting and Telecommunications Convergence Research Laboratory, ETRI, Daejeon, South Korea","Cho, C.W., Department of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea; Lee, J.W., Department of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea; Shin, K.Y., Department of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea; Lee, E.C., Division of Computer Science, Sangmyung University, Seoul, South Korea; Park, K.R., Division of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea; Lee, H., Broadcasting and Telecommunications Convergence Research Laboratory, ETRI, Daejeon, South Korea; Cha, J., Broadcasting and Telecommunications Convergence Research Laboratory, ETRI, Daejeon, South Korea","In this paper, a gaze estimation method is proposed for use with a large-sized display at a distance. Our research has the following four novelties: this is the first study on gaze-tracking for large-sized displays and large Z (viewing) distances; our gaze-tracking accuracy is not affected by head movements since the proposed method tracks the head by using a near infrared camera and an infrared light-emitting diode; the threshold for local binarization of the pupil area is adaptively determined by using a p-tile method based on circular edge detection irrespective of the eyelid or eyelash shadows; and accurate gaze position is calculated by using two support vector regressions without complicated calibrations for the camera, display, and user's eyes, in which the gaze positions and head movements are used as feature values. The root mean square error of gaze detection is calculated as 0.79° for a 30-inch screen. © 2012 ETRI.","Gaze-tracking; Large-sized display; Support vector regression","Binarizations; Eye-tracking; Feature values; Gaze detection; Gaze estimation; Gaze-tracking; Head movements; Infrared light-emitting diodes; Near Infrared Cameras; Root mean square errors; Support vector regression (SVR); Edge detection; Eye movements; Mean square error; Light emitting diodes",Article,"Final","",Scopus,2-s2.0-84864921485
"Valenti R., Sebe N., Gevers T.","57192175392;57204924633;7003472472;","What are you looking at? Improving visual gaze estimation by saliency",2012,"International Journal of Computer Vision","98","3",,"324","334",,17,"10.1007/s11263-011-0511-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868302063&doi=10.1007%2fs11263-011-0511-6&partnerID=40&md5=a4613c741ef1e6b74ecd7a564003b45e","Intelligent Systems Lab. Amsterdam, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, Netherlands; Department of Information Engineering and Computer Science, University of Trento, via Sommarive 14, 38100 Povo (Trento), Italy","Valenti, R., Intelligent Systems Lab. Amsterdam, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, Netherlands; Sebe, N., Department of Information Engineering and Computer Science, University of Trento, via Sommarive 14, 38100 Povo (Trento), Italy; Gevers, T., Intelligent Systems Lab. Amsterdam, University of Amsterdam, Science Park 904, 1098 XH Amsterdam, Netherlands","In this paper we present a novel mechanism to obtain enhanced gaze estimation for subjects looking at a scene or an image. The system makes use of prior knowledge about the scene (e.g. an image on a computer screen), to define a probability map of the scene the subject is gazing at, in order to find the most probable location. The proposed system helps in correcting the fixations which are erroneously estimated by the gaze estimation device by employing a saliency framework to adjust the resulting gaze point vector. The system is tested on three scenarios: using eye tracking data, enhancing a low accuracy webcam based eye tracker, and using a head pose tracker. The correlation between the subjects in the commercial eye tracking data is improved by an average of 13.91%. The correlation on the low accuracy eye gaze tracker is improved by 59.85%, and for the head pose tracker we obtain an improvement of 10.23%. These results show the potential of the system as a way to enhance and self-calibrate different visual gaze estimation systems. © Springer Science+Business Media, LLC 2011.","Eye location; Gaze estimation; HCI; Head pose; Saliency","Computer screens; Eye gaze trackers; Eye location; Eye trackers; Eye-tracking; Gaze estimation; Gaze point; Head pose; Prior knowledge; Probability maps; Saliency; Artificial intelligence; Human computer interaction; Software engineering",Article,"Final","",Scopus,2-s2.0-84868302063
"Cerrolaza J.J., Villanueva A., Cabeza R.","23395863100;7101612861;36763933900;","Study of polynomial mapping functions in video-oculography eye trackers",2012,"ACM Transactions on Computer-Human Interaction","19","2","10","","",,39,"10.1145/2240156.2240158","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866318131&doi=10.1145%2f2240156.2240158&partnerID=40&md5=4dff0c0c0823336318105d905f0245af","Department of Electrical and Electronic Engineering, Public University of Navarra, Campus Arrosadia, Pamplona, Spain","Cerrolaza, J.J., Department of Electrical and Electronic Engineering, Public University of Navarra, Campus Arrosadia, Pamplona, Spain; Villanueva, A., Department of Electrical and Electronic Engineering, Public University of Navarra, Campus Arrosadia, Pamplona, Spain; Cabeza, R., Department of Electrical and Electronic Engineering, Public University of Navarra, Campus Arrosadia, Pamplona, Spain","Gaze-tracking data have been used successfully in the design of new input devices and as an observational technique in usability studies. Polynomial-based Video-Oculography (VOG) systems are one of the most attractive gaze estimation methods thanks to their simplicity and ease of implementation. Although the functionality of these systems is generally acceptable, there has been no thorough comparative study to date of how themapping equations affect the final system response. After developing a taxonomic classification of calibration functions, we examined over 400,000 models and evaluated the validity of several conventional assumptions. Our rigorous experimental procedure enabled us to optimize the calibration process for a real VOG gaze-tracking system and halve the calibration time while avoiding a detrimental effect on the accuracy or tolerance to head movement. Finally, a geometry-based method is implemented and tested. The results and performance is compared with those obtained by the general purpose expressions. © 2012 ACM.","Calibration; Eye-tracking; Gaze-tracking; Human computer interaction; Multiple linear regression; Video-oculography","Calibration functions; Calibration process; Comparative studies; Experimental procedure; Eye trackers; Eye-tracking; Gaze estimation; Gaze-tracking; General purpose; Head movements; Input devices; Mapping functions; Multiple linear regressions; System response; Usability studies; Video oculography; Calibration; Human computer interaction; Linear regression; Video cameras",Article,"Final","",Scopus,2-s2.0-84866318131
"Coutinho F.L., Morimoto C.H.","22233254400;7102275798;","Augmenting the robustness of cross-ratio gaze tracking methods to head movement",2012,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"59","66",,11,"10.1145/2168556.2168565","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862705967&doi=10.1145%2f2168556.2168565&partnerID=40&md5=2d207f1afc87055236e526c38d94b390","Computer Science Department, University of São Paulo, Brazil","Coutinho, F.L., Computer Science Department, University of São Paulo, Brazil; Morimoto, C.H., Computer Science Department, University of São Paulo, Brazil","Remote gaze estimation using a single non-calibrated camera, simple user calibration or calibration free, and robust to head movements are very desirable features of eye tracking systems. Because cross-ratio (CR) is an invariant property of projective geometry, gaze estimation methods that rely on this property have the potential to provide these features, though most current implementations rely on a few simplifications that compromise the performance of the method. In this paper, the CR method for gaze tracking is revisited, and we introduce a new method that explicitly compensates head movements using a simple 3 parameter eye model. The method uses a single non-calibrated camera and requires a simple calibration procedure per user to estimate the eye parameters. We have conducted simulations and experiments with real users that show significant improvements over current state-of-the-art CR methods that do not explicitly compensate for head motion. © 2012 ACM.","cross-ratio; eye gaze tracking; head movement compensation","Calibration free; Calibration procedure; Cross-ratios; Eye gaze tracking; Eye model; Eye parameters; Eye tracking systems; Gaze estimation; Gaze tracking; Head motion; Head movements; Invariant properties; Over current; Projective geometry; Remote gaze estimation; User calibration; Calibration; Gesture recognition; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84862705967
"Świrski L., Bulling A., Dodgson N.","53867304000;6505807414;6603866623;","Robust real-time pupil tracking in highly off-axis images",2012,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"173","176",,144,"10.1145/2168556.2168585","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862703942&doi=10.1145%2f2168556.2168585&partnerID=40&md5=d2f6bfb56c9234cdcabac3b04a55c780","University of Cambridge, United Kingdom","Świrski, L., University of Cambridge, United Kingdom; Bulling, A., University of Cambridge, United Kingdom; Dodgson, N., University of Cambridge, United Kingdom","Robust, accurate, real-time pupil tracking is a key component for online gaze estimation. On head-mounted eye trackers, existing algorithms that rely on circular pupils or contiguous pupil regions fail to detect or accurately track the pupil. This is because the pupil ellipse is often highly eccentric and partially occluded by eyelashes. We present a novel, real-time dark-pupil tracking algorithm that is robust under such conditions. Our approach uses a Haar-like feature detector to roughly estimate the pupil location, performs a k-means segmentation on the surrounding region to refine the pupil centre, and fits an ellipse to the pupil using a novel image-aware Random Sample Concensus (RANSAC) ellipse fitting. We compare our approach against existing real-time pupil tracking implementations, using a set of manually labelled infra-red dark-pupil eye images. We show that our technique has a higher pupil detection rate and greater pupil tracking accuracy. © 2012 ACM.",,"Concensus; Ellipse fitting; Eye images; Eye trackers; Gaze estimation; Haar-like features; K-means; Off-axis; Pupil detection; Random sample; Surrounding regions; Tracking accuracy; Tracking algorithm; Algorithms; Feature extraction; Image segmentation",Conference Paper,"Final","",Scopus,2-s2.0-84862703942
"Zhang Y., Bulling A., Gellersen H.","52664598600;6505807414;6701531333;","Towards pervasive eye tracking using low-level image features",2012,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"261","264",,8,"10.1145/2168556.2168611","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862703581&doi=10.1145%2f2168556.2168611&partnerID=40&md5=b2215f20de2e53dc54105fbafbe721bf","Lancaster University, United Kingdom; University of Cambridge, United Kingdom","Zhang, Y., Lancaster University, United Kingdom; Bulling, A., Lancaster University, United Kingdom, University of Cambridge, United Kingdom; Gellersen, H., Lancaster University, United Kingdom","We contribute a novel gaze estimation technique, which is adaptable for person-independent applications. In a study with 17 participants, using a standard webcam, we recorded the subjects' left eye images for different gaze locations. From these images, we extracted five types of basic visual features. We then sub-selected a set of features with minimum Redundancy Maximum Relevance (mRMR) for the input of a 2-layer regression neural network for estimating the subjects' gaze. We investigated the effect of different visual features on the accuracy of gaze estimation. Using machine learning techniques, by combing different features, we achieved average gaze estimation error of 3.44° horizontally and 1.37° vertically for person-dependent. © 2012 ACM.","appearance-based; gaze estimation; low-level image features; machine learning; pervasive eye tracking","Appearance based; Eye images; Eye-tracking; Gaze estimation; Low-level image features; Machine learning techniques; Person-dependent; Person-independent; Regression neural networks; Visual feature; Estimation; Learning systems; Neural networks; Content based retrieval",Conference Paper,"Final","",Scopus,2-s2.0-84862703581
"Tsukada A., Kanade T.","43262021200;35248192100;","Automatic acquisition of a 3D eye model for a wearable first-person vision device",2012,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"213","216",,14,"10.1145/2168556.2168597","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862702069&doi=10.1145%2f2168556.2168597&partnerID=40&md5=ec47363479bd95b0c3f3459901c1db25","Robotics Institute, Carnegie Mellon University, United States","Tsukada, A., Robotics Institute, Carnegie Mellon University, United States; Kanade, T., Robotics Institute, Carnegie Mellon University, United States","A wearable gaze tracking device can work with users in daily-life. For long time of use, a non-active method that does not employ an infrared illumination system is desirable from safety standpoint. It is well known that the eye model constraints substantially improve the accuracy and robustness of gaze estimation. However, the eye model needs to be calibrated for each person and each device. We propose a method to automatically build the eye model for a wearable gaze tracking device. The key idea is that the eye model, which includes the eye structure and eye-camera relationship, impose constraints on image analysis even when it is incomplete, so we adopt an iterative eye model building process with gradually increasing eye model constraints. Performance of the proposed method is evaluated in various situations, including different eye colors of users and camera configurations. We have confirmed that the gaze tracking system using our eye model works well under general situations: indoor, outdoor and driving scene. © 2012 ACM.","3D eye model; eye tracking; first-person vision; gaze tracking; wearable device","Automatic acquisition; Building process; Camera configuration; Eye color; Eye model; Eye-tracking; Gaze estimation; Gaze tracking; Gaze tracking system; General situation; Infrared illumination; Time of use; Wearable devices; Cameras; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-84862702069
"Nagamatsu T., Yamamoto M., Sugano R., Kamahara J.","23398000100;56328923300;36053963000;14632144300;","Mathematical model for wide range gaze tracking system based on corneal reflections and pupil using stereo cameras",2012,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"257","260",,6,"10.1145/2168556.2168610","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862694996&doi=10.1145%2f2168556.2168610&partnerID=40&md5=881a7f1dfc57f3a725a838fded5766c6","Kobe University, Japan; Kwansei Gakuin University, Japan","Nagamatsu, T., Kobe University, Japan; Yamamoto, M., Kwansei Gakuin University, Japan; Sugano, R., Kobe University, Japan; Kamahara, J., Kobe University, Japan","In this paper, we propose a mathematical model for a wide range gaze tracking system based on corneal reflections and pupil using calibrated stereo cameras and light sources. We demonstrate a general calculation method for estimating the optical axis of the eye for a combination of non-coaxial and coaxial configurations of many cameras and light sources. Gaze estimation is possible only when light is reflected from the spherical surface of the cornea. Moreover, we provide a method for calculating the eye rotation range where gaze tracking can be achieved, which is useful for positioning cameras and light sources in real world applications. © 2012 ACM.","eye model; gaze estimation; gaze interface","Calculation methods; Coaxial configuration; Corneal reflection; Eye model; Gaze estimation; Gaze tracking; Gaze tracking system; Optical axis; Real-world application; Spherical surface; Stereo cameras; Light sources; Mathematical models; Tracking (position); Cameras",Conference Paper,"Final","",Scopus,2-s2.0-84862694996
"Model D., Eizenman M.","11541277100;6701402159;","A general framework for extension of a tracking range of user-calibration-free remote eye-gaze tracking systems",2012,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"253","256",,5,"10.1145/2168556.2168609","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862694973&doi=10.1145%2f2168556.2168609&partnerID=40&md5=20b0fe0699f58ab3b3bf6a1538059d36","University of Toronto, Canada","Model, D., University of Toronto, Canada; Eizenman, M., University of Toronto, Canada","Stereo-camera Remote Eye-Gaze Tracking (REGT) systems can provide calibration-free estimation of gaze. However, such systems have a limited tracking range due to the requirement for the eye to be tracked in both cameras. This paper presents a general framework for extension of a tracking range of stereo-camera user-calibration-free REGT systems. The proposed method consists of two distinct phases. In the brief initial phase, estimates of eye-features [the center of the pupil and corneal reflections] in pairs of stereo-images are used to estimate automatically a set of subject-specific eye parameters. In the second phase, these subject-specific eye parameters are used with estimates of eye-features in images from any one of the systems' cameras to compute the Point-of-Gaze (PoG). Experiments were conducted with a system that includes two cameras in a horizontal plane. The experimental results demonstrate that the tracking range for horizontal gaze directions can be extended by more than 50%: from ±23.2° when the two cameras are used as a stereo pair to ±35.5° when the two cameras are used independently to estimate the PoG. By adding more cameras to the system, the proposed framework allows further extension of the tracking range in both horizontal and vertical direction, while preserving a user-calibration-free status of a REGT system. © 2012 ACM.","calibration-free; distributed eye-gaze tracker; extended range; eye tracking; remote gaze estimation","calibration-free; Extended range; Eye-gaze; Eye-tracking; Remote gaze estimation; Calibration; Estimation; Parameter estimation; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-84862694973
"Sesma L., Villanueva A., Cabeza R.","55320498400;7101612861;36763933900;","Evaluation of pupil center-eye corner vector for gaze estimation using a web cam",2012,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"217","220",,46,"10.1145/2168556.2168598","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862693365&doi=10.1145%2f2168556.2168598&partnerID=40&md5=7da88eeed9f410bca60c6342a0424a8f","Public University of Navarra, Spain","Sesma, L., Public University of Navarra, Spain; Villanueva, A., Public University of Navarra, Spain; Cabeza, R., Public University of Navarra, Spain","Low cost eye tracking is an actual challenging research topic for the eye tracking community. Gaze tracking based on a web cam and without infrared light is a searched goal to broaden the applications of eye tracking systems. Web cam based eye tracking results in new challenges to solve such as a wider field of view and a lower image quality. In addition, no infrared light implies that glints cannot be used anymore as a tracking feature. In this paper, a thorough study has been carried out to evaluate pupil (iris) center-eye corner (PC-EC) vector as feature for gaze estimation based on interpolation methods in low cost eye tracking, as it is considered to be partially equivalent to the pupil center-corneal reflection (PC-CR) vector. The analysis is carried out both based on simulated and real data. The experiments show that eye corner positions in the image move slightly when the user is looking at different points of the screen, even with a static head position. This lowers the possible accuracy of the gaze estimation, significantly reducing the accuracy of the system under standard working conditions to 2 - 3 degrees. © 2012 ACM.","gaze estimation; interpolation methods; iris detection; low cost eye tracking","Eye corners; Eye tracking systems; Eye-tracking; Field of views; Gaze estimation; Gaze tracking; Head position; Infrared light; Interpolation method; Iris detection; Low costs; Research topics; Tracking feature; WebCams; Working conditions; Cams; Image quality; Interpolation; Research; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84862693365
"Turner J., Bulling A., Gellersen H.","52664719500;6505807414;6701531333;","Extending the visual field of a head-mounted eye tracker for pervasive eye-based interaction",2012,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"269","272",,17,"10.1145/2168556.2168613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862685272&doi=10.1145%2f2168556.2168613&partnerID=40&md5=a550cf969d9be905a342e47cd228470d","Lancaster University, United Kingdom; University of Cambridge, United Kingdom","Turner, J., Lancaster University, United Kingdom; Bulling, A., University of Cambridge, United Kingdom; Gellersen, H., Lancaster University, United Kingdom","Pervasive eye-based interaction refers to the vision of eye-based interaction becoming ubiquitously usable in everyday life, e. g. across multiple displays in the environment. While current head-mounted eye trackers work well for interaction with displays at similar distances, the scene camera often fails to cover both remote and close proximity displays, e. g. a public display on a wall and a handheld portable device. In this paper we describe an approach that allows for robust detection and gaze mapping across multiple such displays. Our approach uses an additional scene camera to extend the viewing and gaze mapping area of the eye tracker and automatically switches between both cameras depending on the display in view. Results from a pilot study show that our system achieves a similar gaze estimation accuracy to a single-camera system while at the same time increasing usability. © 2012 ACM.","eye-based interaction; mobile eye tracking; multiple cameras; pervasive eye-based interaction; visual field","eye-based interaction; Eye-tracking; Multiple cameras; pervasive eye-based interaction; Visual fields; Vision; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-84862685272
"Utsumi A., Okamoto K., Hagita N., Takahashi K.","7005241177;55258383400;6701571681;7409417392;","Gaze tracking in wide area using multiple camera observations",2012,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"273","276",,8,"10.1145/2168556.2168614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862679332&doi=10.1145%2f2168556.2168614&partnerID=40&md5=d8e8c76f65eb7e736711a0436fe3d21c","ATR Intelligent Robotics and Communication Laboratories, Japan; Doshisha University, Japan","Utsumi, A., ATR Intelligent Robotics and Communication Laboratories, Japan; Okamoto, K., ATR Intelligent Robotics and Communication Laboratories, Japan, Doshisha University, Japan; Hagita, N., ATR Intelligent Robotics and Communication Laboratories, Japan; Takahashi, K., Doshisha University, Japan","We propose a multi-camera-based gaze tracking system that provides a wide observation area. In our system, multiple camera observations are used to expand the detection area by employing mosaic observations. Each facial feature and eye region image can be observed by different cameras, and in contrast to stereo-based systems, no shared observations are required. This feature relaxes the geometrical constraints in terms of head orientation and camera viewpoints and realizes wide availability of gaze tracking with a small number of cameras. In experiments, we confirmed that our implemented system can track head rotation of 120° with two cameras. The gaze estimation accuracy is 5.4° horizontally and 9.7° vertically. © 2012 ACM.","gaze tracking; multiple camera system; wide area tracking","Facial feature; Gaze estimation; Gaze tracking; Gaze tracking system; Geometrical constraints; Head rotation; Multiple camera system; Multiple cameras; Observation area; Stereo-based; Wide area; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-84862679332
"Model D., Eizenman M.","11541277100;6701402159;","A probabilistic approach for the estimation of angle kappa in infants",2012,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"53","58",,3,"10.1145/2168556.2168564","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862668701&doi=10.1145%2f2168556.2168564&partnerID=40&md5=7f6251cef4c7d6f5d5d43142cbd1d06e","University of Toronto, Canada","Model, D., University of Toronto, Canada; Eizenman, M., University of Toronto, Canada","This paper presents a probabilistic approach for the estimation of the angle between the optical and visual axes (angle kappa) in infants. The approach assumes that when patterned calibration targets are presented on a uniform background, subjects are more likely to look at the calibration targets than at the uniform background, but it does not require accurate and continuous fixation on presented targets. Simulations results show that when subjects attend to roughly half of the presented targets, angle kappa can be estimated accurately with low probability (< 1%) of false detection. In experiments with five babies who attended to the calibration target for only 47% of the time (range from 26% to 70%), the average difference between repeated measurements of angle kappa was 0.04 ± 0.31°. © 2012 ACM.","calibration-free; eye tracking; infants; remote gaze estimation; uncooperative subjects","calibration-free; Eye-tracking; infants; Remote gaze estimation; uncooperative subjects; Calibration; Vision",Conference Paper,"Final","",Scopus,2-s2.0-84862668701
"Pauwels K., Van Hulle M.M.","8724411900;7006258484;","Head-centric disparity and epipolar geometry estimation from a population of binocular energy neurons",2012,"International Journal of Neural Systems","22","3","1250007","","",,4,"10.1142/S0129065712500074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862079980&doi=10.1142%2fS0129065712500074&partnerID=40&md5=f5bc201292ebe0d616c5e6153eafc7c6","Computer Architecture and Technology Department, University of Granada, Calle Periodista Daniel Saucedo, s/n, 18071 Granada, Spain; Laboratorium voor Neuro-en Psychofysiologie, K.U. Leuven, Campus Gasthuisberg, OandN II Herestraat 49Bus 1021, 3000 Leuven, Belgium","Pauwels, K., Computer Architecture and Technology Department, University of Granada, Calle Periodista Daniel Saucedo, s/n, 18071 Granada, Spain; Van Hulle, M.M., Laboratorium voor Neuro-en Psychofysiologie, K.U. Leuven, Campus Gasthuisberg, OandN II Herestraat 49Bus 1021, 3000 Leuven, Belgium","We present a hybrid neural network architecture that supports the estimation of binocular disparity in a cyclopean, head-centric coordinate system without explicitly establishing retinal correspondences. Instead the responses of binocular energy neurons are gain-modulated by oculomotor signals. The network can handle the full six degrees of freedom of binocular gaze and operates directly on image pairs of possibly varying contrast. Furthermore, we show that in the absence of an oculomotor signal the same architecture is capable of estimating the epipolar geometry directly from the population response. The increased complexity of the scenarios considered in this work provides an important step towards the application of computational models centered on gain modulation mechanisms in real-world robotic applications. The proposed network is shown to outperform a standard computer vision technique on a disparity estimation task involving real-world stereo images. © 2012 World Scientific Publishing Company.","Binocular energy neurons; Gaze estimation; Head-centric disparity; Multi-layer perception","Binocular disparity; Co-ordinate system; Computational model; Computer vision techniques; Disparity estimations; Epipolar geometry; Epipolar geometry estimation; Gain modulation; Gaze estimation; Head-centric disparity; Hybrid neural networks; Image pairs; Multi-layer perception; Retinal correspondence; Robotic applications; Six degrees of freedom; Stereo-image; Computer vision; Estimation; Network architecture; Binoculars; biomimetic material; algorithm; artificial neural network; binocular vision; biological model; computer simulation; depth perception; eye fixation; head; hemispheric dominance; human; nerve cell; orientation; pathology; photostimulation; physiology; visual system; article; binocular vision; depth perception; nerve cell; pathology; visual system; Algorithms; Biomimetic Materials; Computer Simulation; Fixation, Ocular; Functional Laterality; Head; Humans; Models, Neurological; Neural Networks (Computer); Neurons; Orientation; Photic Stimulation; Vision Disparity; Vision, Binocular; Visual Pathways; Algorithms; Biomimetic Materials; Computer Simulation; Fixation, Ocular; Functional Laterality; Head; Humans; Models, Neurological; Neural Networks (Computer); Neurons; Orientation; Photic Stimulation; Vision Disparity; Vision, Binocular; Visual Pathways",Article,"Final","",Scopus,2-s2.0-84862079980
"Valenti R., Sebe N., Gevers T.","57192175392;57204924633;7003472472;","Combining head pose and eye location information for gaze estimation",2012,"IEEE Transactions on Image Processing","21","2","5959981","802","815",,212,"10.1109/TIP.2011.2162740","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856293813&doi=10.1109%2fTIP.2011.2162740&partnerID=40&md5=4df98dd7c4700a69c16c308c5cf4fa57","Intelligent Systems Lab, Amsterdam, University of Amsterdam, 1098 XH Amsterdam, Netherlands; Department of Information Engineering and Computer Science, University of Trento, 38123 Trento, Italy; Computer Vision Center, Universitat Autònoma de Barcelona, 08193 Barcelona, Spain","Valenti, R., Intelligent Systems Lab, Amsterdam, University of Amsterdam, 1098 XH Amsterdam, Netherlands; Sebe, N., Department of Information Engineering and Computer Science, University of Trento, 38123 Trento, Italy; Gevers, T., Intelligent Systems Lab, Amsterdam, University of Amsterdam, 1098 XH Amsterdam, Netherlands, Computer Vision Center, Universitat Autònoma de Barcelona, 08193 Barcelona, Spain","Head pose and eye location for gaze estimation have been separately studied in numerous works in the literature. Previous research shows that satisfactory accuracy in head pose and eye location estimation can be achieved in constrained settings. However, in the presence of nonfrontal faces, eye locators are not adequate to accurately locate the center of the eyes. On the other hand, head pose estimation techniques are able to deal with these conditions; hence, they may be suited to enhance the accuracy of eye localization. Therefore, in this paper, a hybrid scheme is proposed to combine head pose and eye location information to obtain enhanced gaze estimation. To this end, the transformation matrix obtained from the head pose is used to normalize the eye regions, and in turn, the transformation matrix generated by the found eye location is used to correct the pose estimation procedure. The scheme is designed to enhance the accuracy of eye location estimations, particularly in low-resolution videos, to extend the operative range of the eye locators, and to improve the accuracy of the head pose tracker. These enhanced estimations are then combined to obtain a novel visual gaze estimation system, which uses both eye location and head information to refine the gaze estimates. From the experimental results, it can be derived that the proposed unified scheme improves the accuracy of eye estimations by 16% to 23%. Furthermore, it considerably extends its operating range by more than 15° by overcoming the problems introduced by extreme head poses. Moreover, the accuracy of the head pose tracker is improved by 12% to 24%. Finally, the experimentation on the proposed combined gaze estimation system shows that it is accurate (with a mean error between 2° and 5°) and that it can be used in cases where classic approaches would fail without imposing restraints on the position of the head. © 2011 IEEE.","Eye center location; gaze estimation; head pose estimation","Eye localization; Eye location; Gaze estimation; Head pose; head pose estimation; Hybrid scheme; Mean errors; Operating ranges; Pose estimation; Transformation matrices; Estimation; Face recognition; Gesture recognition; Linear transformations; Motion estimation; article; body posture; eye; eye movement; head; head movement; histology; human; image processing; methodology; physiology; videorecording; Eye; Eye Movements; Head; Head Movements; Humans; Image Processing, Computer-Assisted; Posture; Video Recording",Article,"Final","",Scopus,2-s2.0-84856293813
"Špakov O., Majaranta P.","12241013900;6508385893;","Enhanced gaze interaction using simple head gestures",2012,"UbiComp'12 - Proceedings of the 2012 ACM Conference on Ubiquitous Computing",,,,"705","710",,37,"10.1145/2370216.2370369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879496342&doi=10.1145%2f2370216.2370369&partnerID=40&md5=2e2af5666b8de44b7275cb9ca74b7ad4","University of Tampere, Kanslerinrinne 1, 33014, Tampere, Finland","Špakov, O., University of Tampere, Kanslerinrinne 1, 33014, Tampere, Finland; Majaranta, P., University of Tampere, Kanslerinrinne 1, 33014, Tampere, Finland","We propose a combination of gaze pointing and head gestures for enhanced hands-free interaction. Instead of the traditional dwell-time selection method, we experimented with five simple head gestures: nodding, turning left/right, and tilting left/right. The gestures were detected from the eye-tracking data by a range-based algorithm, which was found accurate enough in recognizing nodding and leftdirected gestures. The gaze estimation accuracy did not noticeably suffer from the quick head motions. Participants pointed to nodding as the best gesture for occasional selections tasks and rated the other gestures as promising methods for navigation (turning) and functional mode switching (tilting). In general, dwell time works well for repeated tasks such as eye typing. However, considering multimodal games or transient interactions in pervasive and mobile environments, we believe a combination of gaze and head interaction could potentially provide a natural and more accurate interaction method. Copyright 2012 ACM.","Dwell time; Eye tracking; Head gestures; Selection","Eye tracking; Ubiquitous computing; Dwell time; Hands-free interactions; Head gestures; Interaction methods; Mobile environments; Selection; Selection methods; Transient interactions; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84879496342
"Vaitukaitis V., Bulling A.","55776270500;6505807414;","Eye gesture recognition on portable devices",2012,"UbiComp'12 - Proceedings of the 2012 ACM Conference on Ubiquitous Computing",,,,"711","714",,35,"10.1145/2370216.2370370","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879493144&doi=10.1145%2f2370216.2370370&partnerID=40&md5=782e03d0c9f1109cf47b7b0fb5c623a1","University of Cambridge, United Kingdom; Lancaster University, United Kingdom","Vaitukaitis, V., University of Cambridge, United Kingdom; Bulling, A., University of Cambridge, United Kingdom, Lancaster University, United Kingdom","Hand-held portable devices have received only little attention as a platform in the eye tracking community so far. This is mainly due to their - until recently - limited sensing capabilities and processing power. In this work-in-progress paper we present the first prototype eye gesture recognition system for portable devices that does not require any additional equipment. The system combines techniques from image processing, computer vision and pattern recognition to detect eye gestures in the video recorded using the builtin front-facing camera. In a five-participant user study we show that our prototype can recognise four different continuous eye gestures in near real-time with an average accuracy of 60% on an Android-based smartphone (17.6% false positives) and 67.3% on a laptop (5.9% false positives). This initial result is promising and underlines the potential of eye tracking and eye-based interaction on portable devices. Copyright 2012 ACM.","Eye gesture; Eye tracking; Gaze estimation; Laptop; Mobile phone","Cellular telephones; Eye tracking; Image processing; Laptop computers; Pattern recognition systems; Portable equipment; Ubiquitous computing; Additional equipment; Eye-gestures; Gaze estimation; Laptop; Limited sensing capabilities; Portable device; Processing power; Work in progress; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84879493144
"Nagamatsu T., Iwamoto Y., Sugano R., Kamahara J., Tanaka N., Yamamoto M.","23398000100;36052629200;36053963000;14632144300;35318753700;56328923300;","Gaze estimation method involving corneal reflection-based modeling of the eye as a general surface of revolution about the optical axis of the eye",2012,"IEICE Transactions on Information and Systems","E95-D","6",,"1656","1667",,7,"10.1587/transinf.E95.D.1656","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861893615&doi=10.1587%2ftransinf.E95.D.1656&partnerID=40&md5=b9b8d41a0c02ec8d17ddfd7732cab796","Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan; Faculty of Science and Technology, Kwansei Gakuin University, Sanda-shi, 669-1337, Japan; Mitsubishi Electric Corporation, Japan","Nagamatsu, T., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan; Iwamoto, Y., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan, Mitsubishi Electric Corporation, Japan; Sugano, R., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan; Kamahara, J., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan; Tanaka, N., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan; Yamamoto, M., Faculty of Science and Technology, Kwansei Gakuin University, Sanda-shi, 669-1337, Japan","We have proposed a novel geometric model of the eye in order to avoid the problems faced while using the conventional spherical model of the cornea for three dimensional (3D) model-based gaze estimation. The proposed model models the eye, including the boundary region of the cornea, as a general surface of revolution about the optical axis of the eye. Furthermore, a method for calculating the point of gaze (POG) on the basis of our model has been proposed. A prototype system for estimating the POG was developed using this method. The average root mean square errors (RMSEs) of the proposed method were experimentally found to be smaller than those of the gaze estimation method that is based on a spherical model of the cornea. © 2012 The Institute of Electronics, Information and Communication Engineers.","Eye model; Eye movement; Gaze estimation; One-point calibration","Estimation; Mean square error; Corneal reflection; Eye model; Gaze estimation; Geometric modeling; Prototype system; Root mean square errors; Surface of revolution; Three dimensional (3-D) modeling; Eye movements",Article,"Final","",Scopus,2-s2.0-84861893615
"Sommerlade E., Benfold B., Reid I.","24829792700;35223026000;55643335100;","Gaze directed camera control for face image acquisition",2011,"Proceedings - IEEE International Conference on Robotics and Automation",,,"5979585","4227","4233",,3,"10.1109/ICRA.2011.5979585","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871691046&doi=10.1109%2fICRA.2011.5979585&partnerID=40&md5=4fb42a43b5e9d67ab7e926d00cf25d69","Active Vision Lab., Department of Engineering Science, University of Oxford, Parks Road, Oxford, United Kingdom","Sommerlade, E., Active Vision Lab., Department of Engineering Science, University of Oxford, Parks Road, Oxford, United Kingdom; Benfold, B., Active Vision Lab., Department of Engineering Science, University of Oxford, Parks Road, Oxford, United Kingdom; Reid, I., Active Vision Lab., Department of Engineering Science, University of Oxford, Parks Road, Oxford, United Kingdom","Face recognition in surveillance situations usually requires high resolution face images to be captured from remote active cameras. Since the recognition accuracy is typically a function of the face direction - with frontal faces more likely to lead to reliable recognition - we propose a system which optimises the capturing of such images by using coarse gaze estimates from a static camera. By considering the potential information gain from observing each target, our system automatically sets the pan, tilt and zoom values (i.e. the field of view) of multiple cameras observing different tracked targets in order to maximise the likelihood of correct identification. The expected gain in information is influenced by the controllable field of view, and by the false positive and negative rates of the identification process, which are in turn a function of the gaze angle. We validate the approach using a combination of simulated situations and real tracking output to demonstrate superior performance over alternative approaches, notably using no gaze information, or using gaze inferred from direction of travel (i.e. assuming each person is always looking directly ahead). We also show results from a live implementation with a static camera and two pan-tilt-zoom devices, involving real-time tracking, processing and control. © 2011 IEEE.",,"Active camera; Alternative approach; Camera controls; Face direction; Face images; False positive; Field of views; Frontal faces; High resolution; Identification process; Information gain; Multiple cameras; Negative rates; Real time tracking; Recognition accuracy; Reliable recognition; Static cameras; Face recognition; Identification (control systems); Information use; Video cameras; Robotics",Conference Paper,"Final","",Scopus,2-s2.0-84871691046
"Shafi M., Iqbal F., Ali I.","55029162200;57192659782;57202491939;","Face pose estimation using distance transform and normalized cross-correlation",2011,"2011 IEEE International Conference on Signal and Image Processing Applications, ICSIPA 2011",,,"6144086","186","191",,7,"10.1109/ICSIPA.2011.6144086","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857466036&doi=10.1109%2fICSIPA.2011.6144086&partnerID=40&md5=a1965e264acb93022c4b57369689299d","Department of Computer Software Engineering, U.E.T Peshawar Mardan Campus, Charsaddah Road, Mardan, Pakistan; Department of Telecommunication Engineering, U.E.T Peshawar Mardan Campus, Charsaddah Road, Mardan, Pakistan","Shafi, M., Department of Computer Software Engineering, U.E.T Peshawar Mardan Campus, Charsaddah Road, Mardan, Pakistan; Iqbal, F., Department of Telecommunication Engineering, U.E.T Peshawar Mardan Campus, Charsaddah Road, Mardan, Pakistan; Ali, I., Department of Telecommunication Engineering, U.E.T Peshawar Mardan Campus, Charsaddah Road, Mardan, Pakistan","Face pose estimation plays a vital role in human-computer interaction, automatic human behavior analysis, pose-independent face recognition, gaze estimation, virtual reality applications etc. A novel face pose estimation method using distance transform and normalized cross-correlation, is presented in this paper. The use of distance transform has two main advantages: first, unlike intensity image, distance transform is relatively invariant to intensity and illumination of image. Secondly, unlike edge-map the distance transform has a smoother distribution. The distance transform property of being invariant to intensity makes the proposed method suitable for different skin colors. Since distance transform is relatively invariant to illumination, the proposed method is also suitable for different illumination conditions. Further advantages are that the proposed method is relatively invariant to facial hairs and whether the subject is wearing glasses. The proposed method has been tested using the CAS-PEAL pose database with very good results. © 2011 IEEE.","Distance Transform; Face Pose Estimation; Normalized Cross-Correlation; Template Matching","Distance transforms; Face pose estimation; Gaze estimation; Human behaviors; Illumination conditions; Intensity images; Normalized cross-correlation; Skin color; Template matching; Virtual reality; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-84857466036
"Benfold B., Reid I.","35223026000;55643335100;","Unsupervised learning of a scene-specific coarse gaze estimator",2011,"Proceedings of the IEEE International Conference on Computer Vision",,,"6126516","2344","2351",,50,"10.1109/ICCV.2011.6126516","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856670374&doi=10.1109%2fICCV.2011.6126516&partnerID=40&md5=9111123e13060b30019c330af29f94d3","Department of Engineering Science, University of Oxford, Oxford, United Kingdom","Benfold, B., Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Reid, I., Department of Engineering Science, University of Oxford, Oxford, United Kingdom","We present a method to estimate the coarse gaze directions of people from surveillance data. Unlike previous work we aim to do this without recourse to a large hand-labelled corpus of training data. In contrast we propose a method for learning a classifier without any hand labelled data using only the output from an automatic tracking system. A Conditional Random Field is used to model the interactions between the head motion, walking direction, and appearance to recover the gaze directions and simultaneously train randomised decision tree classifiers. Experiments demonstrate performance exceeding that of conventionally trained classifiers on two large surveillance datasets. © 2011 IEEE.",,"Automatic tracking system; Conditional random field; Data sets; Decision tree classifiers; Gaze direction; Hand-labelled corpora; Head motion; Surveillance data; Training data; Walking direction; Decision trees; Image processing",Conference Paper,"Final","",Scopus,2-s2.0-84856670374
"Asteriadis S., Karpouzis K., Kollias S.","55936774500;6602860576;57193712526;","Robust validation of Visual Focus of Attention using adaptive fusion of head and eye gaze patterns",2011,"Proceedings of the IEEE International Conference on Computer Vision",,,"6130271","414","421",,7,"10.1109/ICCVW.2011.6130271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856646356&doi=10.1109%2fICCVW.2011.6130271&partnerID=40&md5=5ae206c569b173e80f94685b5a856775","National Technical University of Athens, Greece; Electrical and Computer Engineering Department, Greece","Asteriadis, S., National Technical University of Athens, Greece; Karpouzis, K., National Technical University of Athens, Greece; Kollias, S., Electrical and Computer Engineering Department, Greece","We propose a framework for inferring the focus of attention of a person, utilizing information coming both from head rotation and eye gaze estimation. To this aim, we use fuzzy logic to estimate confidence on the gaze of a person towards a specific point, and results are compared to human annotation. For head pose we propose Bayesian modality fusion of both local and holistic information, while for eye gaze we propose a methodology that calculates eye gaze directionality, removing the influence of head rotation, using a simple camera. For local information, feature positions are used, while holistic information makes use of face region. Holistic information uses Convolutional Neural Networks which have been shown to be immune to small translations and distortions of test data. This is vital for an application in an unpretending environment, where background noise should be expected. The ability of the system to estimate focus of attention towards specific areas, for unknown users, is grounded at the end of the paper. © 2011 IEEE.",,"Adaptive fusion; Background noise; Convolutional neural network; Eye-gaze; Face regions; Focus of Attention; Head pose; Head rotation; Human annotations; Local information; Modality Fusion; Specific areas; Test data; Fuzzy logic; Image processing; Neural networks; Rotation; Information use",Conference Paper,"Final","",Scopus,2-s2.0-84856646356
"Lu F., Sugano Y., Okabe T., Sato Y.","54956194300;7005470045;7201390055;35230954300;","Inferring human gaze from appearance via adaptive linear regression",2011,"Proceedings of the IEEE International Conference on Computer Vision",,,"6126237","153","160",,94,"10.1109/ICCV.2011.6126237","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856635786&doi=10.1109%2fICCV.2011.6126237&partnerID=40&md5=12d44499123b333338c4ee2962d307a6","Institute of Industrial Science, University of Tokyo, Japan","Lu, F., Institute of Industrial Science, University of Tokyo, Japan; Sugano, Y., Institute of Industrial Science, University of Tokyo, Japan; Okabe, T., Institute of Industrial Science, University of Tokyo, Japan; Sato, Y., Institute of Industrial Science, University of Tokyo, Japan","The problem of estimating human gaze from eye appearance is regarded as mapping high-dimensional features to low-dimensional target space. Conventional methods require densely obtained training samples on the eye appearance manifold, which results in a tedious calibration stage. In this paper, we introduce an adaptive linear regression (ALR) method for accurate mapping via sparsely collected training samples. The key idea is to adaptively find the subset of training samples where the test sample is most linearly representable. We solve the problem via l 1-optimization and thoroughly study the key issues to seek for the best solution for regression. The proposed gaze estimation approach based on ALR is naturally sparse and low-dimensional, giving the ability to infer human gaze from variant resolution eye images using much fewer training samples than existing methods. Especially, the optimization procedure in ALR is extended to solve the subpixel alignment problem simultaneously for low resolution test eye images. Performance of the proposed method is evaluated by extensive experiments against various factors such as number of training samples, feature dimensionality and eye image resolution to verify its effectiveness. © 2011 IEEE.",,"Accurate mapping; Conventional methods; Eye images; Gaze estimation; High-dimensional; Low resolution; Optimization procedures; Sub pixels; Target space; Test samples; Training sample; Image resolution; Optimization; Sampling; Problem solving",Conference Paper,"Final","",Scopus,2-s2.0-84856635786
"Tsukada A., Shino M., Devyver M., Kanade T.","43262021200;7003723170;54956661800;35248192100;","Illumination-free gaze estimation method for first-person vision wearable device",2011,"Proceedings of the IEEE International Conference on Computer Vision",,,"6130505","2084","2091",,39,"10.1109/ICCVW.2011.6130505","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856631222&doi=10.1109%2fICCVW.2011.6130505&partnerID=40&md5=6ba674a934329ce0def946d8c1a2d769","Carnegie Mellon University, United States; University of Tokyo, Japan","Tsukada, A., Carnegie Mellon University, United States; Shino, M., Carnegie Mellon University, United States, University of Tokyo, Japan; Devyver, M., Carnegie Mellon University, United States; Kanade, T., Carnegie Mellon University, United States","Gaze estimation is a key technology to understand a person's interests and intents, and it is becoming more popular in daily situations such as driving scenarios. Wearable gaze estimation devices are use for long periods of time, therefore non-active sources are not desirable from a safety point of view. Gaze estimation that does not rely on active source, is performed by locating iris position. To estimate the iris position accurately, most studies use ellipse fitting in which the ellipse is defined by 5 parameters(position (x,y), rotation angle, semi-major axis and semi-minor axis). We claim that, for iris position estimation, 5 parameters are redundant because they might be influenced by non-iris edges. Therefore, we propose to use 2 parameters(position) introducing a 3D eye model(the transformation between eye and camera coordinate and eyeball/iris size). Given 3D eye model, projected ellipse that represents iris shape can be specified only by position under weak-perspective approximation. We quantitatively evaluate our method on both iris position and gaze estimation. Our results show that our method outperforms other state-of-the-art's iris estimation and is competitive to commercial product that use infrared ray with respect to both accuracy and robustness. © 2011 IEEE.",,"Commercial products; Ellipse fitting; Eye model; Gaze estimation; Infrared rays; Key technologies; Position estimation; Rotation angles; Semimajor axis; Wearable devices; Estimation; Image processing; Infrared radiation; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-84856631222
"Fu B., Yang R.","57214148625;7403924024;","Display control based on eye gaze estimation",2011,"Proceedings - 4th International Congress on Image and Signal Processing, CISP 2011","1",,"6099973","399","403",,16,"10.1109/CISP.2011.6099973","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855568032&doi=10.1109%2fCISP.2011.6099973&partnerID=40&md5=3223098f2aefd7ea962272f092ecd391","Computer Science Department, University of Kentucky, Lexington, KY, United States","Fu, B., Computer Science Department, University of Kentucky, Lexington, KY, United States; Yang, R., Computer Science Department, University of Kentucky, Lexington, KY, United States","Detecting and tracking eye gaze is an active research area and significant progress has been made in this area in the past decades. However, challenge remains due to the differences between individual's eyes, variability in light condition, scale and occlusion. Works on eye location and eye movements have a large number of applications and is an important part of biometrics, human-computer interaction and face detection. Based on the current works and applications on eye gaze detection, the author notice, however, applying eye gaze information on human-computer interaction still lacks of enough work on it. This paper proposes a novel pipeline of employing eye gaze information for display control based on the video captured by integrated camera. The proposed pipeline shows that, despite the low quality of the video and light condition, eye gaze can still be estimated and display of the screen can be controlled accordingly. © 2011 IEEE.","display control; eye gaze; eye tracking","Eye gaze detection; Eye location; eye tracking; Eye-gaze; Integrated cameras; Light conditions; Low qualities; Research areas; Biometrics; Human computer interaction; Quality control; Signal detection; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84855568032
"Cen K., Che M.","36656864400;36098984900;","Research on eye gaze estimation technique base on 3D model",2011,"2011 International Conference on Electronics, Communications and Control, ICECC 2011 - Proceedings",,,"6067739","1623","1626",,6,"10.1109/ICECC.2011.6067739","https://www.scopus.com/inward/record.uri?eid=2-s2.0-81455155647&doi=10.1109%2fICECC.2011.6067739&partnerID=40&md5=2102bb56eef463a14be0137abad5c54b","School of Computer Science and Technology, Tianjin University, Tianjin, China","Cen, K., School of Computer Science and Technology, Tianjin University, Tianjin, China; Che, M., School of Computer Science and Technology, Tianjin University, Tianjin, China","This paper proposes a 3D model gaze estimation method based on a single-camera that can widely use as the computer input device for human computer interaction. To avoid using extra light sources and allow free head movements, our method creates an accurate model of the eye and gaze estimation algorithm, which based on the spatial relationships of the eye and the screen. Establishment of 3D gaze estimation model allows the user at any distance and under natural head movement. Compared with the present gaze estimation methods, our proposed method allows the head have a substantial movement, even the head of rotating. The proposed system is easy to implement and adaptable, and the experimental results show that the accuracy of this algorithm is less than 5 and have a good stability. © 2011 IEEE.","3D model; ditance estimation; face calibration; gaze estimation; human computer interaction","3D models; Computer input devices; Eye-gaze; Free-head; Gaze estimation; Good stability; Head movements; Single cameras; Spatial relationships; Algorithms; Estimation; Eye movements; Human computer interaction; Light sources; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-81455155647
"Zhang C., Chi J., Zhang Z., Gao X., Hu T., Wang Z.","57199501468;8702376200;36931880600;56216075400;57199359132;55880036500;","Gaze estimation in a gaze tracking system",2011,"Science China Information Sciences","54","11",,"2295","2306",,11,"10.1007/s11432-011-4243-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80255137034&doi=10.1007%2fs11432-011-4243-6&partnerID=40&md5=0ce6cfe618e6039b3c5263a5b89bbf44","School of Information Engineering, University of Science and Technology Beijing, Beijing 100083, China","Zhang, C., School of Information Engineering, University of Science and Technology Beijing, Beijing 100083, China; Chi, J., School of Information Engineering, University of Science and Technology Beijing, Beijing 100083, China; Zhang, Z., School of Information Engineering, University of Science and Technology Beijing, Beijing 100083, China; Gao, X., School of Information Engineering, University of Science and Technology Beijing, Beijing 100083, China; Hu, T., School of Information Engineering, University of Science and Technology Beijing, Beijing 100083, China; Wang, Z., School of Information Engineering, University of Science and Technology Beijing, Beijing 100083, China","This article presents a gaze estimation method (GEMHSSO) based on the pupil center cornea reflection (PCCR) technique. Existing PCCR systems suffer from several problems, including the restriction of users' head movement, and the requirement of individual calibration. This paper presents a head position compensation model using a single camera and a single light source, which realizes the analytic compensation of head motion effects on pupil-glint vectors. We then present a transformation model for individual differences to simplify the calibration process to a one-point calibration. On this basis, we establish a novel gaze estimation method that reduces the minimum hardware requirements for accurate estimation to a single camera (not calibrated) and a single light source. Without the need for a complex system, our method has the ability to estimate gaze during natural head movement, and to substantially simplify user calibration. Each step of the proposed method in this paper makes real-time implementation possible, which provides an effective solution for eye-gaze tracking in human-computer interaction systems. © 2011 Science China Press and Springer-Verlag Berlin Heidelberg.","gaze estimation; gaze tracking; head compensation; individual calibration; pupil center cornea reflection (PCCR)",,Article,"Final","",Scopus,2-s2.0-80255137034
"Hommel S., Handmann U.","53866428400;6602644325;","Realtime AAM based user attention estimation",2011,"SISY 2011 - 9th International Symposium on Intelligent Systems and Informatics, Proceedings",,,"6034322","201","206",,2,"10.1109/SISY.2011.6034322","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054829373&doi=10.1109%2fSISY.2011.6034322&partnerID=40&md5=7339bc8922544ac4102069033b0fe2fe","Computer Science Institute, Hochschule Ruhr West, Bottrop 46240, Germany","Hommel, S., Computer Science Institute, Hochschule Ruhr West, Bottrop 46240, Germany; Handmann, U., Computer Science Institute, Hochschule Ruhr West, Bottrop 46240, Germany","In this paper a method of automatic real-time capable visual user attention for a face to face human machine interaction is described. This method based on Active Appearance Models (AAMs) and Multilayer Perceptrons (MLPs) to map the Active Appearance Parameters (AAM-Parameters) onto the current head pose. Afterwards, the chronology of the head pose becomes classified to attention or inattention. This visual attention estimation will be used in service robotic by human-robotic interaction to get a feedback whether the user is interested in the current dialog and for correct interpretation of the current emotional condition. To allow a more natural dialog the head pose is also very efficient interpreted as head nodding or shaking by the use of adaptive statistical moments. Especially, the head movement of many demented people are restricted, so they often only use their eyes to look around. For that reason this paper examine a simple gaze estimation with the help of an ordinary webcam. © 2011 IEEE.","Active Appearance Model; Gaze Estimation; Head Gesture Recognition; Visual Attention","Active appearance models; Face to face; Gaze estimation; Head gesture recognition; Head movements; Head nodding; Head pose; Human machine interaction; Human-Robotic interaction; Real time; Service robotics; Statistical moments; User attention; Visual Attention; Visual Attention Estimation; Behavioral research; Eye movements; Face recognition; Gesture recognition; Information science; Intelligent systems; Pattern recognition systems; Real time systems; Robotics; Estimation",Conference Paper,"Final","",Scopus,2-s2.0-80054829373
"Guestrin E.D., Eizenman M.","14009410300;6701402159;","Remote point-of-gaze estimation with single-point personal calibration based on the pupil boundary and corneal reflections",2011,"Canadian Conference on Electrical and Computer Engineering",,,"6030604","000971","000976",,9,"10.1109/CCECE.2011.6030604","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053972944&doi=10.1109%2fCCECE.2011.6030604&partnerID=40&md5=9daa0ed1cbb89f828586dd0f04e20e61","Department of Electrical and Computer Engineering, Canada; Institute of Biomaterials and Biomedical Engineering, Canada; Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, ON, Canada","Guestrin, E.D., Department of Electrical and Computer Engineering, Canada, Institute of Biomaterials and Biomedical Engineering, Canada; Eizenman, M., Department of Electrical and Computer Engineering, Canada, Institute of Biomaterials and Biomedical Engineering, Canada, Department of Ophthalmology and Vision Sciences, University of Toronto, Toronto, ON, Canada","This paper describes a new method for remote, non-contact point-of-gaze (PoG) estimation that tolerates head movements and requires a simple personal calibration procedure in which the subject has to fixate a single calibration point. This method uses the pupil boundary and at least two corneal reflections (virtual images of light sources) that are extracted from eye images captured by at least two video cameras. Experimental results obtained with a prototype system exhibited RMS PoG estimation errors of approx. 0.4-0.6 of visual angle. Such accuracy is comparable to that of the best commercially available systems, which use multiple-point personal calibration procedures, and significantly better than that of any other systems that use a single-point personal calibration procedure and have been previously described in the literature. © 2011 IEEE.","corneal reflections; Point-of-gaze; pupil boundary; remote gaze estimation; single-point personal calibration","Corneal reflection; Point of gaze; Pupil boundary; Remote gaze estimation; single-point personal calibration; Estimation; Light sources; Video cameras; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-80053972944
"Model D., Eizenman M.","11541277100;6701402159;","User-calibration-free remote eye-gaze tracking system with extended tracking range",2011,"Canadian Conference on Electrical and Computer Engineering",,,"6030667","001268","001271",,14,"10.1109/CCECE.2011.6030667","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053971813&doi=10.1109%2fCCECE.2011.6030667&partnerID=40&md5=a59b671ab8e49d51f4c4634565bb1c57","Department of Electrical and Computer Engineering, Canada; Institute of Biomaterials and Biomedical Engineering, Canada; Department of Ophthalmology and Vision Sciences, University of Toronto, Canada","Model, D., Department of Electrical and Computer Engineering, Canada; Eizenman, M., Department of Electrical and Computer Engineering, Canada, Institute of Biomaterials and Biomedical Engineering, Canada, Department of Ophthalmology and Vision Sciences, University of Toronto, Canada","A novel general method to extend the tracking range of user-calibration- free remote eye-gaze tracking (REGT) systems that are based on the analysis of stereo-images from multiple cameras is presented. The method consists of two distinct phases. In the brief initial phase, estimates of the center of the pupil and corneal reflections in pairs of stereo-images are used to estimate automatically a set of subject-specific eye parameters. In the second phase, these subject specific eye parameters are used with estimates of the center of the pupil and corneal reflections in images from any one of the systems' cameras to compute the Point-of-Gaze (PoG). Experiments with a system that includes two cameras show that the tracking range for horizontal gaze directions can be extended from 23.2 when the two cameras are used as a stereo pair to 35.5 when the two cameras are used independently to estimate the POG. © 2011 IEEE.","Calibration-Free; Distributed Eye-gaze Tracker; Extended Range; Eye Tracking; Remote Gaze Estimation","Calibration-Free; Extended range; Eye-gaze; Eye-tracking; Remote gaze estimation; Calibration; Cameras; Estimation; Parameter estimation",Conference Paper,"Final","",Scopus,2-s2.0-80053971813
"Zhang Y., Bulling A., Gellersen H.","52664598600;6505807414;6701531333;","Discrimination of gaze directions using low-level eye image features",2011,"PETMEI'11 - Proceedings of the 1st International Workshop on Pervasive Eye Tracking and Mobile Eye-Based Interaction",,,,"9","13",,10,"10.1145/2029956.2029961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053634022&doi=10.1145%2f2029956.2029961&partnerID=40&md5=4a814956395fce550c7893e3c07cbc08","Lancaster University, United Kingdom; University of Cambridge, United Kingdom","Zhang, Y., Lancaster University, United Kingdom; Bulling, A., Lancaster University, United Kingdom, University of Cambridge, United Kingdom; Gellersen, H., Lancaster University, United Kingdom","In mobile daily life settings, video-based gaze tracking faces challenges associated with changes in lighting conditions and artefacts in the video images caused by head and body movements. These challenges call for the development of new methods that are robust to such influences. In this paper we investigate the problem of gaze estimation, more specifically how to discriminate different gaze directions from eye images. In a 17 participant user study we record eye images for 13 different gaze directions from a standard webcam. We extract a total of 50 features from these images that encode information on color, intensity and orientations. Using mRMR feature selection and a k-nearest neighbor (kNN) classifier we show that we can estimate these gaze directions with a mean recognition performance of 86%. © 2011 ACM.","appearance-based; gaze estimation; low-level image features; machine learning; wearable eye tracking","Appearance based; Eye-tracking; Gaze estimation; Low-level image features; Machine-learning; Estimation; Feature extraction; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-80053634022
"Caridakis G., Asteriadis S., Karpouzis K., Kollias S.","14041313800;55936774500;6602860576;57193712526;","Detecting human behavior emotional cues in natural interaction",2011,"17th DSP 2011 International Conference on Digital Signal Processing, Proceedings",,,"6004962","","",,2,"10.1109/ICDSP.2011.6004962","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053151369&doi=10.1109%2fICDSP.2011.6004962&partnerID=40&md5=b079b5def0cdb424b945c9c3680c476f","Intelligent Systems, Content and Interaction Lab, National Technical University of Athens, Iroon Polytexneiou 9, 15780 Zografou, Greece","Caridakis, G., Intelligent Systems, Content and Interaction Lab, National Technical University of Athens, Iroon Polytexneiou 9, 15780 Zografou, Greece; Asteriadis, S., Intelligent Systems, Content and Interaction Lab, National Technical University of Athens, Iroon Polytexneiou 9, 15780 Zografou, Greece; Karpouzis, K., Intelligent Systems, Content and Interaction Lab, National Technical University of Athens, Iroon Polytexneiou 9, 15780 Zografou, Greece; Kollias, S., Intelligent Systems, Content and Interaction Lab, National Technical University of Athens, Iroon Polytexneiou 9, 15780 Zografou, Greece","Current work focuses on the detection of human behavior emotional cues and their incorporation into affect aware Natural Interaction. Techniques for extracting emotional cues based on visual non verbal human behavior are presented. Namely, gesture qualitative expressivity features and head pose and eye gaze estimation are derived from hand and facial movement respectively. Extracted emotional cues are employed in expressive synthesis on virtual agents, based on the analysis of actions performed by human users, in a Human-Virtual Agent Interaction setting and in Assistive Technologies aiming to infer in real time the degree of attention or frustration of children with reading difficulties. © 2011 IEEE.","Affective computing; Eye Gaze; Gesture expressivity; Natural Interaction","Affective Computing; Agent interaction; Assistive technology; Eye-gaze; Facial movements; Gesture expressivity; Head pose; Human behaviors; Human users; Natural interactions; Non-verbal human; Real time; Virtual agent; Digital signal processing; Eye movements; Signal detection; Social sciences; Virtual reality; Behavioral research",Conference Paper,"Final","",Scopus,2-s2.0-80053151369
"Takatani M., Ariki Y., Takiguchi T.","51162080500;7003402234;7005058510;","Gaze estimation using regression analysis and AAMs parameters selected based on information criterion",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6468 LNCS","PART1",,"400","409",,,"10.1007/978-3-642-22822-3_40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053129898&doi=10.1007%2f978-3-642-22822-3_40&partnerID=40&md5=3b0ab1ddeb4df9bad07b098a43267ed6","Graduate School of Engineering, Kobe University, Japan; Organization of Advanced Science and Technology, Kobe University, Japan","Takatani, M., Graduate School of Engineering, Kobe University, Japan; Ariki, Y., Organization of Advanced Science and Technology, Kobe University, Japan; Takiguchi, T., Organization of Advanced Science and Technology, Kobe University, Japan","One of the most crucial techniques associated with Computer Vision is technology that deals with the automatic estimation of gaze orientation. In this paper, a method is proposed to estimate horizontal gaze orientation from a monocular camera image using the parameters of Active Appearance Models (AAM) selected based on several model selection methods. The proposed method can estimate horizontal gaze orientation more precisely than the conventional method (Ishikawa's method) because of the following two unique points: simultaneous estimation of horizontal head pose and gaze orientation, and the most suitable model formula for regression selected based on each model selection method. The validity of the proposed method was confirmed by experimental results. © 2011 Springer-Verlag Berlin Heidelberg.",,"Active appearance models; Automatic estimation; Conventional methods; Gaze estimation; Head pose; Information criterion; IS technologies; Model selection methods; Monocular cameras; Simultaneous estimation; Unique points; Active appearance models; Automatic estimation; Conventional methods; Gaze estimation; Information criterion; Model selection methods; Monocular cameras; Simultaneous estimation; Estimation; Regression analysis; Regression analysis; Computer vision; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-80053129898
"Sheela S.V., Vijaya P.A.","57010556300;6603130145;","An appearance based method for eye gaze tracking",2011,"Journal of Computer Science","7","8",,"1194","1203",,8,"10.3844/jcssp.2011.1194.1203","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053101698&doi=10.3844%2fjcssp.2011.1194.1203&partnerID=40&md5=f4c7663abbf51f1276edb272585f5962","BMS College of Engineering, Bangalore, India; Malnad College of Engineering, Hassan, India","Sheela, S.V., BMS College of Engineering, Bangalore, India; Vijaya, P.A., Malnad College of Engineering, Hassan, India","Problem statement: Gaze estimation systems compute the direction of eye gaze based on observed eye movements. The need for gaze-contingent applications is the basis of the current research work. The gaze pointing systems is a substitute for the existing input devices. Approach: The gaze tracking methods are either feature based or appearance based. In this study, an appearance based approach for gaze tracking is proposed based on Run Length Coding (RLC). The experiment was conducted considering transitional changes and the class-intervals in iris pixels. The image acquisition begins from the center of the screen in anticlockwise direction. The center of the screen was the pivot point. Results: Using RLC, the recognition rate of 95% was achieved. The image analysis in different directions determines the gaze point. The directions was determined with respect to the pivot point. Conclusion: The proposed system provides a robust, less computational gaze tracking method using web camera. © 2011 Science Publications.","Class-intervals; Gaze point; Region growing; Run length coding; Transitional changes",,Article,"Final","",Scopus,2-s2.0-80053101698
"Liu X., Che M.","50661569000;36098984900;","A parallel architecture of AdaBoost-based face detection for gaze estimation",2011,"2011 International Conference on Multimedia Technology, ICMT 2011",,,"6003069","2888","2891",,,"10.1109/ICMT.2011.6003069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052935101&doi=10.1109%2fICMT.2011.6003069&partnerID=40&md5=3ecbcf41c162466e19a5af44c7a74ce2","School of Computer Science and Technology, Tianjin University, Tianjin, China","Liu, X., School of Computer Science and Technology, Tianjin University, Tianjin, China; Che, M., School of Computer Science and Technology, Tianjin University, Tianjin, China","Face detection is an essential application in image processing and computer vision. In this paper, a parallel architecture for AdaBoost algorithm is proposed to support face detection in the embedded system. Some optimizations for the AdaBoost algorithm are applied to accelerate the process. Based on the software/hardware co-design methodology, a configurable high performance CPU (Nios) can take charge of task scheduling, image scaling and simple processing. Dedicated hardware accelerators are designed to deal with data intensive computing tasks through multi-level parallelism to meet the real-time requirement in embedded applications. The architecture is prototyped on Altera Cyclone II FPGA. The evaluation result shows that the system performs real-time detection with low resource consumption and high detection rate (96.7%). The detection speed of image consisting 640×480 pixels is 12 frames/s, and 41 frames/s for image consisting 320×240 pixels. It meets the requirement of gaze estimation. © 2011 IEEE.","AdaBoost parallelized architecture FPGA","AdaBoost algorithm; Altera cyclones; Co-design methodology; Configurable; Data-intensive computing; Dedicated hardware; Embedded application; Evaluation results; Gaze estimation; High detection rate; Image processing and computer vision; Image scaling; Multi-level parallelism; Real time requirement; Real-time detection; Resource consumption; Software/hardware; Task-scheduling; Algorithms; Computer architecture; Computer vision; Embedded systems; Parallel architectures; Pixels; Storms; Adaptive boosting",Conference Paper,"Final","",Scopus,2-s2.0-80052935101
"Asteriadis S., Karpouzis K., Kollias S.","55936774500;6602860576;57193712526;","The importance of eye gaze and head pose to estimating levels of attention",2011,"Proceedings - 2011 3rd International Conferenceon Games and Virtual Worlds for Serious Applications, VS-Games 2011",,,"5962089","186","191",,15,"10.1109/VS-GAMES.2011.38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052133168&doi=10.1109%2fVS-GAMES.2011.38&partnerID=40&md5=05f52d3e05f4adb7dca96d07939e455b","School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece","Asteriadis, S., School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Karpouzis, K., School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Kollias, S., School of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece","This paper explores, from a theoretical and technical point of view, the role of head rotation and eye gaze directionality to the human perception of attention from nonverbal cues. We have annotated two different versions of the same dataset, in order to correlate the above parameters with the degree people have considered people in the dataset pay attention to a hypothetical task they have in front of them. Based on our findings, we investigate the role of eye gaze directionality in relation to head rotations and, based on previous studies, we developed an algorithm for estimating attention levels from head pose, eye gaze and facial feature spatial locations. With the help of our AI system and people's annotation, we have made a first attempt towards quantifying the role of each cue to the overall estimation of attention. One of the important properties of the technical part of this work is that all systems we used were non-intrusive and did not demand any personal training or calibration phase, constituting themselves ideal for Game playing. Knowing the behavioral state of a player can be of vital importance for adapting the game design during interaction, or building personal profiles, leading to appropriate game features aiming at maximizing player satisfaction. © 2011 IEEE.","Eye gaze estimation; Head pose estimation; User attention estimation; User modelling","AI systems; Attention level; Behavioral state; Data sets; Eye-gaze; Facial feature; Game design; Game playing; Head pose; Head Pose Estimation; Head rotation; Human perception; Non-intrusive; Overall estimation; Personal profile; Spatial location; User attention; User Modelling; Estimation; Rotation; Social networking (online); Virtual reality; Motion estimation",Conference Paper,"Final","",Scopus,2-s2.0-80052133168
"Sheela S.V., Vijaya P.A.","57010556300;6603130145;","Gaze tracking based on run length coding",2011,"International Conference on Multimedia Computing and Systems -Proceedings",,,"5945682","","",,,"10.1109/ICMCS.2011.5945682","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79961217213&doi=10.1109%2fICMCS.2011.5945682&partnerID=40&md5=28e1b2fe2cf38148457c410baa545d2b","B. M. S. College of Engineering, Bangalore, India; Malnad College of Engineering, Hassan, India","Sheela, S.V., B. M. S. College of Engineering, Bangalore, India; Vijaya, P.A., Malnad College of Engineering, Hassan, India","Eye gaze determines the view point, focus of attention and interest of an individual. Gaze estimation systems compute the direction of eye gaze. These systems play an important role in human-computer interaction. The gaze pointing systems is a replacement for existing input devices in gaze communication applications. The gaze tracking methods are based on interpolation, 2D geometric transform mapping, edge change ratio in successive frames and singular value decomposition. This paper discusses a robust, less computational gaze tracking method using run length coding. 94.5% recognition rate of gaze direction is achieved using run length coding. © 2011 IEEE.","Gaze direction; Region growing; Run length coding; Similarity score","Change ratio; Communication application; Eye-gaze; Focus of Attention; Gaze direction; Gaze estimation; Gaze tracking; Geometric transform; Human-computer; Input devices; Pointing systems; Recognition rates; Region growing; Run-length coding; Similarity scores; Human computer interaction; Knowledge management; Mathematical transformations; Singular value decomposition",Conference Paper,"Final","",Scopus,2-s2.0-79961217213
"Yamamoto M., Sato H., Yoshida K., Nagamatsu T., Watanabe T.","56328923300;56111474600;57189244853;23398000100;55670249000;","Development of an eye-tracking pen display for analyzing embodied interaction",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6771 LNCS","PART 1",,"651","658",,1,"10.1007/978-3-642-21793-7_74","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960334187&doi=10.1007%2f978-3-642-21793-7_74&partnerID=40&md5=8d2dff3fd76a3ad2274adb969ccd00ae","2-1 Gakuen, Sanda Hyogo 669-1137, Japan; 5-1-1 Fukae-minami, Higashi-nada Kobe Hyogo 658-0022, Japan; 111 Kuboki, Soja Okayama 719-1197, Japan","Yamamoto, M., 2-1 Gakuen, Sanda Hyogo 669-1137, Japan; Sato, H., 2-1 Gakuen, Sanda Hyogo 669-1137, Japan; Yoshida, K., 2-1 Gakuen, Sanda Hyogo 669-1137, Japan; Nagamatsu, T., 5-1-1 Fukae-minami, Higashi-nada Kobe Hyogo 658-0022, Japan; Watanabe, T., 111 Kuboki, Soja Okayama 719-1197, Japan","In recent times, intuitive user interfaces such as the touch panel and pen display have become widely used in PCs and PDAs. Previously, the authors developed the bright pupil camera. They subsequently developed an eye-tracking pen display based on this camera and a new aspherical model of the eye. In this paper, a robust gaze estimation method that uses a integrated-light-source camera is proposed for analyzing embodied interaction. Then, a prototype of the eye-tracking pen display was developed. The accuracy of the system was approximately 12 mm on a 15″ pen display, which is sufficient for human interaction support. © 2011 Springer-Verlag.","aspherical model; Embodied interaction; eye-tracking; pen display","Aspherical; aspherical model; Embodied interaction; eye-tracking; Gaze estimation; Human interactions; Intuitive user interface; pen display; Touch panels; User interfaces; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-79960334187
"Mardanbegi D., Hansen D.W.","42761947400;15063910800;","Mobile gaze-based screen interaction in 3D environments",2011,"ACM International Conference Proceeding Series",,,"2","","",,11,"10.1145/1983302.1983304","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960194825&doi=10.1145%2f1983302.1983304&partnerID=40&md5=5ed9483d522ce8251411ad7d97555e69","IT University of Copenhagen, Rued Langgaards Vej 7, 2300 KBH. S., Denmark","Mardanbegi, D., IT University of Copenhagen, Rued Langgaards Vej 7, 2300 KBH. S., Denmark; Hansen, D.W., IT University of Copenhagen, Rued Langgaards Vej 7, 2300 KBH. S., Denmark","Head-mounted eye trackers can be used for mobile interaction as well as gaze estimation purposes. This paper presents a method that enables the user to interact with any planar digital display in a 3D environment using a head-mounted eye tracker. An effective method for identifying the screens in the field of view of the user is also presented which can be applied in a general scenario in which multiple users can interact with multiple screens. A particular application of using this technique is implemented in a home environment with two big screens and a mobile phone. In this application a user was able to interact with these screens using a wireless head-mounted eye tracker. Copyright 2011 ACM.","Domotics; Gaze-based interaction; Head-mounted eye tracker; Screen interaction","3-D environments; Digital display; Domotics; Eye trackers; Field of views; Gaze estimation; Gaze-based interaction; Home environment; Mobile interaction; Multiple user; Screen interaction; Eye controlled devices; Telecommunication equipment; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-79960194825
"Yamamoto M., Komeda M., Nagamatsu T., Watanabe T.","56328923300;37013367800;23398000100;55670249000;","Hyakunin-Eyesshu: A tabletop Hyakunin-Isshu game with computer opponent by the action prediction based on gaze detection",2011,"ACM International Conference Proceeding Series",,,"5","","",,4,"10.1145/1983302.1983307","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960185998&doi=10.1145%2f1983302.1983307&partnerID=40&md5=7edcf4cfec27b633d4dbf1a4904eb1ab","Kwansei Gakuin Univ., 2-1 Gakuen, Sanda, Hyogo 669-1337, Japan; Kobe Univ., 5-1-1 Fukaeminami, Higashinada, Kobe 658-0022, Japan; Okayama Pref. Univ., 111 Kuboki, Soja, mOkayama 719-1197, Japan","Yamamoto, M., Kwansei Gakuin Univ., 2-1 Gakuen, Sanda, Hyogo 669-1337, Japan; Komeda, M., Kwansei Gakuin Univ., 2-1 Gakuen, Sanda, Hyogo 669-1337, Japan; Nagamatsu, T., Kobe Univ., 5-1-1 Fukaeminami, Higashinada, Kobe 658-0022, Japan; Watanabe, T., Okayama Pref. Univ., 111 Kuboki, Soja, mOkayama 719-1197, Japan","A tabletop interface can enable interactions between images and real objects using various sensors; therefore, it can be used to create many works in the media arts field. By focusing on gazeand- touch interaction, we proposed the concept of an eye-tracking tabletop interface (ETTI) as a new type of interaction interface for the creation of media artworks. In this study, we developed ""Hyakunin-Eyesshu,"" a prototype for ETTI content that enables users to play the traditional Japanese card game ""Hyakunin- Isshu"" with a computer character. In addition, we demonstrated this system at an academic meeting and obtained user feedback. We expect that our work will lead to advancements in interfaces for various interactions and to various new media artworks with precise gaze estimation. Copyright 2011 ACM.","Eye tracking; Human interaction; Media arts; Tabletop","Action prediction; Card games; Eye-tracking; Gaze detection; Gaze estimation; Human interactions; Interaction interface; Media arts; New media; Real objects; Tabletop; Tabletop interfaces; Touch interaction; User feedback; Arts computing; Graphical user interfaces; Interfaces (computer)",Conference Paper,"Final","",Scopus,2-s2.0-79960185998
"Maio W., Chen J., Ji Q.","57202360797;22633570700;18935108400;","Constraint-based gaze estimation without active calibration",2011,"2011 IEEE International Conference on Automatic Face and Gesture Recognition and Workshops, FG 2011",,,"5771469","627","631",,3,"10.1109/FG.2011.5771469","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958701473&doi=10.1109%2fFG.2011.5771469&partnerID=40&md5=51147f8c84fc9b60b208e85c0b92e32b","Dept. of Electrical Computer and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY, United States","Maio, W., Dept. of Electrical Computer and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY, United States; Chen, J., Dept. of Electrical Computer and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY, United States; Ji, Q., Dept. of Electrical Computer and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY, United States","Existing eye gaze tracking systems typically require an explicit personal calibration process in order to estimate certain person-specific eye parameters. For natural human computer interaction, such a personal calibration is often cumbersome and unnatural. In this paper, we introduce a new method that estimates a person's gaze without active personal calibration. By exploiting the binocular constraint that the gaze point is produced by the intersection of visual axes of two eyes and some generic person-independent constraints on eye parameters, our method is able to estimate the required person-specific parameters implicitly and naturally without active participation from the user and without use of any special calibration object. Experiments with different subjects show that the proposed method achieves good gaze estimation comparable to the conventional 9 point method. © 2011 IEEE.",,"Calibration process; Constraint-based; Eye gaze tracking; Eye parameters; Gaze estimation; Gaze point; Person-independent; Calibration; Estimation; Face recognition; Human computer interaction; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-79958701473
"Hopmann M., Vexo F., Salamin P., Thalmann D., Chauvin N.","25822125500;6506767458;15124950400;7005885082;8539644500;","Natural activation for gesture recognition systems",2011,"Conference on Human Factors in Computing Systems - Proceedings",,,,"173","183",,6,"10.1145/1979742.1979642","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957940518&doi=10.1145%2f1979742.1979642&partnerID=40&md5=5c71d8fe33c65a42666021a2fe967224","VRLab - EPFL, Lausanne, Switzerland; Incubator - Logitech, Lausanne, Switzerland","Hopmann, M., VRLab - EPFL, Lausanne, Switzerland; Vexo, F., Incubator - Logitech, Lausanne, Switzerland; Salamin, P., Incubator - Logitech, Lausanne, Switzerland; Thalmann, D., VRLab - EPFL, Lausanne, Switzerland; Chauvin, N., Incubator - Logitech, Lausanne, Switzerland","Gesture recognition is becoming a popular way of interaction, but still suffers of important drawbacks to be integrated in everyday life devices. One of these drawbacks is the activation of the recognition system - trigger gesture - which is generally tiring and unnatural. In this paper, we propose two natural solutions to easily activate the gesture interaction. The first one requires a single action from the user: grasping a remote control to start interacting. The second one is completely transparent for the user: the gesture system is only activated when the user's gaze points to the screen, i.e. when s/he is looking at it. Our first evaluation with the 2 proposed solutions plus a default implementation suggests that the gaze estimation activation is efficient enough to remove the need of a trigger gesture in order to activate the recognition system.","Hand gesture; Human-computer interface","Gaze estimation; Gaze point; Gesture interaction; Gesture recognition system; Hand gesture; Human-computer interface; Natural activation; Recognition systems; Human computer interaction; Human engineering; Interfaces (computer); Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-79957940518
"Reale M.J., Canavan S., Yin L., Hu K., Hung T.","27868052800;24490527800;7203060635;36647621300;36508859200;","A multi-gesture interaction system using a 3-D iris disk model for gaze estimation and an active appearance model for 3-D hand pointing",2011,"IEEE Transactions on Multimedia","13","3","5720546","474","486",,65,"10.1109/TMM.2011.2120600","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957456055&doi=10.1109%2fTMM.2011.2120600&partnerID=40&md5=1257f3019f21379cd0d9da1668138efc","Department of Computer Science, State University of New York at Binghamton, Binghamton, NY 13902, United States; Corning Corp., Taichung City 40763, Taiwan","Reale, M.J., Department of Computer Science, State University of New York at Binghamton, Binghamton, NY 13902, United States; Canavan, S., Department of Computer Science, State University of New York at Binghamton, Binghamton, NY 13902, United States; Yin, L., Department of Computer Science, State University of New York at Binghamton, Binghamton, NY 13902, United States; Hu, K., Department of Computer Science, State University of New York at Binghamton, Binghamton, NY 13902, United States; Hung, T., Corning Corp., Taichung City 40763, Taiwan","In this paper, we present a vision-based humancomputer interaction system, which integrates control components using multiple gestures, including eye gaze, head pose, hand pointing, and mouth motions. To track head, eye, and mouth movements, we present a two-camera system that detects the face from a fixed, wide-angle camera, estimates a rough location for the eye region using an eye detector based on topographic features, and directs another active pan-tilt-zoom camera to focus in on this eye region. We also propose a novel eye gaze estimation approach for point-of-regard (POR) tracking on a viewing screen. To allow for greater head pose freedom, we developed a new calibration approach to find the 3-D eyeball location, eyeball radius, and fovea position. Moreover, in order to get the optical axis, we create a 3-D iris disk by mapping both the iris center and iris contour points to the eyeball sphere. We then rotate the fovea accordingly and compute the final, visual axis gaze direction. This part of the system permits natural, non-intrusive, pose-invariant POR estimation from a distance without resorting to infrared or complex hardware setups. We also propose and integrate a two-camera hand pointing estimation algorithm for hand gesture tracking in 3-D from a distance. The algorithms of gaze pointing and hand finger pointing are evaluated individually, and the feasibility of the entire system is validated through two interactive information visualization applications. © 2011 IEEE.","Gaze estimation; hand tracking; humancomputer interaction (HCI)","Active appearance models; Complex hardware; Contour points; Control components; Disk model; Entire system; Estimation algorithm; Eye-gaze; Gaze direction; Gaze estimation; Hand fingers; Hand gesture; hand tracking; Head pose; Human-computer interaction system; humancomputer interaction (HCI); Interaction systems; Interactive information visualization; Non-intrusive; Optical axis; Pan-tilt-zoom camera; Rough location; Topographic features; Vision based; Wide-angle camera; Algorithms; Cameras; Estimation; Eye movements; Face recognition; Information systems; Soil structure interactions; Spheres; Three dimensional; User interfaces; Visualization; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-79957456055
"Noris B., Keller J.-B., Billard A.","24492107300;37021420100;7006389948;","A wearable gaze tracking system for children in unconstrained environments",2011,"Computer Vision and Image Understanding","115","4",,"476","486",,55,"10.1016/j.cviu.2010.11.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79751525444&doi=10.1016%2fj.cviu.2010.11.013&partnerID=40&md5=be125f4d1a505eb47bea1bcc66d94100","Learning Algorithms and Systems Laboratory, EPFL-STI-IMT-LASA, CH1015 Lausanne, Switzerland","Noris, B., Learning Algorithms and Systems Laboratory, EPFL-STI-IMT-LASA, CH1015 Lausanne, Switzerland; Keller, J.-B., Learning Algorithms and Systems Laboratory, EPFL-STI-IMT-LASA, CH1015 Lausanne, Switzerland; Billard, A., Learning Algorithms and Systems Laboratory, EPFL-STI-IMT-LASA, CH1015 Lausanne, Switzerland","We present here a head-mounted gaze tracking system for the study of visual behavior in unconstrained environments. The system is designed both for adults and for infants as young as 1 year of age. The system uses two CCD cameras to record a very wide field of view (96° × 96°) that allows to study both central and peripheral vision. A small motor-driven mirror allows to obtain the direction of the wearer's gaze with no need for active lighting and with little intrusiveness. The calibration of the system is done offline allowing experiments to be conducted with subjects who cannot cooperate in a calibration phase (e.g. very young children, animals). We use illumination normalization to increase the robustness of the system, and eye blinking detection to avoid tracking errors. We use Support Vector Regression to estimate a mapping between the appearance of the eyes and the corresponding gaze direction. The system can be used successfully indoors as well as outdoors and reaches an accuracy of 1.59° with adults and 2.42° with children. © 2011 Published by Elsevier Inc.","Appearance based; Children; Eye blinking; Gaze tracking; Lay users; Support Vector Regression; Unconstrained environments","Appearance based; Children; Eye-blinking; Gaze tracking; Lay users; Support Vector Regression; Unconstrained environments; Animals; Behavioral research; Calibration; Cameras; CCD cameras; Eye controlled devices; Navigation; Regression analysis; Tracking (position)",Article,"Final","",Scopus,2-s2.0-79751525444
"Du C., Yang J., Wu Q., He X.","23027271900;15039078800;57205085145;7404409118;","Locating facial landmarks by support vector machine-based active shape model",2011,"International Journal of Intelligent Systems Technologies and Applications","10","2",,"151","170",,1,"10.1504/IJISTA.2011.039017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863116654&doi=10.1504%2fIJISTA.2011.039017&partnerID=40&md5=665348b1b1a1f946c8f214f8fc0a55f6","Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai 200240, China; School of Computing and Communications, University of Technology, P.O. Box 123, Sydney 2007, Australia","Du, C., Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai 200240, China; Yang, J., Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai 200240, China; Wu, Q., School of Computing and Communications, University of Technology, P.O. Box 123, Sydney 2007, Australia; He, X., School of Computing and Communications, University of Technology, P.O. Box 123, Sydney 2007, Australia","Active shape model (ASM) plays an important role in face research such as face recognition, pose estimation and gaze estimation. A crucial step of the common ASM is finding a new position for each facial landmark at each iteration. Mahalanobis distance minimisation is used for this finding, provided there are enough training data such that the grey-level profiles for each landmark following a multivariate Gaussian distribution. However, this condition could not be satisfied in most cases. In this paper, a novel method support vector machine-based active shape model (SVMBASM) is proposed for this task. It approaches the finding task as a small sample size classification problem. Moreover, considering the poor classification performance caused by the imbalanced dataset which contains more negative instances (incorrect candidates for new position) than positive instances (correct candidates for new position), a multi-class classification framework is further proposed. Performance evaluation on Shanghai Jiao Tong University face database shows that the proposed SVMBASM outperforms the original ASM in terms of the average error and average frequency of convergence. © 2011 Inderscience Enterprises Ltd.","Active shape model; ASM; Facial landmarks; Multi-class classification; New position; Support vector machine; SVM",,Article,"Final","",Scopus,2-s2.0-84863116654
"Lee S.J., Jo J., Jung H.G., Park K.R., Kim J.","36084903600;37026159900;55693458300;8983316300;8859952200;","Real-time gaze estimator based on driver's head orientation for forward collision warning system",2011,"IEEE Transactions on Intelligent Transportation Systems","12","1","5688323","254","267",,81,"10.1109/TITS.2010.2091503","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952068730&doi=10.1109%2fTITS.2010.2091503&partnerID=40&md5=ce0441fb88ae4af287cb8ef5416c533e","School of Electrical and Electronic Engineering, Biometrics Engineering Research Center, Yonsei University, Seoul 120-749, South Korea; Mando Corporation, Yongin 446-901, South Korea; Division of Electronics and Electrical Engineering, Biometrics Engineering Research Center, Dongguk University, Seoul 100-715, South Korea","Lee, S.J., School of Electrical and Electronic Engineering, Biometrics Engineering Research Center, Yonsei University, Seoul 120-749, South Korea; Jo, J., School of Electrical and Electronic Engineering, Biometrics Engineering Research Center, Yonsei University, Seoul 120-749, South Korea; Jung, H.G., Mando Corporation, Yongin 446-901, South Korea; Park, K.R., Division of Electronics and Electrical Engineering, Biometrics Engineering Research Center, Dongguk University, Seoul 100-715, South Korea; Kim, J., School of Electrical and Electronic Engineering, Biometrics Engineering Research Center, Yonsei University, Seoul 120-749, South Korea","This paper presents a vision-based real-time gaze zone estimator based on a driver's head orientation composed of yaw and pitch. Generally, vision-based methods are vulnerable to the wearing of eyeglasses and image variations between day and night. The proposed method is novel in the following four ways: First, the proposed method can work under both day and night conditions and is robust to facial image variation caused by eyeglasses because it only requires simple facial features and not specific features such as eyes, lip corners, and facial contours. Second, an ellipsoidal face model is proposed instead of a cylindrical face model to exactly determine a driver's yaw. Third, we propose new featuresthe normalized mean and the standard deviation of the horizontal edge projection histogramto reliably and rapidly estimate a driver's pitch. Fourth, the proposed method obtains an accurate gaze zone by using a support vector machine. Experimental results from 200000 images showed that the root mean square errors of the estimated yaw and pitch angles are below 7 under both daylight and nighttime conditions. Equivalent results were obtained for drivers with glasses or sunglasses, and 18 gaze zones were accurately estimated using the proposed gaze estimation method. © 2010 IEEE.","Driver monitoring system; forward collision warning (FCW) system; gaze estimation; head orientation estimation; precrash system","Driver monitoring system; Forward collision warnings; gaze estimation; head orientation estimation; precrash system; Alarm systems; Estimation; Eyeglasses; Flight dynamics; Monitoring",Article,"Final","",Scopus,2-s2.0-79952068730
"Lu F., Okabe T., Sugano Y., Sato Y.","54956194300;7201390055;7005470045;35230954300;","A head pose-free approach for appearance-based gaze estimation",2011,"BMVC 2011 - Proceedings of the British Machine Vision Conference 2011",,,,"","",,57,"10.5244/C.25.126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898434495&doi=10.5244%2fC.25.126&partnerID=40&md5=edef28d184fd0855493b76bb4ec386fc","Institute of Industrial Science, University of Tokyo, Tokyo, Japan","Lu, F., Institute of Industrial Science, University of Tokyo, Tokyo, Japan; Okabe, T., Institute of Industrial Science, University of Tokyo, Tokyo, Japan; Sugano, Y., Institute of Industrial Science, University of Tokyo, Tokyo, Japan; Sato, Y., Institute of Industrial Science, University of Tokyo, Tokyo, Japan","To infer human gaze from eye appearance, various methods have been proposed. However, most of them assume a fixed head pose because allowing free head motion adds 6 degrees of freedom to the problem and requires a prohibitively large number of training samples. In this paper, we aim at solving the appearance-based gaze estimation problem under free head motion without significantly increasing the cost of training. The idea is to decompose the problem into subproblems, including initial estimation under fixed head pose and subsequent compensations for estimation biases caused by head rotation and eye appearance distortion. Then each subproblem is solved by either learning-based method or geometric-based calculation. Specifically, the gaze estimation bias caused by eye appearance distortion is learnt effectively from a 5-seconds video clip. Extensive experiments were conducted to verify the effectiveness of the proposed approach. © 2011. The copyright of this document resides with its authors.",,"Appearance based; Estimation bias; Gaze estimation; Head rotation; Initial estimation; Learning-based methods; Sub-problems; Training sample; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84898434495
"Ince I.F., Kim J.W.","25927066200;55900864500;","A 2D eye gaze estimation system with low-resolution webcam images",2011,"Eurasip Journal on Advances in Signal Processing","2011",,"40","","",,21,"10.1186/1687-6180-2011-40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885162111&doi=10.1186%2f1687-6180-2011-40&partnerID=40&md5=537d2e15b5207ef772ef7bf4dbe2a0a4","School of Engineering, University of Tokyo, Tokyo, Japan; Department of Information Communication Engineering, Kyungsung University, Busan, South Korea","Ince, I.F., School of Engineering, University of Tokyo, Tokyo, Japan; Kim, J.W., Department of Information Communication Engineering, Kyungsung University, Busan, South Korea","In this article, a low-cost system for 2D eye gaze estimation with low-resolution webcam images is presented. Two algorithms are proposed for this purpose, one for the eye-ball detection with stable approximate pupil-center and the other one for the eye movements' direction detection. Eyeball is detected using deformable angular integral search by minimum intensity (DAISMI) algorithm. Deformable template-based 2D gaze estimation (DTBGE) algorithm is employed as a noise filter for deciding the stable movement decisions. While DTBGE employs binary images, DAISMI employs gray-scale images. Right and left eye estimates are evaluated separately. DAISMI finds the stable approximate pupil-center location by calculating the mass-center of eyeball border vertices to be employed for initial deformable template alignment. DTBGE starts running with initial alignment and updates the template alignment with resulting eye movements and eyeball size frame by frame. The horizontal and vertical deviation of eye movements through eyeball size is considered as if it is directly proportional with the deviation of cursor movements in a certain screen size and resolution. The core advantage of the system is that it does not employ the real pupil-center as a reference point for gaze estimation which is more reliable against corneal reflection. Visual angle accuracy is used for the evaluation and benchmarking of the system. Effectiveness of the proposed system is presented and experimental results are shown. © 2011 Ince and Kim; licensee Springer.","Deformable template methods; Eyeball detection; Low-cost 2D eye gaze estimation; Robust eye movement detection; Stable approximate pupil-center detection","Binary images; Cost estimating; Deformation; Corneal reflection; Deformable templates; Direction detections; Eye-gaze; Gray-scale images; Initial alignment; Movement detection; Pupil centers; Eye movements",Article,"Final","",Scopus,2-s2.0-84885162111
"Halawani A., Li H., Anani A.","6602265138;56375326400;24779206000;","Building eye contact in e-learning through head-eye coordination",2011,"International Journal of Social Robotics","3","1",,"95","106",,2,"10.1007/s12369-010-0070-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857626609&doi=10.1007%2fs12369-010-0070-y&partnerID=40&md5=6f823398a0f988ccc43a249a0361991f","Digital Media Lab, Dept. of Applied Physics and Electronics, Umeå University, 90187 Umeå, Sweden","Halawani, A., Digital Media Lab, Dept. of Applied Physics and Electronics, Umeå University, 90187 Umeå, Sweden; Li, H., Digital Media Lab, Dept. of Applied Physics and Electronics, Umeå University, 90187 Umeå, Sweden; Anani, A., Digital Media Lab, Dept. of Applied Physics and Electronics, Umeå University, 90187 Umeå, Sweden","Video conferencing is a very effective tool to use for e-learning. Most of the available video conferencing systems suffer a main drawback represented by the lack of eye contact between participants. In this paper we present a new scheme for building eye contact in e-learning sessions. The scheme assumes a video conferencing session with ""one teacher many students"" arrangement. In our system, eye contact is achieved without the need for any gaze estimation technique. Instead, we ""generate the gaze"" by allowing the user communicate his visual attention to the system through head-eye coordination. To enable real time and precise head-eye coordination, a head motion tracking technique is required. Unlike traditional head tracking systems, our procedure suggests mounting the camera on the user's head rather than in front of it. This configuration achieves much better resolution and thus leads to better tracking results. Promising results obtained from both demo and real time experiments demonstrate the effectiveness and efficiency of the proposed scheme. Although this paper concentrates on elearning, the proposed concept can be easily extended to the world of interaction with social robotics, in which introducing eye contact between humans and robots would be of great advantage. © Springer Science & Business Media BV 2010.","E-learning; Eye contact without gaze estimation; Head-eye coordination; Human-robot interaction; Motion tracking; Sift; Video conferencing","Agricultural robots; Behavioral research; E-learning; Human robot interaction; Social robots; Tracking (position); Video conferencing; Visual communication; Effectiveness and efficiencies; Eye coordination; Gaze estimation; Head tracking system; Real-time experiment; Sift; Tracking techniques; Video conferencing system; Motion tracking",Article,"Final","",Scopus,2-s2.0-84857626609
"Pirri F., Pizzoli M., Rudi A.","56990245000;35234885200;36170983300;","A general method for the point of regard estimation in 3D space",2011,"Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition",,,"5995634","921","928",,34,"10.1109/CVPR.2011.5995634","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052906073&doi=10.1109%2fCVPR.2011.5995634&partnerID=40&md5=5b1f59eac7cc9bf359eb5a88202d93ab","Dipartimento di Informatica e Sistemistica, Sapienza Università di Roma, via Ariosto 25, 00185 Rome, Italy","Pirri, F., Dipartimento di Informatica e Sistemistica, Sapienza Università di Roma, via Ariosto 25, 00185 Rome, Italy; Pizzoli, M., Dipartimento di Informatica e Sistemistica, Sapienza Università di Roma, via Ariosto 25, 00185 Rome, Italy; Rudi, A., Dipartimento di Informatica e Sistemistica, Sapienza Università di Roma, via Ariosto 25, 00185 Rome, Italy","A novel approach to 3D gaze estimation for wearable multi-camera devices is proposed and its effectiveness is demonstrated both theoretically and empirically. The proposed approach, firmly grounded on the geometry of the multiple views, introduces a calibration procedure that is efficient, accurate, highly innovative but also practical and easy. Thus, it can run online with little intervention from the user. The overall gaze estimation model is general, as no particular complex model of the human eye is assumed in this work. This is made possible by a novel approach, that can be sketched as follows: each eye is imaged by a camera; two conics are fitted to the imaged pupils and a calibration sequence, consisting in the subject gazing a known 3D point, while moving his/her head, provides information to 1) estimate the optical axis in 3D world; 2) compute the geometry of the multi-camera system; 3) estimate the Point of Regard in 3D world. The resultant model is being used effectively to study visual attention by means of gaze estimation experiments, involving people performing natural tasks in wide-field, unstructured scenarios. © 2011 IEEE.",,"Behavioral research; Calibration; Computational geometry; Pattern recognition; Calibration procedure; Gaze estimation; General method; Multicamera systems; Multiple views; Natural tasks; Point of regards; Visual Attention; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-80052906073
"Chen J., Ji Q.","22633570700;18935108400;","Probabilistic gaze estimation without active personal calibration",2011,"Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition",,,"5995675","609","616",,66,"10.1109/CVPR.2011.5995675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052905437&doi=10.1109%2fCVPR.2011.5995675&partnerID=40&md5=9c25ccee6bd574d0a2c2422a5f5b7623","Department of Electrical, Computer and System Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180, United States","Chen, J., Department of Electrical, Computer and System Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180, United States; Ji, Q., Department of Electrical, Computer and System Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180, United States","Existing eye gaze tracking systems typically require an explicit personal calibration process in order to estimate certain person-specific eye parameters. For natural human computer interaction, such a personal calibration is often cumbersome and unnatural. In this paper, we propose a new probabilistic eye gaze tracking system without explicit personal calibration. Unlike the traditional eye gaze tracking methods, which estimate the eye parameter deterministically, our approach estimates the probability distributions of the eye parameter and the eye gaze, by combining image saliency with the 3D eye model. By using an incremental learning framework, the subject doesn't need personal calibration before using the system. His/her eye parameter and gaze estimation can be improved gradually when he/she is naturally viewing a sequence of images on the screen. The experimental result shows that the proposed system can achieve less than three degrees accuracy for different people without calibration. © 2011 IEEE.",,"Calibration; Human computer interaction; Image enhancement; Parameter estimation; Pattern recognition; Probability distributions; Calibration process; Eye gaze tracking; Eye parameters; Gaze estimation; Image saliencies; Incremental learning; Natural human computer interactions; Sequence of images; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-80052905437
"Nagamatsu T., Sugano R., Iwamoto Y., Kamahara J., Tanaka N.","23398000100;36053963000;36052629200;14632144300;35318753700;","User-calibration-free gaze estimation method using a binocular 3D eye model",2011,"IEICE Transactions on Information and Systems","E94-D","9",,"1817","1829",,8,"10.1587/transinf.E94.D.1817","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052330149&doi=10.1587%2ftransinf.E94.D.1817&partnerID=40&md5=1027612f303b60493d630659d047181b","Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan; Mitsubishi Electric Corporation, Japan","Nagamatsu, T., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan; Sugano, R., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan; Iwamoto, Y., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan, Mitsubishi Electric Corporation, Japan; Kamahara, J., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan; Tanaka, N., Graduate School of Maritime Sciences, Kobe University, Kobe-shi, 658-0022, Japan","This paper presents a user-calibration-free method for estimating the point of gaze (POG). This method provides a fast and stable solution for realizing user-calibration-free gaze estimation more accurately than the conventional method that uses the optical axis of the eye as an approximation of the visual axis of the eye. The optical axis of the eye can be estimated by using two cameras and two light sources. This estimation is carried out by using a spherical model of the cornea. The point of intersection of the optical axis of the eye with the object that the user gazes at is termed POA. On the basis of an assumption that the visual axes of both eyes intersect on the object, the POG is approximately estimated using the binocular 3D eye model as the midpoint of the line joining the POAs of both eyes. Based on this method, we have developed a prototype system that comprises a 19′ display with two pairs of stereo cameras. We evaluated the system experimentally with 20 subjects who were at a distance of 600mm from the display. The root-mean-square error (RMSE) of measurement of POG in the display screen coordinate system is 1.58°. © 2011 The Institute of Electronics, Information and Communication Engineers.","Calibration-free; Eye model; Eye movement; Gaze tracking","Binoculars; Bins; Calibration; Cameras; Estimation; Light sources; Mean square error; Stereo image processing; Tracking (position); Calibration free; Conventional methods; Eye model; Gaze tracking; Point of intersections; Prototype system; Root mean square errors; Stable solutions; Eye movements",Article,"Final","",Scopus,2-s2.0-80052330149
"Arai K., Mardiyanto R.","7403965268;35280403200;","Comparative study on blink detection and gaze estimation methods for HCI, in particular, gabor filter utilized blink detection method",2011,"Proceedings - 2011 8th International Conference on Information Technology: New Generations, ITNG 2011",,,"5945276","441","446",,9,"10.1109/ITNG.2011.84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051534227&doi=10.1109%2fITNG.2011.84&partnerID=40&md5=91d76fc2e166717818f2a7423fa8ebe6","Saga University, Japan","Arai, K., Saga University, Japan; Mardiyanto, R., Saga University, Japan","Blink detection is used for a variety of applications such as Human-Computer Interaction, wearable computing, etc. Blink detection method with Gabor filter is proposed to realize high accurate blink detection. The blink detection accuracy is evaluated by several people and compare to the other existing conventional methods. Through the comparison, it is found that the proposed blink detection method with Gabor filter is superior to the other methods. © 2011 IEEE.","blink detection; Gabor filter; gaze detection; HCI; wearable computing","Human computer interaction; Wearable computers; Blink detections; Comparative studies; Conventional methods; Gaze detection; Gaze estimation; Wearable computing; Gabor filters",Conference Paper,"Final","",Scopus,2-s2.0-80051534227
"Arai K., Mardiyanto R.","7403965268;35280403200;","Eye-based HCI with full specification of mouse and keyboard using pupil knowledge in the gaze estimation",2011,"Proceedings - 2011 8th International Conference on Information Technology: New Generations, ITNG 2011",,,"5945273","423","428",,9,"10.1109/ITNG.2011.81","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051478539&doi=10.1109%2fITNG.2011.81&partnerID=40&md5=26dd8f2024167c695001f309f0c6d812","Saga University, Japan","Arai, K., Saga University, Japan; Mardiyanto, R., Saga University, Japan","Eye-based Human Computer Interaction (HCI) with full specification of mouse and keyboard by human eyes only is proposed for, in particular, disabled person and for input device of wearable computing. It utilizes pupil knowledge in gaze estimation process. Existing conventional eye-mouse, gaze-mouse does not robust against various types of users with different features of color, shape and size of eyes and also is affected by illumination changes, users' movement (attitude changes), etc. Using knowledge about features, the influences are eliminated. Also the proposed eye-based HCI system allows simultaneous key input of more than three keys, for instance, ""Ctl+Alt+Del"" for initiating task manager, left/right click and drag/drop of mouse event capabilities. Although currently commercially available screen keyboard allows simultaneous key input for two keys such as ""shit+*"", ""Alt+*"", it does not allow three keys input at once. Such these full specification of mouse events and keyboard functions are available for the proposed HCI system. Experimental results with six different nationalities of users with the different features shows effectiveness of using the knowledge for improvement of gaze estimation accuracy. © 2011 IEEE.","eye-mouse; gaze estimation; human computer interaction; keyboard operation; mouse event","Computer keyboard substitutes (disability aids); Disabled persons; Eye controlled devices; Eye movements; Human computer interaction; Specifications; Eye mouse; Gaze estimation; Human computer interaction (HCI); Illumination changes; Keyboard operation; mouse event; Screen keyboards; Wearable computing; Mammals",Conference Paper,"Final","",Scopus,2-s2.0-80051478539
"Mori H., Sumiya E., Mashita T., Kiyokawa K., Takemura H.","57202878512;35325478700;14319405900;7005065755;7201655004;","A wide-view parallax-free eye-mark recorder with a hyperboloidal half-silvered mirror and appearance-based gaze estimation",2011,"IEEE Transactions on Visualization and Computer Graphics","17","7","5557872","900","912",,6,"10.1109/TVCG.2010.113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955858495&doi=10.1109%2fTVCG.2010.113&partnerID=40&md5=3600b23cebe92d46665200e78486cb95","Cybermedia Center, Osaka University, 1-32 Machikaneyama, Toyonaka, Osaka 560-0043, Japan","Mori, H., Cybermedia Center, Osaka University, 1-32 Machikaneyama, Toyonaka, Osaka 560-0043, Japan; Sumiya, E., Cybermedia Center, Osaka University, 1-32 Machikaneyama, Toyonaka, Osaka 560-0043, Japan; Mashita, T., Cybermedia Center, Osaka University, 1-32 Machikaneyama, Toyonaka, Osaka 560-0043, Japan; Kiyokawa, K., Cybermedia Center, Osaka University, 1-32 Machikaneyama, Toyonaka, Osaka 560-0043, Japan; Takemura, H., Cybermedia Center, Osaka University, 1-32 Machikaneyama, Toyonaka, Osaka 560-0043, Japan","In this paper, we propose a wide-view parallax-free eye-mark recorder with a hyperboloidal half-silvered mirror and a gaze estimation method suitable for the device. Our eye-mark recorder provides a wide field-of-view video recording of the user's exact view by positioning the focal point of the mirror at the user's viewpoint. The vertical angle of view of the prototype is 122 degree (elevation and depression angles are 38 and 84 degree, respectively) and its horizontal view angle is 116 degree (nasal and temporal view angles are 38 and 78 degree, respectively). We implemented and evaluated a gaze estimation method for our eye-mark recorder. We use an appearance-based approach for our eye-mark recorder to support a wide field-of-view. We apply principal component analysis (PCA) and multiple regression analysis (MRA) to determine the relationship between the captured images and their corresponding gaze points. Experimental results verify that our eye-mark recorder successfully captures a wide field-of-view of a user and estimates gaze direction with an angular accuracy of around 2 to 4 degree. © 2011 IEEE.","Eye-mark recorder; gaze estimation; half-silvered hyperboloidal mirror; head-mounted camera; parallax free; wide field-of-view","Geometrical optics; Mirrors; Regression analysis; Video recording; Eye-mark recorder; Gaze estimation; Head mounted Camera; Hyperboloidal mirrors; parallax free; Wide field of view; Principal component analysis; devices; equipment design; eye fixation; eye movement; human; image processing; optics; physiology; procedures; videorecording; Equipment Design; Eye Movements; Fixation, Ocular; Humans; Image Processing, Computer-Assisted; Optics and Photonics; Video Recording",Article,"Final","",Scopus,2-s2.0-79955858495
"Ge H., Chen X.","36610157800;8284171300;","Directional Binary Pattern (DBP): A novel object representation approach for gaze estimation",2010,"Proceedings - 2010 3rd International Congress on Image and Signal Processing, CISP 2010","2",,"5646899","912","915",,,"10.1109/CISP.2010.5646899","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650584371&doi=10.1109%2fCISP.2010.5646899&partnerID=40&md5=71fa401b072d4a14d609ae9b0fb3a10c","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","Ge, H., School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Chen, X., Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","This paper presents a novel object descriptor, Directional Binary Pattern (DBP), for robust gaze estimation. In DBP, the local directional derivations are proposed to encode the binary patterns in the given orientations. As an object descriptor, DBP has many advantages: noise restrain, robustness to illumination, contrast enhancement on the boundary, and local texture encoded as the directional information. DBP features of eyes are finally fed into Support Vector Regression (SVR) to match the gaze mapping function, which is then used to predict the gaze direction with respect to the camera coordinate system. In our experiments, an eye gaze dataset includes 4089 training samples of 11 persons. Experimental results show that our method can achieve an accuracy of less than 2°. ©2010 IEEE.","Directional Binary Pattern (DBP); Gaze estimation; Local directional derivations; Object descriptor","Binary patterns; Co-ordinate system; Contrast Enhancement; Data sets; Descriptors; Directional information; Eye-gaze; Gaze direction; Gaze estimation; Local directional derivations; Local Texture; Mapping functions; Object representations; Support vector regressions; Training sample; Signal processing; Estimation",Conference Paper,"Final","",Scopus,2-s2.0-78650584371
"Zhang W., Zhang T.-N., Chang S.-J.","7409432038;35306194000;7405604811;","Gazing estimation and correction from elliptical features of one iris",2010,"Proceedings - 2010 3rd International Congress on Image and Signal Processing, CISP 2010","4",,"5647733","1647","1652",,1,"10.1109/CISP.2010.5647733","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650562325&doi=10.1109%2fCISP.2010.5647733&partnerID=40&md5=aac7fcaf5c874ddf410323a212a350a4","Institute of Modern Optics, Nankai University, Tianjin, China","Zhang, W., Institute of Modern Optics, Nankai University, Tianjin, China; Zhang, T.-N., Institute of Modern Optics, Nankai University, Tianjin, China; Chang, S.-J., Institute of Modern Optics, Nankai University, Tianjin, China","The accuracy of eye gaze estimation by image information is affected by several objective factors, including the image resolution, anatomical structure of eye, posture change, etc. Especially, the irregular movements of head and eye are the main problem and key technology being researched. We describe an effective way of estimating the eye gazing from the elliptical features of one iris under the conditions without auxiliary source, head fixing equipment or multiple-camera. Firstly, we give the preliminary estimations of the gazing direction and then obtain the vectors describing translation and rotation of eyeball movement using central projection on the cross section passing through the line-of-sight, which avoids the complex computations involved in known methods. We also disambiguate the solution on the basis of the experimental findings. Secondly, the error correction is carried on the BP neural network trained by a sample collection of the translation and rotation vectors. In our simulations, we achieve an accuracy of 0.8° on the test images which are different from the training images. The result is found to be better than that of the existing non- intrusive method with single-camera. The performance of the algorithm proves that the proposed method has excellent generalized abilities. ©2010 IEEE.","Central projection; Ellipse fitting; Eye gazing; Iris contour; Neural network; Translation estimation","Central projection; Ellipse fitting; Eye gazing; Iris contour; Translation estimation; Auxiliary equipment; Cameras; Estimation; Eye movements; Image resolution; Rotation; Signal processing; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-78650562325
"Keil A., Albuquerque G., Berger K., Magnor M.A.","55247550600;26024644300;24605323600;57191188427;","Real-time gaze tracking with a consumer-grade video camera",2010,"18th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, WSCG 2010 - In Co-operation with EUROGRAPHICS, Full Papers Proceedings",,,,"129","134",,5,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862107745&partnerID=40&md5=c2adb3fd1799e532c427d7fd2b0dc43c","TU Braunschweig, Germany","Keil, A., TU Braunschweig, Germany; Albuquerque, G., TU Braunschweig, Germany; Berger, K., TU Braunschweig, Germany; Magnor, M.A., TU Braunschweig, Germany","Eye gaze can be a rich source of information to identify particular interests of human users. Eye gaze tracking has been largely used in different research areas in the last years, as for example in psychology, visual system design and to leverage the user interaction with computer systems. In this paper, we present an IR-based gaze tracking framework that can be easily coupled to common user applications and allows for real-time gaze estimation. Compared to other gaze tracking systems, our system uses only affordable consumer-grade hardware and still achieves fair accuracy. To evaluate the usability of our gaze tracking system, we performed a user study with persons of different genders and ethnicities.","Application; Eye tracking; Gaze tracking; Human-computer interaction; Interactivity","Eye gaze tracking; Eye-gaze; Eye-tracking; Gaze estimation; Gaze tracking; Gaze tracking system; Human users; Interactivity; System use; User interaction; User study; Visual system designs; Applications; Computer graphics; Computer systems; Gesture recognition; Tracking (position); Visualization; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84862107745
"Yamamoto M., Komeda M., Nagamatsu T., Watanabe T.","56328923300;37013367800;23398000100;55670249000;","Development of eye-tracking tabletop interface for media art works",2010,"ACM International Conference on Interactive Tabletops and Surfaces, ITS 2010",,,,"295","296",,9,"10.1145/1936652.1936723","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952418853&doi=10.1145%2f1936652.1936723&partnerID=40&md5=fa21b9ea44dabc1f401ef079951a7157","Kwansei Gakuin Univ., 2-1 Gakuen, Sanda, Hyogo 669-1337, Japan; Kobe Univ., 5-1-1 Fukaeminami, Higashinada, Kobe 658-0022, Japan; Okayama Pref. Univ., 111 Kuboki, Soja, Okayama 719-1197, Japan","Yamamoto, M., Kwansei Gakuin Univ., 2-1 Gakuen, Sanda, Hyogo 669-1337, Japan; Komeda, M., Kwansei Gakuin Univ., 2-1 Gakuen, Sanda, Hyogo 669-1337, Japan; Nagamatsu, T., Kobe Univ., 5-1-1 Fukaeminami, Higashinada, Kobe 658-0022, Japan; Watanabe, T., Okayama Pref. Univ., 111 Kuboki, Soja, Okayama 719-1197, Japan","A tabletop interface can enable interactions with images and real objects by using various sensors; therefore, such an interface can be applied for the creation of many artworks in the field of media arts. In this study, by focusing on gaze-and-touch interaction, we have proposed the concept of an eye-tracking tabletop interface (ETTI) as a new type of interaction interface for the creation of media artworks. Further, we have developed a prototype ETTI and an interactive art application, ""Hyakunin-Eyesshu"". Thus, we have paved the way for new surface interactions and a new variety of media artworks with precise gaze estimation. © 2010 ACM.","Eye tracking; Human interaction; Media arts; Tabletop","Eye-tracking; Gaze estimation; Human interactions; Interaction interface; Interactive arts; Media arts; Real objects; Surface interactions; Tabletop; Tabletop interfaces; Touch interaction; Interactive devices",Conference Paper,"Final","",Scopus,2-s2.0-79952418853
"Zeng J., Sun Y., Jiang L.","57212322440;36562722100;57198538552;","Driver distraction detection and identity recognition in real-time",2010,"Proceedings - 2010 2nd WRI Global Congress on Intelligent Systems, GCIS 2010","3",,"5709318","43","46",,10,"10.1109/GCIS.2010.83","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952404345&doi=10.1109%2fGCIS.2010.83&partnerID=40&md5=456432051cd9ae00490888786a7c089f","Department of Computer Science and Technology, Tongji University, Shanghai, China","Zeng, J., Department of Computer Science and Technology, Tongji University, Shanghai, China; Sun, Y., Department of Computer Science and Technology, Tongji University, Shanghai, China; Jiang, L., Department of Computer Science and Technology, Tongji University, Shanghai, China","Drivers attending to primary driving tasks show specific eye and head movement behaviours, while the distracted drive generally covers the states including drivers' eyes off the road and long-term eye closure. This paper presents a distraction detection system by using the strategy of ""attention budget"". The states of eyes off the road and face with closed eyes are used to lessen the ""attention budget"" while the reversed conditions gain it. Drivers' gaze estimation is derived from the head motion, and the stage classifiers working with haar-like features are used to detect head movements and eye states. With regard to the factors of drivers' personal characteristics in distraction detection, the recognition of drivers is implemented by extraction and matching of scale invariant feature transform features in detected frontal face. The results of experiments validate the effectiveness and robustness of the system. © 2010 IEEE.","Attention budget; Driver distraction detection; Identity recognition; Scale invariant feature transform","Attention budget; Detection system; Driver distractions; Eye closure; Frontal faces; Gaze estimation; Haar-like features; Head motion; Head movements; Identity recognition; Personal characteristics; Primary driving tasks; Scale invariant feature transforms; Budget control; Eye movements; Feature extraction; Intelligent systems; Roads and streets; Face recognition",Conference Paper,"Final","",Scopus,2-s2.0-79952404345
"Shao G., Che M., Zhang B., Cen K., Gao W.","36629223700;36098984900;57199725661;36656864400;57211732765;","A novel simple 2D model of eye gaze estimation",2010,"Proceedings - 2010 2nd International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2010","1",,"5590853","300","304",,25,"10.1109/IHMSC.2010.81","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78449240571&doi=10.1109%2fIHMSC.2010.81&partnerID=40&md5=17d9c3edb1acf37327237fb1fd7c5d6b","School of Computer Science and Technology, Tianjin University, Tianjin, China","Shao, G., School of Computer Science and Technology, Tianjin University, Tianjin, China; Che, M., School of Computer Science and Technology, Tianjin University, Tianjin, China; Zhang, B., School of Computer Science and Technology, Tianjin University, Tianjin, China; Cen, K., School of Computer Science and Technology, Tianjin University, Tianjin, China; Gao, W., School of Computer Science and Technology, Tianjin University, Tianjin, China","With the development of image process technology, camera-based eye gaze estimation methods make a possible nonintrusive way of human computer interaction (HCI). Most of them base on Pupil-Cornea Reflection Technique (PCRT) which uses extra light sources and estimates gaze point on screen by a polynomial mapping function. In this paper, the input vector of classical PCRT is adjusted to a new one which can be precisely extracted in the system using only one camera. The adjusted PCRT is called Pupil-Corner Technique (PCT). Meanwhile, a novel simple 2D Geometric Model (GM) is proposed. It bases on the geometric relationship of eyeballs and screen and simulates the movement of eyeballs when user looks through the screen with his head still. Furthermore, a fitting method is used to promote the accuracy of GM. At last, the combined technique called Geometric Fitting Technique (GFT) is compared with PCT in experiments and the results show that GFT has an acceptable accuracy which can be applied in HCI applications and future geometric model is suggested by the guide concluded from the paper to promote accuracy. © 2010 IEEE.","Eye gaze estimation; Eye gaze model; Eye gaze tracking; Human computer interaction; Mapping function","2-D model; Combined techniques; Eye gaze tracking; Eye-gaze; Fitting method; Gaze point; Geometric fitting; Geometric models; Geometric relationships; Image process; Input vector; Mapping function; Mapping functions; Non-intrusive; One camera; Cameras; Cybernetics; Estimation; Geometry; Gesture recognition; Light reflection; Light sources; Mapping; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-78449240571
"Reale M., Hung T., Yin L.","27868052800;36508859200;7203060635;","Pointing with the eyes: Gaze estimation using a static/active camera system and 3D iris disk model",2010,"2010 IEEE International Conference on Multimedia and Expo, ICME 2010",,,"5583014","280","285",,13,"10.1109/ICME.2010.5583014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78349239268&doi=10.1109%2fICME.2010.5583014&partnerID=40&md5=d1f5d667cb32137fc3ae87949fbcc0f0","Department of Computer Science, State University of New York, Binghamton, United States","Reale, M., Department of Computer Science, State University of New York, Binghamton, United States; Hung, T., Department of Computer Science, State University of New York, Binghamton, United States; Yin, L., Department of Computer Science, State University of New York, Binghamton, United States","The ability to capture the direction the eyes point in while the subject is a distance away from the camera offers the potential for intuitive human-computer interfaces, allowing for a greater interactivity, more intelligent HCI behavior, and increased flexibility. In this paper, we present a two-camera system that detects the face from a fixed, wide-angle camera, estimates a rough location for the eye region using an eye detector based on topographic features, and directs another active pan-tilt-zoom camera to focus in on this eye region. We also propose a novel eye gaze estimation approach for point-of-regard (PoG) tracking on a large viewing screen. To allow for greater head pose freedom, we developed a new calibration approach to find the 3D eyeball location, eyeball radius, and fovea position. Moreover, we map both the iris center and iris contour points to the eyeball sphere (creating a 3D iris disk) to get the optical axis; we then rotate the fovea accordingly and compute our final, visual axis gaze direction. We intend to integrate this gaze estimation approach with our two-camera system, permitting natural, non-intrusive, pose-invariant PoG estimation in distance and allowing user translational freedom without resorting to infrared or complex hardware setups such as stereo-cameras or ""smart rooms."" © 2010 IEEE.","Eye gaze estimation; Eye tracking; Static/active dual-camera system","Camera systems; Complex hardware; Contour points; Disk model; Eye-gaze; Eye-tracking; Gaze direction; Gaze estimation; Head pose; Human computer interfaces; Increased flexibility; Interactivity; Non-intrusive; Optical axis; Pan-tilt-zoom camera; Rough location; Smart rooms; Static/active dual-camera system; Topographic features; Translational freedom; Wide-angle camera; Disks (structural components); Estimation; Interfaces (computer); Spheres; Three dimensional; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-78349239268
"Ge H.","36610157800;","Gabor directional binary pattern: An image descriptor for gaze estimation",2010,"Eurasip Journal on Advances in Signal Processing","2010",,"807612","","",,3,"10.1155/2010/807612","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78249252359&doi=10.1155%2f2010%2f807612&partnerID=40&md5=0cdf6f55c70976b02e82f0bcd05a852f","School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China","Ge, H., School of Computer Science and Technology, Harbin Institute of Technology, Harbin 150001, China","This paper proposes an image descriptor, Gabor Directional Binary Pattern (GDBP), for robust gaze estimation. In GDBP, Gabor magnitude information is extracted firstly from a cropped subimage. The local directional derivations are then utilized to encode the binary patterns in the given orientations. As an image descriptor, GDBP can suppress noises and robustness to illumination variations. Meanwhile, the encoding pattern can emphasize boundary. We use the GDBP features of eye regions and adopt the Support Vector Regression (SVR) to approximate the gaze mapping function, which is then used to predict the gaze direction with respect to the camera coordinate system. In the person-independent experiments, our dataset includes 4089 samples of 11 persons. Experimental results show that the gaze estimation can achieve an accuracy of less than 2° by using the proposed GDBP and SVR. Copyright © 2010 Hongzhi Ge.",,"Binary patterns; Co-ordinate system; Data sets; Descriptors; Encoding patterns; Gaze direction; Gaze estimation; Illumination variation; Magnitude information; Mapping functions; Person-independent; Support vector regressions; Encoding (symbols); Estimation",Article,"Final","",Scopus,2-s2.0-78249252359
"Valenti R., Lablack A., Sebe N., Djeraba C., Gevers T.","57192175392;25923368300;57204924633;55892468800;7003472472;","Visual gaze estimation by joint head and eye information",2010,"Proceedings - International Conference on Pattern Recognition",,,"5597628","3870","3873",,10,"10.1109/ICPR.2010.1160","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149484031&doi=10.1109%2fICPR.2010.1160&partnerID=40&md5=3ca2cf5aeb555ab71152b682ab3422a3","Intelligent Systems Lab. Amsterdam, University of Amsterdam, Netherlands; Laboratoire d'Informatique Fondamentale de Lille, University of Lille, France; Department of Information Engineering and Computer Science, University of Trento, Italy","Valenti, R., Intelligent Systems Lab. Amsterdam, University of Amsterdam, Netherlands; Lablack, A., Laboratoire d'Informatique Fondamentale de Lille, University of Lille, France; Sebe, N., Department of Information Engineering and Computer Science, University of Trento, Italy; Djeraba, C., Laboratoire d'Informatique Fondamentale de Lille, University of Lille, France; Gevers, T., Intelligent Systems Lab. Amsterdam, University of Amsterdam, Netherlands","In this paper, we present an unconstrained visual gaze estimation system. The proposed method extracts the visual field of view of a person looking at a target scene in order to estimate the approximate location of interest (visual gaze). The novelty of the system is the joint use of head pose and eye location information to fine tune the visual gaze estimated by the head pose only, so that the system can be used in multiple scenarios. The improvements obtained by the proposed approach are validated using the Boston University head pose dataset, on which the standard deviation of the joint visual gaze estimation improved by 61.06% horizontally and 52.23% vertically with respect to the gaze estimation obtained by the head pose only. A user study shows the potential of the proposed system. © 2010 IEEE.",,"Boston University; Data sets; Eye location; Gaze estimation; Head pose; Standard deviation; User study; Visual fields; Estimation; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-78149484031
"Yonetani R., Kawashima H., Hirayama T., Matsuyama T.","36622392900;8870589800;55531799600;7401770410;","Gaze probing: Event-based estimation of objects being focused on",2010,"Proceedings - International Conference on Pattern Recognition",,,"5597638","101","104",,9,"10.1109/ICPR.2010.33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149474730&doi=10.1109%2fICPR.2010.33&partnerID=40&md5=210ba39ededcb29e3cb54c067624d99c","Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University Yoshida-Honmachi, Sakyo, Kyoto, 6068501, Japan","Yonetani, R., Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University Yoshida-Honmachi, Sakyo, Kyoto, 6068501, Japan; Kawashima, H., Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University Yoshida-Honmachi, Sakyo, Kyoto, 6068501, Japan; Hirayama, T., Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University Yoshida-Honmachi, Sakyo, Kyoto, 6068501, Japan; Matsuyama, T., Department of Intelligence Science and Technology, Graduate School of Informatics, Kyoto University Yoshida-Honmachi, Sakyo, Kyoto, 6068501, Japan","We propose a novel method to estimate the object that a user is focusing on by using the synchronization between the movements of objects and a user's eyes as a cue. We first design an event as a characteristic motion pattern, and we then embed it within the movement of each object. Since the user's ocular reactions to these events are easily detected using a passive camera-based eye tracker, we can successfully estimate the object that the user is focusing on as the one whose movement is most synchronized with the user's eye reaction. Experimental results obtained from the application of this system to dynamic content (consisting of scrolling images) demonstrate the effectiveness of the proposed method over existing methods. © 2010 IEEE.","Dynamic content; Event-based gaze estimation; Eye movement; Synchronization","Dynamic content; Event-based; Existing method; Eye trackers; Motion pattern; Novel methods; Estimation; Pattern recognition; Synchronization; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-78149474730
"Reale M., Hung T., Yin L.","27868052800;36508859200;7203060635;","Viewing direction estimation based on 3D eyeball construction for HRI",2010,"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010",,,"5543784","24","31",,19,"10.1109/CVPRW.2010.5543784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956539362&doi=10.1109%2fCVPRW.2010.5543784&partnerID=40&md5=482849385549c26a7e2ec7dac526c54e","Department of Computer Science, State University of New York, Binghamton, United States","Reale, M., Department of Computer Science, State University of New York, Binghamton, United States; Hung, T., Department of Computer Science, State University of New York, Binghamton, United States; Yin, L., Department of Computer Science, State University of New York, Binghamton, United States","Natural human-robot interaction requires leveraging viewing direction information in order to recognize, respond to, and even emulate human behavior. Knowledge of the eye gaze and point of regard gives us insight into what the subject is interested in and /or who the subject is addressing. In this paper, we present a novel eye gaze estimation approach for point-of-regard (PoG) tracking. To allow for greater head pose freedom, we introduce a new calibration approach to find the 3D eyeball location, eyeball radius, and fovea position. To estimate gaze direction, we map both the iris center and iris contour points to the eyeball sphere (creating a 3D iris disk), giving us the optical axis. We then rotate the fovea accordingly and compute our final, visual axis gaze direction. Our intention is to integrate this eye gaze approach with a dual-camera system we have developed that detects the face and eyes from a fixed, wide-angle camera and directs another active pan-tilt-zoom camera to focus in on this eye region. The final system will permit natural, non-intrusive, pose-invariant PoG estimation in distance and allow user translational freedom without resorting to infrared equipment or complex hardware setups. © 2010 IEEE.","Eye gaze estimation; Eye tracking; Iris modeling","Camera systems; Complex hardware; Contour points; Direction estimation; Eye-gaze; Eye-tracking; Gaze direction; Head pose; Human -robot interactions; Human behaviors; Infrared equipment; Iris modeling; Non-intrusive; Optical axis; Pan-tilt-zoom camera; Point of regards; Translational freedom; Wide-angle camera; Behavioral research; Cameras; Computer vision; Human computer interaction; Human robot interaction; Spheres; Technical presentations; Three dimensional; Estimation",Conference Paper,"Final","",Scopus,2-s2.0-77956539362
"Doshi A., Trivedi M.M.","16479757800;7103153314;","Attention estimation by simultaneous observation of viewer and view",2010,"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, CVPRW 2010",,,"5543272","21","27",,39,"10.1109/CVPRW.2010.5543272","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956514653&doi=10.1109%2fCVPRW.2010.5543272&partnerID=40&md5=a986f05db58bd995b1db63ab97c3a110","Computer Vision and Robotics Research Lab, University of California, San Diego, CA 92093-0434, United States","Doshi, A., Computer Vision and Robotics Research Lab, University of California, San Diego, CA 92093-0434, United States; Trivedi, M.M., Computer Vision and Robotics Research Lab, University of California, San Diego, CA 92093-0434, United States","We introduce a new approach to analyzing the attentive state of a human subject, given cameras focused on the subject and their environment. In particular, the task of analyzing the focus of attention of a human driver is of primary concern. Up to 80% of automobile crashes are related to driver inattention; thus it is important for an Intelligent Driver Assistance System (IDAS) to be aware of the driver state. We present a new Bayesian paradigm for estimating human attention specifically addressing the problems arising in dynamic situations. The model incorporates vision-based gaze estimation, ""top-down""-and ""bottomup""-based visual saliency maps, and cognitive considerations such as inhibition of return and center bias that affect the relationship between gaze and attention. Results demonstrate the validity on real driving data, showing quantitative improvements over systems using only gaze or only saliency, and elucidate the value of such a model for any human-machine interface. © 2010 IEEE.",,"Bayesian paradigm; Center bias; Focus of Attention; Gaze estimation; Human attention; Human drivers; Human Machine Interface; Human subjects; Intelligent driver assistance systems; New approaches; Simultaneous observation; Topdown; Vision based; Visual saliency; Automobile drivers; Computer vision; Estimation; Man machine systems; Visualization; Technical presentations",Conference Paper,"Final","",Scopus,2-s2.0-77956514653
"Schnieders D., Fu X., Wong K.-Y.K.","25655251600;36444991400;57216110773;","Reconstruction of display and eyes from a single image",2010,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",,,"5539799","1442","1449",,22,"10.1109/CVPR.2010.5539799","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955997123&doi=10.1109%2fCVPR.2010.5539799&partnerID=40&md5=1a680940431d933b3bfef50c3b0fc83d","Department of Computer Science, University of Hong Kong, Hong Kong","Schnieders, D., Department of Computer Science, University of Hong Kong, Hong Kong; Fu, X., Department of Computer Science, University of Hong Kong, Hong Kong; Wong, K.-Y.K., Department of Computer Science, University of Hong Kong, Hong Kong","This paper introduces a novel method for reconstructing human eyes and visual display from reflections on the cornea. This problem is difficult because the camera is not directly facing the display, but instead captures the eyes of a person in front of the display. Reconstruction of eyes and display is useful for point-of-gaze estimation, which can be approximated from the 3D positions of the iris and display. It is shown that iris boundaries (limbus) and display reflections in a single intrinsically calibrated image provide enough information for such an estimation. The proposed method assumes a simplified geometric eyeball model with certain anatomical constants which are used to reconstruct the eye. A noise performance analysis shows the sensitivity of the proposed method to imperfect data. Experiments on various subjects show that it is possible to determine the approximate area of gaze on a display from a single image. ©2010 IEEE.",,"3D positions; Display reflection; Human eye; Imperfect data; Noise performance; Novel methods; Point of gaze; Single images; Visual display; Computational methods; Computer vision; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-77955997123
"Sugano Y., Matsushita Y., Sato Y.","7005470045;35956654700;35230954300;","Calibration-free gaze sensing using saliency maps",2010,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",,,"5539984","2667","2674",,70,"10.1109/CVPR.2010.5539984","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955992410&doi=10.1109%2fCVPR.2010.5539984&partnerID=40&md5=7ac0d5959524620aed91d05ea497613a","University of Tokyo, Tokyo, 153-8505, Japan; Microsoft Research Asia, Beijing, 100080, China","Sugano, Y., University of Tokyo, Tokyo, 153-8505, Japan; Matsushita, Y., Microsoft Research Asia, Beijing, 100080, China; Sato, Y., University of Tokyo, Tokyo, 153-8505, Japan","We propose a calibration-free gaze sensing method using visual saliency maps. Our goal is to construct a gaze estimator only using eye images captured from a person watching a video clip. The key is treating saliency maps of the video frames as probability distributions of gaze points. To efficiently identify gaze points from saliency maps, we aggregate saliency maps based on the similarity of eye appearances. We establish mapping between eye images to gaze points by Gaussian process regression. The experimental result shows that the proposed method works well with different people and video clips and achieves 6 degrees of accuracy, which is useful for estimating a person's attention on monitors. ©2010 IEEE.",,"Eye images; Gaussian process regression; Gaze point; Saliency map; Video clips; Video frame; Visual saliency; Calibration; Computer vision; Estimation; Image segmentation; Video cameras; Visualization; Probability distributions",Conference Paper,"Final","",Scopus,2-s2.0-77955992410
"Sewell W., Komogortsev O.","36096334000;6506328653;","Real-time eye gaze tracking with an unmodified commodity webcam employing a neural network",2010,"Conference on Human Factors in Computing Systems - Proceedings",,,,"3739","3744",,71,"10.1145/1753846.1754048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953095379&doi=10.1145%2f1753846.1754048&partnerID=40&md5=3dea59f974bacf06906370a209cceea0","Texas State University-San Marcos, Department of Computer Science, 601 University Drive, San Marcos, TX 78666-4616, United States","Sewell, W., Texas State University-San Marcos, Department of Computer Science, 601 University Drive, San Marcos, TX 78666-4616, United States; Komogortsev, O., Texas State University-San Marcos, Department of Computer Science, 601 University Drive, San Marcos, TX 78666-4616, United States","An eye-gaze-guided computer interface could enable computer use by the seriously disabled but existing systems cost tens of thousands of dollars or have cumbersome setups. This paper presents a methodology for real-time eye gaze tracking using a standard webcam without the need for hardware modification or special placement. An artificial neural network was employed to estimate the location of the user's gaze based on an image of the user's eye, mimicking the way that humans determine where another person is looking. Accuracy measurements and usability experiments were performed using a laptop computer with a webcam built into the screen. The results show this approach to be promising for the development of usable eye tracking systems using standard webcams, particularly those built into many laptop computers. © 2010 Copyright is held by the author/owner(s).","Eye tracker; Gaze estimation; Human computer interaction; Neural network; Webcam","Accuracy measurements; Artificial Neural Network; Computer interfaces; Computer use; Existing systems; Eye gaze tracking; Eye trackers; Eye tracking systems; Eye-gaze; Gaze estimation; Hardware modifications; WebCams; Computer networks; Eye controlled devices; Gesture recognition; Human computer interaction; Human engineering; Laptop computers; Standardization; Neural networks",Conference Paper,"Final","",Scopus,2-s2.0-77953095379
[无可用作者姓名],[无可用的作者 ID],"Proceedings of ETRA 2010: ACM Symposium on Eye-Tracking Research and Applications",2010,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",357,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952503338&partnerID=40&md5=fc7408c76071a20dac802a4e64c255b4",,"","The proceedings contain 66 papers. The topics discussed include: homography normalization for robust gaze estimation in uncalibrated setups; head-mounted eye-tracking of infants' natural interactions: a new method; user-calibration-free remote gaze estimation system; eye movement as an interaction mechanism for relevance feedback in a content-based image retrieval system; eye movement as an interaction mechanism for relevance feedback in a content-based image retrieval system; gaze scribing in physics problem solving; estimation of viewer's response for contextual understanding of tasks using features of eye-movements; biometric identification via an oculomotor plant mathematical model; qualitative and quantitative scoring and evaluation of the eye movement classifi cation algorithms; and an interactive interface for remote administration of clinical tests based on eye tracking.",,,Conference Review,"Final","",Scopus,2-s2.0-77952503338
"Hansen D.W., Agustin J.S., Villanueva A.","15063910800;25722947500;7101612861;","Homography normalization for robust gaze estimation in uncalibrated setups",2010,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"13","20",,59,"10.1145/1743666.1743670","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952418585&doi=10.1145%2f1743666.1743670&partnerID=40&md5=21ce07b63a0c6429412cc0830c870d55","IT University, Copenhagen, Denmark; Public University, Navarra, Spain","Hansen, D.W., IT University, Copenhagen, Denmark; Agustin, J.S., IT University, Copenhagen, Denmark; Villanueva, A., Public University, Navarra, Spain","Homography normalization is presented as a novel gaze estimation method for uncalibrated setups. The method applies when head movements are present but without any requirements to camera calibration or geometric calibration. The method is geometrically and empirically demonstrated to be robust to head pose changes and despite being less constrained than cross-ratio methods, it consistently performs favorably by several degrees on both simulated data and data from physical setups. The physical setups include the use of off-the-shelf web cameras with infrared light (night vision) and standard cameras with and without infrared light. The benefits of homography normalization and uncalibrated setups in general are also demonstrated through obtaining gaze estimates (in the visible spectrum) using only the screen reflections on the cornea. © 2010 ACM.","Eye tracking; Gaussian process; Gaze estimation; HCI; Homography normalization; Uncalibrated setup","Camera calibration; Cross-ratios; Eye-tracking; Gaussian Processes; Gaze estimation; Geometric calibrations; Head movements; Head pose; Homographies; Infrared light; Night vision; Simulated data; Standard cameras; Visible spectra; Web camera; Calibration; Cameras; Gaussian distribution; Gaussian noise (electronic); Estimation",Conference Paper,"Final","",Scopus,2-s2.0-77952418585
"Guestrin E.D., Eizenman M.","14009410300;6701402159;","Listing's and Donders' laws and the estimation of the point-of-gaze",2010,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"199","202",,4,"10.1145/1743666.1743715","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952406611&doi=10.1145%2f1743666.1743715&partnerID=40&md5=eef5c11b133efa922bba1b02babd4117","University of Toronto, Toronto Rehabilitation Institute, Canada; University of Toronto, Canada","Guestrin, E.D., University of Toronto, Toronto Rehabilitation Institute, Canada; Eizenman, M., University of Toronto, Canada","This paper examines the use of Listing's and Donders' laws for the calculation of the torsion of the eye in the estimation of the point-of-gaze. After describing Listing's and Donders' laws and providing their analytical representation, experimental results obtained while subjects looked at a computer screen are presented. The experimental results show that when the point-of-gaze was estimated using Listing's and Donders' laws there was no significant accuracy improvement relative to when eye torsion was ignored. While for a larger range of eye rotation the torsion would be more significant and should be taken into account, the torsion predicted by Listing's and Donders' laws may be inaccurate, even in ideal conditions. Moreover, eye torsion resulting from lateral head tilt can be significantly larger than the torsion predicted by Listing's and Donders' laws, and even have opposite direction. To properly account for eye torsion, it should be measured independently (e.g., by tracking the iris pattern and/or the scleral blood vessels). © 2010 ACM.","Donders' law; Eye torsion; Gaze estimation accuracy; Kinematics of the eye; Listing's law","Accuracy Improvement; Computer screens; Donders' law; Gaze estimation; Iris patterns; Point of gaze; Blood vessels; Estimation; Kinematics; Torsional stress",Conference Paper,"Final","",Scopus,2-s2.0-77952406611
"Yamamoto M., Nagamatsu T., Watanabe T.","56328923300;23398000100;55670249000;","Development of eye-tracking pen display based on stereo bright pupil technique",2010,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"165","168",,6,"10.1145/1743666.1743707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952396269&doi=10.1145%2f1743666.1743707&partnerID=40&md5=b3ab5b03d64e9b91dd34e55d720e281e","School of Science and Technology, Kwansei Gakuin University, Japan; Graduate School of Maritime Sciences, Kobe University, Japan; Department of Systems Engineering, Okayama Prefectural University, United States","Yamamoto, M., School of Science and Technology, Kwansei Gakuin University, Japan; Nagamatsu, T., Graduate School of Maritime Sciences, Kobe University, Japan; Watanabe, T., Department of Systems Engineering, Okayama Prefectural University, United States","The intuitive user interfaces of PCs and PDAs, such as pen display and touch panel, have become widely used in recent times. In this study, we have developed an eye-tracking pen display based on the stereo bright pupil technique. First, the bright pupil camera was developed by examining the arrangement of cameras and LEDs for pen display. Next, the gaze estimation method was proposed for the stereo bright pupil camera, which enables one point calibration. Then, the prototype of the eye-tracking pen display was developed. The accuracy of the system was approximately 0.7° on average, which is sufficient for human interaction support. We also developed an eye-tracking tabletop as an application of the proposed stereo bright pupil technique. © 2010 ACM.","Bright pupil technique; Embodied interaction; Eye-tracking; Pen display","Embodied interaction; Eye-tracking; Gaze estimation; Human interactions; Intuitive user interface; Touch panels; User interfaces; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-77952396269
"Nagamatsu T., Iwamoto Y., Kamahara J., Tanaka N., Yamamoto M.","23398000100;36052629200;14632144300;35318753700;56328923300;","Gaze estimation method based on an aspherical model of the cornea: Surface of revolution about the optical axis of the eye",2010,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"255","258",,23,"10.1145/1743666.1743726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952357693&doi=10.1145%2f1743666.1743726&partnerID=40&md5=c3f9f2416c9be61d7fb02c282959b639","Kobe University, Japan; Kwansei Gakuin University, Japan","Nagamatsu, T., Kobe University, Japan; Iwamoto, Y., Kobe University, Japan; Kamahara, J., Kobe University, Japan; Tanaka, N., Kobe University, Japan; Yamamoto, M., Kwansei Gakuin University, Japan","A novel gaze estimation method based on a novel aspherical model of the cornea is proposed in this paper. The model is a surface of revolution about the optical axis of the eye. The calculation method is explained on the basis of the model. A prototype system for estimating the point of gaze (POG) has been developed using this method. The proposed method has been found to be more accurate than the gaze estimation method based on a spherical model of the cornea. © 2010 ACM.","Calibration-free; Eye model; Eye movement; Gaze tracking","Aspherical; Calculation methods; Eye model; Gaze estimation; Gaze tracking; Optical axis; Point of gaze; Prototype system; Spherical models; Surface of revolution; Calibration; Eye movements; Light measurement; Estimation",Conference Paper,"Final","",Scopus,2-s2.0-77952357693
"Model D., Eizenman M.","11541277100;6701402159;","User-calibration-free remote gaze estimation system",2010,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"29","36",,22,"10.1145/1743666.1743672","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952348218&doi=10.1145%2f1743666.1743672&partnerID=40&md5=f9476acba914b6bbaacfa7cb476b8974","University of Toronto, Canada","Model, D., University of Toronto, Canada; Eizenman, M., University of Toronto, Canada","Gaze estimation systems use calibration procedures that require active subject participation to estimate the point-of-gaze accurately. In these procedures, subjects are required to fixate on a specific point or points in space at specific time instances. This paper describes a gaze estimation system that does not use calibration procedures that require active user participation. The system estimates the optical axes of both eyes using images from a stereo pair of video cameras without a personal calibration procedure. To estimate the point-of-gaze, which lies along the visual axis, the angles between the optical and visual axes are estimated by a novel automatic procedure that minimizes the distance between the intersections of the visual axes of the left and right eyes with the surface of a display while subjects look naturally at the display (e.g., watching a video clip). Experiments with four subjects demonstrate that the RMS error of this point-of-gaze estimation system is 1.3°. © 2010 ACM.","Calibration free; Minimal subject cooperation; Point-of-gaze; Remote gaze estimation","Automatic procedures; Calibration free; Calibration procedure; Gaze estimation; Optical axes; Point of gaze; Remote gaze estimation; RMS errors; Specific time; Stereo pair; User participation; Video clips; Calibration; Estimation; Tools; Video cameras; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-77952348218
"Arai K., Mardiyanto R.","7403965268;35280403200;","Improvement of gaze estimation robustness using pupil knowledge",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6017 LNCS","PART 2",,"336","350",,1,"10.1007/978-3-642-12165-4_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952318675&doi=10.1007%2f978-3-642-12165-4_27&partnerID=40&md5=2d1e3956011594857e11ff247721d1f7","Depatment of Information Science, Saga University, Japan; Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","Arai, K., Depatment of Information Science, Saga University, Japan; Mardiyanto, R., Depatment of Information Science, Saga University, Japan, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","This paper presents an eye gaze estimation system which robust against various users. Our method utilizes an IR camera mounted on glass to allow user's movement. Pupil knowledge such as shape, size, location, and motion are used. This knowledge works based on the knowledge priority. Pupil appearance such as size, color, and shape are used as the first priority. When this step fails, then pupil is estimated based on its location as second priority. When all steps fail, then we estimate pupil based on its motion as the last priority. The aim of this proposed method is to make the system compatible for various user as well as to overcome problem associated with illumination changes and user movement. The proposed system is tested using several users with various race as well as nationality and the experiment result are compared to the well-known adaptive threshold method and template matching method. The proposed method shows good performance, robustness, accuracy and stability against illumination changes without any prior calibration. © 2010 Springer-Verlag Berlin Heidelberg.","Eye detection; Gaze; Pupil; Pupil knowledge","Eye protection; Adaptive threshold method; Eye detection; Gaze; Gaze estimation; Illumination changes; Pupil; Pupil knowledge; Template matching method; Template matching",Conference Paper,"Final","",Scopus,2-s2.0-77952318675
"Hansen D.W., Ji Q.","15063910800;18935108400;","In the Eye of the Beholder: A Survey of Models for Eyes and Gaze",2010,"IEEE Transactions on Pattern Analysis and Machine Intelligence","32","3",,"478","500",,970,"10.1109/TPAMI.2009.30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949331734&doi=10.1109%2fTPAMI.2009.30&partnerID=40&md5=19f5763a890a51fc4e87d640a724abd8","IT University of Copenhagen, Rued Langaardsvej 7, 2300 Copenhagen S., Denmark; Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, JEC 7004, Troy, NY 12180-3590, United States","Hansen, D.W., IT University of Copenhagen, Rued Langaardsvej 7, 2300 Copenhagen S., Denmark; Ji, Q., Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, JEC 7004, Troy, NY 12180-3590, United States","Despite active research and significant progress in the last 30 years, eye detection and tracking remains challenging due to the individuality of eyes, occlusion, variability in scale, location, and light conditions. Data on eye location and details of eye movements have numerous applications and are essential in face detection, biometric identification, and particular human-computer interaction tasks. This paper reviews current progress and state of the art in video-based eye detection and tracking in order to identify promising techniques as well as issues to be further addressed. We present a detailed review of recent eye models and techniques for eye detection and tracking. We also survey methods for gaze estimation and compare them based on their geometric properties and reported accuracies. This review shows that, despite their apparent simplicity, the development of a general eye detection technique involves addressing many challenges, requires further theoretical developments, and is consequently of interest to many other domains problems in computer vision and beyond. © 2010, The Institute of Electrical and Electronics Engineers, Inc.","Eye; eye detection; eye tracking; gaze estimation; gaze tracking; human-computer interaction; object detection and tracking; review paper","biological model; eye fixation; eye movement; head movement; human; oculography; physiology; regression analysis; review; Eye Movement Measurements; Eye Movements; Fixation, Ocular; Head Movements; Humans; Models, Biological; Regression Analysis",Article,"Final","",Scopus,2-s2.0-77949331734
"Purwanto D., Mardiyanto R., Arai K.","56600944500;35280403200;7403965268;","Electric wheelchair control with gaze direction and eye blinking",2009,"Proceedings of the 14th International Symposium on Artificial Life and Robotics, AROB 14th'09",,,,"436","439",,15,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149330246&partnerID=40&md5=4aa17ddd3b6d69de44138d1b0aebf12a","Department of Electrical Engineering ITS, Kampus ITS Sukolilo, Surabaya 60111, Indonesia; Department of Information Science, Saga University, 1 Honjo, Saga 840-8502, Japan","Purwanto, D., Department of Electrical Engineering ITS, Kampus ITS Sukolilo, Surabaya 60111, Indonesia; Mardiyanto, R., Department of Electrical Engineering ITS, Kampus ITS Sukolilo, Surabaya 60111, Indonesia; Arai, K., Department of Information Science, Saga University, 1 Honjo, Saga 840-8502, Japan","In this research, the electric wheelchair control with gaze direction and eye blinking is proposed. A camera is set up in front of wheelchair user to capture image information. The sequential captured image is interpreted to obtain the gaze direction and eye blinking properties. The gaze direction is expressed by horizontal angle of gaze, and it is derived from the triangle form formed by the centre positions of eyes and nose. The gaze direction and eye blinking are used to provide the direction and timing command, respectively. The direction command relates to the movement direction of electric wheelchair, and the timing command relates to the time condition when the wheelchair should move. The timing command with eye blinking mechanism is designed to generate ready, backward movement, and stop command for the electric wheelchair. Furthermore, to move in certain velocity, the electric wheelchair also receives the velocity command beside the direction and timing command. The disturbance observer based control system is used to control the direction and velocity. For safety purpose, the emergency stop is generated when electric wheelchair user do not focus the gaze direction consistently in specified time. A number of experiments conducted for the electric wheelchair in a laboratory environment. The simulation results indicate the effectiveness of the proposed electric wheelchair system. ©ISAROB 2009.","Blinking measurement; Electric wheelchair control; Gaze estimation","Blinking measurement; Capture images; Disturbance observer; Electric wheelchair; Emergency stop; Eye-blinking; Gaze direction; Gaze estimation; Horizontal angles; Laboratory environment; Simulation result; Time condition; Electric generators; Electric variables measurement; Eye movements; Observability; Robotics; Time measurement; Wheelchairs",Conference Paper,"Final","",Scopus,2-s2.0-78149330246
"Eizenman M., Model D., Guestrin E.D.","6701402159;11541277100;14009410300;","Covert monitoring of the point-of-gaze",2009,"TIC-STH'09: 2009 IEEE Toronto International Conference - Science and Technology for Humanity",,,"5444437","551","556",,2,"10.1109/TIC-STH.2009.5444437","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951261462&doi=10.1109%2fTIC-STH.2009.5444437&partnerID=40&md5=cc2cf362cb41738541ad712316fe87f0","Dept. of Electrical and Computer Engineering, Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, ON, Canada; Dept. of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada","Eizenman, M., Dept. of Electrical and Computer Engineering, Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, ON, Canada; Model, D., Dept. of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Guestrin, E.D., Dept. of Electrical and Computer Engineering, Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, ON, Canada","Gaze estimation systems use calibration procedures that require active subject participation to estimate the point-ofgaze accurately. Consequently, these systems do not support covert monitoring of visual scanning patterns. This paper presents a novel gaze estimation methodology that does not use calibration procedures that require active user participation. This methodology uses multiple infrared light sources for illumination and a stereo pair of video cameras to obtain images of the eyes. Each pair of images is analyzed and the centers of the pupils and the centers of curvature of the corneas are estimated. These points, which are estimated without a personal calibration procedure, define the optical axis of each eye. To estimate the point-of-gaze, which lies along the visual axis, the angle between the optical and visual axes is estimated by a procedure that minimizes the distance between the intersections of the visual axes of the left and right eyes with the surface of a display while subjects look naturally at the display (e.g., watching a video clip). Simulation results demonstrate that for a subject sitting 75 cm in front of an 80 cm x 60 cm display (40"" TV) the RMS error of the estimated point-of-gaze is 17.8 mm (1.3°). ©2009 IEEE.","Calibration-free gaze estimation; Covert gaze monitoring; Remote gaze estimation","Calibration procedure; Covert gaze monitoring; Gaze estimation; Infrared light sources; Optical axis; Point of gaze; Remote gaze estimation; RMS errors; Simulation result; Stereo pair; User participation; Video clips; Visual scanning; Calibration; Estimation; Light sources; Technological forecasting; Temperature indicating cameras; Tools; Video cameras",Conference Paper,"Final","",Scopus,2-s2.0-77951261462
"Yücel Z., Salah A.A.","15847245400;7006556254;","Resolution of focus of attention using gaze direction estimation and saliency computation",2009,"Proceedings - 2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops, ACII 2009",,,"5349547","","",,6,"10.1109/ACII.2009.5349547","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949408954&doi=10.1109%2fACII.2009.5349547&partnerID=40&md5=3c54f511cab22e9764917e07b93094c2","CWI, Science Park 123, 1098 XG, Amsterdam, Netherlands; ISLA, Science Park 107, 1098 XG, Amsterdam, Netherlands","Yücel, Z., CWI, Science Park 123, 1098 XG, Amsterdam, Netherlands; Salah, A.A., ISLA, Science Park 107, 1098 XG, Amsterdam, Netherlands","Modeling the user's attention is useful for responsive and interactive systems. This paper proposes a method for establishing joint visual attention between an experimenter and an intelligent agent. A rapid procedure is described to track the 3D head pose of the experimenter, which is used to approximate the gaze direction. The head is modeled with a sparse grid of points sampled from the surface of a cylinder. We then propose to employ a bottom-up saliency model to single out interesting objects in the neighborhood of the estimated focus of attention. We report results on a series of experiments, where a human experimenter looks at objects placed at different locations of the visual field, and the proposed algorithm is used to locate target objects automatically. Our results indicate that the proposed approach achieves high localization accuracy and thus constitutes a useful tool for the construction of natural human-computer interfaces. ©2009 IEEE.","Gaze estimation; Head pose estimation; Intelligent interaction; Joint attention modeling; Saliency","3D head; Focus of Attention; Gaze direction; Gaze estimation; Head Pose Estimation; Human computer interfaces; Intelligent interactions; Interactive system; Joint attention; Localization accuracy; Rapid procedures; Sparse grid; Target object; Visual Attention; Visual fields; Algorithms; Estimation; Intelligent computing; Interfaces (computer); Motion estimation",Conference Paper,"Final","",Scopus,2-s2.0-77949408954
"Valenti R., Staiano J., Sebe N., Gevers T.","57192175392;14832068200;57204924633;7003472472;","Webcam-based visual gaze estimation",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5716 LNCS",,,"662","671",,23,"10.1007/978-3-642-04146-4_71","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76349090458&doi=10.1007%2f978-3-642-04146-4_71&partnerID=40&md5=ab05c24cfad783fc0e404ebd894e7f1d","Faculty of Science, University of Amsterdam, Netherlands; University of Trento, Italy","Valenti, R., Faculty of Science, University of Amsterdam, Netherlands; Staiano, J., Faculty of Science, University of Amsterdam, Netherlands; Sebe, N., University of Trento, Italy; Gevers, T., Faculty of Science, University of Amsterdam, Netherlands","In this paper we combine a state of the art eye center locator and a new eye corner locator into a system which estimates the visual gaze of a user in a controlled environment (e.g. sitting in front of a screen). In order to reduce to a minimum the computational costs, the eye corner locator is built upon the same technology of the eye center locator, tweaked for the specific task. If high mapping precision is not a priority of the application, we claim that the system can achieve acceptable accuracy without the requirements of additional dedicated hardware. We believe that this could bring new gaze based methodologies for human-computer interactions into the mainstream. © 2009 Springer Berlin Heidelberg.",,"Computational costs; Controlled environment; Dedicated hardware; Eye corners; Gaze estimation; Specific tasks; State of the art; Human computer interaction; Image analysis; Knowledge management; Cost reduction",Conference Paper,"Final","",Scopus,2-s2.0-76349090458
"Sumiya E., Mashita T., Kiyokawa K., Takemura H.","35325478700;14319405900;7005065755;7201655004;","A wide-view parallax-free eye-mark recorder with a hyperboloidal half-silvered mirror",2009,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,,"19","22",,1,"10.1145/1643928.1643935","https://www.scopus.com/inward/record.uri?eid=2-s2.0-74949138916&doi=10.1145%2f1643928.1643935&partnerID=40&md5=0d780d2e2e6b325bac4e2aeeea4821c0","Osaka University, Japan","Sumiya, E., Osaka University, Japan; Mashita, T., Osaka University, Japan; Kiyokawa, K., Osaka University, Japan; Takemura, H., Osaka University, Japan","In this paper, we propose a wide-view parallax-free eye-mark recorder with a hyperboloidal half-silvered mirror. Our eye-mark recorder provides a wide field-of-view (FOV) video recording of the user's exact view by positioning the focal point of the mirror at the user's viewpoint. The vertical view angle of the prototype is 122 [deg] (elevation and depression angles are 38 and 84 [deg], respectively) and its horizontal view angle is 116 [deg] (nasal and temporal view angles are 38 and 78 [deg], respectively). We have implemented and evaluated a gaze estimation method for our eyemark recorder. Experimental results have verified that our eye-mark recorder successfully captures a wide FOV of a user and estimates a rough gaze direction. Copyright © 2009 by the Association for Computing Machinery, Inc.","Eye-mark recorder; Gaze estimation; Half-silvered hyperboloidal mirror; Head-mounted camera","Focal points; Gaze direction; Gaze estimation; Half-silvered mirrors; Hyperboloidal mirrors; Wide field-of-view; Astrophysics; Cameras; Computer software; Estimation; Geometrical optics; Video recording; Virtual reality; Mirrors",Conference Paper,"Final","",Scopus,2-s2.0-74949138916
"Asteriadis S., Soufleros D., Karpouzis K., Kollias S.","55936774500;35303246600;6602860576;57193712526;","A natural head pose and eye gaze dataset",2009,"Proceedings of the International Workshop on Affective-Aware Virtual Agents and Social Robots, AFFINE '09, held during the ICMI-MLMI'09 Conference",,,"1655261","","",,38,"10.1145/1655260.1655261","https://www.scopus.com/inward/record.uri?eid=2-s2.0-74049088220&doi=10.1145%2f1655260.1655261&partnerID=40&md5=3a32b0011591054073c51419809369dc","National Technical University of Athens, School of Electrical and Computer Engineering, Image, Video and Multimedia Systems Lab., 9, Iroon Polytechniou str, Athens, Greece","Asteriadis, S., National Technical University of Athens, School of Electrical and Computer Engineering, Image, Video and Multimedia Systems Lab., 9, Iroon Polytechniou str, Athens, Greece; Soufleros, D., National Technical University of Athens, School of Electrical and Computer Engineering, Image, Video and Multimedia Systems Lab., 9, Iroon Polytechniou str, Athens, Greece; Karpouzis, K., National Technical University of Athens, School of Electrical and Computer Engineering, Image, Video and Multimedia Systems Lab., 9, Iroon Polytechniou str, Athens, Greece; Kollias, S., National Technical University of Athens, School of Electrical and Computer Engineering, Image, Video and Multimedia Systems Lab., 9, Iroon Polytechniou str, Athens, Greece","We present a new dataset, ideal for Head Pose and Eye Gaze Estimation algorithm testings. Our dataset was recorded using a monocular system, and no information regarding camera or environment parameters is offered, making the dataset ideal to be tested with algorithms that do not utilize such information and do not require any specific equipment in terms of hardware. Copyright 2009 ACM.","Facial feature tracking; Head pose; User attention estimation","Data sets; Eye-gaze; Facial feature tracking; Head pose; User attention; User attention estimation; Estimation; Intelligent robots; Motion estimation",Conference Paper,"Final","",Scopus,2-s2.0-74049088220
"Purwanto D., Mardiyanto R., Arai K.","56600944500;35280403200;7403965268;","Electric wheelchair control with gaze direction and eye blinking",2009,"Artificial Life and Robotics","14","3",,"397","400",,43,"10.1007/s10015-009-0694-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72549118588&doi=10.1007%2fs10015-009-0694-x&partnerID=40&md5=00dc2837cb76bb08d70d5e18310f8f08","Department of Electrical Engineering, Institute of Technology Sepuluh Nopember (ITS), Surabaya, Indonesia; Department of Information Science, Saga University, 1 Honjo, Saga 840-8502, Japan","Purwanto, D., Department of Electrical Engineering, Institute of Technology Sepuluh Nopember (ITS), Surabaya, Indonesia; Mardiyanto, R., Department of Electrical Engineering, Institute of Technology Sepuluh Nopember (ITS), Surabaya, Indonesia; Arai, K., Department of Information Science, Saga University, 1 Honjo, Saga 840-8502, Japan","We propose an electric wheelchair controlled by gaze direction and eye blinking. A camera is set up in front of a wheelchair user to capture image information. The sequential captured image is interpreted to obtain the gaze direction and eye blinking properties. The gaze direction is expressed by the horizontal angle of the gaze, and this is derived from the triangle formed by the centers of the eyes and the nose. The gaze direction and eye blinking are used to provide direction and timing commands, respectively. The direction command relates to the direction of movement of the electric wheelchair, and the timing command relates to the time when the wheelchair should move. The timing command with an eye blinking mechanism is designed to generate ready, backward movement, and stop commands for the electric wheelchair. Furthermore, to move at a certain velocity, the electric wheelchair also receives a velocity command as well as the direction and timing commands. The disturbance observer-based control system is used to control the direction and velocity. For safety purposes, an emergency stop is generated when the electric wheelchair user does not focus their gaze consistently in any direction for a specifi ed time. A number of simulations and experiments were conducted with the electric wheelchair in a laboratory environment. © 2009 International Symposium on Artificial Life and Robotics (ISAROB).","Blinking measurement; Electric wheelchair control; Gaze estimation","Capture images; Disturbance observer; Electric wheelchair; Emergency stop; Eye-blinking; Gaze direction; Gaze estimation; Horizontal angles; Laboratory environment; Control system analysis; Observability; Time measurement; Wheelchairs; Electric generators",Article,"Final","",Scopus,2-s2.0-72549118588
"Lablack A., Maquet F., Ihaddadene N., Djeraba C.","25923368300;6505720425;24166613900;55892468800;","Visual gaze projection in front of a target scene",2009,"Proceedings - 2009 IEEE International Conference on Multimedia and Expo, ICME 2009",,,"5202883","1839","1840",,4,"10.1109/ICME.2009.5202883","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449562990&doi=10.1109%2fICME.2009.5202883&partnerID=40&md5=8cdc251df3f5facb2f70525f1eb3d898","Laboratoire d'Informatique Fondamentale de Lille (LIFL), Université de Lille 1, France","Lablack, A., Laboratoire d'Informatique Fondamentale de Lille (LIFL), Université de Lille 1, France; Maquet, F., Laboratoire d'Informatique Fondamentale de Lille (LIFL), Université de Lille 1, France; Ihaddadene, N., Laboratoire d'Informatique Fondamentale de Lille (LIFL), Université de Lille 1, France; Djeraba, C., Laboratoire d'Informatique Fondamentale de Lille (LIFL), Université de Lille 1, France","In this technical demonstration, we present a tool that projects on a target scene the visual gaze of the people passing in front of it. The target scene could be a large plasma screen, an advertising poster, a shelf or a shop window. The projection of the visual gaze corresponds to the coordinates of a region that represents the person's location of interest in the target scene. It is an important problem with many applications that try to understand human behavior in a controlled environment such as in security or customized marketing. The visual gaze projection is influenced by the parameters of the camera, the settings of the target scene, the method used for the estimation of the gaze (e.g. the information about the head pose, the location of eyes centers and corners, or any combination of them), and the visual gaze starting point. ©2009 IEEE.","Projection; Region of interest; Visual gaze estimation","Controlled environment; Head pose; Human behaviors; Plasma screens; Projection; Region of interest; Visual gaze estimation; Behavioral research; Estimation; Multimedia systems; Targets",Conference Paper,"Final","",Scopus,2-s2.0-70449562990
"Orozco J., Roca F.X., Gonzàlez J.","57193883839;35548951300;55450715500;","Real-time gaze tracking with appearance-based models",2009,"Machine Vision and Applications","20","6",,"353","364",,15,"10.1007/s00138-008-0130-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-69949103167&doi=10.1007%2fs00138-008-0130-6&partnerID=40&md5=08b44c0a365b91ca3b9fcaf4d9e72f8e","Computer Vision Center, Dept. de Ciències de la Computaciò, Campus UAB, Bellaterra 08193, Spain; Institut de Robòtica i Informàtica Industrial (UPC-CSIC), C. Llorens i Artigas 4-6, Barcelona 08028, Spain","Orozco, J., Computer Vision Center, Dept. de Ciències de la Computaciò, Campus UAB, Bellaterra 08193, Spain; Roca, F.X., Computer Vision Center, Dept. de Ciències de la Computaciò, Campus UAB, Bellaterra 08193, Spain; Gonzàlez, J., Institut de Robòtica i Informàtica Industrial (UPC-CSIC), C. Llorens i Artigas 4-6, Barcelona 08028, Spain","Psychological evidence has emphasized the importance of eye gaze analysis in human computer interaction and emotion interpretation. To this end, current image analysis algorithms take into consideration eye-lid and iris motion detection using colour information and edge detectors. However, eye movement is fast and and hence difficult to use to obtain a precise and robust tracking. Instead, our method proposed to describe eyelid and iris movements as continuous variables using appearance-based tracking. This approach combines the strengths of adaptive appearance models, optimization methods and backtracking techniques. Thus, in the proposed method textures are learned on-line from near frontal images and illumination changes, occlusions and fast movements are managed. The method achieves real-time performance by combining two appearance-based trackers to a backtracking algorithm for eyelid estimation and another for iris estimation. These contributions represent a significant advance towards a reliable gaze motion description for HCI and expression analysis, where the strength of complementary methodologies are combined to avoid using high quality images, colour information, texture training, camera settings and other time-consuming processes. © 2008 Springer-Verlag.","Appearance models; Blinking; Eyelid and iris tracking; Iris saccade; Real-time gaze tracking","Appearance models; Blinking; Eyelid and iris tracking; Iris saccade; Real-time gaze tracking; Eye movements; Human computer interaction; Image analysis; Mobile telecommunication systems; Object recognition; Textures; Edge detection",Article,"Final","",Scopus,2-s2.0-69949103167
"Lee E.C., Park K.R.","14009024200;8983316300;","A robust eye gaze tracking method based on a virtual eyeball model",2009,"Machine Vision and Applications","20","5",,"319","337",,30,"10.1007/s00138-008-0129-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67449150094&doi=10.1007%2fs00138-008-0129-z&partnerID=40&md5=0460710d01419fc08faf2d28ca9b6e02","Department of Computer Science, Sangmyung University, 7, Hongji-dong, Jongro-gu, Seoul 110-743, South Korea; Department of Electronics Engineering, Dongguk University, 26, Pil-dong 3-ga, Jung-gu, Seoul 100-715, South Korea","Lee, E.C., Department of Computer Science, Sangmyung University, 7, Hongji-dong, Jongro-gu, Seoul 110-743, South Korea; Park, K.R., Department of Electronics Engineering, Dongguk University, 26, Pil-dong 3-ga, Jung-gu, Seoul 100-715, South Korea","Gaze positions can provide important cues for natural computer interfaces. In this paper, we describe a new gaze estimation method based on a three dimensional analysis of the human eye which can be used in head-mounted display (HMD) environments. This paper presents four advantages over previous works. First, in order to obtain accurate gaze positions, we used a virtual eyeball model based on the 3D characteristics of the human eyeball. Second, we calculated the 3D position of the virtual eyeball and gaze vector by using a camera and three collimated IR-LEDs. Third, three reference frames (the camera, the monitor and the eye reference frames) were unified, which simplified the complex 3D converting calculations and allowed for calculation of the 3D eye position and gaze position on a HMD monitor. Fourth, a simple user-dependent calibration method was proposed by gazing at one position based on Kappa compensation. Experimental results showed that the eye gaze estimation error of the proposed method was lower than 1°. © 2008 Springer-Verlag.",,"3D positions; Calibration method; Computer interfaces; Eye gaze tracking; Eye position; Eye-gaze; Gaze estimation; Head mounted displays; Human eye; Model-based; Reference frame; Three-dimensional analysis; User-dependent; Cameras; Gesture recognition; Helmet mounted displays; Interfaces (computer); Virtual reality; Three dimensional",Article,"Final","",Scopus,2-s2.0-67449150094
"Villanueva A., Daunys G., Hansen D.W., Böhme M., Cabeza R., Meyer A., Barth E.","7101612861;6507179138;15063910800;10240117000;36763933900;57198932924;35616238000;","A geometric approach to remote eye tracking",2009,"Universal Access in the Information Society","8","4",,"241","257",,14,"10.1007/s10209-009-0149-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350728895&doi=10.1007%2fs10209-009-0149-0&partnerID=40&md5=d26d80b668245976e8ae50f3b4ff3719","Electrical and Electronics Engineering Department, Public University of Navarre, Navarre, Spain; Department of Electronics, Siauliai University, Siauliai, Lithuania; Technical University of Denmark/IT University, Copenhagen, Denmark; Institute for Neuro- and Bioinformatics, University of Lübeck, Lübeck, Germany","Villanueva, A., Electrical and Electronics Engineering Department, Public University of Navarre, Navarre, Spain; Daunys, G., Department of Electronics, Siauliai University, Siauliai, Lithuania; Hansen, D.W., Technical University of Denmark/IT University, Copenhagen, Denmark; Böhme, M., Institute for Neuro- and Bioinformatics, University of Lübeck, Lübeck, Germany; Cabeza, R., Electrical and Electronics Engineering Department, Public University of Navarre, Navarre, Spain; Meyer, A., Institute for Neuro- and Bioinformatics, University of Lübeck, Lübeck, Germany; Barth, E., Institute for Neuro- and Bioinformatics, University of Lübeck, Lübeck, Germany","This paper presents a principled analysis of various combinations of image features to determine their suitability for remote eye tracking. It begins by reviewing the basic theory underlying the connection between eye image and gaze direction. Then a set of approaches is proposed based on different combinations of well-known features and their behaviour is evaluated, taking into account various additional criteria such as free head movement, and minimum hardware and calibration requirements. The paper proposes a final method based on multiple glints and the pupil centre; the method is evaluated experimentally. Future trends in eye tracking technology are also discussed. © Springer-Verlag 2009.","Eye model; Eye tracking; Gaze estimation; Geometric modelling","Analysis of various; Basic theory; Eye images; Eye model; Eye tracking; Free-head; Future trends; Gaze direction; Gaze estimation; Geometric approaches; Geometric modelling; Image features",Conference Paper,"Final","",Scopus,2-s2.0-70350728895
"Noris B., Benmachiche K., Billard A.G.","24492107300;25923230300;7006389948;","Calibration-free eye gaze direction detection with Gaussian processes",2008,"VISAPP 2008 - 3rd International Conference on Computer Vision Theory and Applications, Proceedings","2",,,"611","616",,25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57549110884&partnerID=40&md5=5ddb55b768e023e46de20c44992c953a","LASA Laboratory, EPFL, Station 9, CH-1015 Lausanne, Switzerland","Noris, B., LASA Laboratory, EPFL, Station 9, CH-1015 Lausanne, Switzerland; Benmachiche, K., LASA Laboratory, EPFL, Station 9, CH-1015 Lausanne, Switzerland; Billard, A.G., LASA Laboratory, EPFL, Station 9, CH-1015 Lausanne, Switzerland","In this paper we present a solution for eye gaze detection from a wireless head mounted camera designed for children aged between 6 months and 18 months. Due to the constraints of working with very young children, the system does not seek to be as accurate as other state-of-the-art eye trackers, however it requires no calibration process from the wearer, Gaussian Process Regression and Support Vector Machines are used to analyse the raw pixel data from the video input and return an estimate of the child's gaze direction, A confidence map is used to determine the accuracy the system can expect for each coordinate on the image. The best accuracy so far obtained by the system is 2.34° on adult subjects, tests with children remain to be done.","Appearance-based; Eye gaze detection; Gaussian processes; Wireless head mounted camera","Calibration; Cameras; Computer vision; Gaussian noise (electronic); Image processing; Image segmentation; Programming theory; Support vector machines; Trellis codes; Appearance-based; Calibration process; Eye gaze detection; Eye gazes; Eye trackers; Gaussian; Gaussian processes; Gaze directions; Wireless head mounted camera; Young children; Gaussian distribution",Conference Paper,"Final","",Scopus,2-s2.0-57549110884
"Purwanto D., Mardiyanto R., Arai K.","56600944500;35280403200;7403965268;","Robot motion control with human eye command",2008,"Proceedings of the 13th International Symposium on Artificial Life and Robotics, AROB 13th'08",,,,"111","114",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-78449263098&partnerID=40&md5=8b62f9cb84bad8c48c2274800bfe8b0d","Department of Electrical Engineering ITS, Kampus ITS, Sukolilo, Surabaya 60111, Indonesia; Department of Information Science, Saga University, 1 Honjo, Saga 840-8502, Japan","Purwanto, D., Department of Electrical Engineering ITS, Kampus ITS, Sukolilo, Surabaya 60111, Indonesia; Mardiyanto, R., Department of Electrical Engineering ITS, Kampus ITS, Sukolilo, Surabaya 60111, Indonesia; Arai, K., Department of Information Science, Saga University, 1 Honjo, Saga 840-8502, Japan","The robot motion control with human eye command is proposed in this research. The proposed system realizes a viewing system controlled by operator eye command which is able to move the camera equipped at robot arm to capture the scene from environment in accordance with position where the operator intent to see. The eye command interpretation includes the estimation of eye viewing director (eye gaze) to obtain the eye viewing position on computer screen as motion command, and eye blinking measurement to obtain the motion decision. The motion command is used to determine where the robot should move to (and the camera attached at the robot should point to). The motion decision is used to determine the starting time of robot to move to a new position. An image based visual servoing strategy with velocity limiter is applied to obtain high performance of motion and to minimize the blur effect when camera captures image. ©ISAROB 2008.","Eye tracking; Gaze estimation; Human computer interface; Machine vision; Robot control","Eye-tracking; Gaze estimation; Human computer interface; Machine vision; Robot control; Cameras; Estimation; Eye movements; Human computer interaction; Interfaces (computer); Motion control; Motion planning; Robotics; Robots; Visual servoing; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-78449263098
[无可用作者姓名],[无可用的作者 ID],"Proceedings of the Eye Tracking Research and Applications Symposium, ETRA 2008",2008,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"","",291,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950361527&partnerID=40&md5=57d5edf4498d166fe36d5eefdc1413cf",,"","The proceedings contain 49 papers. The topics discussed include: longitudinal evaluation of discrete consecutive gaze gestures for text entry; measurement of eye velocity using active illumination; a method to study visual attention aspects collaboration: eye-tracking pair programmers simultaneously; testing for statistically significant differences between groups of scan patterns; gazing with pEYEs: towards a universal input for various applications; eye typing using word and letter prediction and a fixation algorithm; 3D point-of-gaze estimation on a volumetric display; limbus/pupil switching for wearable eye tracking under variable lighting conditions; improving the accuracy of gaze input for interaction; measuring the task-evoked pupillary response with a remote eye tracker; estimation of certainty for multiple choice tasks using features of eye-movements; assessing usability with eye-movement frequency analysis; and integrated speech and gaze control for realistic desktop environments.",,,Conference Review,"Final","",Scopus,2-s2.0-77950361527
"Guestrin E.D., Eizenman M., Kang J.J., Eizenman E.","14009410300;6701402159;24721852800;24721252900;","Analysis of subject-dependent point-of-gaze estimation bias in the cross-ratios method",2008,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"237","244",,8,"10.1145/1344471.1344526","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950335294&doi=10.1145%2f1344471.1344526&partnerID=40&md5=eb3866ad27781633c94386e1a3c89e7a","University of Toronto, Canada","Guestrin, E.D., University of Toronto, Canada; Eizenman, M., University of Toronto, Canada; Kang, J.J., University of Toronto, Canada; Eizenman, E., University of Toronto, Canada","The cross-ratios method for point-of-gaze estimation uses the invariance property of cross-ratios in projective transformations. The inherent causes of the subject-dependent point-of-gaze estimation bias exhibited by this method have not been well characterized in the literature. Using a model of the eye and the components of a system (camera, light sources) that estimates point-of-gaze, a theoretical framework for the cross-ratios method is developed. The analysis of the cross-ratios method within this framework shows that the subject-dependent estimation bias is caused mainly by (i) the angular deviation of the visual axis from the optic axis and (ii) the fact that the virtual image of the pupil center is not coplanar with the virtual images of the light sources that illuminate the eye (corneal reflections). The theoretical framework provides a closed-form analytical expression that predicts the estimation bias as a function of subject-specific eye parameters. Copyright © 2008 by the Association for Computing Machinery, Inc.","Cross-ratios; Eye model; Eye parameters; Point-of-gaze; Remote gaze estimation","Cross-ratios; Eye model; Eye parameters; Point of gaze; Remote gaze estimation; Light; Light sources; Parameter estimation",Conference Paper,"Final","",Scopus,2-s2.0-77950335294
"Chen J., Tong Y., Gray W., Ji Q.","22633570700;8644533500;7201707411;18935108400;","A robust 3D eye gaze tracking system using noise reduction",2008,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"189","196",,49,"10.1145/1344471.1344518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950329920&doi=10.1145%2f1344471.1344518&partnerID=40&md5=a48b3b4262acff1ab6cd2247065f2af9","Rensselaer Polytechnic Institute, Troy, NY 12180-3590, United States","Chen, J., Rensselaer Polytechnic Institute, Troy, NY 12180-3590, United States; Tong, Y., Rensselaer Polytechnic Institute, Troy, NY 12180-3590, United States; Gray, W., Rensselaer Polytechnic Institute, Troy, NY 12180-3590, United States; Ji, Q., Rensselaer Polytechnic Institute, Troy, NY 12180-3590, United States","This paper describes a novel real-time 3D gaze estimation system. The system consists of two cameras and two IR light sources. There are three novelties in this method. First, in our system, two IR lights are mounted near the centers of the stereo cameras, respectively. Based on this specific configuration, the 3D position of the corneal center can be simply derived by the 3D reconstruction technique. Then, after extracting the 3D position of the ""virtual pupil"" correctly, the optical axis of the eye can be obtained directly by connecting the ""virtual pupil"" with the corneal center. Second, we systematically analyze the noise in our 3D gaze estimation algorithm and propose an effective constraint to reduce this noise. Third, to estimate the user-dependent parameters (i.e. the constraint parameters and the eye parameters), a simple calibration method is proposed by gazing at four positions on the screen. Experimental results show that our system can accurately estimate and track eye gaze under natural head movement. Copyright © 2008 by the Association for Computing Machinery, Inc.","Gaze estimation; Noise reduction; Stereo cameras","3D positions; 3D reconstruction; Calibration method; Eye gaze tracking; Eye parameters; Eye-gaze; Gaze estimation; Head movements; Noise reductions; Optical axis; Stereo cameras; User-dependent; Acoustic noise measurement; Cameras; Estimation; Gesture recognition; Image reconstruction; Light; Light sources; Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-77950329920
"Guestrin E.D., Eizenman M.","14009410300;6701402159;","Remote point-of-gaze estimation requiring a single-point calibration for applications with infants",2008,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"267","274",,48,"10.1145/1344471.1344531","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950329250&doi=10.1145%2f1344471.1344531&partnerID=40&md5=05c0747a074db61fbcf762fac3635480","University of Toronto, Canada","Guestrin, E.D., University of Toronto, Canada; Eizenman, M., University of Toronto, Canada","This paper describes a method for remote, non-contact point-of-gaze estimation that tolerates free head movements and requires a simple calibration procedure in which the subject has to fixate only on a single point. This method uses the centers of the pupil and at least two corneal reflections that are estimated from eye images captured by at least two cameras. Experimental results obtained with three adult subjects exhibited RMS point-of-gaze estimation errors ranging from 7 to 12 mm (equivalent to about 0.6 - 1° of visual angle) for head movements in a volume of about 1 dm3. Preliminary results with two infants demonstrated the ability of a system that requires a single-point calibration procedure to estimate infants' point-of-gaze. The ability to record infants' visual scanning behavior can be used for the study of visual development, the determination of attention allocation and the assessment of visual function in preverbal infants. Copyright © 2008 by the Association for Computing Machinery, Inc.","Infants' gaze; Minimal subject cooperation; Point-of-gaze; Remote gaze estimation; Single-point calibration","Calibration procedure; Corneal reflection; Eye images; Free-head; Head movements; Minimal subject cooperation; Non-contact; Point of gaze; Remote gaze estimation; Single point; Visual angle; Visual development; Visual functions; Visual scanning; Estimation; Calibration",Conference Paper,"Final","",Scopus,2-s2.0-77950329250
"Hennessey C., Lawrence P.","23992499900;36729888200;","3D point-of-gaze estimation on a volumetric display",2008,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"59","",,12,"10.1145/1344471.1344486","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950312984&doi=10.1145%2f1344471.1344486&partnerID=40&md5=46cc62dea6fa751c3707e76396303f73","University of British Columbia, Canada","Hennessey, C., University of British Columbia, Canada; Lawrence, P., University of British Columbia, Canada","Eye-gaze tracking devices are typically used to estimate the point-of-gaze (POG) of a subject on a 2D surface such as a computer screen. Using model based methods for POG estimation we have developed a system based on the vergence of the eyes which can be used to estimate the POG on a real-world volumetric display. Copyright © 2008 by the Association for Computing Machinery, Inc.",,"Computer screens; Eye-gaze; Model-based method; Point of gaze; Real-world; System-based; Vergences; Volumetric display; Two dimensional",Conference Paper,"Final","",Scopus,2-s2.0-77950312984
"Yamazoe H., Utsumi A., Yonezawa T., Abe S.","8517824300;7005241177;35773010300;55726116200;","Remote gaze estimation with a single camera based on facial-feature tracking without special calibration actions",2008,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"245","250",,90,"10.1145/1344471.1344527","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949587464&doi=10.1145%2f1344471.1344527&partnerID=40&md5=c6c54686275c7b7a861c9a1d41d8c235","ATR Intelligent Robotics and Communication Laboratories, Japan","Yamazoe, H., ATR Intelligent Robotics and Communication Laboratories, Japan; Utsumi, A., ATR Intelligent Robotics and Communication Laboratories, Japan; Yonezawa, T., ATR Intelligent Robotics and Communication Laboratories, Japan; Abe, S., ATR Intelligent Robotics and Communication Laboratories, Japan","We propose a real-time gaze estimation method based on facial-feature tracking using a single video camera that does not require any special user action for calibration. Many gaze estimation methods have been already proposed; however, most conventional gaze tracking algorithms can only be applied to experimental environments due to their complex calibration procedures and lacking of usability. In this paper, we propose a gaze estimation method that can apply to daily-life situations. Gaze directions are determined as 3D vectors connecting both the eyeball and the iris centers. Since the eyeball center and radius cannot be directly observed from images, the geometrical relationship between the eyeball centers and the facial features and eyeball radius (face/eye model) are calculated in advance. Then, the 2D positions of the eyeball centers can be determined by tracking the facial features. While conventional methods require instructing users to perform such special actions as looking at several reference points in the calibration process, the proposed method does not require such special calibration action of users and can be realized by combining 3D eye-model-based gaze estimation and circle-based algorithms for eye-model calibration. Experimental results show that the gaze estimation accuracy of the proposed method is 5° horizontally and 7° vertically. With our proposed method, various application such as gaze-communication robots, gaze-based interactive signboards, etc. that require gaze information in daily-life situations are possible. Copyright © 2008 by the Association for Computing Machinery, Inc.","Daily-life situations; Non-intrusive; Remote gaze tracking","3D vectors; Calibration procedure; Calibration process; Communication robot; Conventional methods; Daily-life situations; Experimental environment; Facial feature; Feature-tracking; Gaze direction; Gaze estimation; Gaze tracking; Geometrical relationship; Model calibration; Model-based; Non-intrusive; Reference points; Remote gaze estimation; Single cameras; User action; Calibration; Cameras; Estimation; Three dimensional; Video cameras; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-77949587464
"Böhme M., Dorr M., Graw M., Martinetz T., Barth E.","10240117000;10244404800;35772398800;6701443660;35616238000;","A software framework for simulating eye trackers",2008,"Eye Tracking Research and Applications Symposium (ETRA)",,,,"251","258",,33,"10.1145/1344471.1344529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650847143&doi=10.1145%2f1344471.1344529&partnerID=40&md5=7a8c7ff7811d1d98722b9254569c02a0","Institute for Neuro- and Bioinformatics, University of Lübeck, Germany","Böhme, M., Institute for Neuro- and Bioinformatics, University of Lübeck, Germany; Dorr, M., Institute for Neuro- and Bioinformatics, University of Lübeck, Germany; Graw, M., Institute for Neuro- and Bioinformatics, University of Lübeck, Germany; Martinetz, T., Institute for Neuro- and Bioinformatics, University of Lübeck, Germany; Barth, E., Institute for Neuro- and Bioinformatics, University of Lübeck, Germany","We describe an open-source software framework that simulates the measurements made using one or several cameras in a video-oculographic eye tracker. The framework can be used to compare objectively the performance of different eye tracking setups (number and placement of cameras and light sources) and gaze estimation algorithms. We demonstrate the utility of the framework by using it to compare two remote eye tracking methods, one using a single camera, the other using two cameras. Copyright © 2008 by the Association for Computing Machinery, Inc.","Eye tracking; Gaze estimation; Optical simulation","Eye trackers; Eye tracking methods; Eye-tracking; Gaze estimation; Open-source softwares; Optical simulation; Single cameras; Software frameworks; Computer software; Estimation; Eye controlled devices; Light; Light sources; Cameras",Conference Paper,"Final","",Scopus,2-s2.0-67650847143
"Kinoshita K., Lao S.","16202883200;7004254253;","A fast and robust 3D head pose and gaze estimation system",2008,"2008 8th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2008",,,"4813466","","",,,"10.1109/AFGR.2008.4813466","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650677306&doi=10.1109%2fAFGR.2008.4813466&partnerID=40&md5=719502ef9d8a095b5dcec8ff604cce3c","Core Technology Center, OMRON Corporation, 9-1, Kizugawadai, Kizugawa-city, Kyoto 619-0283, Japan","Kinoshita, K., Core Technology Center, OMRON Corporation, 9-1, Kizugawadai, Kizugawa-city, Kyoto 619-0283, Japan; Lao, S., Core Technology Center, OMRON Corporation, 9-1, Kizugawadai, Kizugawa-city, Kyoto 619-0283, Japan","We developed a fast and robust head pose and gaze estimation system. This system can detect facial feature points and estimate 3D pose angles and gaze direction in various conditions including changes in facial expression, partial occlusion, etc. The system needs only one face image as input and doesn't need any special devices such as blinking LED or stereo camera. Moreover, no calibration process is needed. It shows 95% of head pose estimation accuracy and 81% of gaze estimation accuracy (when the error margin is 15 degrees). The processing time is approximately 15ms/frame (Pentium4 3.2GHz). Acceptable range of facial pose is within +/- 60 degrees in yaw (left-right) and within +/- 30 degrees in pitch (up-down). © 2008 IEEE.",,"3D head; Calibration process; Error margins; Face images; Facial Expressions; Facial feature points; Gaze direction; Gaze estimation; Head pose; Head Pose Estimation; Partial occlusions; Processing Time; Special devices; Stereo cameras; Estimation; Face recognition; Three dimensional; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-67650677306
"Magee J.J., Betke M., Gips J., Scott M.R., Waber B.N.","13605734900;7004214113;6603961354;26638464500;13604779700;","A human-computer interface using symmetry between eyes to detect gaze direction",2008,"IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans","38","6",,"1248","1261",,46,"10.1109/TSMCA.2008.2003466","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56449100551&doi=10.1109%2fTSMCA.2008.2003466&partnerID=40&md5=d8b5bd226ae1c964d7351889bb5ee4d2","Computer Science Department, Boston University, Boston, MA 02215, United States; Harvard Medical School, Boston, MA 02115, United States; Information Systems Department, Boston College, Chestnut Hill, MA 02467, United States; Microsoft, Redmond, WA 98052-6399, United States; The Media Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139-4307, United States","Magee, J.J., Computer Science Department, Boston University, Boston, MA 02215, United States; Betke, M., Computer Science Department, Boston University, Boston, MA 02215, United States, Harvard Medical School, Boston, MA 02115, United States; Gips, J., Information Systems Department, Boston College, Chestnut Hill, MA 02467, United States; Scott, M.R., Microsoft, Redmond, WA 98052-6399, United States; Waber, B.N., The Media Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139-4307, United States","In the cases of paralysis so severe that a person's ability to control movement is limited to the muscles around the eyes, eye movements or blinks are the only way for the person to communicate. Interfaces that assist in such communication are often intrusive, require special hardware, or rely on active infrared illumination. A nonintrusive communication interface system called EyeKeys was therefore developed, which runs on a consumer-grade computer with video input from an inexpensive Universal Serial Bus camera and works without special lighting. The system detects and tracks the person's face using multiscale template correlation. The symmetry between left and right eyes is exploited to detect if the person is looking at the camera or to the left or right side. The detected eye direction can then be used to control applications such as spelling programs or games. The game ""BlockEscape""was developed to evaluate the performance of EyeKeys and compare it to a mouse substitution interface. Experiments with EyeKeys have shown that it is an easily used computer input and control device for able-bodied people and has the potential to become a practical tool for people with severe paralysis. © 2008 IEEE.","Assistive technology; Disabled computer users; Face detection; Face tracking; Gaze estimation; Video-based human-computer interfaces; Webcams","Cameras; Concurrency control; Eye movements; Face recognition; Game theory; Human computer interaction; Assistive technology; Disabled computer users; Face detection; Face tracking; Gaze estimation; Video-based human-computer interfaces; Webcams; Interfaces (computer)",Article,"Final","",Scopus,2-s2.0-56449100551
"Yamazoe H., Utsumi A., Yonezawa T., Abe S.","8517824300;7005241177;35773010300;55726116200;","Remote and head-motion-free gaze tracking for real environments with automated head-eye model calibrations",2008,"2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops",,,"4563184","","",,20,"10.1109/CVPRW.2008.4563184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-51849148445&doi=10.1109%2fCVPRW.2008.4563184&partnerID=40&md5=4c17eaa03dbf39f7f93fe5a1963c0e73","ATR Intelligent Robotics and Communication Laboratories, 2-2-2 Hikaridal, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan","Yamazoe, H., ATR Intelligent Robotics and Communication Laboratories, 2-2-2 Hikaridal, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan; Utsumi, A., ATR Intelligent Robotics and Communication Laboratories, 2-2-2 Hikaridal, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan; Yonezawa, T., ATR Intelligent Robotics and Communication Laboratories, 2-2-2 Hikaridal, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan; Abe, S., ATR Intelligent Robotics and Communication Laboratories, 2-2-2 Hikaridal, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan","We propose a gaze estimation method that substantially relaxes the practical constraints possessed by most conventional methods. Gaze estimation research has a long history, and many systems including some commercial schemes have been proposed. However, the application domain of gaze estimation is still limited (e.g, measurement devices for HCI issues, input devices for VDT works) due to the limitations of such systems. First, users must be close to the system (or must wear it) since most systems employ IR illumination and/or stereo cameras. Second, users are required to perform manual calibrations to get geometrically meaningful data. These limitations prevent applications of the system that capture and utilize useful human gaze information in daily situations. In our method, inspired by a bundled adjustment framework, the parameters of the 3D head-eye model are robustly estimated by minimizing pixelwise re-projection errors between single-camera input images and eye model projections for multiple frames with adjacently estimated head poses. Since this process runs automatically, users does not need to be aware of it. Using the estimated parameters, 3D head poses and gaze directions for newly observed images can be directly determined with the same error minimization manner. This mechanism enables robust gaze estimation with single-camera-based low resolution images without user-aware preparation tasks (i.e., calibration). Experimental results show the proposed method achieves 6° accuracy with QVGA (320 × 240) images. The proposed algorithm is free from observation distances. We confirmed that our system works with longdistance observations (10 meters). © 2008 IEEE.",,"Artificial intelligence; Calibration; Cameras; Computational methods; Computer vision; Electric currents; Error analysis; Estimation; Face recognition; Feature extraction; Image enhancement; Image processing; Modal analysis; Parameter estimation; Pattern recognition; Three dimensional; 3D head; Application domains; Conventional methods; Error minimization; Estimated parameters; Eye model; Gaze estimation; Gaze tracking; Head poses; Input devices; Low-resolution images; Measurement devices; Multiple frames; Real environments; Single cameras; Stereo cameras; Computer systems",Conference Paper,"Final","",Scopus,2-s2.0-51849148445
"Ki J., Kwon Y.-M.","24483713600;7403459357;","3D gaze estimation and interaction",2008,"2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, 3DTV-CON 2008 Proceedings",,,"4547886","373","376",,13,"10.1109/3DTV.2008.4547886","https://www.scopus.com/inward/record.uri?eid=2-s2.0-51149120604&doi=10.1109%2f3DTV.2008.4547886&partnerID=40&md5=dfd221541b556bb867cb2cbfb87d3986","Imaging Media Research Center, Korea Institute of Science and Technology, Seoul, South Korea","Ki, J., Imaging Media Research Center, Korea Institute of Science and Technology, Seoul, South Korea; Kwon, Y.-M., Imaging Media Research Center, Korea Institute of Science and Technology, Seoul, South Korea","There are several researches on 2D gaze tracking techniques for the 2D screen for the human-computer interaction. However, the researches for gaze-based interaction to the 3D stereo images or contents are not reported. The 3D display techniques are emerging now for the reality service. Moreover, the 3D interaction techniques are much more needed in the 3D contents service environments. This paper addresses gaze-based 3D interaction techniques on autostereoscopic display, such as parallax barrier or lenticular display. This paper presents our researches on 3D gaze estimation and gaze-based interaction to autostereoscopic display. The evaluation of our system is shown in terms of accuracy in gaze direction and gaze depth. ©2008 IEEE.","3D gaze; 3D interaction; Autostereoscopic display; Pupil center distance (PCD)","Astrophysics; Computer networks; Digital television; Flow interactions; Geometrical optics; Information management; Knowledge management; Stereo vision; Television broadcasting; 3D gaze; 3D interaction; Autostereoscopic display; Autostereoscopic displays; Pupil center distance (PCD); Three dimensional",Conference Paper,"Final","",Scopus,2-s2.0-51149120604
"Villanueva A., Cabeza R.","7101612861;36763933900;","A novel gaze estimation system with one calibration point",2008,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics","38","4",,"1123","1138",,134,"10.1109/TSMCB.2008.926606","https://www.scopus.com/inward/record.uri?eid=2-s2.0-49049095059&doi=10.1109%2fTSMCB.2008.926606&partnerID=40&md5=14f3abe8ad6b138d3a9e46bbd7c56ea1","Electrical and Electronic Engineering Department, Public University of Navarra, 31006 Pamplona, Spain","Villanueva, A., Electrical and Electronic Engineering Department, Public University of Navarra, 31006 Pamplona, Spain; Cabeza, R., Electrical and Electronic Engineering Department, Public University of Navarra, 31006 Pamplona, Spain","The design of robust and high-performance gaze-tracking systems is one of the most important objectives of the eye-tracking community. In general, a subject calibration procedure is needed to learn system parameters and be able to estimate the gaze direction accurately. In this paper, we attempt to determine if subject calibration can be eliminated. A geometric analysis of a gaze-tracking system is conducted to determine user calibration requirements. The eye model used considers the offset between optical and visual axes, the refraction of the cornea, and Donder's law. This paper demonstrates the minimal number of cameras, light sources, and user calibration points needed to solve for gaze estimation. The underlying geometric model is based on glint positions and pupil ellipse in the image, and the minimal hardware needed for this model is one camera and multiple light-emitting diodes. This paper proves that subject calibration is compulsory for correct gaze estimation and proposes a model based on a single point for subject calibration. The experiments carried out show that, although two glints and one calibration point are sufficient to perform gaze estimation (error ∼1°), using more light sources and calibration points can result in lower average errors. © 2008 IEEE.","Calibration; Gaze estimation; Line of sight (LOS); Point of regard (POR); Video oculography","Cameras; Control theory; Estimation; Feedback control; Gesture recognition; Light; Light emitting diodes; Light sources; Lighting; Mathematical programming; Modal analysis; Parameter estimation; Reinforcement; Reinforcement learning; Robust control; Systems engineering; Calibration points; Calibration procedures; Eye model; Eye-tracking; Gaze direction; Gaze estimation; Gaze estimation system; Geometric analysis; Geometric modelling; Line of sight (LOS); Lower average; Model based; One camera; Optical-; Point of regard (POR); Single point; System parameters; Tracking systems; Video oculography; Calibration; article; artificial intelligence; automated pattern recognition; biological model; body posture; computer assisted diagnosis; computer simulation; computer system; eye fixation; human; methodology; physiology; pupil; Artificial Intelligence; Computer Simulation; Computer Systems; Fixation, Ocular; Humans; Image Interpretation, Computer-Assisted; Models, Biological; Pattern Recognition, Automated; Posture; Pupil",Article,"Final","",Scopus,2-s2.0-49049095059
"Chen J., Ji Q.","22633570700;18935108400;","3D gaze estimation with a single camera without IR illumination",2008,"Proceedings - International Conference on Pattern Recognition",,,"4761343","","",,75,"10.1109/icpr.2008.4761343","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957955652&doi=10.1109%2ficpr.2008.4761343&partnerID=40&md5=3e5e8eafad83291d625080aefe20ba3d","Rensselaer Polytechnic Institute, Troy, NY, 12180-3590, United States","Chen, J., Rensselaer Polytechnic Institute, Troy, NY, 12180-3590, United States; Ji, Q., Rensselaer Polytechnic Institute, Troy, NY, 12180-3590, United States","This paper proposes a 3D eye gaze estimation and tracking algorithm based on facial feature tracking using a single camera. Instead of using the infrared(IR) lights and the corneal reflections (glint), this algorithm estimates the 3D visual axis using the tracked facial feature points. For this, we first introduce an extended 3D eye model which includes both the eyeball and the eye-corners. Based on this eye model, we derive the equations to solve for the 3D eyeball center, the 3D pupil center and the 3D visual axis, from which we can solve for the point of gaze after a one-time personal calibration. The experimental results show the accuracy of this algorithm is less than 3°. Compared with the existing IR-based eye tracking methods, the proposed method is simple to setup and can work both indoor and outdoor. © 2008 IEEE.",,"3D modeling; Cameras; Pattern recognition; Corneal reflection; Eye tracking methods; Facial feature points; Facial feature tracking; Gaze estimation; Infrared light; Pupil centers; Single cameras; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-77957955652
"Iqbal N., Lee S.-Y.","36848293500;15033748300;","A study on human gaze estimation using screen reflection",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5326 LNCS",,,"104","111",,3,"10.1007/978-3-540-88906-9_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049121416&doi=10.1007%2f978-3-540-88906-9_14&partnerID=40&md5=e99a8a4e739bd5c1e9b520ebae0cbcb7","Computational NeuroSystems Lab., Department of Bio and Brain Engineering, KAIST, Daejeon 305-701, South Korea","Iqbal, N., Computational NeuroSystems Lab., Department of Bio and Brain Engineering, KAIST, Daejeon 305-701, South Korea; Lee, S.-Y., Computational NeuroSystems Lab., Department of Bio and Brain Engineering, KAIST, Daejeon 305-701, South Korea","Many eye gaze systems use special infrared (IR) illuminator and choose IR-sensitive CCD camera to estimate eye gaze. The IR based system has the limitation of inaccurate gaze detection in ambient natural light and the number of IR illuminator and their particular location has also effect on gaze detection. In this paper, we present a eye gaze detection method based on computer screen illumination as light emitting source and choose high speed camera for image acquisition. In order to capture the periodic flicker patterns of monitor screen the camera is operated on frame rate greater than twice of the screen refresh rate. The screen illumination produced a mark on the corneal surface of the subject's eye as screen-glint. The screen reflection information has two fold advantages. First, we can utilize the screen reflection as screen-glint, which is very useful to determine where eye is gazing. Secondly the screen-glint information utilize to localized eye in face image. The direction of the user's eye gaze can be determined through polynomial calibration function from the relative position of the center of iris and screen-glint in both eyes. The results showed that our propose configuration could be used for gaze detection method and this will lead to increased gaze detection role for the next generation of human computer interfaces. © 2008 Springer Berlin Heidelberg.","Gaze estimation; Human computer interaction; Screen reflection; Screen-glint","CCD cameras; Chemical detection; High speed cameras; Light emission; Calibration functions; Computer screen illumination; Eye gaze detection; Gaze detection; Gaze estimation; Human computer interfaces; Light emitting sources; Relative positions; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-58049121416
"Sugano Y., Matsushita Y., Sato Y., Koike H.","7005470045;35956654700;35230954300;7202758799;","An incremental learning method for unconstrained gaze estimation",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5304 LNCS","PART 3",,"656","667",,75,"10.1007/978-3-540-88690-7_49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749174729&doi=10.1007%2f978-3-540-88690-7_49&partnerID=40&md5=8b4aeefa41bf951992c86892c33ab64c","University of Tokyo, Tokyo, Japan; Microsoft Research Asia, Beijing, China; University of Electro-Communications, Tokyo, Japan","Sugano, Y., University of Tokyo, Tokyo, Japan; Matsushita, Y., Microsoft Research Asia, Beijing, China; Sato, Y., University of Tokyo, Tokyo, Japan; Koike, H., University of Electro-Communications, Tokyo, Japan","This paper presents an online learning algorithm for appea- rance-based gaze estimation that allows free head movement in a casual desktop environment. Our method avoids the lengthy calibration stage using an incremental learning approach. Our system keeps running as a background process on the desktop PC and continuously updates the estimation parameters by taking user's operations on the PC monitor as input. To handle free head movement of a user, we propose a pose-based clustering approach that efficiently extends an appearance manifold model to handle the large variations of the head pose. The effectiveness of the proposed method is validated by quantitative performance evaluation with three users. © 2008 Springer Berlin Heidelberg.",,"Computer vision; Learning systems; Appearance manifolds; Based clustering; Desktop environment; Estimation parameters; Gaze estimation; Incremental learning; Online learning algorithms; PC monitors; Learning algorithms",Conference Paper,"Final","",Scopus,2-s2.0-56749174729
"Tsukizawa S., Oka K., Maruya K., Sato Y.","6507749852;7201489838;35174976900;35230954300;","Eye-gaze estimation for driver monitoring",2007,"14th World Congress on Intelligent Transport Systems, ITS 2007","4",,,"2994","3001",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893702609&partnerID=40&md5=4bbaf4bd76e228d8dd4b9aa9947ed1d0","Matsushita Electric Industrial Co., Ltd, 600 Saedo-cho, Tsuzuki-ku, Yokohama, 224-8539, Japan; Institute of Industrial Science, The University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo, 153-8505, Japan","Tsukizawa, S., Matsushita Electric Industrial Co., Ltd, 600 Saedo-cho, Tsuzuki-ku, Yokohama, 224-8539, Japan; Oka, K., Matsushita Electric Industrial Co., Ltd, 600 Saedo-cho, Tsuzuki-ku, Yokohama, 224-8539, Japan; Maruya, K., Matsushita Electric Industrial Co., Ltd, 600 Saedo-cho, Tsuzuki-ku, Yokohama, 224-8539, Japan; Sato, Y., Institute of Industrial Science, The University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo, 153-8505, Japan","In order to develop a system that can avoid traffic accidents caused by a driver's inattentive driving, we propose an eye-gaze estimation method for determining whether the driver is focusing on driving tasks. Our gaze estimation method, built on the top of a stereo-based 3D face tracking system, is capable of estimating a horizontal direction of a driver's gaze with high accuracy. Unlike most of existing eye-gaze estimation techniques, our method is calibration-free , i.e., no calibration step is needed to determine parameters for each driver. The key is the two assumptions that the driver's face is symmetrical, and that the driver's two eyes are looking in the same direction. In our method, a 3D correct face shape is obtained using a stereo sensor, and the gaze direction is estimated by computing the symmetry plane of the face and the midpoint of the pupils. In this paper, we describe the details of our method and show the results of experiments conducted for evaluating the estimation accuracy of our method.",,"3D face tracking; Driver monitoring; Estimation methods; Estimation techniques; Gaze direction; Gaze estimation; Stereo sensors; Symmetry planes; Calibration; Three dimensional; Traffic control; Estimation",Conference Paper,"Final","",Scopus,2-s2.0-84893702609
"Guestrin E.D., Eizenman M.","14009410300;6701402159;","Remote point-of-gaze estimation with free head movements requiring a single-point calibration",2007,"Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings",,,"4353353","4556","4560",,31,"10.1109/IEMBS.2007.4353353","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57649177788&doi=10.1109%2fIEMBS.2007.4353353&partnerID=40&md5=c0313c72888556ce9f0227ecd597a2f6","IEEE; Department of Electrical and Computer Engineering, Institute of Biomaterials and Biomedical Engineering, University of Toronto, 164 College Street, Toronto, Ont. M5S 3G9, Canada; Departments of Electrical and Computer Engineering and Ophthalmology, Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, Ont. M5S 3G9, Canada","Guestrin, E.D., IEEE, Department of Electrical and Computer Engineering, Institute of Biomaterials and Biomedical Engineering, University of Toronto, 164 College Street, Toronto, Ont. M5S 3G9, Canada; Eizenman, M., Departments of Electrical and Computer Engineering and Ophthalmology, Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, Ont. M5S 3G9, Canada","This paper describes a method for remote, non-contact point-of-gaze (POG) estimation that tolerates free head movements and requires a simple calibration procedure in which the subject has to fixate only on a single point. This method uses the centers of the pupil and at least two corneal reflections (virtual images of light sources) that are estimated from eye images captured by at least two cameras. Experimental results obtained with a prototype system that tolerates head movements in a volume of about 1 dm, exhibited RMS POG estimation errors of approximately 0.6-1° of visual angle. This system can enable applications with infants that, otherwise, would not be possible with existing POG estimation methods, which typically require multiple-point calibration procedures. © 2007 IEEE.",,"Calibration; Eye movements; Image acquisition; Light reflection; Measurement errors; Remote sensing; Eye images; Point of gaze estimation; Virtual images; Imaging systems; article; calibration; head; human; locomotion; methodology; movement (physiology); pupil; telemetry; theoretical model; Calibration; Head; Humans; Locomotion; Models, Theoretical; Movement; Pupil; Telemetry",Conference Paper,"Final","",Scopus,2-s2.0-57649177788
"Ki J., Kwon Y.-M., Sohn K.","24483713600;7403459357;7101931558;","3D gaze tracking and analysis for attentive human computer interaction",2007,"Proceedings of the Frontiers in the Convergence of Bioscience and Information Technologies, FBIT 2007",,,"4524177","617","621",,8,"10.1109/FBIT.2007.15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-49449109868&doi=10.1109%2fFBIT.2007.15&partnerID=40&md5=2c9feb57a1b51cd63acac8fa90d4b6a6","Imaging Media Research Center, Korea Institute of Science and Technology, South Korea; Department of Electrical and Electronic Engineering, Yonsei University, South Korea","Ki, J., Imaging Media Research Center, Korea Institute of Science and Technology, South Korea, Department of Electrical and Electronic Engineering, Yonsei University, South Korea; Kwon, Y.-M., Imaging Media Research Center, Korea Institute of Science and Technology, South Korea; Sohn, K., Department of Electrical and Electronic Engineering, Yonsei University, South Korea","There are several researches on 2D gaze tracking techniques for the 2D screen for the Human-Computer Interaction. However, the researches for the gaze-based interaction to the stereo images or contents are not reported. The 3D display techniques are emerging now for the reality service. Moreover, the 3D interaction techniques are much more needed in the 3D contents service environments. This paper addresses gaze-based 3D interaction techniques on stereo display, such as parallax barrier or lenticular stereo display. This paper presents our researches on 3D gaze estimation and gaze-based interaction to stereo display. © 2007 IEEE.",,"Astrophysics; Flow interactions; Geometrical optics; Human engineering; Information management; Information technology; Knowledge management; Stereo vision; Technology; Three dimensional; 3-D displays; 3D interactions; Gaze estimation; Gaze tracking; Gaze-based interaction; Paper addresses; Parallax barriers; Stereo displays; Stereo imaging; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-49449109868
"Sun X., Xu L., Yang J.","55727844100;57199909195;12773171100;","Driver fatigue alarm based on eye detection and gaze estimation",2007,"Proceedings of SPIE - The International Society for Optical Engineering","6786",,"678612","","",,6,"10.1117/12.747671","https://www.scopus.com/inward/record.uri?eid=2-s2.0-42549135586&doi=10.1117%2f12.747671&partnerID=40&md5=a3199aba5b6647c0d2efa61ce716958f","School of Computer, Nanjing University of Science and Technology, Nanjing 210094, China","Sun, X., School of Computer, Nanjing University of Science and Technology, Nanjing 210094, China; Xu, L., School of Computer, Nanjing University of Science and Technology, Nanjing 210094, China; Yang, J., School of Computer, Nanjing University of Science and Technology, Nanjing 210094, China","The driver assistant system has attracted much attention as an essential component of intelligent transportation systems. One task of driver assistant system is to prevent the drivers from fatigue. For the fatigue detection it is natural that the information about eyes should be utilized. The driver fatigue can be divided into two types, one is the sleep with eyes close and another is the sleep with eyes open. Considering that the fatigue detection is related with the prior knowledge and probabilistic statistics, the dynamic Bayesian network is used as the analysis tool to perform the reasoning of fatigue. Two kinds of experiments are performed to verify the system effectiveness, one is based on the video got from the laboratory and another is based on the video got from the real driving situation. Ten persons participate in the test and the experimental result is that, in the laboratory all the fatigue events can be detected, and in the practical vehicle the detection ratio is about 85%. Experiments show that in most of situations the proposed system works and the corresponding performance is satisfying.","Bayesian network; Eye detection; Fatigue alarm; Gaze estimation; Iris tracking","Bayesian networks; Intelligent systems; Statistics; Systems analysis; Eye detection; Fatigue alarm; Gaze estimation; Iris tracking; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-42549135586
[无可用作者姓名],[无可用的作者 ID],"Human - Computer Interaction: IEEE International Workshop, HCI 2007 Proceedings",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4796 LNCS",,,"","",163,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149075098&partnerID=40&md5=36b3bcf01eb007799f80ccf0edf1797c",,"","The proceedings contain 16 papers. The topics discussed include: drowsy driver detection through facial movement analysis; an artificial imagination for interactive search; non-intrusive physiological monitoring for automated stress detection in human-computer interaction; vision-based projected tabletop interface for finger interactions; a system for hybrid vision- and sound-based interaction with distal and proximal targets on wall-sized, high-resolution tiled displays; real-time automatic kinematic model building for optical motion capture using a Markov random field; interactive feedback for video tracking using a hybrid maximum likelihood similarity measure; large lexicon detection of sign language; multiple cue integrated action detection; combined support vector machines and hidden Markov models for modeling facial action temporal dynamics; and pose and gaze estimation in multi-camera networks for non-restrictive HCI.",,"Digital cameras; Markov processes; Mathematical models; User interfaces; Artificial imagination; Physiological monitoring; Video tracking; Human computer interaction",Conference Review,"Final","",Scopus,2-s2.0-38149075098
"Hnatow J., Savakis A.","22950661900;6603719841;","An eye model for uncalibrated eye gaze estimation under variable head pose",2007,"Proceedings of SPIE - The International Society for Optical Engineering","6539",,"65390Q","","",,,"10.1117/12.720834","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35648983589&doi=10.1117%2f12.720834&partnerID=40&md5=59d3f6696aaa7d3fd97839ea111202df","Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY 14623, United States","Hnatow, J., Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY 14623, United States; Savakis, A., Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY 14623, United States","Gaze estimation is an important component of computer vision systems that monitor human activity for surveillance, human-computer interaction, and various other applications including iris recognition. Gaze estimation methods are particularly valuable when they are non-intrusive, do not require calibration, and generalize well across users. This paper presents a novel eye model that is employed for efficiently performing uncalibrated eye gaze estimation. The proposed eye model was constructed from a geometric simplification of the eye and anthropometric data about eye feature sizes in order to circumvent the requirement of calibration procedures for each individual user. The positions of the two eye corners and the midpupil, the distance between the two eye corners, and the radius of the eye sphere are required for gaze angle calculation. The locations of the eye corners and midpupil are estimated via processing following eye detection, and the remaining parameters are obtained from anthropometric data. This eye model is easily extended to estimating eye gaze under variable head pose. The eye model was tested on still images of subjects at frontal pose (0°) and side pose (34°). An upper bound of the model's performance was obtained by manually selecting the eye feature locations. The resulting average absolute error was 2.98° for frontal pose and 2.87° for side pose. The error was consistent across subjects, which indicates that good generalization was obtained. This level of performance compares well with other gaze estimation systems that utilize a calibration procedure to measure eye features.",,"Gaze estimation; Geometric simplification; Calibration; Error analysis; Geometry; Gesture recognition; Human computer interaction; Mathematical models; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-35648983589
"Dongshi X., Zongcai R.","22634189200;15844280300;","IR image based eye gaze estimation",2007,"Proceedings - SNPD 2007: Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing","1",,"4287506","220","224",,17,"10.1109/SNPD.2007.325","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148851619&doi=10.1109%2fSNPD.2007.325&partnerID=40&md5=fdaaee88ca6a4c0980a6dc9b603dca1e","Research Center of Learning Science, Southeast University, Nanjing, China; School of Biological Science and Medical Engineering, Southeast University, Nanjing, China","Dongshi, X., Research Center of Learning Science, Southeast University, Nanjing, China, School of Biological Science and Medical Engineering, Southeast University, Nanjing, China; Zongcai, R., Research Center of Learning Science, Southeast University, Nanjing, China","In this paper, we present a novel approach to estimate the human's eye gaze from a single infrared spectrum image of two eyes. Observing that the pupil contour is a circle, we estimate the normal direction of this circle, considered as the eye gaze, from its elliptical image. By assuming that the two gaze vectors of both eyes and the vector which is determined by the two centers of the right and left pupil are coplanar, we can calculate the both eye gaze. Compared to most existing 3-D gaze estimation algorithms, our approach does not require those of face feature information such as the eye corners. Since the focal length can be calculated by our ""Coplanar Constraint"", ours does not need to know the exact focal length of the camera. The extensive experiments over simulated images and real images demonstrate the feasibility and the effectiveness of our method. © 2007 IEEE.",,"Algorithms; Computational methods; Infrared spectroscopy; Parameter estimation; Three dimensional; Eye gaze; Focal length; Gaze vector; Infrared imaging",Conference Paper,"Final","",Scopus,2-s2.0-35148851619
"Matthews I., Xiao J., Baker S.","7004579016;57190986110;35619939700;","2D vs. 3D deformable face models: Representational power, construction, and real-time fitting",2007,"International Journal of Computer Vision","75","1",,"93","113",,82,"10.1007/s11263-007-0043-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547181920&doi=10.1007%2fs11263-007-0043-2&partnerID=40&md5=d9776a4233ca9775c4ff4111cfb5851a","Robotics Institute, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, United States; Epson Palo Alto Laboratory, Epson Research and Development, 2580 Orchard Parkway, San Jose, CA 95131, United States; Microsoft Research, Microsoft Corporation, One Microsoft Way, Redmond, WA 98052-6339, United States","Matthews, I., Robotics Institute, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, United States; Xiao, J., Epson Palo Alto Laboratory, Epson Research and Development, 2580 Orchard Parkway, San Jose, CA 95131, United States; Baker, S., Microsoft Research, Microsoft Corporation, One Microsoft Way, Redmond, WA 98052-6339, United States","Model-based face analysis is a general paradigm with applications that include face recognition, expression recognition, lip-reading, head pose estimation, and gaze estimation. A face model is first constructed from a collection of training data, either 2D images or 3D range scans. The face model is then fit to the input image(s) and the model parameters used in whatever the application is. Most existing face models can be classified as either 2D (e.g. Active Appearance Models) or 3D (e.g. Morphable Models). In this paper we compare 2D and 3D face models along three axes: (1) representational power, (2) construction, and (3) real-time fitting. For each axis in turn, we outline the differences that result from using a 2D or a 3D face model. © 2007 Springer Science+Business Media, LLC.","2D Active Appearance Models; 3D Morphable Models; Constrained fitting; Factorization; Model construction; Model-based face analysis; Non-rigid structure-from-motion; Real-time fitting; Representational power; The inverse compositional algorithm","Algorithms; Constrained optimization; Face recognition; Mathematical models; Real time control; Three dimensional; 2D Active Appearance Models; 3D Morphable Models; Constrained fitting; Inverse compositional algorithms; Model-based face analysis; Non-rigid structure-from-motion; Representational power; Computer vision",Article,"Final","",Scopus,2-s2.0-34547181920
"Wu H., Kitagawa Y., Wada T., Kato T., Chen Q.","7405584998;23397436700;36026892800;7405278972;7406336206;","Tracking iris contour with a 3D eye-model for gaze estimation",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4843 LNCS","PART 1",,"688","697",,31,"10.1007/978-3-540-76386-4_65","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149108368&doi=10.1007%2f978-3-540-76386-4_65&partnerID=40&md5=09050214dab36f7c6d18cc80d49d94af","Faculty of Systems Engineering, Wakayama University, Japan","Wu, H., Faculty of Systems Engineering, Wakayama University, Japan; Kitagawa, Y., Faculty of Systems Engineering, Wakayama University, Japan; Wada, T., Faculty of Systems Engineering, Wakayama University, Japan; Kato, T., Faculty of Systems Engineering, Wakayama University, Japan; Chen, Q., Faculty of Systems Engineering, Wakayama University, Japan","This paper describes a sophisticated method to track iris contour and to estimate eye gaze for blinking eyes with a monocular camera. A 3D eye-model that consists of eyeballs, iris contours and eyelids is designed that describes the geometrical properties and the movements of eyes. Both the iris contours and the eyelid contours are tracked by using this eye-model and a particle filter. This algorithm is able to detect ""pure"" iris contours because it can distinguish iris contours from eyelids contours. The eye gaze is described by the movement parameters of the 3D eye model, which are estimated by the particle filter during tracking. Other distinctive features of this algorithm are: 1) it does not require any special light sources (e.g. an infrared illuminator) and 2) it can operate at video rate. Through extensive experiments on real video sequences we confirmed the robustness and the effectiveness of our method. © Springer-Verlag Berlin Heidelberg 2007.",,"Algorithms; Computational geometry; Feature extraction; Parameter estimation; Three dimensional; Monocular camera; Real video sequences; Target tracking",Conference Paper,"Final","",Scopus,2-s2.0-38149108368
"Chang C.-C., Wu C., Aghajan H.","23395786900;23399131700;18041521100;","Pose and gaze estimation in multi-camera networks for non-restrictive HCI",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4796 LNCS",,,"129","137",,2,"10.1007/978-3-540-75773-3_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149045685&doi=10.1007%2f978-3-540-75773-3_14&partnerID=40&md5=bfa81242a11058a5346c3786234028ab","Wireless Sensor Networks Lab., Stanford University, Stanford, CA 94305, United States","Chang, C.-C., Wireless Sensor Networks Lab., Stanford University, Stanford, CA 94305, United States; Wu, C., Wireless Sensor Networks Lab., Stanford University, Stanford, CA 94305, United States; Aghajan, H., Wireless Sensor Networks Lab., Stanford University, Stanford, CA 94305, United States","Multi-camera networks offer potentials for a variety of novel human-centric applications through provisioning of rich visual information. In this paper, face orientation analysis and posture analysis are combined as components of a human-centered interface system that allows the user's intentions and region of interest to be estimated without requiring carried or wearable sensors. In pose estimation, image observations at the cameras are first locally reduced to parametrical descriptions, and Particle Swarm Optimization (PSO) is then used for optimization of the kinematics chain of the 3D human model. In face analysis, a discrete-time linear dynamical system (LDS), based on kinematics of the head, combines the local estimates of the user's gaze angle produced by the cameras and employs spatiotemporal filters to correct any inconsistencies. Knowing the intention and the region of interest of the user facilitates further interpretation of human behavior, which is the key to non-restrictive and intuitive human-centered interfaces. Applications in assisted living, speaker tracking, and gaming can benefit from such unobtrusive interfaces. © Springer-Vorlag Berlin Heidelberg 2007.",,"Digital cameras; Human computer interaction; Information systems; Optimization; User interfaces; 3D human model; Gaze estimation; Visual information; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-38149045685
"Lee E.C., Park K.R., Whang M.C., Park J.","14009024200;8983316300;13106072200;36118003200;","Robust gaze tracking method for stereoscopic virtual reality systems",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4552 LNCS","PART 3",,"700","709",,12,"10.1007/978-3-540-73110-8_76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149030983&doi=10.1007%2f978-3-540-73110-8_76&partnerID=40&md5=efab5ad548960c6271fe05535adc3613","Dept. of Computer Science, Sangmyung University, 7 Hongji-dong, Jongro-Ku, Seoul, South Korea; Division of Digital Media Technology, Sangmyung University, 7 Hongji-dong, Jongro-Ku, Seoul, South Korea; Electronics and Telecommunications Research Institute, 161 Gajeong-Dong, Yuseong-gu, Daejon, South Korea","Lee, E.C., Dept. of Computer Science, Sangmyung University, 7 Hongji-dong, Jongro-Ku, Seoul, South Korea; Park, K.R., Division of Digital Media Technology, Sangmyung University, 7 Hongji-dong, Jongro-Ku, Seoul, South Korea; Whang, M.C., Division of Digital Media Technology, Sangmyung University, 7 Hongji-dong, Jongro-Ku, Seoul, South Korea; Park, J., Electronics and Telecommunications Research Institute, 161 Gajeong-Dong, Yuseong-gu, Daejon, South Korea","In this paper, we propose a new face and eye gaze tracking method that works by attaching gaze tracking devices to stereoscopic shutter glasses. This paper presents six advantages over previous works. First, through using the proposed method with stereoscopic VR systems, users feel more immersed and comfortable. Second, by capturing reflected eye images with a hot mirror, we were able to increase eye gaze accuracy in a vertical direction. Third, by attaching the infrared passing filter and using an IR illuminator, we were able to obtain robust gaze tracking performance irrespective of environmental lighting conditions. Fourth, we used a simple 2D-based eye gaze estimation method based on the detected pupil center and the 'geometric transform' process. Fifth, to prevent gaze positions from being unintentionally moved by natural eye blinking, we discriminated between different kinds of eye blinking by measuring pupil sizes. This information was also used for button clicking or mode toggling. Sixth, the final gaze position was calculated by the vector summation of face and eye gaze positions and allowing for natural face and eye movements. Experimental results showed that the face and eye gaze estimation error was less than one degree. © Springer-Verlag Berlin Heidelberg 2007.",,"Error analysis; Face recognition; Numerical methods; Robust control; User interfaces; Eye blinking; Eye gaze positions; Stereoscopic shutter glasses; Virtual reality",Conference Paper,"Final","",Scopus,2-s2.0-38149030983
"Lee E.C., Park K.R.","14009024200;8983316300;","A study on eye gaze estimation method based on cornea model of human eye",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4418 LNCS",,,"307","317",,14,"10.1007/978-3-540-71457-6_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149033861&doi=10.1007%2f978-3-540-71457-6_28&partnerID=40&md5=401c6a1d3d68f0417ef6c00800a54154","Dept. of Computer Science, Sangmyung University, Biometrics Engineering Research Center (BERC), 7 Hongji-dong, Jongro-Ku, Seoul, South Korea; Division of Digital Media Technology, Sangmyung University, Biometrics Engineering Research Center (BERC), 7 Hongji-dong, Jongro-Ku, Seoul, South Korea","Lee, E.C., Dept. of Computer Science, Sangmyung University, Biometrics Engineering Research Center (BERC), 7 Hongji-dong, Jongro-Ku, Seoul, South Korea; Park, K.R., Division of Digital Media Technology, Sangmyung University, Biometrics Engineering Research Center (BERC), 7 Hongji-dong, Jongro-Ku, Seoul, South Korea","In this paper, we propose a new gaze estimation method by analyzing the comea surface model which is estimated through three dimensional analysis of human eye in HMD (Head Mounted Display) environments. This paper has four advantages over previous works. First, in order to obtain accurate gaze position, we use a cornea sphere model based on Gullstrand eye scheme. Second, we calculate the 3D position of the cornea sphere and a gaze vector by using a camera, three collimated IR-LEDs and one illuminated IRLED. Third, three coordinates such as camera, monitor and eye coordinates are unified, which can simplify the complex 3D converting calculation and allow for calculation of the 3D eye position and gaze position on a HMD monitor. Fourth, a simple user dependent calibration method is proposed by gazing at one position of HMD monitor based on Kappa compensation. Experimental results showed that the average gaze estimation error of the proposed method was 0.89 degrees. © Springer-Verlag Berlin Heidelberg 2007.",,"Display devices; Image acquisition; Large scale systems; Three dimensional; Vectors; Gaze estimation; Gullstrand eye; Head Mounted Displays; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-37149033861
"Yoo D.H., Chung M.J., Ju D.B., Choi I.H.","7103242493;7402437119;15743761900;56812471900;","Non-intrusive eye gaze estimation using a projective invariant under head movement",2006,"Proceedings - IEEE International Conference on Robotics and Automation","2006",,"1642228","3443","3448",,11,"10.1109/ROBOT.2006.1642228","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845621140&doi=10.1109%2fROBOT.2006.1642228&partnerID=40&md5=3fd0da18908b4319b2c84c128cf8857d","LG Electronics, 360-5, Yatap-dong, Bundang-gu, Sungnam-si, Kyunggi-do, South Korea; KAIST, 373-1, Guseong-dong, Yuseong-gu, Daejeon, South Korea","Yoo, D.H., LG Electronics, 360-5, Yatap-dong, Bundang-gu, Sungnam-si, Kyunggi-do, South Korea; Chung, M.J., KAIST, 373-1, Guseong-dong, Yuseong-gu, Daejeon, South Korea; Ju, D.B., LG Electronics, 360-5, Yatap-dong, Bundang-gu, Sungnam-si, Kyunggi-do, South Korea; Choi, I.H., LG Electronics, 360-5, Yatap-dong, Bundang-gu, Sungnam-si, Kyunggi-do, South Korea","In this paper, a non-intrusive eye gaze estimation system is proposed. The proposed system consists of five light sources and two cameras, and the direction of the user's eye gaze can be computed by using a projective property without computing the geometrical relation among the eye, the cameras, and the monitor in 3D space. Our method is accurate under head movement, and is comparatively simple and fast. We will introduce our method and show experimental results. The experimental results will verify the feasibility of the proposed system as a human-machine interface. ©2006 IEEE.",,"Cameras; Computational geometry; Human computer interaction; Light sources; Man machine systems; Pattern recognition systems; Three dimensional; User interfaces; Eye gaze estimation; Head movements; Human machine interface; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-33845621140
"Yeo A.W., Chiu P.-C.","7006663989;36968467900;","Gaze Estimation Model for eye drawing",2006,"Conference on Human Factors in Computing Systems - Proceedings",,,,"1559","1564",,4,"10.1145/1125451.1125736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869110630&doi=10.1145%2f1125451.1125736&partnerID=40&md5=fe5ba98a513b26b753b5eecec49eb84d","Faculty of Computer Science and Information Technology, Universiti Malaysia Sarawak, 94300 Kota Samarahan Sarawak, Malaysia","Yeo, A.W., Faculty of Computer Science and Information Technology, Universiti Malaysia Sarawak, 94300 Kota Samarahan Sarawak, Malaysia; Chiu, P.-C., Faculty of Computer Science and Information Technology, Universiti Malaysia Sarawak, 94300 Kota Samarahan Sarawak, Malaysia","This paper describes a model that can be employed in eye drawing software applications. Unlike most of the existing interfaces for eye typing, eye drawing focuses on small target selection and moves the cursor to a precise location. This is made possible by a proposed Gaze Estimation Model which interprets users' interest when they want to draw new objects in a particular position.","Assistive technology; Drawing; Eye movement; Eye tracking; Input devices; Interaction techniques","Assistive technology; Drawing softwares; Eye-tracking; Gaze estimation; Input devices; Interaction techniques; Small targets; Users' interests; Drawing (graphics); Eye movements; Human engineering",Conference Paper,"Final","",Scopus,2-s2.0-84869110630
[无可用作者姓名],[无可用的作者 ID],"Advances in Image and Video Technology - First Pacific Rim Symposium, PSIVT 2006, Proceedings",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4319 LNCS",,,"","",1370,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350639007&partnerID=40&md5=fdd6d1cea94764acec1c1e8c388665fd",,"","The proceedings contain 135 papers. The topics discussed include: error analysis of feature based disparity estimation; collinearity and coplanarity constraints for structure from motion; reconstruction of building models with curvilinear boundaries from laser scanner and aerial imagery; octree subdivision using coplanar criterion for hierarchical point simplification; target calibration and tracking using conformal geometric algebra; boundary based orientation of polygonal shapes; the invariance properties of chromatic characteristics; a scale invariant surface curvature estimator; guided importance sampling based particle filtering for visual tracking; gaze estimation from low resolution images; image similarity comparison using dual-tree wavelet transform; a novel supervised dimensionality reduction algorithm for online image recognition; and two thresholding for deriving the bi-level document image.",,,Conference Review,"Final","",Scopus,2-s2.0-70350639007
[无可用作者姓名],[无可用的作者 ID],"ICMI 06: 8th International Conference on Multimodal Interfaces, Conference Proceedings",2006,"ICMI'06: 8th International Conference on Multimodal Interfaces, Conference Proceeding",,,,"","",390,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-36749021244&partnerID=40&md5=347f175af8408ec12c2958d365c6a6ee",,"","The proceedings contain 60 papers. The topics discussed include: mixing virtual and actual; collaborative multimodal photo annotation over digital paper; human perception of intended addressee during computer-assisted meetings; automatic detection of group functional roles in face to face interactions; cross-modal coordination of expressive strength between voice and gesture for personified media; from vocal to multimodal dialogue management; co-adaptation of audio-visual speech and gesture classifiers; towards the integration of shape-related information in 3-D gestures and speech; multimodal estimation of user interruptibility for smart mobile telephones; short message dictation on Symbian series 60 mobile phones; a contextual multimodal integrator; gender and age estimation system robust to pose variations; a fast and robust 3D head pose and gaze estimation system; and audio-visual emotion recognition in adult attachment interview.",,"Computer supported cooperative work; Information management; Three dimensional computer graphics; User interfaces; Automatic detection; Contextual multimodal integrators; Multimodal dialogue management; Photo annotation; Virtual reality",Conference Review,"Final","",Scopus,2-s2.0-36749021244
"Kinoshita K., Ma Y., Lao S., Kawaade M.","16202883200;55687270000;7004254253;17345990600;","A fast and robust 3D head pose and gaze estimation system",2006,"ICMI'06: 8th International Conference on Multimodal Interfaces, Conference Proceeding",,,,"137","138",,7,"10.1145/1180995.1181026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547222939&doi=10.1145%2f1180995.1181026&partnerID=40&md5=6e2b7404a95d137dd9043df5605eea63","Sensing and Control Technology Laboratory, OMRON Corporation, 9-1, Kizugawadai, Kizu-cho, Soraku-Gun, Kyoto 619-0283, Japan","Kinoshita, K., Sensing and Control Technology Laboratory, OMRON Corporation, 9-1, Kizugawadai, Kizu-cho, Soraku-Gun, Kyoto 619-0283, Japan; Ma, Y., Sensing and Control Technology Laboratory, OMRON Corporation, 9-1, Kizugawadai, Kizu-cho, Soraku-Gun, Kyoto 619-0283, Japan; Lao, S., Sensing and Control Technology Laboratory, OMRON Corporation, 9-1, Kizugawadai, Kizu-cho, Soraku-Gun, Kyoto 619-0283, Japan; Kawaade, M., Sensing and Control Technology Laboratory, OMRON Corporation, 9-1, Kizugawadai, Kizu-cho, Soraku-Gun, Kyoto 619-0283, Japan","We developed a fast and robust head pose and gaze estimation system. This system can detect facial points and estimate 3D pose angles and gaze direction under various conditions including facial expression changes and partial occlusion. We need only one face image as input and do not need special devices such as blinking LEDs or stereo cameras. Moreover, no calibration is needed. The system shows a 95% head pose estimation accuracy and 81% gaze estimation accuracy (when the error margin is 15 degrees). The processing time is about 15 ms/frame (Pentium4 3.2 GHz). Acceptable range of facial pose is within a yaw (left-right) of ±60 degrees and within a pitch (up-down) of ±30 degrees.","Facial image; Gaze estimation; Pose estimation","Artificial intelligence; Cameras; Error analysis; Face recognition; Light emitting diodes; Facial expression; Gaze estimation system; Occlusion; Pose estimation; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-34547222939
"Gross R., Matthews I., Baker S.","12446072700;7004579016;35619939700;","Active appearance models with occlusion",2006,"Image and Vision Computing","24","6",,"593","604",,93,"10.1016/j.imavis.2005.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745007363&doi=10.1016%2fj.imavis.2005.08.001&partnerID=40&md5=379d259759fe86cbe9ff21c77de509af","The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Gross, R., The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Matthews, I., The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States; Baker, S., The Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States","Active Appearance Models (AAMs) are generative parametric models that have been successfully used in the past to track faces in video. A variety of video applications are possible, including dynamic head pose and gaze estimation for real-time user interfaces, lip-reading, and expression recognition. To construct an AAM, a number of training images of faces with a mesh of canonical feature points (usually hand-marked) are needed. All feature points have to be visible in all training images. However, in many scenarios parts of the face may be occluded. Perhaps the most common cause of occlusion is 3D pose variation, which can cause self-occlusion of the face. Furthermore, tracking using standard AAM fitting algorithms often fails in the presence of even small occlusions. In this paper we propose algorithms to construct AAMs from occluded training images and to track faces efficiently in videos containing occlusion. We evaluate our algorithms both quantitatively and qualitatively and show successful real-time face tracking on a number of image sequences containing varying degrees and types of occlusions. © 2005 Elsevier B.V. All rights reserved.","Fitting with occlusion; Model-based face analysis; Robust model fitting","Algorithms; Image analysis; Image processing; Real time systems; User interfaces; Fitting with occlusion; Model-based face analysis; Robust model fitting; Parameter estimation",Article,"Final","",Scopus,2-s2.0-33745007363
"Hoffman M.W., Grimes D.B., Shon A.P., Rao R.P.N.","57197647601;35725345900;6603103614;7403068988;","A probabilistic model of gaze imitation and shared attention",2006,"Neural Networks","19","3",,"299","310",,48,"10.1016/j.neunet.2006.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646546015&doi=10.1016%2fj.neunet.2006.02.008&partnerID=40&md5=eccf3e63d180a58de31c9c1ba2143301","Department of Computer Science and Engineering, University of Washington, Seattle, WA 98195, United States","Hoffman, M.W., Department of Computer Science and Engineering, University of Washington, Seattle, WA 98195, United States; Grimes, D.B., Department of Computer Science and Engineering, University of Washington, Seattle, WA 98195, United States; Shon, A.P., Department of Computer Science and Engineering, University of Washington, Seattle, WA 98195, United States; Rao, R.P.N., Department of Computer Science and Engineering, University of Washington, Seattle, WA 98195, United States","An important component of language acquisition and cognitive learning is gaze imitation. Infants as young as one year of age can follow the gaze of an adult to determine the object the adult is focusing on. The ability to follow gaze is a precursor to shared attention, wherein two or more agents simultaneously focus their attention on a single object in the environment. Shared attention is a necessary skill for many complex, natural forms of learning, including learning based on imitation. This paper presents a probabilistic model of gaze imitation and shared attention that is inspired by Meltzoff and Moore's AIM model for imitation in infants. Our model combines a probabilistic algorithm for estimating gaze vectors with bottom-up saliency maps of visual scenes to produce maximum a posteriori (MAP) estimates of objects being looked at by an observed instructor. We test our model using a robotic system involving a pan-tilt camera head and show that combining saliency maps with gaze estimates leads to greater accuracy than using gaze alone. We additionally show that the system can learn instructor-specific probability distributions over objects, leading to increasing gaze accuracy over successive interactions with the instructor. Our results provide further support for probabilistic models of imitation and suggest new ways of implementing robotic systems that can interact with humans over an extended period of time. © 2006.","Gaze tracking; Human-robot interaction; Imitation learning; Shared attention","Algorithms; Cognitive systems; Human computer interaction; Mathematical models; Probabilistic logics; Probability distributions; Robotics; Gaze tracking; Human-robot interaction; Imitation learning; Shared attention; Learning systems; algorithm; article; attention; Bayes theorem; gaze; imitation; language ability; learning; mathematical analysis; motor activity; priority journal; probability sample; robotics; vision; Attention; Bayes Theorem; Fixation, Ocular; Humans; Imitative Behavior; Models, Psychological; Models, Statistical; Visual Perception",Article,"Final","",Scopus,2-s2.0-33646546015
"Merad D., Metz S., Miguet S.","17434913000;57217411457;55932981600;","Eye gaze estimation from a video",2006,"Proceedings - SCCG 2006: 22nd Spring Conference on Computer Graphics",,,,"161","168",,1,"10.1145/2602161.2602180","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901979925&doi=10.1145%2f2602161.2602180&partnerID=40&md5=e14fa3658578d6c823e2d0524d1bd902","LIRIS UMR-5205 Laboratory, France; ICAR UMR-5191 Laboratory, France","Merad, D., LIRIS UMR-5205 Laboratory, France; Metz, S., ICAR UMR-5191 Laboratory, France; Miguet, S., LIRIS UMR-5205 Laboratory, France","Our work focuses on the interdisciplinary field of detailed analysis of behaviors exhibited by individuals during sessions of distributed collaboration. With a particular focus on ergonomics, we propose new mechanisms to be integrated into existing tools to enable increased productivity in distributed learning and working. Our technique is to record ocular movements (eye tracking) to analyze various scenarios of distributed collaboration in the context of computer-based training. In this article, we present a low-cost oculometric device that is capable of making ocular measurements without interfering with the natural behavior of the subject. We expect that this device could be employed anywhere that a natural, non-intrusive method of observation is required, and its low-cost permits it to be readily integrated into existing popular tools, particularly E-learning campus. Copyright © 2006 by the Association for Computing Machinery, Inc.","Head tracking; Visual direction estimation","E-learning; Ergonomics; Computer based training; Distributed collaboration; Distributed learning; Head tracking; Increased productivity; Interdisciplinary fields; Non-intrusive method; Visual directions; Computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-84901979925
"Ono Y., Okabe T., Sato Y.","35102974600;7201390055;35230954300;","Gaze estimation from low resolution images",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4319 LNCS",,,"178","188",,16,"10.1007/11949534_18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350276507&doi=10.1007%2f11949534_18&partnerID=40&md5=f5a6dfdf9a74e76e04c900ae3c552dc5","Institute of Industrial Science, University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan","Ono, Y., Institute of Industrial Science, University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan; Okabe, T., Institute of Industrial Science, University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan; Sato, Y., Institute of Industrial Science, University of Tokyo, 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505, Japan","The purpose of this study is to develop an appearance-based method for estimating gaze directions from low resolution images. The problem of estimating directions using low resolution images is that the position of an eye region cannot be determined accurately. In this work, we introduce two key ideas to cope with the problem: incorporating training images of eye regions with artificially added positioning errors, and separating the factor of gaze variation from that of positioning error based on N-mode SVD (Singular Value Decomposition). We show that estimation of gaze direction in this framework is formulated as a bilinear problem that is then solved by alternatively minimizing a bilinear cost function with respect to gaze direction and position of the eye region. In this paper, we describe the details of our proposed method and show experimental results that demonstrate the merits of our method. © 2006 Springer-Verlag.","Appearance-based method; Gaze estimation; Low resolution; N-mode SVD; Positioning","Cost functions; Appearance-based methods; Gaze estimation; Low resolution; Low resolution images; Positioning; Positioning error; SVD(singular value decomposition); Training image; Singular value decomposition",Conference Paper,"Final","",Scopus,2-s2.0-70350276507
"Xinghua S., Guoyong C., Chunxia Z., Jingyu Y.","23493804800;24334448500;23003513500;24830600000;","Gaze estimation of human eye",2006,"ITST 2006 - 2006 6th International Conference on ITS Telecommunications,  Proceedings",,,"4068592","310","313",,4,"10.1109/ITST.2006.288901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-44449118142&doi=10.1109%2fITST.2006.288901&partnerID=40&md5=c37e134f03eae16b05ba2f1799c25cef","School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094, China","Xinghua, S., School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094, China; Guoyong, C., School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094, China; Chunxia, Z., School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094, China; Jingyu, Y., School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094, China","As an indispensable component of Intelligent Transportation Systems, the driving assistant has attracted much attention of scholars and engineers. The gaze estimation of human eye just serves for the driving assistant. In this paper the eye and gaze models are introduced first and then improved by us, so that the new models are presented. Based on the new models the Hough transform method with gradient direction is used to perform the iris detection and gaze estimation. In experiments the two parameters of false estimation rate and average detection time are investigated. To the good-quality frame images the false estimation rate is 5.87 % and the average detection time is 94.3 milliseconds. In the end the proposed gaze estimation algorithm is applied to a real-time gaze tracking system and the comparison result shows that it has the more accuracy. © 2006 IEEE.",,"Eye movements; Hough transforms; Intelligent systems; Mathematical models; Real time systems; Vehicle locating systems; Gaze estimation; Human eye; Iris detection; Real time gaze tracking systems; Motion estimation",Conference Paper,"Final","",Scopus,2-s2.0-44449118142
"Kwon Y.-M., Jeon K.-W., Kim S.-K.","7403459357;15765182700;56272760600;","Research on gaze-based interaction to 3D display system",2006,"Proceedings of SPIE - The International Society for Optical Engineering","6392",,"63920J","","",,2,"10.1117/12.685571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846213900&doi=10.1117%2f12.685571&partnerID=40&md5=700271d3660b41ec0b7c70bc08cb4d03","Imaging Media Research Center, Korea Institute of Science and Technology, 39-1 Hawolgok-dong, Seongbuk-gu, Seoul 136-791, South Korea","Kwon, Y.-M., Imaging Media Research Center, Korea Institute of Science and Technology, 39-1 Hawolgok-dong, Seongbuk-gu, Seoul 136-791, South Korea; Jeon, K.-W., Imaging Media Research Center, Korea Institute of Science and Technology, 39-1 Hawolgok-dong, Seongbuk-gu, Seoul 136-791, South Korea; Kim, S.-K., Imaging Media Research Center, Korea Institute of Science and Technology, 39-1 Hawolgok-dong, Seongbuk-gu, Seoul 136-791, South Korea","There have been reported several researches on gaze tracking techniques using monocular camera or stereo camera. The most popular used gaze estimation techniques are based on PCCR (Pupil Center & Cornea Reflection). These techniques are for gaze tracking for 2D screen or images. In this paper, we address the gaze-based 3D interaction to stereo image for 3D virtual space. To the best of our knowledge, our paper first addresses the 3D gaze interaction techniques to 3D display system. Our research goal is the estimation of both of gaze direction and gaze depth. Until now, the most researches are focused on only gaze direction for the application to 2D display system. It should be noted that both of gaze direction and gaze depth should be estimated for the gaze-based interaction in 3D virtual space. In this paper, we address the gaze-based 3D interaction techniques with glassless stereo display. The estimation of gaze direction and gaze depth from both eyes is a new important research topic for gaze-based 3D interaction. We present our approach for the estimation of gaze direction and gaze depth and show experimentation results.","3D computer mouse; 3d display system; Eye vergence; Gaze-based interaction; Stereo image depth","Cameras; Human computer interaction; Image reconstruction; User interfaces; Virtual reality; 3D computer mouse; 3d display systems; Eye vergence; Gaze-based interaction; Stereo image depth; Display devices",Conference Paper,"Final","",Scopus,2-s2.0-33846213900
"Hansen D.W., Hansen J.P.","15063910800;55465667700;","Robustifying eye interaction",2006,"Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition","2006",,"1640598","152","",,13,"10.1109/CVPRW.2006.181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845532980&doi=10.1109%2fCVPRW.2006.181&partnerID=40&md5=175e2b6e838bab2bfe2f3ebc495bbc41","IT University, Copenhagen, Denmark","Hansen, D.W., IT University, Copenhagen, Denmark; Hansen, J.P., IT University, Copenhagen, Denmark","This paper presents a gaze typing system based on consumer hardware. Eye tracking based on consumer hardware is subject to several unknown factors. We propose methods using robust statistical principles to accommodate uncertainties in image data as well as in gaze estimates to improve accuracy. We have succeeded to track the gaze of people with a standard consumer camera, obtaining accuracies about 160 pixels on screen. Proper design of the typing interface, however, reduces the need for high accuracy. We have observed typing speeds in the range of 3 - 5 words per minute for untrained subjects using large on-screen buttons and a new noise tolerant dwell-time principle © 2006 IEEE.",,"Cameras; Consumer electronics; Hardware; Robustness (control systems); Spurious signal noise; Statistical methods; Consumer hardware; Eye interactions; Typing speeds; Image reconstruction",Conference Paper,"Final","",Scopus,2-s2.0-33845532980
"Park K.R.","8983316300;","Robust gaze estimation for human computer interaction",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4099 LNAI",,,"1222","1226",,4,"10.1007/11801603_165","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749569011&doi=10.1007%2f11801603_165&partnerID=40&md5=e79347496edc9f12d018da495a577302","Division of Media Technology, Sangmyung University, 7 Hongji-Dong, Jongro-ku, Seoul, South Korea","Park, K.R., Division of Media Technology, Sangmyung University, 7 Hongji-Dong, Jongro-ku, Seoul, South Korea","To achieve natural human computer interface, a new gaze detection method is proposed, which allows user's natural head and eye movement with one camera system and four IR-LED illuminators. This paper has following 4 advancements compared to previous works. First, all procedures for detecting gaze position are operated automatically. Second, although we use the see-through glasses 'attached with eye detecting camera, the change of facial position cannot affect the gaze detection accuracy. Third, we use elliptical hough transform and geometric transform in order to detect accurate pupil region. Fourth, to solve the problem of ambiguous coin face-on of pupil shape, we use the EKF (Extended Kalman Filter) and can track continuous eye movement. © Springer-Verlag Berlin Heidelberg 2006.","EKF; Elliptical Hough Transform; Gaze Detection; Geometric Transform","Cameras; Eye movements; Hough transforms; Kalman filtering; Light emitting diodes; Parameter estimation; Robustness (control systems); Elliptical Hough Transform; Extended Kalman Filter (EKF); Gaze Detection; Geometric Transform; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-33749569011
"Otsuka K., Takemae Y., Yarnato J., Murase H.","35303289200;8640494900;6604064362;7101900108;","Probabilistic inference of gaze patterns and structure of multiparty conversations from head directions and utterances",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4012 LNAI",,,"353","364",,1,"10.1007/11780496_38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746031439&doi=10.1007%2f11780496_38&partnerID=40&md5=d761052b3040739161300e6050088362","NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, Atsugi, 243-0198, Japan; NTT Cyber Solutions Laboratories, Nippon Telegraph and Telephone Corporation, Yokosuka, 239-0847, Japan; Graduate School of Information Science, Nagoya University, Nagoya, 464-8601, Japan","Otsuka, K., NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, Atsugi, 243-0198, Japan, Graduate School of Information Science, Nagoya University, Nagoya, 464-8601, Japan; Takemae, Y., NTT Cyber Solutions Laboratories, Nippon Telegraph and Telephone Corporation, Yokosuka, 239-0847, Japan; Yarnato, J., NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation, Atsugi, 243-0198, Japan; Murase, H., Graduate School of Information Science, Nagoya University, Nagoya, 464-8601, Japan","A novel probabilistic framework is proposed for inferring gaze patterns and the structure of conversation in face-to-face multiparty communication, based on head directions and the presence/absence of utterances of participants. First, we define three classes of conversational regimes, which are characterized by the topology of the gaze pattern; we assume that they indicate the structure of the conversation, i.e. who is talking to whom. Next, the problem is formulated as joint estimation of both regime state from the gaze pattern and utterance, and the gaze pattern from head directions. We then devise a dynamic Bayesian network, called the Markov-switching model. The regime changes over time are based on Markov transitions, and controls the dynamics of the gaze patterns and utterances. Furthermore, Bayesian estimation of regime, gaze pattern, and model parameters are implemented using a Markov chain Monte Carlo method. Experiments on four-person conversations confirm accurate gaze estimation and the effectiveness of the framework toward identification of the conversation structures. © Springer-Verlag Berlin Heidelberg 2006.",,"Computer networks; Markov processes; Monte Carlo methods; Parameter estimation; Probability; Problem solving; Dynamic Bayesian network; Gaze patterns; Markov-switching models; Multiparty conversations; Pattern recognition",Conference Paper,"Final","",Scopus,2-s2.0-33746031439
"Kisačanin B., Pavlović V., Huang T.S.","56000219500;13606159500;35513984600;","Real-time vision for human-computer interaction",2005,"Real-Time Vision for Human-Computer Interaction",,,,"1","301",,28,"10.1007/0-387-27890-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892243964&doi=10.1007%2f0-387-27890-7&partnerID=40&md5=cf12f3880f29defa7d3878b98960daaf","Delphi Corporation, United States; Rutgers University, United States; University of Illinois at Urbana-Champaign, United States","Kisačanin, B., Delphi Corporation, United States; Pavlović, V., Rutgers University, United States; Huang, T.S., University of Illinois at Urbana-Champaign, United States","As computers become prevalent in all aspects of daily life, the need for natural and effective Human-Computer Interaction (HCI) becomes increasingly important. Computer vision and pattern recognition continue to play a dominant role in the HCI realm. However, computer vision methods often fail to become pervasive in the field due to the lack of real-time, robust algorithms, as well as novel and convincing applications. This state-of-the-art contributed volume presents a series of peer-reviewed survey articles written by international leading experts in computer vision, pattern recognition and Human-Computer Interaction. It is the first published text capturing the latest research in this rapidly advancing field with exclusive focus on real-time algorithms and practical applications in numerous industries, including computer games and medical and automotive systems. It is also an excellent starting point for further research in these areas. Contributions to this volume address specific topics such as: Real-Time Algorithms: from Signal Processing to Computer Vision Recognition of Isolated Fingerspelling Gestures Using Depth Edges Appearance-Based Real-Time Understanding of Gestures Using Projected Euler Angles Flocks of Features for Tracking Articulated Objects Static Hand Posture Recognition Based on Okapi-Chamfer Matching Visual Modeling of Dynamic Gestures Using 3D Appearance and Motion Features Head and Facial Animation Tracking Using Appearance-Adaptive Models and Particle Filters A Real-Time Vision Interface Based on Gaze Detection - Eyekeys Map Building From Human-Computer Interactions Real-Time Inference of Complex Mental States from Facial Expressions and Head Gestures Epipolar Constrained User Pushbutton Selection in Projected Interfaces Vision-Based HCI Applications The Office of the Past MPEG-4 Face and Body Animation Coding Applied to HCI Multimodal Human-Computer Interaction Real-Time Vision for Human-Computer Interaction is an invaluable reference for HCI researchers in both academia and industry, and a useful supplement for advanced-level courses in HCI and Computer Vision. © 2005 by Springer Science+Business Media, Inc. All rights reserved.",,,Book,"Final","",Scopus,2-s2.0-84892243964
"Park J., Kwon Y.-M., Sohn K.","56812991500;7403459357;7101931558;","Gaze tracking system using single camera and Purkinje image",2005,"ACM International Conference Proceeding Series","157",,,"280","",,2,"10.1145/1152399.1152470","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953516609&doi=10.1145%2f1152399.1152470&partnerID=40&md5=f0fbeb4c7484e03d4644db5904572d17","Imaging Media Research Center, Korea Institute of Science and Technology, 39-1 Hawolgok-dong, Seongbuk-gu, Seoul, South Korea; Digital Image Media Lab., Yonsei University, 134 Sinchon-dong, Seodaemun-gu, Seoul, South Korea","Park, J., Imaging Media Research Center, Korea Institute of Science and Technology, 39-1 Hawolgok-dong, Seongbuk-gu, Seoul, South Korea, Digital Image Media Lab., Yonsei University, 134 Sinchon-dong, Seodaemun-gu, Seoul, South Korea; Kwon, Y.-M., Imaging Media Research Center, Korea Institute of Science and Technology, 39-1 Hawolgok-dong, Seongbuk-gu, Seoul, South Korea; Sohn, K., Digital Image Media Lab., Yonsei University, 134 Sinchon-dong, Seodaemun-gu, Seoul, South Korea","A simple 2D gaze tracking algorithm using single camera and Purkinje image is proposed. This algorithm employs single camera with infrared filter to capture one's eye and two infrared point light sources to make reflection point, called Purkinje image, to estimate corresponding gaze point on the screen from user's eyes.","gaze estimation; gaze tracking; gaze-based interaction; human-computer interaction","Gaze estimation; Gaze point; Gaze tracking; Gaze tracking system; Gaze-based interaction; Infrared filters; Point light source; Reflection points; Single cameras; Algorithms; Cameras; Knowledge management; Light reflection; Light sources; Tracking (position); Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-77953516609
"Trucco E., Anderson T., Razeto M., Ivekovic S.","57201786780;55547136434;57199573537;55943710800;","Robust correspondenceless 3-D iris location for immersive environments",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","3617 LNCS",,,"123","130",,2,"10.1007/11553595_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745167365&doi=10.1007%2f11553595_15&partnerID=40&md5=97b4adcd618456f9f0dec5ec066ad0e8","EECE-EPS, Heriot Watt University, Riccarton, Edinburgh, EH14 4AS, United Kingdom","Trucco, E., EECE-EPS, Heriot Watt University, Riccarton, Edinburgh, EH14 4AS, United Kingdom; Anderson, T., EECE-EPS, Heriot Watt University, Riccarton, Edinburgh, EH14 4AS, United Kingdom; Razeto, M., EECE-EPS, Heriot Watt University, Riccarton, Edinburgh, EH14 4AS, United Kingdom; Ivekovic, S., EECE-EPS, Heriot Watt University, Riccarton, Edinburgh, EH14 4AS, United Kingdom","We present a system locating the contour of an iris in space using robust active ellipse search and correspondenceless stereo. Robust iris location is the basis for gaze estimation and tracking, and, as such, an essential module for augmented and virtual reality environments, The system implements a robust active ellipse search based on a multi-scale contour detection model. The search is carried out by a simulated annealing algorithm, guaranteeing excellent performance in spite of heavy occlusions due to blinking, uncontrolled lighting, erratic target motion, and reflections of unpredictable scene elements. Stereo correspondence is avoided altogether by intersecting conjugate epipolar lines with the located ellipses. Experiments on synthetic and real images indicate very good performance of both location and reconstruction modules. © Springer-Verlag Berlin Heidelberg 2006.",,"Contour measurement; Motion estimation; Robustness (control systems); Simulated annealing; Stereo vision; Tracking (position); Virtual reality; Active ellipse search; Contour detection model; Gaze estimation; Occlusions; Three dimensional computer graphics",Conference Paper,"Final","",Scopus,2-s2.0-33745167365
"Miyake T., Iwami T., Horihata S., Zhang Z.","57205594283;12788806300;6602930456;55721844200;","Single video image based eye-gaze estimation for human interface",2005,"WSEAS Transactions on Computers","4","11",,"1576","1582",,5,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645122977&partnerID=40&md5=fd88813fc367d77093ffd5ee54960a9a","Department of Production Systems Engineering, Toyohashi University of Technology, Hibarigaoka 1-1, Tenpaku-cho, Toyohashi 441-8580, Japan","Miyake, T., Department of Production Systems Engineering, Toyohashi University of Technology, Hibarigaoka 1-1, Tenpaku-cho, Toyohashi 441-8580, Japan; Iwami, T., Department of Production Systems Engineering, Toyohashi University of Technology, Hibarigaoka 1-1, Tenpaku-cho, Toyohashi 441-8580, Japan; Horihata, S., Department of Production Systems Engineering, Toyohashi University of Technology, Hibarigaoka 1-1, Tenpaku-cho, Toyohashi 441-8580, Japan; Zhang, Z., Department of Production Systems Engineering, Toyohashi University of Technology, Hibarigaoka 1-1, Tenpaku-cho, Toyohashi 441-8580, Japan","It is well known that human eyes are useful to inform one's will to others. Previous works for constructing a human interface that uses a line of sight are divided into two groups according to the way of detection of the line. The one uses a special light source or a hardware device. The other uses a software program for image processing. Our system, based on image processing, has some remarkable characteristics compared to previous ones. 1)It uses only a single facial image obtained by a video camera without using any instrument. 2)The distance between the camera and a user of the system is optically far enough. Consequently, the system can be applicable for not only helping handicapped persons in bed but also persons moving freely in home. Some experiments demonstrate the validity of the system.","Gaze; Human interface; Image processing; Real time; Single facial image","Computer simulation; Edge detection; Feature extraction; Light sources; User interfaces; Video cameras; Vision; Eye-gaze estimation; Human interface; Single facial image; Image processing",Article,"Final","",Scopus,2-s2.0-33645122977
"Yoo D.H., Chung M.J.","7103242493;7402437119;","A novel non-intrusive eye gaze estimation using cross-ratio under large head motion",2005,"Computer Vision and Image Understanding","98","1",,"25","51",,183,"10.1016/j.cviu.2004.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044279389&doi=10.1016%2fj.cviu.2004.07.011&partnerID=40&md5=5bef1abf79bca7ced6e0fdc7b5081b46","Division of Electrical Engineering, Department of Electrical Engineering, Korea Adv. Inst. Sci. and Technol., 373-1, Guseong-dong, Yuseong-gu, Daejeon 305-701, South Korea","Yoo, D.H., Division of Electrical Engineering, Department of Electrical Engineering, Korea Adv. Inst. Sci. and Technol., 373-1, Guseong-dong, Yuseong-gu, Daejeon 305-701, South Korea; Chung, M.J., Division of Electrical Engineering, Department of Electrical Engineering, Korea Adv. Inst. Sci. and Technol., 373-1, Guseong-dong, Yuseong-gu, Daejeon 305-701, South Korea","Eye gaze estimation systems calculate the direction of human eye gaze. Numerous accurate eye gaze estimation systems considering a user's head movement have been reported. Although the systems allow large head motion, they require multiple devices and complicate computation in order to obtain the geometrical positions of an eye, cameras, and a monitor. The light-reflection-based method proposed in this paper does not require any knowledge of their positions, so the system utilizing the proposed method is lighter and easier to use than the conventional systems. To estimate where the user looks allowing ample head movement, we utilize an invariant value (cross-ratio) of a projective space. Also, a robust feature detection using an ellipse-specific active contour is suggested in order to find features exactly. Our proposed feature detection and estimation method are simple and fast, and shows accurate results under large head motion. © 2004 Elsevier Inc. All rights reserved.","Cross-ratio; Large head motion; Non-intrusive eye gaze estimation; Robust pupil detection","Cameras; Computational geometry; Feature extraction; Interactive computer systems; Light reflection; Parameter estimation; Robustness (control systems); Cross-ratio; Large head motion; Non-intrusive eye gaze; Robust pupil detection; Computer vision",Article,"Final","",Scopus,2-s2.0-10044279389
"Noureddin B., Lawrence P.D., Man C.F.","55903711500;36729888200;7005722364;","A non-contact device for tracking gaze in a human computer interface",2005,"Computer Vision and Image Understanding","98","1",,"52","82",,79,"10.1016/j.cviu.2004.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044269878&doi=10.1016%2fj.cviu.2004.07.005&partnerID=40&md5=cd79f2ae510aa80982df711cb9db0b4f","Robotics and Control Laboratory, Department of Electrical Engineering, University of British Columbia, Vancouver, BC V6T 1Z4, Canada","Noureddin, B., Robotics and Control Laboratory, Department of Electrical Engineering, University of British Columbia, Vancouver, BC V6T 1Z4, Canada; Lawrence, P.D., Robotics and Control Laboratory, Department of Electrical Engineering, University of British Columbia, Vancouver, BC V6T 1Z4, Canada; Man, C.F., Robotics and Control Laboratory, Department of Electrical Engineering, University of British Columbia, Vancouver, BC V6T 1Z4, Canada","This paper presents a novel design for a non-contact eye detection and gaze tracking device. It uses two cameras to maintain real-time tracking of a person's eye in the presence of head motion. Image analysis techniques are used to obtain accurate locations of the pupil and corneal reflections. All the computations are performed in software and the device only requires simple, compact optics and electronics attached to the user's computer. Three methods of estimating the user's point of gaze on a computer monitor are evaluated. The camera motion system is capable of tracking the user's eye in real-time (9 fps) in the presence of natural head movements as fast as 100°/s horizontally and 77°/s vertically. Experiments using synthetic images have shown its ability to track the location of the eye in an image to within 0.758 pixels horizontally and 0.492 pixels vertically. The system has also been tested with users with different eye colors and shapes, different ambient lighting conditions and the use of eyeglasses. A gaze accuracy of 2.9° was observed. © 2004 Elsevier Inc. All rights reserved.","Eye tracking; Gaze estimation; HCI","Cameras; Color; Computer monitors; Eye controlled devices; Eye movements; Eyeglasses; Image analysis; Light reflection; Lighting; Tracking (position); Eye tracking; Gaze estimation; Gaze tracking; Non-contact device; Human computer interaction",Article,"Final","",Scopus,2-s2.0-10044269878
"Wang J.-G., Sung E., Venkateswarlu R.","57223821965;7006254261;56248905000;","Estimating the eye gaze from one eye",2005,"Computer Vision and Image Understanding","98","1",,"83","103",,91,"10.1016/j.cviu.2004.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044264502&doi=10.1016%2fj.cviu.2004.07.008&partnerID=40&md5=da95694734c26338bec3e8c2e549dcfd","Institute for Infocomm Research, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore; School of EEE, Nanyang Technological University, Singapore 639798, Singapore","Wang, J.-G., Institute for Infocomm Research, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore; Sung, E., School of EEE, Nanyang Technological University, Singapore 639798, Singapore; Venkateswarlu, R., Institute for Infocomm Research, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore","Eye gaze estimation via images has been well investigated and all methods worked on images with the full face. This makes the eye details small and accuracy is affected. We address this problem by zooming in on a single eye. In this paper, we will show the validity of the method and investigate the performance with controlled synthetic data and also with real images. The principle is to rely on the fact that the outer boundary of the iris is a circle. With a fully calibrated camera, its elliptical image can be back-projected into the 3D space yielding two possible circles. To disambiguate, the solution is found by making use of anthropomorphic knowledge of the structure of the eyeball. Hence, getting a larger eye image with a zoom camera enabled us to achieve higher resolutions and thereby higher accuracies. The robustness of the algorithm was verified by extensive statistical trials conducted on synthetic data and real images. The two key contributions in this paper are to show that it is possible to estimate eye gaze with only one eye image and that consequently this achieves higher accuracy of eye gaze estimation. © 2004 Elsevier Inc. All rights reserved.","Circle/ellipse; Eye gaze; Head pose; Iris; One-circle; Zoom camera","Circle/ellipse; Eye gaze; Head pose; Iris; Zoom camera; Algorithms; Cameras; Data reduction; Eye controlled devices; Face recognition; Image communication systems; Optical resolving power; Robustness (control systems); Statistical methods; Computer vision",Article,"Final","",Scopus,2-s2.0-10044264502
"Hansen D.W., Pece A.E.C.","15063910800;35611447500;","Eye tracking in the wild",2005,"Computer Vision and Image Understanding","98","1",,"155","181",,145,"10.1016/j.cviu.2004.07.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044254550&doi=10.1016%2fj.cviu.2004.07.013&partnerID=40&md5=b5dfa5034630a9ff708f53b469ee7e12","IT University Copenhagen, Rued Langgaardsvej 7, 2300 Copenhagen S, Denmark; Heimdall Vision, Bjørnsonsvej 29, 2500 Valby, Denmark; Department of Computer Science, University of Copenhagen, Universitetsparken 1, 2100 Copenhagen, Denmark","Hansen, D.W., IT University Copenhagen, Rued Langgaardsvej 7, 2300 Copenhagen S, Denmark; Pece, A.E.C., Heimdall Vision, Bjørnsonsvej 29, 2500 Valby, Denmark, Department of Computer Science, University of Copenhagen, Universitetsparken 1, 2100 Copenhagen, Denmark","An active contour tracker is presented which can be used for gaze-based interaction with off-the-shelf components. The underlying contour model is based on image statistics and avoids explicit feature detection. The tracker combines particle filtering with the EM algorithm. The method exhibits robustness to light changes and camera defocusing; consequently, the model is well suited for use in systems using off-the-shelf hardware, but may equally well be used in controlled environments, such as in IR-based settings. The method is even capable of handling sudden changes between IR and non-IR light conditions, without changing parameters. For the purpose of determining where the user is looking, calibration is usually needed. The number of calibration points used in different methods varies from a few to several thousands, depending on the prior knowledge used on the setup and equipment. We examine basic properties of gaze determination when the geometry of the camera, screen, and user is unknown. In particular we present a lower bound on the number of calibration points needed for gaze determination on planar objects, and we examine degenerate configurations. Based on this lower bound we apply a simple calibration procedure, to facilitate gaze estimation. © 2004 Elsevier Inc. All rights reserved.","Condensation; Contour tracking; Expectation maximization; Eye tracking; Gaze estimation; Particle filter","Cameras; Computational geometry; Eye movements; Feature extraction; Focusing; Imaging techniques; Infrared radiation; Optimization; Parameter estimation; Tracking (position); Contour tracking; Expectation maximization; Eye tracking; Gaze estimation; Particle filters; Computer vision",Article,"Final","",Scopus,2-s2.0-10044254550
"Miyake T., Haruta S., Horihata S.","57205594283;8212471200;6602930456;","Eye-gaze estimation by using features irrespective of face direction",2005,"Systems and Computers in Japan","36","3",,"18","26",,5,"10.1002/scj.20143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-15244352003&doi=10.1002%2fscj.20143&partnerID=40&md5=a851e041788fde16df32c0e083f87336","Dept. of Prod. Systems Engineering, Toyohashi University of Technology, Toyohashi, 441-8580, Japan","Miyake, T., Dept. of Prod. Systems Engineering, Toyohashi University of Technology, Toyohashi, 441-8580, Japan; Haruta, S., Dept. of Prod. Systems Engineering, Toyohashi University of Technology, Toyohashi, 441-8580, Japan; Horihata, S., Dept. of Prod. Systems Engineering, Toyohashi University of Technology, Toyohashi, 441-8580, Japan","Most of the previous methods for constructing a human machine interface that uses a line of sight are divided into two groups according to the method of line detection. One uses a special light source or a hardware device, and the other uses software for image processing. Our proposed method, which is of the latter type, determines whether a person gazes at a camera lens or not by using only a single facial image captured by a video camera. In previous methods of this kind, it is common understanding that the directional angle of a head with respect to the world coordinate system and that of the eye in relation to the head are estimated independently, and the direction of the line is determined by adding one to the other. A feature of the proposed method is that eye-gaze estimation is conducted by using a pair of marks that are attached at a special position on a facial surface, without considering the head direction in space. Our proposed method can be realized easily, and can estimate the eye-gaze in a short time. Experimental results show the effectiveness of the proposed method. © 2005 Wiley Periodicals, Inc.","Facial image; Gaze; Human interface; Line of sight","Computer simulation; Computer software; Human computer interaction; Image processing; Light sources; Video cameras; Facial image; Gaze; Human interface; Line of sight; Feature extraction",Article,"Final","",Scopus,2-s2.0-15244352003
"Martínez J.E., Erol A., Bebis G., Boyle R., Twombly X.","55476496900;35986162000;57204295991;57206170253;13907905800;","Rendering optimizations guided by head-pose estimates and their uncertainty",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","3804 LNCS",,,"253","262",,,"10.1007/11595755_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744805806&doi=10.1007%2f11595755_31&partnerID=40&md5=cd71cdd75d1fd501d1b902922f83b486","Computer Vision Laboratory, University of Nevada, Reno, NV 89557, United States; BioVis Laboratory, NASA Ames Research Center, Moffett Field, CA 94035, United States","Martínez, J.E., Computer Vision Laboratory, University of Nevada, Reno, NV 89557, United States; Erol, A., Computer Vision Laboratory, University of Nevada, Reno, NV 89557, United States; Bebis, G., Computer Vision Laboratory, University of Nevada, Reno, NV 89557, United States; Boyle, R., BioVis Laboratory, NASA Ames Research Center, Moffett Field, CA 94035, United States; Twombly, X., BioVis Laboratory, NASA Ames Research Center, Moffett Field, CA 94035, United States","In virtual environments, head pose and/or eye-gaze estimation can be employed to improve the visual experience of the user by enabling adaptive level of detail during rendering. In this study, we present a real-time system for rendering complex scenes in an immersive virtual environment based on head pose estimation and perceptual level of detail. In our system, the position and orientation of the head are estimated using stereo vision approach and markers placed on a pair of glasses used to view images projected on a stereo display device. The main innovation of our work is the incorporation of uncertainty estimates to improve the visual experience perceived by the user. The estimated pose and its uncertainty are used to determine the desired level of detail for different parts of the scene based on criteria originating from physiological and psychological aspects of human vision. Subject tests have been performed to evaluate our approach. © Springer-Verlag Berlin Heidelberg 2005.",,"Computer vision; Parameter estimation; Physiology; Stereo vision; Uncertain systems; Virtual reality; Complex scenes; Head-pose estimates; Uncertainty estimates; Optimization",Conference Paper,"Final","",Scopus,2-s2.0-33744805806
"Wu H., Chen Q., Wada T.","7405584998;7406336206;36026892800;","Visual direction estimation from a monocular image",2005,"IEICE Transactions on Information and Systems","E88-D","10",,"2277","2285",,13,"10.1093/ietisy/e88-d.10.2277","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645696612&doi=10.1093%2fietisy%2fe88-d.10.2277&partnerID=40&md5=500046b4791713e09366ef41916c6b79","Faculty of Systems Engineering, Wakayama University, Wakayama-shi, 640-8510, Japan; Wakayama University, Japan; IPSJ, Japan; ISCIE, Japan; Human Interface, Japan; RSJ; Department of Computer and Communication Sciences, Wakayama University, Japan; Japanese Society for Artificial Intelligence, Japan; IEEE","Wu, H., Faculty of Systems Engineering, Wakayama University, Wakayama-shi, 640-8510, Japan, Wakayama University, Japan, IPSJ, Japan, ISCIE, Japan, Human Interface, Japan; Chen, Q., Faculty of Systems Engineering, Wakayama University, Wakayama-shi, 640-8510, Japan, Wakayama University, Japan, IPSJ, Japan, RSJ; Wada, T., Faculty of Systems Engineering, Wakayama University, Wakayama-shi, 640-8510, Japan, IPSJ, Japan, Department of Computer and Communication Sciences, Wakayama University, Japan, Japanese Society for Artificial Intelligence, Japan, IEEE","This paper describes a sophisticated method to estimate visual direction using iris contours. This method requires only one monocular image taken by a camera with unknown focal length. In order to estimate the visual direction, we assume the visual directions of both eyes are parallel and iris boundaries are circles in 3D space. In this case, the two planes where the iris boundaries reside are also parallel. We estimate the normal vector of the two planes from the iris contours extracted from an input image by using an extended ""two-circle"" algorithm. Unlike most existing gaze estimation algorithms that require information about eye corners and heuristic knowledge about 3D structure of the eye in addition to the iris contours, our method uses two iris contours only. Another contribution of our method is the ability of estimating the focal length of the camera. It allows one to use a zoom lens to take images and the focal length can be adjusted at any time. The extensive experiments over simulated images and real images demonstrate the robustness and the effectiveness of our method. Copyright © 2005 The Institute of Electronics, Information and Communication Engineers.","Conic-based; Monocular image; Unknown focal length; Visual direction estimation","Algorithms; Eye movements; Heuristic methods; Image analysis; Robustness (control systems); Three dimensional; Conic-based; Monocular images; Unknown focal length; Visual direction estimation; Parameter estimation",Conference Paper,"Final","",Scopus,2-s2.0-33645696612
"Lim E.T., Venkarteswarlu R.","7201710079;7801620526;","Robust face and facial feature tracking for gaze estimation",2004,"Proceedings of SPIE - The International Society for Optical Engineering","5429",,,"534","541",,,"10.1117/12.541994","https://www.scopus.com/inward/record.uri?eid=2-s2.0-8844244111&doi=10.1117%2f12.541994&partnerID=40&md5=db3f4683368c057fa0e31bd09aa28fed","Institute for Infocomm Research, 21, Heng Mui Keng, Terrace, 119613, Singapore","Lim, E.T., Institute for Infocomm Research, 21, Heng Mui Keng, Terrace, 119613, Singapore; Venkarteswarlu, R., Institute for Infocomm Research, 21, Heng Mui Keng, Terrace, 119613, Singapore","Robust, real-time, user-friendly, non-restrictive and fully automatic natural like human computer interfaces are required to move away from the present machine-empowered-technologies to future human-empowered-technologies (HET). As one of HET interface technologies, this paper presents a cost-effective stereo face detection and tracking of facial features for determining facial pose. The object features are extracted using max-median filters and a progressive threshold algorithm, the face is verified on 'prominent feature configuration template'. Once face is confirmed, the features are tracked using dynamic programming filter. The results are impressive. Video clips would be shown during presentation in the symposium.","Dynamic Programming; Human Computer Interaction; Max-median; Stereo Camera","Algorithms; Cameras; Cost effectiveness; Estimation; Face recognition; Human computer interaction; Interfaces (computer); Real time systems; Dynamic programming (DP); Human computer interaction (HCI); Max-median; Stereo camera; Tracking (position)",Conference Paper,"Final","",Scopus,2-s2.0-8844244111
"Kim S.C., Sked M., Ji Q.","16552527900;6504811904;18935108400;","Non-intrusive eye gaze tracking under natural head movements",2004,"Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings","26 III",,,"2271","2274",,22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-11144332043&partnerID=40&md5=5f6f2c3dd4906f46fbff589b49ec1152","Department of Electrical Engineering, Rensselaer Polytechnic Institute, NY, United States","Kim, S.C., Department of Electrical Engineering, Rensselaer Polytechnic Institute, NY, United States; Sked, M., Department of Electrical Engineering, Rensselaer Polytechnic Institute, NY, United States; Ji, Q., Department of Electrical Engineering, Rensselaer Polytechnic Institute, NY, United States","We propose an eye gaze tracking system under natural head movements. The system consists of one CCD camera and two mirrors. Based on geometric and linear algebra calculations, the mirrors rotate to follow head movements in order to keep the eyes within the view of the camera. Our system allows the subjects head to move 30 cm horizontally and 20 cm vertically, with spatial gaze resolutions about 6 degree and 7 degree, respectively and a frame rate about 10 Hz. We also introduce a hierarchical generalized regression neural networks (H-GRNN) scheme to map eye and mirror parameters to gaze, achieving a gaze estimation accuracy of 92% under head movements. The use of H-GRNN also eliminates the need for personal calibration for new subjects since H-GRNN can generalize. Preliminary experiments show our system is accurate and robust in gaze tracking under large head movements.","Eye movement; Gaze tracking; Generalized neural network; Head movement","Bandpass filters; Charge coupled devices; Human computer interaction; Light emitting diodes; Linear algebra; Neural networks; Synchronization; Eye images; Gaze tracking; Generalized neural network; Head movement; Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-11144332043
"Ye G., Corso J.J., Burschka D., Hager G.D.","7103331672;34976388300;6602080187;7102912496;","VICs: A modular HCI framework using spatiotemporal dynamics",2004,"Machine Vision and Applications","16","1",,"13","20",,12,"10.1007/s00138-004-0159-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-10844286441&doi=10.1007%2fs00138-004-0159-0&partnerID=40&md5=eb3364b67c67a57ef4f3f62fc724a3aa","Compl. Intrac. and Robotics Lab., Johns Hopkins University, 3400 N. Charles St., Baltimoire, MD 21218, United States","Ye, G., Compl. Intrac. and Robotics Lab., Johns Hopkins University, 3400 N. Charles St., Baltimoire, MD 21218, United States; Corso, J.J., Compl. Intrac. and Robotics Lab., Johns Hopkins University, 3400 N. Charles St., Baltimoire, MD 21218, United States; Burschka, D., Compl. Intrac. and Robotics Lab., Johns Hopkins University, 3400 N. Charles St., Baltimoire, MD 21218, United States; Hager, G.D., Compl. Intrac. and Robotics Lab., Johns Hopkins University, 3400 N. Charles St., Baltimoire, MD 21218, United States","Many vision-based human-computer interaction systems are based on the tracking of user actions. Examples include gaze tracking, head tracking, finger tracking, etc. In this paper, we present a framework that employs no user tracking; instead, all interface components continuously observe and react to changes within a local neighborhood. More specifically, components expect a predefined sequence of visual events called visual interface cues (VICs). VICs include color, texture, motion, and geometric elements, arranged to maximize the veridicality of the resulting interface element. A component is executed when this stream of cues has been satisfied. We present a general architecture for an interface system operating under the VIC-based HCI paradigm and then focus specifically on an appearance-based system in which a hidden Markov model (HMM) is employed to learn the gesture dynamics. Our implementation of the system successfully recognizes a button push with a 96% success rate.","Gesture recognition; Human-computer interaction; Vision-based interaction","Algorithms; Human computer interaction; Parameter estimation; Pattern recognition; Problem solving; User interfaces; Facial expressions; Vision-based interaction; Vision-based interfaces; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-10844286441
"Talukder A., Morookian J.-M., Monacos S., Lam R., Lebaw C., Bond A.","55513805900;6602842386;6603706671;7101916947;8298374100;57198259997;","Fast non-invasive eyetracking and eye-gaze determination for biomedical and remote monitoring applications",2004,"Proceedings of SPIE - The International Society for Optical Engineering","5437",,,"179","190",,1,"10.1117/12.548073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-3843050216&doi=10.1117%2f12.548073&partnerID=40&md5=d5e4673b0f3c451757feeeee7067a933","Jet Propulsion Laboratory, In-Situ Instruments Section, Intelligent Instrum./Syst. T.G., 4800 Oak Grove Drive, Pasadena, CA 91109, United States","Talukder, A., Jet Propulsion Laboratory, In-Situ Instruments Section, Intelligent Instrum./Syst. T.G., 4800 Oak Grove Drive, Pasadena, CA 91109, United States; Morookian, J.-M., Jet Propulsion Laboratory, In-Situ Instruments Section, Intelligent Instrum./Syst. T.G., 4800 Oak Grove Drive, Pasadena, CA 91109, United States; Monacos, S., Jet Propulsion Laboratory, In-Situ Instruments Section, Intelligent Instrum./Syst. T.G., 4800 Oak Grove Drive, Pasadena, CA 91109, United States; Lam, R., Jet Propulsion Laboratory, In-Situ Instruments Section, Intelligent Instrum./Syst. T.G., 4800 Oak Grove Drive, Pasadena, CA 91109, United States; Lebaw, C., Jet Propulsion Laboratory, In-Situ Instruments Section, Intelligent Instrum./Syst. T.G., 4800 Oak Grove Drive, Pasadena, CA 91109, United States; Bond, A., Jet Propulsion Laboratory, In-Situ Instruments Section, Intelligent Instrum./Syst. T.G., 4800 Oak Grove Drive, Pasadena, CA 91109, United States","Eyetracking is one of the latest technologies that has shown potential in several areas including human-computer interaction for people with and without disabilities, and for noninvasive monitoring, detection, and even diagnosis of physiological and neurological problems in individuals. Current non-invasive eyetracking methods achieve a 30 Hz rate with possibly low accuracy in gaze estimation, that is insufficient for many applications. We propose a new non-invasive visual eyetracking system that is capable of operating at speeds as high as 6-12 KHz. A new CCD video camera and hardware architecture is used, and a novel fast image processing algorithm leverages specific features of the input CCD camera to yield a real-time eyetracking system. A field programmable gate array (FPGA) is used to control the CCD camera and execute the image processing operations. Initial results show the excellent performance of our system under severe head motion and low contrast conditions.","Ccd (charge coupled device); Detection; Eyetracking; Field programmable gate array (fpga); Human-computer interaction (hci); Image processing hardware; Tracking","Cameras; Demodulation; Diagnosis; Field programmable gate arrays; Human computer interaction; Image processing; Monitoring; Neurology; Particle beam tracking; Audio-based interactions; Eyetracking; Image processing hardware; Non-invasive eyetracking; Charge coupled devices",Conference Paper,"Final","",Scopus,2-s2.0-3843050216
"Yoo D.H., Chung M.J.","7103242493;7402437119;","Eye-mouse under large head movement for human-computer interface",2004,"Proceedings - IEEE International Conference on Robotics and Automation","2004","1",,"237","242",,12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042675646&partnerID=40&md5=56c34331069d37bd36d9a224f9c48ea4","Division of Electrical Engineering, Department of Electrical Engineering, Korea Adv. Inst. of Sci./Technology, 373-1, Guseong-dong, Yuseong-gu, Daejeon, South Korea","Yoo, D.H., Division of Electrical Engineering, Department of Electrical Engineering, Korea Adv. Inst. of Sci./Technology, 373-1, Guseong-dong, Yuseong-gu, Daejeon, South Korea; Chung, M.J., Division of Electrical Engineering, Department of Electrical Engineering, Korea Adv. Inst. of Sci./Technology, 373-1, Guseong-dong, Yuseong-gu, Daejeon, South Korea","In this paper, an eye gaze estimation system is proposed. The eye gaze estimation system estimates the eye-gaze direction of a user. The proposed system consisted of five IR LEDs and a CCD camera. The direction of the user's eye gaze can be computed without computing the geometrical relation among the eye, the camera, and the monitor in 3D space. Our method is comparatively simple and fast. The disabled can manipulate a computer using this system. We will introduce our method and show experimental results. The experimental results will verify the feasibility of the proposed system as an interface for the disabled.","Eye gaze estimation; Projective invariant","Calibration; Charge coupled devices; Eye movements; Imaging techniques; Infrared devices; Light emitting diodes; Reflection; Robots; Robustness (control systems); CCD camera; Eye gaze estimation; Head movement; Projective invariant; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-3042675646
"Zhu Z., Ji Q.","55721576700;18935108400;","Eye and gaze tracking for interactive graphic display",2004,"Machine Vision and Applications","15","3",,"139","148",,165,"10.1007/s00138-004-0139-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-21144452144&doi=10.1007%2fs00138-004-0139-4&partnerID=40&md5=86053705ea63a3b211d1cd49b39c45de","Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, JEC 6219, Troy, NY 12180-3590, United States","Zhu, Z., Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, JEC 6219, Troy, NY 12180-3590, United States; Ji, Q., Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, JEC 6219, Troy, NY 12180-3590, United States","This paper describes a computer vision system based on active IR illumination for real-time gaze tracking for interactive graphic display. Unlike most of the existing gaze tracking techniques, which often require assuming a static head to work well and require a cumbersome calibration process for each person, our gaze tracker can perform robust and accurate gaze estimation without calibration and under rather significant head movement. This is made possible by a new gaze calibration procedure that identifies the mapping from pupil parameters to screen coordinates using generalized regression neural networks (GRNNs). With GRNNs, the mapping does not have to be an analytical function and head movement is explicitly accounted for by the gaze mapping function. Furthermore, the mapping function can generalize to other individuals not used in the training. To further improve the gaze estimation accuracy, we employ a hierarchical classification scheme that deals with the classes that tend to be misclassified. This leads to a 10% improvement in classification error. The angular gaze accuracy is about 5° horizontally and 8° vertically. The effectiveness of our gaze tracker is demonstrated by experiments that involve gaze-contingent interactive graphic display. © Springer-Verlag 2004.","Eye tracking; Gaze estimation; Generalized regression neural networks; Human-computer interaction; Interactive graphic display","Computer vision; Conformal mapping; Display devices; Functions; Human computer interaction; Neural networks; Optical resolving power; Polynomials; Principal component analysis; Regression analysis; Vectors; Eye tracking; Gaze estimation; Generalized regression neural networks (GRNN); Interactive graphic displays; Interactive computer graphics",Article,"Final","",Scopus,2-s2.0-21144452144
[无可用作者姓名],[无可用的作者 ID],"9th Iberoamerican Congress on Pattern Recognition, CIARP 2004",2004,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","3287",,,"1","702",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959018779&partnerID=40&md5=76879959c314abda4d3a23758ed76fb3",,"","The proceedings contain 87 papers. The special focus in this conference is on Progress in Pattern Recognition, Image Analysis and Applications. The topics include: Use of context in automatic annotation of sports videos; content based retrieval of 3D data; a promising framework for computer vision, robotics and learning; adaptive color model for figure-ground segmentation in dynamic environments; real-time infrared object tracking based on mean shift; optimal positioning of sensors in 2D; computer vision algorithms versus traditional methods in food technology; radiance function estimation for object classification; detecting and ranking saliency for scene description; decision fusion for object detection and tracking using mobile cameras; selection of an automated morphological gradient threshold for image segmentation; localization of caption texts in natural scenes using a wavelet transformation; a depth measurement system with the active vision of the striped lighting and rotating mirror; fast noncontinuous path phase-unwrapping algorithm based on gradients and mask; color active contours for tracking roads in natural environments; generation of N-parametric appearance-based models through non-uniform sampling; gaze detection by wide and narrow view stereo camera; a new auto-associative memory based on lattice algebra; image segmentation using morphological watershed applied to cartography; 3D object surface reconstruction using growing self-organised networks; single layer morphological perceptron solution to the N-bit parity problem; robust self-organizing maps; extended associative memories for recalling gray level patterns; new associative memories to recall real-valued patterns and feature maps for non-supervised classification of low-uniform patterns of handwritten letters.",,,Conference Review,"Final","",Scopus,2-s2.0-84959018779
"Brolly X.L.C., Mulligan J.B.","6507673087;7103366589;","Implicit calibration of a remote gaze tracker",2004,"IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2004-January","January","1384930","","",,54,"10.1109/CVPR.2004.366","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932602929&doi=10.1109%2fCVPR.2004.366&partnerID=40&md5=92e9ef5de91718472177b73fe98c6212","NASA Ames Research Center, United States","Brolly, X.L.C., NASA Ames Research Center, United States; Mulligan, J.B., NASA Ames Research Center, United States","We describe a system designed to monitor the gaze of a user working naturally at a computer workstation. The system consists of three cameras situated between the keyboard and the monitor. Free head movements are allowed within a three-dimensional volume approximately 40 centimeters in diameter. Two fixed, wide-field ""face"" cameras equipped with active-illumination systems enable rapid localization of the subject's pupils. A third steerable ""eye"" camera has a relatively narrow field of view, and acquires the images of the eyes which are used for gaze estimation. Unlike previous approaches which construct an explicit three-dimensional representation of the subject's head and eye, we derive mappings for steering control and gaze estimation using a procedure we call implicit calibration. Implicit calibration is performed by collecting a ""training set"" of parameters and associated measurements, and solving for a set of coefficients relating the measurements back to the parameters of interest. Preliminary data on three subjects indicate an median gaze estimation error of ap-proximately 0.8 degree. © 2004 IEEE.",,"Calibration; Cameras; Computer workstations; Pattern recognition; Active illumination; Dimensional representation; Field of views; Gaze estimation; Gaze tracker; Steering control; Three-dimensional volume; Training sets; Computer vision",Conference Paper,"Final","",Scopus,2-s2.0-84932602929
"Wu H., Chen Q., Wada T.","7405584998;7406336206;36026892800;","Estimating the visual direction with two-circle algorithm",2004,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","3338",,,"125","136",,,"10.1007/978-3-540-30548-4_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048845130&doi=10.1007%2f978-3-540-30548-4_15&partnerID=40&md5=899894f20a249d4a06869c5500b7074b","Faculty of Systems Engineering, Wakayama University, Japan","Wu, H., Faculty of Systems Engineering, Wakayama University, Japan; Chen, Q., Faculty of Systems Engineering, Wakayama University, Japan; Wada, T., Faculty of Systems Engineering, Wakayama University, Japan","This paper describes a novel method to estimate visual direction from a single monocular image with ""two-circle"" algorithm. We assume that the visual direction of both eyes is parallel and iris boundaries are circles in 3-D space. Our ""two-circle"" algorithm can estimate the normal vector of the supporting plane of two iris boundaries, from which the direction of the visual direction can be calculated. Most existing gaze estimation algorithms require eye corners and some heuristic knowledge about the structure of the eye as well as the iris contours. In contrast to the exiting methods, ours does not use that additional information. Another advantage of our algorithm is that it does not require the focal length, therefore, it is capable of estimating the visual direction from an image taken by an active camera. The extensive experiments over simulated images and real images demonstrate the robustness and the effectiveness of our method. © Springer-Verlag Berlin Heidelberg 2004.",,"Artificial intelligence; Computer science; Computers; Active camera; Focal lengths; Gaze estimation; Heuristic knowledge; Monocular image; Normal vector; Simulated images; Visual directions; Heuristic algorithms",Article,"Final","",Scopus,2-s2.0-35048845130
"Jeong M.-H., Ohsugi M., Funayama R., Mori H.","9240161200;22433849200;57079962500;57202878512;","Gaze from motion: Towards natural user interfaces",2004,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","3332",,,"736","745",,2,"10.1007/978-3-540-30542-2_91","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048825261&doi=10.1007%2f978-3-540-30542-2_91&partnerID=40&md5=ad455db32120fbed0e35a4916450a9d1","Korea Institute of Science and Technology, South Korea; Toyota Motor Corporation, Japan","Jeong, M.-H., Korea Institute of Science and Technology, South Korea; Ohsugi, M., Toyota Motor Corporation, Japan; Funayama, R., Toyota Motor Corporation, Japan; Mori, H., Toyota Motor Corporation, Japan","We propose a method of 3-D gaze estimation allowing the head motion under an uncalibrated monocular camera system. The paper describes the eyeball structure model with compact descriptions of the eyeball motion and its static 3-D structure. Assuming that the eyeball motion is independent of the head motion, we present a dynamic converging-connected model to make the gaze estimation allowing the head motion more systematic and simple. The gaze estimation is performed through the extended Kalman filter using the eyeball structure model and the dynamic converging-connected model. The preliminary test suggests satisfactory results. © Springer-Verlag Berlin Heidelberg 2004.","Converging-connected model; Eyeball structure; Gaze estimation; Gaze from motion","Kalman filters; 3D structures; Gaze estimation; Gaze from motion; Head motion; Monocular cameras; Natural user interfaces; Structure modeling; Uncalibrated; User interfaces",Article,"Final","",Scopus,2-s2.0-35048825261
"Brolly X.L.C., Stratelos C., Mulligan J.B.","6507673087;6504758738;7103366589;","Model-based head pose estimation for air-traffic controllers",2003,"IEEE International Conference on Image Processing","2",,,"113","116",,11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344687469&partnerID=40&md5=e1167ce8af08b3868c457b5081edd2d6","NASA Ames Research Center, Mail Stop 262-2, Moffett Field, CA 94035-1000, United States; San Jose State University, United States","Brolly, X.L.C., San Jose State University, United States; Stratelos, C., San Jose State University, United States; Mulligan, J.B., NASA Ames Research Center, Mail Stop 262-2, Moffett Field, CA 94035-1000, United States","We present a method for estimating the point of fixation of an air traffic controller from a low resolution video sequence. A geometric model of the head is used to estimate head orientation; head pose estimates are combined with a 3D model of the environment to compute the target of gaze. The head model is constructed from a small set of images. Two lighting models are considered: in the first, we only use ambient lighting; in the second, we add a finite distance point source. In both cases, we jointly estimate the albedo of each facet of the head model and the parameters of the lighting model. Because ground-truth data are unavailable, the absolute accuracy of the gaze estimates is unknown. With either method, the results are sufficiently accurate to answer questions of operational interest, such as ""is the controller looking out the window"".",,"Video sequences; Video telephony; Air traffic control; Data processing; Estimation; Image quality; Interfaces (computer); Iterative methods; Optical resolving power; Vectors; Video signal processing",Conference Paper,"Final","",Scopus,2-s2.0-0344687469
"Wang J.-G., Sung E., Venkateswarlu R.","57223821965;7006254261;56248905000;","Eye gaze estimation from a single image of one eye",2003,"Proceedings of the IEEE International Conference on Computer Vision","1",,,"136","143",,129,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0345414141&partnerID=40&md5=9693adbd1fe1d5f0f927934a5d56c72b","Institute for Infocom Research, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore; School of EEE, Nanyang Technological University, Singapore 639798, Singapore","Wang, J.-G., Institute for Infocom Research, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore; Sung, E., School of EEE, Nanyang Technological University, Singapore 639798, Singapore; Venkateswarlu, R., Institute for Infocom Research, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore","In this paper, we present a novel approach, called the. ""one-circle"" algorithm, for measuring the eye gaze using a monocular image thai zooms in on only one eye of a person. Observing thai the iris contour is a circle, we estimate the normal direction of this iris circle, considered as the eye gaze, from its elliptical image. From basic protective geometry, an ellipse can be back-projected into space onto two circles of different orientations. However, by using an anthropometric property of the eyeball, the correct solution can be disambiguated. This allows us to obtain a higher resolution image of the iris with a zoom-in camera and thereby achieving higher accuracies in the estimation. The robustness of our gaze determination approach was verified statistically by the extensive experiments on synthetic and real image data. The two key contributions in this paper are that we show the possibility of finding the unique eye gaze direction from a single image of one eye and thai one can obtain better accuracy as a consequence of this.",,"Algorithms; Computational geometry; Computer simulation; Eye movements; Image analysis; Image quality; Three dimensional computer graphics; Video cameras; Algorithms; Cameras; Computer vision; Edge detection; Electric connectors; Face recognition; Image resolution; Robustness (control systems); Eye gaze estimation; Iris detection; One circle algorithm; Computer vision; Geometry; Head; Humans; Image edge detection; Iris; Joining process",Conference Paper,"Final","",Scopus,2-s2.0-0345414141
"Ohno T., Mukawa N., Kawato S.","7402658914;7003913791;55372395400;","Just blink your eyes: A head-free gaze tracking system",2003,"Conference on Human Factors in Computing Systems - Proceedings",,,,"950","951",,53,"10.1145/765891.766088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869004472&doi=10.1145%2f765891.766088&partnerID=40&md5=4d5f00fda2470d4c6823c7f933b79dcb","NTT Communication Science Labs., NTT Corporation, 3-1 Morinosato Wakamiya, Atsugi, Kanagawa, 243-0198, Japan; ATR Media Information Science Labs., 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288, Japan","Ohno, T., NTT Communication Science Labs., NTT Corporation, 3-1 Morinosato Wakamiya, Atsugi, Kanagawa, 243-0198, Japan; Mukawa, N., NTT Communication Science Labs., NTT Corporation, 3-1 Morinosato Wakamiya, Atsugi, Kanagawa, 243-0198, Japan; Kawato, S., ATR Media Information Science Labs., 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0288, Japan","We propose a head-free, easy-setup gaze tracking system designed for a gaze-based Human-Computer Interaction. Our system enables the user to interact with the computer soon after catching the user's eye blinks. The user can move his/her head freely since the system keeps tracking the user's eye. In addition, our system only needs a 10 second calibration procedure at the very first time of use. An eye tracking method based on our unique eye blink detection and a sophisticated gaze estimation method using the geometrical eyeball model realize these advantages.","Eye blink detection; Eye movement; Eye tracking; Eyeball model; Gaze tracking","Calibration procedure; Eye blink; Eye tracking methods; Eye-tracking; Gaze estimation; Gaze tracking; Gaze tracking system; Time of use; Human computer interaction; Human engineering; Tracking (position); Eye movements",Conference Paper,"Final","",Scopus,2-s2.0-84869004472
"Ye G., Corso J., Burschka D., Hager G.D.","7103331672;34976388300;6602080187;7102912496;","VICs: A modular vision-based HCI framework",2003,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","2626",,,"257","267",,10,"10.1007/3-540-36592-3_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886606642&doi=10.1007%2f3-540-36592-3_25&partnerID=40&md5=33c111f16cf800daf17f3b55dcb7b0be","Johns Hopkins University, Computational Interaction and Robotics Laboratory, United States","Ye, G., Johns Hopkins University, Computational Interaction and Robotics Laboratory, United States; Corso, J., Johns Hopkins University, Computational Interaction and Robotics Laboratory, United States; Burschka, D., Johns Hopkins University, Computational Interaction and Robotics Laboratory, United States; Hager, G.D., Johns Hopkins University, Computational Interaction and Robotics Laboratory, United States","Many Vision-Based Human-Computer Interaction (VB-HCI) systems are based on the tracking of user actions. Examples include gaze-tracking, head-tracking, finger-tracking, and so forth. In this paper, we present a framework that employs no user-tracking; instead, all interface components continuously observe and react to changes within a local image neighborhood. More specifically, components expect a predefined sequence of visual events called Visual Interface Cues (VICs). VICs include color, texture, motion and geometric elements, arranged to maximize the veridicality of the resulting interface element. A component is executed when this stream of cues has been satisfied. We present a general architecture for an interface system operating under the VIC-Based HCI paradigm, and then focus specifically on an appearance-based system in which a Hidden Markov Model (HMM) is employed to learn the gesture dynamics. Our implementation of the system successfully recognizes a button-push with a 96% success rate. The system operates at frame-rate on standard PCs. © Springer-Verlag Berlin Heidelberg 2003.",,"Hidden Markov models; Human computer interaction; Textures; Appearance based; General architectures; Geometric elements; Head tracking; Interface elements; Interface system; Modular vision; Visual Interface; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-84886606642
"Ji Q., Zhu Z.","18935108400;55721576700;","Eye and gaze tracking for interactive graphic display",2002,"ACM International Conference Proceeding Series","22",,,"79","85",,14,"10.1145/569005.569017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953892903&doi=10.1145%2f569005.569017&partnerID=40&md5=cc080ab5129aa1712f3317febe8e17f9","Dept. of Electrical, Computer, and Systems Eng., Rensselaer Polytechnic Institute, United States; Dept. of Computer Science, Univ. of Nevada, Reno, NV, United States","Ji, Q., Dept. of Electrical, Computer, and Systems Eng., Rensselaer Polytechnic Institute, United States; Zhu, Z., Dept. of Computer Science, Univ. of Nevada, Reno, NV, United States","This paper describes preliminary results we have obtained in developing a computer vision system based on active IR illumination for real time gaze tracking for interactive graphic display. Unlike most of the existing gaze tracking techniques, which often require assuming a static head to work well and require a cumbersome calibration process for each person, our gaze tracker can perform robust and accurate gaze estimation without calibration and under rather significant head movement. This is made possible by a new gaze calibration procedure that identifies the mapping from pupil parameters to screen coordinates using the Generalized Regression Neural Networks (GRNN). With GRNN, the mapping does not have to be an analytical function and head movement is explicitly accounted for by the gaze mapping function. Furthermore, the mapping function can generalize to other individuals not used in the training. The effectiveness of our gaze tracker is demonstrated by preliminary experiments that involve gaze-contingent interactive graphic display. Copyright 2002 ACM.","eye tracking; gaze estimation; HCI; interactive graphic display","Analytical functions; Calibration procedure; Calibration process; Computer vision system; Eye-tracking; Gaze estimation; Gaze tracker; Gaze tracking; Gaze-contingent; Generalized regression neural networks; Head movements; Interactive graphic display; Mapping functions; Real time; Calibration; Computer vision; Estimation; Neural networks; Mapping",Conference Paper,"Final","",Scopus,2-s2.0-77953892903
"Wang J.-G., Sung E.","57223821965;7006254261;","Study on eye gaze estimation",2002,"IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics","32","3",,"332","350",,142,"10.1109/TSMCB.2002.999809","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036608309&doi=10.1109%2fTSMCB.2002.999809&partnerID=40&md5=eca2b854a41c3dc67233345864ea108e","Centre for Signal Processing, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; Division of Control and Instrumentation, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore","Wang, J.-G., Centre for Signal Processing, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; Sung, E., Division of Control and Instrumentation, School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore","There are two components to the human visual line-of-sight: pose of human head and the orientation of the eye within their sockets. We have investigated these two aspects but will concentrate on the eye gaze estimation in this paper. We present a novel approach called the ""one-circle"" algorithm for measuring the eye gaze using a monocular image that zooms in on only one eye of a person. Observing that the iris contour is a circle, we estimate the normal direction of this iris circle, considered as the eye gaze, from its elliptical image. From basic projective geometry, an ellipse can be back-projected into space onto two circles of different orientations. However, by using a geometric constraint, namely, that the distance between the eyeball's center and the two eye corners should be equal to each other, the correct solution can be disambiguated. This allows us to obtain a higher resolution image of the iris with a zoom-in camera, thereby achieving higher accuracies in the estimation. A general approach that combines head pose determination with eye gaze estimation is also proposed. The searching of the eye gaze is guided by the head pose information. The robustness of our gaze determination approach was verified statistically by the extensive experiments on synthetic and real image data. The two key contributions in this paper are that we show the possibility of finding the unique eye gaze direction from a single image of one eye and that one can obtain better accuracy as a consequence of this.","Circle/ellipse; Distance constraint; Eye gaze; Head pose; Human-machine interaction; Iris contour; Monocular vision; One-circle; Point-of-regard","Algorithms; Calibration; Cameras; Image analysis; Monocular vision; Human computer interaction",Article,"Final","",Scopus,2-s2.0-0036608309
"Ji Q.","18935108400;","3D face pose estimation and tracking from a monocular camera",2002,"Image and Vision Computing","20","7",,"499","511",,87,"10.1016/S0262-8856(02)00024-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036567664&doi=10.1016%2fS0262-8856%2802%2900024-0&partnerID=40&md5=720e3397c38a105ec712e02f25f650d2","Department of Electrical, Computer and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180, United States","Ji, Q., Department of Electrical, Computer and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180, United States","In this paper, we describe a new approach for estimating and tracking three-dimensional (3D) pose of a human face from the face images obtained from a single monocular view with full perspective projection. We assume that the shape of a 3D face can be approximated by an ellipse and that the aspect ratio of 3D face ellipse is given. Given a monocular image of a face, we first perform an ellipse detection to locate the face in the image and the 3D position and orientation of the face are then estimated from the detected image face. The face detection is greatly facilitated by exploring the physiological properties of eyes under a special IR illumination and some geometric constraints. The detected initial face ellipse is then tracked in subsequent frames, allowing to track 3D face pose from frame to frame. Compared with the existing feature-based approaches for face pose estimation, our approach is more robust, since ellipse can be more reliably and robustly detected and tracked. Experimental study using a large number of synthetic and real images demonstrates the accuracy and robustness of the proposed approach. © 2002 Elsevier Science B.V. All rights reserved.","Face detection; Face tracking; Gaze estimation; Pose estimation","Aspect ratio; Cameras; Feature extraction; Human computer interaction; Infrared radiation; Monocular cameras; Face recognition",Article,"Final","",Scopus,2-s2.0-0036567664
"Tan K.-H., Kriegman D.J., Ahuja N.","7403999168;7005060164;35515078200;","Appearance-based eye gaze estimation",2002,"Proceedings of IEEE Workshop on Applications of Computer Vision","2002-January",,"1182180","191","195",,176,"10.1109/ACV.2002.1182180","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948750450&doi=10.1109%2fACV.2002.1182180&partnerID=40&md5=bd35bf779146525bba10b2d9c09932f1","Department of CS, Beckman Institute, University of Illinois, Urbana-Champaign, IL, United States; Department of CS, University of California, San Diego, CA, United States; Department of ECE, Beckman Institute, University of Illinois, Urbana-Champaign, IL, United States","Tan, K.-H., Department of CS, Beckman Institute, University of Illinois, Urbana-Champaign, IL, United States; Kriegman, D.J., Department of CS, University of California, San Diego, CA, United States; Ahuja, N., Department of ECE, Beckman Institute, University of Illinois, Urbana-Champaign, IL, United States","We present a method for estimating eye gaze direction, which represents a departure from conventional eye gaze estimation methods, the majority of which are based on tracking specific optical phenomena like corneal reflection and the Purkinje images. We employ an appearance manifold model, but instead of using a densely sampled spline to perform the nearest manifold point query, we retain the original set of sparse appearance samples and use linear interpolation among a small subset of samples to approximate the nearest manifold point. The advantage of this approach is that since we are only storing a sparse set of samples, each sample can be a high dimensional vector that retains more representational accuracy than short vectors produced with dimensionality reduction methods. The algorithm was tested with a set of eye images labelled with ground truth point-of-regard coordinates. We have found that the algorithm is capable of estimating eye gaze with a mean angular error of 0.38 degrees, which is comparable to that obtained by commercially available eye trackers. © 2002 IEEE.","Application software; Deformable models; Displays; Gray-scale; Humans; Interpolation; Iris; Optical reflection; Spline; Vectors","Application programs; Computer vision; Display devices; Estimation; Splines; Vectors; Deformable models; Gray scale; Humans; Iris; Optical reflection; Interpolation",Conference Paper,"Final","",Scopus,2-s2.0-84948750450
"Yoo D.H., Kim J.H., Kim D.H., Chung M.J.","7103242493;57191682420;56984348200;7402437119;","A human-robot interface using vision-based eye gaze estimation system",2002,"IEEE International Conference on Intelligent Robots and Systems","2",,,"1196","1201",,10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036450936&partnerID=40&md5=825b3cc215f701cef5af2f08c0e512e4","Div. of Elec. Eng., Dept. of EECS, KAIST, 373-1, Guseong-dong Yuseong-gu, Daejeon, South Korea","Yoo, D.H., Div. of Elec. Eng., Dept. of EECS, KAIST, 373-1, Guseong-dong Yuseong-gu, Daejeon, South Korea; Kim, J.H., Div. of Elec. Eng., Dept. of EECS, KAIST, 373-1, Guseong-dong Yuseong-gu, Daejeon, South Korea; Kim, D.H., Div. of Elec. Eng., Dept. of EECS, KAIST, 373-1, Guseong-dong Yuseong-gu, Daejeon, South Korea; Chung, M.J., Div. of Elec. Eng., Dept. of EECS, KAIST, 373-1, Guseong-dong Yuseong-gu, Daejeon, South Korea","Recently, service area has been emerging field of robotic applications. Even though assistant robots play an important role for the disabled and the elderly, they still suffer from operating the robots using conventional interface devices such as joysticks or keyboards. In this paper, we suggest a human-robot interface using a new eye gaze estimation system. The eye gaze estimation system can estimate the direction of a user's eye gaze. The proposed system consisted of five IR LEDs and a CCD camera. The direction of the user's eye gaze can be computed without computing the geometrical relation among the eye, the camera, and the monitor in 3D space. Our method is comparatively simple and fast. The disabled, who is especially people with motor disabilities, can control a robot using this system. We will introduce the method and show experimental results. The experimental results will verify the feasibility of the proposed system as a human-robot interface for the disabled.",,"Charge coupled devices; Computer vision; Light emitting diodes; Light reflection; Robotics; Three dimensional computer graphics; User interfaces; Video cameras; Eye gaze estimation; Human-robot interface; Human computer interaction",Conference Paper,"Final","",Scopus,2-s2.0-0036450936
"Wang J.-G., Sung E.","57223821965;7006254261;","Gaze determination via images of irises",2001,"Image and Vision Computing","19","12",,"891","911",,53,"10.1016/S0262-8856(01)00051-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0347238295&doi=10.1016%2fS0262-8856%2801%2900051-8&partnerID=40&md5=e819e86b9ca716fcaace10deabb82fdb","Center for Signal Processing, Sch. of Elec. and Electron. Eng., Nanyang Technological University, Nanyang Avenue, Singapore 639798, Singapore; Div. of Control and Instrumentation, Sch. of Elec. and Electron. Eng., Nanyang Technological University, Nanyang Avenue, Singapore, 639798, Singapore","Wang, J.-G., Center for Signal Processing, Sch. of Elec. and Electron. Eng., Nanyang Technological University, Nanyang Avenue, Singapore 639798, Singapore; Sung, E., Div. of Control and Instrumentation, Sch. of Elec. and Electron. Eng., Nanyang Technological University, Nanyang Avenue, Singapore, 639798, Singapore","In this paper, we present a new approach for measuring eye-gaze of human via images of the irises. Two iris contours are approximately modeled as two circles having known radii and we estimate the ellipses of their projections onto image plane, respectively. The eye-gaze, defined in this paper as the unit surface normal to the supporting plane of the iris, can be estimated from the projection of the iris contour (limbus). The unique solution of the eye-gaze was obtained based on a geometric constraint, namely that the difference between the two normal directions to the supporting planes of the left and right iris should be minimal, irrespective of eyeball rotations and head movements. Furthermore, the 3D location of the center of the irises can be calculated if the radius of the iris contour is given. The robustness of this approach was verified by experiments on synthetic and real image data. In order to obtain a higher resolution of the iris, a zoom-in camera is used. This creates an additional problem of having to relate the camera for estimating the head-pose to the iris camera. A general approach that combines head-pose determination with gaze estimation is proposed. © 2001 Elsevier Science B.V. All rights reserved.","Circle/ellipse; Focus point; Gaze determination; Human - machine interaction; Iris contour; Model-based monocular vision",,Article,"Final","",Scopus,2-s2.0-0347238295
"Ji Q., Yang X.","18935108400;7406506681;","Real time visual cues extraction for monitoring driver vigilance",2001,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","2095",,,"107","124",,47,"10.1007/3-540-48222-9_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937438273&doi=10.1007%2f3-540-48222-9_8&partnerID=40&md5=3ad7c00ab20e5a905d31fb56779bc58f","Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, United States; Department of Computer Science, University of Nevada, Reno, United States","Ji, Q., Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, United States; Yang, X., Department of Computer Science, University of Nevada, Reno, United States","This paper describes a real-time prototype computer vision system for monitoring driver vigilance. The main components of the system consists of a specially-designed hardware system for real time image acquisition and for controlling the illuminator and the alarm system, and various computer vision algorithms for real time eye tracking, eyelid movement monitoring, face pose discrimination, and gaze estimation. Specific contributions include the development of an infrared illuminator to produce the desired bright/dark pupil effect, the development a digital circuitry to perform real time image subtraction, and the development of numerous real time computer vision algorithms for eye tracking, face orientation discrimination, and gaze tracking. The system was tested extensively in a simulating environment with subjects of different ethnic backgrounds, different genders, ages, with/without glasses, and under different illumination conditions, and it was found very robust, reliable and accurate. © Springer-Verlag Berlin Heidelberg 2001.",,"Alarm systems; Computer hardware; Computer vision; Eye movements; Computer vision algorithms; Computer vision system; Illumination conditions; Infrared illuminators; Movement monitoring; Real-time computer vision; Real-time eye tracking; Real-time image acquisitions; Eye tracking",Conference Paper,"Final","",Scopus,2-s2.0-84937438273
[无可用作者姓名],[无可用的作者 ID],"Proceedings - 4th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2000",2000,"Proceedings - 4th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2000",,,,"","",569,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905402098&partnerID=40&md5=708639a051eccf50e586a97b3f3d11a2",,"","The proceedings contain 84 papers. The topics discussed include: real-time multiple face detection using active illumination; face detection using multi-modal information; viewpoint-invariant learning and detection of human heads; face shape extraction and recognition using 3d morphing and distance mapping; comprehensive database for facial expression analysis; comparative performance of different skin chrominance models and chrominance spaces for the automatic detection of human faces in color images; face detection using mixtures of linear subspaces; a fast and accurate face detector for indexation of face images; robust face tracking using color; learning-based approach to real time tracking and analysis of faces; self-organized integration of adaptive visual cues for face tracking; and real-time stereo tracking for head pose and gaze estimation.",,,Conference Review,"Final","",Scopus,2-s2.0-84905402098
"Newman R., Matsumoto Y., Rougeaux S., Zelinsky A.","16741229700;7404547304;6603005747;7005972235;","Real-time stereo tracking for head pose and gaze estimation",2000,"Proceedings - 4th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2000",,,"840622","122","128",,91,"10.1109/AFGR.2000.840622","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70450219222&doi=10.1109%2fAFGR.2000.840622&partnerID=40&md5=54d83f10587e6e3344f85b04bb9b6bed","Research School of Information Sciences and Engineering, Australian National University, Canberra ACT 0200, Australia","Newman, R., Research School of Information Sciences and Engineering, Australian National University, Canberra ACT 0200, Australia; Matsumoto, Y., Research School of Information Sciences and Engineering, Australian National University, Canberra ACT 0200, Australia; Rougeaux, S., Research School of Information Sciences and Engineering, Australian National University, Canberra ACT 0200, Australia; Zelinsky, A., Research School of Information Sciences and Engineering, Australian National University, Canberra ACT 0200, Australia","Computer systems which analyse human face/head motion have attracted significant attention recently as there are a number of interesting and useful applications. Not least among these is the goal of tracking the head in real time. A useful extension of this problem is to estimate the subject's gaze point in addition to his/her head pose. This paper describes a real-time stereo vision system which determines the head pose and gaze direction of a human subject. Its accuracy makes it useful for a number of applications including human/computer interaction, consumer research and ergonomic assessment. © 2000 IEEE.",,"Computer vision; Stereo image processing; Stereo vision; Consumer research; Ergonomic assessment; Gaze direction; Gaze estimation; Gaze point; Human faces; Human subjects; Real time stereo; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-70450219222
"Betke M., Kawai J.","7004214113;57200182938;","Gaze detection via self-organizing gray-scale units",1999,"Proceedings - International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems, RATFG-RTS 1999",,,"799226","70","76",,24,"10.1109/RATFG.1999.799226","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957043975&doi=10.1109%2fRATFG.1999.799226&partnerID=40&md5=7149c29deb78eb6e403c40c92c811da7","Computer Science Department, Boston College, Chestnut Hill, MA  02467-3808, United States","Betke, M., Computer Science Department, Boston College, Chestnut Hill, MA  02467-3808, United States; Kawai, J., Computer Science Department, Boston College, Chestnut Hill, MA  02467-3808, United States","We present a gaze estimation algorithm that detects an eye in a face image and estimates the gaze direction by computing the position of the pupil with respect to the center of the eye. The algorithm is information conserving and based on unsupervised learning. It creates a map of self- organized gray-scale image units that collectively learn to describe the eye outline.",,"Face recognition; Interactive computer systems; Face images; Gaze detection; Gaze direction; Gaze estimation; Gray scale; Gray-scale images; Real time systems",Conference Paper,"Final","",Scopus,2-s2.0-84957043975
"Rikert T.D., Jones M.J.","6507263446;55700793100;","Gaze estimation using morphable models",1998,"Proceedings - 3rd IEEE International Conference on Automatic Face and Gesture Recognition, FG 1998",,,"670987","436","441",,45,"10.1109/AFGR.1998.670987","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905406299&doi=10.1109%2fAFGR.1998.670987&partnerID=40&md5=ef64ae15a5c5d689be6bb5c7376e31bb","Digital Equipment Corporation, Cambridge Research Lab, One Kendall Sq., Bldg 700, Cambridge, MA 02139, United States","Rikert, T.D., Digital Equipment Corporation, Cambridge Research Lab, One Kendall Sq., Bldg 700, Cambridge, MA 02139, United States; Jones, M.J., Digital Equipment Corporation, Cambridge Research Lab, One Kendall Sq., Bldg 700, Cambridge, MA 02139, United States","The paper presents preliminary work on a novel technique for gaze estimation from a single image. The goal is to provide rough estimates of where a person is looking at a monitor. Many applications for human-computer interaction are possible for such a technique. The approach uses the morphable model framework of Jones and Poggio (1998) to model a region around the eyes of a person. After matching this model to an input image of a person's eye region, the resulting model parameters are sent to a neural network which approximates the screen coordinates being viewed. The system is user independent and can handle changes in both head orientation and iris location. Currently the system is not real-time. Future work will focus on improving the speed of the system. © 1998 IEEE.",,"Eye regions; Gaze estimation; Iris location; Model parameters; Morphable model; Novel techniques; Single images; User independents; Gesture recognition",Conference Paper,"Final","",Scopus,2-s2.0-84905406299
"Klingspohr H., Block T., Grigat R.-R.","57214634372;57130194300;6601907420;","A passive real-time gaze estimation system a passive real-time gaze estimation system for human-machine interfaces",1997,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","1296",,,"718","725",,4,"10.1007/3-540-63460-6_183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-21744459709&doi=10.1007%2f3-540-63460-6_183&partnerID=40&md5=f28e223dbf5eb1929f4f7626fc87e5cf","Technische Universität Hamburg-Harburg, Technische Informatik I, Harburger Schloßstraße 20, Hamburg, 21079, Germany","Klingspohr, H., Technische Universität Hamburg-Harburg, Technische Informatik I, Harburger Schloßstraße 20, Hamburg, 21079, Germany; Block, T., Technische Universität Hamburg-Harburg, Technische Informatik I, Harburger Schloßstraße 20, Hamburg, 21079, Germany; Grigat, R.-R., Technische Universität Hamburg-Harburg, Technische Informatik I, Harburger Schloßstraße 20, Hamburg, 21079, Germany","A gaze detection system with real-time properties is described. In contrast to most systems available today it is fully passive and only relies on the images taken by a CCD camera without special illumination.To avoid the computational intensive algorithms usually associated with Hough transforms for ellipses (e.g. [Bal81], [TMT8]) a two-stage approach is presented, in which the Hough transform yields a segmented edge image used for a two-dimensional fitting algorithm. The algorithm has been implemented; experimental results are reported. © Springer-Verlag Berlin Heidelberg 1997.",,"CCD cameras; Feature extraction; Gesture recognition; Hough transforms; Image analysis; Edge image; Fitting algorithms; Gaze detection; Gaze estimation; Human Machine Interface; Real time; Real-time properties; Two stage approach; Edge detection",Conference Paper,"Final","",Scopus,2-s2.0-21744459709
"Gee A., Cipolla R.","7102175461;7006935878;","Determining the gaze of faces in images",1994,"Image and Vision Computing","12","10",,"639","647",,176,"10.1016/0262-8856(94)90039-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028728305&doi=10.1016%2f0262-8856%2894%2990039-6&partnerID=40&md5=e1d40965990dd2ecba9935c4a2b3f789","University of Cambridge, Department of Engineering, Trumpington Street, Cambridge, CB2 1PZ, United Kingdom","Gee, A., University of Cambridge, Department of Engineering, Trumpington Street, Cambridge, CB2 1PZ, United Kingdom; Cipolla, R., University of Cambridge, Department of Engineering, Trumpington Street, Cambridge, CB2 1PZ, United Kingdom","A person's gaze is a potentially powerful input device for human-computer interaction. Current approaches to gaze tracking tend to be highly intrusive: the subject must either remain perfectly still, or wear cumbersome headgear to maintain a constant separation between the sensor and the eye. This paper describes a more flexible vision-based approach, which can estimate the direction of gaze from a single, monocular view of a face. The technique makes minimal assumptions about the structure of the face, requires lew image measurements, and produces an accurate estimate of the facial orientation, which is relatively insensitive to noise in the image and errors in the underlying assumptions. The computational requirements are insignificant, so with automatic tracking of a few facial features it is possible to produce gaze estimates at video rate. © 1994.","gaze tracking; human-computer interaction; real-time feature tracking; symmetry; weak perspective","Computer simulation; Estimation; Eye movements; Human computer interaction; Image processing; User interfaces; Virtual reality; Faces; Facial orientation; Gaze tracking; Real time feature tracking; Symmetry; Weak perspective; Computer vision",Article,"Final","",Scopus,2-s2.0-0028728305
